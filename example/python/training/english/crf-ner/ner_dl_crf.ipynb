{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m32x7R0tyHH6"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/crf-ner/ner_dl_crf.ipynb)\n",
        "\n",
        "## 0. Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95EqKJCoySwe",
        "outputId": "45aa98d4-e6be-49f8-f491-29ba3af04171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-23 15:09:40--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://setup.johnsnowlabs.com/colab.sh [following]\n",
            "--2022-12-23 15:09:40--  https://setup.johnsnowlabs.com/colab.sh\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2022-12-23 15:09:41--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1191 (1.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-12-23 15:09:41 (72.2 MB/s) - written to stdout [1191/1191]\n",
            "\n",
            "Installing PySpark 3.2.3 and Spark NLP 4.2.6\n",
            "setup Colab for PySpark 3.2.3 and Spark NLP 4.2.6\n",
            "\u001b[K     |████████████████████████████████| 281.5 MB 67 kB/s \n",
            "\u001b[K     |████████████████████████████████| 453 kB 74.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 199 kB 58.6 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxBXISVpyHIA"
      },
      "source": [
        "## CRF Named Entity Recognition\n",
        "In the following example, we walk-through a Conditional Random Fields NER model training and prediction.\n",
        "\n",
        "This challenging annotator will require the user to provide either a labeled dataset during fit() stage, or use external CoNLL 2003 resources to train. It may optionally use an external word embeddings set and a list of additional entities.\n",
        "\n",
        "The CRF Annotator will also require Part-of-speech tags so we add those in the same Pipeline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jePQ1RvIyHIC"
      },
      "source": [
        "#### 1. Call necessary imports and set the resource path to read local data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yljiat0_yHIE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *\n",
        "\n",
        "import time\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9kYUUsYyHIP"
      },
      "source": [
        "#### 2. Download training dataset if not already there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKJQpW57yHIR",
        "outputId": "17a986a0-d32d-4a91-b294-3b9f274fa0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File Not found will downloading it!\n"
          ]
        }
      ],
      "source": [
        "# Download CoNLL 2003 Dataset\n",
        "import os\n",
        "from pathlib import Path\n",
        "import urllib.request\n",
        "\n",
        "if not Path(\"eng.train\").is_file():\n",
        "    print(\"File Not found will downloading it!\")\n",
        "    url = \"https://github.com/patverga/torch-ner-nlp-from-scratch/raw/master/data/conll2003/eng.train\"\n",
        "    urllib.request.urlretrieve(url, 'eng.train')\n",
        "else:\n",
        "    print(\"File already present.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7ixJew3yHIc"
      },
      "source": [
        "#### 3. Load SparkSession if not already there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ENw-DRoyHIe",
        "outputId": "1bdf3d19-e8e8-43b3-c826-4fb4074cb5e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  4.2.6\n",
            "Apache Spark version:  3.2.3\n"
          ]
        }
      ],
      "source": [
        "import sparknlp \n",
        "\n",
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmJej2gGyHIp"
      },
      "source": [
        "#### 4. Create annotator components in the right order, with their training Params. Finisher will output only NER. Put all in pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6uKWI52qyHIs"
      },
      "outputs": [],
      "source": [
        "nerTagger = NerCrfApproach()\\\n",
        "  .setInputCols([\"sentence\", \"token\", \"pos\", \"embeddings\"])\\\n",
        "  .setLabelColumn(\"label\")\\\n",
        "  .setOutputCol(\"ner\")\\\n",
        "  .setMinEpochs(1)\\\n",
        "  .setMaxEpochs(1)\\\n",
        "  .setLossEps(1e-3)\\\n",
        "  .setL2(1)\\\n",
        "  .setC0(1250000)\\\n",
        "  .setRandomSeed(0)\\\n",
        "  .setVerbose(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVRUX_hpyHIz"
      },
      "source": [
        "#### 6. Load a dataset for prediction. Training is not relevant from this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edIbRJiNyHI1",
        "outputId": "37d8cf0a-a119-4a82-c185-fa2f52bcc8fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|                 pos|               label|          embeddings|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|EU rejects German...|[{document, 0, 47...|[{document, 0, 47...|[{token, 0, 1, EU...|[{pos, 0, 1, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
            "|     Peter Blackburn|[{document, 0, 14...|[{document, 0, 14...|[{token, 0, 4, Pe...|[{pos, 0, 4, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
            "| BRUSSELS 1996-08-22|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 7, BR...|[{pos, 0, 7, NNP,...|[{named_entity, 0...|[{word_embeddings...|\n",
            "|The European Comm...|[{document, 0, 18...|[{document, 0, 18...|[{token, 0, 2, Th...|[{pos, 0, 2, DT, ...|[{named_entity, 0...|[{word_embeddings...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sparknlp.training import CoNLL\n",
        "conll = CoNLL()\n",
        "data = conll.readDataset(spark, path='eng.train')\n",
        "\n",
        "embeddings = WordEmbeddingsModel.pretrained()\\\n",
        ".setOutputCol('embeddings')\n",
        "\n",
        "ready_data = embeddings.transform(data)\n",
        "\n",
        "ready_data.show(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUPzqnGqyHI-"
      },
      "source": [
        "#### 7. Training the model. Training doesn't really do anything from the dataset itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bCbmaLDyHI_",
        "outputId": "5321a525-1efe-4a82-d75b-9854f8f60564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start fitting\n",
            "Fitting has ended\n",
            "5.912823915481567\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "print(\"Start fitting\")\n",
        "ready_data = ready_data.limit(100)\n",
        "ner_model = nerTagger.fit(ready_data)\n",
        "print(\"Fitting has ended\")\n",
        "print (time.time() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7vA5MiOyHJH"
      },
      "source": [
        "#### 8. Save NerCrfModel into disk after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "n_LY10D9yHJJ"
      },
      "outputs": [],
      "source": [
        "ner_model.write().overwrite().save(\"./pip_wo_embedd/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H6qtW2x5yHJQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "ner_dl_crf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}