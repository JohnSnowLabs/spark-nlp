<!DOCTYPE html >
<html>
        <head>
          <meta http-equiv="X-UA-Compatible" content="IE=edge" />
          <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
          <title>Spark NLP 6.0.4 ScalaDoc  - com.johnsnowlabs.nlp.annotators.cv</title>
          <meta name="description" content="Spark NLP 6.0.4 ScalaDoc - com.johnsnowlabs.nlp.annotators.cv" />
          <meta name="keywords" content="Spark NLP 6.0.4 ScalaDoc com.johnsnowlabs.nlp.annotators.cv" />
          <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
          
      
      <link href="../../../../../lib/index.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../../../../lib/template.css" media="screen" type="text/css" rel="stylesheet" />
      <link href="../../../../../lib/diagrams.css" media="screen" type="text/css" rel="stylesheet" id="diagrams-css" />
      <script type="text/javascript" src="../../../../../lib/jquery.min.js"></script>
      <script type="text/javascript" src="../../../../../lib/jquery.panzoom.min.js"></script>
      <script type="text/javascript" src="../../../../../lib/jquery.mousewheel.min.js"></script>
      <script type="text/javascript" src="../../../../../lib/index.js"></script>
      <script type="text/javascript" src="../../../../../index.js"></script>
      <script type="text/javascript" src="../../../../../lib/scheduler.js"></script>
      <script type="text/javascript" src="../../../../../lib/template.js"></script>
      
      <script type="text/javascript">
        /* this variable can be used by the JS to determine the path to the root document */
        var toRoot = '../../../../../';
      </script>
    
        </head>
        <body>
      <div id="search">
        <span id="doc-title">Spark NLP 6.0.4 ScalaDoc<span id="doc-version"></span></span>
        <span class="close-results"><span class="left">&lt;</span> Back</span>
        <div id="textfilter">
          <span class="input">
            <input autocapitalize="none" placeholder="Search" id="index-input" type="text" accesskey="/" />
            <i class="clear material-icons"></i>
            <i id="search-icon" class="material-icons"></i>
          </span>
        </div>
    </div>
      <div id="search-results">
        <div id="search-progress">
          <div id="progress-fill"></div>
        </div>
        <div id="results-content">
          <div id="entity-results"></div>
          <div id="member-results"></div>
        </div>
      </div>
      <div id="content-scroll-container" style="-webkit-overflow-scrolling: touch;">
        <div id="content-container" style="-webkit-overflow-scrolling: touch;">
          <div id="subpackage-spacer">
            <div id="packages">
              <h1>Packages</h1>
              <ul>
                <li name="_root_.root" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="_root_"></a><a id="root:_root_"></a>
      <span class="permalink">
      <a href="../../../../../index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../../../../../index.html"><span class="name">root</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../../../../index.html" class="extype" name="_root_">root</a></dd></dl></div>
    </li><li name="_root_.com" visbl="pub" class="indented1 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="com"></a><a id="com:com"></a>
      <span class="permalink">
      <a href="../../../../../com/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../../../../index.html"><span class="name">com</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../../../../index.html" class="extype" name="_root_">root</a></dd></dl></div>
    </li><li name="com.johnsnowlabs" visbl="pub" class="indented2 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="johnsnowlabs"></a><a id="johnsnowlabs:johnsnowlabs"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../../../index.html"><span class="name">johnsnowlabs</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../../../index.html" class="extype" name="com">com</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp" visbl="pub" class="indented3 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="nlp"></a><a id="nlp:nlp"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../../index.html"><span class="name">nlp</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../../index.html" class="extype" name="com.johnsnowlabs">johnsnowlabs</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators" visbl="pub" class="indented4 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="annotators"></a><a id="annotators:annotators"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../index.html"><span class="name">annotators</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.audio" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="audio"></a><a id="audio:audio"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/audio/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../audio/index.html"><span class="name">audio</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.btm" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="btm"></a><a id="btm:btm"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/btm/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../btm/index.html"><span class="name">btm</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.classifier" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="classifier"></a><a id="classifier:classifier"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/classifier/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../classifier/index.html"><span class="name">classifier</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cleaners" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="cleaners"></a><a id="cleaners:cleaners"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cleaners/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../cleaners/index.html"><span class="name">cleaners</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.common" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="common"></a><a id="common:common"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/common/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../common/index.html"><span class="name">common</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.coref" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="coref"></a><a id="coref:coref"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/coref/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../coref/index.html"><span class="name">coref</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv" visbl="pub" class="indented5 current" data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="cv"></a><a id="cv:cv"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <span class="name">cv</span>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.util" visbl="pub" class="indented6 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="util"></a><a id="util:util"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/util/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="util/index.html"><span class="name">util</span></a>
      </span>
      
      
    </li><li class="current-entities indented5">
                        <a class="object" href="BLIPForQuestionAnswering$.html" title=""></a>
                        <a class="class" href="BLIPForQuestionAnswering.html" title="BLIPForQuestionAnswering can load BLIP models for visual question answering."></a>
                        <a href="BLIPForQuestionAnswering.html" title="BLIPForQuestionAnswering can load BLIP models for visual question answering.">BLIPForQuestionAnswering</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="CLIPForZeroShotClassification$.html" title="This is the companion object of CLIPForZeroShotClassification."></a>
                        <a class="class" href="CLIPForZeroShotClassification.html" title="Zero Shot Image Classifier based on CLIP."></a>
                        <a href="CLIPForZeroShotClassification.html" title="Zero Shot Image Classifier based on CLIP.">CLIPForZeroShotClassification</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="ConvNextForImageClassification$.html" title="This is the companion object of ConvNextForImageClassification."></a>
                        <a class="class" href="ConvNextForImageClassification.html" title="ConvNextForImageClassification is an image classifier based on ConvNet models."></a>
                        <a href="ConvNextForImageClassification.html" title="ConvNextForImageClassification is an image classifier based on ConvNet models.">ConvNextForImageClassification</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="Florence2Transformer$.html" title=""></a>
                        <a class="class" href="Florence2Transformer.html" title="Florence2: Advancing a Unified Representation for a Variety of Vision Tasks"></a>
                        <a href="Florence2Transformer.html" title="Florence2: Advancing a Unified Representation for a Variety of Vision Tasks">Florence2Transformer</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="Gemma3ForMultiModal$.html" title=""></a>
                        <a class="class" href="Gemma3ForMultiModal.html" title="Gemma3ForMultiModal can load Gemma3 Vision models for visual question answering."></a>
                        <a href="Gemma3ForMultiModal.html" title="Gemma3ForMultiModal can load Gemma3 Vision models for visual question answering.">Gemma3ForMultiModal</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="HasRescaleFactor.html" title="Enables parameters to handle rescaling for image pre-processors."></a>
                        <a href="HasRescaleFactor.html" title="Enables parameters to handle rescaling for image pre-processors.">HasRescaleFactor</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="InternVLForMultiModal$.html" title=""></a>
                        <a class="class" href="InternVLForMultiModal.html" title="InternVLForMultiModal can load InternVL Vision models for visual question answering."></a>
                        <a href="InternVLForMultiModal.html" title="InternVLForMultiModal can load InternVL Vision models for visual question answering.">InternVLForMultiModal</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="JanusForMultiModal$.html" title=""></a>
                        <a class="class" href="JanusForMultiModal.html" title="JanusForMultiModal can load Janus models for unified multimodal understanding and generation."></a>
                        <a href="JanusForMultiModal.html" title="JanusForMultiModal can load Janus models for unified multimodal understanding and generation.">JanusForMultiModal</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="LLAVAForMultiModal$.html" title=""></a>
                        <a class="class" href="LLAVAForMultiModal.html" title="LLAVAForMultiModal can load LLAVA Vision models for visual question answering."></a>
                        <a href="LLAVAForMultiModal.html" title="LLAVAForMultiModal can load LLAVA Vision models for visual question answering.">LLAVAForMultiModal</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="MLLamaForMultimodal$.html" title=""></a>
                        <a class="class" href="MLLamaForMultimodal.html" title="MLLamaForMultimodal can load LLAMA 3.2 Vision models for visual question answering."></a>
                        <a href="MLLamaForMultimodal.html" title="MLLamaForMultimodal can load LLAMA 3.2 Vision models for visual question answering.">MLLamaForMultimodal</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="PaliGemmaForMultiModal$.html" title=""></a>
                        <a class="class" href="PaliGemmaForMultiModal.html" title="PaliGemmaForMultiModal can load PaliGemma Vision models for visual question answering."></a>
                        <a href="PaliGemmaForMultiModal.html" title="PaliGemmaForMultiModal can load PaliGemma Vision models for visual question answering.">PaliGemmaForMultiModal</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="Phi3Vision$.html" title=""></a>
                        <a class="class" href="Phi3Vision.html" title="Phi3Vision can load Phi3 Vision models for visual question answering."></a>
                        <a href="Phi3Vision.html" title="Phi3Vision can load Phi3 Vision models for visual question answering.">Phi3Vision</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="Qwen2VLTransformer$.html" title=""></a>
                        <a class="class" href="Qwen2VLTransformer.html" title="Qwen2VLTransformer can load Qwen2 Vision-Language models for visual question answering and multimodal instruction following."></a>
                        <a href="Qwen2VLTransformer.html" title="Qwen2VLTransformer can load Qwen2 Vision-Language models for visual question answering and multimodal instruction following.">Qwen2VLTransformer</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadBLIPForQuestionAnsweringDLModel.html" title=""></a>
                        <a href="ReadBLIPForQuestionAnsweringDLModel.html" title="">ReadBLIPForQuestionAnsweringDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadCLIPForZeroShotClassificationModel.html" title=""></a>
                        <a href="ReadCLIPForZeroShotClassificationModel.html" title="">ReadCLIPForZeroShotClassificationModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadConvNextForImageDLModel.html" title=""></a>
                        <a href="ReadConvNextForImageDLModel.html" title="">ReadConvNextForImageDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadFlorence2TransformerDLModel.html" title=""></a>
                        <a href="ReadFlorence2TransformerDLModel.html" title="">ReadFlorence2TransformerDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadGemma3ForMultiModalDLModel.html" title=""></a>
                        <a href="ReadGemma3ForMultiModalDLModel.html" title="">ReadGemma3ForMultiModalDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadInternVLForMultiModalDLModel.html" title=""></a>
                        <a href="ReadInternVLForMultiModalDLModel.html" title="">ReadInternVLForMultiModalDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadJanusForMultiModalDLModel.html" title=""></a>
                        <a href="ReadJanusForMultiModalDLModel.html" title="">ReadJanusForMultiModalDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadLLAVAForMultiModalDLModel.html" title=""></a>
                        <a href="ReadLLAVAForMultiModalDLModel.html" title="">ReadLLAVAForMultiModalDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadMLLamaForMultimodalDLModel.html" title=""></a>
                        <a href="ReadMLLamaForMultimodalDLModel.html" title="">ReadMLLamaForMultimodalDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadPaliGemmaForMultiModalDLModel.html" title=""></a>
                        <a href="ReadPaliGemmaForMultiModalDLModel.html" title="">ReadPaliGemmaForMultiModalDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadPhi3VisionDLModel.html" title=""></a>
                        <a href="ReadPhi3VisionDLModel.html" title="">ReadPhi3VisionDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadQwen2VLTransformerDLModel.html" title=""></a>
                        <a href="ReadQwen2VLTransformerDLModel.html" title="">ReadQwen2VLTransformerDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadSmolVLMTransformerDLModel.html" title=""></a>
                        <a href="ReadSmolVLMTransformerDLModel.html" title="">ReadSmolVLMTransformerDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadSwinForImageDLModel.html" title=""></a>
                        <a href="ReadSwinForImageDLModel.html" title="">ReadSwinForImageDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadViTForImageDLModel.html" title=""></a>
                        <a href="ReadViTForImageDLModel.html" title="">ReadViTForImageDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadVisionEncoderDecoderDLModel.html" title=""></a>
                        <a href="ReadVisionEncoderDecoderDLModel.html" title="">ReadVisionEncoderDecoderDLModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedBLIPForQuestionAnswering.html" title=""></a>
                        <a href="ReadablePretrainedBLIPForQuestionAnswering.html" title="">ReadablePretrainedBLIPForQuestionAnswering</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedCLIPForZeroShotClassificationModel.html" title=""></a>
                        <a href="ReadablePretrainedCLIPForZeroShotClassificationModel.html" title="">ReadablePretrainedCLIPForZeroShotClassificationModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedConvNextForImageModel.html" title=""></a>
                        <a href="ReadablePretrainedConvNextForImageModel.html" title="">ReadablePretrainedConvNextForImageModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedFlorence2TransformerModel.html" title=""></a>
                        <a href="ReadablePretrainedFlorence2TransformerModel.html" title="">ReadablePretrainedFlorence2TransformerModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedGemma3ForMultiModal.html" title=""></a>
                        <a href="ReadablePretrainedGemma3ForMultiModal.html" title="">ReadablePretrainedGemma3ForMultiModal</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedInternVLForMultiModal.html" title=""></a>
                        <a href="ReadablePretrainedInternVLForMultiModal.html" title="">ReadablePretrainedInternVLForMultiModal</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedJanusForMultiModal.html" title=""></a>
                        <a href="ReadablePretrainedJanusForMultiModal.html" title="">ReadablePretrainedJanusForMultiModal</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedLLAVAForMultiModal.html" title=""></a>
                        <a href="ReadablePretrainedLLAVAForMultiModal.html" title="">ReadablePretrainedLLAVAForMultiModal</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedMLLamaForMultimodal.html" title=""></a>
                        <a href="ReadablePretrainedMLLamaForMultimodal.html" title="">ReadablePretrainedMLLamaForMultimodal</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedPaliGemmaForMultiModal.html" title=""></a>
                        <a href="ReadablePretrainedPaliGemmaForMultiModal.html" title="">ReadablePretrainedPaliGemmaForMultiModal</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedPhi3Vision.html" title=""></a>
                        <a href="ReadablePretrainedPhi3Vision.html" title="">ReadablePretrainedPhi3Vision</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedQwen2VLTransformer.html" title=""></a>
                        <a href="ReadablePretrainedQwen2VLTransformer.html" title="">ReadablePretrainedQwen2VLTransformer</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedSmolVLMTransformer.html" title=""></a>
                        <a href="ReadablePretrainedSmolVLMTransformer.html" title="">ReadablePretrainedSmolVLMTransformer</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedSwinForImageModel.html" title=""></a>
                        <a href="ReadablePretrainedSwinForImageModel.html" title="">ReadablePretrainedSwinForImageModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedViTForImageModel.html" title=""></a>
                        <a href="ReadablePretrainedViTForImageModel.html" title="">ReadablePretrainedViTForImageModel</a>
                      </li><li class="current-entities indented5">
                        <span class="separator"></span>
                        <a class="trait" href="ReadablePretrainedVisionEncoderDecoderModel.html" title=""></a>
                        <a href="ReadablePretrainedVisionEncoderDecoderModel.html" title="">ReadablePretrainedVisionEncoderDecoderModel</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="SmolVLMTransformer$.html" title=""></a>
                        <a class="class" href="SmolVLMTransformer.html" title="SmolVLMTransformer can load SmolVLM models for visual question answering."></a>
                        <a href="SmolVLMTransformer.html" title="SmolVLMTransformer can load SmolVLM models for visual question answering.">SmolVLMTransformer</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="SwinForImageClassification$.html" title="This is the companion object of SwinForImageClassification."></a>
                        <a class="class" href="SwinForImageClassification.html" title="SwinImageClassification is an image classifier based on Swin."></a>
                        <a href="SwinForImageClassification.html" title="SwinImageClassification is an image classifier based on Swin.">SwinForImageClassification</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="ViTForImageClassification$.html" title="This is the companion object of ViTForImageClassification."></a>
                        <a class="class" href="ViTForImageClassification.html" title="Vision Transformer (ViT) for image classification."></a>
                        <a href="ViTForImageClassification.html" title="Vision Transformer (ViT) for image classification.">ViTForImageClassification</a>
                      </li><li class="current-entities indented5">
                        <a class="object" href="VisionEncoderDecoderForImageCaptioning$.html" title="This is the companion object of VisionEncoderDecoderForImageCaptioning."></a>
                        <a class="class" href="VisionEncoderDecoderForImageCaptioning.html" title="VisionEncoderDecoder model that converts images into text captions."></a>
                        <a href="VisionEncoderDecoderForImageCaptioning.html" title="VisionEncoderDecoder model that converts images into text captions.">VisionEncoderDecoderForImageCaptioning</a>
                      </li><li name="com.johnsnowlabs.nlp.annotators.er" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="er"></a><a id="er:er"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/er/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../er/index.html"><span class="name">er</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.keyword" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="keyword"></a><a id="keyword:keyword"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/keyword/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../keyword/index.html"><span class="name">keyword</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.ld" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ld"></a><a id="ld:ld"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/ld/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../ld/index.html"><span class="name">ld</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.ner" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ner"></a><a id="ner:ner"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/ner/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../ner/index.html"><span class="name">ner</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.param" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="param"></a><a id="param:param"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/param/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../param/index.html"><span class="name">param</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.parser" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="parser"></a><a id="parser:parser"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/parser/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../parser/index.html"><span class="name">parser</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.pos" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="pos"></a><a id="pos:pos"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/pos/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../pos/index.html"><span class="name">pos</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.sbd" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="sbd"></a><a id="sbd:sbd"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/sbd/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../sbd/index.html"><span class="name">sbd</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.sda" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="sda"></a><a id="sda:sda"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/sda/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../sda/index.html"><span class="name">sda</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.sentence_detector_dl" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="sentence_detector_dl"></a><a id="sentence_detector_dl:sentence_detector_dl"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/sentence_detector_dl/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../sentence_detector_dl/index.html"><span class="name">sentence_detector_dl</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.seq2seq" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="seq2seq"></a><a id="seq2seq:seq2seq"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/seq2seq/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../seq2seq/index.html"><span class="name">seq2seq</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.similarity" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="similarity"></a><a id="similarity:similarity"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/similarity/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../similarity/index.html"><span class="name">similarity</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.spell" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="spell"></a><a id="spell:spell"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/spell/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../spell/index.html"><span class="name">spell</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.tapas" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="tapas"></a><a id="tapas:tapas"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/tapas/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../tapas/index.html"><span class="name">tapas</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.tokenizer" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="tokenizer"></a><a id="tokenizer:tokenizer"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/tokenizer/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../tokenizer/index.html"><span class="name">tokenizer</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.ws" visbl="pub" class="indented5 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ws"></a><a id="ws:ws"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/ws/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <a title="" href="../ws/index.html"><span class="name">ws</span></a>
      </span>
      
      <div class="fullcomment"><dl class="attributes block"> <dt>Definition Classes</dt><dd><a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></dd></dl></div>
    </li>
              </ul>
            </div>
          </div>
          <div id="content">
            <body class="package value">
      <div id="definition">
        <div class="big-circle package">p</div>
        <p id="owner"><a href="../../../../index.html" class="extype" name="com">com</a>.<a href="../../../index.html" class="extype" name="com.johnsnowlabs">johnsnowlabs</a>.<a href="../../index.html" class="extype" name="com.johnsnowlabs.nlp">nlp</a>.<a href="../index.html" class="extype" name="com.johnsnowlabs.nlp.annotators">annotators</a></p>
        <h1>cv<span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/index.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span></h1>
        
      </div>

      <h4 id="signature" class="signature">
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">package</span>
      </span>
      <span class="symbol">
        <span class="name">cv</span>
      </span>
      </h4>

      
          <div id="comment" class="fullcommenttop"></div>
        

      <div id="mbrsel">
        <div class="toggle"></div>
        <div id="memberfilter">
          <i class="material-icons arrow"></i>
          <span class="input">
            <input id="mbrsel-input" placeholder="Filter all members" type="text" accesskey="/" />
          </span>
          <i class="clear material-icons"></i>
        </div>
        <div id="filterby">
          <div id="order">
            <span class="filtertype">Ordering</span>
            <ol>
              
              <li class="alpha in"><span>Alphabetic</span></li>
              
            </ol>
          </div>
          
          <div id="visbl">
              <span class="filtertype">Visibility</span>
              <ol><li class="public in"><span>Public</span></li><li class="all out"><span>All</span></li></ol>
            </div>
        </div>
      </div>

      <div id="template">
        <div id="allMembers">
        

        <div id="types" class="types members">
              <h3>Type Members</h3>
              <ol><li name="com.johnsnowlabs.nlp.annotators.cv.BLIPForQuestionAnswering" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="BLIPForQuestionAnsweringextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.BLIPForQuestionAnswering]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.BLIPForQuestionAnswering]withHasImageFeaturePropertieswithWriteTensorflowModelwithHasEngine"></a><a id="BLIPForQuestionAnswering:BLIPForQuestionAnswering"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/BLIPForQuestionAnswering.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="BLIPForQuestionAnswering can load BLIP models for visual question answering." href="BLIPForQuestionAnswering.html"><span class="name">BLIPForQuestionAnswering</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="BLIPForQuestionAnswering.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.BLIPForQuestionAnswering">BLIPForQuestionAnswering</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="BLIPForQuestionAnswering.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.BLIPForQuestionAnswering">BLIPForQuestionAnswering</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">BLIPForQuestionAnswering can load BLIP models for visual question answering.</p><div class="fullcomment"><div class="comment cmt"><p>BLIPForQuestionAnswering can load BLIP models for visual question answering. The model
consists of a vision encoder, a text encoder as well as a text decoder. The vision encoder
will encode the input image, the text encoder will encode the input question together with the
encoding of the image, and the text decoder will output the answer to the question.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> visualQAClassifier = BLIPForQuestionAnswering.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)</pre><p>The default model is <code>&quot;blip_vqa_base&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Question+Answering" target="_blank">Models Hub</a>.</p><p>Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. To
see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a> and to see more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/BLIPForQuestionAnsweringTest.scala" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/BLIPForQuestionAnsweringTest.scala</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF: DataFrame = ResourceHelper.spark.read
 .format(<span class="lit">"image"</span>)
 .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
 .load(imageFolder)

<span class="kw">val</span> testDF: DataFrame = imageDF.withColumn(<span class="lit">"text"</span>, lit(<span class="lit">"What's this picture about?"</span>))

<span class="kw">val</span> imageAssembler: ImageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> visualQAClassifier = BLIPForQuestionAnswering.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  imageAssembler,
  visualQAClassifier
))

<span class="kw">val</span> result = pipeline.fit(testDF).transform(testDF)

result.select(<span class="lit">"image_assembler.origin"</span>, <span class="lit">"answer.result"</span>).show(<span class="kw">false</span>)
+--------------------------------------+------+
|origin                                |result|
+--------------------------------------+------+
|[file:<span class="cmt">///content/images/cat_image.jpg]|[cats]|</span>
+--------------------------------------+------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="CLIPForZeroShotClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification">CLIPForZeroShotClassification</a> for Zero Shot Image Classifier</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based classifiers</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="CLIPForZeroShotClassificationextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification]withHasImageFeaturePropertieswithWriteTensorflowModelwithWriteOnnxModelwithWriteOpenvinoModelwithHasEnginewithHasRescaleFactor"></a><a id="CLIPForZeroShotClassification:CLIPForZeroShotClassification"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/CLIPForZeroShotClassification.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Zero Shot Image Classifier based on CLIP." href="CLIPForZeroShotClassification.html"><span class="name">CLIPForZeroShotClassification</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="CLIPForZeroShotClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification">CLIPForZeroShotClassification</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="CLIPForZeroShotClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification">CLIPForZeroShotClassification</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a> with <a href="HasRescaleFactor.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.HasRescaleFactor">HasRescaleFactor</a></span>
      </span>
      
      <p class="shortcomment cmt">Zero Shot Image Classifier based on CLIP.</p><div class="fullcomment"><div class="comment cmt"><p>Zero Shot Image Classifier based on CLIP.</p><p>CLIP (Contrastive Language-Image Pre-Training) is a neural network that was trained on image
and text pairs. It has the ability to predict images without training on any hard-coded
labels. This makes it very flexible, as labels can be provided during inference. This is
similar to the zero-shot capabilities of the GPT-2 and 3 models.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> imageClassifier = CLIPForZeroShotClassification.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"label"</span>)</pre><p>The default model is <code>&quot;zero_shot_classifier_clip_vit_base_patch32&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Zero-Shot+Classification" target="_blank">Models Hub</a>.</p><p>Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. To
see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a> and to see more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/CLIPForZeroShotClassificationTestSpec.scala" target="_blank">CLIPForZeroShotClassificationTestSpec</a>.</p><h4>Example</h4><pre><span class="kw">import</span> com.johnsnowlabs.nlp.ImageAssembler
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF = ResourceHelper.spark.read
  .format(<span class="lit">"image"</span>)
  .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
  .load(<span class="lit">"src/test/resources/image/"</span>)

<span class="kw">val</span> imageAssembler: ImageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> candidateLabels = <span class="std">Array</span>(
  <span class="lit">"a photo of a bird"</span>,
  <span class="lit">"a photo of a cat"</span>,
  <span class="lit">"a photo of a dog"</span>,
  <span class="lit">"a photo of a hen"</span>,
  <span class="lit">"a photo of a hippo"</span>,
  <span class="lit">"a photo of a room"</span>,
  <span class="lit">"a photo of a tractor"</span>,
  <span class="lit">"a photo of an ostrich"</span>,
  <span class="lit">"a photo of an ox"</span>)

<span class="kw">val</span> imageClassifier = CLIPForZeroShotClassification
  .pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"label"</span>)
  .setCandidateLabels(candidateLabels)

<span class="kw">val</span> pipeline =
  <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(imageAssembler, imageClassifier)).fit(imageDF).transform(imageDF)

pipeline
  .selectExpr(<span class="lit">"reverse(split(image.origin, '/'))[0] as image_name"</span>, <span class="lit">"label.result"</span>)
  .show(truncate = <span class="kw">false</span>)
+-----------------+-----------------------+
|image_name       |result                 |
+-----------------+-----------------------+
|palace.JPEG      |[a photo of a room]    |
|egyptian_cat.jpeg|[a photo of a cat]     |
|hippopotamus.JPEG|[a photo of a hippo]   |
|hen.JPEG         |[a photo of a hen]     |
|ostrich.JPEG     |[a photo of an ostrich]|
|junco.JPEG       |[a photo of a bird]    |
|bluetick.jpg     |[a photo of a dog]     |
|chihuahua.jpg    |[a photo of a dog]     |
|tractor.JPEG     |[a photo of a tractor] |
|ox.JPEG          |[a photo of an ox]     |
+-----------------+-----------------------+</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ConvNextForImageClassification" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ConvNextForImageClassificationextendsSwinForImageClassification"></a><a id="ConvNextForImageClassification:ConvNextForImageClassification"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ConvNextForImageClassification.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="ConvNextForImageClassification is an image classifier based on ConvNet models." href="ConvNextForImageClassification.html"><span class="name">ConvNextForImageClassification</span></a><span class="result"> extends <a href="SwinForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.SwinForImageClassification">SwinForImageClassification</a></span>
      </span>
      
      <p class="shortcomment cmt">ConvNextForImageClassification is an image classifier based on ConvNet models.</p><div class="fullcomment"><div class="comment cmt"><p>ConvNextForImageClassification is an image classifier based on ConvNet models.</p><p>The ConvNeXT model was proposed in A ConvNet for the 2020s by Zhuang Liu, Hanzi Mao, Chao-Yuan
Wu, Christoph Feichtenhofer, Trevor Darrell, Saining Xie. ConvNeXT is a pure convolutional
model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform
them.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> imageClassifier = ConvNextForImageClassification.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"class"</span>)</pre><p>The default model is <code>&quot;image_classifier_convnext_tiny_224_local&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Image+Classification" target="_blank">Models Hub</a>.</p><p>Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. To
see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a> and to see more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/ConvNextForImageClassificationTestSpec.scala" target="_blank">ConvNextForImageClassificationTestSpec</a>.</p><p><b>References:</b></p><p><a href="https://arxiv.org/abs/2201.03545" target="_blank">A ConvNet for the 2020s</a></p><p><b>Paper Abstract:</b></p><p><i>The &quot;Roaring 20s&quot; of visual recognition began with the introduction of Vision Transformers
(ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model.
A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision
tasks such as object detection and semantic segmentation. It is the hierarchical Transformers
(e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers
practically viable as a generic vision backbone and demonstrating remarkable performance on a
wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still
largely credited to the intrinsic superiority of Transformers, rather than the inherent
inductive biases of convolutions. In this work, we reexamine the design spaces and test the
limits of what a pure ConvNet can achieve. We gradually &quot;modernize&quot; a standard ResNet toward
the design of a vision Transformer, and discover several key components that contribute to the
performance difference along the way. The outcome of this exploration is a family of pure
ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts
compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8%
ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K
segmentation, while maintaining the simplicity and efficiency of standard ConvNets. </i></p><h4>Example</h4><pre><span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> com.johnsnowlabs.nlp.ImageAssembler
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF: DataFrame = spark.read
  .format(<span class="lit">"image"</span>)
  .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
  .load(<span class="lit">"src/test/resources/image/"</span>)

<span class="kw">val</span> imageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> imageClassifier = ConvNextForImageClassification
  .pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"class"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(imageAssembler, imageClassifier))
<span class="kw">val</span> pipelineDF = pipeline.fit(imageDF).transform(imageDF)

pipelineDF
  .selectExpr(<span class="lit">"reverse(split(image.origin, '/'))[0] as image_name"</span>, <span class="lit">"class.result"</span>)
  .show(truncate = <span class="kw">false</span>)
+-----------------+----------------------------------------------------------+
|image_name       |result                                                    |
+-----------------+----------------------------------------------------------+
|palace.JPEG      |[palace]                                                  |
|egyptian_cat.jpeg|[tabby, tabby cat]                                        |
|hippopotamus.JPEG|[hippopotamus, hippo, river horse, Hippopotamus amphibius]|
|hen.JPEG         |[hen]                                                     |
|ostrich.JPEG     |[ostrich, Struthio camelus]                               |
|junco.JPEG       |[junco, snowbird]                                         |
|bluetick.jpg     |[bluetick]                                                |
|chihuahua.jpg    |[Chihuahua]                                               |
|tractor.JPEG     |[tractor]                                                 |
|ox.JPEG          |[ox]                                                      |
+-----------------+----------------------------------------------------------+</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.Florence2Transformer" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Florence2TransformerextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.Florence2Transformer]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.Florence2Transformer]withHasImageFeaturePropertieswithWriteOpenvinoModelwithHasGeneratorPropertieswithHasEngine"></a><a id="Florence2Transformer:Florence2Transformer"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/Florence2Transformer.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Florence2: Advancing a Unified Representation for a Variety of Vision Tasks" href="Florence2Transformer.html"><span class="name">Florence2Transformer</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="Florence2Transformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Florence2Transformer">Florence2Transformer</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="Florence2Transformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Florence2Transformer">Florence2Transformer</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasGeneratorProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasGeneratorProperties">HasGeneratorProperties</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Florence2: Advancing a Unified Representation for a Variety of Vision Tasks</p><div class="fullcomment"><div class="comment cmt"><p>Florence2: Advancing a Unified Representation for a Variety of Vision Tasks</p><p>Florence-2 is an advanced vision foundation model from Microsoft that uses a prompt-based
approach to handle a wide range of vision and vision-language tasks. It can interpret simple
text prompts to perform tasks like captioning, object detection, segmentation, OCR, and more.
The model leverages the FLD-5B dataset, containing 5.4 billion annotations across 126 million
images, to master multi-task learning. Its sequence-to-sequence architecture enables it to
excel in both zero-shot and fine-tuned settings.</p><p>Pretrained and finetuned models can be loaded with <code>pretrained</code> of the companion object: {{ {
val florence2 = Florence2Transformer.pretrained() .setInputCols(&quot;image&quot;)
.setOutputCol(&quot;generation&quot;) }} } The default model is <code>&quot;florence2_base_ft_int4&quot;</code>, if no name
is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Vision+Tasks" target="_blank">Models Hub</a>.</p><h4>Supported Tasks</h4><p>Florence-2 supports a variety of tasks through prompt engineering. The following prompt tokens
can be used:</p><ul><li>&lt;CAPTION&gt;: Image captioning</li><li>&lt;DETAILED_CAPTION&gt;: Detailed image captioning</li><li>&lt;MORE_DETAILED_CAPTION&gt;: Paragraph-level captioning</li><li>&lt;CAPTION_TO_PHRASE_GROUNDING&gt;: Phrase grounding from caption (requires additional text
    input)</li><li>&lt;OD&gt;: Object detection</li><li>&lt;DENSE_REGION_CAPTION&gt;: Dense region captioning</li><li>&lt;REGION_PROPOSAL&gt;: Region proposal</li><li>&lt;OCR&gt;: Optical Character Recognition (plain text extraction)</li><li>&lt;OCR_WITH_REGION&gt;: OCR with region information</li><li>&lt;REFERRING_EXPRESSION_SEGMENTATION&gt;: Segmentation for a referred phrase (requires
    additional text input)</li><li>&lt;REGION_TO_SEGMENTATION&gt;: Polygon mask for a region (requires additional text input)</li><li>&lt;OPEN_VOCABULARY_DETECTION&gt;: Open vocabulary detection for a phrase (requires additional
    text input)</li><li>&lt;REGION_TO_CATEGORY&gt;: Category of a region (requires additional text input)</li><li>&lt;REGION_TO_DESCRIPTION&gt;: Description of a region (requires additional text input)</li><li>&lt;REGION_TO_OCR&gt;: OCR for a region (requires additional text input)</li></ul><h4>Example Usage</h4><p>{{ { import com.johnsnowlabs.nlp.base.ImageAssembler import
com.johnsnowlabs.nlp.annotators.cv.Florence2Transformer import org.apache.spark.ml.Pipeline</p><p>val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;)
.setOutputCol(&quot;image_assembler&quot;)</p><p>val florence2 = Florence2Transformer.pretrained(&quot;florence2_base_ft_int4&quot;)
.setInputCols(&quot;image_assembler&quot;) .setOutputCol(&quot;answer&quot;) .setMaxOutputLength(50)</p><p>val pipeline = new Pipeline().setStages(Array(imageAssembler, florence2))</p><p>val data = Seq(&quot;/path/to/image.jpg&quot;).toDF(&quot;image&quot;) val result =
pipeline.fit(data).transform(data) result.select(&quot;answer.result&quot;).show(truncate = false) }} }</p><h4>References</h4><ul><li>Florence-2 technical report: https://arxiv.org/abs/2311.06242</li><li>Hugging Face model card: https://huggingface.co/microsoft/Florence-2-base-ft</li><li>Official sample notebook:
    https://huggingface.co/microsoft/Florence-2-large/blob/main/sample_inference.ipynb</li></ul><p>For more details and advanced usage, see the official documentation and sample notebooks.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.Gemma3ForMultiModal" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Gemma3ForMultiModalextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.Gemma3ForMultiModal]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.Gemma3ForMultiModal]withHasImageFeaturePropertieswithWriteOpenvinoModelwithHasGeneratorPropertieswithHasEngine"></a><a id="Gemma3ForMultiModal:Gemma3ForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/Gemma3ForMultiModal.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Gemma3ForMultiModal can load Gemma3 Vision models for visual question answering." href="Gemma3ForMultiModal.html"><span class="name">Gemma3ForMultiModal</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="Gemma3ForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Gemma3ForMultiModal">Gemma3ForMultiModal</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="Gemma3ForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Gemma3ForMultiModal">Gemma3ForMultiModal</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasGeneratorProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasGeneratorProperties">HasGeneratorProperties</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Gemma3ForMultiModal can load Gemma3 Vision models for visual question answering.</p><div class="fullcomment"><div class="comment cmt"><p>Gemma3ForMultiModal can load Gemma3 Vision models for visual question answering. The model
consists of a vision encoder, a text encoder, a text decoder and a model merger. The vision
encoder will encode the input image, the text encoder will encode the input text, the model
merger will merge the image and text embeddings, and the text decoder will output the answer.</p><p>Gemma 3 is a family of lightweight, state-of-the-art open models from Google, built from the
same research and technology used to create the Gemini models. Key features include:</p><ul><li>Large 128K context window</li><li>Multilingual support in over 140 languages</li><li>Multimodal capabilities handling both text and image inputs</li><li>Optimized for deployment on limited resources (laptops, desktops, cloud)</li></ul><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> visualQA = Gemma3ForMultiModal.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)</pre><p>The default model is <code>&quot;gemma3_4b_it_int4&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Question+Answering" target="_blank">Models Hub</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF = spark.read
  .format(<span class="lit">"image"</span>)
  .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
  .load(imageFolder)

<span class="kw">val</span> testDF = imageDF.withColumn(<span class="lit">"text"</span>, lit(<span class="lit">"&lt;bos&gt;&lt;start_of_turn&gt;user\nYou are a helpful assistant.\n\n&lt;start_of_image&gt;Describe this image in detail.&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\n"</span>))

<span class="kw">val</span> imageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> visualQA = Gemma3ForMultiModal.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  imageAssembler,
  visualQA
))

<span class="kw">val</span> result = pipeline.fit(testDF).transform(testDF)

result.select(<span class="lit">"image_assembler.origin"</span>, <span class="lit">"answer.result"</span>).show(truncate = <span class="kw">false</span>)</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.HasRescaleFactor" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="HasRescaleFactorextendsAnyRef"></a><a id="HasRescaleFactor:HasRescaleFactor"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/HasRescaleFactor.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="Enables parameters to handle rescaling for image pre-processors." href="HasRescaleFactor.html"><span class="name">HasRescaleFactor</span></a><span class="result"> extends <a href="../../../../../scala/index.html#AnyRef=Object" class="extmbr" name="scala.AnyRef">AnyRef</a></span>
      </span>
      
      <p class="shortcomment cmt">Enables parameters to handle rescaling for image pre-processors.</p>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.InternVLForMultiModal" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="InternVLForMultiModalextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.InternVLForMultiModal]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.InternVLForMultiModal]withHasImageFeaturePropertieswithWriteOpenvinoModelwithHasGeneratorPropertieswithHasEngine"></a><a id="InternVLForMultiModal:InternVLForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/InternVLForMultiModal.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="InternVLForMultiModal can load InternVL Vision models for visual question answering." href="InternVLForMultiModal.html"><span class="name">InternVLForMultiModal</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="InternVLForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.InternVLForMultiModal">InternVLForMultiModal</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="InternVLForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.InternVLForMultiModal">InternVLForMultiModal</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasGeneratorProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasGeneratorProperties">HasGeneratorProperties</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">InternVLForMultiModal can load InternVL Vision models for visual question answering.</p><div class="fullcomment"><div class="comment cmt"><p>InternVLForMultiModal can load InternVL Vision models for visual question answering. The model
consists of a vision encoder, a text encoder, a text decoder and a model merger. The vision
encoder will encode the input image, the text encoder will encode the input text, the model
merger will merge the image and text embeddings, and the text decoder will output the answer.</p><p>InternVL 2.5 is an advanced multimodal large language model (MLLM) series that builds upon
InternVL 2.0, maintaining its core model architecture while introducing significant
enhancements in training and testing strategies as well as data quality. Key features include:</p><ul><li>Large context window support</li><li>Multilingual support</li><li>Multimodal capabilities handling both text and image inputs</li><li>Optimized for deployment with int4 quantization</li></ul><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> visualQA = InternVLForMultiModal.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)</pre><p>The default model is <code>&quot;internvl2_5_1b_int4&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Question+Answering" target="_blank">Models Hub</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF = spark.read
  .format(<span class="lit">"image"</span>)
  .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
  .load(imageFolder)

<span class="kw">val</span> testDF = imageDF.withColumn(<span class="lit">"text"</span>, lit(<span class="lit">"&lt;|im_start|&gt;&lt;image&gt;\nDescribe this image in detail.&lt;|im_end|&gt;&lt;|im_start|&gt;assistant\n"</span>))

<span class="kw">val</span> imageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> visualQA = InternVLForMultiModal.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  imageAssembler,
  visualQA
))

<span class="kw">val</span> result = pipeline.fit(testDF).transform(testDF)

result.select(<span class="lit">"image_assembler.origin"</span>, <span class="lit">"answer.result"</span>).show(truncate = <span class="kw">false</span>)</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.JanusForMultiModal" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="JanusForMultiModalextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.JanusForMultiModal]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.JanusForMultiModal]withHasImageFeaturePropertieswithWriteOpenvinoModelwithHasGeneratorPropertieswithHasEngine"></a><a id="JanusForMultiModal:JanusForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/JanusForMultiModal.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="JanusForMultiModal can load Janus models for unified multimodal understanding and generation." href="JanusForMultiModal.html"><span class="name">JanusForMultiModal</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="JanusForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.JanusForMultiModal">JanusForMultiModal</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="JanusForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.JanusForMultiModal">JanusForMultiModal</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasGeneratorProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasGeneratorProperties">HasGeneratorProperties</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">JanusForMultiModal can load Janus models for unified multimodal understanding and generation.</p><div class="fullcomment"><div class="comment cmt"><p>JanusForMultiModal can load Janus models for unified multimodal understanding and generation.
The model consists of a vision encoder, a text encoder, and a text decoder. Janus decouples
visual encoding for enhanced flexibility, leveraging a unified transformer architecture for
both understanding and generation tasks.</p><p>Janus uses SigLIP-L as the vision encoder, supporting 384 x 384 image inputs. For image
generation, it utilizes a tokenizer with a downsample rate of 16. The framework is based on
DeepSeek-LLM-1.3b-base, trained on approximately 500B text tokens.</p><p>Pretrained models can be loaded with <code>pretrained</code> from the companion object: {{ val visualQA =
JanusForMultiModal.pretrained() .setInputCols(&quot;image_assembler&quot;) .setOutputCol(&quot;answer&quot;) }}
The default model is &quot;janus_1_3b_int4&quot; if no name is provided.</p><p>For available pretrained models, please refer to the
<a href="https://sparknlp.org/models?task=Question+Answering" target="_blank">Models Hub</a>.</p><p>Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. For
compatibility details and import instructions, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>. For extended examples, refer
to
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/JanusForMultiModalTest.scala" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/JanusForMultiModalTest.scala</a>.</p><h4>Example</h4><p>{{ import spark.implicits._</p><p>import com.johnsnowlabs.nlp.base._</p><p>import com.johnsnowlabs.nlp.annotator._</p><p>import org.apache.spark.ml.Pipeline</p><p>val imageDF: DataFrame = ResourceHelper.spark.read .format(&quot;image&quot;) .option(&quot;dropInvalid&quot;,
value = true) .load(imageFolder)</p><p>val testDF: DataFrame = imageDF.withColumn(&quot;text&quot;, lit(&quot;User: &lt;image_placeholder&gt;Describe
image in details Assistant:&quot;))</p><p>val imageAssembler: ImageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;)
.setOutputCol(&quot;image_assembler&quot;)</p><p>val visualQAClassifier = JanusForMultiModal.pretrained() .setInputCols(&quot;image_assembler&quot;)
.setOutputCol(&quot;answer&quot;)</p><p>val pipeline = new Pipeline().setStages(Array( imageAssembler, visualQAClassifier ))</p><p>val result = pipeline.fit(testDF).transform(testDF)</p><p>result.select(&quot;image_assembler.origin&quot;, &quot;answer.result&quot;).show(false)</p><table class="doctbl">
      <thead>
        <tr><th class="doctbl-left"><p> origin                                 </p></th><th class="doctbl-left"><p> result                                                                                  </p></th></tr>
      </thead>
      <tbody><tr><td class="doctbl-left"><p> [file:///content/images/cat_image.jpg] </p></td><td class="doctbl-left"><p> [The unusual aspect of this picture is the presence of two cats lying on a pink couch.] </p></td></tr>
          </tbody>
    </table><p>}}
</p></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="CLIPForZeroShotClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification">CLIPForZeroShotClassification</a> for Zero Shot Image Classification</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of
  transformer-based classifiers</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.LLAVAForMultiModal" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="LLAVAForMultiModalextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.LLAVAForMultiModal]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.LLAVAForMultiModal]withHasImageFeaturePropertieswithWriteOpenvinoModelwithHasGeneratorPropertieswithHasEngine"></a><a id="LLAVAForMultiModal:LLAVAForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/LLAVAForMultiModal.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="LLAVAForMultiModal can load LLAVA Vision models for visual question answering." href="LLAVAForMultiModal.html"><span class="name">LLAVAForMultiModal</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="LLAVAForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.LLAVAForMultiModal">LLAVAForMultiModal</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="LLAVAForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.LLAVAForMultiModal">LLAVAForMultiModal</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasGeneratorProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasGeneratorProperties">HasGeneratorProperties</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">LLAVAForMultiModal can load LLAVA Vision models for visual question answering.</p><div class="fullcomment"><div class="comment cmt"><p>LLAVAForMultiModal can load LLAVA Vision models for visual question answering. The model
consists of a vision encoder, a text encoder as well as a text decoder. The vision encoder
will encode the input image, the text encoder will encode the input question together with the
encoding of the image, and the text decoder will output the answer to the question.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> visualQA = LLAVAForMultiModal.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)</pre><p>The default model is <code>&quot;llava_1_5_7b_hf&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Question+Answering" target="_blank">Models Hub</a>.</p><p>Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. To
see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a> and to see more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/LLAVAForMultiModalTest.scala" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/LLAVAForMultiModalTest.scala</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF: DataFrame = ResourceHelper.spark.read
 .format(<span class="lit">"image"</span>)
 .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
 .load(imageFolder)

<span class="kw">val</span> testDF: DataFrame = imageDF.withColumn(<span class="lit">"text"</span>, lit(<span class="lit">"USER: \n &lt;|image|&gt; \nWhat is unusual on this picture? \n ASSISTANT:\n"</span>))

<span class="kw">val</span> imageAssembler: ImageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> visualQAClassifier = LLAVAForMultiModal.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  imageAssembler,
  visualQAClassifier
))

<span class="kw">val</span> result = pipeline.fit(testDF).transform(testDF)

result.select(<span class="lit">"image_assembler.origin"</span>, <span class="lit">"answer.result"</span>).show(<span class="kw">false</span>)
+--------------------------------------+------+
|origin                                |result|
+--------------------------------------+------+
|[file:<span class="cmt">///content/images/cat_image.jpg]|[The unusual aspect of this picture is the presence of two cats lying on a pink couch]|</span>
+--------------------------------------+------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="CLIPForZeroShotClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification">CLIPForZeroShotClassification</a> for Zero Shot Image Classifier</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based classifiers</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.MLLamaForMultimodal" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="MLLamaForMultimodalextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.MLLamaForMultimodal]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.MLLamaForMultimodal]withHasImageFeaturePropertieswithWriteOpenvinoModelwithHasGeneratorPropertieswithHasEngine"></a><a id="MLLamaForMultimodal:MLLamaForMultimodal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/MLLamaForMultimodal.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="MLLamaForMultimodal can load LLAMA 3.2 Vision models for visual question answering." href="MLLamaForMultimodal.html"><span class="name">MLLamaForMultimodal</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="MLLamaForMultimodal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.MLLamaForMultimodal">MLLamaForMultimodal</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="MLLamaForMultimodal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.MLLamaForMultimodal">MLLamaForMultimodal</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasGeneratorProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasGeneratorProperties">HasGeneratorProperties</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">MLLamaForMultimodal can load LLAMA 3.2 Vision models for visual question answering.</p><div class="fullcomment"><div class="comment cmt"><p>MLLamaForMultimodal can load LLAMA 3.2 Vision models for visual question answering. The model
consists of a vision encoder, a text encoder as well as a text decoder. The vision encoder
will encode the input image, the text encoder will encode the input question together with the
encoding of the image, and the text decoder will output the answer to the question.</p><p>The Llama 3.2-Vision collection of multimodal large language models (LLMs) is a collection of
pretrained and instruction-tuned image reasoning generative models in 11B and 90B sizes (text
+ images in / text out). The Llama 3.2-Vision instruction-tuned models are optimized for
visual recognition, image reasoning, captioning, and answering general questions about an
image. The models outperform many of the available open source and closed multimodal models on
common industry benchmarks.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> visualQA = MLLamaForMultimodal.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)</pre><p>The default model is <code>&quot;llama_3_2_11b_vision_instruct_int4&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Question+Answering" target="_blank">Models Hub</a>.</p><p>Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. To
see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a> and to see more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/MLLamaForMultimodalTest.scala" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/MLLamaForMultimodalTest.scala</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF: DataFrame = ResourceHelper.spark.read
 .format(<span class="lit">"image"</span>)
 .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
 .load(imageFolder)

<span class="kw">val</span> testDF: DataFrame = imageDF.withColumn(<span class="lit">"text"</span>, lit(<span class="lit">"&lt;|begin_of_text|&gt;&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n&lt;|image|&gt;What is unusual on this image?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n"</span>))

<span class="kw">val</span> imageAssembler: ImageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> visualQAClassifier = MLLamaForMultimodal.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  imageAssembler,
  visualQAClassifier
))

<span class="kw">val</span> result = pipeline.fit(testDF).transform(testDF)

result.select(<span class="lit">"image_assembler.origin"</span>, <span class="lit">"answer.result"</span>).show(<span class="kw">false</span>)
+--------------------------------------+------+
|origin                                |result|
+--------------------------------------+------+
|[file:<span class="cmt">///content/images/cat_image.jpg]|[The unusual aspect of this picture is the presence of two cats lying on a pink couch]|</span>
+--------------------------------------+------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="CLIPForZeroShotClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification">CLIPForZeroShotClassification</a> for Zero Shot Image Classifier</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based classifiers</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.PaliGemmaForMultiModal" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="PaliGemmaForMultiModalextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.PaliGemmaForMultiModal]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.PaliGemmaForMultiModal]withHasImageFeaturePropertieswithWriteOpenvinoModelwithHasGeneratorPropertieswithHasEngine"></a><a id="PaliGemmaForMultiModal:PaliGemmaForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/PaliGemmaForMultiModal.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="PaliGemmaForMultiModal can load PaliGemma Vision models for visual question answering." href="PaliGemmaForMultiModal.html"><span class="name">PaliGemmaForMultiModal</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="PaliGemmaForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.PaliGemmaForMultiModal">PaliGemmaForMultiModal</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="PaliGemmaForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.PaliGemmaForMultiModal">PaliGemmaForMultiModal</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasGeneratorProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasGeneratorProperties">HasGeneratorProperties</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">PaliGemmaForMultiModal can load PaliGemma Vision models for visual question answering.</p><div class="fullcomment"><div class="comment cmt"><p>PaliGemmaForMultiModal can load PaliGemma Vision models for visual question answering. The
model consists of a vision encoder, a text encoder, a text decoder and a model merger. The
vision encoder will encode the input image, the text encoder will encode the input text, the
model merger will merge the image and text embeddings, and the text decoder will output the
answer.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> visualQA = PaliGemmaForMultiModal.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)</pre><p>The default model is <code>&quot;paligemma_3b_pt_224_int4&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Question+Answering" target="_blank">Models Hub</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF: DataFrame = ResourceHelper.spark.read
 .format(<span class="lit">"image"</span>)
 .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
 .load(imageFolder)

<span class="kw">val</span> testDF: DataFrame = imageDF.withColumn(<span class="lit">"text"</span>, lit(<span class="lit">"USER: \n &lt;|image|&gt; \nWhat is unusual on this picture? \n ASSISTANT:\n"</span>))

<span class="kw">val</span> imageAssembler: ImageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> visualQAClassifier = PaliGemmaForMultiModal.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  imageAssembler,
  visualQAClassifier
))

<span class="kw">val</span> result = pipeline.fit(testDF).transform(testDF)

result.select(<span class="lit">"image_assembler.origin"</span>, <span class="lit">"answer.result"</span>).show(<span class="kw">false</span>)</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.Phi3Vision" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Phi3VisionextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.Phi3Vision]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.Phi3Vision]withHasImageFeaturePropertieswithWriteOpenvinoModelwithHasGeneratorPropertieswithHasEngine"></a><a id="Phi3Vision:Phi3Vision"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/Phi3Vision.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Phi3Vision can load Phi3 Vision models for visual question answering." href="Phi3Vision.html"><span class="name">Phi3Vision</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="Phi3Vision.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Phi3Vision">Phi3Vision</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="Phi3Vision.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Phi3Vision">Phi3Vision</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasGeneratorProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasGeneratorProperties">HasGeneratorProperties</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Phi3Vision can load Phi3 Vision models for visual question answering.</p><div class="fullcomment"><div class="comment cmt"><p>Phi3Vision can load Phi3 Vision models for visual question answering. The model consists of a
vision encoder, a text encoder as well as a text decoder. The vision encoder will encode the
input image, the text encoder will encode the input question together with the encoding of the
image, and the text decoder will output the answer to the question.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> visualQA = Phi3Vision.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)</pre><p>The default model is <code>&quot;phi_3_vision_128k_instruct&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Question+Answering" target="_blank">Models Hub</a>.</p><p>Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. To
see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a> and to see more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/Phi3VisionTest.scala" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/Phi3VisionTest.scala</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF: DataFrame = ResourceHelper.spark.read
 .format(<span class="lit">"image"</span>)
 .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
 .load(imageFolder)

<span class="kw">val</span> testDF: DataFrame = imageDF.withColumn(<span class="lit">"text"</span>, lit(<span class="lit">"&lt;|user|&gt; \n &lt;|image_1|&gt; \nWhat is unusual on this picture? &lt;|end|&gt;\n &lt;|assistant|&gt;\n"</span>))

<span class="kw">val</span> imageAssembler: ImageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> visualQAClassifier = Phi3Vision.pretrained(<span class="lit">"phi_3_vision_128k_instruct"</span>,<span class="lit">"en"</span>)
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  imageAssembler,
  visualQAClassifier
))

<span class="kw">val</span> result = pipeline.fit(testDF).transform(testDF)

result.select(<span class="lit">"image_assembler.origin"</span>, <span class="lit">"answer.result"</span>).show(<span class="kw">false</span>)
+--------------------------------------+------+
|origin                                |result|
+--------------------------------------+------+
|[file:<span class="cmt">///content/images/cat_image.jpg]|[The unusual aspect of this picture is the presence of two cats lying on a pink couch]|</span>
+--------------------------------------+------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="CLIPForZeroShotClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification">CLIPForZeroShotClassification</a> for Zero Shot Image Classifier</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based classifiers</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.Qwen2VLTransformer" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="Qwen2VLTransformerextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.Qwen2VLTransformer]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.Qwen2VLTransformer]withHasImageFeaturePropertieswithWriteOpenvinoModelwithHasGeneratorPropertieswithHasEngine"></a><a id="Qwen2VLTransformer:Qwen2VLTransformer"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/Qwen2VLTransformer.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Qwen2VLTransformer can load Qwen2 Vision-Language models for visual question answering and multimodal instruction following." href="Qwen2VLTransformer.html"><span class="name">Qwen2VLTransformer</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="Qwen2VLTransformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Qwen2VLTransformer">Qwen2VLTransformer</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="Qwen2VLTransformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Qwen2VLTransformer">Qwen2VLTransformer</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasGeneratorProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasGeneratorProperties">HasGeneratorProperties</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Qwen2VLTransformer can load Qwen2 Vision-Language models for visual question answering and
multimodal instruction following.</p><div class="fullcomment"><div class="comment cmt"><p>Qwen2VLTransformer can load Qwen2 Vision-Language models for visual question answering and
multimodal instruction following. The model consists of a vision encoder, a text encoder, and
a text decoder. The vision encoder processes the input image, the text encoder integrates the
encoding of the image with the input text, and the text decoder outputs the response to the
query or instruction.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> visualQA = Qwen2VLTransformer.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)</pre><p>The default model is <code>&quot;qwen2_vl_2b_instruct_int4&quot;</code>, if no name is provided.</p><p>For available pretrained models, please see the
<a href="https://sparknlp.org/models?task=Question+Answering" target="_blank">Models Hub</a>.</p><p>Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. To
see which models are compatible and how to import them, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a>. To explore more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/Qwen2VLTransformerTest.scala" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/Qwen2VLTransformerTest.scala</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF: DataFrame = ResourceHelper.spark.read
  .format(<span class="lit">"image"</span>)
  .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
  .load(imageFolder)

<span class="kw">val</span> testDF: DataFrame = imageDF.withColumn(<span class="lit">"text"</span>, lit(<span class="lit">"&lt;|im_start|&gt;system\nYou are a helpful assistant.&lt;|im_end|&gt;\n&lt;|im_start|&gt;user\n&lt;|vision_start|&gt;&lt;|image_pad|&gt;&lt;|vision_end|&gt;Describe this image.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n"</span>))

<span class="kw">val</span> imageAssembler: ImageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> visualQAClassifier = Qwen2VLTransformer.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  imageAssembler,
  visualQAClassifier
))

<span class="kw">val</span> result = pipeline.fit(testDF).transform(testDF)

result.select(<span class="lit">"image_assembler.origin"</span>, <span class="lit">"answer.result"</span>).show(<span class="kw">false</span>)
+--------------------------------------+------+
|origin                                |result|
+--------------------------------------+------+
|[file:<span class="cmt">///content/images/cat_image.jpg]|[This image is unusual because it features two cats lying on a pink couch.]|</span>
+--------------------------------------+------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer-
  based classifiers</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadBLIPForQuestionAnsweringDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadBLIPForQuestionAnsweringDLModelextendsReadTensorflowModel"></a><a id="ReadBLIPForQuestionAnsweringDLModel:ReadBLIPForQuestionAnsweringDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadBLIPForQuestionAnsweringDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadBLIPForQuestionAnsweringDLModel.html"><span class="name">ReadBLIPForQuestionAnsweringDLModel</span></a><span class="result"> extends <a href="../../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadCLIPForZeroShotClassificationModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadCLIPForZeroShotClassificationModelextendsReadTensorflowModelwithReadOnnxModelwithReadOpenvinoModel"></a><a id="ReadCLIPForZeroShotClassificationModel:ReadCLIPForZeroShotClassificationModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadCLIPForZeroShotClassificationModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadCLIPForZeroShotClassificationModel.html"><span class="name">ReadCLIPForZeroShotClassificationModel</span></a><span class="result"> extends <a href="../../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a> with <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadConvNextForImageDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadConvNextForImageDLModelextendsReadTensorflowModelwithReadOnnxModelwithReadOpenvinoModel"></a><a id="ReadConvNextForImageDLModel:ReadConvNextForImageDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadConvNextForImageDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadConvNextForImageDLModel.html"><span class="name">ReadConvNextForImageDLModel</span></a><span class="result"> extends <a href="../../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a> with <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadFlorence2TransformerDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadFlorence2TransformerDLModelextendsReadOpenvinoModel"></a><a id="ReadFlorence2TransformerDLModel:ReadFlorence2TransformerDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadFlorence2TransformerDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadFlorence2TransformerDLModel.html"><span class="name">ReadFlorence2TransformerDLModel</span></a><span class="result"> extends <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadGemma3ForMultiModalDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadGemma3ForMultiModalDLModelextendsReadOpenvinoModel"></a><a id="ReadGemma3ForMultiModalDLModel:ReadGemma3ForMultiModalDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadGemma3ForMultiModalDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadGemma3ForMultiModalDLModel.html"><span class="name">ReadGemma3ForMultiModalDLModel</span></a><span class="result"> extends <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadInternVLForMultiModalDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadInternVLForMultiModalDLModelextendsReadOpenvinoModel"></a><a id="ReadInternVLForMultiModalDLModel:ReadInternVLForMultiModalDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadInternVLForMultiModalDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadInternVLForMultiModalDLModel.html"><span class="name">ReadInternVLForMultiModalDLModel</span></a><span class="result"> extends <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadJanusForMultiModalDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadJanusForMultiModalDLModelextendsReadOpenvinoModel"></a><a id="ReadJanusForMultiModalDLModel:ReadJanusForMultiModalDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadJanusForMultiModalDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadJanusForMultiModalDLModel.html"><span class="name">ReadJanusForMultiModalDLModel</span></a><span class="result"> extends <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadLLAVAForMultiModalDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadLLAVAForMultiModalDLModelextendsReadOpenvinoModel"></a><a id="ReadLLAVAForMultiModalDLModel:ReadLLAVAForMultiModalDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadLLAVAForMultiModalDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadLLAVAForMultiModalDLModel.html"><span class="name">ReadLLAVAForMultiModalDLModel</span></a><span class="result"> extends <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadMLLamaForMultimodalDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadMLLamaForMultimodalDLModelextendsReadOpenvinoModel"></a><a id="ReadMLLamaForMultimodalDLModel:ReadMLLamaForMultimodalDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadMLLamaForMultimodalDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadMLLamaForMultimodalDLModel.html"><span class="name">ReadMLLamaForMultimodalDLModel</span></a><span class="result"> extends <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadPaliGemmaForMultiModalDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadPaliGemmaForMultiModalDLModelextendsReadOpenvinoModel"></a><a id="ReadPaliGemmaForMultiModalDLModel:ReadPaliGemmaForMultiModalDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadPaliGemmaForMultiModalDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadPaliGemmaForMultiModalDLModel.html"><span class="name">ReadPaliGemmaForMultiModalDLModel</span></a><span class="result"> extends <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadPhi3VisionDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadPhi3VisionDLModelextendsReadOpenvinoModel"></a><a id="ReadPhi3VisionDLModel:ReadPhi3VisionDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadPhi3VisionDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadPhi3VisionDLModel.html"><span class="name">ReadPhi3VisionDLModel</span></a><span class="result"> extends <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadQwen2VLTransformerDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadQwen2VLTransformerDLModelextendsReadOpenvinoModel"></a><a id="ReadQwen2VLTransformerDLModel:ReadQwen2VLTransformerDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadQwen2VLTransformerDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadQwen2VLTransformerDLModel.html"><span class="name">ReadQwen2VLTransformerDLModel</span></a><span class="result"> extends <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadSmolVLMTransformerDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadSmolVLMTransformerDLModelextendsReadOpenvinoModel"></a><a id="ReadSmolVLMTransformerDLModel:ReadSmolVLMTransformerDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadSmolVLMTransformerDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadSmolVLMTransformerDLModel.html"><span class="name">ReadSmolVLMTransformerDLModel</span></a><span class="result"> extends <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadSwinForImageDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadSwinForImageDLModelextendsReadTensorflowModelwithReadOnnxModelwithReadOpenvinoModel"></a><a id="ReadSwinForImageDLModel:ReadSwinForImageDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadSwinForImageDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadSwinForImageDLModel.html"><span class="name">ReadSwinForImageDLModel</span></a><span class="result"> extends <a href="../../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a> with <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadViTForImageDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadViTForImageDLModelextendsReadTensorflowModelwithReadOnnxModelwithReadOpenvinoModel"></a><a id="ReadViTForImageDLModel:ReadViTForImageDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadViTForImageDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadViTForImageDLModel.html"><span class="name">ReadViTForImageDLModel</span></a><span class="result"> extends <a href="../../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a> with <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadVisionEncoderDecoderDLModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadVisionEncoderDecoderDLModelextendsReadTensorflowModelwithReadOnnxModelwithReadOpenvinoModel"></a><a id="ReadVisionEncoderDecoderDLModel:ReadVisionEncoderDecoderDLModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadVisionEncoderDecoderDLModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadVisionEncoderDecoderDLModel.html"><span class="name">ReadVisionEncoderDecoderDLModel</span></a><span class="result"> extends <a href="../../../ml/tensorflow/ReadTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.ReadTensorflowModel">ReadTensorflowModel</a> with <a href="../../../ml/onnx/ReadOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.ReadOnnxModel">ReadOnnxModel</a> with <a href="../../../ml/openvino/ReadOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.ReadOpenvinoModel">ReadOpenvinoModel</a></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedBLIPForQuestionAnswering" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedBLIPForQuestionAnsweringextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.BLIPForQuestionAnswering]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.BLIPForQuestionAnswering]"></a><a id="ReadablePretrainedBLIPForQuestionAnswering:ReadablePretrainedBLIPForQuestionAnswering"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedBLIPForQuestionAnswering.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedBLIPForQuestionAnswering.html"><span class="name">ReadablePretrainedBLIPForQuestionAnswering</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="BLIPForQuestionAnswering.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.BLIPForQuestionAnswering">BLIPForQuestionAnswering</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="BLIPForQuestionAnswering.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.BLIPForQuestionAnswering">BLIPForQuestionAnswering</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedCLIPForZeroShotClassificationModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedCLIPForZeroShotClassificationModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification]"></a><a id="ReadablePretrainedCLIPForZeroShotClassificationModel:ReadablePretrainedCLIPForZeroShotClassificationModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedCLIPForZeroShotClassificationModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedCLIPForZeroShotClassificationModel.html"><span class="name">ReadablePretrainedCLIPForZeroShotClassificationModel</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="CLIPForZeroShotClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification">CLIPForZeroShotClassification</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="CLIPForZeroShotClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification">CLIPForZeroShotClassification</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedConvNextForImageModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedConvNextForImageModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.ConvNextForImageClassification]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.ConvNextForImageClassification]"></a><a id="ReadablePretrainedConvNextForImageModel:ReadablePretrainedConvNextForImageModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedConvNextForImageModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedConvNextForImageModel.html"><span class="name">ReadablePretrainedConvNextForImageModel</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="ConvNextForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ConvNextForImageClassification">ConvNextForImageClassification</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="ConvNextForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ConvNextForImageClassification">ConvNextForImageClassification</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedFlorence2TransformerModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedFlorence2TransformerModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.Florence2Transformer]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.Florence2Transformer]"></a><a id="ReadablePretrainedFlorence2TransformerModel:ReadablePretrainedFlorence2TransformerModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedFlorence2TransformerModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedFlorence2TransformerModel.html"><span class="name">ReadablePretrainedFlorence2TransformerModel</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="Florence2Transformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Florence2Transformer">Florence2Transformer</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="Florence2Transformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Florence2Transformer">Florence2Transformer</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedGemma3ForMultiModal" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedGemma3ForMultiModalextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.Gemma3ForMultiModal]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.Gemma3ForMultiModal]"></a><a id="ReadablePretrainedGemma3ForMultiModal:ReadablePretrainedGemma3ForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedGemma3ForMultiModal.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedGemma3ForMultiModal.html"><span class="name">ReadablePretrainedGemma3ForMultiModal</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="Gemma3ForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Gemma3ForMultiModal">Gemma3ForMultiModal</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="Gemma3ForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Gemma3ForMultiModal">Gemma3ForMultiModal</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedInternVLForMultiModal" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedInternVLForMultiModalextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.InternVLForMultiModal]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.InternVLForMultiModal]"></a><a id="ReadablePretrainedInternVLForMultiModal:ReadablePretrainedInternVLForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedInternVLForMultiModal.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedInternVLForMultiModal.html"><span class="name">ReadablePretrainedInternVLForMultiModal</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="InternVLForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.InternVLForMultiModal">InternVLForMultiModal</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="InternVLForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.InternVLForMultiModal">InternVLForMultiModal</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedJanusForMultiModal" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedJanusForMultiModalextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.JanusForMultiModal]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.JanusForMultiModal]"></a><a id="ReadablePretrainedJanusForMultiModal:ReadablePretrainedJanusForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedJanusForMultiModal.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedJanusForMultiModal.html"><span class="name">ReadablePretrainedJanusForMultiModal</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="JanusForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.JanusForMultiModal">JanusForMultiModal</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="JanusForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.JanusForMultiModal">JanusForMultiModal</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedLLAVAForMultiModal" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedLLAVAForMultiModalextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.LLAVAForMultiModal]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.LLAVAForMultiModal]"></a><a id="ReadablePretrainedLLAVAForMultiModal:ReadablePretrainedLLAVAForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedLLAVAForMultiModal.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedLLAVAForMultiModal.html"><span class="name">ReadablePretrainedLLAVAForMultiModal</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="LLAVAForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.LLAVAForMultiModal">LLAVAForMultiModal</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="LLAVAForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.LLAVAForMultiModal">LLAVAForMultiModal</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedMLLamaForMultimodal" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedMLLamaForMultimodalextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.MLLamaForMultimodal]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.MLLamaForMultimodal]"></a><a id="ReadablePretrainedMLLamaForMultimodal:ReadablePretrainedMLLamaForMultimodal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedMLLamaForMultimodal.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedMLLamaForMultimodal.html"><span class="name">ReadablePretrainedMLLamaForMultimodal</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="MLLamaForMultimodal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.MLLamaForMultimodal">MLLamaForMultimodal</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="MLLamaForMultimodal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.MLLamaForMultimodal">MLLamaForMultimodal</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedPaliGemmaForMultiModal" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedPaliGemmaForMultiModalextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.PaliGemmaForMultiModal]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.PaliGemmaForMultiModal]"></a><a id="ReadablePretrainedPaliGemmaForMultiModal:ReadablePretrainedPaliGemmaForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedPaliGemmaForMultiModal.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedPaliGemmaForMultiModal.html"><span class="name">ReadablePretrainedPaliGemmaForMultiModal</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="PaliGemmaForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.PaliGemmaForMultiModal">PaliGemmaForMultiModal</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="PaliGemmaForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.PaliGemmaForMultiModal">PaliGemmaForMultiModal</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedPhi3Vision" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedPhi3VisionextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.Phi3Vision]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.Phi3Vision]"></a><a id="ReadablePretrainedPhi3Vision:ReadablePretrainedPhi3Vision"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedPhi3Vision.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedPhi3Vision.html"><span class="name">ReadablePretrainedPhi3Vision</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="Phi3Vision.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Phi3Vision">Phi3Vision</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="Phi3Vision.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Phi3Vision">Phi3Vision</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedQwen2VLTransformer" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedQwen2VLTransformerextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.Qwen2VLTransformer]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.Qwen2VLTransformer]"></a><a id="ReadablePretrainedQwen2VLTransformer:ReadablePretrainedQwen2VLTransformer"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedQwen2VLTransformer.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedQwen2VLTransformer.html"><span class="name">ReadablePretrainedQwen2VLTransformer</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="Qwen2VLTransformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Qwen2VLTransformer">Qwen2VLTransformer</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="Qwen2VLTransformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.Qwen2VLTransformer">Qwen2VLTransformer</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedSmolVLMTransformer" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedSmolVLMTransformerextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.SmolVLMTransformer]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.SmolVLMTransformer]"></a><a id="ReadablePretrainedSmolVLMTransformer:ReadablePretrainedSmolVLMTransformer"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedSmolVLMTransformer.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedSmolVLMTransformer.html"><span class="name">ReadablePretrainedSmolVLMTransformer</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="SmolVLMTransformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.SmolVLMTransformer">SmolVLMTransformer</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="SmolVLMTransformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.SmolVLMTransformer">SmolVLMTransformer</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedSwinForImageModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedSwinForImageModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.SwinForImageClassification]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.SwinForImageClassification]"></a><a id="ReadablePretrainedSwinForImageModel:ReadablePretrainedSwinForImageModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedSwinForImageModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedSwinForImageModel.html"><span class="name">ReadablePretrainedSwinForImageModel</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="SwinForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.SwinForImageClassification">SwinForImageClassification</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="SwinForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.SwinForImageClassification">SwinForImageClassification</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedViTForImageModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedViTForImageModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification]"></a><a id="ReadablePretrainedViTForImageModel:ReadablePretrainedViTForImageModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedViTForImageModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedViTForImageModel.html"><span class="name">ReadablePretrainedViTForImageModel</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="ViTForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification">ViTForImageClassification</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="ViTForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification">ViTForImageClassification</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedVisionEncoderDecoderModel" visbl="pub" class="indented0 " data-isabs="true" fullComment="no" group="Ungrouped">
      <a id="ReadablePretrainedVisionEncoderDecoderModelextendsParamsAndFeaturesReadable[com.johnsnowlabs.nlp.annotators.cv.VisionEncoderDecoderForImageCaptioning]withHasPretrained[com.johnsnowlabs.nlp.annotators.cv.VisionEncoderDecoderForImageCaptioning]"></a><a id="ReadablePretrainedVisionEncoderDecoderModel:ReadablePretrainedVisionEncoderDecoderModel"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ReadablePretrainedVisionEncoderDecoderModel.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">trait</span>
      </span>
      <span class="symbol">
        <a title="" href="ReadablePretrainedVisionEncoderDecoderModel.html"><span class="name">ReadablePretrainedVisionEncoderDecoderModel</span></a><span class="result"> extends <a href="../../ParamsAndFeaturesReadable.html" class="extype" name="com.johnsnowlabs.nlp.ParamsAndFeaturesReadable">ParamsAndFeaturesReadable</a>[<a href="VisionEncoderDecoderForImageCaptioning.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.VisionEncoderDecoderForImageCaptioning">VisionEncoderDecoderForImageCaptioning</a>] with <a href="../../HasPretrained.html" class="extype" name="com.johnsnowlabs.nlp.HasPretrained">HasPretrained</a>[<a href="VisionEncoderDecoderForImageCaptioning.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.VisionEncoderDecoderForImageCaptioning">VisionEncoderDecoderForImageCaptioning</a>]</span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.SmolVLMTransformer" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SmolVLMTransformerextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.SmolVLMTransformer]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.SmolVLMTransformer]withHasImageFeaturePropertieswithWriteOpenvinoModelwithHasGeneratorPropertieswithHasEngine"></a><a id="SmolVLMTransformer:SmolVLMTransformer"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/SmolVLMTransformer.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="SmolVLMTransformer can load SmolVLM models for visual question answering." href="SmolVLMTransformer.html"><span class="name">SmolVLMTransformer</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="SmolVLMTransformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.SmolVLMTransformer">SmolVLMTransformer</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="SmolVLMTransformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.SmolVLMTransformer">SmolVLMTransformer</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasGeneratorProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasGeneratorProperties">HasGeneratorProperties</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">SmolVLMTransformer can load SmolVLM models for visual question answering.</p><div class="fullcomment"><div class="comment cmt"><p>SmolVLMTransformer can load SmolVLM models for visual question answering. The model consists
of a vision encoder, a text encoder as well as a text decoder. The vision encoder will encode
the input image, the text encoder will encode the input question together with the encoding of
the image, and the text decoder will output the answer to the question.</p><p>SmolVLM is a compact open multimodal model that accepts arbitrary sequences of image and text
inputs to produce text outputs. Designed for efficiency, SmolVLM can answer questions about
images, describe visual content, create stories grounded on multiple images, or function as a
pure language model without visual inputs. Its lightweight architecture makes it suitable for
on-device applications while maintaining strong performance on multimodal tasks.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> visualQA = SmolVLMTransformer.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)</pre><p>The default model is <code>&quot;smolvlm_instruct_int4&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Question+Answering" target="_blank">Models Hub</a>.</p><p>Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. To
see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a> and to see more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/SmolVLMTransformerTest.scala" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/SmolVLMTransformerTest.scala</a>.</p><h4>Example</h4><pre><span class="kw">import</span> spark.implicits._
<span class="kw">import</span> com.johnsnowlabs.nlp.base._
<span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF: DataFrame = ResourceHelper.spark.read
 .format(<span class="lit">"image"</span>)
 .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
 .load(imageFolder)

<span class="kw">val</span> testDF: DataFrame = imageDF.withColumn(<span class="lit">"text"</span>, lit(<span class="lit">"&lt;|im_start|&gt;User:&lt;image&gt;Can you describe the image?&lt;end_of_utterance&gt;\nAssistant:"</span>))

<span class="kw">val</span> imageAssembler: ImageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> visualQAClassifier = SmolVLMTransformer.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"answer"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(
  imageAssembler,
  visualQAClassifier
))

<span class="kw">val</span> result = pipeline.fit(testDF).transform(testDF)

result.select(<span class="lit">"image_assembler.origin"</span>, <span class="lit">"answer.result"</span>).show(<span class="kw">false</span>)
+--------------------------------------+------+
|origin                                |result|
+--------------------------------------+------+
|[file:<span class="cmt">///content/images/cat_image.jpg]|[The unusual aspect of this picture is the presence of two cats lying on a pink couch]|</span>
+--------------------------------------+------+</pre></div><dl class="attributes block"> <dt>See also</dt><dd><span class="cmt"><p>
  <a href="CLIPForZeroShotClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification">CLIPForZeroShotClassification</a> for Zero Shot Image Classifier</p></span><span class="cmt"><p>
  <a href="https://sparknlp.org/docs/en/annotators" target="_blank">Annotators Main Page</a> for a list of transformer
  based classifiers</p></span></dd></dl></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.SwinForImageClassification" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SwinForImageClassificationextendsViTForImageClassificationwithHasRescaleFactor"></a><a id="SwinForImageClassification:SwinForImageClassification"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/SwinForImageClassification.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="SwinImageClassification is an image classifier based on Swin." href="SwinForImageClassification.html"><span class="name">SwinForImageClassification</span></a><span class="result"> extends <a href="ViTForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification">ViTForImageClassification</a> with <a href="HasRescaleFactor.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.HasRescaleFactor">HasRescaleFactor</a></span>
      </span>
      
      <p class="shortcomment cmt">SwinImageClassification is an image classifier based on Swin.</p><div class="fullcomment"><div class="comment cmt"><p>SwinImageClassification is an image classifier based on Swin.</p><p>The Swin Transformer was proposed in Swin Transformer: Hierarchical Vision Transformer using
Shifted Windows by Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,
Baining Guo.</p><p>It is basically a hierarchical Transformer whose representation is computed with shifted
windows. The shifted windowing scheme brings greater efficiency by limiting self-attention
computation to non-overlapping local windows while also allowing for cross-window connection.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> imageClassifier = SwinForImageClassification.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"class"</span>)</pre><p>The default model is <code>&quot;image_classifier_swin_base_patch4_window7_224&quot;</code>, if no name is
provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Image+Classification" target="_blank">Models Hub</a>.</p><p>Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. To
see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a> and to see more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/SwinForImageClassificationTest.scala" target="_blank">SwinForImageClassificationTest</a>.</p><p><b>References:</b></p><p><a href="https://arxiv.org/pdf/2103.14030.pdf" target="_blank">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</a></p><p><b>Paper Abstract:</b></p><p><i>This paper presents a new vision Transformer, called Swin Transformer, that capably serves
as a general-purpose backbone for computer vision. Challenges in adapting Transformer from
language to vision arise from differences between the two domains, such as large variations in
the scale of visual entities and the high resolution of pixels in images compared to words in
text. To address these differences, we propose a hierarchical Transformer whose representation
is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by
limiting self-attention computation to non-overlapping local windows while also allowing for
cross-window connection. This hierarchical architecture has the flexibility to model at
various scales and has linear computational complexity with respect to image size. These
qualities of Swin Transformer make it compatible with a broad range of vision tasks, including
image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as
object detection (58.7 box AP and 51.1 mask AP on COCO test- dev) and semantic segmentation
(53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the- art by a large
margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the
potential of Transformer-based models as vision backbones. The hierarchical design and the
shifted window approach also prove beneficial for all-MLP architectures.</i></p><h4>Example</h4><pre><span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> com.johnsnowlabs.nlp.ImageAssembler
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF: DataFrame = spark.read
  .format(<span class="lit">"image"</span>)
  .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
  .load(<span class="lit">"src/test/resources/image/"</span>)

<span class="kw">val</span> imageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> imageClassifier = SwinForImageClassification
  .pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"class"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(imageAssembler, imageClassifier))
<span class="kw">val</span> pipelineDF = pipeline.fit(imageDF).transform(imageDF)

pipelineDF
  .selectExpr(<span class="lit">"reverse(split(image.origin, '/'))[0] as image_name"</span>, <span class="lit">"class.result"</span>)
  .show(truncate = <span class="kw">false</span>)
+-----------------+----------------------------------------------------------+
|image_name       |result                                                    |
+-----------------+----------------------------------------------------------+
|palace.JPEG      |[palace]                                                  |
|egyptian_cat.jpeg|[tabby, tabby cat]                                        |
|hippopotamus.JPEG|[hippopotamus, hippo, river horse, Hippopotamus amphibius]|
|hen.JPEG         |[hen]                                                     |
|ostrich.JPEG     |[ostrich, Struthio camelus]                               |
|junco.JPEG       |[junco, snowbird]                                         |
|bluetick.jpg     |[bluetick]                                                |
|chihuahua.jpg    |[Chihuahua]                                               |
|tractor.JPEG     |[tractor]                                                 |
|ox.JPEG          |[ox]                                                      |
+-----------------+----------------------------------------------------------+</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ViTForImageClassificationextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification]withHasImageFeaturePropertieswithWriteTensorflowModelwithWriteOnnxModelwithWriteOpenvinoModelwithHasEngine"></a><a id="ViTForImageClassification:ViTForImageClassification"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ViTForImageClassification.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="Vision Transformer (ViT) for image classification." href="ViTForImageClassification.html"><span class="name">ViTForImageClassification</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="ViTForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification">ViTForImageClassification</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="ViTForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification">ViTForImageClassification</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a></span>
      </span>
      
      <p class="shortcomment cmt">Vision Transformer (ViT) for image classification.</p><div class="fullcomment"><div class="comment cmt"><p>Vision Transformer (ViT) for image classification.</p><p>ViT is a transformer based alternative to the convolutional neural networks usually used for
image recognition tasks.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> imageClassifier = ViTForImageClassification.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"class"</span>)</pre><p>The default model is <code>&quot;image_classifier_vit_base_patch16_224&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Image+Classification" target="_blank">Models Hub</a>.</p><p>Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. To
see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a> and to see more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/ViTImageClassificationTestSpec.scala" target="_blank">ViTImageClassificationTestSpec</a>.</p><p><b>References:</b></p><p><a href="https://arxiv.org/abs/2010.11929" target="_blank">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p><p><b>Paper Abstract:</b></p><p><i>While the Transformer architecture has become the de-facto standard for natural language
processing tasks, its applications to computer vision remain limited. In vision, attention is
either applied in conjunction with convolutional networks, or used to replace certain
components of convolutional networks while keeping their overall structure in place. We show
that this reliance on CNNs is not necessary and a pure transformer applied directly to
sequences of image patches can perform very well on image classification tasks. When
pre-trained on large amounts of data and transferred to multiple mid-sized or small image
recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains
excellent results compared to state-of-the-art convolutional networks while requiring
substantially fewer computational resources to train.</i></p><h4>Example</h4><pre><span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> com.johnsnowlabs.nlp.ImageAssembler
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF: DataFrame = spark.read
  .format(<span class="lit">"image"</span>)
  .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
  .load(<span class="lit">"src/test/resources/image/"</span>)

<span class="kw">val</span> imageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> imageClassifier = ViTForImageClassification
  .pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"class"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(imageAssembler, imageClassifier))
<span class="kw">val</span> pipelineDF = pipeline.fit(imageDF).transform(imageDF)

pipelineDF
  .selectExpr(<span class="lit">"reverse(split(image.origin, '/'))[0] as image_name"</span>, <span class="lit">"class.result"</span>)
  .show(truncate = <span class="kw">false</span>)
+-----------------+----------------------------------------------------------+
|image_name       |result                                                    |
+-----------------+----------------------------------------------------------+
|palace.JPEG      |[palace]                                                  |
|egyptian_cat.jpeg|[Egyptian cat]                                            |
|hippopotamus.JPEG|[hippopotamus, hippo, river horse, Hippopotamus amphibius]|
|hen.JPEG         |[hen]                                                     |
|ostrich.JPEG     |[ostrich, Struthio camelus]                               |
|junco.JPEG       |[junco, snowbird]                                         |
|bluetick.jpg     |[bluetick]                                                |
|chihuahua.jpg    |[Chihuahua]                                               |
|tractor.JPEG     |[tractor]                                                 |
|ox.JPEG          |[ox]                                                      |
+-----------------+----------------------------------------------------------+</pre></div></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.VisionEncoderDecoderForImageCaptioning" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="VisionEncoderDecoderForImageCaptioningextendsAnnotatorModel[com.johnsnowlabs.nlp.annotators.cv.VisionEncoderDecoderForImageCaptioning]withHasBatchedAnnotateImage[com.johnsnowlabs.nlp.annotators.cv.VisionEncoderDecoderForImageCaptioning]withHasImageFeaturePropertieswithWriteTensorflowModelwithWriteOnnxModelwithWriteOpenvinoModelwithHasEnginewithHasRescaleFactorwithHasGeneratorProperties"></a><a id="VisionEncoderDecoderForImageCaptioning:VisionEncoderDecoderForImageCaptioning"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/VisionEncoderDecoderForImageCaptioning.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">class</span>
      </span>
      <span class="symbol">
        <a title="VisionEncoderDecoder model that converts images into text captions." href="VisionEncoderDecoderForImageCaptioning.html"><span class="name">VisionEncoderDecoderForImageCaptioning</span></a><span class="result"> extends <a href="../../AnnotatorModel.html" class="extype" name="com.johnsnowlabs.nlp.AnnotatorModel">AnnotatorModel</a>[<a href="VisionEncoderDecoderForImageCaptioning.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.VisionEncoderDecoderForImageCaptioning">VisionEncoderDecoderForImageCaptioning</a>] with <a href="../../HasBatchedAnnotateImage.html" class="extype" name="com.johnsnowlabs.nlp.HasBatchedAnnotateImage">HasBatchedAnnotateImage</a>[<a href="VisionEncoderDecoderForImageCaptioning.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.VisionEncoderDecoderForImageCaptioning">VisionEncoderDecoderForImageCaptioning</a>] with <a href="../../HasImageFeatureProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasImageFeatureProperties">HasImageFeatureProperties</a> with <a href="../../../ml/tensorflow/WriteTensorflowModel.html" class="extype" name="com.johnsnowlabs.ml.tensorflow.WriteTensorflowModel">WriteTensorflowModel</a> with <a href="../../../ml/onnx/WriteOnnxModel.html" class="extype" name="com.johnsnowlabs.ml.onnx.WriteOnnxModel">WriteOnnxModel</a> with <a href="../../../ml/openvino/WriteOpenvinoModel.html" class="extype" name="com.johnsnowlabs.ml.openvino.WriteOpenvinoModel">WriteOpenvinoModel</a> with <a href="../../HasEngine.html" class="extype" name="com.johnsnowlabs.nlp.HasEngine">HasEngine</a> with <a href="HasRescaleFactor.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.HasRescaleFactor">HasRescaleFactor</a> with <a href="../../HasGeneratorProperties.html" class="extype" name="com.johnsnowlabs.nlp.HasGeneratorProperties">HasGeneratorProperties</a></span>
      </span>
      
      <p class="shortcomment cmt">VisionEncoderDecoder model that converts images into text captions.</p><div class="fullcomment"><div class="comment cmt"><p>VisionEncoderDecoder model that converts images into text captions. It allows for the use of
pretrained vision auto-encoding models, such as ViT, BEiT, or DeiT as the encoder, in
combination with pretrained language models, like RoBERTa, GPT2, or BERT as the decoder.</p><p>Pretrained models can be loaded with <code>pretrained</code> of the companion object:</p><pre><span class="kw">val</span> imageClassifier = VisionEncoderDecoderForImageCaptioning.pretrained()
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"caption"</span>)</pre><p>The default model is <code>&quot;image_captioning_vit_gpt2&quot;</code>, if no name is provided.</p><p>For available pretrained models please see the
<a href="https://sparknlp.org/models?task=Image+Captioning" target="_blank">Models Hub</a>.</p><p>Models from the HuggingFace 🤗 Transformers library are also compatible with Spark NLP 🚀. To
see which models are compatible and how to import them see
<a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669" target="_blank">https://github.com/JohnSnowLabs/spark-nlp/discussions/5669</a> and to see more extended
examples, see
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/cv/VisionEncoderDecoderForImageCaptioningTestSpec.scala" target="_blank">VisionEncoderDecoderTestSpec</a>.</p><p><b>Note:</b></p><p>This is a very computationally expensive module especially on larger batch sizes. The use of
an accelerator such as GPU is recommended.</p><h4>Example</h4><pre><span class="kw">import</span> com.johnsnowlabs.nlp.annotator._
<span class="kw">import</span> com.johnsnowlabs.nlp.ImageAssembler
<span class="kw">import</span> org.apache.spark.ml.Pipeline

<span class="kw">val</span> imageDF: DataFrame = spark.read
  .format(<span class="lit">"image"</span>)
  .option(<span class="lit">"dropInvalid"</span>, value = <span class="kw">true</span>)
  .load(<span class="lit">"src/test/resources/image/"</span>)

<span class="kw">val</span> imageAssembler = <span class="kw">new</span> ImageAssembler()
  .setInputCol(<span class="lit">"image"</span>)
  .setOutputCol(<span class="lit">"image_assembler"</span>)

<span class="kw">val</span> imageCaptioning = VisionEncoderDecoderForImageCaptioning
  .pretrained()
  .setBeamSize(<span class="num">2</span>)
  .setDoSample(<span class="kw">false</span>)
  .setInputCols(<span class="lit">"image_assembler"</span>)
  .setOutputCol(<span class="lit">"caption"</span>)

<span class="kw">val</span> pipeline = <span class="kw">new</span> Pipeline().setStages(<span class="std">Array</span>(imageAssembler, imageCaptioning))
<span class="kw">val</span> pipelineDF = pipeline.fit(imageDF).transform(imageDF)

pipelineDF
  .selectExpr(<span class="lit">"reverse(split(image.origin, '/'))[0] as image_name"</span>, <span class="lit">"caption.result"</span>)
  .show(truncate = <span class="kw">false</span>)

+-----------------+---------------------------------------------------------+
|image_name       |result                                                   |
+-----------------+---------------------------------------------------------+
|palace.JPEG      |[a large room filled <span class="kw">with</span> furniture and a large window]  |
|egyptian_cat.jpeg|[a cat laying on a couch next to another cat]            |
|hippopotamus.JPEG|[a brown bear in a body of water]                        |
|hen.JPEG         |[a flock of chickens standing next to each other]        |
|ostrich.JPEG     |[a large bird standing on top of a lush green field]     |
|junco.JPEG       |[a small bird standing on a wet ground]                  |
|bluetick.jpg     |[a small dog standing on a wooden floor]                 |
|chihuahua.jpg    |[a small brown dog wearing a blue sweater]               |
|tractor.JPEG     |[a man is standing in a field <span class="kw">with</span> a tractor]            |
|ox.JPEG          |[a large brown cow standing on top of a lush green field]|
+-----------------+---------------------------------------------------------+</pre></div></div>
    </li></ol>
            </div>

        

        <div class="values members">
              <h3>Value Members</h3>
              <ol>
                <li name="com.johnsnowlabs.nlp.annotators.cv.BLIPForQuestionAnswering" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="BLIPForQuestionAnswering"></a><a id="BLIPForQuestionAnswering:BLIPForQuestionAnswering"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/BLIPForQuestionAnswering$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="BLIPForQuestionAnswering$.html"><span class="name">BLIPForQuestionAnswering</span></a><span class="result"> extends <a href="ReadablePretrainedBLIPForQuestionAnswering.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedBLIPForQuestionAnswering">ReadablePretrainedBLIPForQuestionAnswering</a> with <a href="ReadBLIPForQuestionAnsweringDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadBLIPForQuestionAnsweringDLModel">ReadBLIPForQuestionAnsweringDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="CLIPForZeroShotClassification"></a><a id="CLIPForZeroShotClassification:CLIPForZeroShotClassification"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/CLIPForZeroShotClassification$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of CLIPForZeroShotClassification." href="CLIPForZeroShotClassification$.html"><span class="name">CLIPForZeroShotClassification</span></a><span class="result"> extends <a href="ReadablePretrainedCLIPForZeroShotClassificationModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedCLIPForZeroShotClassificationModel">ReadablePretrainedCLIPForZeroShotClassificationModel</a> with <a href="ReadCLIPForZeroShotClassificationModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadCLIPForZeroShotClassificationModel">ReadCLIPForZeroShotClassificationModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="CLIPForZeroShotClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification">CLIPForZeroShotClassification</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="CLIPForZeroShotClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.CLIPForZeroShotClassification">CLIPForZeroShotClassification</a>. Please refer to that class
for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ConvNextForImageClassification" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ConvNextForImageClassification"></a><a id="ConvNextForImageClassification:ConvNextForImageClassification"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ConvNextForImageClassification$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of ConvNextForImageClassification." href="ConvNextForImageClassification$.html"><span class="name">ConvNextForImageClassification</span></a><span class="result"> extends <a href="ReadablePretrainedConvNextForImageModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedConvNextForImageModel">ReadablePretrainedConvNextForImageModel</a> with <a href="ReadConvNextForImageDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadConvNextForImageDLModel">ReadConvNextForImageDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="ConvNextForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ConvNextForImageClassification">ConvNextForImageClassification</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="ConvNextForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ConvNextForImageClassification">ConvNextForImageClassification</a>. Please refer to that class
for the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.Florence2Transformer" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="Florence2Transformer"></a><a id="Florence2Transformer:Florence2Transformer"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/Florence2Transformer$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="Florence2Transformer$.html"><span class="name">Florence2Transformer</span></a><span class="result"> extends <a href="ReadablePretrainedFlorence2TransformerModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedFlorence2TransformerModel">ReadablePretrainedFlorence2TransformerModel</a> with <a href="ReadFlorence2TransformerDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadFlorence2TransformerDLModel">ReadFlorence2TransformerDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.Gemma3ForMultiModal" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="Gemma3ForMultiModal"></a><a id="Gemma3ForMultiModal:Gemma3ForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/Gemma3ForMultiModal$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="Gemma3ForMultiModal$.html"><span class="name">Gemma3ForMultiModal</span></a><span class="result"> extends <a href="ReadablePretrainedGemma3ForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedGemma3ForMultiModal">ReadablePretrainedGemma3ForMultiModal</a> with <a href="ReadGemma3ForMultiModalDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadGemma3ForMultiModalDLModel">ReadGemma3ForMultiModalDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.InternVLForMultiModal" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="InternVLForMultiModal"></a><a id="InternVLForMultiModal:InternVLForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/InternVLForMultiModal$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="InternVLForMultiModal$.html"><span class="name">InternVLForMultiModal</span></a><span class="result"> extends <a href="ReadablePretrainedInternVLForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedInternVLForMultiModal">ReadablePretrainedInternVLForMultiModal</a> with <a href="ReadInternVLForMultiModalDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadInternVLForMultiModalDLModel">ReadInternVLForMultiModalDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.JanusForMultiModal" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="JanusForMultiModal"></a><a id="JanusForMultiModal:JanusForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/JanusForMultiModal$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="JanusForMultiModal$.html"><span class="name">JanusForMultiModal</span></a><span class="result"> extends <a href="ReadablePretrainedJanusForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedJanusForMultiModal">ReadablePretrainedJanusForMultiModal</a> with <a href="ReadJanusForMultiModalDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadJanusForMultiModalDLModel">ReadJanusForMultiModalDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.LLAVAForMultiModal" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="LLAVAForMultiModal"></a><a id="LLAVAForMultiModal:LLAVAForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/LLAVAForMultiModal$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="LLAVAForMultiModal$.html"><span class="name">LLAVAForMultiModal</span></a><span class="result"> extends <a href="ReadablePretrainedLLAVAForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedLLAVAForMultiModal">ReadablePretrainedLLAVAForMultiModal</a> with <a href="ReadLLAVAForMultiModalDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadLLAVAForMultiModalDLModel">ReadLLAVAForMultiModalDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.MLLamaForMultimodal" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="MLLamaForMultimodal"></a><a id="MLLamaForMultimodal:MLLamaForMultimodal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/MLLamaForMultimodal$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="MLLamaForMultimodal$.html"><span class="name">MLLamaForMultimodal</span></a><span class="result"> extends <a href="ReadablePretrainedMLLamaForMultimodal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedMLLamaForMultimodal">ReadablePretrainedMLLamaForMultimodal</a> with <a href="ReadMLLamaForMultimodalDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadMLLamaForMultimodalDLModel">ReadMLLamaForMultimodalDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.PaliGemmaForMultiModal" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="PaliGemmaForMultiModal"></a><a id="PaliGemmaForMultiModal:PaliGemmaForMultiModal"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/PaliGemmaForMultiModal$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="PaliGemmaForMultiModal$.html"><span class="name">PaliGemmaForMultiModal</span></a><span class="result"> extends <a href="ReadablePretrainedPaliGemmaForMultiModal.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedPaliGemmaForMultiModal">ReadablePretrainedPaliGemmaForMultiModal</a> with <a href="ReadPaliGemmaForMultiModalDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadPaliGemmaForMultiModalDLModel">ReadPaliGemmaForMultiModalDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.Phi3Vision" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="Phi3Vision"></a><a id="Phi3Vision:Phi3Vision"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/Phi3Vision$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="Phi3Vision$.html"><span class="name">Phi3Vision</span></a><span class="result"> extends <a href="ReadablePretrainedPhi3Vision.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedPhi3Vision">ReadablePretrainedPhi3Vision</a> with <a href="ReadPhi3VisionDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadPhi3VisionDLModel">ReadPhi3VisionDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.Qwen2VLTransformer" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="Qwen2VLTransformer"></a><a id="Qwen2VLTransformer:Qwen2VLTransformer"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/Qwen2VLTransformer$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="Qwen2VLTransformer$.html"><span class="name">Qwen2VLTransformer</span></a><span class="result"> extends <a href="ReadablePretrainedQwen2VLTransformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedQwen2VLTransformer">ReadablePretrainedQwen2VLTransformer</a> with <a href="ReadQwen2VLTransformerDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadQwen2VLTransformerDLModel">ReadQwen2VLTransformerDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.SmolVLMTransformer" visbl="pub" class="indented0 " data-isabs="false" fullComment="no" group="Ungrouped">
      <a id="SmolVLMTransformer"></a><a id="SmolVLMTransformer:SmolVLMTransformer"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/SmolVLMTransformer$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="" href="SmolVLMTransformer$.html"><span class="name">SmolVLMTransformer</span></a><span class="result"> extends <a href="ReadablePretrainedSmolVLMTransformer.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedSmolVLMTransformer">ReadablePretrainedSmolVLMTransformer</a> with <a href="ReadSmolVLMTransformerDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadSmolVLMTransformerDLModel">ReadSmolVLMTransformerDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.SwinForImageClassification" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="SwinForImageClassification"></a><a id="SwinForImageClassification:SwinForImageClassification"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/SwinForImageClassification$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of SwinForImageClassification." href="SwinForImageClassification$.html"><span class="name">SwinForImageClassification</span></a><span class="result"> extends <a href="ReadablePretrainedSwinForImageModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedSwinForImageModel">ReadablePretrainedSwinForImageModel</a> with <a href="ReadSwinForImageDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadSwinForImageDLModel">ReadSwinForImageDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="SwinForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.SwinForImageClassification">SwinForImageClassification</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="SwinForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.SwinForImageClassification">SwinForImageClassification</a>. Please refer to that class for
the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="ViTForImageClassification"></a><a id="ViTForImageClassification:ViTForImageClassification"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/ViTForImageClassification$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of ViTForImageClassification." href="ViTForImageClassification$.html"><span class="name">ViTForImageClassification</span></a><span class="result"> extends <a href="ReadablePretrainedViTForImageModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedViTForImageModel">ReadablePretrainedViTForImageModel</a> with <a href="ReadViTForImageDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadViTForImageDLModel">ReadViTForImageDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="ViTForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification">ViTForImageClassification</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="ViTForImageClassification.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ViTForImageClassification">ViTForImageClassification</a>. Please refer to that class for
the documentation.
</p></div></div>
    </li><li name="com.johnsnowlabs.nlp.annotators.cv.VisionEncoderDecoderForImageCaptioning" visbl="pub" class="indented0 " data-isabs="false" fullComment="yes" group="Ungrouped">
      <a id="VisionEncoderDecoderForImageCaptioning"></a><a id="VisionEncoderDecoderForImageCaptioning:VisionEncoderDecoderForImageCaptioning"></a>
      <span class="permalink">
      <a href="../../../../../com/johnsnowlabs/nlp/annotators/cv/VisionEncoderDecoderForImageCaptioning$.html" title="Permalink">
        <i class="material-icons"></i>
      </a>
    </span>
      <span class="modifier_kind">
        <span class="modifier"></span>
        <span class="kind">object</span>
      </span>
      <span class="symbol">
        <a title="This is the companion object of VisionEncoderDecoderForImageCaptioning." href="VisionEncoderDecoderForImageCaptioning$.html"><span class="name">VisionEncoderDecoderForImageCaptioning</span></a><span class="result"> extends <a href="ReadablePretrainedVisionEncoderDecoderModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadablePretrainedVisionEncoderDecoderModel">ReadablePretrainedVisionEncoderDecoderModel</a> with <a href="ReadVisionEncoderDecoderDLModel.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.ReadVisionEncoderDecoderDLModel">ReadVisionEncoderDecoderDLModel</a> with <span class="extype" name="scala.Serializable">Serializable</span></span>
      </span>
      
      <p class="shortcomment cmt">This is the companion object of <a href="VisionEncoderDecoderForImageCaptioning.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.VisionEncoderDecoderForImageCaptioning">VisionEncoderDecoderForImageCaptioning</a>.</p><div class="fullcomment"><div class="comment cmt"><p>This is the companion object of <a href="VisionEncoderDecoderForImageCaptioning.html" class="extype" name="com.johnsnowlabs.nlp.annotators.cv.VisionEncoderDecoderForImageCaptioning">VisionEncoderDecoderForImageCaptioning</a>. Please refer to
that class for the documentation.
</p></div></div>
    </li>
              </ol>
            </div>

        

        
        </div>

        <div id="inheritedMembers">
        
        
        </div>

        <div id="groupedMembers">
        <div class="group" name="Ungrouped">
              <h3>Ungrouped</h3>
              
            </div>
        </div>

      </div>

      <div id="tooltip"></div>

      <div id="footer">  </div>
    </body>
          </div>
        </div>
      </div>
    </body>
      </html>
