
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell &#8212; Spark NLP 4.2.5 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../../static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../../../../../static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="../../../../../../static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="../../../../../../static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../../static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../../static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../../../../static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../../../static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../../static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="../../../../../../static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="../../../../../../" id="documentation_options" src="../../../../../../static/documentation_options.js"></script>
    <script src="../../../../../../static/jquery.js"></script>
    <script src="../../../../../../static/underscore.js"></script>
    <script src="../../../../../../static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../../../../static/doctools.js"></script>
    <script src="../../../../../../static/sphinx_highlight.js"></script>
    <script src="../../../../../../static/toggleprompt.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'modules/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell';</script>
    <link rel="shortcut icon" href="../../../../../../static/fav.ico"/>
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../../../../../../search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../../../../../../index.html">

  
  
  
  
  
  
  

  
    <img src="../../../../../../static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../../../../../static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../../../getting_started/index.html">
                        Getting Started
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../../../user_guide/index.html">
                        User Guide
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../../../third_party/index.html">
                        Third Party Projects
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../../../reference/index.html">
                        API Reference
                      </a>
                    </li>
                
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar hide-on-wide">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../../../getting_started/index.html">
                        Getting Started
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../../../user_guide/index.html">
                        User Guide
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../../../third_party/index.html">
                        Third Party Projects
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="../../../../../../reference/index.html">
                        API Reference
                      </a>
                    </li>
                
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        
        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                
            </div>
            
            
            <article class="bd-article" role="main">
              
  <h1>Source code for python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Module for constructing RNN Cells.&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">from</span> <span class="nn">tensorflow.contrib.compiler</span> <span class="kn">import</span> <span class="n">jit</span>
<span class="kn">from</span> <span class="nn">tensorflow.contrib.layers.python.layers</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">tensorflow.contrib.rnn.python.ops</span> <span class="kn">import</span> <span class="n">core_rnn_cell</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">constant_op</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">dtypes</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">op_def_registry</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.framework</span> <span class="kn">import</span> <span class="n">tensor_shape</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="kn">import</span> <span class="n">activations</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras</span> <span class="kn">import</span> <span class="n">initializers</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.keras.engine</span> <span class="kn">import</span> <span class="n">input_spec</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">clip_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">control_flow_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">gen_array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">init_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">math_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">nn_impl</span>  <span class="c1"># pylint: disable=unused-import</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">nn_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">random_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">rnn_cell_impl</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">variable_scope</span> <span class="k">as</span> <span class="n">vs</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="kn">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.util</span> <span class="kn">import</span> <span class="n">nest</span>


<span class="k">def</span> <span class="nf">_get_concat_variable</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">num_shards</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get a sharded variable concatenated into one tensor.&quot;&quot;&quot;</span>
    <span class="n">sharded_variable</span> <span class="o">=</span> <span class="n">_get_sharded_variable</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">num_shards</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sharded_variable</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">sharded_variable</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">concat_name</span> <span class="o">=</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/concat&quot;</span>
    <span class="n">concat_full_name</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">()</span><span class="o">.</span><span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;/&quot;</span> <span class="o">+</span> <span class="n">concat_name</span> <span class="o">+</span> <span class="s2">&quot;:0&quot;</span>
    <span class="k">for</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">ops</span><span class="o">.</span><span class="n">get_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">CONCATENATED_VARIABLES</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="n">concat_full_name</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">value</span>

    <span class="n">concat_variable</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">sharded_variable</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">concat_name</span><span class="p">)</span>
    <span class="n">ops</span><span class="o">.</span><span class="n">add_to_collection</span><span class="p">(</span><span class="n">ops</span><span class="o">.</span><span class="n">GraphKeys</span><span class="o">.</span><span class="n">CONCATENATED_VARIABLES</span><span class="p">,</span> <span class="n">concat_variable</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">concat_variable</span>


<span class="k">def</span> <span class="nf">_get_sharded_variable</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">num_shards</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get a list of sharded variables with the given dtype.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">num_shards</span> <span class="o">&gt;</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Too many shards: shape=</span><span class="si">%s</span><span class="s2">, num_shards=</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">shape</span><span class="p">,</span>
                                                                       <span class="n">num_shards</span><span class="p">))</span>
    <span class="n">unit_shard_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">num_shards</span><span class="p">))</span>
    <span class="n">remaining_rows</span> <span class="o">=</span> <span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">unit_shard_size</span> <span class="o">*</span> <span class="n">num_shards</span>

    <span class="n">shards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_shards</span><span class="p">):</span>
        <span class="n">current_size</span> <span class="o">=</span> <span class="n">unit_shard_size</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">remaining_rows</span><span class="p">:</span>
            <span class="n">current_size</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">shards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span> <span class="p">[</span><span class="n">current_size</span><span class="p">]</span> <span class="o">+</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">shards</span>


<span class="k">def</span> <span class="nf">_norm</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">scope</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
    <span class="n">gamma_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
    <span class="n">beta_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
        <span class="c1"># Initialize beta and gamma for use by layer_norm.</span>
        <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">gamma_init</span><span class="p">)</span>
        <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">beta_init</span><span class="p">)</span>
    <span class="n">normalized</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">normalized</span>


<div class="viewcode-block" id="CoupledInputForgetGateLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.CoupledInputForgetGateLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">CoupledInputForgetGateLSTMCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Long short-term memory unit (LSTM) recurrent network cell.</span>

<span class="sd">    The default non-peephole implementation is based on:</span>

<span class="sd">      https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf</span>

<span class="sd">    Felix Gers, Jurgen Schmidhuber, and Fred Cummins.</span>
<span class="sd">    &quot;Learning to forget: Continual prediction with LSTM.&quot; IET, 850-855, 1999.</span>

<span class="sd">    The peephole implementation is based on:</span>

<span class="sd">      https://research.google.com/pubs/archive/43905.pdf</span>

<span class="sd">    Hasim Sak, Andrew Senior, and Francoise Beaufays.</span>
<span class="sd">    &quot;Long short-term memory recurrent neural network architectures for</span>
<span class="sd">     large scale acoustic modeling.&quot; INTERSPEECH, 2014.</span>

<span class="sd">    The coupling of input and forget gate is based on:</span>

<span class="sd">      http://arxiv.org/pdf/1503.04069.pdf</span>

<span class="sd">    Greff et al. &quot;LSTM: A Search Space Odyssey&quot;</span>

<span class="sd">    The class uses optional peep-hole connections, and an optional projection</span>
<span class="sd">    layer.</span>
<span class="sd">    Layer normalization implementation is based on:</span>

<span class="sd">      https://arxiv.org/abs/1607.06450.</span>

<span class="sd">    &quot;Layer Normalization&quot;</span>
<span class="sd">    Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</span>

<span class="sd">    and is applied before the internal nonlinearities.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">use_peepholes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_proj</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">proj_clip</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_unit_shards</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">num_proj_shards</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">state_is_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">layer_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">norm_gain</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">norm_shift</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the parameters for an LSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the LSTM cell</span>
<span class="sd">          use_peepholes: bool, set True to enable diagonal/peephole connections.</span>
<span class="sd">          initializer: (optional) The initializer to use for the weight and</span>
<span class="sd">            projection matrices.</span>
<span class="sd">          num_proj: (optional) int, The output dimensionality for the projection</span>
<span class="sd">            matrices.  If None, no projection is performed.</span>
<span class="sd">          proj_clip: (optional) A float value.  If `num_proj &gt; 0` and `proj_clip` is</span>
<span class="sd">          provided, then the projected values are clipped elementwise to within</span>
<span class="sd">          `[-proj_clip, proj_clip]`.</span>
<span class="sd">          num_unit_shards: How to split the weight matrix.  If &gt;1, the weight</span>
<span class="sd">            matrix is stored across num_unit_shards.</span>
<span class="sd">          num_proj_shards: How to split the projection matrix.  If &gt;1, the</span>
<span class="sd">            projection matrix is stored across num_proj_shards.</span>
<span class="sd">          forget_bias: Biases of the forget gate are initialized by default to 1</span>
<span class="sd">            in order to reduce the scale of forgetting at the beginning of</span>
<span class="sd">            the training.</span>
<span class="sd">          state_is_tuple: If True, accepted and returned states are 2-tuples of</span>
<span class="sd">            the `c_state` and `m_state`.  By default (False), they are concatenated</span>
<span class="sd">            along the column axis.  This default behavior will soon be deprecated.</span>
<span class="sd">          activation: Activation function of the inner states.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>
<span class="sd">          layer_norm: If `True`, layer normalization will be applied.</span>
<span class="sd">          norm_gain: float, The layer normalization gain initial value. If</span>
<span class="sd">            `layer_norm` has been set to `False`, this argument will be ignored.</span>
<span class="sd">          norm_shift: float, The layer normalization shift initial value. If</span>
<span class="sd">            `layer_norm` has been set to `False`, this argument will be ignored.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CoupledInputForgetGateLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">state_is_tuple</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: Using a concatenated state is slower and will soon be &quot;</span>
                         <span class="s2">&quot;deprecated.  Use state_is_tuple=True.&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span> <span class="o">=</span> <span class="n">use_peepholes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="o">=</span> <span class="n">initializer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="o">=</span> <span class="n">num_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_proj_clip</span> <span class="o">=</span> <span class="n">proj_clip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_unit_shards</span> <span class="o">=</span> <span class="n">num_unit_shards</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj_shards</span> <span class="o">=</span> <span class="n">num_proj_shards</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_tuple</span> <span class="o">=</span> <span class="n">state_is_tuple</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">reuse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layer_norm</span> <span class="o">=</span> <span class="n">layer_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span> <span class="o">=</span> <span class="n">norm_gain</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span> <span class="o">=</span> <span class="n">norm_shift</span>

        <span class="k">if</span> <span class="n">num_proj</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">num_units</span><span class="p">,</span> <span class="n">num_proj</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">state_is_tuple</span> <span class="k">else</span> <span class="n">num_units</span> <span class="o">+</span> <span class="n">num_proj</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">num_proj</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">num_units</span><span class="p">,</span> <span class="n">num_units</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">state_is_tuple</span> <span class="k">else</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_units</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">num_units</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span>

<div class="viewcode-block" id="CoupledInputForgetGateLSTMCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.CoupledInputForgetGateLSTMCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run one step of LSTM.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: input Tensor, 2D, batch x num_units.</span>
<span class="sd">          state: if `state_is_tuple` is False, this must be a state Tensor,</span>
<span class="sd">            `2-D, batch x state_size`.  If `state_is_tuple` is True, this must be a</span>
<span class="sd">            tuple of state Tensors, both `2-D`, with column sizes `c_state` and</span>
<span class="sd">            `m_state`.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tuple containing:</span>
<span class="sd">          - A `2-D, [batch x output_dim]`, Tensor representing the output of the</span>
<span class="sd">            LSTM after reading `inputs` when previous state was `state`.</span>
<span class="sd">            Here output_dim is:</span>
<span class="sd">               num_proj if num_proj was set,</span>
<span class="sd">               num_units otherwise.</span>
<span class="sd">          - Tensor(s) representing the new state of LSTM after reading `inputs` when</span>
<span class="sd">            the previous state was `state`.  Same type and shape(s) as `state`.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: If input size cannot be inferred from inputs via</span>
<span class="sd">            static shape inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sigmoid</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span>

        <span class="n">num_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_tuple</span><span class="p">:</span>
            <span class="p">(</span><span class="n">c_prev</span><span class="p">,</span> <span class="n">m_prev</span><span class="p">)</span> <span class="o">=</span> <span class="n">state</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">c_prev</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">])</span>
            <span class="n">m_prev</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_proj</span><span class="p">])</span>

        <span class="n">dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">input_size</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not infer input size from inputs.get_shape()[-1]&quot;</span><span class="p">)</span>
        <span class="n">concat_w</span> <span class="o">=</span> <span class="n">_get_concat_variable</span><span class="p">(</span>
            <span class="s2">&quot;W&quot;</span><span class="p">,</span>
            <span class="p">[</span><span class="n">input_size</span><span class="o">.</span><span class="n">value</span> <span class="o">+</span> <span class="n">num_proj</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">dtype</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_num_unit_shards</span><span class="p">)</span>

        <span class="n">b</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
            <span class="s2">&quot;B&quot;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">(),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># j = new_input, f = forget_gate, o = output_gate</span>
        <span class="n">cell_inputs</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">m_prev</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">lstm_matrix</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">cell_inputs</span><span class="p">,</span> <span class="n">concat_w</span><span class="p">)</span>

        <span class="c1"># If layer nomalization is applied, do not add bias</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_norm</span><span class="p">:</span>
            <span class="n">lstm_matrix</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">lstm_matrix</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

        <span class="n">j</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">lstm_matrix</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Apply layer normalization</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_norm</span><span class="p">:</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="s2">&quot;transform&quot;</span><span class="p">)</span>
            <span class="n">f</span> <span class="o">=</span> <span class="n">_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="s2">&quot;forget&quot;</span><span class="p">)</span>
            <span class="n">o</span> <span class="o">=</span> <span class="n">_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span>

        <span class="c1"># Diagonal connections</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
            <span class="n">w_f_diag</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="s2">&quot;W_F_diag&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">w_o_diag</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="s2">&quot;W_O_diag&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
            <span class="n">f_act</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">+</span> <span class="n">w_f_diag</span> <span class="o">*</span> <span class="n">c_prev</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">f_act</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="n">f_act</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">f_act</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>

        <span class="c1"># Apply layer normalization</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_norm</span><span class="p">:</span>
            <span class="n">c</span> <span class="o">=</span> <span class="n">_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="s2">&quot;state&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span> <span class="o">+</span> <span class="n">w_o_diag</span> <span class="o">*</span> <span class="n">c</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">m</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">concat_w_proj</span> <span class="o">=</span> <span class="n">_get_concat_variable</span><span class="p">(</span><span class="s2">&quot;W_P&quot;</span><span class="p">,</span>
                                                 <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span><span class="p">],</span>
                                                 <span class="n">dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj_shards</span><span class="p">)</span>

            <span class="n">m</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">concat_w_proj</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proj_clip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># pylint: disable=invalid-unary-operand-type</span>
                <span class="n">m</span> <span class="o">=</span> <span class="n">clip_ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_proj_clip</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proj_clip</span><span class="p">)</span>
                <span class="c1"># pylint: enable=invalid-unary-operand-type</span>

        <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_tuple</span> <span class="k">else</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">c</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">new_state</span></div></div>


<div class="viewcode-block" id="TimeFreqLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.TimeFreqLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">TimeFreqLSTMCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Time-Frequency Long short-term memory unit (LSTM) recurrent network cell.</span>

<span class="sd">    This implementation is based on:</span>

<span class="sd">      Tara N. Sainath and Bo Li</span>
<span class="sd">      &quot;Modeling Time-Frequency Patterns with LSTM vs. Convolutional Architectures</span>
<span class="sd">      for LVCSR Tasks.&quot; submitted to INTERSPEECH, 2016.</span>

<span class="sd">    It uses peep-hole connections and optional cell clipping.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">use_peepholes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">cell_clip</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_unit_shards</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">feature_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">frequency_skip</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the parameters for an LSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the LSTM cell</span>
<span class="sd">          use_peepholes: bool, set True to enable diagonal/peephole connections.</span>
<span class="sd">          cell_clip: (optional) A float value, if provided the cell state is clipped</span>
<span class="sd">            by this value prior to the cell output activation.</span>
<span class="sd">          initializer: (optional) The initializer to use for the weight and</span>
<span class="sd">            projection matrices.</span>
<span class="sd">          num_unit_shards: int, How to split the weight matrix.  If &gt;1, the weight</span>
<span class="sd">            matrix is stored across num_unit_shards.</span>
<span class="sd">          forget_bias: float, Biases of the forget gate are initialized by default</span>
<span class="sd">            to 1 in order to reduce the scale of forgetting at the beginning</span>
<span class="sd">            of the training.</span>
<span class="sd">          feature_size: int, The size of the input feature the LSTM spans over.</span>
<span class="sd">          frequency_skip: int, The amount the LSTM filter is shifted by in</span>
<span class="sd">            frequency.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TimeFreqLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span> <span class="o">=</span> <span class="n">use_peepholes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span> <span class="o">=</span> <span class="n">cell_clip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="o">=</span> <span class="n">initializer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_unit_shards</span> <span class="o">=</span> <span class="n">num_unit_shards</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_feature_size</span> <span class="o">=</span> <span class="n">feature_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_skip</span> <span class="o">=</span> <span class="n">frequency_skip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">reuse</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span>

<div class="viewcode-block" id="TimeFreqLSTMCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.TimeFreqLSTMCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run one step of LSTM.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: input Tensor, 2D, batch x num_units.</span>
<span class="sd">          state: state Tensor, 2D, batch x state_size.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tuple containing:</span>
<span class="sd">          - A 2D, batch x output_dim, Tensor representing the output of the LSTM</span>
<span class="sd">            after reading &quot;inputs&quot; when previous state was &quot;state&quot;.</span>
<span class="sd">            Here output_dim is num_units.</span>
<span class="sd">          - A 2D, batch x state_size, Tensor representing the new state of LSTM</span>
<span class="sd">            after reading &quot;inputs&quot; when previous state was &quot;state&quot;.</span>
<span class="sd">        Raises:</span>
<span class="sd">          ValueError: if an input_size was specified and the provided inputs have</span>
<span class="sd">            a different dimension.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sigmoid</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span>
        <span class="n">tanh</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span>

        <span class="n">freq_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_tf_features</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">actual_input_size</span> <span class="o">=</span> <span class="n">freq_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">concat_w</span> <span class="o">=</span> <span class="n">_get_concat_variable</span><span class="p">(</span>
            <span class="s2">&quot;W&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">actual_input_size</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_unit_shards</span><span class="p">)</span>

        <span class="n">b</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
            <span class="s2">&quot;B&quot;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">(),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Diagonal connections</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
            <span class="n">w_f_diag</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="s2">&quot;W_F_diag&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">w_i_diag</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="s2">&quot;W_I_diag&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">w_o_diag</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="s2">&quot;W_O_diag&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># initialize the first freq state to be zero</span>
        <span class="n">m_prev_freq</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
            <span class="p">[</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span> <span class="ow">or</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">dtype</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">fq</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">freq_inputs</span><span class="p">)):</span>
            <span class="n">c_prev</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">fq</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
                                     <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">])</span>
            <span class="n">m_prev</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">fq</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
                                     <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">])</span>
            <span class="c1"># i = input_gate, j = new_input, f = forget_gate, o = output_gate</span>
            <span class="n">cell_inputs</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">freq_inputs</span><span class="p">[</span><span class="n">fq</span><span class="p">],</span> <span class="n">m_prev</span><span class="p">,</span> <span class="n">m_prev_freq</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">lstm_matrix</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">cell_inputs</span><span class="p">,</span> <span class="n">concat_w</span><span class="p">),</span> <span class="n">b</span><span class="p">)</span>
            <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                <span class="n">value</span><span class="o">=</span><span class="n">lstm_matrix</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                <span class="n">c</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">+</span> <span class="n">w_f_diag</span> <span class="o">*</span> <span class="n">c_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span>
                        <span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">w_i_diag</span> <span class="o">*</span> <span class="n">c_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">c</span> <span class="o">=</span> <span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># pylint: disable=invalid-unary-operand-type</span>
                <span class="n">c</span> <span class="o">=</span> <span class="n">clip_ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span><span class="p">)</span>
                <span class="c1"># pylint: enable=invalid-unary-operand-type</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                <span class="n">m</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span> <span class="o">+</span> <span class="n">w_o_diag</span> <span class="o">*</span> <span class="n">c</span><span class="p">)</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">m</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
            <span class="n">m_prev_freq</span> <span class="o">=</span> <span class="n">m</span>
            <span class="k">if</span> <span class="n">fq</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">state_out</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">c</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">m_out</span> <span class="o">=</span> <span class="n">m</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">state_out</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">state_out</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">m_out</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">m_out</span><span class="p">,</span> <span class="n">m</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">m_out</span><span class="p">,</span> <span class="n">state_out</span></div>

    <span class="k">def</span> <span class="nf">_make_tf_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_feat</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Make the frequency features.</span>

<span class="sd">        Args:</span>
<span class="sd">          input_feat: input Tensor, 2D, batch x num_units.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A list of frequency features, with each element containing:</span>
<span class="sd">          - A 2D, batch x output_dim, Tensor representing the time-frequency feature</span>
<span class="sd">            for that frequency index. Here output_dim is feature_size.</span>
<span class="sd">        Raises:</span>
<span class="sd">          ValueError: if input_size cannot be inferred from static shape inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">input_feat</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
        <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot infer input_size from static shape inference.&quot;</span><span class="p">)</span>
        <span class="n">num_feats</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
            <span class="p">(</span><span class="n">input_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_size</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_frequency_skip</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="n">freq_inputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_feats</span><span class="p">):</span>
            <span class="n">cur_input</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">input_feat</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">f</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_skip</span><span class="p">],</span>
                                        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_size</span><span class="p">])</span>
            <span class="n">freq_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">freq_inputs</span></div>


<div class="viewcode-block" id="GridLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.GridLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">GridLSTMCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Grid Long short-term memory unit (LSTM) recurrent network cell.</span>

<span class="sd">    The default is based on:</span>
<span class="sd">      Nal Kalchbrenner, Ivo Danihelka and Alex Graves</span>
<span class="sd">      &quot;Grid Long Short-Term Memory,&quot; Proc. ICLR 2016.</span>
<span class="sd">      http://arxiv.org/abs/1507.01526</span>

<span class="sd">    When peephole connections are used, the implementation is based on:</span>
<span class="sd">      Tara N. Sainath and Bo Li</span>
<span class="sd">      &quot;Modeling Time-Frequency Patterns with LSTM vs. Convolutional Architectures</span>
<span class="sd">      for LVCSR Tasks.&quot; submitted to INTERSPEECH, 2016.</span>

<span class="sd">    The code uses optional peephole connections, shared_weights and cell clipping.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">use_peepholes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">share_time_frequency_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">cell_clip</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_unit_shards</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">feature_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">frequency_skip</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_frequency_blocks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">start_freqindex_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">end_freqindex_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">couple_input_forget_gates</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">state_is_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the parameters for an LSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the LSTM cell</span>
<span class="sd">          use_peepholes: (optional) bool, default False. Set True to enable</span>
<span class="sd">            diagonal/peephole connections.</span>
<span class="sd">          share_time_frequency_weights: (optional) bool, default False. Set True to</span>
<span class="sd">            enable shared cell weights between time and frequency LSTMs.</span>
<span class="sd">          cell_clip: (optional) A float value, default None, if provided the cell</span>
<span class="sd">            state is clipped by this value prior to the cell output activation.</span>
<span class="sd">          initializer: (optional) The initializer to use for the weight and</span>
<span class="sd">            projection matrices, default None.</span>
<span class="sd">          num_unit_shards: (optional) int, default 1, How to split the weight</span>
<span class="sd">            matrix. If &gt; 1, the weight matrix is stored across num_unit_shards.</span>
<span class="sd">          forget_bias: (optional) float, default 1.0, The initial bias of the</span>
<span class="sd">            forget gates, used to reduce the scale of forgetting at the beginning</span>
<span class="sd">            of the training.</span>
<span class="sd">          feature_size: (optional) int, default None, The size of the input feature</span>
<span class="sd">            the LSTM spans over.</span>
<span class="sd">          frequency_skip: (optional) int, default None, The amount the LSTM filter</span>
<span class="sd">            is shifted by in frequency.</span>
<span class="sd">          num_frequency_blocks: [required] A list of frequency blocks needed to</span>
<span class="sd">            cover the whole input feature splitting defined by start_freqindex_list</span>
<span class="sd">            and end_freqindex_list.</span>
<span class="sd">          start_freqindex_list: [optional], list of ints, default None,  The</span>
<span class="sd">            starting frequency index for each frequency block.</span>
<span class="sd">          end_freqindex_list: [optional], list of ints, default None. The ending</span>
<span class="sd">            frequency index for each frequency block.</span>
<span class="sd">          couple_input_forget_gates: (optional) bool, default False, Whether to</span>
<span class="sd">            couple the input and forget gates, i.e. f_gate = 1.0 - i_gate, to reduce</span>
<span class="sd">            model parameters and computation cost.</span>
<span class="sd">          state_is_tuple: If True, accepted and returned states are 2-tuples of</span>
<span class="sd">            the `c_state` and `m_state`.  By default (False), they are concatenated</span>
<span class="sd">            along the column axis.  This default behavior will soon be deprecated.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>
<span class="sd">        Raises:</span>
<span class="sd">          ValueError: if the num_frequency_blocks list is not specified</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GridLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">state_is_tuple</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: Using a concatenated state is slower and will soon be &quot;</span>
                         <span class="s2">&quot;deprecated.  Use state_is_tuple=True.&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span> <span class="o">=</span> <span class="n">use_peepholes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_share_time_frequency_weights</span> <span class="o">=</span> <span class="n">share_time_frequency_weights</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_couple_input_forget_gates</span> <span class="o">=</span> <span class="n">couple_input_forget_gates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_tuple</span> <span class="o">=</span> <span class="n">state_is_tuple</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span> <span class="o">=</span> <span class="n">cell_clip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="o">=</span> <span class="n">initializer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_unit_shards</span> <span class="o">=</span> <span class="n">num_unit_shards</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_feature_size</span> <span class="o">=</span> <span class="n">feature_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_skip</span> <span class="o">=</span> <span class="n">frequency_skip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_start_freqindex_list</span> <span class="o">=</span> <span class="n">start_freqindex_list</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_end_freqindex_list</span> <span class="o">=</span> <span class="n">end_freqindex_list</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span> <span class="o">=</span> <span class="n">num_frequency_blocks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_total_blocks</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">reuse</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must specify num_frequency_blocks&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">block_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">)):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_total_blocks</span> <span class="o">+=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">[</span><span class="n">block_index</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">state_is_tuple</span><span class="p">:</span>
            <span class="n">state_names</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
            <span class="k">for</span> <span class="n">block_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">freq_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">[</span><span class="n">block_index</span><span class="p">]):</span>
                    <span class="n">name_prefix</span> <span class="o">=</span> <span class="s2">&quot;state_f</span><span class="si">%02d</span><span class="s2">_b</span><span class="si">%02d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">freq_index</span><span class="p">,</span> <span class="n">block_index</span><span class="p">)</span>
                    <span class="n">state_names</span> <span class="o">+=</span> <span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_c, </span><span class="si">%s</span><span class="s2">_m,&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name_prefix</span><span class="p">,</span> <span class="n">name_prefix</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_tuple_type</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span><span class="s2">&quot;GridLSTMStateTuple&quot;</span><span class="p">,</span>
                                                            <span class="n">state_names</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_tuple_type</span><span class="p">(</span><span class="o">*</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">num_units</span><span class="p">,</span> <span class="n">num_units</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_blocks</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_tuple_type</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">num_units</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_blocks</span> <span class="o">*</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">num_units</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_blocks</span> <span class="o">*</span> <span class="mi">2</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_tuple_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_tuple_type</span>

<div class="viewcode-block" id="GridLSTMCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.GridLSTMCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run one step of LSTM.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: input Tensor, 2D, [batch, feature_size].</span>
<span class="sd">          state: Tensor or tuple of Tensors, 2D, [batch, state_size], depends on the</span>
<span class="sd">            flag self._state_is_tuple.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tuple containing:</span>
<span class="sd">          - A 2D, [batch, output_dim], Tensor representing the output of the LSTM</span>
<span class="sd">            after reading &quot;inputs&quot; when previous state was &quot;state&quot;.</span>
<span class="sd">            Here output_dim is num_units.</span>
<span class="sd">          - A 2D, [batch, state_size], Tensor representing the new state of LSTM</span>
<span class="sd">            after reading &quot;inputs&quot; when previous state was &quot;state&quot;.</span>
<span class="sd">        Raises:</span>
<span class="sd">          ValueError: if an input_size was specified and the provided inputs have</span>
<span class="sd">            a different dimension.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">or</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">freq_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_tf_features</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">m_out_lst</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">state_out_lst</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">freq_inputs</span><span class="p">)):</span>
            <span class="n">m_out_lst_current</span><span class="p">,</span> <span class="n">state_out_lst_current</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute</span><span class="p">(</span>
                <span class="n">freq_inputs</span><span class="p">[</span><span class="n">block</span><span class="p">],</span>
                <span class="n">block</span><span class="p">,</span>
                <span class="n">state</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="p">,</span>
                <span class="n">state_is_tuple</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_state_is_tuple</span><span class="p">)</span>
            <span class="n">m_out_lst</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">m_out_lst_current</span><span class="p">)</span>
            <span class="n">state_out_lst</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">state_out_lst_current</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_tuple</span><span class="p">:</span>
            <span class="n">state_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_tuple_type</span><span class="p">(</span><span class="o">*</span><span class="n">state_out_lst</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state_out</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">state_out_lst</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">m_out</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">m_out_lst</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">m_out</span><span class="p">,</span> <span class="n">state_out</span></div>

    <span class="k">def</span> <span class="nf">_compute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">freq_inputs</span><span class="p">,</span>
                 <span class="n">block</span><span class="p">,</span>
                 <span class="n">state</span><span class="p">,</span>
                 <span class="n">batch_size</span><span class="p">,</span>
                 <span class="n">state_prefix</span><span class="o">=</span><span class="s2">&quot;state&quot;</span><span class="p">,</span>
                 <span class="n">state_is_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run the actual computation of one step LSTM.</span>

<span class="sd">        Args:</span>
<span class="sd">          freq_inputs: list of Tensors, 2D, [batch, feature_size].</span>
<span class="sd">          block: int, current frequency block index to process.</span>
<span class="sd">          state: Tensor or tuple of Tensors, 2D, [batch, state_size], it depends on</span>
<span class="sd">            the flag state_is_tuple.</span>
<span class="sd">          batch_size: int32, batch size.</span>
<span class="sd">          state_prefix: (optional) string, name prefix for states, defaults to</span>
<span class="sd">            &quot;state&quot;.</span>
<span class="sd">          state_is_tuple: boolean, indicates whether the state is a tuple or Tensor.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tuple, containing:</span>
<span class="sd">          - A list of [batch, output_dim] Tensors, representing the output of the</span>
<span class="sd">            LSTM given the inputs and state.</span>
<span class="sd">          - A list of [batch, state_size] Tensors, representing the LSTM state</span>
<span class="sd">            values given the inputs and previous state.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sigmoid</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span>
        <span class="n">tanh</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span>
        <span class="n">num_gates</span> <span class="o">=</span> <span class="mi">3</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_couple_input_forget_gates</span> <span class="k">else</span> <span class="mi">4</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">freq_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">actual_input_size</span> <span class="o">=</span> <span class="n">freq_inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">concat_w_f</span> <span class="o">=</span> <span class="n">_get_concat_variable</span><span class="p">(</span>
            <span class="s2">&quot;W_f_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span>
            <span class="p">[</span><span class="n">actual_input_size</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="n">num_gates</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_unit_shards</span><span class="p">)</span>
        <span class="n">b_f</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
            <span class="s2">&quot;B_f_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">num_gates</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">(),</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_share_time_frequency_weights</span><span class="p">:</span>
            <span class="n">concat_w_t</span> <span class="o">=</span> <span class="n">_get_concat_variable</span><span class="p">(</span><span class="s2">&quot;W_t_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="p">[</span>
                <span class="n">actual_input_size</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="n">num_gates</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>
            <span class="p">],</span> <span class="n">dtype</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_unit_shards</span><span class="p">)</span>
            <span class="n">b_t</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="s2">&quot;B_t_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">num_gates</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">(),</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
            <span class="c1"># Diagonal connections</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_couple_input_forget_gates</span><span class="p">:</span>
                <span class="n">w_f_diag_freqf</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                    <span class="s2">&quot;W_F_diag_freqf_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">w_f_diag_freqt</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                    <span class="s2">&quot;W_F_diag_freqt_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">w_i_diag_freqf</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="s2">&quot;W_I_diag_freqf_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">w_i_diag_freqt</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="s2">&quot;W_I_diag_freqt_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">w_o_diag_freqf</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="s2">&quot;W_O_diag_freqf_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">w_o_diag_freqt</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="s2">&quot;W_O_diag_freqt_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_share_time_frequency_weights</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_couple_input_forget_gates</span><span class="p">:</span>
                    <span class="n">w_f_diag_timef</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                        <span class="s2">&quot;W_F_diag_timef_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="n">w_f_diag_timet</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                        <span class="s2">&quot;W_F_diag_timet_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">w_i_diag_timef</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                    <span class="s2">&quot;W_I_diag_timef_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">w_i_diag_timet</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                    <span class="s2">&quot;W_I_diag_timet_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">w_o_diag_timef</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                    <span class="s2">&quot;W_O_diag_timef_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">w_o_diag_timet</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                    <span class="s2">&quot;W_O_diag_timet_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">block</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># initialize the first freq state to be zero</span>
        <span class="n">m_prev_freq</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="n">c_prev_freq</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">freq_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">freq_inputs</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">state_is_tuple</span><span class="p">:</span>
                <span class="n">name_prefix</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_f</span><span class="si">%02d</span><span class="s2">_b</span><span class="si">%02d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">state_prefix</span><span class="p">,</span> <span class="n">freq_index</span><span class="p">,</span> <span class="n">block</span><span class="p">)</span>
                <span class="n">c_prev_time</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">name_prefix</span> <span class="o">+</span> <span class="s2">&quot;_c&quot;</span><span class="p">)</span>
                <span class="n">m_prev_time</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">name_prefix</span> <span class="o">+</span> <span class="s2">&quot;_m&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">c_prev_time</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span>
                    <span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">freq_index</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">])</span>
                <span class="n">m_prev_time</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span>
                    <span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">freq_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
                    <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">])</span>

            <span class="c1"># i = input_gate, j = new_input, f = forget_gate, o = output_gate</span>
            <span class="n">cell_inputs</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                <span class="p">[</span><span class="n">freq_inputs</span><span class="p">[</span><span class="n">freq_index</span><span class="p">],</span> <span class="n">m_prev_time</span><span class="p">,</span> <span class="n">m_prev_freq</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>

            <span class="c1"># F-LSTM</span>
            <span class="n">lstm_matrix_freq</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span>
                <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">cell_inputs</span><span class="p">,</span> <span class="n">concat_w_f</span><span class="p">),</span> <span class="n">b_f</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_couple_input_forget_gates</span><span class="p">:</span>
                <span class="n">i_freq</span><span class="p">,</span> <span class="n">j_freq</span><span class="p">,</span> <span class="n">o_freq</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                    <span class="n">value</span><span class="o">=</span><span class="n">lstm_matrix_freq</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="n">num_gates</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">f_freq</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">i_freq</span><span class="p">,</span> <span class="n">j_freq</span><span class="p">,</span> <span class="n">f_freq</span><span class="p">,</span> <span class="n">o_freq</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                    <span class="n">value</span><span class="o">=</span><span class="n">lstm_matrix_freq</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="n">num_gates</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># T-LSTM</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_share_time_frequency_weights</span><span class="p">:</span>
                <span class="n">i_time</span> <span class="o">=</span> <span class="n">i_freq</span>
                <span class="n">j_time</span> <span class="o">=</span> <span class="n">j_freq</span>
                <span class="n">f_time</span> <span class="o">=</span> <span class="n">f_freq</span>
                <span class="n">o_time</span> <span class="o">=</span> <span class="n">o_freq</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">lstm_matrix_time</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span>
                    <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">cell_inputs</span><span class="p">,</span> <span class="n">concat_w_t</span><span class="p">),</span> <span class="n">b_t</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_couple_input_forget_gates</span><span class="p">:</span>
                    <span class="n">i_time</span><span class="p">,</span> <span class="n">j_time</span><span class="p">,</span> <span class="n">o_time</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                        <span class="n">value</span><span class="o">=</span><span class="n">lstm_matrix_time</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="n">num_gates</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">f_time</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">i_time</span><span class="p">,</span> <span class="n">j_time</span><span class="p">,</span> <span class="n">f_time</span><span class="p">,</span> <span class="n">o_time</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                        <span class="n">value</span><span class="o">=</span><span class="n">lstm_matrix_time</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="n">num_gates</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># F-LSTM c_freq</span>
            <span class="c1"># input gate activations</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                <span class="n">i_freq_g</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">i_freq</span> <span class="o">+</span> <span class="n">w_i_diag_freqf</span> <span class="o">*</span> <span class="n">c_prev_freq</span> <span class="o">+</span>
                                   <span class="n">w_i_diag_freqt</span> <span class="o">*</span> <span class="n">c_prev_time</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">i_freq_g</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">i_freq</span><span class="p">)</span>
            <span class="c1"># forget gate activations</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_couple_input_forget_gates</span><span class="p">:</span>
                <span class="n">f_freq_g</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">i_freq_g</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                    <span class="n">f_freq_g</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f_freq</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">+</span> <span class="n">w_f_diag_freqf</span> <span class="o">*</span>
                                       <span class="n">c_prev_freq</span> <span class="o">+</span> <span class="n">w_f_diag_freqt</span> <span class="o">*</span> <span class="n">c_prev_time</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">f_freq_g</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f_freq</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span>
            <span class="c1"># cell state</span>
            <span class="n">c_freq</span> <span class="o">=</span> <span class="n">f_freq_g</span> <span class="o">*</span> <span class="n">c_prev_freq</span> <span class="o">+</span> <span class="n">i_freq_g</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">j_freq</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># pylint: disable=invalid-unary-operand-type</span>
                <span class="n">c_freq</span> <span class="o">=</span> <span class="n">clip_ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">c_freq</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span><span class="p">,</span>
                                                <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span><span class="p">)</span>
                <span class="c1"># pylint: enable=invalid-unary-operand-type</span>

            <span class="c1"># T-LSTM c_freq</span>
            <span class="c1"># input gate activations</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_share_time_frequency_weights</span><span class="p">:</span>
                    <span class="n">i_time_g</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">i_time</span> <span class="o">+</span> <span class="n">w_i_diag_freqf</span> <span class="o">*</span> <span class="n">c_prev_freq</span> <span class="o">+</span>
                                       <span class="n">w_i_diag_freqt</span> <span class="o">*</span> <span class="n">c_prev_time</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">i_time_g</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">i_time</span> <span class="o">+</span> <span class="n">w_i_diag_timef</span> <span class="o">*</span> <span class="n">c_prev_freq</span> <span class="o">+</span>
                                       <span class="n">w_i_diag_timet</span> <span class="o">*</span> <span class="n">c_prev_time</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">i_time_g</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">i_time</span><span class="p">)</span>
            <span class="c1"># forget gate activations</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_couple_input_forget_gates</span><span class="p">:</span>
                <span class="n">f_time_g</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">i_time_g</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_share_time_frequency_weights</span><span class="p">:</span>
                        <span class="n">f_time_g</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f_time</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">+</span> <span class="n">w_f_diag_freqf</span> <span class="o">*</span>
                                           <span class="n">c_prev_freq</span> <span class="o">+</span> <span class="n">w_f_diag_freqt</span> <span class="o">*</span> <span class="n">c_prev_time</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">f_time_g</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f_time</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">+</span> <span class="n">w_f_diag_timef</span> <span class="o">*</span>
                                           <span class="n">c_prev_freq</span> <span class="o">+</span> <span class="n">w_f_diag_timet</span> <span class="o">*</span> <span class="n">c_prev_time</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">f_time_g</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f_time</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span>
            <span class="c1"># cell state</span>
            <span class="n">c_time</span> <span class="o">=</span> <span class="n">f_time_g</span> <span class="o">*</span> <span class="n">c_prev_time</span> <span class="o">+</span> <span class="n">i_time_g</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">j_time</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># pylint: disable=invalid-unary-operand-type</span>
                <span class="n">c_time</span> <span class="o">=</span> <span class="n">clip_ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">c_time</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span><span class="p">,</span>
                                                <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span><span class="p">)</span>
                <span class="c1"># pylint: enable=invalid-unary-operand-type</span>

            <span class="c1"># F-LSTM m_freq</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                <span class="n">m_freq</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o_freq</span> <span class="o">+</span> <span class="n">w_o_diag_freqf</span> <span class="o">*</span> <span class="n">c_freq</span> <span class="o">+</span>
                                 <span class="n">w_o_diag_freqt</span> <span class="o">*</span> <span class="n">c_time</span><span class="p">)</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c_freq</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">m_freq</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o_freq</span><span class="p">)</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c_freq</span><span class="p">)</span>

            <span class="c1"># T-LSTM m_time</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_share_time_frequency_weights</span><span class="p">:</span>
                    <span class="n">m_time</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o_time</span> <span class="o">+</span> <span class="n">w_o_diag_freqf</span> <span class="o">*</span> <span class="n">c_freq</span> <span class="o">+</span>
                                     <span class="n">w_o_diag_freqt</span> <span class="o">*</span> <span class="n">c_time</span><span class="p">)</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c_time</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">m_time</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o_time</span> <span class="o">+</span> <span class="n">w_o_diag_timef</span> <span class="o">*</span> <span class="n">c_freq</span> <span class="o">+</span>
                                     <span class="n">w_o_diag_timet</span> <span class="o">*</span> <span class="n">c_time</span><span class="p">)</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c_time</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">m_time</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o_time</span><span class="p">)</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">c_time</span><span class="p">)</span>

            <span class="n">m_prev_freq</span> <span class="o">=</span> <span class="n">m_freq</span>
            <span class="n">c_prev_freq</span> <span class="o">=</span> <span class="n">c_freq</span>
            <span class="c1"># Concatenate the outputs for T-LSTM and F-LSTM for each shift</span>
            <span class="k">if</span> <span class="n">freq_index</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">state_out_lst</span> <span class="o">=</span> <span class="p">[</span><span class="n">c_time</span><span class="p">,</span> <span class="n">m_time</span><span class="p">]</span>
                <span class="n">m_out_lst</span> <span class="o">=</span> <span class="p">[</span><span class="n">m_time</span><span class="p">,</span> <span class="n">m_freq</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">state_out_lst</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">c_time</span><span class="p">,</span> <span class="n">m_time</span><span class="p">])</span>
                <span class="n">m_out_lst</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">m_time</span><span class="p">,</span> <span class="n">m_freq</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">m_out_lst</span><span class="p">,</span> <span class="n">state_out_lst</span>

    <span class="k">def</span> <span class="nf">_make_tf_features</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_feat</span><span class="p">,</span> <span class="n">slice_offset</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Make the frequency features.</span>

<span class="sd">        Args:</span>
<span class="sd">          input_feat: input Tensor, 2D, [batch, num_units].</span>
<span class="sd">          slice_offset: (optional) Python int, default 0, the slicing offset is only</span>
<span class="sd">            used for the backward processing in the BidirectionalGridLSTMCell. It</span>
<span class="sd">            specifies a different starting point instead of always 0 to enable the</span>
<span class="sd">            forward and backward processing look at different frequency blocks.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A list of frequency features, with each element containing:</span>
<span class="sd">          - A 2D, [batch, output_dim], Tensor representing the time-frequency</span>
<span class="sd">            feature for that frequency index. Here output_dim is feature_size.</span>
<span class="sd">        Raises:</span>
<span class="sd">          ValueError: if input_size cannot be inferred from static shape inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">input_feat</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
        <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Cannot infer input_size from static shape inference.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">slice_offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Padding to the end</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">input_feat</span><span class="p">,</span>
                                   <span class="n">array_ops</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
                                       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">slice_offset</span><span class="p">],</span>
                                       <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                                       <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="s2">&quot;CONSTANT&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">slice_offset</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Padding to the front</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">input_feat</span><span class="p">,</span>
                                   <span class="n">array_ops</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
                                       <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">slice_offset</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
                                       <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                                       <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="s2">&quot;CONSTANT&quot;</span><span class="p">)</span>
            <span class="n">slice_offset</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">input_feat</span>
        <span class="n">freq_inputs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_freqindex_list</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Length of num_frequency_blocks&quot;</span>
                                 <span class="s2">&quot; is not 1, but instead is </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                                 <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">))</span>
            <span class="n">num_feats</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
                <span class="p">(</span><span class="n">input_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_size</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_frequency_skip</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">num_feats</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Invalid num_frequency_blocks, requires </span><span class="si">%d</span><span class="s2"> but gets </span><span class="si">%d</span><span class="s2">, please&quot;</span>
                    <span class="s2">&quot; check the input size and filter config are correct.&quot;</span> <span class="o">%</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">num_feats</span><span class="p">))</span>
            <span class="n">block_inputs</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_feats</span><span class="p">):</span>
                <span class="n">cur_input</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span>
                    <span class="n">inputs</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">slice_offset</span> <span class="o">+</span> <span class="n">f</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_skip</span><span class="p">],</span>
                    <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_size</span><span class="p">])</span>
                <span class="n">block_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_input</span><span class="p">)</span>
            <span class="n">freq_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block_inputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_start_freqindex_list</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_end_freqindex_list</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Length of start and end freqindex_list&quot;</span>
                                 <span class="s2">&quot; does not match </span><span class="si">%d</span><span class="s2"> </span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">,</span>
                                 <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_start_freqindex_list</span><span class="p">),</span>
                                 <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_end_freqindex_list</span><span class="p">))</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_start_freqindex_list</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Length of num_frequency_blocks&quot;</span>
                                 <span class="s2">&quot; is not equal to start_freqindex_list </span><span class="si">%d</span><span class="s2"> </span><span class="si">%d</span><span class="s2">&quot;</span><span class="p">,</span>
                                 <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">),</span>
                                 <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_start_freqindex_list</span><span class="p">))</span>
            <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_start_freqindex_list</span><span class="p">)):</span>
                <span class="n">start_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_start_freqindex_list</span><span class="p">[</span><span class="n">b</span><span class="p">]</span>
                <span class="n">end_index</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_end_freqindex_list</span><span class="p">[</span><span class="n">b</span><span class="p">]</span>
                <span class="n">cur_size</span> <span class="o">=</span> <span class="n">end_index</span> <span class="o">-</span> <span class="n">start_index</span>
                <span class="n">block_feats</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
                    <span class="p">(</span><span class="n">cur_size</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_size</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_frequency_skip</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="n">block_feats</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">]:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Invalid num_frequency_blocks, requires </span><span class="si">%d</span><span class="s2"> but gets </span><span class="si">%d</span><span class="s2">, please&quot;</span>
                        <span class="s2">&quot; check the input size and filter config are correct.&quot;</span> <span class="o">%</span>
                        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">[</span><span class="n">b</span><span class="p">],</span> <span class="n">block_feats</span><span class="p">))</span>
                <span class="n">block_inputs</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">block_feats</span><span class="p">):</span>
                    <span class="n">cur_input</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span>
                        <span class="n">inputs</span><span class="p">,</span>
                        <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">start_index</span> <span class="o">+</span> <span class="n">slice_offset</span> <span class="o">+</span> <span class="n">f</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_frequency_skip</span><span class="p">],</span>
                        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_feature_size</span><span class="p">])</span>
                    <span class="n">block_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_input</span><span class="p">)</span>
                <span class="n">freq_inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">block_inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">freq_inputs</span></div>


<div class="viewcode-block" id="BidirectionalGridLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.BidirectionalGridLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">BidirectionalGridLSTMCell</span><span class="p">(</span><span class="n">GridLSTMCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Bidirectional GridLstm cell.</span>

<span class="sd">    The bidirection connection is only used in the frequency direction, which</span>
<span class="sd">    hence doesn&#39;t affect the time direction&#39;s real-time processing that is</span>
<span class="sd">    required for online recognition systems.</span>
<span class="sd">    The current implementation uses different weights for the two directions.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">use_peepholes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">share_time_frequency_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">cell_clip</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_unit_shards</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">feature_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">frequency_skip</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_frequency_blocks</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">start_freqindex_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">end_freqindex_list</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">couple_input_forget_gates</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">backward_slice_offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the parameters for an LSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the LSTM cell</span>
<span class="sd">          use_peepholes: (optional) bool, default False. Set True to enable</span>
<span class="sd">            diagonal/peephole connections.</span>
<span class="sd">          share_time_frequency_weights: (optional) bool, default False. Set True to</span>
<span class="sd">            enable shared cell weights between time and frequency LSTMs.</span>
<span class="sd">          cell_clip: (optional) A float value, default None, if provided the cell</span>
<span class="sd">            state is clipped by this value prior to the cell output activation.</span>
<span class="sd">          initializer: (optional) The initializer to use for the weight and</span>
<span class="sd">            projection matrices, default None.</span>
<span class="sd">          num_unit_shards: (optional) int, default 1, How to split the weight</span>
<span class="sd">            matrix. If &gt; 1, the weight matrix is stored across num_unit_shards.</span>
<span class="sd">          forget_bias: (optional) float, default 1.0, The initial bias of the</span>
<span class="sd">            forget gates, used to reduce the scale of forgetting at the beginning</span>
<span class="sd">            of the training.</span>
<span class="sd">          feature_size: (optional) int, default None, The size of the input feature</span>
<span class="sd">            the LSTM spans over.</span>
<span class="sd">          frequency_skip: (optional) int, default None, The amount the LSTM filter</span>
<span class="sd">            is shifted by in frequency.</span>
<span class="sd">          num_frequency_blocks: [required] A list of frequency blocks needed to</span>
<span class="sd">            cover the whole input feature splitting defined by start_freqindex_list</span>
<span class="sd">            and end_freqindex_list.</span>
<span class="sd">          start_freqindex_list: [optional], list of ints, default None,  The</span>
<span class="sd">            starting frequency index for each frequency block.</span>
<span class="sd">          end_freqindex_list: [optional], list of ints, default None. The ending</span>
<span class="sd">            frequency index for each frequency block.</span>
<span class="sd">          couple_input_forget_gates: (optional) bool, default False, Whether to</span>
<span class="sd">            couple the input and forget gates, i.e. f_gate = 1.0 - i_gate, to reduce</span>
<span class="sd">            model parameters and computation cost.</span>
<span class="sd">          backward_slice_offset: (optional) int32, default 0, the starting offset to</span>
<span class="sd">            slice the feature for backward processing.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BidirectionalGridLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">num_units</span><span class="p">,</span> <span class="n">use_peepholes</span><span class="p">,</span> <span class="n">share_time_frequency_weights</span><span class="p">,</span> <span class="n">cell_clip</span><span class="p">,</span>
            <span class="n">initializer</span><span class="p">,</span> <span class="n">num_unit_shards</span><span class="p">,</span> <span class="n">forget_bias</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">frequency_skip</span><span class="p">,</span>
            <span class="n">num_frequency_blocks</span><span class="p">,</span> <span class="n">start_freqindex_list</span><span class="p">,</span> <span class="n">end_freqindex_list</span><span class="p">,</span>
            <span class="n">couple_input_forget_gates</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="n">reuse</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_slice_offset</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">backward_slice_offset</span><span class="p">)</span>
        <span class="n">state_names</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="k">for</span> <span class="n">direction</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;fwd&quot;</span><span class="p">,</span> <span class="s2">&quot;bwd&quot;</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">block_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">freq_index</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_frequency_blocks</span><span class="p">[</span><span class="n">block_index</span><span class="p">]):</span>
                    <span class="n">name_prefix</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_state_f</span><span class="si">%02d</span><span class="s2">_b</span><span class="si">%02d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">direction</span><span class="p">,</span> <span class="n">freq_index</span><span class="p">,</span>
                                                            <span class="n">block_index</span><span class="p">)</span>
                    <span class="n">state_names</span> <span class="o">+=</span> <span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_c, </span><span class="si">%s</span><span class="s2">_m,&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name_prefix</span><span class="p">,</span> <span class="n">name_prefix</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_tuple_type</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
            <span class="s2">&quot;BidirectionalGridLSTMStateTuple&quot;</span><span class="p">,</span> <span class="n">state_names</span><span class="o">.</span><span class="n">strip</span><span class="p">(</span><span class="s2">&quot;,&quot;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_tuple_type</span><span class="p">(</span><span class="o">*</span><span class="p">(</span>
                <span class="p">[</span><span class="n">num_units</span><span class="p">,</span> <span class="n">num_units</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_blocks</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">num_units</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_total_blocks</span> <span class="o">*</span> <span class="mi">2</span>

<div class="viewcode-block" id="BidirectionalGridLSTMCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.BidirectionalGridLSTMCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run one step of LSTM.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: input Tensor, 2D, [batch, num_units].</span>
<span class="sd">          state: tuple of Tensors, 2D, [batch, state_size].</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tuple containing:</span>
<span class="sd">          - A 2D, [batch, output_dim], Tensor representing the output of the LSTM</span>
<span class="sd">            after reading &quot;inputs&quot; when previous state was &quot;state&quot;.</span>
<span class="sd">            Here output_dim is num_units.</span>
<span class="sd">          - A 2D, [batch, state_size], Tensor representing the new state of LSTM</span>
<span class="sd">            after reading &quot;inputs&quot; when previous state was &quot;state&quot;.</span>
<span class="sd">        Raises:</span>
<span class="sd">          ValueError: if an input_size was specified and the provided inputs have</span>
<span class="sd">            a different dimension.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">or</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">fwd_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_tf_features</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_slice_offset</span><span class="p">:</span>
            <span class="n">bwd_inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_tf_features</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_slice_offset</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bwd_inputs</span> <span class="o">=</span> <span class="n">fwd_inputs</span>

        <span class="c1"># Forward processing</span>
        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;fwd&quot;</span><span class="p">):</span>
            <span class="n">fwd_m_out_lst</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">fwd_state_out_lst</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">fwd_inputs</span><span class="p">)):</span>
                <span class="n">fwd_m_out_lst_current</span><span class="p">,</span> <span class="n">fwd_state_out_lst_current</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute</span><span class="p">(</span>
                    <span class="n">fwd_inputs</span><span class="p">[</span><span class="n">block</span><span class="p">],</span>
                    <span class="n">block</span><span class="p">,</span>
                    <span class="n">state</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">state_prefix</span><span class="o">=</span><span class="s2">&quot;fwd_state&quot;</span><span class="p">,</span>
                    <span class="n">state_is_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">fwd_m_out_lst</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">fwd_m_out_lst_current</span><span class="p">)</span>
                <span class="n">fwd_state_out_lst</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">fwd_state_out_lst_current</span><span class="p">)</span>
        <span class="c1"># Backward processing</span>
        <span class="n">bwd_m_out_lst</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">bwd_state_out_lst</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;bwd&quot;</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bwd_inputs</span><span class="p">)):</span>
                <span class="c1"># Reverse the blocks</span>
                <span class="n">bwd_inputs_reverse</span> <span class="o">=</span> <span class="n">bwd_inputs</span><span class="p">[</span><span class="n">block</span><span class="p">][::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">bwd_m_out_lst_current</span><span class="p">,</span> <span class="n">bwd_state_out_lst_current</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute</span><span class="p">(</span>
                    <span class="n">bwd_inputs_reverse</span><span class="p">,</span>
                    <span class="n">block</span><span class="p">,</span>
                    <span class="n">state</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">state_prefix</span><span class="o">=</span><span class="s2">&quot;bwd_state&quot;</span><span class="p">,</span>
                    <span class="n">state_is_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">bwd_m_out_lst</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">bwd_m_out_lst_current</span><span class="p">)</span>
                <span class="n">bwd_state_out_lst</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">bwd_state_out_lst_current</span><span class="p">)</span>
        <span class="n">state_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_tuple_type</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">fwd_state_out_lst</span> <span class="o">+</span> <span class="n">bwd_state_out_lst</span><span class="p">))</span>
        <span class="c1"># Outputs are always concated as it is never used separately.</span>
        <span class="n">m_out</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">fwd_m_out_lst</span> <span class="o">+</span> <span class="n">bwd_m_out_lst</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">m_out</span><span class="p">,</span> <span class="n">state_out</span></div></div>


<span class="c1"># pylint: disable=protected-access</span>
<span class="n">_Linear</span> <span class="o">=</span> <span class="n">core_rnn_cell</span><span class="o">.</span><span class="n">_Linear</span>  <span class="c1"># pylint: disable=invalid-name</span>


<span class="c1"># pylint: enable=protected-access</span>


<div class="viewcode-block" id="AttentionCellWrapper"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.AttentionCellWrapper">[docs]</a><span class="k">class</span> <span class="nc">AttentionCellWrapper</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Basic attention cell wrapper.</span>

<span class="sd">    Implementation based on https://arxiv.org/abs/1601.06733.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">cell</span><span class="p">,</span>
                 <span class="n">attn_length</span><span class="p">,</span>
                 <span class="n">attn_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">attn_vec_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">state_is_tuple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Create a cell with attention.</span>

<span class="sd">        Args:</span>
<span class="sd">          cell: an RNNCell, an attention is added to it.</span>
<span class="sd">          attn_length: integer, the size of an attention window.</span>
<span class="sd">          attn_size: integer, the size of an attention vector. Equal to</span>
<span class="sd">              cell.output_size by default.</span>
<span class="sd">          attn_vec_size: integer, the number of convolutional features calculated</span>
<span class="sd">              on attention state and a size of the hidden layer built from</span>
<span class="sd">              base cell state. Equal attn_size to by default.</span>
<span class="sd">          input_size: integer, the size of a hidden linear layer,</span>
<span class="sd">              built from inputs and attention. Derived from the input tensor</span>
<span class="sd">              by default.</span>
<span class="sd">          state_is_tuple: If True, accepted and returned states are n-tuples, where</span>
<span class="sd">            `n = len(cells)`.  By default (False), the states are all</span>
<span class="sd">            concatenated along the column axis.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>

<span class="sd">        Raises:</span>
<span class="sd">          TypeError: if cell is not an RNNCell.</span>
<span class="sd">          ValueError: if cell returns a state tuple but the flag</span>
<span class="sd">              `state_is_tuple` is `False` or if attn_length is zero or less.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AttentionCellWrapper</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span>
        <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">assert_like_rnncell</span><span class="p">(</span><span class="s2">&quot;cell&quot;</span><span class="p">,</span> <span class="n">cell</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">nest</span><span class="o">.</span><span class="n">is_sequence</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">state_size</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">state_is_tuple</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Cell returns tuple of states, but the flag &quot;</span>
                <span class="s2">&quot;state_is_tuple is not set. State size is: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">cell</span><span class="o">.</span><span class="n">state_size</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">attn_length</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;attn_length should be greater than zero, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">attn_length</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">state_is_tuple</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: Using a concatenated state is slower and will soon be &quot;</span>
                         <span class="s2">&quot;deprecated.  Use state_is_tuple=True.&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn_size</span> <span class="o">=</span> <span class="n">cell</span><span class="o">.</span><span class="n">output_size</span>
        <span class="k">if</span> <span class="n">attn_vec_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn_vec_size</span> <span class="o">=</span> <span class="n">attn_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_tuple</span> <span class="o">=</span> <span class="n">state_is_tuple</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span> <span class="o">=</span> <span class="n">cell</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attn_vec_size</span> <span class="o">=</span> <span class="n">attn_vec_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span> <span class="o">=</span> <span class="n">attn_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_attn_length</span> <span class="o">=</span> <span class="n">attn_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">reuse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_linear3</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">state_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_length</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_tuple</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span>

<div class="viewcode-block" id="AttentionCellWrapper.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.AttentionCellWrapper.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Long short-term memory cell with attention (LSTMA).&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_tuple</span><span class="p">:</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">attns</span><span class="p">,</span> <span class="n">attn_states</span> <span class="o">=</span> <span class="n">state</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">states</span> <span class="o">=</span> <span class="n">state</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">state_size</span><span class="p">])</span>
            <span class="n">attns</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">state_size</span><span class="p">],</span>
                                    <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span><span class="p">])</span>
            <span class="n">attn_states</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span>
                <span class="n">states</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">state_size</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span><span class="p">],</span>
                <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_length</span><span class="p">])</span>
        <span class="n">attn_states</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attn_states</span><span class="p">,</span>
                                        <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span><span class="p">])</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_size</span>
        <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">attns</span><span class="p">],</span> <span class="n">input_size</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">attns</span><span class="p">])</span>
        <span class="n">cell_output</span><span class="p">,</span> <span class="n">new_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_tuple</span><span class="p">:</span>
            <span class="n">new_state_cat</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">new_state</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">new_state_cat</span> <span class="o">=</span> <span class="n">new_state</span>
        <span class="n">new_attns</span><span class="p">,</span> <span class="n">new_attn_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attention</span><span class="p">(</span><span class="n">new_state_cat</span><span class="p">,</span> <span class="n">attn_states</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;attn_output_projection&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">([</span><span class="n">cell_output</span><span class="p">,</span> <span class="n">new_attns</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span><span class="p">([</span><span class="n">cell_output</span><span class="p">,</span> <span class="n">new_attns</span><span class="p">])</span>
        <span class="n">new_attn_states</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">new_attn_states</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">new_attn_states</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">new_attn_states</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_length</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span><span class="p">])</span>
        <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">new_state</span><span class="p">,</span> <span class="n">new_attns</span><span class="p">,</span> <span class="n">new_attn_states</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_is_tuple</span><span class="p">:</span>
            <span class="n">new_state</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">new_state</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">new_state</span></div>

    <span class="k">def</span> <span class="nf">_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">attn_states</span><span class="p">):</span>
        <span class="n">conv2d</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">conv2d</span>
        <span class="n">reduce_sum</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span>
        <span class="n">softmax</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">softmax</span>
        <span class="n">tanh</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span>

        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;attention&quot;</span><span class="p">):</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;attn_w&quot;</span><span class="p">,</span>
                                <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_vec_size</span><span class="p">])</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;attn_v&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_attn_vec_size</span><span class="p">])</span>
            <span class="n">hidden</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">attn_states</span><span class="p">,</span>
                                       <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span><span class="p">])</span>
            <span class="n">hidden_features</span> <span class="o">=</span> <span class="n">conv2d</span><span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear3</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_linear3</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_vec_size</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear3</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_vec_size</span><span class="p">])</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">v</span> <span class="o">*</span> <span class="n">tanh</span><span class="p">(</span><span class="n">hidden_features</span> <span class="o">+</span> <span class="n">y</span><span class="p">),</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">d</span> <span class="o">=</span> <span class="n">reduce_sum</span><span class="p">(</span>
                <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_length</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="n">hidden</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
            <span class="n">new_attns</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_attn_size</span><span class="p">])</span>
            <span class="n">new_attn_states</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">attn_states</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">new_attns</span><span class="p">,</span> <span class="n">new_attn_states</span></div>


<div class="viewcode-block" id="HighwayWrapper"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.HighwayWrapper">[docs]</a><span class="k">class</span> <span class="nc">HighwayWrapper</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;RNNCell wrapper that adds highway connection on cell input and output.</span>

<span class="sd">    Based on:</span>
<span class="sd">      R. K. Srivastava, K. Greff, and J. Schmidhuber, &quot;Highway networks&quot;,</span>
<span class="sd">      arXiv preprint arXiv:1505.00387, 2015.</span>
<span class="sd">      https://arxiv.org/abs/1505.00387</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">cell</span><span class="p">,</span>
                 <span class="n">couple_carry_transform_gates</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">carry_bias_init</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Constructs a `HighwayWrapper` for `cell`.</span>

<span class="sd">        Args:</span>
<span class="sd">          cell: An instance of `RNNCell`.</span>
<span class="sd">          couple_carry_transform_gates: boolean, should the Carry and Transform gate</span>
<span class="sd">            be coupled.</span>
<span class="sd">          carry_bias_init: float, carry gates bias initialization.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span> <span class="o">=</span> <span class="n">cell</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_couple_carry_transform_gates</span> <span class="o">=</span> <span class="n">couple_carry_transform_gates</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_carry_bias_init</span> <span class="o">=</span> <span class="n">carry_bias_init</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">state_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">output_size</span>

    <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;ZeroState&quot;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">]):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_highway</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>
        <span class="n">carry_weight</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;carry_w&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">input_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">])</span>
        <span class="n">carry_bias</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
            <span class="s2">&quot;carry_b&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">input_size</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_carry_bias_init</span><span class="p">))</span>
        <span class="n">carry</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">nn_ops</span><span class="o">.</span><span class="n">xw_plus_b</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">carry_weight</span><span class="p">,</span> <span class="n">carry_bias</span><span class="p">))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_couple_carry_transform_gates</span><span class="p">:</span>
            <span class="n">transform</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">carry</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">transform_weight</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;transform_w&quot;</span><span class="p">,</span>
                                               <span class="p">[</span><span class="n">input_size</span><span class="p">,</span> <span class="n">input_size</span><span class="p">])</span>
            <span class="n">transform_bias</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="s2">&quot;transform_b&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">input_size</span><span class="p">],</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_carry_bias_init</span><span class="p">))</span>
            <span class="n">transform</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span>
                <span class="n">nn_ops</span><span class="o">.</span><span class="n">xw_plus_b</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">transform_weight</span><span class="p">,</span> <span class="n">transform_bias</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">inp</span> <span class="o">*</span> <span class="n">carry</span> <span class="o">+</span> <span class="n">out</span> <span class="o">*</span> <span class="n">transform</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run the cell and add its inputs to its outputs.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: cell inputs.</span>
<span class="sd">          state: cell state.</span>
<span class="sd">          scope: optional cell scope.</span>

<span class="sd">        Returns:</span>
<span class="sd">          Tuple of cell outputs and new state.</span>

<span class="sd">        Raises:</span>
<span class="sd">          TypeError: If cell inputs and outputs have different structure (type).</span>
<span class="sd">          ValueError: If cell inputs and outputs have different structure (value).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">new_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
        <span class="n">nest</span><span class="o">.</span><span class="n">assert_same_structure</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

        <span class="c1"># Ensure shapes match</span>
        <span class="k">def</span> <span class="nf">assert_shape_match</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
            <span class="n">inp</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">assert_is_compatible_with</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">get_shape</span><span class="p">())</span>

        <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">assert_shape_match</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
        <span class="n">res_outputs</span> <span class="o">=</span> <span class="n">nest</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_highway</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">res_outputs</span><span class="p">,</span> <span class="n">new_state</span><span class="p">)</span></div>


<div class="viewcode-block" id="LayerNormBasicLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.LayerNormBasicLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">LayerNormBasicLSTMCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;LSTM unit with layer normalization and recurrent dropout.</span>

<span class="sd">    This class adds layer normalization and recurrent dropout to a</span>
<span class="sd">    basic LSTM unit. Layer normalization implementation is based on:</span>

<span class="sd">      https://arxiv.org/abs/1607.06450.</span>

<span class="sd">    &quot;Layer Normalization&quot;</span>
<span class="sd">    Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</span>

<span class="sd">    and is applied before the internal nonlinearities.</span>
<span class="sd">    Recurrent dropout is base on:</span>

<span class="sd">      https://arxiv.org/abs/1603.05118</span>

<span class="sd">    &quot;Recurrent Dropout without Memory Loss&quot;</span>
<span class="sd">    Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                 <span class="n">layer_norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">norm_gain</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">norm_shift</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">dropout_keep_prob</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">dropout_prob_seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initializes the basic LSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the LSTM cell.</span>
<span class="sd">          forget_bias: float, The bias added to forget gates (see above).</span>
<span class="sd">          input_size: Deprecated and unused.</span>
<span class="sd">          activation: Activation function of the inner states.</span>
<span class="sd">          layer_norm: If `True`, layer normalization will be applied.</span>
<span class="sd">          norm_gain: float, The layer normalization gain initial value. If</span>
<span class="sd">            `layer_norm` has been set to `False`, this argument will be ignored.</span>
<span class="sd">          norm_shift: float, The layer normalization shift initial value. If</span>
<span class="sd">            `layer_norm` has been set to `False`, this argument will be ignored.</span>
<span class="sd">          dropout_keep_prob: unit Tensor or float between 0 and 1 representing the</span>
<span class="sd">            recurrent dropout probability value. If float and 1.0, no dropout will</span>
<span class="sd">            be applied.</span>
<span class="sd">          dropout_prob_seed: (optional) integer, the randomness seed.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LayerNormBasicLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: The input_size parameter is deprecated.&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_keep_prob</span> <span class="o">=</span> <span class="n">dropout_keep_prob</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_seed</span> <span class="o">=</span> <span class="n">dropout_prob_seed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layer_norm</span> <span class="o">=</span> <span class="n">layer_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span> <span class="o">=</span> <span class="n">norm_gain</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span> <span class="o">=</span> <span class="n">norm_shift</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">reuse</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

    <span class="k">def</span> <span class="nf">_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inp</span><span class="p">,</span> <span class="n">scope</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
        <span class="n">gamma_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span><span class="p">)</span>
        <span class="n">beta_init</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">):</span>
            <span class="c1"># Initialize beta and gamma for use by layer_norm.</span>
            <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">gamma_init</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">beta_init</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">normalized</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">normalized</span>

    <span class="k">def</span> <span class="nf">_linear</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="n">out_size</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>
        <span class="n">proj_size</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;kernel&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">proj_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_norm</span><span class="p">:</span>
            <span class="n">bias</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">out_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>

<div class="viewcode-block" id="LayerNormBasicLSTMCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.LayerNormBasicLSTMCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;LSTM cell with layer normalization and recurrent dropout.&quot;&quot;&quot;</span>
        <span class="n">c</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">state</span>
        <span class="n">args</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">h</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">concat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">dtype</span>

        <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">concat</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_norm</span><span class="p">:</span>
            <span class="n">i</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">j</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="s2">&quot;transform&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">f</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s2">&quot;forget&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
            <span class="n">o</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">g</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">j</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_keep_prob</span><span class="p">,</span> <span class="nb">float</span><span class="p">))</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keep_prob</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">g</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_keep_prob</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_seed</span><span class="p">)</span>

        <span class="n">new_c</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">c</span> <span class="o">*</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span> <span class="o">+</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_norm</span><span class="p">:</span>
            <span class="n">new_c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm</span><span class="p">(</span><span class="n">new_c</span><span class="p">,</span> <span class="s2">&quot;state&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">new_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">new_c</span><span class="p">)</span> <span class="o">*</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>

        <span class="n">new_state</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">new_c</span><span class="p">,</span> <span class="n">new_h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_state</span></div></div>


<div class="viewcode-block" id="NASCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.NASCell">[docs]</a><span class="k">class</span> <span class="nc">NASCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LayerRNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Neural Architecture Search (NAS) recurrent network cell.</span>

<span class="sd">    This implements the recurrent cell from the paper:</span>

<span class="sd">      https://arxiv.org/abs/1611.01578</span>

<span class="sd">    Barret Zoph and Quoc V. Le.</span>
<span class="sd">    &quot;Neural Architecture Search with Reinforcement Learning&quot; Proc. ICLR 2017.</span>

<span class="sd">    The class uses an optional projection layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># NAS cell&#39;s architecture base.</span>
    <span class="n">_NAS_BASE</span> <span class="o">=</span> <span class="mi">8</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">num_proj</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the parameters for a NAS cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the NAS cell.</span>
<span class="sd">          num_proj: (optional) int, The output dimensionality for the projection</span>
<span class="sd">            matrices.  If None, no projection is performed.</span>
<span class="sd">          use_bias: (optional) bool, If True then use biases within the cell. This</span>
<span class="sd">            is False by default.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>
<span class="sd">          **kwargs: Additional keyword arguments.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NASCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="o">=</span> <span class="n">num_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">reuse</span>

        <span class="k">if</span> <span class="n">num_proj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">num_units</span><span class="p">,</span> <span class="n">num_proj</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">num_proj</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">num_units</span><span class="p">,</span> <span class="n">num_units</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">num_units</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">):</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span>
            <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">)</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not infer input size from inputs.get_shape()[-1]&quot;</span><span class="p">)</span>

        <span class="n">num_proj</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span>

        <span class="c1"># Variables for the NAS cell. `recurrent_kernel` is all matrices multiplying</span>
        <span class="c1"># the hiddenstate and `kernel` is all matrices multiplying the inputs.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recurrent_kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;recurrent_kernel&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">num_proj</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_NAS_BASE</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;kernel&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">input_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_NAS_BASE</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">])</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span><span class="s2">&quot;bias&quot;</span><span class="p">,</span>
                                          <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_NAS_BASE</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
                                          <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>

        <span class="c1"># Projection layer if specified</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">projection_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
                <span class="s2">&quot;projection_weights&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

<div class="viewcode-block" id="NASCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.NASCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run one step of NAS Cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: input Tensor, 2D, batch x num_units.</span>
<span class="sd">          state: This must be a tuple of state Tensors, both `2-D`, with column</span>
<span class="sd">            sizes `c_state` and `m_state`.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tuple containing:</span>
<span class="sd">          - A `2-D, [batch x output_dim]`, Tensor representing the output of the</span>
<span class="sd">            NAS Cell after reading `inputs` when previous state was `state`.</span>
<span class="sd">            Here output_dim is:</span>
<span class="sd">               num_proj if num_proj was set,</span>
<span class="sd">               num_units otherwise.</span>
<span class="sd">          - Tensor(s) representing the new state of NAS Cell after reading `inputs`</span>
<span class="sd">            when the previous state was `state`.  Same type and shape(s) as `state`.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: If input size cannot be inferred from inputs via</span>
<span class="sd">            static shape inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sigmoid</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span>
        <span class="n">tanh</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span>
        <span class="n">relu</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">relu</span>

        <span class="p">(</span><span class="n">c_prev</span><span class="p">,</span> <span class="n">m_prev</span><span class="p">)</span> <span class="o">=</span> <span class="n">state</span>

        <span class="n">m_matrix</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">m_prev</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">recurrent_kernel</span><span class="p">)</span>
        <span class="n">inputs_matrix</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">:</span>
            <span class="n">m_matrix</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">m_matrix</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>

        <span class="c1"># The NAS cell branches into 8 different splits for both the hiddenstate</span>
        <span class="c1"># and the input</span>
        <span class="n">m_matrix_splits</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_NAS_BASE</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">m_matrix</span><span class="p">)</span>
        <span class="n">inputs_matrix_splits</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_NAS_BASE</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">inputs_matrix</span><span class="p">)</span>

        <span class="c1"># First layer</span>
        <span class="n">layer1_0</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">inputs_matrix_splits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">m_matrix_splits</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">layer1_1</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">inputs_matrix_splits</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">m_matrix_splits</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">layer1_2</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">inputs_matrix_splits</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">m_matrix_splits</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">layer1_3</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">inputs_matrix_splits</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*</span> <span class="n">m_matrix_splits</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
        <span class="n">layer1_4</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">inputs_matrix_splits</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">+</span> <span class="n">m_matrix_splits</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
        <span class="n">layer1_5</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">inputs_matrix_splits</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span> <span class="o">+</span> <span class="n">m_matrix_splits</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span>
        <span class="n">layer1_6</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">inputs_matrix_splits</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span> <span class="o">+</span> <span class="n">m_matrix_splits</span><span class="p">[</span><span class="mi">6</span><span class="p">])</span>
        <span class="n">layer1_7</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">inputs_matrix_splits</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span> <span class="o">+</span> <span class="n">m_matrix_splits</span><span class="p">[</span><span class="mi">7</span><span class="p">])</span>

        <span class="c1"># Second layer</span>
        <span class="n">l2_0</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">layer1_0</span> <span class="o">*</span> <span class="n">layer1_1</span><span class="p">)</span>
        <span class="n">l2_1</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">layer1_2</span> <span class="o">+</span> <span class="n">layer1_3</span><span class="p">)</span>
        <span class="n">l2_2</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">layer1_4</span> <span class="o">*</span> <span class="n">layer1_5</span><span class="p">)</span>
        <span class="n">l2_3</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">layer1_6</span> <span class="o">+</span> <span class="n">layer1_7</span><span class="p">)</span>

        <span class="c1"># Inject the cell</span>
        <span class="n">l2_0</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">l2_0</span> <span class="o">+</span> <span class="n">c_prev</span><span class="p">)</span>

        <span class="c1"># Third layer</span>
        <span class="n">l3_0_pre</span> <span class="o">=</span> <span class="n">l2_0</span> <span class="o">*</span> <span class="n">l2_1</span>
        <span class="n">new_c</span> <span class="o">=</span> <span class="n">l3_0_pre</span>  <span class="c1"># create new cell</span>
        <span class="n">l3_0</span> <span class="o">=</span> <span class="n">l3_0_pre</span>
        <span class="n">l3_1</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">l2_2</span> <span class="o">+</span> <span class="n">l2_3</span><span class="p">)</span>

        <span class="c1"># Final layer</span>
        <span class="n">new_m</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">l3_0</span> <span class="o">*</span> <span class="n">l3_1</span><span class="p">)</span>

        <span class="c1"># Projection layer if specified</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">new_m</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">new_m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection_weights</span><span class="p">)</span>

        <span class="n">new_state</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">new_c</span><span class="p">,</span> <span class="n">new_m</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_m</span><span class="p">,</span> <span class="n">new_state</span></div></div>


<div class="viewcode-block" id="UGRNNCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.UGRNNCell">[docs]</a><span class="k">class</span> <span class="nc">UGRNNCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Update Gate Recurrent Neural Network (UGRNN) cell.</span>

<span class="sd">    Compromise between a LSTM/GRU and a vanilla RNN.  There is only one</span>
<span class="sd">    gate, and that is to determine whether the unit should be</span>
<span class="sd">    integrating or computing instantaneously.  This is the recurrent</span>
<span class="sd">    idea of the feedforward Highway Network.</span>

<span class="sd">    This implements the recurrent cell from the paper:</span>

<span class="sd">      https://arxiv.org/abs/1611.09913</span>

<span class="sd">    Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo.</span>
<span class="sd">    &quot;Capacity and Trainability in Recurrent Neural Networks&quot; Proc. ICLR 2017.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the parameters for an UGRNN cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the UGRNN cell</span>
<span class="sd">          initializer: (optional) The initializer to use for the weight matrices.</span>
<span class="sd">          forget_bias: (optional) float, default 1.0, The initial bias of the</span>
<span class="sd">            forget gate, used to reduce the scale of forgetting at the beginning</span>
<span class="sd">            of the training.</span>
<span class="sd">          activation: (optional) Activation function of the inner states.</span>
<span class="sd">            Default is `tf.tanh`.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">UGRNNCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="o">=</span> <span class="n">initializer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">reuse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_linear</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

<div class="viewcode-block" id="UGRNNCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.UGRNNCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run one step of UGRNN.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: input Tensor, 2D, batch x input size.</span>
<span class="sd">          state: state Tensor, 2D, batch x num units.</span>

<span class="sd">        Returns:</span>
<span class="sd">          new_output: batch x num units, Tensor representing the output of the UGRNN</span>
<span class="sd">            after reading `inputs` when previous state was `state`. Identical to</span>
<span class="sd">            `new_state`.</span>
<span class="sd">          new_state: batch x num units, Tensor representing the state of the UGRNN</span>
<span class="sd">            after reading `inputs` when previous state was `state`.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: If input size cannot be inferred from inputs via</span>
<span class="sd">            static shape inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sigmoid</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span>

        <span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">input_size</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not infer input size from inputs.get_shape()[-1]&quot;</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span>
                <span class="n">vs</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">(),</span> <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span><span class="p">):</span>
            <span class="n">cell_inputs</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_linear</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">cell_inputs</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">rnn_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear</span><span class="p">(</span><span class="n">cell_inputs</span><span class="p">)</span>

            <span class="p">[</span><span class="n">g_act</span><span class="p">,</span> <span class="n">c_act</span><span class="p">]</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">rnn_matrix</span><span class="p">)</span>

            <span class="n">c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">c_act</span><span class="p">)</span>
            <span class="n">g</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">g_act</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span>
            <span class="n">new_state</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="n">state</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span>
            <span class="n">new_output</span> <span class="o">=</span> <span class="n">new_state</span>

        <span class="k">return</span> <span class="n">new_output</span><span class="p">,</span> <span class="n">new_state</span></div></div>


<div class="viewcode-block" id="IntersectionRNNCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.IntersectionRNNCell">[docs]</a><span class="k">class</span> <span class="nc">IntersectionRNNCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Intersection Recurrent Neural Network (+RNN) cell.</span>

<span class="sd">    Architecture with coupled recurrent gate as well as coupled depth</span>
<span class="sd">    gate, designed to improve information flow through stacked RNNs. As the</span>
<span class="sd">    architecture uses depth gating, the dimensionality of the depth</span>
<span class="sd">    output (y) also should not change through depth (input size == output size).</span>
<span class="sd">    To achieve this, the first layer of a stacked Intersection RNN projects</span>
<span class="sd">    the inputs to N (num units) dimensions. Therefore when initializing an</span>
<span class="sd">    IntersectionRNNCell, one should set `num_in_proj = N` for the first layer</span>
<span class="sd">    and use default settings for subsequent layers.</span>

<span class="sd">    This implements the recurrent cell from the paper:</span>

<span class="sd">      https://arxiv.org/abs/1611.09913</span>

<span class="sd">    Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo.</span>
<span class="sd">    &quot;Capacity and Trainability in Recurrent Neural Networks&quot; Proc. ICLR 2017.</span>

<span class="sd">    The Intersection RNN is built for use in deeply stacked</span>
<span class="sd">    RNNs so it may not achieve best performance with depth 1.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">num_in_proj</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">y_activation</span><span class="o">=</span><span class="n">nn_ops</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the parameters for an +RNN cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the +RNN cell</span>
<span class="sd">          num_in_proj: (optional) int, The input dimensionality for the RNN.</span>
<span class="sd">            If creating the first layer of an +RNN, this should be set to</span>
<span class="sd">            `num_units`. Otherwise, this should be set to `None` (default).</span>
<span class="sd">            If `None`, dimensionality of `inputs` should be equal to `num_units`,</span>
<span class="sd">            otherwise ValueError is thrown.</span>
<span class="sd">          initializer: (optional) The initializer to use for the weight matrices.</span>
<span class="sd">          forget_bias: (optional) float, default 1.0, The initial bias of the</span>
<span class="sd">            forget gates, used to reduce the scale of forgetting at the beginning</span>
<span class="sd">            of the training.</span>
<span class="sd">          y_activation: (optional) Activation function of the states passed</span>
<span class="sd">            through depth. Default is &#39;tf.nn.relu`.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">IntersectionRNNCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="o">=</span> <span class="n">initializer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_input_proj</span> <span class="o">=</span> <span class="n">num_in_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_y_activation</span> <span class="o">=</span> <span class="n">y_activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">reuse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

<div class="viewcode-block" id="IntersectionRNNCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.IntersectionRNNCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run one step of the Intersection RNN.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: input Tensor, 2D, batch x input size.</span>
<span class="sd">          state: state Tensor, 2D, batch x num units.</span>

<span class="sd">        Returns:</span>
<span class="sd">          new_y: batch x num units, Tensor representing the output of the +RNN</span>
<span class="sd">            after reading `inputs` when previous state was `state`.</span>
<span class="sd">          new_state: batch x num units, Tensor representing the state of the +RNN</span>
<span class="sd">            after reading `inputs` when previous state was `state`.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: If input size cannot be inferred from `inputs` via</span>
<span class="sd">            static shape inference.</span>
<span class="sd">          ValueError: If input size != output size (these must be equal when</span>
<span class="sd">            using the Intersection RNN).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sigmoid</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span>
        <span class="n">tanh</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span>

        <span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">input_size</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not infer input size from inputs.get_shape()[-1]&quot;</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span>
                <span class="n">vs</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">(),</span> <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span><span class="p">):</span>
            <span class="c1"># read-in projections (should be used for first layer in deep +RNN</span>
            <span class="c1"># to transform size of inputs from I --&gt; N)</span>
            <span class="k">if</span> <span class="n">input_size</span><span class="o">.</span><span class="n">value</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_input_proj</span><span class="p">:</span>
                    <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;in_projection&quot;</span><span class="p">):</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
                        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must have input size == output size for &quot;</span>
                                     <span class="s2">&quot;Intersection RNN. To fix, num_in_proj should &quot;</span>
                                     <span class="s2">&quot;be set to num_units at cell init.&quot;</span><span class="p">)</span>

            <span class="n">n_dim</span> <span class="o">=</span> <span class="n">i_dim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>
            <span class="n">cell_inputs</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">cell_inputs</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_dim</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i_dim</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">rnn_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span><span class="p">(</span><span class="n">cell_inputs</span><span class="p">)</span>

            <span class="n">gh_act</span> <span class="o">=</span> <span class="n">rnn_matrix</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_dim</span><span class="p">]</span>  <span class="c1"># b x n</span>
            <span class="n">h_act</span> <span class="o">=</span> <span class="n">rnn_matrix</span><span class="p">[:,</span> <span class="n">n_dim</span><span class="p">:</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_dim</span><span class="p">]</span>  <span class="c1"># b x n</span>
            <span class="n">gy_act</span> <span class="o">=</span> <span class="n">rnn_matrix</span><span class="p">[:,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_dim</span><span class="p">:</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_dim</span> <span class="o">+</span> <span class="n">i_dim</span><span class="p">]</span>  <span class="c1"># b x i</span>
            <span class="n">y_act</span> <span class="o">=</span> <span class="n">rnn_matrix</span><span class="p">[:,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">n_dim</span> <span class="o">+</span> <span class="n">i_dim</span><span class="p">:</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_dim</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">i_dim</span><span class="p">]</span>  <span class="c1"># b x i</span>

            <span class="n">h</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">h_act</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_y_activation</span><span class="p">(</span><span class="n">y_act</span><span class="p">)</span>
            <span class="n">gh</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">gh_act</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span>
            <span class="n">gy</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">gy_act</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span>

            <span class="n">new_state</span> <span class="o">=</span> <span class="n">gh</span> <span class="o">*</span> <span class="n">state</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">gh</span><span class="p">)</span> <span class="o">*</span> <span class="n">h</span>  <span class="c1"># passed thru time</span>
            <span class="n">new_y</span> <span class="o">=</span> <span class="n">gy</span> <span class="o">*</span> <span class="n">inputs</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">gy</span><span class="p">)</span> <span class="o">*</span> <span class="n">y</span>  <span class="c1"># passed thru depth</span>

        <span class="k">return</span> <span class="n">new_y</span><span class="p">,</span> <span class="n">new_state</span></div></div>


<span class="n">_REGISTERED_OPS</span> <span class="o">=</span> <span class="kc">None</span>


<div class="viewcode-block" id="CompiledWrapper"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.CompiledWrapper">[docs]</a><span class="k">class</span> <span class="nc">CompiledWrapper</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wraps step execution in an XLA JIT scope.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell</span><span class="p">,</span> <span class="n">compile_stateful</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Create CompiledWrapper cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          cell: Instance of `RNNCell`.</span>
<span class="sd">          compile_stateful: Whether to compile stateful ops like initializers</span>
<span class="sd">            and random number generators (default: False).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span> <span class="o">=</span> <span class="n">cell</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_compile_stateful</span> <span class="o">=</span> <span class="n">compile_stateful</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">state_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">output_size</span>

    <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">name_scope</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;ZeroState&quot;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">]):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compile_stateful</span><span class="p">:</span>
            <span class="n">compile_ops</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>

            <span class="k">def</span> <span class="nf">compile_ops</span><span class="p">(</span><span class="n">node_def</span><span class="p">):</span>
                <span class="k">global</span> <span class="n">_REGISTERED_OPS</span>
                <span class="k">if</span> <span class="n">_REGISTERED_OPS</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">_REGISTERED_OPS</span> <span class="o">=</span> <span class="n">op_def_registry</span><span class="o">.</span><span class="n">get_registered_ops</span><span class="p">()</span>
                <span class="k">return</span> <span class="ow">not</span> <span class="n">_REGISTERED_OPS</span><span class="p">[</span><span class="n">node_def</span><span class="o">.</span><span class="n">op</span><span class="p">]</span><span class="o">.</span><span class="n">is_stateful</span>

        <span class="k">with</span> <span class="n">jit</span><span class="o">.</span><span class="n">experimental_jit_scope</span><span class="p">(</span><span class="n">compile_ops</span><span class="o">=</span><span class="n">compile_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_random_exp_initializer</span><span class="p">(</span><span class="n">minval</span><span class="p">,</span> <span class="n">maxval</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns an exponential distribution initializer.</span>

<span class="sd">    Args:</span>
<span class="sd">      minval: float or a scalar float Tensor. With value &gt; 0. Lower bound of the</span>
<span class="sd">          range of random values to generate.</span>
<span class="sd">      maxval: float or a scalar float Tensor. With value &gt; minval. Upper bound of</span>
<span class="sd">          the range of random values to generate.</span>
<span class="sd">      seed: An integer. Used to create random seeds.</span>
<span class="sd">      dtype: The data type.</span>

<span class="sd">    Returns:</span>
<span class="sd">      An initializer that generates tensors with an exponential distribution.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_initializer</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">partition_info</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">del</span> <span class="n">partition_info</span>  <span class="c1"># Unused.</span>
        <span class="k">return</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
            <span class="n">random_ops</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">(</span>
                <span class="n">shape</span><span class="p">,</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">minval</span><span class="p">),</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">maxval</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span>
                <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">_initializer</span>


<div class="viewcode-block" id="PhasedLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.PhasedLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">PhasedLSTMCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Phased LSTM recurrent network cell.</span>

<span class="sd">    https://arxiv.org/pdf/1610.09513v1.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">use_peepholes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">leak</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span>
                 <span class="n">ratio_on</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">trainable_ratio_on</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">period_init_min</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">period_init_max</span><span class="o">=</span><span class="mf">1000.0</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the Phased LSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the Phased LSTM cell.</span>
<span class="sd">          use_peepholes: bool, set True to enable peephole connections.</span>
<span class="sd">          leak: float or scalar float Tensor with value in [0, 1]. Leak applied</span>
<span class="sd">              during training.</span>
<span class="sd">          ratio_on: float or scalar float Tensor with value in [0, 1]. Ratio of the</span>
<span class="sd">              period during which the gates are open.</span>
<span class="sd">          trainable_ratio_on: bool, weather ratio_on is trainable.</span>
<span class="sd">          period_init_min: float or scalar float Tensor. With value &gt; 0.</span>
<span class="sd">              Minimum value of the initialized period.</span>
<span class="sd">              The period values are initialized by drawing from the distribution:</span>
<span class="sd">              e^U(log(period_init_min), log(period_init_max))</span>
<span class="sd">              Where U(.,.) is the uniform distribution.</span>
<span class="sd">          period_init_max: float or scalar float Tensor.</span>
<span class="sd">              With value &gt; period_init_min. Maximum value of the initialized period.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope. If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># We pass autocast=False because this layer can accept inputs of different</span>
        <span class="c1"># dtypes, so we do not want to automatically cast them to the same dtype.</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PhasedLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span> <span class="n">autocast</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span> <span class="o">=</span> <span class="n">use_peepholes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_leak</span> <span class="o">=</span> <span class="n">leak</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_ratio_on</span> <span class="o">=</span> <span class="n">ratio_on</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_trainable_ratio_on</span> <span class="o">=</span> <span class="n">trainable_ratio_on</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_period_init_min</span> <span class="o">=</span> <span class="n">period_init_min</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_period_init_max</span> <span class="o">=</span> <span class="n">period_init_max</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_reuse</span> <span class="o">=</span> <span class="n">reuse</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_linear3</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

    <span class="k">def</span> <span class="nf">_mod</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Modulo function that propagates x gradients.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stop_gradient</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">mod</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">_get_cycle_ratio</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">phase</span><span class="p">,</span> <span class="n">period</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Compute the cycle ratio in the dtype of the time.&quot;&quot;&quot;</span>
        <span class="n">phase_casted</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">phase</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">time</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">period_casted</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">period</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">time</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">shifted_time</span> <span class="o">=</span> <span class="n">time</span> <span class="o">-</span> <span class="n">phase_casted</span>
        <span class="n">cycle_ratio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_mod</span><span class="p">(</span><span class="n">shifted_time</span><span class="p">,</span> <span class="n">period_casted</span><span class="p">)</span> <span class="o">/</span> <span class="n">period_casted</span>
        <span class="k">return</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">cycle_ratio</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<div class="viewcode-block" id="PhasedLSTMCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.PhasedLSTMCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Phased LSTM Cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: A tuple of 2 Tensor.</span>
<span class="sd">             The first Tensor has shape [batch, 1], and type float32 or float64.</span>
<span class="sd">             It stores the time.</span>
<span class="sd">             The second Tensor has shape [batch, features_size], and type float32.</span>
<span class="sd">             It stores the features.</span>
<span class="sd">          state: rnn_cell_impl.LSTMStateTuple, state from previous timestep.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tuple containing:</span>
<span class="sd">          - A Tensor of float32, and shape [batch_size, num_units], representing the</span>
<span class="sd">            output of the cell.</span>
<span class="sd">          - A rnn_cell_impl.LSTMStateTuple, containing 2 Tensors of float32, shape</span>
<span class="sd">            [batch_size, num_units], representing the new state and the output.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="p">(</span><span class="n">c_prev</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">)</span> <span class="o">=</span> <span class="n">state</span>
        <span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">inputs</span>

        <span class="n">in_mask_gates</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
            <span class="n">in_mask_gates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c_prev</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;mask_gates&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">in_mask_gates</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

            <span class="n">mask_gates</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span><span class="p">(</span><span class="n">in_mask_gates</span><span class="p">))</span>
            <span class="p">[</span><span class="n">input_gate</span><span class="p">,</span> <span class="n">forget_gate</span><span class="p">]</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">mask_gates</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;new_input&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">new_input</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">]))</span>

        <span class="n">new_c</span> <span class="o">=</span> <span class="p">(</span><span class="n">c_prev</span> <span class="o">*</span> <span class="n">forget_gate</span> <span class="o">+</span> <span class="n">input_gate</span> <span class="o">*</span> <span class="n">new_input</span><span class="p">)</span>

        <span class="n">in_out_gate</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">h_prev</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
            <span class="n">in_out_gate</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_c</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;output_gate&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear3</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_linear3</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">in_out_gate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
            <span class="n">output_gate</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_linear3</span><span class="p">(</span><span class="n">in_out_gate</span><span class="p">))</span>

        <span class="n">new_h</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">new_c</span><span class="p">)</span> <span class="o">*</span> <span class="n">output_gate</span>

        <span class="n">period</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
            <span class="s2">&quot;period&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">_random_exp_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_period_init_min</span><span class="p">,</span>
                                                <span class="bp">self</span><span class="o">.</span><span class="n">_period_init_max</span><span class="p">))</span>
        <span class="n">phase</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
            <span class="s2">&quot;phase&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span><span class="mf">0.</span><span class="p">,</span>
                                                            <span class="n">period</span><span class="o">.</span><span class="n">initial_value</span><span class="p">))</span>
        <span class="n">ratio_on</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
            <span class="s2">&quot;ratio_on&quot;</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_ratio_on</span><span class="p">),</span>
            <span class="n">trainable</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_trainable_ratio_on</span><span class="p">)</span>

        <span class="n">cycle_ratio</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_cycle_ratio</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">phase</span><span class="p">,</span> <span class="n">period</span><span class="p">)</span>

        <span class="n">k_up</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">cycle_ratio</span> <span class="o">/</span> <span class="n">ratio_on</span>
        <span class="n">k_down</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">k_up</span>
        <span class="n">k_closed</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_leak</span> <span class="o">*</span> <span class="n">cycle_ratio</span>

        <span class="n">k</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">cycle_ratio</span> <span class="o">&lt;</span> <span class="n">ratio_on</span><span class="p">,</span> <span class="n">k_down</span><span class="p">,</span> <span class="n">k_closed</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">cycle_ratio</span> <span class="o">&lt;</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">ratio_on</span><span class="p">,</span> <span class="n">k_up</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>

        <span class="n">new_c</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">new_c</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="n">c_prev</span>
        <span class="n">new_h</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">new_h</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="n">h_prev</span>

        <span class="n">new_state</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">new_c</span><span class="p">,</span> <span class="n">new_h</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_state</span></div></div>


<div class="viewcode-block" id="ConvLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.ConvLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">ConvLSTMCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convolutional LSTM recurrent network cell.</span>

<span class="sd">    https://arxiv.org/pdf/1506.04214v1.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">conv_ndims</span><span class="p">,</span>
                 <span class="n">input_shape</span><span class="p">,</span>
                 <span class="n">output_channels</span><span class="p">,</span>
                 <span class="n">kernel_shape</span><span class="p">,</span>
                 <span class="n">use_bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">skip_connection</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">initializers</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_lstm_cell&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct ConvLSTMCell.</span>

<span class="sd">        Args:</span>
<span class="sd">          conv_ndims: Convolution dimensionality (1, 2 or 3).</span>
<span class="sd">          input_shape: Shape of the input as int tuple, excluding the batch size.</span>
<span class="sd">          output_channels: int, number of output channels of the conv LSTM.</span>
<span class="sd">          kernel_shape: Shape of kernel as an int tuple (of size 1, 2 or 3).</span>
<span class="sd">          use_bias: (bool) Use bias in convolutions.</span>
<span class="sd">          skip_connection: If set to `True`, concatenate the input to the</span>
<span class="sd">            output of the conv LSTM. Default: `False`.</span>
<span class="sd">          forget_bias: Forget bias.</span>
<span class="sd">          initializers: Unused.</span>
<span class="sd">          name: Name of the module.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: If `skip_connection` is `True` and stride is different from 1</span>
<span class="sd">            or if `input_shape` is incompatible with `conv_ndims`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ConvLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">conv_ndims</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid input_shape </span><span class="si">{}</span><span class="s2"> for conv_ndims=</span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">input_shape</span><span class="p">,</span> <span class="n">conv_ndims</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_conv_ndims</span> <span class="o">=</span> <span class="n">conv_ndims</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="n">input_shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">kernel_shape</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span> <span class="o">=</span> <span class="n">use_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_skip_connection</span> <span class="o">=</span> <span class="n">skip_connection</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_total_output_channels</span> <span class="o">=</span> <span class="n">output_channels</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_skip_connection</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_total_output_channels</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">state_size</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">state_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_total_output_channels</span><span class="p">])</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">cell</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">state</span>
        <span class="n">new_hidden</span> <span class="o">=</span> <span class="n">_conv</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">hidden</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_shape</span><span class="p">,</span>
                           <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_channels</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_bias</span><span class="p">)</span>
        <span class="n">gates</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">new_hidden</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_conv_ndims</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">input_gate</span><span class="p">,</span> <span class="n">new_input</span><span class="p">,</span> <span class="n">forget_gate</span><span class="p">,</span> <span class="n">output_gate</span> <span class="o">=</span> <span class="n">gates</span>
        <span class="n">new_cell</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">forget_gate</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span> <span class="o">*</span> <span class="n">cell</span>
        <span class="n">new_cell</span> <span class="o">+=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">input_gate</span><span class="p">)</span> <span class="o">*</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">new_input</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">new_cell</span><span class="p">)</span> <span class="o">*</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">output_gate</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_skip_connection</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">output</span><span class="p">,</span> <span class="n">inputs</span><span class="p">],</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">new_state</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">new_cell</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">new_state</span></div>


<div class="viewcode-block" id="Conv1DLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.Conv1DLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">Conv1DLSTMCell</span><span class="p">(</span><span class="n">ConvLSTMCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;1D Convolutional LSTM recurrent network cell.</span>

<span class="sd">    https://arxiv.org/pdf/1506.04214v1.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_1d_lstm_cell&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct Conv1DLSTM. See `ConvLSTMCell` for more details.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv1DLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">conv_ndims</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv2DLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.Conv2DLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">Conv2DLSTMCell</span><span class="p">(</span><span class="n">ConvLSTMCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;2D Convolutional LSTM recurrent network cell.</span>

<span class="sd">    https://arxiv.org/pdf/1506.04214v1.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_2d_lstm_cell&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct Conv2DLSTM. See `ConvLSTMCell` for more details.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2DLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">conv_ndims</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<div class="viewcode-block" id="Conv3DLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.Conv3DLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">Conv3DLSTMCell</span><span class="p">(</span><span class="n">ConvLSTMCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;3D Convolutional LSTM recurrent network cell.</span>

<span class="sd">    https://arxiv.org/pdf/1506.04214v1.pdf</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;conv_3d_lstm_cell&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct Conv3DLSTM. See `ConvLSTMCell` for more details.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv3DLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">conv_ndims</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_conv</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="n">filter_size</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">bias_start</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convolution.</span>

<span class="sd">    Args:</span>
<span class="sd">      args: a Tensor or a list of Tensors of dimension 3D, 4D or 5D,</span>
<span class="sd">      batch x n, Tensors.</span>
<span class="sd">      filter_size: int tuple of filter shape (of size 1, 2 or 3).</span>
<span class="sd">      num_features: int, number of features.</span>
<span class="sd">      bias: Whether to use biases in the convolution layer.</span>
<span class="sd">      bias_start: starting value to initialize the bias; 0 by default.</span>

<span class="sd">    Returns:</span>
<span class="sd">      A 3D, 4D, or 5D Tensor with shape [batch ... num_features]</span>

<span class="sd">    Raises:</span>
<span class="sd">      ValueError: if some of the arguments has unspecified or wrong shape.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Calculate the total size of arguments on dimension 1.</span>
    <span class="n">total_arg_size_depth</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">args</span><span class="p">]</span>
    <span class="n">shape_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">shape</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Conv Linear expects 3D, 4D &quot;</span>
                             <span class="s2">&quot;or 5D arguments: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">shapes</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapes</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Conv Linear expects all args &quot;</span>
                             <span class="s2">&quot;to be of same Dimension: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">shapes</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">total_arg_size_depth</span> <span class="o">+=</span> <span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">args</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># determine correct conv operation</span>
    <span class="k">if</span> <span class="n">shape_length</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">conv_op</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">conv1d</span>
        <span class="n">strides</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">shape_length</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">conv_op</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">conv2d</span>
        <span class="n">strides</span> <span class="o">=</span> <span class="n">shape_length</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">shape_length</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">conv_op</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">conv3d</span>
        <span class="n">strides</span> <span class="o">=</span> <span class="n">shape_length</span> <span class="o">*</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Now the computation.</span>
    <span class="n">kernel</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
        <span class="s2">&quot;kernel&quot;</span><span class="p">,</span> <span class="n">filter_size</span> <span class="o">+</span> <span class="p">[</span><span class="n">total_arg_size_depth</span><span class="p">,</span> <span class="n">num_features</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">conv_op</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">strides</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">conv_op</span><span class="p">(</span>
            <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">shape_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">args</span><span class="p">),</span>
            <span class="n">kernel</span><span class="p">,</span>
            <span class="n">strides</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;SAME&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">bias</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">res</span>
    <span class="n">bias_term</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
        <span class="s2">&quot;biases&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">num_features</span><span class="p">],</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="n">bias_start</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">res</span> <span class="o">+</span> <span class="n">bias_term</span>


<div class="viewcode-block" id="GLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.GLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">GLSTMCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Group LSTM cell (G-LSTM).</span>

<span class="sd">    The implementation is based on:</span>

<span class="sd">      https://arxiv.org/abs/1703.10722</span>

<span class="sd">    O. Kuchaiev and B. Ginsburg</span>
<span class="sd">    &quot;Factorization Tricks for LSTM Networks&quot;, ICLR 2017 workshop.</span>

<span class="sd">    In brief, a G-LSTM cell consists of one LSTM sub-cell per group, where each</span>
<span class="sd">    sub-cell operates on an evenly-sized sub-vector of the input and produces an</span>
<span class="sd">    evenly-sized sub-vector of the output.  For example, a G-LSTM cell with 128</span>
<span class="sd">    units and 4 groups consists of 4 LSTMs sub-cells with 32 units each.  If that</span>
<span class="sd">    G-LSTM cell is fed a 200-dim input, then each sub-cell receives a 50-dim part</span>
<span class="sd">    of the input and produces a 32-dim part of the output.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_proj</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">number_of_groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the parameters of G-LSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the G-LSTM cell</span>
<span class="sd">          initializer: (optional) The initializer to use for the weight and</span>
<span class="sd">            projection matrices.</span>
<span class="sd">          num_proj: (optional) int, The output dimensionality for the projection</span>
<span class="sd">            matrices.  If None, no projection is performed.</span>
<span class="sd">          number_of_groups: (optional) int, number of groups to use.</span>
<span class="sd">            If `number_of_groups` is 1, then it should be equivalent to LSTM cell</span>
<span class="sd">          forget_bias: Biases of the forget gate are initialized by default to 1</span>
<span class="sd">            in order to reduce the scale of forgetting at the beginning of</span>
<span class="sd">            the training.</span>
<span class="sd">          activation: Activation function of the inner states.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already</span>
<span class="sd">            has the given variables, an error is raised.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: If `num_units` or `num_proj` is not divisible by</span>
<span class="sd">            `number_of_groups`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="o">=</span> <span class="n">initializer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="o">=</span> <span class="n">num_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_groups</span> <span class="o">=</span> <span class="n">number_of_groups</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;num_units must be divisible by number_of_groups&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;num_proj must be divisible by number_of_groups&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_group_shape</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_groups</span><span class="p">),</span>
                <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_groups</span><span class="p">)</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_group_shape</span> <span class="o">=</span> <span class="p">[</span>
                <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_groups</span><span class="p">),</span>
                <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_groups</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="n">num_proj</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">num_units</span><span class="p">,</span> <span class="n">num_proj</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">num_proj</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">num_units</span><span class="p">,</span> <span class="n">num_units</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">number_of_groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span>

    <span class="k">def</span> <span class="nf">_get_input_for_group</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">group_id</span><span class="p">,</span> <span class="n">group_size</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Slices inputs into groups to prepare for processing by cell&#39;s groups.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: cell input or it&#39;s previous state,</span>
<span class="sd">                  a Tensor, 2D, [batch x num_units]</span>
<span class="sd">          group_id: group id, a Scalar, for which to prepare input</span>
<span class="sd">          group_size: size of the group</span>

<span class="sd">        Returns:</span>
<span class="sd">          subset of inputs corresponding to group &quot;group_id&quot;,</span>
<span class="sd">          a Tensor, 2D, [batch x num_units/number_of_groups]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span>
            <span class="n">input_</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">begin</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">group_id</span> <span class="o">*</span> <span class="n">group_size</span><span class="p">],</span>
            <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span><span class="p">,</span> <span class="n">group_size</span><span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;GLSTM_group</span><span class="si">%d</span><span class="s2">_input_generation&quot;</span> <span class="o">%</span> <span class="n">group_id</span><span class="p">))</span>

<div class="viewcode-block" id="GLSTMCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.GLSTMCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run one step of G-LSTM.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: input Tensor, 2D, [batch x num_inputs].  num_inputs must be</span>
<span class="sd">            statically-known and evenly divisible into groups.  The innermost</span>
<span class="sd">            vectors of the inputs are split into evenly-sized sub-vectors and fed</span>
<span class="sd">            into the per-group LSTM sub-cells.</span>
<span class="sd">          state: this must be a tuple of state Tensors, both `2-D`, with column</span>
<span class="sd">            sizes `c_state` and `m_state`.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tuple containing:</span>

<span class="sd">          - A `2-D, [batch x output_dim]`, Tensor representing the output of the</span>
<span class="sd">            G-LSTM after reading `inputs` when previous state was `state`.</span>
<span class="sd">            Here output_dim is:</span>
<span class="sd">               num_proj if num_proj was set,</span>
<span class="sd">               num_units otherwise.</span>
<span class="sd">          - LSTMStateTuple representing the new state of G-LSTM cell</span>
<span class="sd">            after reading `inputs` when the previous state was `state`.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: If input size cannot be inferred from inputs via</span>
<span class="sd">            static shape inference, or if the input shape is incompatible</span>
<span class="sd">            with the number of groups.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="p">(</span><span class="n">c_prev</span><span class="p">,</span> <span class="n">m_prev</span><span class="p">)</span> <span class="o">=</span> <span class="n">state</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_batch_size</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span>
            <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">or</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># If the input size is statically-known, calculate and validate its group</span>
        <span class="c1"># size.  Otherwise, use the output group size.</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input size must be statically known&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">input_size</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;input size (</span><span class="si">%d</span><span class="s2">) must be divisible by number_of_groups (</span><span class="si">%d</span><span class="s2">)&quot;</span> <span class="o">%</span>
                <span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_groups</span><span class="p">))</span>
        <span class="n">input_group_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">input_size</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">_number_of_groups</span><span class="p">)</span>

        <span class="n">dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">scope</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span><span class="p">):</span>
            <span class="n">i_parts</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">j_parts</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">f_parts</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">o_parts</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">group_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_number_of_groups</span><span class="p">):</span>
                <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;group</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">group_id</span><span class="p">):</span>
                    <span class="n">x_g_id</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
                        <span class="p">[</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_get_input_for_group</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">group_id</span><span class="p">,</span> <span class="n">input_group_size</span><span class="p">),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_get_input_for_group</span><span class="p">(</span><span class="n">m_prev</span><span class="p">,</span> <span class="n">group_id</span><span class="p">,</span>
                                                      <span class="bp">self</span><span class="o">.</span><span class="n">_group_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                        <span class="p">],</span>
                        <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">linear</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span><span class="p">[</span><span class="n">group_id</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">linear</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="n">linear</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">x_g_id</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_group_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="kc">False</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_linear1</span><span class="p">[</span><span class="n">group_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">linear</span>
                    <span class="n">R_k</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">x_g_id</span><span class="p">)</span>  <span class="c1"># pylint: disable=invalid-name</span>
                    <span class="n">i_k</span><span class="p">,</span> <span class="n">j_k</span><span class="p">,</span> <span class="n">f_k</span><span class="p">,</span> <span class="n">o_k</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">R_k</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                <span class="n">i_parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i_k</span><span class="p">)</span>
                <span class="n">j_parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">j_k</span><span class="p">)</span>
                <span class="n">f_parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">f_k</span><span class="p">)</span>
                <span class="n">o_parts</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">o_k</span><span class="p">)</span>

            <span class="n">bi</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bias_i&quot;</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
            <span class="n">bj</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bias_j&quot;</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
            <span class="n">bf</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bias_f&quot;</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>
            <span class="n">bo</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="n">name</span><span class="o">=</span><span class="s2">&quot;bias_o&quot;</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">))</span>

            <span class="n">i</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">i_parts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">bi</span><span class="p">)</span>
            <span class="n">j</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">j_parts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">bj</span><span class="p">)</span>
            <span class="n">f</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">f_parts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">bf</span><span class="p">)</span>
            <span class="n">o</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">o_parts</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">bo</span><span class="p">)</span>

        <span class="n">c</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span>
                <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;projection&quot;</span><span class="p">):</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span> <span class="o">=</span> <span class="n">_Linear</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>
                <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear2</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

        <span class="n">new_state</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">new_state</span></div></div>


<div class="viewcode-block" id="LayerNormLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.LayerNormLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">LayerNormLSTMCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Long short-term memory unit (LSTM) recurrent network cell.</span>

<span class="sd">    The default non-peephole implementation is based on:</span>

<span class="sd">      https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf</span>

<span class="sd">    Felix Gers, Jurgen Schmidhuber, and Fred Cummins.</span>
<span class="sd">    &quot;Learning to forget: Continual prediction with LSTM.&quot; IET, 850-855, 1999.</span>

<span class="sd">    The peephole implementation is based on:</span>

<span class="sd">      https://research.google.com/pubs/archive/43905.pdf</span>

<span class="sd">    Hasim Sak, Andrew Senior, and Francoise Beaufays.</span>
<span class="sd">    &quot;Long short-term memory recurrent neural network architectures for</span>
<span class="sd">     large scale acoustic modeling.&quot; INTERSPEECH, 2014.</span>

<span class="sd">    The class uses optional peep-hole connections, optional cell clipping, and</span>
<span class="sd">    an optional projection layer.</span>

<span class="sd">    Layer normalization implementation is based on:</span>

<span class="sd">      https://arxiv.org/abs/1607.06450.</span>

<span class="sd">    &quot;Layer Normalization&quot;</span>
<span class="sd">    Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</span>

<span class="sd">    and is applied before the internal nonlinearities.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">use_peepholes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">cell_clip</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_proj</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">proj_clip</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">layer_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">norm_gain</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">norm_shift</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the parameters for an LSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the LSTM cell</span>
<span class="sd">          use_peepholes: bool, set True to enable diagonal/peephole connections.</span>
<span class="sd">          cell_clip: (optional) A float value, if provided the cell state is clipped</span>
<span class="sd">            by this value prior to the cell output activation.</span>
<span class="sd">          initializer: (optional) The initializer to use for the weight and</span>
<span class="sd">            projection matrices.</span>
<span class="sd">          num_proj: (optional) int, The output dimensionality for the projection</span>
<span class="sd">            matrices.  If None, no projection is performed.</span>
<span class="sd">          proj_clip: (optional) A float value.  If `num_proj &gt; 0` and `proj_clip` is</span>
<span class="sd">            provided, then the projected values are clipped elementwise to within</span>
<span class="sd">            `[-proj_clip, proj_clip]`.</span>
<span class="sd">          forget_bias: Biases of the forget gate are initialized by default to 1</span>
<span class="sd">            in order to reduce the scale of forgetting at the beginning of</span>
<span class="sd">            the training. Must set it manually to `0.0` when restoring from</span>
<span class="sd">            CudnnLSTM trained checkpoints.</span>
<span class="sd">          activation: Activation function of the inner states.  Default: `tanh`.</span>
<span class="sd">          layer_norm: If `True`, layer normalization will be applied.</span>
<span class="sd">          norm_gain: float, The layer normalization gain initial value. If</span>
<span class="sd">            `layer_norm` has been set to `False`, this argument will be ignored.</span>
<span class="sd">          norm_shift: float, The layer normalization shift initial value. If</span>
<span class="sd">            `layer_norm` has been set to `False`, this argument will be ignored.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>

<span class="sd">          When restoring from CudnnLSTM-trained checkpoints, must use</span>
<span class="sd">          CudnnCompatibleLSTMCell instead.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LayerNormLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span> <span class="o">=</span> <span class="n">use_peepholes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span> <span class="o">=</span> <span class="n">cell_clip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="o">=</span> <span class="n">initializer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="o">=</span> <span class="n">num_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_proj_clip</span> <span class="o">=</span> <span class="n">proj_clip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span> <span class="ow">or</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_layer_norm</span> <span class="o">=</span> <span class="n">layer_norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span> <span class="o">=</span> <span class="n">norm_gain</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span> <span class="o">=</span> <span class="n">norm_shift</span>

        <span class="k">if</span> <span class="n">num_proj</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">num_units</span><span class="p">,</span> <span class="n">num_proj</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">num_proj</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">num_units</span><span class="p">,</span> <span class="n">num_units</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">num_units</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span>

    <span class="k">def</span> <span class="nf">_linear</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">args</span><span class="p">,</span>
                <span class="n">output_size</span><span class="p">,</span>
                <span class="n">bias</span><span class="p">,</span>
                <span class="n">bias_initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">kernel_initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">layer_norm</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Linear map: sum_i(args[i] * W[i]), where W[i] is a Variable.</span>

<span class="sd">        Args:</span>
<span class="sd">          args: a 2D Tensor or a list of 2D, batch x n, Tensors.</span>
<span class="sd">          output_size: int, second dimension of W[i].</span>
<span class="sd">          bias: boolean, whether to add a bias term or not.</span>
<span class="sd">          bias_initializer: starting value to initialize the bias</span>
<span class="sd">            (default is all zeros).</span>
<span class="sd">          kernel_initializer: starting value to initialize the weight.</span>
<span class="sd">          layer_norm: boolean, whether to apply layer normalization.</span>


<span class="sd">        Returns:</span>
<span class="sd">          A 2D Tensor with shape [batch x output_size] taking value</span>
<span class="sd">          sum_i(args[i] * W[i]), where each W[i] is a newly created Variable.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: if some of the arguments has unspecified or wrong shape.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">is_sequence</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`args` must be specified&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">nest</span><span class="o">.</span><span class="n">is_sequence</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">args</span><span class="p">]</span>

        <span class="c1"># Calculate the total size of arguments on dimension 1.</span>
        <span class="n">total_arg_size</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">args</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">shape</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;linear is expecting 2D arguments: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">shapes</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;linear expects shape[1] to be provided for shape </span><span class="si">%s</span><span class="s2">, &quot;</span>
                                 <span class="s2">&quot;but saw </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">total_arg_size</span> <span class="o">+=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">dtype</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">args</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Now the computation.</span>
        <span class="n">scope</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">)</span> <span class="k">as</span> <span class="n">outer_scope</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="s2">&quot;kernel&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">total_arg_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">],</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">weights</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">bias</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">res</span>
            <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">outer_scope</span><span class="p">)</span> <span class="k">as</span> <span class="n">inner_scope</span><span class="p">:</span>
                <span class="n">inner_scope</span><span class="o">.</span><span class="n">set_partitioner</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">bias_initializer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">bias_initializer</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">biases</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                    <span class="s2">&quot;bias&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">output_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="n">bias_initializer</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">layer_norm</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">res</span>

<div class="viewcode-block" id="LayerNormLSTMCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.LayerNormLSTMCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run one step of LSTM.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: input Tensor, 2D, batch x num_units.</span>
<span class="sd">          state: this must be a tuple of state Tensors,</span>
<span class="sd">           both `2-D`, with column sizes `c_state` and</span>
<span class="sd">            `m_state`.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tuple containing:</span>

<span class="sd">          - A `2-D, [batch x output_dim]`, Tensor representing the output of the</span>
<span class="sd">            LSTM after reading `inputs` when previous state was `state`.</span>
<span class="sd">            Here output_dim is:</span>
<span class="sd">               num_proj if num_proj was set,</span>
<span class="sd">               num_units otherwise.</span>
<span class="sd">          - Tensor(s) representing the new state of LSTM after reading `inputs` when</span>
<span class="sd">            the previous state was `state`.  Same type and shape(s) as `state`.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: If input size cannot be inferred from inputs via</span>
<span class="sd">            static shape inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sigmoid</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span>

        <span class="p">(</span><span class="n">c_prev</span><span class="p">,</span> <span class="n">m_prev</span><span class="p">)</span> <span class="o">=</span> <span class="n">state</span>

        <span class="n">dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">input_size</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not infer input size from inputs.get_shape()[-1]&quot;</span><span class="p">)</span>
        <span class="n">scope</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span><span class="p">)</span> <span class="k">as</span> <span class="n">unit_scope</span><span class="p">:</span>

            <span class="c1"># i = input_gate, j = new_input, f = forget_gate, o = output_gate</span>
            <span class="n">lstm_matrix</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear</span><span class="p">(</span>
                <span class="p">[</span><span class="n">inputs</span><span class="p">,</span> <span class="n">m_prev</span><span class="p">],</span>
                <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">bias_initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">layer_norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_layer_norm</span><span class="p">)</span>
            <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
                <span class="n">value</span><span class="o">=</span><span class="n">lstm_matrix</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_norm</span><span class="p">:</span>
                <span class="n">i</span> <span class="o">=</span> <span class="n">_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="s2">&quot;input&quot;</span><span class="p">)</span>
                <span class="n">j</span> <span class="o">=</span> <span class="n">_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="s2">&quot;transform&quot;</span><span class="p">)</span>
                <span class="n">f</span> <span class="o">=</span> <span class="n">_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="s2">&quot;forget&quot;</span><span class="p">)</span>
                <span class="n">o</span> <span class="o">=</span> <span class="n">_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span><span class="p">,</span> <span class="n">o</span><span class="p">,</span> <span class="s2">&quot;output&quot;</span><span class="p">)</span>

            <span class="c1"># Diagonal connections</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">unit_scope</span><span class="p">):</span>
                    <span class="n">w_f_diag</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                        <span class="s2">&quot;w_f_diag&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="n">w_i_diag</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                        <span class="s2">&quot;w_i_diag&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
                    <span class="n">w_o_diag</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                        <span class="s2">&quot;w_o_diag&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                <span class="n">c</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">+</span> <span class="n">w_f_diag</span> <span class="o">*</span> <span class="n">c_prev</span><span class="p">)</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span>
                        <span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">w_i_diag</span> <span class="o">*</span> <span class="n">c_prev</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">c</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span> <span class="o">*</span> <span class="n">c_prev</span> <span class="o">+</span>
                        <span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_layer_norm</span><span class="p">:</span>
                <span class="n">c</span> <span class="o">=</span> <span class="n">_norm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm_gain</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_norm_shift</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="s2">&quot;state&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># pylint: disable=invalid-unary-operand-type</span>
                <span class="n">c</span> <span class="o">=</span> <span class="n">clip_ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span><span class="p">)</span>
                <span class="c1"># pylint: enable=invalid-unary-operand-type</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                <span class="n">m</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span> <span class="o">+</span> <span class="n">w_o_diag</span> <span class="o">*</span> <span class="n">c</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">m</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;projection&quot;</span><span class="p">):</span>
                    <span class="n">m</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proj_clip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># pylint: disable=invalid-unary-operand-type</span>
                    <span class="n">m</span> <span class="o">=</span> <span class="n">clip_ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_proj_clip</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proj_clip</span><span class="p">)</span>
                    <span class="c1"># pylint: enable=invalid-unary-operand-type</span>

        <span class="n">new_state</span> <span class="o">=</span> <span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">new_state</span></div></div>


<div class="viewcode-block" id="SRUCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.SRUCell">[docs]</a><span class="k">class</span> <span class="nc">SRUCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LayerRNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;SRU, Simple Recurrent Unit.</span>

<span class="sd">       Implementation based on</span>
<span class="sd">       Training RNNs as Fast as CNNs (cf. https://arxiv.org/abs/1709.02755).</span>

<span class="sd">       This variation of RNN cell is characterized by the simplified data</span>
<span class="sd">       dependence</span>
<span class="sd">       between hidden states of two consecutive time steps. Traditionally, hidden</span>
<span class="sd">       states from a cell at time step t-1 needs to be multiplied with a matrix</span>
<span class="sd">       W_hh before being fed into the ensuing cell at time step t.</span>
<span class="sd">       This flavor of RNN replaces the matrix multiplication between h_{t-1}</span>
<span class="sd">       and W_hh with a pointwise multiplication, resulting in performance</span>
<span class="sd">       gain.</span>

<span class="sd">    Args:</span>
<span class="sd">      num_units: int, The number of units in the SRU cell.</span>
<span class="sd">      activation: Nonlinearity to use.  Default: `tanh`.</span>
<span class="sd">      reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">        in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">        the given variables, an error is raised.</span>
<span class="sd">      name: (optional) String, the name of the layer. Layers with the same name</span>
<span class="sd">        will share weights, but to avoid mistakes we require reuse=True in such</span>
<span class="sd">        cases.</span>
<span class="sd">      **kwargs: Additional keyword arguments.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SRUCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span> <span class="ow">or</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span>

        <span class="c1"># Restrict inputs to be 2-dimensional matrices</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span> <span class="o">=</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">InputSpec</span><span class="p">(</span><span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected inputs.shape[-1] to be known, saw shape: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">inputs_shape</span><span class="p">)</span>

        <span class="n">input_depth</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="c1"># pylint: disable=protected-access</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_WEIGHTS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">input_depth</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">])</span>
        <span class="c1"># pylint: enable=protected-access</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_BIAS_VARIABLE_NAME</span><span class="p">,</span>  <span class="c1"># pylint: disable=protected-access</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_built</span> <span class="o">=</span> <span class="kc">True</span>

<div class="viewcode-block" id="SRUCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.SRUCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Simple recurrent unit (SRU) with num_units cells.&quot;&quot;&quot;</span>

        <span class="n">U</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel</span><span class="p">)</span>  <span class="c1"># pylint: disable=invalid-name</span>
        <span class="n">x_bar</span><span class="p">,</span> <span class="n">f_intermediate</span><span class="p">,</span> <span class="n">r_intermediate</span><span class="p">,</span> <span class="n">x_tx</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">U</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">f_r</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span>
            <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span>
                <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">f_intermediate</span><span class="p">,</span> <span class="n">r_intermediate</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span><span class="p">))</span>
        <span class="n">f</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">f_r</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">c</span> <span class="o">=</span> <span class="n">f</span> <span class="o">*</span> <span class="n">state</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">f</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_bar</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">r</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_tx</span>

        <span class="k">return</span> <span class="n">h</span><span class="p">,</span> <span class="n">c</span></div></div>


<div class="viewcode-block" id="WeightNormLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.WeightNormLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">WeightNormLSTMCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">RNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Weight normalized LSTM Cell. Adapted from `rnn_cell_impl.LSTMCell`.</span>

<span class="sd">      The weight-norm implementation is based on:</span>
<span class="sd">      https://arxiv.org/abs/1602.07868</span>
<span class="sd">      Tim Salimans, Diederik P. Kingma.</span>
<span class="sd">      Weight Normalization: A Simple Reparameterization to Accelerate</span>
<span class="sd">      Training of Deep Neural Networks</span>

<span class="sd">      The default LSTM implementation based on:</span>

<span class="sd">        https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf</span>

<span class="sd">      Felix Gers, Jurgen Schmidhuber, and Fred Cummins.</span>
<span class="sd">      &quot;Learning to forget: Continual prediction with LSTM.&quot; IET, 850-855, 1999.</span>

<span class="sd">      The class uses optional peephole connections, optional cell clipping</span>
<span class="sd">      and an optional projection layer.</span>

<span class="sd">      The optional peephole implementation is based on:</span>
<span class="sd">      https://research.google.com/pubs/archive/43905.pdf</span>
<span class="sd">      Hasim Sak, Andrew Senior, and Francoise Beaufays.</span>
<span class="sd">      &quot;Long short-term memory recurrent neural network architectures for</span>
<span class="sd">      large scale acoustic modeling.&quot; INTERSPEECH, 2014.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">norm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">use_peepholes</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">cell_clip</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">num_proj</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">proj_clip</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the parameters of a weight-normalized LSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the LSTM cell</span>
<span class="sd">          norm: If `True`, apply normalization to the weight matrices. If False,</span>
<span class="sd">            the result is identical to that obtained from `rnn_cell_impl.LSTMCell`</span>
<span class="sd">          use_peepholes: bool, set `True` to enable diagonal/peephole connections.</span>
<span class="sd">          cell_clip: (optional) A float value, if provided the cell state is clipped</span>
<span class="sd">            by this value prior to the cell output activation.</span>
<span class="sd">          initializer: (optional) The initializer to use for the weight matrices.</span>
<span class="sd">          num_proj: (optional) int, The output dimensionality for the projection</span>
<span class="sd">            matrices.  If None, no projection is performed.</span>
<span class="sd">          proj_clip: (optional) A float value.  If `num_proj &gt; 0` and `proj_clip` is</span>
<span class="sd">            provided, then the projected values are clipped elementwise to within</span>
<span class="sd">            `[-proj_clip, proj_clip]`.</span>
<span class="sd">          forget_bias: Biases of the forget gate are initialized by default to 1</span>
<span class="sd">            in order to reduce the scale of forgetting at the beginning of</span>
<span class="sd">            the training.</span>
<span class="sd">          activation: Activation function of the inner states.  Default: `tanh`.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WeightNormLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_scope</span> <span class="o">=</span> <span class="s2">&quot;wn_lstm_cell&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_norm</span> <span class="o">=</span> <span class="n">norm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span> <span class="o">=</span> <span class="n">initializer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span> <span class="o">=</span> <span class="n">use_peepholes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span> <span class="o">=</span> <span class="n">cell_clip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="o">=</span> <span class="n">num_proj</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_proj_clip</span> <span class="o">=</span> <span class="n">proj_clip</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span> <span class="ow">or</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_weights_variable_name</span> <span class="o">=</span> <span class="s2">&quot;kernel&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias_variable_name</span> <span class="o">=</span> <span class="s2">&quot;bias&quot;</span>

        <span class="k">if</span> <span class="n">num_proj</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">num_units</span><span class="p">,</span> <span class="n">num_proj</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">num_proj</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">num_units</span><span class="p">,</span> <span class="n">num_units</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span> <span class="o">=</span> <span class="n">num_units</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_state_size</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_size</span>

    <span class="k">def</span> <span class="nf">_normalize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Apply weight normalization.</span>

<span class="sd">        Args:</span>
<span class="sd">          weight: a 2D tensor with known number of columns.</span>
<span class="sd">          name: string, variable name for the normalizer.</span>
<span class="sd">        Returns:</span>
<span class="sd">          A tensor with the same shape as `weight`.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">output_size</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">as_list</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="p">[</span><span class="n">output_size</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">nn_impl</span><span class="o">.</span><span class="n">l2_normalize</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span>

    <span class="k">def</span> <span class="nf">_linear</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                <span class="n">args</span><span class="p">,</span>
                <span class="n">output_size</span><span class="p">,</span>
                <span class="n">norm</span><span class="p">,</span>
                <span class="n">bias</span><span class="p">,</span>
                <span class="n">bias_initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">kernel_initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.</span>

<span class="sd">        Args:</span>
<span class="sd">          args: a 2D Tensor or a list of 2D, batch x n, Tensors.</span>
<span class="sd">          output_size: int, second dimension of W[i].</span>
<span class="sd">          norm: bool, whether to normalize the weights.</span>
<span class="sd">          bias: boolean, whether to add a bias term or not.</span>
<span class="sd">          bias_initializer: starting value to initialize the bias</span>
<span class="sd">            (default is all zeros).</span>
<span class="sd">          kernel_initializer: starting value to initialize the weight.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A 2D Tensor with shape [batch x output_size] equal to</span>
<span class="sd">          sum_i(args[i] * W[i]), where W[i]s are newly created matrices.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: if some of the arguments has unspecified or wrong shape.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">args</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="p">(</span><span class="n">nest</span><span class="o">.</span><span class="n">is_sequence</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">args</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`args` must be specified&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">nest</span><span class="o">.</span><span class="n">is_sequence</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">args</span><span class="p">]</span>

        <span class="c1"># Calculate the total size of arguments on dimension 1.</span>
        <span class="n">total_arg_size</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">shapes</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">args</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">shape</span> <span class="ow">in</span> <span class="n">shapes</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">shape</span><span class="o">.</span><span class="n">ndims</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;linear is expecting 2D arguments: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">shapes</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;linear expects shape[1] to be provided for shape </span><span class="si">%s</span><span class="s2">, &quot;</span>
                                 <span class="s2">&quot;but saw </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">total_arg_size</span> <span class="o">+=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>

        <span class="n">dtype</span> <span class="o">=</span> <span class="p">[</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">args</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Now the computation.</span>
        <span class="n">scope</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable_scope</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">scope</span><span class="p">)</span> <span class="k">as</span> <span class="n">outer_scope</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_weights_variable_name</span><span class="p">,</span> <span class="p">[</span><span class="n">total_arg_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">],</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">kernel_initializer</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">norm</span><span class="p">:</span>
                <span class="n">wn</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">st</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="k">with</span> <span class="n">ops</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="kc">None</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)):</span>
                        <span class="n">en</span> <span class="o">=</span> <span class="n">st</span> <span class="o">+</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">shapes</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                        <span class="n">wn</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">_normalize</span><span class="p">(</span><span class="n">weights</span><span class="p">[</span><span class="n">st</span><span class="p">:</span><span class="n">en</span><span class="p">,</span> <span class="p">:],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;norm_</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">)))</span>
                        <span class="n">st</span> <span class="o">=</span> <span class="n">en</span>

                    <span class="n">weights</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">wn</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">args</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">weights</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">res</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">args</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">weights</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">bias</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">res</span>

            <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="n">outer_scope</span><span class="p">)</span> <span class="k">as</span> <span class="n">inner_scope</span><span class="p">:</span>
                <span class="n">inner_scope</span><span class="o">.</span><span class="n">set_partitioner</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">bias_initializer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">bias_initializer</span> <span class="o">=</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

                <span class="n">biases</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_bias_variable_name</span><span class="p">,</span> <span class="p">[</span><span class="n">output_size</span><span class="p">],</span>
                    <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                    <span class="n">initializer</span><span class="o">=</span><span class="n">bias_initializer</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">biases</span><span class="p">)</span>

<div class="viewcode-block" id="WeightNormLSTMCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.WeightNormLSTMCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run one step of LSTM.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: input Tensor, 2D, batch x num_units.</span>
<span class="sd">          state: A tuple of state Tensors, both `2-D`, with column sizes</span>
<span class="sd">           `c_state` and `m_state`.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tuple containing:</span>

<span class="sd">          - A `2-D, [batch x output_dim]`, Tensor representing the output of the</span>
<span class="sd">            LSTM after reading `inputs` when previous state was `state`.</span>
<span class="sd">            Here output_dim is:</span>
<span class="sd">               num_proj if num_proj was set,</span>
<span class="sd">               num_units otherwise.</span>
<span class="sd">          - Tensor(s) representing the new state of LSTM after reading `inputs` when</span>
<span class="sd">            the previous state was `state`.  Same type and shape(s) as `state`.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: If input size cannot be inferred from inputs via</span>
<span class="sd">            static shape inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span>
        <span class="n">num_units</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>
        <span class="n">sigmoid</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span>
        <span class="n">c</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">state</span>

        <span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()</span><span class="o">.</span><span class="n">with_rank</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">dims</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">input_size</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not infer input size from inputs.get_shape()[-1]&quot;</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_scope</span><span class="p">,</span> <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_initializer</span><span class="p">):</span>

            <span class="n">concat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear</span><span class="p">(</span>
                <span class="p">[</span><span class="n">inputs</span><span class="p">,</span> <span class="n">h</span><span class="p">],</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">num_units</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># i = input_gate, j = new_input, f = forget_gate, o = output_gate</span>
            <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">concat</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                <span class="n">w_f_diag</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w_f_diag&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">w_i_diag</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w_i_diag&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
                <span class="n">w_o_diag</span> <span class="o">=</span> <span class="n">vs</span><span class="o">.</span><span class="n">get_variable</span><span class="p">(</span><span class="s2">&quot;w_o_diag&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">num_units</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

                <span class="n">new_c</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">c</span> <span class="o">*</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">+</span> <span class="n">w_f_diag</span> <span class="o">*</span> <span class="n">c</span><span class="p">)</span> <span class="o">+</span>
                        <span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="n">w_i_diag</span> <span class="o">*</span> <span class="n">c</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_c</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">c</span> <span class="o">*</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">f</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">)</span> <span class="o">+</span>
                        <span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">j</span><span class="p">))</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># pylint: disable=invalid-unary-operand-type</span>
                <span class="n">new_c</span> <span class="o">=</span> <span class="n">clip_ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">new_c</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell_clip</span><span class="p">)</span>
                <span class="c1"># pylint: enable=invalid-unary-operand-type</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_peepholes</span><span class="p">:</span>
                <span class="n">new_h</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span> <span class="o">+</span> <span class="n">w_o_diag</span> <span class="o">*</span> <span class="n">new_c</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">new_c</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_h</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">new_c</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">with</span> <span class="n">vs</span><span class="o">.</span><span class="n">variable_scope</span><span class="p">(</span><span class="s2">&quot;projection&quot;</span><span class="p">):</span>
                    <span class="n">new_h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_linear</span><span class="p">(</span>
                        <span class="n">new_h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_proj</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_norm</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_proj_clip</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="c1"># pylint: disable=invalid-unary-operand-type</span>
                    <span class="n">new_h</span> <span class="o">=</span> <span class="n">clip_ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">new_h</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">_proj_clip</span><span class="p">,</span>
                                                   <span class="bp">self</span><span class="o">.</span><span class="n">_proj_clip</span><span class="p">)</span>
                    <span class="c1"># pylint: enable=invalid-unary-operand-type</span>

            <span class="n">new_state</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">new_c</span><span class="p">,</span> <span class="n">new_h</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_state</span></div></div>


<div class="viewcode-block" id="IndRNNCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell">[docs]</a><span class="k">class</span> <span class="nc">IndRNNCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LayerRNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Independently Recurrent Neural Network (IndRNN) cell</span>
<span class="sd">      (cf. https://arxiv.org/abs/1803.04831).</span>

<span class="sd">    Args:</span>
<span class="sd">      num_units: int, The number of units in the RNN cell.</span>
<span class="sd">      activation: Nonlinearity to use.  Default: `tanh`.</span>
<span class="sd">      reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">       in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">       the given variables, an error is raised.</span>
<span class="sd">      name: String, the name of the layer. Layers with the same name will</span>
<span class="sd">        share weights, but to avoid mistakes we require reuse=True in such</span>
<span class="sd">        cases.</span>
<span class="sd">      dtype: Default dtype of the layer (default of `None` means use the type</span>
<span class="sd">        of the first input). Required when `build` is called before `call`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">IndRNNCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Inputs must be 2-dimensional.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span> <span class="o">=</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">InputSpec</span><span class="p">(</span><span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span> <span class="ow">or</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected inputs.shape[-1] to be known, saw shape: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">inputs_shape</span><span class="p">)</span>

        <span class="n">input_depth</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_w&quot;</span> <span class="o">%</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_WEIGHTS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">input_depth</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_u&quot;</span> <span class="o">%</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_WEIGHTS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
                <span class="n">minval</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_BIAS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="c1"># pylint: enable=protected-access</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

<div class="viewcode-block" id="IndRNNCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;IndRNN: output = new_state = act(W * input + u * state + B).&quot;&quot;&quot;</span>

        <span class="n">gate_inputs</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_w</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span>
                <span class="n">state</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_u</span><span class="p">)</span>
        <span class="n">gate_inputs</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">gate_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">gate_inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">output</span></div></div>


<div class="viewcode-block" id="IndyGRUCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell">[docs]</a><span class="k">class</span> <span class="nc">IndyGRUCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LayerRNNCell</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Independently Gated Recurrent Unit cell.</span>

<span class="sd">    Based on IndRNNs (https://arxiv.org/abs/1803.04831) and similar to GRUCell,</span>
<span class="sd">    yet with the \\(U_r\\), \\(U_z\\), and \\(U\\) matrices in equations 5, 6, and</span>
<span class="sd">    8 of http://arxiv.org/abs/1406.1078 respectively replaced by diagonal</span>
<span class="sd">    matrices, i.e. a Hadamard product with a single vector:</span>

<span class="sd">      $$r_j = \sigma\left([\mathbf W_r\mathbf x]_j +</span>
<span class="sd">        [\mathbf u_r\circ \mathbf h_{(t-1)}]_j\right)$$</span>
<span class="sd">      $$z_j = \sigma\left([\mathbf W_z\mathbf x]_j +</span>
<span class="sd">        [\mathbf u_z\circ \mathbf h_{(t-1)}]_j\right)$$</span>
<span class="sd">      $$\tilde{h}^{(t)}_j = \phi\left([\mathbf W \mathbf x]_j +</span>
<span class="sd">        [\mathbf u \circ \mathbf r \circ \mathbf h_{(t-1)}]_j\right)$$</span>

<span class="sd">    where \\(\circ\\) denotes the Hadamard operator. This means that each IndyGRU</span>
<span class="sd">    node sees only its own state, as opposed to seeing all states in the same</span>
<span class="sd">    layer.</span>

<span class="sd">    Args:</span>
<span class="sd">      num_units: int, The number of units in the GRU cell.</span>
<span class="sd">      activation: Nonlinearity to use.  Default: `tanh`.</span>
<span class="sd">      reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">       in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">       the given variables, an error is raised.</span>
<span class="sd">      kernel_initializer: (optional) The initializer to use for the weight</span>
<span class="sd">        matrices applied to the input.</span>
<span class="sd">      bias_initializer: (optional) The initializer to use for the bias.</span>
<span class="sd">      name: String, the name of the layer. Layers with the same name will</span>
<span class="sd">        share weights, but to avoid mistakes we require reuse=True in such</span>
<span class="sd">        cases.</span>
<span class="sd">      dtype: Default dtype of the layer (default of `None` means use the type</span>
<span class="sd">        of the first input). Required when `build` is called before `call`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bias_initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">IndyGRUCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Inputs must be 2-dimensional.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span> <span class="o">=</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">InputSpec</span><span class="p">(</span><span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span> <span class="ow">or</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_initializer</span> <span class="o">=</span> <span class="n">kernel_initializer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias_initializer</span> <span class="o">=</span> <span class="n">bias_initializer</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected inputs.shape[-1] to be known, saw shape: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">inputs_shape</span><span class="p">)</span>

        <span class="n">input_depth</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gate_kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;gates/</span><span class="si">%s</span><span class="s2">_w&quot;</span> <span class="o">%</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_WEIGHTS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">input_depth</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_initializer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gate_kernel_u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;gates/</span><span class="si">%s</span><span class="s2">_u&quot;</span> <span class="o">%</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_WEIGHTS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
                <span class="n">minval</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_gate_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;gates/</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_BIAS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bias_initializer</span>
                         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias_initializer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span>
                         <span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_candidate_kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;candidate/</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_WEIGHTS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">input_depth</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_initializer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_candidate_kernel_u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;candidate/</span><span class="si">%s</span><span class="s2">_u&quot;</span> <span class="o">%</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_WEIGHTS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
                <span class="n">minval</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_candidate_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;candidate/</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_BIAS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bias_initializer</span>
                         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias_initializer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span>
                         <span class="n">init_ops</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
        <span class="c1"># pylint: enable=protected-access</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

<div class="viewcode-block" id="IndyGRUCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Recurrently independent Gated Recurrent Unit (GRU) with nunits cells.&quot;&quot;&quot;</span>

        <span class="n">gate_inputs</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gate_kernel_w</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span>
                <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gate_kernel_u</span><span class="p">)</span>
        <span class="n">gate_inputs</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">gate_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_gate_bias</span><span class="p">)</span>

        <span class="n">value</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gate_inputs</span><span class="p">)</span>
        <span class="n">r</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">r_state</span> <span class="o">=</span> <span class="n">r</span> <span class="o">*</span> <span class="n">state</span>

        <span class="n">candidate</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_candidate_kernel_w</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span>
                <span class="n">r_state</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_candidate_kernel_u</span><span class="p">)</span>
        <span class="n">candidate</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">candidate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_candidate_bias</span><span class="p">)</span>

        <span class="n">c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">candidate</span><span class="p">)</span>
        <span class="n">new_h</span> <span class="o">=</span> <span class="n">u</span> <span class="o">*</span> <span class="n">state</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span>
        <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_h</span></div></div>


<div class="viewcode-block" id="IndyLSTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell">[docs]</a><span class="k">class</span> <span class="nc">IndyLSTMCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LayerRNNCell</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Basic IndyLSTM recurrent network cell.</span>

<span class="sd">    Based on IndRNNs (https://arxiv.org/abs/1803.04831) and similar to</span>
<span class="sd">    BasicLSTMCell, yet with the \\(U_f\\), \\(U_i\\), \\(U_o\\) and \\(U_c\\)</span>
<span class="sd">    matrices in the regular LSTM equations replaced by diagonal matrices, i.e. a</span>
<span class="sd">    Hadamard product with a single vector:</span>

<span class="sd">      $$f_t = \sigma_g\left(W_f x_t + u_f \circ h_{t-1} + b_f\right)$$</span>
<span class="sd">      $$i_t = \sigma_g\left(W_i x_t + u_i \circ h_{t-1} + b_i\right)$$</span>
<span class="sd">      $$o_t = \sigma_g\left(W_o x_t + u_o \circ h_{t-1} + b_o\right)$$</span>
<span class="sd">      $$c_t = f_t \circ c_{t-1} +</span>
<span class="sd">              i_t \circ \sigma_c\left(W_c x_t + u_c \circ h_{t-1} + b_c\right)$$</span>

<span class="sd">    where \\(\circ\\) denotes the Hadamard operator. This means that each IndyLSTM</span>
<span class="sd">    node sees only its own state \\(h\\) and \\(c\\), as opposed to seeing all</span>
<span class="sd">    states in the same layer.</span>

<span class="sd">    We add forget_bias (default: 1) to the biases of the forget gate in order to</span>
<span class="sd">    reduce the scale of forgetting in the beginning of the training.</span>

<span class="sd">    It does not allow cell clipping, a projection layer, and does not</span>
<span class="sd">    use peep-hole connections: it is the basic baseline.</span>

<span class="sd">    For a detailed analysis of IndyLSTMs, see https://arxiv.org/abs/1903.08023.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">num_units</span><span class="p">,</span>
                 <span class="n">forget_bias</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">reuse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">bias_initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the IndyLSTM cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          num_units: int, The number of units in the LSTM cell.</span>
<span class="sd">          forget_bias: float, The bias added to forget gates (see above).</span>
<span class="sd">            Must set to `0.0` manually when restoring from CudnnLSTM-trained</span>
<span class="sd">            checkpoints.</span>
<span class="sd">          activation: Activation function of the inner states.  Default: `tanh`.</span>
<span class="sd">          reuse: (optional) Python boolean describing whether to reuse variables</span>
<span class="sd">            in an existing scope.  If not `True`, and the existing scope already has</span>
<span class="sd">            the given variables, an error is raised.</span>
<span class="sd">          kernel_initializer: (optional) The initializer to use for the weight</span>
<span class="sd">            matrix applied to the inputs.</span>
<span class="sd">          bias_initializer: (optional) The initializer to use for the bias.</span>
<span class="sd">          name: String, the name of the layer. Layers with the same name will</span>
<span class="sd">            share weights, but to avoid mistakes we require reuse=True in such</span>
<span class="sd">            cases.</span>
<span class="sd">          dtype: Default dtype of the layer (default of `None` means use the type</span>
<span class="sd">            of the first input). Required when `build` is called before `call`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">IndyLSTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">_reuse</span><span class="o">=</span><span class="n">reuse</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="c1"># Inputs must be 2-dimensional.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span> <span class="o">=</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">InputSpec</span><span class="p">(</span><span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span> <span class="o">=</span> <span class="n">num_units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span> <span class="o">=</span> <span class="n">forget_bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span> <span class="o">=</span> <span class="n">activation</span> <span class="ow">or</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_initializer</span> <span class="o">=</span> <span class="n">kernel_initializer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias_initializer</span> <span class="o">=</span> <span class="n">bias_initializer</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected inputs.shape[-1] to be known, saw shape: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">inputs_shape</span><span class="p">)</span>

        <span class="n">input_depth</span> <span class="o">=</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_w&quot;</span> <span class="o">%</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_WEIGHTS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">input_depth</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_kernel_initializer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">_u&quot;</span> <span class="o">%</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_WEIGHTS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">random_uniform_initializer</span><span class="p">(</span>
                <span class="n">minval</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_BIAS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_num_units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_bias_initializer</span>
                         <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias_initializer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span>
                         <span class="n">init_ops</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)))</span>
        <span class="c1"># pylint: enable=protected-access</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

<div class="viewcode-block" id="IndyLSTMCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Independent Long short-term memory cell (IndyLSTM).</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: `2-D` tensor with shape `[batch_size, input_size]`.</span>
<span class="sd">          state: An `LSTMStateTuple` of state tensors, each shaped</span>
<span class="sd">            `[batch_size, num_units]`.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A pair containing the new hidden state, and the new state (a</span>
<span class="sd">            `LSTMStateTuple`).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sigmoid</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span>
        <span class="n">one</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
        <span class="n">c</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">state</span>

        <span class="n">gate_inputs</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_w</span><span class="p">)</span>
        <span class="n">gate_inputs</span> <span class="o">+=</span> <span class="n">gen_array_ops</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">_kernel_u</span>
        <span class="n">gate_inputs</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">gate_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bias</span><span class="p">)</span>

        <span class="c1"># i = input_gate, j = new_input, f = forget_gate, o = output_gate</span>
        <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">o</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="n">gate_inputs</span><span class="p">,</span> <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">one</span><span class="p">)</span>

        <span class="n">forget_bias_tensor</span> <span class="o">=</span> <span class="n">constant_op</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_forget_bias</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">f</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="c1"># Note that using `add` and `multiply` instead of `+` and `*` gives a</span>
        <span class="c1"># performance improvement. So using those at the cost of readability.</span>
        <span class="n">add</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">add</span>
        <span class="n">multiply</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">multiply</span>
        <span class="n">new_c</span> <span class="o">=</span> <span class="n">add</span><span class="p">(</span>
            <span class="n">multiply</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">add</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">forget_bias_tensor</span><span class="p">))),</span>
            <span class="n">multiply</span><span class="p">(</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">i</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">j</span><span class="p">)))</span>
        <span class="n">new_h</span> <span class="o">=</span> <span class="n">multiply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_activation</span><span class="p">(</span><span class="n">new_c</span><span class="p">),</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">o</span><span class="p">))</span>

        <span class="n">new_state</span> <span class="o">=</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LSTMStateTuple</span><span class="p">(</span><span class="n">new_c</span><span class="p">,</span> <span class="n">new_h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_state</span></div></div>


<span class="n">NTMControllerState</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">namedtuple</span><span class="p">(</span>
    <span class="s2">&quot;NTMControllerState&quot;</span><span class="p">,</span>
    <span class="p">(</span><span class="s2">&quot;controller_state&quot;</span><span class="p">,</span> <span class="s2">&quot;read_vector_list&quot;</span><span class="p">,</span> <span class="s2">&quot;w_list&quot;</span><span class="p">,</span> <span class="s2">&quot;M&quot;</span><span class="p">,</span> <span class="s2">&quot;time&quot;</span><span class="p">))</span>


<div class="viewcode-block" id="NTMCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.NTMCell">[docs]</a><span class="k">class</span> <span class="nc">NTMCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LayerRNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Neural Turing Machine Cell with RNN controller.</span>

<span class="sd">      Implementation based on:</span>
<span class="sd">      https://arxiv.org/abs/1807.08518</span>
<span class="sd">      Mark Collier, Joeran Beel</span>

<span class="sd">      which is in turn based on the source code of:</span>
<span class="sd">      https://github.com/snowkylin/ntm</span>

<span class="sd">      and of course the original NTM paper:</span>
<span class="sd">      Neural Turing Machines</span>
<span class="sd">      https://arxiv.org/abs/1410.5401</span>
<span class="sd">      A Graves, G Wayne, I Danihelka</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">controller</span><span class="p">,</span>
                 <span class="n">memory_size</span><span class="p">,</span>
                 <span class="n">memory_vector_dim</span><span class="p">,</span>
                 <span class="n">read_head_num</span><span class="p">,</span>
                 <span class="n">write_head_num</span><span class="p">,</span>
                 <span class="n">shift_range</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">output_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">clip_value</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the NTM Cell.</span>

<span class="sd">          Args:</span>
<span class="sd">            controller: an RNNCell, the RNN controller.</span>
<span class="sd">            memory_size: int, The number of memory locations in the NTM memory</span>
<span class="sd">              matrix</span>
<span class="sd">            memory_vector_dim: int, The dimensionality of each location in the NTM</span>
<span class="sd">              memory matrix</span>
<span class="sd">            read_head_num: int, The number of read heads from the controller into</span>
<span class="sd">              memory</span>
<span class="sd">            write_head_num: int, The number of write heads from the controller into</span>
<span class="sd">              memory</span>
<span class="sd">            shift_range: int, The number of places to the left/right it is possible</span>
<span class="sd">              to iterate the previous address to in a single step</span>
<span class="sd">            output_dim: int, The number of dimensions to make a linear projection of</span>
<span class="sd">              the NTM controller outputs to. If None, no linear projection is</span>
<span class="sd">              applied</span>
<span class="sd">            clip_value: float, The maximum absolute value the controller parameters</span>
<span class="sd">              are clipped to</span>
<span class="sd">            dtype: Default dtype of the layer (default of `None` means use the type</span>
<span class="sd">              of the first input). Required when `build` is called before `call`.</span>
<span class="sd">            name: String, the name of the layer. Layers with the same name will</span>
<span class="sd">              share weights, but to avoid mistakes we require reuse=True in such</span>
<span class="sd">              cases.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NTMCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">assert_like_rnncell</span><span class="p">(</span><span class="s2">&quot;NTM RNN controller cell&quot;</span><span class="p">,</span> <span class="n">controller</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">controller</span> <span class="o">=</span> <span class="n">controller</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span> <span class="o">=</span> <span class="n">memory_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span> <span class="o">=</span> <span class="n">memory_vector_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span> <span class="o">=</span> <span class="n">read_head_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">write_head_num</span> <span class="o">=</span> <span class="n">write_head_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">clip_value</span> <span class="o">=</span> <span class="n">clip_value</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shift_range</span> <span class="o">=</span> <span class="n">shift_range</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_parameters_per_head</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">shift_range</span> <span class="o">+</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">write_head_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_parameter_num</span> <span class="o">=</span> <span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_parameters_per_head</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">+</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">write_head_num</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">NTMControllerState</span><span class="p">(</span>
            <span class="n">controller_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">state_size</span><span class="p">,</span>
            <span class="n">read_vector_list</span><span class="o">=</span><span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span><span class="p">)</span>
            <span class="p">],</span>
            <span class="n">w_list</span><span class="o">=</span><span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span>
                <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">write_head_num</span><span class="p">)</span>
            <span class="p">],</span>
            <span class="n">M</span><span class="o">=</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span><span class="p">]),</span>
            <span class="n">time</span><span class="o">=</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">TensorShape</span><span class="p">([]))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Expected inputs.shape[-1] to be known, saw shape: </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span>
                    <span class="n">inputs_shape</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">value</span>

        <span class="k">def</span> <span class="nf">_create_linear_initializer</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtypes</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
            <span class="n">stddev</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">init_ops</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="n">stddev</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_params_kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;parameters_kernel&quot;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">output_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_parameter_num</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">_create_linear_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">output_size</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_params_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;parameters_bias&quot;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">total_parameter_num</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_output_kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;output_kernel&quot;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">output_size</span> <span class="o">+</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span>
            <span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">_create_linear_initializer</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">output_size</span> <span class="o">+</span>
                                                   <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span> <span class="o">*</span>
                                                   <span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_output_bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;output_bias&quot;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_read_vectors</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
                <span class="s2">&quot;initial_read_vector_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span><span class="p">],</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">initializers</span><span class="o">.</span><span class="n">glorot_uniform</span><span class="p">())</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_init_address_weights</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
                <span class="s2">&quot;initial_address_weights_</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">i</span><span class="p">,</span>
                <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span><span class="p">],</span>
                <span class="n">initializer</span><span class="o">=</span><span class="n">initializers</span><span class="o">.</span><span class="n">glorot_uniform</span><span class="p">())</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">write_head_num</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_M</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_variable</span><span class="p">(</span>
            <span class="s2">&quot;memory&quot;</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="n">init_ops</span><span class="o">.</span><span class="n">constant_initializer</span><span class="p">(</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">prev_state</span><span class="p">):</span>
        <span class="c1"># Addressing Mechanisms (Sec 3.3)</span>

        <span class="k">def</span> <span class="nf">_prev_read_vector_list_initial_value</span><span class="p">():</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_expand</span><span class="p">(</span>
                    <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span>
                        <span class="n">array_ops</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
                            <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
                                <span class="n">array_ops</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span> <span class="bp">self</span><span class="o">.</span><span class="n">_init_read_vectors</span><span class="p">[</span><span class="n">i</span><span class="p">]))),</span>
                    <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">N</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span> <span class="ow">or</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="n">prev_read_vector_list</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span>
            <span class="n">math_ops</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">prev_state</span><span class="o">.</span><span class="n">time</span><span class="p">,</span>
                           <span class="mi">0</span><span class="p">),</span> <span class="n">_prev_read_vector_list_initial_value</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span>
            <span class="n">prev_state</span><span class="o">.</span><span class="n">read_vector_list</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">prev_read_vector_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">prev_read_vector_list</span><span class="p">]</span>

        <span class="n">controller_input</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">x</span><span class="p">]</span> <span class="o">+</span> <span class="n">prev_read_vector_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">controller_output</span><span class="p">,</span> <span class="n">controller_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">controller</span><span class="p">(</span>
            <span class="n">controller_input</span><span class="p">,</span> <span class="n">prev_state</span><span class="o">.</span><span class="n">controller_state</span><span class="p">)</span>

        <span class="n">parameters</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">controller_output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params_kernel</span><span class="p">)</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_params_bias</span><span class="p">)</span>
        <span class="n">parameters</span> <span class="o">=</span> <span class="n">clip_ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">clip_value</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">clip_value</span><span class="p">)</span>
        <span class="n">head_parameter_list</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">parameters</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">num_parameters_per_head</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">erase_add_list</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">parameters</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_parameters_per_head</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">:],</span>
            <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">write_head_num</span><span class="p">,</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">_prev_w_list_initial_value</span><span class="p">():</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_expand</span><span class="p">(</span>
                    <span class="n">nn_ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span>
                        <span class="n">array_ops</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span>
                            <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
                                <span class="n">array_ops</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]),</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">_init_address_weights</span><span class="p">[</span><span class="n">i</span><span class="p">]))),</span>
                    <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                    <span class="n">N</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span> <span class="ow">or</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
                <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">write_head_num</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="n">prev_w_list</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span>
            <span class="n">math_ops</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">prev_state</span><span class="o">.</span><span class="n">time</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">_prev_w_list_initial_value</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">prev_state</span><span class="o">.</span><span class="n">w_list</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">write_head_num</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">prev_w_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">prev_w_list</span><span class="p">]</span>

        <span class="n">prev_M</span> <span class="o">=</span> <span class="n">control_flow_ops</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span>
            <span class="n">math_ops</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">prev_state</span><span class="o">.</span><span class="n">time</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="k">lambda</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expand</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_M</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span> <span class="ow">or</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">]),</span>
            <span class="k">lambda</span><span class="p">:</span> <span class="n">prev_state</span><span class="o">.</span><span class="n">M</span><span class="p">)</span>

        <span class="n">w_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">head_parameter</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">head_parameter_list</span><span class="p">):</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">head_parameter</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span><span class="p">])</span>
            <span class="n">beta</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">head_parameter</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span><span class="p">])</span>
            <span class="n">g</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">head_parameter</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span> <span class="o">+</span> <span class="mi">1</span><span class="p">])</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">head_parameter</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span> <span class="o">+</span>
                                                 <span class="mi">2</span><span class="p">:(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">+</span>
                                                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shift_range</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))])</span>
            <span class="n">gamma</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">head_parameter</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">w</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_addressing</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">prev_M</span><span class="p">,</span> <span class="n">prev_w_list</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">w_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

        <span class="c1"># Reading (Sec 3.1)</span>

        <span class="n">read_w_list</span> <span class="o">=</span> <span class="n">w_list</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span><span class="p">]</span>
        <span class="n">read_vector_list</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span><span class="p">):</span>
            <span class="n">read_vector</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
                <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">read_w_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">prev_M</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">read_vector_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">read_vector</span><span class="p">)</span>

        <span class="c1"># Writing (Sec 3.2)</span>

        <span class="n">write_w_list</span> <span class="o">=</span> <span class="n">w_list</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span><span class="p">:]</span>
        <span class="n">M</span> <span class="o">=</span> <span class="n">prev_M</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">write_head_num</span><span class="p">):</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">write_w_list</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">erase_vector</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
                <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">erase_add_list</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">add_vector</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span>
                <span class="n">math_ops</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">erase_add_list</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">erase_M</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">M</span><span class="p">)</span> <span class="o">-</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">erase_vector</span><span class="p">)</span>
            <span class="n">M</span> <span class="o">=</span> <span class="n">M</span> <span class="o">*</span> <span class="n">erase_M</span> <span class="o">+</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">add_vector</span><span class="p">)</span>

        <span class="n">output</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
            <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">controller_output</span><span class="p">]</span> <span class="o">+</span> <span class="n">read_vector_list</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_output_kernel</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_output_bias</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">clip_ops</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">clip_value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">clip_value</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">NTMControllerState</span><span class="p">(</span>
            <span class="n">controller_state</span><span class="o">=</span><span class="n">controller_state</span><span class="p">,</span>
            <span class="n">read_vector_list</span><span class="o">=</span><span class="n">read_vector_list</span><span class="p">,</span>
            <span class="n">w_list</span><span class="o">=</span><span class="n">w_list</span><span class="p">,</span>
            <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">,</span>
            <span class="n">time</span><span class="o">=</span><span class="n">prev_state</span><span class="o">.</span><span class="n">time</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_expand</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)],</span>
                                <span class="n">axis</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_addressing</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">prev_M</span><span class="p">,</span> <span class="n">prev_w</span><span class="p">):</span>
        <span class="c1"># Sec 3.3.1 Focusing by Content</span>

        <span class="n">k</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">inner_product</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">prev_M</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        <span class="n">k_norm</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
            <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">k</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">M_norm</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
            <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">prev_M</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">norm_product</span> <span class="o">=</span> <span class="n">M_norm</span> <span class="o">*</span> <span class="n">k_norm</span>

        <span class="c1"># eq (6)</span>
        <span class="n">K</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">inner_product</span> <span class="o">/</span> <span class="p">(</span><span class="n">norm_product</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span>

        <span class="n">K_amplified</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">K</span><span class="p">)</span>

        <span class="c1"># eq (5)</span>
        <span class="n">w_c</span> <span class="o">=</span> <span class="n">K_amplified</span> <span class="o">/</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">K_amplified</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># Sec 3.3.2 Focusing by Location</span>

        <span class="n">g</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># eq (7)</span>
        <span class="n">w_g</span> <span class="o">=</span> <span class="n">g</span> <span class="o">*</span> <span class="n">w_c</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">g</span><span class="p">)</span> <span class="o">*</span> <span class="n">prev_w</span>

        <span class="n">s</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span>
            <span class="n">s</span><span class="p">[:,</span> <span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">shift_range</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
            <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span>
                <span class="n">s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">value</span> <span class="ow">or</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">s</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span> <span class="o">-</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shift_range</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="p">]),</span> <span class="n">s</span><span class="p">[:,</span> <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">shift_range</span><span class="p">:]</span>
        <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">array_ops</span><span class="o">.</span><span class="n">reverse</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
             <span class="n">array_ops</span><span class="o">.</span><span class="n">reverse</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">])],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">s_matrix</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span>
            <span class="n">t</span><span class="p">[:,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span><span class="p">)</span>
        <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># eq (8)</span>
        <span class="n">w_</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span>
            <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">w_g</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">s_matrix</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">w_sharpen</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">w_</span><span class="p">,</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">gamma</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>

        <span class="c1"># eq (9)</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w_sharpen</span> <span class="o">/</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">w_sharpen</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">w</span>

    <span class="k">def</span> <span class="nf">zero_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">):</span>
        <span class="n">read_vector_list</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="n">w_list</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span><span class="p">])</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">read_head_num</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">write_head_num</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="n">controller_init_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">controller</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>

        <span class="n">M</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">memory_vector_dim</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">NTMControllerState</span><span class="p">(</span>
            <span class="n">controller_state</span><span class="o">=</span><span class="n">controller_init_state</span><span class="p">,</span>
            <span class="n">read_vector_list</span><span class="o">=</span><span class="n">read_vector_list</span><span class="p">,</span>
            <span class="n">w_list</span><span class="o">=</span><span class="n">w_list</span><span class="p">,</span>
            <span class="n">M</span><span class="o">=</span><span class="n">M</span><span class="p">,</span>
            <span class="n">time</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></div>


<div class="viewcode-block" id="MinimalRNNCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell">[docs]</a><span class="k">class</span> <span class="nc">MinimalRNNCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LayerRNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;MinimalRNN cell.</span>

<span class="sd">    The implementation is based on:</span>

<span class="sd">      https://arxiv.org/pdf/1806.05394v2.pdf</span>

<span class="sd">    Minmin Chen, Jeffrey Pennington, Samuel S. Schoenholz.</span>
<span class="sd">    &quot;Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal</span>
<span class="sd">     Propagation in Recurrent Neural Networks.&quot; ICML, 2018.</span>

<span class="sd">    A MinimalRNN cell first projects the input to the hidden space. The new</span>
<span class="sd">    hidden state is then calculated as a weighted sum of the projected input and</span>
<span class="sd">    the previous hidden state, using a single update gate.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">units</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;glorot_uniform&quot;</span><span class="p">,</span>
                 <span class="n">bias_initializer</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the parameters for a MinimalRNN cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          units: int, The number of units in the MinimalRNN cell.</span>
<span class="sd">          activation: Nonlinearity to use in the feedforward network. Default:</span>
<span class="sd">            `tanh`.</span>
<span class="sd">          kernel_initializer: The initializer to use for the weight in the update</span>
<span class="sd">            gate and feedforward network. Default: `glorot_uniform`.</span>
<span class="sd">          bias_initializer: The initializer to use for the bias in the update</span>
<span class="sd">            gate. Default: `ones`.</span>
<span class="sd">          name: String, the name of the cell.</span>
<span class="sd">          dtype: Default dtype of the cell.</span>
<span class="sd">          **kwargs: Dict, keyword named properties for common cell attributes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MinimalRNNCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Inputs must be 2-dimensional.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span> <span class="o">=</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">InputSpec</span><span class="p">(</span><span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">kernel_initializer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">bias_initializer</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected inputs.shape[-1] to be known, saw shape: </span><span class="si">%s</span><span class="s2">&quot;</span>
                             <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">))</span>

        <span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="c1"># self._kernel contains W_x, W, V</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_WEIGHTS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">input_size</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">name</span><span class="o">=</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_BIAS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">],</span>
            <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bias_initializer</span><span class="p">)</span>
        <span class="c1"># pylint: enable=protected-access</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

<div class="viewcode-block" id="MinimalRNNCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run one step of MinimalRNN.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: input Tensor, must be 2-D, `[batch, input_size]`.</span>
<span class="sd">          state: state Tensor, must be 2-D, `[batch, state_size]`.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tuple containing:</span>

<span class="sd">          - Output: A `2-D` tensor with shape `[batch_size, state_size]`.</span>
<span class="sd">          - New state: A `2-D` tensor with shape `[batch_size, state_size]`.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: If input size cannot be inferred from inputs via</span>
<span class="sd">            static shape inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not infer input size from inputs.get_shape()[-1]&quot;</span><span class="p">)</span>

        <span class="n">feedforward_weight</span><span class="p">,</span> <span class="n">gate_weight</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
            <span class="n">num_or_size_splits</span><span class="o">=</span><span class="p">[</span><span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">input_size</span><span class="p">),</span>
                                <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">feedforward</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">feedforward_weight</span><span class="p">)</span>
        <span class="n">feedforward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">feedforward</span><span class="p">)</span>

        <span class="n">gate_inputs</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span>
            <span class="n">array_ops</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">feedforward</span><span class="p">,</span> <span class="n">state</span><span class="p">],</span> <span class="mi">1</span><span class="p">),</span> <span class="n">gate_weight</span><span class="p">)</span>
        <span class="n">gate_inputs</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">gate_inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gate_inputs</span><span class="p">)</span>

        <span class="n">new_h</span> <span class="o">=</span> <span class="n">u</span> <span class="o">*</span> <span class="n">state</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span> <span class="o">*</span> <span class="n">feedforward</span>
        <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_h</span></div></div>


<div class="viewcode-block" id="CFNCell"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.CFNCell">[docs]</a><span class="k">class</span> <span class="nc">CFNCell</span><span class="p">(</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">LayerRNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Chaos Free Network cell.</span>

<span class="sd">    The implementation is based on:</span>

<span class="sd">      https://openreview.net/pdf?id=S1dIzvclg</span>

<span class="sd">    Thomas Laurent, James von Brecht.</span>
<span class="sd">    &quot;A recurrent neural network without chaos.&quot; ICLR, 2017.</span>

<span class="sd">    A CFN cell first projects the input to the hidden space. The hidden state</span>
<span class="sd">    goes through a contractive mapping. The new hidden state is then calculated</span>
<span class="sd">    as a linear combination of the projected input and the contracted previous</span>
<span class="sd">    hidden state, using decoupled input and forget gates.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">units</span><span class="p">,</span>
                 <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;tanh&quot;</span><span class="p">,</span>
                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;glorot_uniform&quot;</span><span class="p">,</span>
                 <span class="n">bias_initializer</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the parameters for a CFN cell.</span>

<span class="sd">        Args:</span>
<span class="sd">          units: int, The number of units in the CFN cell.</span>
<span class="sd">          activation: Nonlinearity to use. Default: `tanh`.</span>
<span class="sd">          kernel_initializer: Initializer for the `kernel` weights</span>
<span class="sd">            matrix. Default: `glorot_uniform`.</span>
<span class="sd">          bias_initializer: The initializer to use for the bias in the</span>
<span class="sd">            gates. Default: `ones`.</span>
<span class="sd">          name: String, the name of the cell.</span>
<span class="sd">          dtype: Default dtype of the cell.</span>
<span class="sd">          **kwargs: Dict, keyword named properties for common cell attributes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CFNCell</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="c1"># Inputs must be 2-dimensional.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_spec</span> <span class="o">=</span> <span class="n">input_spec</span><span class="o">.</span><span class="n">InputSpec</span><span class="p">(</span><span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">units</span> <span class="o">=</span> <span class="n">units</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">activations</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">activation</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">kernel_initializer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias_initializer</span> <span class="o">=</span> <span class="n">initializers</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">bias_initializer</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">state_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">output_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs_shape</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected inputs.shape[-1] to be known, saw shape: </span><span class="si">%s</span><span class="s2">&quot;</span>
                             <span class="o">%</span> <span class="nb">str</span><span class="p">(</span><span class="n">inputs_shape</span><span class="p">))</span>

        <span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs_shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="c1"># pylint: disable=protected-access</span>
        <span class="c1"># `self.kernel` contains V_{\theta}, V_{\eta}, W.</span>
        <span class="c1"># `self.recurrent_kernel` contains U_{\theta}, U_{\eta}.</span>
        <span class="c1"># `self.bias` contains b_{\theta}, b_{\eta}.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_WEIGHTS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recurrent_kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="s2">&quot;recurrent_</span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_WEIGHTS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_initializer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add_weight</span><span class="p">(</span>
            <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">],</span>
            <span class="n">name</span><span class="o">=</span><span class="n">rnn_cell_impl</span><span class="o">.</span><span class="n">_BIAS_VARIABLE_NAME</span><span class="p">,</span>
            <span class="n">initializer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">bias_initializer</span><span class="p">)</span>
        <span class="c1"># pylint: enable=protected-access</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">built</span> <span class="o">=</span> <span class="kc">True</span>

<div class="viewcode-block" id="CFNCell.call"><a class="viewcode-back" href="../../../../../../reference/autosummary/python/sparknlp/training/_tf_graph_builders/tf2contrib/rnn_cell/index.html#python.sparknlp.training._tf_graph_builders.tf2contrib.rnn_cell.CFNCell.call">[docs]</a>    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run one step of CFN.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: input Tensor, must be 2-D, `[batch, input_size]`.</span>
<span class="sd">          state: state Tensor, must be 2-D, `[batch, state_size]`.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A tuple containing:</span>

<span class="sd">          - Output: A `2-D` tensor with shape `[batch_size, state_size]`.</span>
<span class="sd">          - New state: A `2-D` tensor with shape `[batch_size, state_size]`.</span>

<span class="sd">        Raises:</span>
<span class="sd">          ValueError: If input size cannot be inferred from inputs via</span>
<span class="sd">            static shape inference.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">get_shape</span><span class="p">()[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">tensor_shape</span><span class="o">.</span><span class="n">dimension_value</span><span class="p">(</span><span class="n">input_size</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Could not infer input size from inputs.get_shape()[-1]&quot;</span><span class="p">)</span>

        <span class="c1"># The variable names u, v, w, b are consistent with the notations in the</span>
        <span class="c1"># original paper.</span>
        <span class="n">v</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span>
            <span class="n">value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span>
            <span class="n">num_or_size_splits</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">units</span><span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">recurrent_kernel</span>
        <span class="n">b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>

        <span class="n">gates</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">u</span><span class="p">)</span> <span class="o">+</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="n">gates</span> <span class="o">=</span> <span class="n">nn_ops</span><span class="o">.</span><span class="n">bias_add</span><span class="p">(</span><span class="n">gates</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">gates</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">)</span>
        <span class="n">theta</span><span class="p">,</span> <span class="n">eta</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">gates</span><span class="p">,</span>
                                     <span class="n">num_or_size_splits</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                     <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">proj_input</span> <span class="o">=</span> <span class="n">math_ops</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>

        <span class="c1"># The input gate is (1 - eta), which is different from the original paper.</span>
        <span class="c1"># This is for the propose of initialization. With the default</span>
        <span class="c1"># bias_initializer `ones`, the input gate is initialized to a small number.</span>
        <span class="n">new_h</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">eta</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span>
            <span class="n">proj_input</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">new_h</span><span class="p">,</span> <span class="n">new_h</span></div></div>
</pre></div>

            </article>
            
            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div id="searchbox"></div>
</div>

<div class="toc-item">
  
</div>

<div class="toc-item">
  
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
          </div>
        </footer>
        
      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../../../static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="../../../../../../static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  <footer class="bd-footer"><div class="bd-footer__inner container">
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2022, John Snow Labs.<br>

</p>

  </div>
  
  <div class="footer-item">
    <p class="theme-version">
    Built with the
    <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">
        PyData Sphinx Theme
    </a>
    0.12.0.
</p>
  </div>
  
  <div class="footer-item">
    
<p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.3.0.<br>
</p>

  </div>
  
</div>
  </footer>
  </body>
</html>