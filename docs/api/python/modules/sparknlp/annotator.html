
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>sparknlp.annotator &#8212; Spark NLP 3.3.4 documentation</title>
    
  <link href="../../static/css/theme.css" rel="stylesheet">
  <link href="../../static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../static/css/blank.css" />
    <link rel="stylesheet" type="text/css" href="../../static/css/custom.css" />
    
  <link rel="preload" as="script" href="../../static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../" id="documentation_options" src="../../static/documentation_options.js"></script>
    <script src="../../static/jquery.js"></script>
    <script src="../../static/underscore.js"></script>
    <script src="../../static/doctools.js"></script>
    <script src="../../static/toggleprompt.js"></script>
    <link rel="shortcut icon" href="../../static/fav.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../index.html">
  <img src="../../static/logo.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../getting_started/index.html">
  Getting Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../user_guide/index.html">
  User Guide
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../third_party/index.html">
  Third Party Projects
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../reference/index.html">
  API Reference
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for sparknlp.annotator</h1><div class="highlight"><pre>
<span></span><span class="c1">#  Copyright 2017-2021 John Snow Labs</span>
<span class="c1">#</span>
<span class="c1">#  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1">#  you may not use this file except in compliance with the License.</span>
<span class="c1">#  You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1">#  Unless required by applicable law or agreed to in writing, software</span>
<span class="c1">#  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1">#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1">#  See the License for the specific language governing permissions and</span>
<span class="c1">#  limitations under the License.</span>

<span class="sd">&quot;&quot;&quot;Module containing all available Annotators of Spark NLP and their base</span>
<span class="sd">classes.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>

<span class="c1"># Do NOT delete. Looks redundant but this is key work around for python 2 support.</span>
<span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">version_info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="n">DocumentAssembler</span><span class="p">,</span> <span class="n">Finisher</span><span class="p">,</span> <span class="n">EmbeddingsFinisher</span><span class="p">,</span> <span class="n">TokenAssembler</span>
<span class="k">else</span><span class="p">:</span>
    <span class="kn">import</span> <span class="nn">com.johnsnowlabs.nlp</span>

<span class="n">annotators</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">pos</span><span class="o">.</span><span class="n">perceptron</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">ner</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">ner</span><span class="o">.</span><span class="n">crf</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">ner</span><span class="o">.</span><span class="n">dl</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">regex</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">sbd</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">sbd</span><span class="o">.</span><span class="n">pragmatic</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">sda</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">sda</span><span class="o">.</span><span class="n">pragmatic</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">sda</span><span class="o">.</span><span class="n">vivekn</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">spell</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">spell</span><span class="o">.</span><span class="n">norvig</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">spell</span><span class="o">.</span><span class="n">symmetric</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">spell</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">parser</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">parser</span><span class="o">.</span><span class="n">dep</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">parser</span><span class="o">.</span><span class="n">typdep</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">dl</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">ld</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">ld</span><span class="o">.</span><span class="n">dl</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">keyword</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">keyword</span><span class="o">.</span><span class="n">yake</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">sentence_detector_dl</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">seq2seq</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">ws</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>
<span class="n">er</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">[</span><span class="vm">__name__</span><span class="p">]</span>


<div class="viewcode-block" id="RecursiveTokenizer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RecursiveTokenizer.html#sparknlp.annotator.RecursiveTokenizer">[docs]</a><span class="k">class</span> <span class="nc">RecursiveTokenizer</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Tokenizes raw text recursively based on a handful of definable rules.</span>

<span class="sd">    Unlike the Tokenizer, the RecursiveTokenizer operates based on these array</span>
<span class="sd">    string parameters only:</span>

<span class="sd">    - ``prefixes``: Strings that will be split when found at the beginning of</span>
<span class="sd">      token.</span>
<span class="sd">    - ``suffixes``: Strings that will be split when found at the end of token.</span>
<span class="sd">    - ``infixes``: Strings that will be split when found at the middle of token.</span>
<span class="sd">    - ``whitelist``: Whitelist of strings not to split</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/7.Context_Spell_Checker.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    prefixes</span>
<span class="sd">        Strings to be considered independent tokens when found at the beginning</span>
<span class="sd">        of a word, by default [&quot;&#39;&quot;, &#39;&quot;&#39;, &#39;(&#39;, &#39;[&#39;, &#39;\\n&#39;]</span>
<span class="sd">    suffixes</span>
<span class="sd">        Strings to be considered independent tokens when found at the end of a</span>
<span class="sd">        word, by default [&#39;.&#39;, &#39;:&#39;, &#39;%&#39;, &#39;,&#39;, &#39;;&#39;, &#39;?&#39;, &quot;&#39;&quot;, &#39;&quot;&#39;, &#39;)&#39;, &#39;]&#39;,</span>
<span class="sd">        &#39;\\n&#39;, &#39;!&#39;, &quot;&#39;s&quot;]</span>
<span class="sd">    infixes</span>
<span class="sd">        Strings to be considered independent tokens when found in the middle of</span>
<span class="sd">        a word, by default [&#39;\\n&#39;, &#39;(&#39;, &#39;)&#39;]</span>
<span class="sd">    whitelist</span>
<span class="sd">        Strings to be considered as single tokens , by default [&quot;it\&#39;s&quot;,</span>
<span class="sd">        &quot;that\&#39;s&quot;, &quot;there\&#39;s&quot;, &quot;he\&#39;s&quot;, &quot;she\&#39;s&quot;, &quot;what\&#39;s&quot;, &quot;let\&#39;s&quot;, &quot;who\&#39;s&quot;,</span>
<span class="sd">        &quot;It\&#39;s&quot;, &quot;That\&#39;s&quot;, &quot;There\&#39;s&quot;, &quot;He\&#39;s&quot;, &quot;She\&#39;s&quot;, &quot;What\&#39;s&quot;, &quot;Let\&#39;s&quot;,</span>
<span class="sd">        &quot;Who\&#39;s&quot;]</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = RecursiveTokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;One, after the Other, (and) again. PO, QAM,&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;token.result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------------+</span>
<span class="sd">    |result                                                            |</span>
<span class="sd">    +------------------------------------------------------------------+</span>
<span class="sd">    |[One, ,, after, the, Other, ,, (, and, ), again, ., PO, ,, QAM, ,]|</span>
<span class="sd">    +------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;RecursiveTokenizer&#39;</span>

    <span class="n">prefixes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;prefixes&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;strings to be considered independent tokens when found at the beginning of a word&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">suffixes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;suffixes&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;strings to be considered independent tokens when found at the end of a word&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">infixes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                    <span class="s2">&quot;infixes&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;strings to be considered independent tokens when found in the middle of a word&quot;</span><span class="p">,</span>
                    <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">whitelist</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;whitelist&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;strings to be considered as single tokens&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="RecursiveTokenizer.setPrefixes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RecursiveTokenizer.html#sparknlp.annotator.RecursiveTokenizer.setPrefixes">[docs]</a>    <span class="k">def</span> <span class="nf">setPrefixes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets strings to be considered independent tokens when found at the</span>
<span class="sd">        beginning of a word, by default [&quot;&#39;&quot;, &#39;&quot;&#39;, &#39;(&#39;, &#39;[&#39;, &#39;\\n&#39;].</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        p : List[str]</span>
<span class="sd">            Strings to be considered independent tokens when found at the</span>
<span class="sd">            beginning of a word</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">prefixes</span><span class="o">=</span><span class="n">p</span><span class="p">)</span></div>

<div class="viewcode-block" id="RecursiveTokenizer.setSuffixes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RecursiveTokenizer.html#sparknlp.annotator.RecursiveTokenizer.setSuffixes">[docs]</a>    <span class="k">def</span> <span class="nf">setSuffixes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets strings to be considered independent tokens when found at the</span>
<span class="sd">        end of a word, by default [&#39;.&#39;, &#39;:&#39;, &#39;%&#39;, &#39;,&#39;, &#39;;&#39;, &#39;?&#39;, &quot;&#39;&quot;, &#39;&quot;&#39;, &#39;)&#39;,</span>
<span class="sd">        &#39;]&#39;, &#39;\\n&#39;, &#39;!&#39;, &quot;&#39;s&quot;].</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        s : List[str]</span>
<span class="sd">            Strings to be considered independent tokens when found at the end of</span>
<span class="sd">            a word</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">suffixes</span><span class="o">=</span><span class="n">s</span><span class="p">)</span></div>

<div class="viewcode-block" id="RecursiveTokenizer.setInfixes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RecursiveTokenizer.html#sparknlp.annotator.RecursiveTokenizer.setInfixes">[docs]</a>    <span class="k">def</span> <span class="nf">setInfixes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets strings to be considered independent tokens when found in the</span>
<span class="sd">        middle of a word, by default [&#39;\\n&#39;, &#39;(&#39;, &#39;)&#39;].</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        i : List[str]</span>
<span class="sd">            Strings to be considered independent tokens when found in the middle</span>
<span class="sd">            of a word</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        [type]</span>
<span class="sd">            [description]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">infixes</span><span class="o">=</span><span class="n">i</span><span class="p">)</span></div>

<div class="viewcode-block" id="RecursiveTokenizer.setWhitelist"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RecursiveTokenizer.html#sparknlp.annotator.RecursiveTokenizer.setWhitelist">[docs]</a>    <span class="k">def</span> <span class="nf">setWhitelist</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets strings to be considered as single tokens, by default [&quot;it\&#39;s&quot;,</span>
<span class="sd">        &quot;that\&#39;s&quot;, &quot;there\&#39;s&quot;, &quot;he\&#39;s&quot;, &quot;she\&#39;s&quot;, &quot;what\&#39;s&quot;, &quot;let\&#39;s&quot;, &quot;who\&#39;s&quot;,</span>
<span class="sd">        &quot;It\&#39;s&quot;, &quot;That\&#39;s&quot;, &quot;There\&#39;s&quot;, &quot;He\&#39;s&quot;, &quot;She\&#39;s&quot;, &quot;What\&#39;s&quot;, &quot;Let\&#39;s&quot;,</span>
<span class="sd">        &quot;Who\&#39;s&quot;].</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        w : List[str]</span>
<span class="sd">            Strings to be considered as single tokens</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">whitelist</span><span class="o">=</span><span class="n">w</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.RecursiveTokenizer&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RecursiveTokenizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.RecursiveTokenizer&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">prefixes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;&#39;&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;(&quot;</span><span class="p">,</span> <span class="s2">&quot;[&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">],</span>
            <span class="n">infixes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;(&quot;</span><span class="p">,</span> <span class="s2">&quot;)&quot;</span><span class="p">],</span>
            <span class="n">suffixes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="s2">&quot;%&quot;</span><span class="p">,</span> <span class="s2">&quot;,&quot;</span><span class="p">,</span> <span class="s2">&quot;;&quot;</span><span class="p">,</span> <span class="s2">&quot;?&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;)&quot;</span><span class="p">,</span> <span class="s2">&quot;]&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;!&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;s&quot;</span><span class="p">],</span>
            <span class="n">whitelist</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;it&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;that&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;there&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;he&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;she&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;what&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;let&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;who&#39;s&quot;</span><span class="p">,</span> \
                       <span class="s2">&quot;It&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;That&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;There&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;He&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;She&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;What&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;Let&#39;s&quot;</span><span class="p">,</span> <span class="s2">&quot;Who&#39;s&quot;</span><span class="p">]</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">RecursiveTokenizerModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="RecursiveTokenizerModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RecursiveTokenizerModel.html#sparknlp.annotator.RecursiveTokenizerModel">[docs]</a><span class="k">class</span> <span class="nc">RecursiveTokenizerModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Instantiated model of the RecursiveTokenizer.</span>

<span class="sd">    This is the instantiated model of the :class:`.RecursiveTokenizer`.</span>
<span class="sd">    For training your own model, please see the documentation of that class.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;RecursiveTokenizerModel&#39;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.RecursiveTokenizerModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RecursiveTokenizerModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Tokenizer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer">[docs]</a><span class="k">class</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Tokenizes raw text in document type columns into TokenizedSentence .</span>

<span class="sd">    This class represents a non fitted tokenizer. Fitting it will cause the</span>
<span class="sd">    internal RuleFactory to construct the rules for tokenizing from the input</span>
<span class="sd">    configuration.</span>

<span class="sd">    Identifies tokens with tokenization open standards. A few rules will help</span>
<span class="sd">    customizing it if defaults do not fit user needs.</span>

<span class="sd">    For extended examples of usage see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    targetPattern</span>
<span class="sd">        Pattern to grab from text as token candidates, by default ``\\S+``</span>
<span class="sd">    prefixPattern</span>
<span class="sd">        Regex with groups and begins with ``\\A`` to match target prefix, by</span>
<span class="sd">        default ``\\A([^\\s\\w\\$\\.]*)``</span>
<span class="sd">    suffixPattern</span>
<span class="sd">        Regex with groups and ends with ``\\z`` to match target suffix, by</span>
<span class="sd">        default ``([^\\s\\w]?)([^\\s\\w]*)\\z``</span>
<span class="sd">    infixPatterns</span>
<span class="sd">        Regex patterns that match tokens within a single target. groups identify</span>
<span class="sd">        different sub-tokens. multiple defaults</span>
<span class="sd">    exceptions</span>
<span class="sd">        Words that won&#39;t be affected by tokenization rules</span>
<span class="sd">    exceptionsPath</span>
<span class="sd">        Path to file containing list of exceptions</span>
<span class="sd">    caseSensitiveExceptions</span>
<span class="sd">        Whether to care for case sensitiveness in exceptions, by default True</span>
<span class="sd">    contextChars</span>
<span class="sd">        Character list used to separate from token boundaries, by default [&#39;.&#39;,</span>
<span class="sd">        &#39;,&#39;, &#39;;&#39;, &#39;:&#39;, &#39;!&#39;, &#39;?&#39;, &#39;*&#39;, &#39;-&#39;, &#39;(&#39;, &#39;)&#39;, &#39;&quot;&#39;, &quot;&#39;&quot;]</span>
<span class="sd">    splitPattern</span>
<span class="sd">        Pattern to separate from the inside of tokens. Takes priority over</span>
<span class="sd">        splitChars.</span>
<span class="sd">    splitChars</span>
<span class="sd">        Character list used to separate from the inside of tokens</span>
<span class="sd">    minLength</span>
<span class="sd">        Set the minimum allowed length for each token, by default 0</span>
<span class="sd">    maxLength</span>
<span class="sd">        Set the maximum allowed length for each token, by default 99999</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;I&#39;d like to say we didn&#39;t expect that. Jane&#39;s boyfriend.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer().setInputCols([&quot;document&quot;]).setOutputCol(&quot;token&quot;).fit(data)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([documentAssembler, tokenizer]).fit(data)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;token.result&quot;).show(truncate=False)</span>
<span class="sd">    +-----------------------------------------------------------------------+</span>
<span class="sd">    |output                                                                 |</span>
<span class="sd">    +-----------------------------------------------------------------------+</span>
<span class="sd">    |[I&#39;d, like, to, say, we, didn&#39;t, expect, that, ., Jane&#39;s, boyfriend, .]|</span>
<span class="sd">    +-----------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">targetPattern</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;targetPattern&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;pattern to grab from text as token candidates. Defaults \S+&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">prefixPattern</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;prefixPattern&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;regex with groups and begins with \A to match target prefix. Defaults to \A([^\s\w\$\.]*)&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">suffixPattern</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;suffixPattern&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;regex with groups and ends with \z to match target suffix. Defaults to ([^\s\w]?)([^\s\w]*)\z&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">infixPatterns</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;infixPatterns&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;regex patterns that match tokens within a single target. groups identify different sub-tokens. multiple defaults&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">exceptions</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;exceptions&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;Words that won&#39;t be affected by tokenization rules&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">exceptionsPath</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;exceptionsPath&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;path to file containing list of exceptions&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">caseSensitiveExceptions</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                    <span class="s2">&quot;caseSensitiveExceptions&quot;</span><span class="p">,</span>
                                    <span class="s2">&quot;Whether to care for case sensitiveness in exceptions&quot;</span><span class="p">,</span>
                                    <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">contextChars</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;contextChars&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;character list used to separate from token boundaries&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">splitPattern</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;splitPattern&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;character list used to separate from the inside of tokens&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">splitChars</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;splitChars&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;character list used to separate from the inside of tokens&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">minLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;minLength&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Set the minimum allowed legth for each token&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">maxLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;maxLength&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Set the maximum allowed legth for each token&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;Tokenizer&#39;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Tokenizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.Tokenizer&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">targetPattern</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">S+&quot;</span><span class="p">,</span>
            <span class="n">contextChars</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;,&quot;</span><span class="p">,</span> <span class="s2">&quot;;&quot;</span><span class="p">,</span> <span class="s2">&quot;:&quot;</span><span class="p">,</span> <span class="s2">&quot;!&quot;</span><span class="p">,</span> <span class="s2">&quot;?&quot;</span><span class="p">,</span> <span class="s2">&quot;*&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot;(&quot;</span><span class="p">,</span> <span class="s2">&quot;)&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\&quot;</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;&#39;&quot;</span><span class="p">],</span>
            <span class="n">caseSensitiveExceptions</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">minLength</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">maxLength</span><span class="o">=</span><span class="mi">99999</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Tokenizer.getInfixPatterns"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.getInfixPatterns">[docs]</a>    <span class="k">def</span> <span class="nf">getInfixPatterns</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets regex patterns that match tokens within a single target. Groups</span>
<span class="sd">        identify different sub-tokens.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[str]</span>
<span class="sd">            The infix patterns</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="s2">&quot;infixPatterns&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.getSuffixPattern"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.getSuffixPattern">[docs]</a>    <span class="k">def</span> <span class="nf">getSuffixPattern</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets regex with groups and ends with ``\\z`` to match target suffix.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            The suffix pattern</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="s2">&quot;suffixPattern&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.getPrefixPattern"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.getPrefixPattern">[docs]</a>    <span class="k">def</span> <span class="nf">getPrefixPattern</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets regex with groups and begins with ``\\A`` to match target</span>
<span class="sd">        prefix.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            The prefix pattern</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="s2">&quot;prefixPattern&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.getContextChars"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.getContextChars">[docs]</a>    <span class="k">def</span> <span class="nf">getContextChars</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets character list used to separate from token boundaries.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[str]</span>
<span class="sd">            Character list used to separate from token boundaries</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="s2">&quot;contextChars&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.getSplitChars"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.getSplitChars">[docs]</a>    <span class="k">def</span> <span class="nf">getSplitChars</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets character list used to separate from the inside of tokens.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[str]</span>
<span class="sd">            Character list used to separate from the inside of tokens</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="s2">&quot;splitChars&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.setTargetPattern"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.setTargetPattern">[docs]</a>    <span class="k">def</span> <span class="nf">setTargetPattern</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets pattern to grab from text as token candidates, by default</span>
<span class="sd">        ``\\S+``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Pattern to grab from text as token candidates</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">targetPattern</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.setPrefixPattern"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.setPrefixPattern">[docs]</a>    <span class="k">def</span> <span class="nf">setPrefixPattern</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets regex with groups and begins with ``\\A`` to match target prefix, by</span>
<span class="sd">        default ``\\A([^\\s\\w\\$\\.]*)``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Regex with groups and begins with ``\\A`` to match target prefix</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">prefixPattern</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.setSuffixPattern"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.setSuffixPattern">[docs]</a>    <span class="k">def</span> <span class="nf">setSuffixPattern</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets regex with groups and ends with ``\\z`` to match target suffix,</span>
<span class="sd">        by default ``([^\\s\\w]?)([^\\s\\w]*)\\z``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Regex with groups and ends with ``\\z`` to match target suffix</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">suffixPattern</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.setInfixPatterns"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.setInfixPatterns">[docs]</a>    <span class="k">def</span> <span class="nf">setInfixPatterns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets regex patterns that match tokens within a single target. Groups</span>
<span class="sd">        identify different sub-tokens.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Regex patterns that match tokens within a single target</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">infixPatterns</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.addInfixPattern"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.addInfixPattern">[docs]</a>    <span class="k">def</span> <span class="nf">addInfixPattern</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds an additional regex pattern that match tokens within a single</span>
<span class="sd">        target. Groups identify different sub-tokens.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Regex pattern that match tokens within a single target</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">infix_patterns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getInfixPatterns</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="n">infix_patterns</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">infix_patterns</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">infixPatterns</span><span class="o">=</span><span class="n">infix_patterns</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.setExceptions"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.setExceptions">[docs]</a>    <span class="k">def</span> <span class="nf">setExceptions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets words that won&#39;t be affected by tokenization rules.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Words that won&#39;t be affected by tokenization rules</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">exceptions</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.getExceptions"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.getExceptions">[docs]</a>    <span class="k">def</span> <span class="nf">getExceptions</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets words that won&#39;t be affected by tokenization rules.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[str]</span>
<span class="sd">            Words that won&#39;t be affected by tokenization rules</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="s2">&quot;exceptions&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.addException"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.addException">[docs]</a>    <span class="k">def</span> <span class="nf">addException</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds an additional word that won&#39;t be affected by tokenization rules.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Additional word that won&#39;t be affected by tokenization rules</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">exception_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getExceptions</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="n">exception_tokens</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">exception_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">exceptions</span><span class="o">=</span><span class="n">exception_tokens</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.setCaseSensitiveExceptions"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.setCaseSensitiveExceptions">[docs]</a>    <span class="k">def</span> <span class="nf">setCaseSensitiveExceptions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to care for case sensitiveness in exceptions, by default</span>
<span class="sd">        True.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to care for case sensitiveness in exceptions</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">caseSensitiveExceptions</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.getCaseSensitiveExceptions"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.getCaseSensitiveExceptions">[docs]</a>    <span class="k">def</span> <span class="nf">getCaseSensitiveExceptions</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets whether to care for case sensitiveness in exceptions.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        bool</span>
<span class="sd">            Whether to care for case sensitiveness in exceptions</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="s2">&quot;caseSensitiveExceptions&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.setContextChars"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.setContextChars">[docs]</a>    <span class="k">def</span> <span class="nf">setContextChars</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets character list used to separate from token boundaries, by</span>
<span class="sd">        default [&#39;.&#39;, &#39;,&#39;, &#39;;&#39;, &#39;:&#39;, &#39;!&#39;, &#39;?&#39;, &#39;*&#39;, &#39;-&#39;, &#39;(&#39;, &#39;)&#39;, &#39;&quot;&#39;, &quot;&#39;&quot;].</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Character list used to separate from token boundaries</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">contextChars</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.addContextChars"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.addContextChars">[docs]</a>    <span class="k">def</span> <span class="nf">addContextChars</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds an additional character to the list used to separate from token</span>
<span class="sd">        boundaries.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Additional context character</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">context_chars</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getContextChars</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="n">context_chars</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">context_chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">contextChars</span><span class="o">=</span><span class="n">context_chars</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.setSplitPattern"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.setSplitPattern">[docs]</a>    <span class="k">def</span> <span class="nf">setSplitPattern</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets pattern to separate from the inside of tokens. Takes priority</span>
<span class="sd">        over splitChars.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Pattern used to separate from the inside of tokens</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">splitPattern</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.setSplitChars"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.setSplitChars">[docs]</a>    <span class="k">def</span> <span class="nf">setSplitChars</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets character list used to separate from the inside of tokens.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Character list used to separate from the inside of tokens</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">splitChars</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.addSplitChars"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.addSplitChars">[docs]</a>    <span class="k">def</span> <span class="nf">addSplitChars</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds an additional character to separate from the inside of tokens.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Additional character to separate from the inside of tokens</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">split_chars</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getSplitChars</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="n">split_chars</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">split_chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">splitChars</span><span class="o">=</span><span class="n">split_chars</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.setMinLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.setMinLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMinLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the minimum allowed legth for each token, by default 0.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Minimum allowed legth for each token</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">minLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tokenizer.setMaxLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Tokenizer.html#sparknlp.annotator.Tokenizer.setMaxLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the maximum allowed legth for each token, by default 99999.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Maximum allowed legth for each token</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">TokenizerModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="TokenizerModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TokenizerModel.html#sparknlp.annotator.TokenizerModel">[docs]</a><span class="k">class</span> <span class="nc">TokenizerModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Tokenizes raw text into word pieces, tokens. Identifies tokens with</span>
<span class="sd">    tokenization open standards. A few rules will help customizing it if</span>
<span class="sd">    defaults do not fit user needs.</span>

<span class="sd">    This class represents an already fitted :class:`.Tokenizer`.</span>

<span class="sd">    See the main class Tokenizer for more examples of usage.</span>

<span class="sd">    ======================  ======================</span>
<span class="sd">    Input Annotation types  Output Annotation type</span>
<span class="sd">    ======================  ======================</span>
<span class="sd">    ``DOCUMENT``            ``TOKEN``</span>
<span class="sd">    ======================  ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    splitPattern</span>
<span class="sd">        Character list used to separate from the inside of tokens</span>
<span class="sd">    splitChars</span>
<span class="sd">        Character list used to separate from the inside of tokens</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;TokenizerModel&quot;</span>

    <span class="n">exceptions</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;exceptions&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;Words that won&#39;t be affected by tokenization rules&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">caseSensitiveExceptions</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                    <span class="s2">&quot;caseSensitiveExceptions&quot;</span><span class="p">,</span>
                                    <span class="s2">&quot;Whether to care for case sensitiveness in exceptions&quot;</span><span class="p">,</span>
                                    <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">targetPattern</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;targetPattern&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;pattern to grab from text as token candidates. Defaults \S+&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">rules</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                  <span class="s2">&quot;rules&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;Rules structure factory containing pre processed regex rules&quot;</span><span class="p">,</span>
                  <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">splitPattern</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;splitPattern&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;character list used to separate from the inside of tokens&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">splitChars</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;splitChars&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;character list used to separate from the inside of tokens&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.TokenizerModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TokenizerModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">targetPattern</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">S+&quot;</span><span class="p">,</span>
            <span class="n">caseSensitiveExceptions</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="TokenizerModel.setSplitPattern"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TokenizerModel.html#sparknlp.annotator.TokenizerModel.setSplitPattern">[docs]</a>    <span class="k">def</span> <span class="nf">setSplitPattern</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets pattern to separate from the inside of tokens. Takes priority</span>
<span class="sd">        over splitChars.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Pattern used to separate from the inside of tokens</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">splitPattern</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="TokenizerModel.setSplitChars"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TokenizerModel.html#sparknlp.annotator.TokenizerModel.setSplitChars">[docs]</a>    <span class="k">def</span> <span class="nf">setSplitChars</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets character list used to separate from the inside of tokens.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Character list used to separate from the inside of tokens</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">splitChars</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="TokenizerModel.addSplitChars"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TokenizerModel.html#sparknlp.annotator.TokenizerModel.addSplitChars">[docs]</a>    <span class="k">def</span> <span class="nf">addSplitChars</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds an additional character to separate from the inside of tokens.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Additional character to separate from the inside of tokens</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">split_chars</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getSplitChars</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="n">split_chars</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">split_chars</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">splitChars</span><span class="o">=</span><span class="n">split_chars</span><span class="p">)</span></div>

<div class="viewcode-block" id="TokenizerModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TokenizerModel.html#sparknlp.annotator.TokenizerModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;token_rules&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;token_rules&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        TokenizerModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">TokenizerModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="RegexTokenizer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RegexTokenizer.html#sparknlp.annotator.RegexTokenizer">[docs]</a><span class="k">class</span> <span class="nc">RegexTokenizer</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A tokenizer that splits text by a regex pattern.</span>

<span class="sd">    The pattern needs to be set with :meth:`.setPattern` and this sets the</span>
<span class="sd">    delimiting pattern or how the tokens should be split. By default this</span>
<span class="sd">    pattern is ``\\s+`` which means that tokens should be split by 1 or more</span>
<span class="sd">    whitespace characters.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    minLength</span>
<span class="sd">        Set the minimum allowed legth for each token, by default 1</span>
<span class="sd">    maxLength</span>
<span class="sd">        Set the maximum allowed legth for each token</span>
<span class="sd">    toLowercase</span>
<span class="sd">        Indicates whether to convert all characters to lowercase before</span>
<span class="sd">        tokenizing, by default False</span>
<span class="sd">    pattern</span>
<span class="sd">        Regex pattern used for tokenizing, by default ``\\s+``</span>
<span class="sd">    positionalMask</span>
<span class="sd">        Using a positional mask to guarantee the incremental progression of the</span>
<span class="sd">        tokenization, by default False</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; regexTokenizer = RegexTokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;regexToken&quot;) \\</span>
<span class="sd">    ...     .setToLowercase(True) \\</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       regexTokenizer</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is my first sentence.\\nThis is my second.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;regexToken.result&quot;).show(truncate=False)</span>
<span class="sd">    +-------------------------------------------------------+</span>
<span class="sd">    |result                                                 |</span>
<span class="sd">    +-------------------------------------------------------+</span>
<span class="sd">    |[this, is, my, first, sentence., this, is, my, second.]|</span>
<span class="sd">    +-------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;RegexTokenizer&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RegexTokenizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.RegexTokenizer&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">inputCols</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;document&quot;</span><span class="p">],</span>
            <span class="n">outputCol</span><span class="o">=</span><span class="s2">&quot;regexToken&quot;</span><span class="p">,</span>
            <span class="n">toLowercase</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">minLength</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">pattern</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\\</span><span class="s2">s+&quot;</span><span class="p">,</span>
            <span class="n">positionalMask</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

    <span class="n">minLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;minLength&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Set the minimum allowed legth for each token&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">maxLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;maxLength&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Set the maximum allowed legth for each token&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">toLowercase</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;toLowercase&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;Indicates whether to convert all characters to lowercase before tokenizing.&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">pattern</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                    <span class="s2">&quot;pattern&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;regex pattern used for tokenizing. Defaults \S+&quot;</span><span class="p">,</span>
                    <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">positionalMask</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;positionalMask&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;Using a positional mask to guarantee the incremental progression of the tokenization.&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

<div class="viewcode-block" id="RegexTokenizer.setMinLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RegexTokenizer.html#sparknlp.annotator.RegexTokenizer.setMinLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMinLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the minimum allowed legth for each token, by default 1.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Minimum allowed legth for each token</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">minLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="RegexTokenizer.setMaxLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RegexTokenizer.html#sparknlp.annotator.RegexTokenizer.setMaxLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the maximum allowed legth for each token.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Maximum allowed legth for each token</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="RegexTokenizer.setToLowercase"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RegexTokenizer.html#sparknlp.annotator.RegexTokenizer.setToLowercase">[docs]</a>    <span class="k">def</span> <span class="nf">setToLowercase</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to convert all characters to lowercase before</span>
<span class="sd">        tokenizing, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to convert all characters to lowercase before tokenizing</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">toLowercase</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="RegexTokenizer.setPattern"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RegexTokenizer.html#sparknlp.annotator.RegexTokenizer.setPattern">[docs]</a>    <span class="k">def</span> <span class="nf">setPattern</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the regex pattern used for tokenizing, by default ``\\s+``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Regex pattern used for tokenizing</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">pattern</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="RegexTokenizer.setPositionalMask"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RegexTokenizer.html#sparknlp.annotator.RegexTokenizer.setPositionalMask">[docs]</a>    <span class="k">def</span> <span class="nf">setPositionalMask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to use a positional mask to guarantee the incremental</span>
<span class="sd">        progression of the tokenization, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to use a positional mask</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">positionalMask</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ChunkTokenizer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ChunkTokenizer.html#sparknlp.annotator.ChunkTokenizer">[docs]</a><span class="k">class</span> <span class="nc">ChunkTokenizer</span><span class="p">(</span><span class="n">Tokenizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Tokenizes and flattens extracted NER chunks.</span>

<span class="sd">    The ChunkTokenizer will split the extracted NER ``CHUNK`` type Annotations</span>
<span class="sd">    and will create ``TOKEN`` type Annotations.</span>
<span class="sd">    The result is then flattened, resulting in a single array.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``CHUNK``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; sparknlp.common import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentenceDetector = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; entityExtractor = TextMatcher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setEntities(&quot;src/test/resources/entity-extractor/test-chunks.txt&quot;, ReadAs.TEXT) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;entity&quot;)</span>
<span class="sd">    &gt;&gt;&gt; chunkTokenizer = ChunkTokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;entity&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;chunk_token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...         documentAssembler,</span>
<span class="sd">    ...         sentenceDetector,</span>
<span class="sd">    ...         tokenizer,</span>
<span class="sd">    ...         entityExtractor,</span>
<span class="sd">    ...         chunkTokenizer</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([</span>
<span class="sd">    ...     [&quot;Hello world, my name is Michael, I am an artist and I work at Benezar&quot;],</span>
<span class="sd">    ...     [&quot;Robert, an engineer from Farendell, graduated last year. The other one, Lucas, graduated last week.&quot;]</span>
<span class="sd">    &gt;&gt;&gt; ]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;entity.result as entity&quot; , &quot;chunk_token.result as chunk_token&quot;).show(truncate=False)</span>
<span class="sd">    +-----------------------------------------------+---------------------------------------------------+</span>
<span class="sd">    |entity                                         |chunk_token                                        |</span>
<span class="sd">    +-----------------------------------------------+---------------------------------------------------+</span>
<span class="sd">    |[world, Michael, work at Benezar]              |[world, Michael, work, at, Benezar]                |</span>
<span class="sd">    |[engineer from Farendell, last year, last week]|[engineer, from, Farendell, last, year, last, week]|</span>
<span class="sd">    +-----------------------------------------------+---------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;ChunkTokenizer&#39;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Tokenizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.ChunkTokenizer&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ChunkTokenizerModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="ChunkTokenizerModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ChunkTokenizerModel.html#sparknlp.annotator.ChunkTokenizerModel">[docs]</a><span class="k">class</span> <span class="nc">ChunkTokenizerModel</span><span class="p">(</span><span class="n">TokenizerModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Instantiated model of the ChunkTokenizer.</span>

<span class="sd">    This is the instantiated model of the :class:`.ChunkTokenizer`.</span>
<span class="sd">    For training your own model, please see the documentation of that class.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``CHUNK``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;ChunkTokenizerModel&#39;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.ChunkTokenizerModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TokenizerModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Token2Chunk"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Token2Chunk.html#sparknlp.annotator.Token2Chunk">[docs]</a><span class="k">class</span> <span class="nc">Token2Chunk</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts ``TOKEN`` type Annotations to ``CHUNK`` type.</span>

<span class="sd">    This can be useful if a entities have been already extracted as ``TOKEN``</span>
<span class="sd">    and following annotators require ``CHUNK`` types.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``CHUNK``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; token2chunk = Token2Chunk() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;chunk&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     token2chunk</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;One Two Three Four&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(chunk) as result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------+</span>
<span class="sd">    |result                                    |</span>
<span class="sd">    +------------------------------------------+</span>
<span class="sd">    |[chunk, 0, 2, One, [sentence -&gt; 0], []]   |</span>
<span class="sd">    |[chunk, 4, 6, Two, [sentence -&gt; 0], []]   |</span>
<span class="sd">    |[chunk, 8, 12, Three, [sentence -&gt; 0], []]|</span>
<span class="sd">    |[chunk, 14, 17, Four, [sentence -&gt; 0], []]|</span>
<span class="sd">    +------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Token2Chunk&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Token2Chunk</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.Token2Chunk&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="Stemmer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Stemmer.html#sparknlp.annotator.Stemmer">[docs]</a><span class="k">class</span> <span class="nc">Stemmer</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns hard-stems out of words with the objective of retrieving the</span>
<span class="sd">    meaningful part of the word.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; stemmer = Stemmer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;stem&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     stemmer</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]) \\</span>
<span class="sd">    ...     .toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;stem.result&quot;).show(truncate = False)</span>
<span class="sd">    +-------------------------------------------------------------+</span>
<span class="sd">    |result                                                       |</span>
<span class="sd">    +-------------------------------------------------------------+</span>
<span class="sd">    |[peter, piper, employe, ar, pick, peck, of, pickl, pepper, .]|</span>
<span class="sd">    +-------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">language</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;language&quot;</span><span class="p">,</span> <span class="s2">&quot;stemmer algorithm&quot;</span><span class="p">,</span> <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Stemmer&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Stemmer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.Stemmer&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">language</span><span class="o">=</span><span class="s2">&quot;english&quot;</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="Chunker"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Chunker.html#sparknlp.annotator.Chunker">[docs]</a><span class="k">class</span> <span class="nc">Chunker</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This annotator matches a pattern of part-of-speech tags in order to</span>
<span class="sd">    return meaningful phrases from document. Extracted part-of-speech tags are</span>
<span class="sd">    mapped onto the sentence, which can then be parsed by regular expressions.</span>
<span class="sd">    The part-of-speech tags are wrapped by angle brackets ``&lt;&gt;`` to be easily</span>
<span class="sd">    distinguishable in the text itself.</span>

<span class="sd">    This example sentence will result in the form:</span>

<span class="sd">    .. code-block:: none</span>

<span class="sd">        &quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;</span>
<span class="sd">        &quot;&lt;NNP&gt;&lt;NNP&gt;&lt;NNS&gt;&lt;VBP&gt;&lt;VBG&gt;&lt;NNS&gt;&lt;IN&gt;&lt;JJ&gt;&lt;NNS&gt;&lt;.&gt;&quot;</span>


<span class="sd">    To then extract these tags, ``regexParsers`` need to be set with e.g.:</span>

<span class="sd">    &gt;&gt;&gt; chunker = Chunker() \\</span>
<span class="sd">    ...    .setInputCols([&quot;sentence&quot;, &quot;pos&quot;]) \\</span>
<span class="sd">    ...    .setOutputCol(&quot;chunk&quot;) \\</span>
<span class="sd">    ...    .setRegexParsers([&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;])</span>

<span class="sd">    When defining the regular expressions, tags enclosed in angle brackets are</span>
<span class="sd">    treated as groups, so here specifically ``&quot;&lt;NNP&gt;+&quot;`` means 1 or more nouns</span>
<span class="sd">    in succession.</span>

<span class="sd">    For more extended examples see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, POS``      ``CHUNK``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    regexParsers</span>
<span class="sd">        An array of grammar based chunk parsers</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; POSTag = PerceptronModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;, &quot;token&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;pos&quot;)</span>
<span class="sd">    &gt;&gt;&gt; chunker = Chunker() \\</span>
<span class="sd">    ...     .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;chunk&quot;) \\</span>
<span class="sd">    ...     .setRegexParsers([&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;])</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       sentence,</span>
<span class="sd">    ...       tokenizer,</span>
<span class="sd">    ...       POSTag,</span>
<span class="sd">    ...       chunker</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(chunk) as result&quot;).show(truncate=False)</span>
<span class="sd">    +-------------------------------------------------------------+</span>
<span class="sd">    |result                                                       |</span>
<span class="sd">    +-------------------------------------------------------------+</span>
<span class="sd">    |[chunk, 0, 11, Peter Pipers, [sentence -&gt; 0, chunk -&gt; 0], []]|</span>
<span class="sd">    |[chunk, 13, 21, employees, [sentence -&gt; 0, chunk -&gt; 1], []]  |</span>
<span class="sd">    |[chunk, 35, 39, pecks, [sentence -&gt; 0, chunk -&gt; 2], []]      |</span>
<span class="sd">    |[chunk, 52, 58, peppers, [sentence -&gt; 0, chunk -&gt; 3], []]    |</span>
<span class="sd">    +-------------------------------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    PerceptronModel : for Part-Of-Speech tagging</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">regexParsers</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;regexParsers&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;an array of grammar based chunk parsers&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Chunker&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Chunker</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.Chunker&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="Chunker.setRegexParsers"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Chunker.html#sparknlp.annotator.Chunker.setRegexParsers">[docs]</a>    <span class="k">def</span> <span class="nf">setRegexParsers</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets an array of grammar based chunk parsers.</span>

<span class="sd">        POS classes should be enclosed in angle brackets, then treated as</span>
<span class="sd">        groups.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Array of grammar based chunk parsers</span>


<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; chunker = Chunker() \\</span>
<span class="sd">        ...     .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) \\</span>
<span class="sd">        ...     .setOutputCol(&quot;chunk&quot;) \\</span>
<span class="sd">        ...     .setRegexParsers([&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;])</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">regexParsers</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="DocumentNormalizer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DocumentNormalizer.html#sparknlp.annotator.DocumentNormalizer">[docs]</a><span class="k">class</span> <span class="nc">DocumentNormalizer</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Annotator which normalizes raw text from tagged text, e.g. scraped web</span>
<span class="sd">    pages or xml documents, from document type columns into Sentence.</span>

<span class="sd">    Removes all dirty characters from text following one or more input regex</span>
<span class="sd">    patterns. Can apply not wanted character removal with a specific policy.</span>
<span class="sd">    Can apply lower case normalization.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``DOCUMENT``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    action</span>
<span class="sd">        action to perform before applying regex patterns on text, by default</span>
<span class="sd">        &quot;clean&quot;</span>
<span class="sd">    patterns</span>
<span class="sd">        normalization regex patterns which match will be removed from document,</span>
<span class="sd">        by default [&#39;&lt;[^&gt;]*&gt;&#39;]</span>
<span class="sd">    replacement</span>
<span class="sd">        replacement string to apply when regexes match, by default &quot; &quot;</span>
<span class="sd">    lowercase</span>
<span class="sd">        whether to convert strings to lowercase, by default False</span>
<span class="sd">    policy</span>
<span class="sd">        policy to remove pattern from text, by default &quot;pretty_all&quot;</span>
<span class="sd">    encoding</span>
<span class="sd">        file encoding to apply on normalized documents, by default &quot;UTF-8&quot;</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; cleanUpPatterns = [&quot;&lt;[^&gt;]&gt;&quot;]</span>
<span class="sd">    &gt;&gt;&gt; documentNormalizer = DocumentNormalizer() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;normalizedDocument&quot;) \\</span>
<span class="sd">    ...     .setAction(&quot;clean&quot;) \\</span>
<span class="sd">    ...     .setPatterns(cleanUpPatterns) \\</span>
<span class="sd">    ...     .setReplacement(&quot; &quot;) \\</span>
<span class="sd">    ...     .setPolicy(&quot;pretty_all&quot;) \\</span>
<span class="sd">    ...     .setLowercase(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     documentNormalizer</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; text = \&quot;\&quot;\&quot;</span>
<span class="sd">    ... &lt;div id=&quot;theworldsgreatest&quot; class=&#39;my-right my-hide-small my-wide toptext&#39; style=&quot;font-family:&#39;Segoe UI&#39;,Arial,sans-serif&quot;&gt;</span>
<span class="sd">    ...     THE WORLD&#39;S LARGEST WEB DEVELOPER SITE</span>
<span class="sd">    ...     &lt;h1 style=&quot;font-size:300%;&quot;&gt;THE WORLD&#39;S LARGEST WEB DEVELOPER SITE&lt;/h1&gt;</span>
<span class="sd">    ...     &lt;p style=&quot;font-size:160%;&quot;&gt;Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..&lt;/p&gt;</span>
<span class="sd">    ... &lt;/div&gt;</span>
<span class="sd">    ... &lt;/div&gt;\&quot;\&quot;\&quot;</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[text]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(data)</span>
<span class="sd">    &gt;&gt;&gt; result = pipelineModel.transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;normalizedDocument.result&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |</span>
<span class="sd">    +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="sd">    |[ the world&#39;s largest web developer site the world&#39;s largest web developer site lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum..]|</span>
<span class="sd">    +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">action</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                   <span class="s2">&quot;action&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;action to perform applying regex patterns on text&quot;</span><span class="p">,</span>
                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">patterns</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;patterns&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;normalization regex patterns which match will be removed from document. Defaults is &lt;[^&gt;]*&gt;&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">replacement</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;replacement&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;replacement string to apply when regexes match&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">lowercase</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;lowercase&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;whether to convert strings to lowercase&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">policy</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                   <span class="s2">&quot;policy&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;policy to remove pattern from text&quot;</span><span class="p">,</span>
                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">encoding</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;encoding&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;file encoding to apply on normalized documents&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DocumentNormalizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.DocumentNormalizer&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">action</span><span class="o">=</span><span class="s2">&quot;clean&quot;</span><span class="p">,</span>
            <span class="n">patterns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;&lt;[^&gt;]*&gt;&quot;</span><span class="p">],</span>
            <span class="n">replacement</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">,</span>
            <span class="n">lowercase</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">policy</span><span class="o">=</span><span class="s2">&quot;pretty_all&quot;</span><span class="p">,</span>
            <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;UTF-8&quot;</span>
        <span class="p">)</span>

<div class="viewcode-block" id="DocumentNormalizer.setAction"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DocumentNormalizer.html#sparknlp.annotator.DocumentNormalizer.setAction">[docs]</a>    <span class="k">def</span> <span class="nf">setAction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets action to perform before applying regex patterns on text, by</span>
<span class="sd">        default &quot;clean&quot;.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Action to perform before applying regex patterns</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="DocumentNormalizer.setPatterns"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DocumentNormalizer.html#sparknlp.annotator.DocumentNormalizer.setPatterns">[docs]</a>    <span class="k">def</span> <span class="nf">setPatterns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets normalization regex patterns which match will be removed from</span>
<span class="sd">        document, by default [&#39;&lt;[^&gt;]*&gt;&#39;].</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Normalization regex patterns which match will be removed from</span>
<span class="sd">            document</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">patterns</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="DocumentNormalizer.setReplacement"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DocumentNormalizer.html#sparknlp.annotator.DocumentNormalizer.setReplacement">[docs]</a>    <span class="k">def</span> <span class="nf">setReplacement</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets replacement string to apply when regexes match, by default &quot; &quot;.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Replacement string to apply when regexes match</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">replacement</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="DocumentNormalizer.setLowercase"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DocumentNormalizer.html#sparknlp.annotator.DocumentNormalizer.setLowercase">[docs]</a>    <span class="k">def</span> <span class="nf">setLowercase</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to convert strings to lowercase, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to convert strings to lowercase, by default False</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">lowercase</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="DocumentNormalizer.setPolicy"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DocumentNormalizer.html#sparknlp.annotator.DocumentNormalizer.setPolicy">[docs]</a>    <span class="k">def</span> <span class="nf">setPolicy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets policy to remove pattern from text, by default &quot;pretty_all&quot;.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Policy to remove pattern from text, by default &quot;pretty_all&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">policy</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="DocumentNormalizer.setEncoding"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DocumentNormalizer.html#sparknlp.annotator.DocumentNormalizer.setEncoding">[docs]</a>    <span class="k">def</span> <span class="nf">setEncoding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets file encoding to apply on normalized documents, by default</span>
<span class="sd">        &quot;UTF-8&quot;.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            File encoding to apply on normalized documents, by default &quot;UTF-8&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">encoding</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Normalizer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Normalizer.html#sparknlp.annotator.Normalizer">[docs]</a><span class="k">class</span> <span class="nc">Normalizer</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Annotator that cleans out tokens. Requires stems, hence tokens. Removes</span>
<span class="sd">    all dirty characters from text following a regex pattern and transforms</span>
<span class="sd">    words based on a provided dictionary</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cleanupPatterns</span>
<span class="sd">        Normalization regex patterns which match will be removed from token, by default [&#39;[^\\pL+]&#39;]</span>
<span class="sd">    lowercase</span>
<span class="sd">        Whether to convert strings to lowercase, by default False</span>
<span class="sd">    slangDictionary</span>
<span class="sd">        Slang dictionary is a delimited text. needs &#39;delimiter&#39; in options</span>
<span class="sd">    minLength</span>
<span class="sd">        The minimum allowed legth for each token, by default 0</span>
<span class="sd">    maxLength</span>
<span class="sd">        The maximum allowed legth for each token</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; normalizer = Normalizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;normalized&quot;) \\</span>
<span class="sd">    ...     .setLowercase(True) \\</span>
<span class="sd">    ...     .setCleanupPatterns([\&quot;\&quot;\&quot;[^\\w\\d\\s]\&quot;\&quot;\&quot;])</span>

<span class="sd">    The pattern removes punctuations (keeps alphanumeric chars). If we don&#39;t set</span>
<span class="sd">    CleanupPatterns, it will only keep alphabet letters ([^A-Za-z])</span>

<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     normalizer</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;John and Peter are brothers. However they don&#39;t support each other that much.&quot;]]) \\</span>
<span class="sd">    ...     .toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;normalized.result&quot;).show(truncate = False)</span>
<span class="sd">    +----------------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                                  |</span>
<span class="sd">    +----------------------------------------------------------------------------------------+</span>
<span class="sd">    |[john, and, peter, are, brothers, however, they, dont, support, each, other, that, much]|</span>
<span class="sd">    +----------------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">cleanupPatterns</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;cleanupPatterns&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;normalization regex patterns which match will be removed from token&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">lowercase</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;lowercase&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;whether to convert strings to lowercase&quot;</span><span class="p">)</span>

    <span class="n">slangMatchCase</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;slangMatchCase&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;whether or not to be case sensitive to match slangs. Defaults to false.&quot;</span><span class="p">)</span>

    <span class="n">slangDictionary</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;slangDictionary&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;slang dictionary is a delimited text. needs &#39;delimiter&#39; in options&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">minLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;minLength&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Set the minimum allowed legth for each token&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">maxLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;maxLength&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Set the maximum allowed legth for each token&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Normalizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.Normalizer&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">cleanupPatterns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;[^</span><span class="se">\\</span><span class="s2">pL+]&quot;</span><span class="p">],</span>
            <span class="n">lowercase</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">slangMatchCase</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">minLength</span><span class="o">=</span><span class="mi">0</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Normalizer.setCleanupPatterns"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Normalizer.html#sparknlp.annotator.Normalizer.setCleanupPatterns">[docs]</a>    <span class="k">def</span> <span class="nf">setCleanupPatterns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets normalization regex patterns which match will be removed from</span>
<span class="sd">        token, by default [&#39;[^\\pL+]&#39;].</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Normalization regex patterns which match will be removed from token</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">cleanupPatterns</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Normalizer.setLowercase"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Normalizer.html#sparknlp.annotator.Normalizer.setLowercase">[docs]</a>    <span class="k">def</span> <span class="nf">setLowercase</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to convert strings to lowercase, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to convert strings to lowercase, by default False</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">lowercase</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Normalizer.setSlangDictionary"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Normalizer.html#sparknlp.annotator.Normalizer.setSlangDictionary">[docs]</a>    <span class="k">def</span> <span class="nf">setSlangDictionary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">delimiter</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets slang dictionary is a delimited text. Needs &#39;delimiter&#39; in</span>
<span class="sd">        options.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the source files</span>
<span class="sd">        delimiter : str</span>
<span class="sd">            Delimiter for the values</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the file, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options to read the resource, by default {&quot;format&quot;: &quot;text&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;delimiter&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">:</span>
            <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;delimiter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">delimiter</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">slangDictionary</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">opts</span><span class="p">))</span></div>

<div class="viewcode-block" id="Normalizer.setMinLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Normalizer.html#sparknlp.annotator.Normalizer.setMinLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMinLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the minimum allowed legth for each token, by default 0.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Minimum allowed legth for each token.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">minLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="Normalizer.setMaxLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Normalizer.html#sparknlp.annotator.Normalizer.setMaxLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the maximum allowed legth for each token.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Maximum allowed legth for each token</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">NormalizerModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="NormalizerModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NormalizerModel.html#sparknlp.annotator.NormalizerModel">[docs]</a><span class="k">class</span> <span class="nc">NormalizerModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Instantiated Model of the Normalizer.</span>

<span class="sd">    This is the instantiated model of the :class:`.Normalizer`.</span>
<span class="sd">    For training your own model, please see the documentation of that class.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    cleanupPatterns</span>
<span class="sd">        normalization regex patterns which match will be removed from token</span>
<span class="sd">    lowercase</span>
<span class="sd">        whether to convert strings to lowercase</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">cleanupPatterns</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;cleanupPatterns&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;normalization regex patterns which match will be removed from token&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">lowercase</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;lowercase&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;whether to convert strings to lowercase&quot;</span><span class="p">)</span>

    <span class="n">slangMatchCase</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;slangMatchCase&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;whether or not to be case sensitive to match slangs. Defaults to false.&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.NormalizerModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NormalizerModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;NormalizerModel&quot;</span></div>


<div class="viewcode-block" id="RegexMatcher"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RegexMatcher.html#sparknlp.annotator.RegexMatcher">[docs]</a><span class="k">class</span> <span class="nc">RegexMatcher</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Uses a reference file to match a set of regular expressions and associate</span>
<span class="sd">    them with a provided identifier.</span>

<span class="sd">    A dictionary of predefined regular expressions must be provided with</span>
<span class="sd">    :meth:`.setExternalRules`. The dictionary can be set in the form of a</span>
<span class="sd">    delimited text file.</span>

<span class="sd">    Pretrained pipelines are available for this module, see `Pipelines</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/docs/en/pipelines&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``CHUNK``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    strategy</span>
<span class="sd">        Can be either MATCH_FIRST|MATCH_ALL|MATCH_COMPLETE, by default</span>
<span class="sd">        &quot;MATCH_ALL&quot;</span>
<span class="sd">    externalRules</span>
<span class="sd">        external resource to rules, needs &#39;delimiter&#39; in options</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    In this example, the ``rules.txt`` has the form of::</span>

<span class="sd">        the\\s\\w+, followed by &#39;the&#39;</span>
<span class="sd">        ceremonies, ceremony</span>

<span class="sd">    where each regex is separated by the identifier ``&quot;,&quot;``</span>

<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; regexMatcher = RegexMatcher() \\</span>
<span class="sd">    ...     .setExternalRules(&quot;src/test/resources/regex-matcher/rules.txt&quot;,  &quot;,&quot;) \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;regex&quot;) \\</span>
<span class="sd">    ...     .setStrategy(&quot;MATCH_ALL&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([documentAssembler, sentence, regexMatcher])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[</span>
<span class="sd">    ...     &quot;My first sentence with the first rule. This is my second sentence with ceremonies rule.&quot;</span>
<span class="sd">    ... ]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; results = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; results.selectExpr(&quot;explode(regex) as result&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                                      |</span>
<span class="sd">    +--------------------------------------------------------------------------------------------+</span>
<span class="sd">    |[chunk, 23, 31, the first, [identifier -&gt; followed by &#39;the&#39;, sentence -&gt; 0, chunk -&gt; 0], []]|</span>
<span class="sd">    |[chunk, 71, 80, ceremonies, [identifier -&gt; ceremony, sentence -&gt; 1, chunk -&gt; 0], []]        |</span>
<span class="sd">    +--------------------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">strategy</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;strategy&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;MATCH_FIRST|MATCH_ALL|MATCH_COMPLETE&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>
    <span class="n">externalRules</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;externalRules&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;external resource to rules, needs &#39;delimiter&#39; in options&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RegexMatcher</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.RegexMatcher&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;MATCH_ALL&quot;</span>
        <span class="p">)</span>

<div class="viewcode-block" id="RegexMatcher.setStrategy"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RegexMatcher.html#sparknlp.annotator.RegexMatcher.setStrategy">[docs]</a>    <span class="k">def</span> <span class="nf">setStrategy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets matching strategy, by default &quot;MATCH_ALL&quot;.</span>

<span class="sd">        Can be either MATCH_FIRST|MATCH_ALL|MATCH_COMPLETE.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Matching Strategy</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="RegexMatcher.setExternalRules"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RegexMatcher.html#sparknlp.annotator.RegexMatcher.setExternalRules">[docs]</a>    <span class="k">def</span> <span class="nf">setExternalRules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">delimiter</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets external resource to rules, needs &#39;delimiter&#39; in options.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the source files</span>
<span class="sd">        delimiter : str</span>
<span class="sd">            Delimiter for the dictionary file. Can also be set it `options`.</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the file, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options to read the resource, by default {&quot;format&quot;: &quot;text&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;delimiter&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">:</span>
            <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;delimiter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">delimiter</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">externalRules</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">opts</span><span class="p">))</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">RegexMatcherModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="RegexMatcherModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RegexMatcherModel.html#sparknlp.annotator.RegexMatcherModel">[docs]</a><span class="k">class</span> <span class="nc">RegexMatcherModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Instantiated model of the RegexMatcher.</span>

<span class="sd">    This is the instantiated model of the :class:`.RegexMatcher`.</span>
<span class="sd">    For training your own model, please see the documentation of that class.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``CHUNK``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.RegexMatcherModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RegexMatcherModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;RegexMatcherModel&quot;</span></div>


<div class="viewcode-block" id="Lemmatizer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Lemmatizer.html#sparknlp.annotator.Lemmatizer">[docs]</a><span class="k">class</span> <span class="nc">Lemmatizer</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Class to find lemmas out of words with the objective of returning a base</span>
<span class="sd">    dictionary word.</span>

<span class="sd">    Retrieves the significant part of a word. A dictionary of predefined lemmas</span>
<span class="sd">    must be provided with :meth:`.setDictionary`.</span>

<span class="sd">    For instantiated/pretrained models, see :class:`.LemmatizerModel`.</span>

<span class="sd">    For available pretrained models please see the `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Lemmatization&gt;`__.</span>
<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dictionary</span>
<span class="sd">        lemmatizer external dictionary.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    In this example, the lemma dictionary ``lemmas_small.txt`` has the form of::</span>

<span class="sd">        ...</span>
<span class="sd">        pick	-&gt;	pick	picks	picking	picked</span>
<span class="sd">        peck	-&gt;	peck	pecking	pecked	pecks</span>
<span class="sd">        pickle	-&gt;	pickle	pickles	pickled	pickling</span>
<span class="sd">        pepper	-&gt;	pepper	peppers	peppered	peppering</span>
<span class="sd">        ...</span>

<span class="sd">    where each key is delimited by ``-&gt;`` and values are delimited by ``\\t``</span>

<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentenceDetector = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; lemmatizer = Lemmatizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;lemma&quot;) \\</span>
<span class="sd">    ...     .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot;\\t&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       sentenceDetector,</span>
<span class="sd">    ...       tokenizer,</span>
<span class="sd">    ...       lemmatizer</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]) \\</span>
<span class="sd">    ...     .toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;lemma.result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------------+</span>
<span class="sd">    |result                                                            |</span>
<span class="sd">    +------------------------------------------------------------------+</span>
<span class="sd">    |[Peter, Pipers, employees, are, pick, peck, of, pickle, pepper, .]|</span>
<span class="sd">    +------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;dictionary&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;lemmatizer external dictionary.&quot;</span> <span class="o">+</span>
                       <span class="s2">&quot; needs &#39;keyDelimiter&#39; and &#39;valueDelimiter&#39; in options for parsing target text&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Lemmatizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.Lemmatizer&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">LemmatizerModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span>

<div class="viewcode-block" id="Lemmatizer.setDictionary"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Lemmatizer.html#sparknlp.annotator.Lemmatizer.setDictionary">[docs]</a>    <span class="k">def</span> <span class="nf">setDictionary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">key_delimiter</span><span class="p">,</span> <span class="n">value_delimiter</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span>
                      <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets the external dictionary for the lemmatizer.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the source files</span>
<span class="sd">        key_delimiter : str</span>
<span class="sd">            Delimiter for the key</span>
<span class="sd">        value_delimiter : str</span>
<span class="sd">            Delimiter for the values</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the file, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options to read the resource, by default {&quot;format&quot;: &quot;text&quot;}</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        Here the file has each key is delimited by ``&quot;-&gt;&quot;`` and values are</span>
<span class="sd">        delimited by ``\\t``::</span>

<span class="sd">            ...</span>
<span class="sd">            pick	-&gt;	pick	picks	picking	picked</span>
<span class="sd">            peck	-&gt;	peck	pecking	pecked	pecks</span>
<span class="sd">            pickle	-&gt;	pickle	pickles	pickled	pickling</span>
<span class="sd">            pepper	-&gt;	pepper	peppers	peppered	peppering</span>
<span class="sd">            ...</span>

<span class="sd">        This file can then be parsed with</span>

<span class="sd">        &gt;&gt;&gt; lemmatizer = Lemmatizer() \\</span>
<span class="sd">        ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">        ...     .setOutputCol(&quot;lemma&quot;) \\</span>
<span class="sd">        ...     .setDictionary(&quot;lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot;\\t&quot;)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;keyDelimiter&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">:</span>
            <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;keyDelimiter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">key_delimiter</span>
        <span class="k">if</span> <span class="s2">&quot;valueDelimiter&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">:</span>
            <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;valueDelimiter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">value_delimiter</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">dictionary</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">opts</span><span class="p">))</span></div></div>


<div class="viewcode-block" id="LemmatizerModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LemmatizerModel.html#sparknlp.annotator.LemmatizerModel">[docs]</a><span class="k">class</span> <span class="nc">LemmatizerModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Instantiated Model of the Lemmatizer.</span>

<span class="sd">    This is the instantiated model of the :class:`.Lemmatizer`.</span>
<span class="sd">    For training your own model, please see the documentation of that class.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; lemmatizer = LemmatizerModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;lemma&quot;)</span>

<span class="sd">    For available pretrained models please see the `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Lemmatization&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    The lemmatizer from the example of the :class:`.Lemmatizer` can be replaced</span>
<span class="sd">    with:</span>

<span class="sd">    &gt;&gt;&gt; lemmatizer = LemmatizerModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;lemma&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;LemmatizerModel&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.LemmatizerModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LemmatizerModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="LemmatizerModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LemmatizerModel.html#sparknlp.annotator.LemmatizerModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;lemma_antbnc&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;lemma_antbnc&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        LemmatizerModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">LemmatizerModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="DateMatcherUtils"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DateMatcherUtils.html#sparknlp.annotator.DateMatcherUtils">[docs]</a><span class="k">class</span> <span class="nc">DateMatcherUtils</span><span class="p">(</span><span class="n">Params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for DateMatcher Annotators</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dateFormat</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;dateFormat&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;desired format for dates extracted&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">readMonthFirst</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;readMonthFirst&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;Whether to parse july 07/05/2015 or as 05/07/2015&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span>
                           <span class="p">)</span>

    <span class="n">defaultDayWhenMissing</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                  <span class="s2">&quot;defaultDayWhenMissing&quot;</span><span class="p">,</span>
                                  <span class="s2">&quot;which day to set when it is missing from parsed input&quot;</span><span class="p">,</span>
                                  <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span>
                                  <span class="p">)</span>

    <span class="n">anchorDateYear</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;anchorDateYear&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;Add an anchor year for the relative dates such as a day after tomorrow. If not set it &quot;</span>
                           <span class="s2">&quot;will use the current year. Example: 2021&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span>
                           <span class="p">)</span>

    <span class="n">anchorDateMonth</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;anchorDateMonth&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Add an anchor month for the relative dates such as a day after tomorrow. If not set it &quot;</span>
                            <span class="s2">&quot;will use the current month. Example: 1 which means January&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span>
                            <span class="p">)</span>

    <span class="n">anchorDateDay</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;anchorDateDay&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;Add an anchor day of the day for the relative dates such as a day after tomorrow. If not &quot;</span>
                          <span class="s2">&quot;set it will use the current day. Example: 11&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span>
                          <span class="p">)</span>

    <span class="n">sourceLanguage</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;sourceLanguage&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;source language for explicit translation&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

<div class="viewcode-block" id="DateMatcherUtils.setFormat"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DateMatcherUtils.html#sparknlp.annotator.DateMatcherUtils.setFormat">[docs]</a>    <span class="k">def</span> <span class="nf">setFormat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets desired format for extracted dates, by default yyyy/MM/dd.</span>

<span class="sd">        Not all of the date information needs to be included. For example</span>
<span class="sd">        ``&quot;YYYY&quot;`` is also a valid input.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Desired format for dates extracted.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">dateFormat</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="DateMatcherUtils.setReadMonthFirst"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DateMatcherUtils.html#sparknlp.annotator.DateMatcherUtils.setReadMonthFirst">[docs]</a>    <span class="k">def</span> <span class="nf">setReadMonthFirst</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to parse the date in mm/dd/yyyy format instead of</span>
<span class="sd">        dd/mm/yyyy, by default True.</span>

<span class="sd">        For example July 5th 2015, would be parsed as 07/05/2015 instead of</span>
<span class="sd">        05/07/2015.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to parse the date in mm/dd/yyyy format instead of</span>
<span class="sd">            dd/mm/yyyy.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">readMonthFirst</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="DateMatcherUtils.setDefaultDayWhenMissing"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DateMatcherUtils.html#sparknlp.annotator.DateMatcherUtils.setDefaultDayWhenMissing">[docs]</a>    <span class="k">def</span> <span class="nf">setDefaultDayWhenMissing</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets which day to set when it is missing from parsed input,</span>
<span class="sd">        by default 1.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            [description]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">defaultDayWhenMissing</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="DateMatcherUtils.setAnchorDateYear"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DateMatcherUtils.html#sparknlp.annotator.DateMatcherUtils.setAnchorDateYear">[docs]</a>    <span class="k">def</span> <span class="nf">setAnchorDateYear</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets an anchor year for the relative dates such as a day after</span>
<span class="sd">        tomorrow. If not set it will use the current year.</span>

<span class="sd">        Example: 2021</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            The anchor year for relative dates</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">anchorDateYear</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="DateMatcherUtils.setAnchorDateMonth"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DateMatcherUtils.html#sparknlp.annotator.DateMatcherUtils.setAnchorDateMonth">[docs]</a>    <span class="k">def</span> <span class="nf">setAnchorDateMonth</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets an anchor month for the relative dates such as a day after</span>
<span class="sd">        tomorrow. If not set it will use the current month.</span>

<span class="sd">        Example: 1 which means January</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            The anchor month for relative dates</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">normalizedMonth</span> <span class="o">=</span> <span class="n">value</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">anchorDateMonth</span><span class="o">=</span><span class="n">normalizedMonth</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">setSourceLanguage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">sourceLanguage</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>

<div class="viewcode-block" id="DateMatcherUtils.setAnchorDateDay"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DateMatcherUtils.html#sparknlp.annotator.DateMatcherUtils.setAnchorDateDay">[docs]</a>    <span class="k">def</span> <span class="nf">setAnchorDateDay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets an anchor day of the day for the relative dates such as a day</span>
<span class="sd">        after tomorrow. If not set it will use the current day.</span>

<span class="sd">        Example: 11</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            The anchor day for relative dates</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">anchorDateDay</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="DateMatcher"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DateMatcher.html#sparknlp.annotator.DateMatcher">[docs]</a><span class="k">class</span> <span class="nc">DateMatcher</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">DateMatcherUtils</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Matches standard date formats into a provided format</span>
<span class="sd">    Reads from different forms of date and time expressions and converts them</span>
<span class="sd">    to a provided date format.</span>

<span class="sd">    Extracts only **one** date per document. Use with sentence detector to find</span>
<span class="sd">    matches in each sentence.</span>
<span class="sd">    To extract multiple dates from a document, please use the</span>
<span class="sd">    :class:`.MultiDateMatcher`.</span>

<span class="sd">    Reads the following kind of dates::</span>

<span class="sd">        &quot;1978-01-28&quot;, &quot;1984/04/02,1/02/1980&quot;, &quot;2/28/79&quot;,</span>
<span class="sd">        &quot;The 31st of April in the year 2008&quot;, &quot;Fri, 21 Nov 1997&quot;, &quot;Jan 21,</span>
<span class="sd">        97&quot;, &quot;Sun&quot;, &quot;Nov 21&quot;, &quot;jan 1st&quot;, &quot;next thursday&quot;, &quot;last wednesday&quot;,</span>
<span class="sd">        &quot;today&quot;, &quot;tomorrow&quot;, &quot;yesterday&quot;, &quot;next week&quot;, &quot;next month&quot;,</span>
<span class="sd">        &quot;next year&quot;, &quot;day after&quot;, &quot;the day before&quot;, &quot;0600h&quot;, &quot;06:00 hours&quot;,</span>
<span class="sd">        &quot;6pm&quot;, &quot;5:30 a.m.&quot;, &quot;at 5&quot;, &quot;12:59&quot;, &quot;23:59&quot;, &quot;1988/11/23 6pm&quot;,</span>
<span class="sd">        &quot;next week at 7.30&quot;, &quot;5 am tomorrow&quot;</span>

<span class="sd">    For example ``&quot;The 31st of April in the year 2008&quot;`` will be converted into</span>
<span class="sd">    ``2008/04/31``.</span>

<span class="sd">    Pretrained pipelines are available for this module, see</span>
<span class="sd">    `Pipelines &lt;https://nlp.johnsnowlabs.com/docs/en/pipelines&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the</span>
<span class="sd">    `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``DATE``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dateFormat</span>
<span class="sd">        Desired format for dates extracted, by default yyyy/MM/dd.</span>
<span class="sd">    readMonthFirst</span>
<span class="sd">        Whether to parse the date in mm/dd/yyyy format instead of dd/mm/yyyy,</span>
<span class="sd">        by default True.</span>
<span class="sd">    defaultDayWhenMissing</span>
<span class="sd">        Which day to set when it is missing from parsed input, by default 1.</span>
<span class="sd">    anchorDateYear</span>
<span class="sd">        Add an anchor year for the relative dates such as a day after tomorrow.</span>
<span class="sd">        If not set it will use the current year. Example: 2021</span>
<span class="sd">    anchorDateMonth</span>
<span class="sd">        Add an anchor month for the relative dates such as a day after tomorrow.</span>
<span class="sd">        If not set it will use the current month. Example: 1 which means January</span>
<span class="sd">    anchorDateDay</span>
<span class="sd">        Add an anchor day of the day for the relative dates such as a day after</span>
<span class="sd">        tomorrow. If not set it will use the current day. Example: 11</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; date = DateMatcher() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;date&quot;) \\</span>
<span class="sd">    ...     .setAnchorDateYear(2020) \\</span>
<span class="sd">    ...     .setAnchorDateMonth(1) \\</span>
<span class="sd">    ...     .setAnchorDateDay(11) \\</span>
<span class="sd">    ...     .setDateFormat(&quot;yyyy/MM/dd&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     date</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;Fri, 21 Nov 1997&quot;], [&quot;next week at 7.30&quot;], [&quot;see you a day after&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;date&quot;).show(truncate=False)</span>
<span class="sd">    +-------------------------------------------------+</span>
<span class="sd">    |date                                             |</span>
<span class="sd">    +-------------------------------------------------+</span>
<span class="sd">    |[[date, 5, 15, 1997/11/21, [sentence -&gt; 0], []]] |</span>
<span class="sd">    |[[date, 0, 8, 2020/01/18, [sentence -&gt; 0], []]]  |</span>
<span class="sd">    |[[date, 10, 18, 2020/01/12, [sentence -&gt; 0], []]]|</span>
<span class="sd">    +-------------------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    MultiDateMatcher : for matching multiple dates in a document</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;DateMatcher&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DateMatcher</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.DateMatcher&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">dateFormat</span><span class="o">=</span><span class="s2">&quot;yyyy/MM/dd&quot;</span><span class="p">,</span>
            <span class="n">readMonthFirst</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">defaultDayWhenMissing</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">anchorDateYear</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">anchorDateMonth</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">anchorDateDay</span><span class="o">=-</span><span class="mi">1</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="MultiDateMatcher"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiDateMatcher.html#sparknlp.annotator.MultiDateMatcher">[docs]</a><span class="k">class</span> <span class="nc">MultiDateMatcher</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">DateMatcherUtils</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Matches standard date formats into a provided format.</span>

<span class="sd">    Reads the following kind of dates::</span>

<span class="sd">        &quot;1978-01-28&quot;, &quot;1984/04/02,1/02/1980&quot;, &quot;2/28/79&quot;,</span>
<span class="sd">        &quot;The 31st of April in the year 2008&quot;, &quot;Fri, 21 Nov 1997&quot;, &quot;Jan 21,</span>
<span class="sd">        97&quot;, &quot;Sun&quot;, &quot;Nov 21&quot;, &quot;jan 1st&quot;, &quot;next thursday&quot;, &quot;last wednesday&quot;,</span>
<span class="sd">        &quot;today&quot;, &quot;tomorrow&quot;, &quot;yesterday&quot;, &quot;next week&quot;, &quot;next month&quot;,</span>
<span class="sd">        &quot;next year&quot;, &quot;day after&quot;, &quot;the day before&quot;, &quot;0600h&quot;, &quot;06:00 hours&quot;,</span>
<span class="sd">        &quot;6pm&quot;, &quot;5:30 a.m.&quot;, &quot;at 5&quot;, &quot;12:59&quot;, &quot;23:59&quot;, &quot;1988/11/23 6pm&quot;,</span>
<span class="sd">        &quot;next week at 7.30&quot;, &quot;5 am tomorrow&quot;</span>

<span class="sd">    For example ``&quot;The 31st of April in the year 2008&quot;`` will be converted into</span>
<span class="sd">    ``2008/04/31``.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``DATE``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dateFormat</span>
<span class="sd">        Desired format for dates extracted, by default yyyy/MM/dd.</span>
<span class="sd">    readMonthFirst</span>
<span class="sd">        Whether to parse the date in mm/dd/yyyy format instead of dd/mm/yyyy,</span>
<span class="sd">        by default True.</span>
<span class="sd">    defaultDayWhenMissing</span>
<span class="sd">        Which day to set when it is missing from parsed input, by default 1.</span>
<span class="sd">    anchorDateYear</span>
<span class="sd">        Add an anchor year for the relative dates such as a day after tomorrow.</span>
<span class="sd">        If not set it will use the current year. Example: 2021</span>
<span class="sd">    anchorDateMonth</span>
<span class="sd">        Add an anchor month for the relative dates such as a day after tomorrow.</span>
<span class="sd">        If not set it will use the current month. Example: 1 which means January</span>
<span class="sd">    anchorDateDay</span>
<span class="sd">        Add an anchor day of the day for the relative dates such as a day after</span>
<span class="sd">        tomorrow. If not set it will use the current day. Example: 11</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; date = MultiDateMatcher() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;date&quot;) \\</span>
<span class="sd">    ...     .setAnchorDateYear(2020) \\</span>
<span class="sd">    ...     .setAnchorDateMonth(1) \\</span>
<span class="sd">    ...     .setAnchorDateDay(11) \\</span>
<span class="sd">    ...     .setDateFormat(&quot;yyyy/MM/dd&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     date</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;I saw him yesterday and he told me that he will visit us next week&quot;]]) \\</span>
<span class="sd">    ...     .toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(date) as dates&quot;).show(truncate=False)</span>
<span class="sd">    +-----------------------------------------------+</span>
<span class="sd">    |dates                                          |</span>
<span class="sd">    +-----------------------------------------------+</span>
<span class="sd">    |[date, 57, 65, 2020/01/18, [sentence -&gt; 0], []]|</span>
<span class="sd">    |[date, 10, 18, 2020/01/10, [sentence -&gt; 0], []]|</span>
<span class="sd">    +-----------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;MultiDateMatcher&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiDateMatcher</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.MultiDateMatcher&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">dateFormat</span><span class="o">=</span><span class="s2">&quot;yyyy/MM/dd&quot;</span><span class="p">,</span>
            <span class="n">readMonthFirst</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">defaultDayWhenMissing</span><span class="o">=</span><span class="mi">1</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="TextMatcher"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TextMatcher.html#sparknlp.annotator.TextMatcher">[docs]</a><span class="k">class</span> <span class="nc">TextMatcher</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Annotator to match exact phrases (by token) provided in a file against a</span>
<span class="sd">    Document.</span>

<span class="sd">    A text file of predefined phrases must be provided with</span>
<span class="sd">    :meth:`.setEntities`.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``CHUNK``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    entities</span>
<span class="sd">        ExternalResource for entities</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to match regardless of case, by default True</span>
<span class="sd">    mergeOverlapping</span>
<span class="sd">        Whether to merge overlapping matched chunks, by default False</span>
<span class="sd">    entityValue</span>
<span class="sd">        Value for the entity metadata field</span>
<span class="sd">    buildFromTokens</span>
<span class="sd">        Whether the TextMatcher should take the CHUNK from TOKEN or not</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    In this example, the entities file is of the form::</span>

<span class="sd">        ...</span>
<span class="sd">        dolore magna aliqua</span>
<span class="sd">        lorem ipsum dolor. sit</span>
<span class="sd">        laborum</span>
<span class="sd">        ...</span>

<span class="sd">    where each line represents an entity phrase to be extracted.</span>

<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; entityExtractor = TextMatcher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setEntities(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;entity&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([documentAssembler, tokenizer, entityExtractor])</span>
<span class="sd">    &gt;&gt;&gt; results = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; results.selectExpr(&quot;explode(entity) as result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                                    |</span>
<span class="sd">    +------------------------------------------------------------------------------------------+</span>
<span class="sd">    |[chunk, 6, 24, dolore magna aliqua, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 0], []]    |</span>
<span class="sd">    |[chunk, 27, 48, Lorem ipsum dolor. sit, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 1], []]|</span>
<span class="sd">    |[chunk, 53, 59, laborum, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 2], []]               |</span>
<span class="sd">    +------------------------------------------------------------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    BigTextMatcher : to match large amounts of text</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">entities</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;entities&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;ExternalResource for entities&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">caseSensitive</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;caseSensitive&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;whether to match regardless of case. Defaults true&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">mergeOverlapping</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;mergeOverlapping&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;whether to merge overlapping matched chunks. Defaults false&quot;</span><span class="p">,</span>
                             <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">entityValue</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;entityValue&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;value for the entity metadata field&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">buildFromTokens</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;buildFromTokens&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;whether the TextMatcher should take the CHUNK from TOKEN or not&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TextMatcher</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.TextMatcher&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">mergeOverlapping</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">TextMatcherModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span>

<div class="viewcode-block" id="TextMatcher.setEntities"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TextMatcher.html#sparknlp.annotator.TextMatcher.setEntities">[docs]</a>    <span class="k">def</span> <span class="nf">setEntities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets the external resource for the entities.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the external resource</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the resource, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options for reading the resource, by default {&quot;format&quot;: &quot;text&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">entities</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()))</span></div>

<div class="viewcode-block" id="TextMatcher.setCaseSensitive"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TextMatcher.html#sparknlp.annotator.TextMatcher.setCaseSensitive">[docs]</a>    <span class="k">def</span> <span class="nf">setCaseSensitive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to match regardless of case, by default True.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : bool</span>
<span class="sd">            Whether to match regardless of case</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">caseSensitive</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="TextMatcher.setMergeOverlapping"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TextMatcher.html#sparknlp.annotator.TextMatcher.setMergeOverlapping">[docs]</a>    <span class="k">def</span> <span class="nf">setMergeOverlapping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to merge overlapping matched chunks, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : bool</span>
<span class="sd">            Whether to merge overlapping matched chunks</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">mergeOverlapping</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="TextMatcher.setEntityValue"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TextMatcher.html#sparknlp.annotator.TextMatcher.setEntityValue">[docs]</a>    <span class="k">def</span> <span class="nf">setEntityValue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets value for the entity metadata field.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : str</span>
<span class="sd">            Value for the entity metadata field</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">entityValue</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="TextMatcher.setBuildFromTokens"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TextMatcher.html#sparknlp.annotator.TextMatcher.setBuildFromTokens">[docs]</a>    <span class="k">def</span> <span class="nf">setBuildFromTokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether the TextMatcher should take the CHUNK from TOKEN or not.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : bool</span>
<span class="sd">            Whether the TextMatcher should take the CHUNK from TOKEN or not</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">buildFromTokens</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="TextMatcherModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TextMatcherModel.html#sparknlp.annotator.TextMatcherModel">[docs]</a><span class="k">class</span> <span class="nc">TextMatcherModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Instantiated model of the TextMatcher.</span>

<span class="sd">    This is the instantiated model of the :class:`.TextMatcher`. For training</span>
<span class="sd">    your own model, please see the documentation of that class.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``CHUNK``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    mergeOverlapping</span>
<span class="sd">        Whether to merge overlapping matched chunks, by default False</span>
<span class="sd">    entityValue</span>
<span class="sd">        Value for the entity metadata field</span>
<span class="sd">    buildFromTokens</span>
<span class="sd">        Whether the TextMatcher should take the CHUNK from TOKEN or not</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;TextMatcherModel&quot;</span>

    <span class="n">mergeOverlapping</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;mergeOverlapping&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;whether to merge overlapping matched chunks. Defaults false&quot;</span><span class="p">,</span>
                             <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">searchTrie</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;searchTrie&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;searchTrie&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">entityValue</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;entityValue&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;value for the entity metadata field&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">buildFromTokens</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;buildFromTokens&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;whether the TextMatcher should take the CHUNK from TOKEN or not&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.TextMatcherModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TextMatcherModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="TextMatcherModel.setMergeOverlapping"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TextMatcherModel.html#sparknlp.annotator.TextMatcherModel.setMergeOverlapping">[docs]</a>    <span class="k">def</span> <span class="nf">setMergeOverlapping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to merge overlapping matched chunks, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : bool</span>
<span class="sd">            Whether to merge overlapping matched chunks</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">mergeOverlapping</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="TextMatcherModel.setEntityValue"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TextMatcherModel.html#sparknlp.annotator.TextMatcherModel.setEntityValue">[docs]</a>    <span class="k">def</span> <span class="nf">setEntityValue</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets value for the entity metadata field.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : str</span>
<span class="sd">            Value for the entity metadata field</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">entityValue</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="TextMatcherModel.setBuildFromTokens"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TextMatcherModel.html#sparknlp.annotator.TextMatcherModel.setBuildFromTokens">[docs]</a>    <span class="k">def</span> <span class="nf">setBuildFromTokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether the TextMatcher should take the CHUNK from TOKEN or not.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : bool</span>
<span class="sd">            Whether the TextMatcher should take the CHUNK from TOKEN or not</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">buildFromTokens</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="TextMatcherModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TextMatcherModel.html#sparknlp.annotator.TextMatcherModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        TextMatcherModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">TextMatcherModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BigTextMatcher"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BigTextMatcher.html#sparknlp.annotator.BigTextMatcher">[docs]</a><span class="k">class</span> <span class="nc">BigTextMatcher</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">,</span> <span class="n">HasStorage</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Annotator to match exact phrases (by token) provided in a file against a</span>
<span class="sd">    Document.</span>

<span class="sd">    A text file of predefined phrases must be provided with ``setStoragePath``.</span>

<span class="sd">    In contrast to the normal ``TextMatcher``, the ``BigTextMatcher`` is</span>
<span class="sd">    designed for large corpora.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``CHUNK``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    entities</span>
<span class="sd">        ExternalResource for entities</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        whether to ignore case in index lookups, by default True</span>
<span class="sd">    mergeOverlapping</span>
<span class="sd">        whether to merge overlapping matched chunks, by default False</span>
<span class="sd">    tokenizer</span>
<span class="sd">        TokenizerModel to use to tokenize input file for building a Trie</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    In this example, the entities file is of the form::</span>

<span class="sd">        ...</span>
<span class="sd">        dolore magna aliqua</span>
<span class="sd">        lorem ipsum dolor. sit</span>
<span class="sd">        laborum</span>
<span class="sd">        ...</span>

<span class="sd">    where each line represents an entity phrase to be extracted.</span>

<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; entityExtractor = BigTextMatcher() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;, &quot;token&quot;) \\</span>
<span class="sd">    ...     .setStoragePath(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;entity&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([documentAssembler, tokenizer, entityExtractor])</span>
<span class="sd">    &gt;&gt;&gt; results = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; results.selectExpr(&quot;explode(entity)&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------------------------------------------------------+</span>
<span class="sd">    |col                                                                 |</span>
<span class="sd">    +--------------------------------------------------------------------+</span>
<span class="sd">    |[chunk, 6, 24, dolore magna aliqua, [sentence -&gt; 0, chunk -&gt; 0], []]|</span>
<span class="sd">    |[chunk, 53, 59, laborum, [sentence -&gt; 0, chunk -&gt; 1], []]           |</span>
<span class="sd">    +--------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">entities</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;entities&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;ExternalResource for entities&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">caseSensitive</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;caseSensitive&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;whether to ignore case in index lookups&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">mergeOverlapping</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;mergeOverlapping&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;whether to merge overlapping matched chunks. Defaults false&quot;</span><span class="p">,</span>
                             <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;tokenizer&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;TokenizerModel to use to tokenize input file for building a Trie&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BigTextMatcher</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.btm.BigTextMatcher&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">mergeOverlapping</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">TextMatcherModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span>

<div class="viewcode-block" id="BigTextMatcher.setEntities"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BigTextMatcher.html#sparknlp.annotator.BigTextMatcher.setEntities">[docs]</a>    <span class="k">def</span> <span class="nf">setEntities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets ExternalResource for entities.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the resource</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the resource, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options for reading the resource, by default {&quot;format&quot;: &quot;text&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">entities</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()))</span></div>

<div class="viewcode-block" id="BigTextMatcher.setCaseSensitive"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BigTextMatcher.html#sparknlp.annotator.BigTextMatcher.setCaseSensitive">[docs]</a>    <span class="k">def</span> <span class="nf">setCaseSensitive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to ignore case in index lookups, by default True.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : bool</span>
<span class="sd">            Whether to ignore case in index lookups</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">caseSensitive</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="BigTextMatcher.setMergeOverlapping"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BigTextMatcher.html#sparknlp.annotator.BigTextMatcher.setMergeOverlapping">[docs]</a>    <span class="k">def</span> <span class="nf">setMergeOverlapping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to merge overlapping matched chunks, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : bool</span>
<span class="sd">            Whether to merge overlapping matched chunks</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">mergeOverlapping</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="BigTextMatcher.setTokenizer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BigTextMatcher.html#sparknlp.annotator.BigTextMatcher.setTokenizer">[docs]</a>    <span class="k">def</span> <span class="nf">setTokenizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenizer_model</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets TokenizerModel to use to tokenize input file for building a</span>
<span class="sd">        Trie.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        tokenizer_model : :class:`TokenizerModel &lt;sparknlp.annotator.TokenizerModel&gt;`</span>
<span class="sd">            TokenizerModel to use to tokenize input file</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">tokenizer_model</span><span class="o">.</span><span class="n">_transfer_params_to_java</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">tokenizer_model</span><span class="o">.</span><span class="n">_java_obj</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BigTextMatcherModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BigTextMatcherModel.html#sparknlp.annotator.BigTextMatcherModel">[docs]</a><span class="k">class</span> <span class="nc">BigTextMatcherModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasStorageModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Instantiated model of the BigTextMatcher.</span>

<span class="sd">    This is the instantiated model of the :class:`.BigTextMatcher`.</span>
<span class="sd">    For training your own model, please see the documentation of that class.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``CHUNK``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in index lookups</span>
<span class="sd">    mergeOverlapping</span>
<span class="sd">        Whether to merge overlapping matched chunks, by default False</span>
<span class="sd">    searchTrie</span>
<span class="sd">        SearchTrie</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;BigTextMatcherModel&quot;</span>
    <span class="n">databases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;TMVOCAB&#39;</span><span class="p">,</span> <span class="s1">&#39;TMEDGES&#39;</span><span class="p">,</span> <span class="s1">&#39;TMNODES&#39;</span><span class="p">]</span>

    <span class="n">caseSensitive</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;caseSensitive&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;whether to ignore case in index lookups&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">mergeOverlapping</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;mergeOverlapping&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;whether to merge overlapping matched chunks. Defaults false&quot;</span><span class="p">,</span>
                             <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">searchTrie</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;searchTrie&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;searchTrie&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.btm.TextMatcherModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BigTextMatcherModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="BigTextMatcherModel.setMergeOverlapping"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BigTextMatcherModel.html#sparknlp.annotator.BigTextMatcherModel.setMergeOverlapping">[docs]</a>    <span class="k">def</span> <span class="nf">setMergeOverlapping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to merge overlapping matched chunks, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : bool</span>
<span class="sd">            Whether to merge overlapping matched chunks, by default False</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">mergeOverlapping</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="BigTextMatcherModel.setCaseSensitive"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BigTextMatcherModel.html#sparknlp.annotator.BigTextMatcherModel.setCaseSensitive">[docs]</a>    <span class="k">def</span> <span class="nf">setCaseSensitive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to ignore case in index lookups.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : bool</span>
<span class="sd">            Whether to ignore case in index lookups</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">caseSensitive</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

<div class="viewcode-block" id="BigTextMatcherModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BigTextMatcherModel.html#sparknlp.annotator.BigTextMatcherModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        TextMatcherModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">TextMatcherModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div>

<div class="viewcode-block" id="BigTextMatcherModel.loadStorage"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BigTextMatcherModel.html#sparknlp.annotator.BigTextMatcherModel.loadStorage">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadStorage</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">spark</span><span class="p">,</span> <span class="n">storage_ref</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads the model from storage.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the model</span>
<span class="sd">        spark : :class:`pyspark.sql.SparkSession`</span>
<span class="sd">            The current SparkSession</span>
<span class="sd">        storage_ref : str</span>
<span class="sd">            Identifiers for the model parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">HasStorageModel</span><span class="o">.</span><span class="n">loadStorages</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">spark</span><span class="p">,</span> <span class="n">storage_ref</span><span class="p">,</span> <span class="n">BigTextMatcherModel</span><span class="o">.</span><span class="n">databases</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="PerceptronApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.PerceptronApproach.html#sparknlp.annotator.PerceptronApproach">[docs]</a><span class="k">class</span> <span class="nc">PerceptronApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains an averaged Perceptron model to tag words part-of-speech. Sets a</span>
<span class="sd">    POS tag to each word within a sentence.</span>

<span class="sd">    For pretrained models please see the :class:`.PerceptronModel`.</span>

<span class="sd">    The training data needs to be in a Spark DataFrame, where the column needs</span>
<span class="sd">    to consist of Annotations of type ``POS``. The `Annotation` needs to have</span>
<span class="sd">    member ``result`` set to the POS tag and have a ``&quot;word&quot;`` mapping to its</span>
<span class="sd">    word inside of member ``metadata``. This DataFrame for training can easily</span>
<span class="sd">    created by the helper class :class:`.POS`.</span>


<span class="sd">    &gt;&gt;&gt; POS().readDataset(spark, datasetPath) \\</span>
<span class="sd">    ...     .selectExpr(&quot;explode(tags) as tags&quot;).show(truncate=False)</span>
<span class="sd">    +---------------------------------------------+</span>
<span class="sd">    |tags                                         |</span>
<span class="sd">    +---------------------------------------------+</span>
<span class="sd">    |[pos, 0, 5, NNP, [word -&gt; Pierre], []]       |</span>
<span class="sd">    |[pos, 7, 12, NNP, [word -&gt; Vinken], []]      |</span>
<span class="sd">    |[pos, 14, 14, ,, [word -&gt; ,], []]            |</span>
<span class="sd">    |[pos, 31, 34, MD, [word -&gt; will], []]        |</span>
<span class="sd">    |[pos, 36, 39, VB, [word -&gt; join], []]        |</span>
<span class="sd">    |[pos, 41, 43, DT, [word -&gt; the], []]         |</span>
<span class="sd">    |[pos, 45, 49, NN, [word -&gt; board], []]       |</span>
<span class="sd">                            ...</span>


<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/french/Train-Perceptron-French.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN, DOCUMENT``    ``POS``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    posCol</span>
<span class="sd">        Column name for Array of POS tags that match tokens</span>
<span class="sd">    nIterations</span>
<span class="sd">        Number of iterations in training, converges to better accuracy, by</span>
<span class="sd">        default 5</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.training import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; datasetPath = &quot;src/test/resources/anc-pos-corpus-small/test-training.txt&quot;</span>
<span class="sd">    &gt;&gt;&gt; trainingPerceptronDF = POS().readDataset(spark, datasetPath)</span>
<span class="sd">    &gt;&gt;&gt; trainedPos = PerceptronApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;pos&quot;) \\</span>
<span class="sd">    ...     .setPosColumn(&quot;tags&quot;) \\</span>
<span class="sd">    ...     .fit(trainingPerceptronDF)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     trainedPos</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;To be or not to be, is this the question?&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;pos.result&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------------------------------------+</span>
<span class="sd">    |result                                            |</span>
<span class="sd">    +--------------------------------------------------+</span>
<span class="sd">    |[NNP, NNP, CD, JJ, NNP, NNP, ,, MD, VB, DT, CD, .]|</span>
<span class="sd">    +--------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">posCol</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                   <span class="s2">&quot;posCol&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;column of Array of POS tags that match tokens&quot;</span><span class="p">,</span>
                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">nIterations</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;nIterations&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;Number of iterations in training, converges to better accuracy&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PerceptronApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronApproach&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">nIterations</span><span class="o">=</span><span class="mi">5</span>
        <span class="p">)</span>

<div class="viewcode-block" id="PerceptronApproach.setPosColumn"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.PerceptronApproach.html#sparknlp.annotator.PerceptronApproach.setPosColumn">[docs]</a>    <span class="k">def</span> <span class="nf">setPosColumn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets column name for Array of POS tags that match tokens.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Name of column for Array of POS tags</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">posCol</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="PerceptronApproach.setIterations"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.PerceptronApproach.html#sparknlp.annotator.PerceptronApproach.setIterations">[docs]</a>    <span class="k">def</span> <span class="nf">setIterations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets number of iterations in training, by default 5.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Number of iterations in training</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">nIterations</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="PerceptronApproach.getNIterations"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.PerceptronApproach.html#sparknlp.annotator.PerceptronApproach.getNIterations">[docs]</a>    <span class="k">def</span> <span class="nf">getNIterations</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets number of iterations in training, by default 5.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Number of iterations in training</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nIterations</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">PerceptronModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="PerceptronModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.PerceptronModel.html#sparknlp.annotator.PerceptronModel">[docs]</a><span class="k">class</span> <span class="nc">PerceptronModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Averaged Perceptron model to tag words part-of-speech. Sets a POS tag to</span>
<span class="sd">    each word within a sentence.</span>

<span class="sd">    This is the instantiated model of the :class:`.PerceptronApproach`. For</span>
<span class="sd">    training your own model, please see the documentation of that class.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; posTagger = PerceptronModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;pos&quot;)</span>


<span class="sd">    The default model is ``&quot;pos_anc&quot;``, if no name is provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Part+of+Speech+Tagging&gt;`__.</span>
<span class="sd">    Additionally, pretrained pipelines are available for this module, see</span>
<span class="sd">    `Pipelines &lt;https://nlp.johnsnowlabs.com/docs/en/pipelines&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN, DOCUMENT``    ``POS``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; posTagger = PerceptronModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;pos&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     posTagger</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(pos) as pos&quot;).show(truncate=False)</span>
<span class="sd">    +-------------------------------------------+</span>
<span class="sd">    |pos                                        |</span>
<span class="sd">    +-------------------------------------------+</span>
<span class="sd">    |[pos, 0, 4, NNP, [word -&gt; Peter], []]      |</span>
<span class="sd">    |[pos, 6, 11, NNP, [word -&gt; Pipers], []]    |</span>
<span class="sd">    |[pos, 13, 21, NNS, [word -&gt; employees], []]|</span>
<span class="sd">    |[pos, 23, 25, VBP, [word -&gt; are], []]      |</span>
<span class="sd">    |[pos, 27, 33, VBG, [word -&gt; picking], []]  |</span>
<span class="sd">    |[pos, 35, 39, NNS, [word -&gt; pecks], []]    |</span>
<span class="sd">    |[pos, 41, 42, IN, [word -&gt; of], []]        |</span>
<span class="sd">    |[pos, 44, 50, JJ, [word -&gt; pickled], []]   |</span>
<span class="sd">    |[pos, 52, 58, NNS, [word -&gt; peppers], []]  |</span>
<span class="sd">    +-------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;PerceptronModel&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PerceptronModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="PerceptronModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.PerceptronModel.html#sparknlp.annotator.PerceptronModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;pos_anc&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;pos_anc&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        PerceptronModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">PerceptronModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="SentenceDetectorParams"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetectorParams.html#sparknlp.annotator.SentenceDetectorParams">[docs]</a><span class="k">class</span> <span class="nc">SentenceDetectorParams</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Base class for SentenceDetector parameters</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">useAbbreviations</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;useAbbreviations&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;whether to apply abbreviations at sentence detection&quot;</span><span class="p">,</span>
                             <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">customBounds</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;customBounds&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;characters used to explicitly mark sentence bounds&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">useCustomBoundsOnly</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                <span class="s2">&quot;useCustomBoundsOnly&quot;</span><span class="p">,</span>
                                <span class="s2">&quot;Only utilize custom bounds in sentence detection&quot;</span><span class="p">,</span>
                                <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">explodeSentences</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;explodeSentences&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;whether to explode each sentence into a different row, for better parallelization. Defaults to false.&quot;</span><span class="p">,</span>
                             <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">splitLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;splitLength&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;length at which sentences will be forcibly split.&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">minLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;minLength&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Set the minimum allowed length for each sentence.&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">maxLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;maxLength&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Set the maximum allowed length for each sentence&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span></div>


<div class="viewcode-block" id="SentenceDetector"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetector.html#sparknlp.annotator.SentenceDetector">[docs]</a><span class="k">class</span> <span class="nc">SentenceDetector</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">SentenceDetectorParams</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Annotator that detects sentence boundaries using any provided approach.</span>

<span class="sd">    Each extracted sentence can be returned in an Array or exploded to separate</span>
<span class="sd">    rows, if `explodeSentences` is set to ``True``.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``DOCUMENT``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    useAbbreviations</span>
<span class="sd">        whether to apply abbreviations at sentence detection, by default True</span>
<span class="sd">    customBounds</span>
<span class="sd">        characters used to explicitly mark sentence bounds, by default []</span>
<span class="sd">    useCustomBoundsOnly</span>
<span class="sd">        Only utilize custom bounds in sentence detection, by default False</span>
<span class="sd">    explodeSentences</span>
<span class="sd">        whether to explode each sentence into a different row, for better</span>
<span class="sd">        parallelization, by default False</span>
<span class="sd">    splitLength</span>
<span class="sd">        length at which sentences will be forcibly split</span>
<span class="sd">    minLength</span>
<span class="sd">        Set the minimum allowed length for each sentence, by default 0</span>
<span class="sd">    maxLength</span>
<span class="sd">        Set the maximum allowed length for each sentence, by default 99999</span>
<span class="sd">    detectLists</span>
<span class="sd">        whether detect lists during sentence detection, by default True</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is my first sentence. This my second. How about a third?&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(sentence) as sentences&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------------+</span>
<span class="sd">    |sentences                                                         |</span>
<span class="sd">    +------------------------------------------------------------------+</span>
<span class="sd">    |[document, 0, 25, This is my first sentence., [sentence -&gt; 0], []]|</span>
<span class="sd">    |[document, 27, 41, This my second., [sentence -&gt; 1], []]          |</span>
<span class="sd">    |[document, 43, 60, How about a third?, [sentence -&gt; 2], []]       |</span>
<span class="sd">    +------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;SentenceDetector&#39;</span>

    <span class="c1"># this one is exclusive to this detector</span>
    <span class="n">detectLists</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;detectLists&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;whether detect lists during sentence detection&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

<div class="viewcode-block" id="SentenceDetector.setCustomBounds"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetector.html#sparknlp.annotator.SentenceDetector.setCustomBounds">[docs]</a>    <span class="k">def</span> <span class="nf">setCustomBounds</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets characters used to explicitly mark sentence bounds, by default</span>
<span class="sd">        [].</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Characters used to explicitly mark sentence bounds</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">customBounds</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetector.setUseAbbreviations"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetector.html#sparknlp.annotator.SentenceDetector.setUseAbbreviations">[docs]</a>    <span class="k">def</span> <span class="nf">setUseAbbreviations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to apply abbreviations at sentence detection, by default</span>
<span class="sd">        True</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to apply abbreviations at sentence detection</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">useAbbreviations</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetector.setDetectLists"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetector.html#sparknlp.annotator.SentenceDetector.setDetectLists">[docs]</a>    <span class="k">def</span> <span class="nf">setDetectLists</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether detect lists during sentence detection, by default True</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether detect lists during sentence detection</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">detectLists</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetector.setUseCustomBoundsOnly"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetector.html#sparknlp.annotator.SentenceDetector.setUseCustomBoundsOnly">[docs]</a>    <span class="k">def</span> <span class="nf">setUseCustomBoundsOnly</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to only utilize custom bounds in sentence detection, by</span>
<span class="sd">        default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to only utilize custom bounds</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">useCustomBoundsOnly</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetector.setExplodeSentences"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetector.html#sparknlp.annotator.SentenceDetector.setExplodeSentences">[docs]</a>    <span class="k">def</span> <span class="nf">setExplodeSentences</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to explode each sentence into a different row, for</span>
<span class="sd">        better parallelization, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to explode each sentence into a different row</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">explodeSentences</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetector.setSplitLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetector.html#sparknlp.annotator.SentenceDetector.setSplitLength">[docs]</a>    <span class="k">def</span> <span class="nf">setSplitLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets length at which sentences will be forcibly split.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Length at which sentences will be forcibly split.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">splitLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetector.setMinLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetector.html#sparknlp.annotator.SentenceDetector.setMinLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMinLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets minimum allowed length for each sentence, by default 0</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Minimum allowed length for each sentence</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">minLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetector.setMaxLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetector.html#sparknlp.annotator.SentenceDetector.setMaxLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the maximum allowed length for each sentence, by default</span>
<span class="sd">        99999</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Maximum allowed length for each sentence</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SentenceDetector</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">useAbbreviations</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">detectLists</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">useCustomBoundsOnly</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">customBounds</span><span class="o">=</span><span class="p">[],</span>
            <span class="n">explodeSentences</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">minLength</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">maxLength</span><span class="o">=</span><span class="mi">99999</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="SentimentDetector"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDetector.html#sparknlp.annotator.SentimentDetector">[docs]</a><span class="k">class</span> <span class="nc">SentimentDetector</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains a rule based sentiment detector, which calculates a score based on</span>
<span class="sd">    predefined keywords.</span>

<span class="sd">    A dictionary of predefined sentiment keywords must be provided with</span>
<span class="sd">    :meth:`.setDictionary`, where each line is a word delimited to its class</span>
<span class="sd">    (either ``positive`` or ``negative``). The dictionary can be set in the form</span>
<span class="sd">    of a delimited text file.</span>

<span class="sd">    By default, the sentiment score will be assigned labels ``&quot;positive&quot;`` if</span>
<span class="sd">    the score is ``&gt;= 0``, else ``&quot;negative&quot;``.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dictionary-sentiment/sentiment.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN, DOCUMENT``    ``SENTIMENT``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dictionary</span>
<span class="sd">        path for dictionary to sentiment analysis</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    In this example, the dictionary ``default-sentiment-dict.txt`` has the form</span>
<span class="sd">    of::</span>

<span class="sd">        ...</span>
<span class="sd">        cool,positive</span>
<span class="sd">        superb,positive</span>
<span class="sd">        bad,negative</span>
<span class="sd">        uninspired,negative</span>
<span class="sd">        ...</span>

<span class="sd">    where each sentiment keyword is delimited by ``&quot;,&quot;``.</span>

<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; lemmatizer = Lemmatizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;lemma&quot;) \\</span>
<span class="sd">    ...     .setDictionary(&quot;lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot;\\t&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentimentDetector = SentimentDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;lemma&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentimentScore&quot;) \\</span>
<span class="sd">    ...     .setDictionary(&quot;default-sentiment-dict.txt&quot;, &quot;,&quot;, ReadAs.TEXT)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     lemmatizer,</span>
<span class="sd">    ...     sentimentDetector,</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([</span>
<span class="sd">    ...     [&quot;The staff of the restaurant is nice&quot;],</span>
<span class="sd">    ...     [&quot;I recommend others to avoid because it is too expensive&quot;]</span>
<span class="sd">    ... ]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;sentimentScore.result&quot;).show(truncate=False)</span>
<span class="sd">    +----------+</span>
<span class="sd">    |result    |</span>
<span class="sd">    +----------+</span>
<span class="sd">    |[positive]|</span>
<span class="sd">    |[negative]|</span>
<span class="sd">    +----------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    ViveknSentimentApproach : for an alternative approach to sentiment extraction</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;dictionary&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;path for dictionary to sentiment analysis&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">positiveMultiplier</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                               <span class="s2">&quot;positiveMultiplier&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;multiplier for positive sentiments. Defaults 1.0&quot;</span><span class="p">,</span>
                               <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">negativeMultiplier</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                               <span class="s2">&quot;negativeMultiplier&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;multiplier for negative sentiments. Defaults -1.0&quot;</span><span class="p">,</span>
                               <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">incrementMultiplier</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                <span class="s2">&quot;incrementMultiplier&quot;</span><span class="p">,</span>
                                <span class="s2">&quot;multiplier for increment sentiments. Defaults 2.0&quot;</span><span class="p">,</span>
                                <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">decrementMultiplier</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                <span class="s2">&quot;decrementMultiplier&quot;</span><span class="p">,</span>
                                <span class="s2">&quot;multiplier for decrement sentiments. Defaults -2.0&quot;</span><span class="p">,</span>
                                <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">reverseMultiplier</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;reverseMultiplier&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;multiplier for revert sentiments. Defaults -1.0&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">enableScore</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;enableScore&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;if true, score will show as the double value, else will output string </span><span class="se">\&quot;</span><span class="s2">positive</span><span class="se">\&quot;</span><span class="s2"> or </span><span class="se">\&quot;</span><span class="s2">negative</span><span class="se">\&quot;</span><span class="s2">. Defaults false&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SentimentDetector</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.sda.pragmatic.SentimentDetector&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">positiveMultiplier</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">negativeMultiplier</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">incrementMultiplier</span><span class="o">=</span><span class="mf">2.0</span><span class="p">,</span>
                         <span class="n">decrementMultiplier</span><span class="o">=-</span><span class="mf">2.0</span><span class="p">,</span> <span class="n">reverseMultiplier</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">enableScore</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<div class="viewcode-block" id="SentimentDetector.setDictionary"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDetector.html#sparknlp.annotator.SentimentDetector.setDictionary">[docs]</a>    <span class="k">def</span> <span class="nf">setDictionary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">delimiter</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;format&#39;</span><span class="p">:</span> <span class="s1">&#39;text&#39;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets path for dictionary to sentiment analysis</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to dictionary file</span>
<span class="sd">        delimiter : str</span>
<span class="sd">            Delimiter for entries</span>
<span class="sd">        read_as : sttr, optional</span>
<span class="sd">            How to read the resource, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options for reading the resource, by default {&#39;format&#39;: &#39;text&#39;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;delimiter&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">:</span>
            <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;delimiter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">delimiter</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">dictionary</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">opts</span><span class="p">))</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">SentimentDetectorModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="SentimentDetectorModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDetectorModel.html#sparknlp.annotator.SentimentDetectorModel">[docs]</a><span class="k">class</span> <span class="nc">SentimentDetectorModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Rule based sentiment detector, which calculates a score based on</span>
<span class="sd">    predefined keywords.</span>

<span class="sd">    This is the instantiated model of the :class:`.SentimentDetector`. For</span>
<span class="sd">    training your own model, please see the documentation of that class.</span>

<span class="sd">    By default, the sentiment score will be assigned labels ``&quot;positive&quot;`` if</span>
<span class="sd">    the score is ``&gt;= 0``, else ``&quot;negative&quot;``.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dictionary-sentiment/sentiment.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN, DOCUMENT``    ``SENTIMENT``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;SentimentDetectorModel&quot;</span>

    <span class="n">positiveMultiplier</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                               <span class="s2">&quot;positiveMultiplier&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;multiplier for positive sentiments. Defaults 1.0&quot;</span><span class="p">,</span>
                               <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.sda.pragmatic.SentimentDetectorModel&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SentimentDetectorModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="ViveknSentimentApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ViveknSentimentApproach.html#sparknlp.annotator.ViveknSentimentApproach">[docs]</a><span class="k">class</span> <span class="nc">ViveknSentimentApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains a sentiment analyser inspired by the algorithm by Vivek Narayanan.</span>

<span class="sd">    The analyzer requires sentence boundaries to give a score in context.</span>
<span class="sd">    Tokenization is needed to make sure tokens are within bounds. Transitivity</span>
<span class="sd">    requirements are also required.</span>

<span class="sd">    The training data needs to consist of a column for normalized text and a</span>
<span class="sd">    label column (either ``&quot;positive&quot;`` or ``&quot;negative&quot;``).</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/vivekn-sentiment/VivekNarayanSentimentApproach.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN, DOCUMENT``    ``SENTIMENT``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    sentimentCol</span>
<span class="sd">        column with the sentiment result of every row. Must be &#39;positive&#39; or &#39;negative&#39;</span>
<span class="sd">    pruneCorpus</span>
<span class="sd">        Removes unfrequent scenarios from scope. The higher the better performance. Defaults 1</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    The algorithm is based on the paper `&quot;Fast and accurate sentiment</span>
<span class="sd">    classification using an enhanced Naive Bayes model&quot;</span>
<span class="sd">    &lt;https://arxiv.org/abs/1305.6143&gt;`__.</span>

<span class="sd">    https://github.com/vivekn/sentiment/</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; document = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; token = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; normalizer = Normalizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;normal&quot;)</span>
<span class="sd">    &gt;&gt;&gt; vivekn = ViveknSentimentApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;normal&quot;]) \\</span>
<span class="sd">    ...     .setSentimentCol(&quot;train_sentiment&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;result_sentiment&quot;)</span>
<span class="sd">    &gt;&gt;&gt; finisher = Finisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;result_sentiment&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;final_sentiment&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher])</span>
<span class="sd">    &gt;&gt;&gt; training = spark.createDataFrame([</span>
<span class="sd">    ...     (&quot;I really liked this movie!&quot;, &quot;positive&quot;),</span>
<span class="sd">    ...     (&quot;The cast was horrible&quot;, &quot;negative&quot;),</span>
<span class="sd">    ...     (&quot;Never going to watch this again or recommend it to anyone&quot;, &quot;negative&quot;),</span>
<span class="sd">    ...     (&quot;It&#39;s a waste of time&quot;, &quot;negative&quot;),</span>
<span class="sd">    ...     (&quot;I loved the protagonist&quot;, &quot;positive&quot;),</span>
<span class="sd">    ...     (&quot;The music was really really good&quot;, &quot;positive&quot;)</span>
<span class="sd">    ... ]).toDF(&quot;text&quot;, &quot;train_sentiment&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(training)</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([</span>
<span class="sd">    ...     [&quot;I recommend this movie&quot;],</span>
<span class="sd">    ...     [&quot;Dont waste your time!!!&quot;]</span>
<span class="sd">    ... ]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipelineModel.transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;final_sentiment&quot;).show(truncate=False)</span>
<span class="sd">    +---------------+</span>
<span class="sd">    |final_sentiment|</span>
<span class="sd">    +---------------+</span>
<span class="sd">    |[positive]     |</span>
<span class="sd">    |[negative]     |</span>
<span class="sd">    +---------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">sentimentCol</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;sentimentCol&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;column with the sentiment result of every row. Must be &#39;positive&#39; or &#39;negative&#39;&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">pruneCorpus</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;pruneCorpus&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;Removes unfrequent scenarios from scope. The higher the better performance. Defaults 1&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">importantFeatureRatio</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                  <span class="s2">&quot;importantFeatureRatio&quot;</span><span class="p">,</span>
                                  <span class="s2">&quot;proportion of feature content to be considered relevant. Defaults to 0.5&quot;</span><span class="p">,</span>
                                  <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">unimportantFeatureStep</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                   <span class="s2">&quot;unimportantFeatureStep&quot;</span><span class="p">,</span>
                                   <span class="s2">&quot;proportion to lookahead in unimportant features. Defaults to 0.025&quot;</span><span class="p">,</span>
                                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">featureLimit</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;featureLimit&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;content feature limit, to boost performance in very dirt text. Default disabled with -1&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ViveknSentimentApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentApproach&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">pruneCorpus</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">importantFeatureRatio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">unimportantFeatureStep</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span> <span class="n">featureLimit</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<div class="viewcode-block" id="ViveknSentimentApproach.setSentimentCol"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ViveknSentimentApproach.html#sparknlp.annotator.ViveknSentimentApproach.setSentimentCol">[docs]</a>    <span class="k">def</span> <span class="nf">setSentimentCol</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets column with the sentiment result of every row.</span>

<span class="sd">        Must be either &#39;positive&#39; or &#39;negative&#39;.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Name of the column</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">sentimentCol</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="ViveknSentimentApproach.setPruneCorpus"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ViveknSentimentApproach.html#sparknlp.annotator.ViveknSentimentApproach.setPruneCorpus">[docs]</a>    <span class="k">def</span> <span class="nf">setPruneCorpus</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the removal of unfrequent scenarios from scope, by default 1.</span>

<span class="sd">        The higher the better performance.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            The frequency</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">pruneCorpus</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ViveknSentimentModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="ViveknSentimentModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ViveknSentimentModel.html#sparknlp.annotator.ViveknSentimentModel">[docs]</a><span class="k">class</span> <span class="nc">ViveknSentimentModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sentiment analyser inspired by the algorithm by Vivek Narayanan.</span>

<span class="sd">    This is the instantiated model of the :class:`.ViveknSentimentApproach`. For</span>
<span class="sd">    training your own model, please see the documentation of that class.</span>

<span class="sd">    The analyzer requires sentence boundaries to give a score in context.</span>
<span class="sd">    Tokenization is needed to make sure tokens are within bounds. Transitivity</span>
<span class="sd">    requirements are also required.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/vivekn-sentiment/VivekNarayanSentimentApproach.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN, DOCUMENT``    ``SENTIMENT``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    The algorithm is based on the paper `&quot;Fast and accurate sentiment</span>
<span class="sd">    classification using an enhanced Naive Bayes model&quot;</span>
<span class="sd">    &lt;https://arxiv.org/abs/1305.6143&gt;`__.</span>

<span class="sd">    https://github.com/vivekn/sentiment/</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;ViveknSentimentModel&quot;</span>

    <span class="n">importantFeatureRatio</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                  <span class="s2">&quot;importantFeatureRatio&quot;</span><span class="p">,</span>
                                  <span class="s2">&quot;proportion of feature content to be considered relevant. Defaults to 0.5&quot;</span><span class="p">,</span>
                                  <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">unimportantFeatureStep</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                   <span class="s2">&quot;unimportantFeatureStep&quot;</span><span class="p">,</span>
                                   <span class="s2">&quot;proportion to lookahead in unimportant features. Defaults to 0.025&quot;</span><span class="p">,</span>
                                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">featureLimit</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;featureLimit&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;content feature limit, to boost performance in very dirt text. Default disabled with -1&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ViveknSentimentModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="ViveknSentimentModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ViveknSentimentModel.html#sparknlp.annotator.ViveknSentimentModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;sentiment_vivekn&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;sentiment_vivekn&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ViveknSentimentModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">ViveknSentimentModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="NorvigSweetingApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NorvigSweetingApproach.html#sparknlp.annotator.NorvigSweetingApproach">[docs]</a><span class="k">class</span> <span class="nc">NorvigSweetingApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains annotator, that retrieves tokens and makes corrections</span>
<span class="sd">    automatically if not found in an English dictionary.</span>

<span class="sd">    The Symmetric Delete spelling correction algorithm reduces the complexity of</span>
<span class="sd">    edit candidate generation and dictionary lookup for a given</span>
<span class="sd">    Damerau-Levenshtein distance. It is six orders of magnitude faster (than the</span>
<span class="sd">    standard approach with deletes + transposes + replaces + inserts) and</span>
<span class="sd">    language independent. A dictionary of correct spellings must be provided</span>
<span class="sd">    with :meth:`.setDictionary` in the form of a text file, where each word is</span>
<span class="sd">    parsed by a regex pattern.</span>

<span class="sd">    For instantiated/pretrained models, see :class:`.NorvigSweetingModel`.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/vivekn-sentiment/VivekNarayanSentimentApproach.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dictionary</span>
<span class="sd">        Dictionary needs &#39;tokenPattern&#39; regex in dictionary for separating words</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case sensitivity, by default False</span>
<span class="sd">    doubleVariants</span>
<span class="sd">        Whether to use more expensive spell checker, by default False</span>

<span class="sd">        Increase search at cost of performance. Enables extra check for word</span>
<span class="sd">        combinations.</span>
<span class="sd">    shortCircuit</span>
<span class="sd">        Whether to use faster mode, by default False</span>

<span class="sd">        Increase performance at cost of accuracy. Faster but less accurate.</span>
<span class="sd">    frequencyPriority</span>
<span class="sd">        Applies frequency over hamming in intersections, when false hamming</span>
<span class="sd">        takes priority, by default True</span>
<span class="sd">    wordSizeIgnore</span>
<span class="sd">        Minimum size of word before ignoring, by default 3</span>
<span class="sd">    dupsLimit</span>
<span class="sd">        Maximum duplicate of characters in a word to consider, by default 2</span>
<span class="sd">    reductLimit</span>
<span class="sd">        Word reductions limit, by default 3</span>
<span class="sd">    intersections</span>
<span class="sd">        Hamming intersections to attempt, by default 10</span>
<span class="sd">    vowelSwapLimit</span>
<span class="sd">        Vowel swap attempts, by default 6</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Inspired by Norvig model and `SymSpell</span>
<span class="sd">    &lt;https://github.com/wolfgarbe/SymSpell&gt;`__.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    In this example, the dictionary ``&quot;words.txt&quot;`` has the form of::</span>

<span class="sd">        ...</span>
<span class="sd">        gummy</span>
<span class="sd">        gummic</span>
<span class="sd">        gummier</span>
<span class="sd">        gummiest</span>
<span class="sd">        gummiferous</span>
<span class="sd">        ...</span>

<span class="sd">    This dictionary is then set to be the basis of the spell checker.</span>

<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spellChecker = NorvigSweetingApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;spell&quot;) \\</span>
<span class="sd">    ...     .setDictionary(&quot;src/test/resources/spell/words.txt&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     spellChecker</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(trainingData)</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    SymmetricDeleteApproach : for an alternative approach to spell checking</span>
<span class="sd">    ContextSpellCheckerApproach : for a DL based approach</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dictionary</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;dictionary&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;dictionary needs &#39;tokenPattern&#39; regex in dictionary for separating words&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">caseSensitive</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;caseSensitive&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;whether to ignore case sensitivty&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">doubleVariants</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;doubleVariants&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;whether to use more expensive spell checker&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">shortCircuit</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;shortCircuit&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;whether to use faster mode&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">frequencyPriority</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;frequencyPriority&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;applies frequency over hamming in intersections. When false hamming takes priority&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">wordSizeIgnore</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;wordSizeIgnore&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;minimum size of word before ignoring. Defaults to 3&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">dupsLimit</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;dupsLimit&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;maximum duplicate of characters in a word to consider. Defaults to 2&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">reductLimit</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;reductLimit&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;word reductions limit. Defaults to 3&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">intersections</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;intersections&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;hamming intersections to attempt. Defaults to 10&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">vowelSwapLimit</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;vowelSwapLimit&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;vowel swap attempts. Defaults to 6&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NorvigSweetingApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.spell.norvig.NorvigSweetingApproach&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">caseSensitive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">doubleVariants</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">shortCircuit</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">wordSizeIgnore</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dupsLimit</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                         <span class="n">reductLimit</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">intersections</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">vowelSwapLimit</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">frequencyPriority</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dictionary_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

<div class="viewcode-block" id="NorvigSweetingApproach.setDictionary"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NorvigSweetingApproach.html#sparknlp.annotator.NorvigSweetingApproach.setDictionary">[docs]</a>    <span class="k">def</span> <span class="nf">setDictionary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">token_pattern</span><span class="o">=</span><span class="s2">&quot;\S+&quot;</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets dictionary which needs &#39;tokenPattern&#39; regex for separating</span>
<span class="sd">        words.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the source file</span>
<span class="sd">        token_pattern : str, optional</span>
<span class="sd">            Pattern for token separation, by default ``\\S+``</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the file, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options to read the resource, by default {&quot;format&quot;: &quot;text&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dictionary_path</span> <span class="o">=</span> <span class="n">path</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;tokenPattern&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">:</span>
            <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;tokenPattern&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_pattern</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">dictionary</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">opts</span><span class="p">))</span></div>

<div class="viewcode-block" id="NorvigSweetingApproach.setCaseSensitive"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NorvigSweetingApproach.html#sparknlp.annotator.NorvigSweetingApproach.setCaseSensitive">[docs]</a>    <span class="k">def</span> <span class="nf">setCaseSensitive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to ignore case sensitivity, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to ignore case sensitivity</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">caseSensitive</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NorvigSweetingApproach.setDoubleVariants"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NorvigSweetingApproach.html#sparknlp.annotator.NorvigSweetingApproach.setDoubleVariants">[docs]</a>    <span class="k">def</span> <span class="nf">setDoubleVariants</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to use more expensive spell checker, by default False.</span>

<span class="sd">        Increase search at cost of performance. Enables extra check for word</span>
<span class="sd">        combinations.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            [description]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">doubleVariants</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NorvigSweetingApproach.setShortCircuit"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NorvigSweetingApproach.html#sparknlp.annotator.NorvigSweetingApproach.setShortCircuit">[docs]</a>    <span class="k">def</span> <span class="nf">setShortCircuit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to use faster mode, by default False.</span>

<span class="sd">        Increase performance at cost of accuracy. Faster but less accurate.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to use faster mode</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">shortCircuit</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NorvigSweetingApproach.setFrequencyPriority"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NorvigSweetingApproach.html#sparknlp.annotator.NorvigSweetingApproach.setFrequencyPriority">[docs]</a>    <span class="k">def</span> <span class="nf">setFrequencyPriority</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to consider frequency over hamming in intersections,</span>
<span class="sd">        when false hamming takes priority, by default True.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to consider frequency over hamming in intersections</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">frequencyPriority</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">NorvigSweetingModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="NorvigSweetingModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NorvigSweetingModel.html#sparknlp.annotator.NorvigSweetingModel">[docs]</a><span class="k">class</span> <span class="nc">NorvigSweetingModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This annotator retrieves tokens and makes corrections automatically if</span>
<span class="sd">    not found in an English dictionary.</span>

<span class="sd">    The Symmetric Delete spelling correction algorithm reduces the complexity of</span>
<span class="sd">    edit candidate generation and dictionary lookup for a given</span>
<span class="sd">    Damerau-Levenshtein distance. It is six orders of magnitude faster (than the</span>
<span class="sd">    standard approach with deletes + transposes + replaces + inserts) and</span>
<span class="sd">    language independent.</span>

<span class="sd">    This is the instantiated model of the :class:`.NorvigSweetingApproach`. For</span>
<span class="sd">    training your own model, please see the documentation of that class.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt;    spellChecker = NorvigSweetingModel.pretrained() \\</span>
<span class="sd">    ...        .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...        .setOutputCol(&quot;spell&quot;) \\</span>


<span class="sd">    The default model is ``&quot;spellcheck_norvig&quot;``, if no name is provided. For</span>
<span class="sd">    available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Spell+Check&gt;`__.</span>


<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/vivekn-sentiment/VivekNarayanSentimentApproach.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Inspired by Norvig model and `SymSpell</span>
<span class="sd">    &lt;https://github.com/wolfgarbe/SymSpell&gt;`__.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spellChecker = NorvigSweetingModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;spell&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     spellChecker</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;somtimes i wrrite wordz erong.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;spell.result&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------------------------+</span>
<span class="sd">    |result                                |</span>
<span class="sd">    +--------------------------------------+</span>
<span class="sd">    |[sometimes, i, write, words, wrong, .]|</span>
<span class="sd">    +--------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    SymmetricDeleteModel : for an alternative approach to spell checking</span>
<span class="sd">    ContextSpellCheckerModel : for a DL based approach</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;NorvigSweetingModel&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.spell.norvig.NorvigSweetingModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NorvigSweetingModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="NorvigSweetingModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NorvigSweetingModel.html#sparknlp.annotator.NorvigSweetingModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;spellcheck_norvig&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;spellcheck_norvig&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        NorvigSweetingModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">NorvigSweetingModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="SymmetricDeleteApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SymmetricDeleteApproach.html#sparknlp.annotator.SymmetricDeleteApproach">[docs]</a><span class="k">class</span> <span class="nc">SymmetricDeleteApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains a Symmetric Delete spelling correction algorithm. Retrieves tokens</span>
<span class="sd">    and utilizes distance metrics to compute possible derived words.</span>

<span class="sd">    For instantiated/pretrained models, see :class:`.SymmetricDeleteModel`.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dictionary</span>
<span class="sd">        folder or file with text that teaches about the language</span>
<span class="sd">    maxEditDistance</span>
<span class="sd">        max edit distance characters to derive strings from a word, by default 3</span>
<span class="sd">    frequencyThreshold</span>
<span class="sd">        minimum frequency of words to be considered from training, by default 0</span>
<span class="sd">    deletesThreshold</span>
<span class="sd">        minimum frequency of corrections a word needs to have to be considered</span>
<span class="sd">        from training, by default 0</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Inspired by `SymSpell &lt;https://github.com/wolfgarbe/SymSpell&gt;`__.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    In this example, the dictionary ``&quot;words.txt&quot;`` has the form of::</span>

<span class="sd">        ...</span>
<span class="sd">        gummy</span>
<span class="sd">        gummic</span>
<span class="sd">        gummier</span>
<span class="sd">        gummiest</span>
<span class="sd">        gummiferous</span>
<span class="sd">        ...</span>

<span class="sd">    This dictionary is then set to be the basis of the spell checker.</span>

<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spellChecker = SymmetricDeleteApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;spell&quot;) \\</span>
<span class="sd">    ...     .setDictionary(&quot;src/test/resources/spell/words.txt&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     spellChecker</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(trainingData)</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    NorvigSweetingApproach : for an alternative approach to spell checking</span>
<span class="sd">    ContextSpellCheckerApproach : for a DL based approach</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">corpus</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                   <span class="s2">&quot;corpus&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;folder or file with text that teaches about the language&quot;</span><span class="p">,</span>
                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">dictionary</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;dictionary&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;folder or file with text that teaches about the language&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">maxEditDistance</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;maxEditDistance&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;max edit distance characters to derive strings from a word&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">frequencyThreshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                               <span class="s2">&quot;frequencyThreshold&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;minimum frequency of words to be considered from training. &quot;</span> <span class="o">+</span>
                               <span class="s2">&quot;Increase if training set is LARGE. Defaults to 0&quot;</span><span class="p">,</span>
                               <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">deletesThreshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;deletesThreshold&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;minimum frequency of corrections a word needs to have to be considered from training.&quot;</span> <span class="o">+</span>
                             <span class="s2">&quot;Increase if training set is LARGE. Defaults to 0&quot;</span><span class="p">,</span>
                             <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">dupsLimit</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;dupsLimit&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;maximum duplicate of characters in a word to consider. Defaults to 2&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SymmetricDeleteApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteApproach&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">maxEditDistance</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">frequencyThreshold</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">deletesThreshold</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dupsLimit</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dictionary_path</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>

<div class="viewcode-block" id="SymmetricDeleteApproach.setDictionary"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SymmetricDeleteApproach.html#sparknlp.annotator.SymmetricDeleteApproach.setDictionary">[docs]</a>    <span class="k">def</span> <span class="nf">setDictionary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">token_pattern</span><span class="o">=</span><span class="s2">&quot;\S+&quot;</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets folder or file with text that teaches about the language.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the resource</span>
<span class="sd">        token_pattern : str, optional</span>
<span class="sd">            Regex patttern to extract tokens, by default &quot;\S+&quot;</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the resource, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options for reading the resource, by default {&quot;format&quot;: &quot;text&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dictionary_path</span> <span class="o">=</span> <span class="n">path</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;tokenPattern&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">:</span>
            <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;tokenPattern&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">token_pattern</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">dictionary</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">opts</span><span class="p">))</span></div>

<div class="viewcode-block" id="SymmetricDeleteApproach.setMaxEditDistance"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SymmetricDeleteApproach.html#sparknlp.annotator.SymmetricDeleteApproach.setMaxEditDistance">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxEditDistance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max edit distance characters to derive strings from a word, by</span>
<span class="sd">        default 3.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : int</span>
<span class="sd">            Max edit distance characters to derive strings from a word</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxEditDistance</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

<div class="viewcode-block" id="SymmetricDeleteApproach.setFrequencyThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SymmetricDeleteApproach.html#sparknlp.annotator.SymmetricDeleteApproach.setFrequencyThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setFrequencyThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets minimum frequency of words to be considered from training, by</span>
<span class="sd">        default 0.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : int</span>
<span class="sd">            Minimum frequency of words to be considered from training</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">frequencyThreshold</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

<div class="viewcode-block" id="SymmetricDeleteApproach.setDeletesThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SymmetricDeleteApproach.html#sparknlp.annotator.SymmetricDeleteApproach.setDeletesThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setDeletesThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets minimum frequency of corrections a word needs to have to be</span>
<span class="sd">        considered from training, by default 0.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : int</span>
<span class="sd">            Minimum frequency of corrections a word needs to have to be</span>
<span class="sd">            considered from training</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">deletesThreshold</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">SymmetricDeleteModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="SymmetricDeleteModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SymmetricDeleteModel.html#sparknlp.annotator.SymmetricDeleteModel">[docs]</a><span class="k">class</span> <span class="nc">SymmetricDeleteModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Symmetric Delete spelling correction algorithm.</span>

<span class="sd">    The Symmetric Delete spelling correction algorithm reduces the complexity of</span>
<span class="sd">    edit candidate generation and dictionary lookup for a given</span>
<span class="sd">    Damerau-Levenshtein distance. It is six orders of magnitude faster (than the</span>
<span class="sd">    standard approach with deletes + transposes + replaces + inserts) and</span>
<span class="sd">    language independent.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; spell = SymmetricDeleteModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;spell&quot;)</span>


<span class="sd">    The default model is ``&quot;spellcheck_sd&quot;``, if no name is provided. For</span>
<span class="sd">    available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Spell+Check&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    Inspired by `SymSpell &lt;https://github.com/wolfgarbe/SymSpell&gt;`__.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spellChecker = SymmetricDeleteModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;spell&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     spellChecker</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;spmetimes i wrrite wordz erong.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;spell.result&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------------------------+</span>
<span class="sd">    |result                                |</span>
<span class="sd">    +--------------------------------------+</span>
<span class="sd">    |[sometimes, i, write, words, wrong, .]|</span>
<span class="sd">    +--------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    NorvigSweetingModel : for an alternative approach to spell checking</span>
<span class="sd">    ContextSpellCheckerModel : for a DL based approach</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;SymmetricDeleteModel&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteModel&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SymmetricDeleteModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="SymmetricDeleteModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SymmetricDeleteModel.html#sparknlp.annotator.SymmetricDeleteModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;spellcheck_sd&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;spellcheck_sd&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SymmetricDeleteModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">SymmetricDeleteModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="NerApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerApproach.html#sparknlp.annotator.NerApproach">[docs]</a><span class="k">class</span> <span class="nc">NerApproach</span><span class="p">(</span><span class="n">Params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for Ner*Approach Annotators</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">labelColumn</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;labelColumn&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;Column with label per each token&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">entities</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;entities&quot;</span><span class="p">,</span> <span class="s2">&quot;Entities to recognize&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">minEpochs</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;minEpochs&quot;</span><span class="p">,</span> <span class="s2">&quot;Minimum number of epochs to train&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">maxEpochs</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;maxEpochs&quot;</span><span class="p">,</span> <span class="s2">&quot;Maximum number of epochs to train&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">verbose</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;verbose&quot;</span><span class="p">,</span> <span class="s2">&quot;Level of verbosity during training&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">randomSeed</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;randomSeed&quot;</span><span class="p">,</span> <span class="s2">&quot;Random seed&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

<div class="viewcode-block" id="NerApproach.setLabelColumn"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerApproach.html#sparknlp.annotator.NerApproach.setLabelColumn">[docs]</a>    <span class="k">def</span> <span class="nf">setLabelColumn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets name of column for data labels.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Column for data labels</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">labelColumn</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerApproach.setEntities"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerApproach.html#sparknlp.annotator.NerApproach.setEntities">[docs]</a>    <span class="k">def</span> <span class="nf">setEntities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tags</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets entities to recognize.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        tags : List[str]</span>
<span class="sd">            List of entities</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">entities</span><span class="o">=</span><span class="n">tags</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerApproach.setMinEpochs"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerApproach.html#sparknlp.annotator.NerApproach.setMinEpochs">[docs]</a>    <span class="k">def</span> <span class="nf">setMinEpochs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets minimum number of epochs to train.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        epochs : int</span>
<span class="sd">            Minimum number of epochs to train</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">minEpochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerApproach.setMaxEpochs"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerApproach.html#sparknlp.annotator.NerApproach.setMaxEpochs">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxEpochs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets maximum number of epochs to train.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        epochs : int</span>
<span class="sd">            Maximum number of epochs to train</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxEpochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerApproach.setVerbose"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerApproach.html#sparknlp.annotator.NerApproach.setVerbose">[docs]</a>    <span class="k">def</span> <span class="nf">setVerbose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">verboseValue</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets level of verbosity during training.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        verboseValue : int</span>
<span class="sd">            Level of verbosity</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="n">verboseValue</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerApproach.setRandomSeed"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerApproach.html#sparknlp.annotator.NerApproach.setRandomSeed">[docs]</a>    <span class="k">def</span> <span class="nf">setRandomSeed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets random seed for shuffling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        seed : int</span>
<span class="sd">            Random seed for shuffling</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">randomSeed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerApproach.getLabelColumn"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerApproach.html#sparknlp.annotator.NerApproach.getLabelColumn">[docs]</a>    <span class="k">def</span> <span class="nf">getLabelColumn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets column for label per each token.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        str</span>
<span class="sd">            Column with label per each token</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labelColumn</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="NerCrfApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerCrfApproach.html#sparknlp.annotator.NerCrfApproach">[docs]</a><span class="k">class</span> <span class="nc">NerCrfApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">,</span> <span class="n">NerApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Algorithm for training a Named Entity Recognition Model</span>

<span class="sd">    For instantiated/pretrained models, see :class:`.NerCrfModel`.</span>

<span class="sd">    This Named Entity recognition annotator allows for a generic model to be</span>
<span class="sd">    trained by utilizing a CRF machine learning algorithm. The training data</span>
<span class="sd">    should be a labeled Spark Dataset, e.g. :class:`.CoNLL` 2003 IOB with</span>
<span class="sd">    `Annotation` type columns. The data should have columns of type</span>
<span class="sd">    ``DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS`` and an additional label column of</span>
<span class="sd">    annotator type ``NAMED_ENTITY``.</span>

<span class="sd">    Excluding the label, this can be done with for example:</span>

<span class="sd">    - a :class:`.SentenceDetector`,</span>
<span class="sd">    - a :class:`.Tokenizer`,</span>
<span class="sd">    - a :class:`.PerceptronModel` and</span>
<span class="sd">    - a :class:`.WordEmbeddingsModel`.</span>

<span class="sd">    Optionally the user can provide an entity dictionary file with</span>
<span class="sd">    :meth:`.setExternalFeatures` for better accuracy.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/crf-ner/ner_dl_crf.ipynb&gt;`__.</span>

<span class="sd">    ========================================= ======================</span>
<span class="sd">    Input Annotation types                    Output Annotation type</span>
<span class="sd">    ========================================= ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS`` ``NAMED_ENTITY``</span>
<span class="sd">    ========================================= ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    labelColumn</span>
<span class="sd">        Column with label per each token</span>
<span class="sd">    entities</span>
<span class="sd">        Entities to recognize</span>
<span class="sd">    minEpochs</span>
<span class="sd">        Minimum number of epochs to train, by default 0</span>
<span class="sd">    maxEpochs</span>
<span class="sd">        Maximum number of epochs to train, by default 1000</span>
<span class="sd">    verbose</span>
<span class="sd">        Level of verbosity during training, by default 4</span>
<span class="sd">    randomSeed</span>
<span class="sd">        Random seed</span>
<span class="sd">    l2</span>
<span class="sd">        L2 regularization coefficient, by default 1.0</span>
<span class="sd">    c0</span>
<span class="sd">        c0 params defining decay speed for gradient, by default 2250000</span>
<span class="sd">    lossEps</span>
<span class="sd">        If Epoch relative improvement less than eps then training is stopped, by</span>
<span class="sd">        default 0.001</span>
<span class="sd">    minW</span>
<span class="sd">        Features with less weights then this param value will be filtered</span>
<span class="sd">    includeConfidence</span>
<span class="sd">        Whether to include confidence scores in annotation metadata, by default</span>
<span class="sd">        False</span>
<span class="sd">    externalFeatures</span>
<span class="sd">        Additional dictionaries paths to use as a features</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.training import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    This CoNLL dataset already includes a sentence, token, POS tags and label</span>
<span class="sd">    column with their respective annotator types. If a custom dataset is used,</span>
<span class="sd">    these need to be defined with for example:</span>

<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; posTagger = PerceptronModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;pos&quot;)</span>

<span class="sd">    Then training can start:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = WordEmbeddingsModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(False)</span>
<span class="sd">    &gt;&gt;&gt; nerTagger = NerCrfApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setLabelColumn(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setMinEpochs(1) \\</span>
<span class="sd">    ...     .setMaxEpochs(3) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;ner&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     nerTagger</span>
<span class="sd">    ... ])</span>

<span class="sd">    We use the sentences, tokens, POS tags and labels from the CoNLL dataset.</span>

<span class="sd">    &gt;&gt;&gt; conll = CoNLL()</span>
<span class="sd">    &gt;&gt;&gt; trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(trainingData)</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    NerDLApproach : for a deep learning based approach</span>
<span class="sd">    NerConverter : to further process the results</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">l2</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="s2">&quot;L2 regularization coefficient&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">c0</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;c0&quot;</span><span class="p">,</span> <span class="s2">&quot;c0 params defining decay speed for gradient&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">lossEps</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;lossEps&quot;</span><span class="p">,</span> <span class="s2">&quot;If Epoch relative improvement less than eps then training is stopped&quot;</span><span class="p">,</span>
                    <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">minW</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;minW&quot;</span><span class="p">,</span> <span class="s2">&quot;Features with less weights then this param value will be filtered&quot;</span><span class="p">,</span>
                 <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">includeConfidence</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;includeConfidence&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;external features is a delimited text. needs &#39;delimiter&#39; in options&quot;</span><span class="p">,</span>
                              <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">externalFeatures</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;externalFeatures&quot;</span><span class="p">,</span> <span class="s2">&quot;Additional dictionaries paths to use as a features&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

<div class="viewcode-block" id="NerCrfApproach.setL2"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerCrfApproach.html#sparknlp.annotator.NerCrfApproach.setL2">[docs]</a>    <span class="k">def</span> <span class="nf">setL2</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l2value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets L2 regularization coefficient, by default 1.0.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        l2value : float</span>
<span class="sd">            L2 regularization coefficient</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">l2</span><span class="o">=</span><span class="n">l2value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerCrfApproach.setC0"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerCrfApproach.html#sparknlp.annotator.NerCrfApproach.setC0">[docs]</a>    <span class="k">def</span> <span class="nf">setC0</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c0value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets c0 params defining decay speed for gradient, by default 2250000.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        c0value : int</span>
<span class="sd">            c0 params defining decay speed for gradient</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">c0</span><span class="o">=</span><span class="n">c0value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerCrfApproach.setLossEps"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerCrfApproach.html#sparknlp.annotator.NerCrfApproach.setLossEps">[docs]</a>    <span class="k">def</span> <span class="nf">setLossEps</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eps</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets If Epoch relative improvement less than eps then training is</span>
<span class="sd">        stopped, by default 0.001.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        eps : float</span>
<span class="sd">            The threshold</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">lossEps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerCrfApproach.setMinW"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerCrfApproach.html#sparknlp.annotator.NerCrfApproach.setMinW">[docs]</a>    <span class="k">def</span> <span class="nf">setMinW</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets minimum weight value.</span>

<span class="sd">        Features with less weights then this param value will be filtered.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        w : float</span>
<span class="sd">            Minimum weight value</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">minW</span><span class="o">=</span><span class="n">w</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerCrfApproach.setExternalFeatures"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerCrfApproach.html#sparknlp.annotator.NerCrfApproach.setExternalFeatures">[docs]</a>    <span class="k">def</span> <span class="nf">setExternalFeatures</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">delimiter</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;text&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets Additional dictionaries paths to use as a features.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the source files</span>
<span class="sd">        delimiter : str</span>
<span class="sd">            Delimiter for the dictionary file. Can also be set it `options`.</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the file, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options to read the resource, by default {&quot;format&quot;: &quot;text&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">if</span> <span class="s2">&quot;delimiter&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">opts</span><span class="p">:</span>
            <span class="n">opts</span><span class="p">[</span><span class="s2">&quot;delimiter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">delimiter</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">externalFeatures</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">opts</span><span class="p">))</span></div>

<div class="viewcode-block" id="NerCrfApproach.setIncludeConfidence"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerCrfApproach.html#sparknlp.annotator.NerCrfApproach.setIncludeConfidence">[docs]</a>    <span class="k">def</span> <span class="nf">setIncludeConfidence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to include confidence scores in annotation metadata, by</span>
<span class="sd">        default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : bool</span>
<span class="sd">            Whether to include the confidence value in the output.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">includeConfidence</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">NerCrfModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NerCrfApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.ner.crf.NerCrfApproach&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">minEpochs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">maxEpochs</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
            <span class="n">l2</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">c0</span><span class="o">=</span><span class="mi">2250000</span><span class="p">,</span>
            <span class="n">lossEps</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">),</span>
            <span class="n">verbose</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
            <span class="n">includeConfidence</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="NerCrfModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerCrfModel.html#sparknlp.annotator.NerCrfModel">[docs]</a><span class="k">class</span> <span class="nc">NerCrfModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Extracts Named Entities based on a CRF Model.</span>

<span class="sd">    This Named Entity recognition annotator allows for a generic model to be</span>
<span class="sd">    trained by utilizing a CRF machine learning algorithm. The data should have</span>
<span class="sd">    columns of type ``DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS``. These can be</span>
<span class="sd">    extracted with for example</span>

<span class="sd">    - a SentenceDetector,</span>
<span class="sd">    - a Tokenizer and</span>
<span class="sd">    - a PerceptronModel.</span>

<span class="sd">    This is the instantiated model of the :class:`.NerCrfApproach`. For training</span>
<span class="sd">    your own model, please see the documentation of that class.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; nerTagger = NerCrfModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;, &quot;pos&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;ner&quot;)</span>


<span class="sd">    The default model is ``&quot;ner_crf&quot;``, if no name is provided. For available</span>
<span class="sd">    pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/annotation/english/model-downloader/Running_Pretrained_pipelines.ipynb&gt;`__.</span>

<span class="sd">    ========================================= ======================</span>
<span class="sd">    Input Annotation types                    Output Annotation type</span>
<span class="sd">    ========================================= ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS`` ``NAMED_ENTITY``</span>
<span class="sd">    ========================================= ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    includeConfidence</span>
<span class="sd">        Whether to include confidence scores in annotation metadata, by default</span>
<span class="sd">        False</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    First extract the prerequisites for the NerCrfModel</span>

<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = WordEmbeddingsModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;word_embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; posTagger = PerceptronModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;pos&quot;)</span>

<span class="sd">    Then NER can be extracted</span>

<span class="sd">    &gt;&gt;&gt; nerTagger = NerCrfModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;, &quot;pos&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;ner&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     posTagger,</span>
<span class="sd">    ...     nerTagger</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;U.N. official Ekeus heads for Baghdad.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;ner.result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------+</span>
<span class="sd">    |result                              |</span>
<span class="sd">    +------------------------------------+</span>
<span class="sd">    |[I-ORG, O, O, I-PER, O, O, I-LOC, O]|</span>
<span class="sd">    +------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    NerDLModel : for a deep learning based approach</span>
<span class="sd">    NerConverter : to further process the results</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;NerCrfModel&quot;</span>

    <span class="n">includeConfidence</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;includeConfidence&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;external features is a delimited text. needs &#39;delimiter&#39; in options&quot;</span><span class="p">,</span>
                              <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.ner.crf.NerCrfModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NerCrfModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="NerCrfModel.setIncludeConfidence"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerCrfModel.html#sparknlp.annotator.NerCrfModel.setIncludeConfidence">[docs]</a>    <span class="k">def</span> <span class="nf">setIncludeConfidence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to include confidence scores in annotation metadata, by</span>
<span class="sd">        default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : bool</span>
<span class="sd">            Whether to include the confidence value in the output.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">includeConfidence</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerCrfModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerCrfModel.html#sparknlp.annotator.NerCrfModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;ner_crf&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;ner_crf&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        NerCrfModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">NerCrfModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="NerDLApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach">[docs]</a><span class="k">class</span> <span class="nc">NerDLApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">,</span> <span class="n">NerApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This Named Entity recognition annotator allows to train generic NER model</span>
<span class="sd">    based on Neural Networks.</span>

<span class="sd">    The architecture of the neural network is a Char CNNs - BiLSTM - CRF that</span>
<span class="sd">    achieves state-of-the-art in most datasets.</span>

<span class="sd">    For instantiated/pretrained models, see :class:`.NerDLModel`.</span>

<span class="sd">    The training data should be a labeled Spark Dataset, in the format of</span>
<span class="sd">    :class:`.CoNLL` 2003 IOB with `Annotation` type columns. The data should</span>
<span class="sd">    have columns of type ``DOCUMENT, TOKEN, WORD_EMBEDDINGS`` and an additional</span>
<span class="sd">    label column of annotator type ``NAMED_ENTITY``.</span>

<span class="sd">    Excluding the label, this can be done with for example:</span>

<span class="sd">    - a SentenceDetector,</span>
<span class="sd">    - a Tokenizer and</span>
<span class="sd">    - a WordEmbeddingsModel (any embeddings can be chosen, e.g. BertEmbeddings</span>
<span class="sd">      for BERT based embeddings).</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/jupyter/training/english/dl-ner&gt;`__.</span>

<span class="sd">    ==================================== ======================</span>
<span class="sd">    Input Annotation types               Output Annotation type</span>
<span class="sd">    ==================================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN, WORD_EMBEDDINGS`` ``NAMED_ENTITY``</span>
<span class="sd">    ==================================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    labelColumn</span>
<span class="sd">        Column with label per each token</span>
<span class="sd">    entities</span>
<span class="sd">        Entities to recognize</span>
<span class="sd">    minEpochs</span>
<span class="sd">        Minimum number of epochs to train, by default 0</span>
<span class="sd">    maxEpochs</span>
<span class="sd">        Maximum number of epochs to train, by default 50</span>
<span class="sd">    verbose</span>
<span class="sd">        Level of verbosity during training, by default 2</span>
<span class="sd">    randomSeed</span>
<span class="sd">        Random seed</span>
<span class="sd">    lr</span>
<span class="sd">        Learning Rate, by default 0.001</span>
<span class="sd">    po</span>
<span class="sd">        Learning rate decay coefficient. Real Learning Rage = lr / (1 + po *</span>
<span class="sd">        epoch), by default 0.005</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size, by default 8</span>
<span class="sd">    dropout</span>
<span class="sd">        Dropout coefficient, by default 0.5</span>
<span class="sd">    graphFolder</span>
<span class="sd">        Folder path that contain external graph files</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    useContrib</span>
<span class="sd">        whether to use contrib LSTM Cells. Not compatible with Windows. Might</span>
<span class="sd">        slightly improve accuracy</span>
<span class="sd">    validationSplit</span>
<span class="sd">        Choose the proportion of training dataset to be validated against the</span>
<span class="sd">        model on each Epoch. The value should be between 0.0 and 1.0 and by</span>
<span class="sd">        default it is 0.0 and off, by default 0.0</span>
<span class="sd">    evaluationLogExtended</span>
<span class="sd">        Whether logs for validation to be extended, by default False.</span>
<span class="sd">    testDataset</span>
<span class="sd">        Path to test dataset. If set used to calculate statistic on it during</span>
<span class="sd">        training.</span>
<span class="sd">    includeConfidence</span>
<span class="sd">        whether to include confidence scores in annotation metadata, by default</span>
<span class="sd">        False</span>
<span class="sd">    includeAllConfidenceScores</span>
<span class="sd">        whether to include all confidence scores in annotation metadata or just</span>
<span class="sd">        the score of the predicted tag, by default False</span>
<span class="sd">    enableOutputLogs</span>
<span class="sd">        Whether to use stdout in addition to Spark logs, by default False</span>
<span class="sd">    outputLogsPath</span>
<span class="sd">        Folder path to save training logs</span>
<span class="sd">    enableMemoryOptimizer</span>
<span class="sd">        Whether to optimize for large datasets or not. Enabling this option can</span>
<span class="sd">        slow down training, by default False</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.training import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    This CoNLL dataset already includes a sentence, token and label</span>
<span class="sd">    column with their respective annotator types. If a custom dataset is used,</span>
<span class="sd">    these need to be defined with for example:</span>

<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>

<span class="sd">    Then the training can start</span>

<span class="sd">    &gt;&gt;&gt; embeddings = BertEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; nerTagger = NerDLApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setLabelColumn(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;ner&quot;) \\</span>
<span class="sd">    ...     .setMaxEpochs(1) \\</span>
<span class="sd">    ...     .setRandomSeed(0) \\</span>
<span class="sd">    ...     .setVerbose(0)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     nerTagger</span>
<span class="sd">    ... ])</span>

<span class="sd">    We use the sentences, tokens, and labels from the CoNLL dataset.</span>

<span class="sd">    &gt;&gt;&gt; conll = CoNLL()</span>
<span class="sd">    &gt;&gt;&gt; trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(trainingData)</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    NerCrfApproach : for a generic CRF approach</span>
<span class="sd">    NerConverter : to further process the results</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lr</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="s2">&quot;Learning Rate&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">po</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;po&quot;</span><span class="p">,</span> <span class="s2">&quot;Learning rate decay coefficient. Real Learning Rage = lr / (1 + po * epoch)&quot;</span><span class="p">,</span>
               <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">batchSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;batchSize&quot;</span><span class="p">,</span> <span class="s2">&quot;Batch size&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">dropout</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="s2">&quot;Dropout coefficient&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">graphFolder</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;graphFolder&quot;</span><span class="p">,</span> <span class="s2">&quot;Folder path that contain external graph files&quot;</span><span class="p">,</span>
                        <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">useContrib</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;useContrib&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;whether to use contrib LSTM Cells. Not compatible with Windows. Might slightly improve accuracy.&quot;</span><span class="p">,</span>
                       <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">validationSplit</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;validationSplit&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Choose the proportion of training dataset to be validated against the model on each Epoch. The value should be between 0.0 and 1.0 and by default it is 0.0 and off.&quot;</span><span class="p">,</span>
                            <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">evaluationLogExtended</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;evaluationLogExtended&quot;</span><span class="p">,</span>
                                  <span class="s2">&quot;Whether logs for validation to be extended: it displays time and evaluation of each label. Default is False.&quot;</span><span class="p">,</span>
                                  <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">testDataset</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;testDataset&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;Path to test dataset. If set used to calculate statistic on it during training.&quot;</span><span class="p">,</span>
                        <span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">includeConfidence</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;includeConfidence&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;whether to include confidence scores in annotation metadata&quot;</span><span class="p">,</span>
                              <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">includeAllConfidenceScores</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;includeAllConfidenceScores&quot;</span><span class="p">,</span>
                                       <span class="s2">&quot;whether to include all confidence scores in annotation metadata or just the score of the predicted tag&quot;</span><span class="p">,</span>
                                       <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">enableOutputLogs</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;enableOutputLogs&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;Whether to use stdout in addition to Spark logs.&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">outputLogsPath</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;outputLogsPath&quot;</span><span class="p">,</span> <span class="s2">&quot;Folder path to save training logs&quot;</span><span class="p">,</span>
                           <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">enableMemoryOptimizer</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;enableMemoryOptimizer&quot;</span><span class="p">,</span>
                                  <span class="s2">&quot;Whether to optimize for large datasets or not. Enabling this option can slow down training.&quot;</span><span class="p">,</span>
                                  <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

<div class="viewcode-block" id="NerDLApproach.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerDLApproach.setGraphFolder"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setGraphFolder">[docs]</a>    <span class="k">def</span> <span class="nf">setGraphFolder</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets folder path that contain external graph files.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        p : str</span>
<span class="sd">            Folder path that contain external graph files</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">graphFolder</span><span class="o">=</span><span class="n">p</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerDLApproach.setUseContrib"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setUseContrib">[docs]</a>    <span class="k">def</span> <span class="nf">setUseContrib</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to use contrib LSTM Cells. Not compatible with Windows.</span>
<span class="sd">        Might slightly improve accuracy.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : bool</span>
<span class="sd">            Whether to use contrib LSTM Cells</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        Exception</span>
<span class="sd">            Windows not supported to use contrib</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">v</span> <span class="ow">and</span> <span class="n">sys</span><span class="o">.</span><span class="n">version</span> <span class="o">==</span> <span class="s1">&#39;win32&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Windows not supported to use contrib&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">useContrib</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerDLApproach.setLr"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setLr">[docs]</a>    <span class="k">def</span> <span class="nf">setLr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets Learning Rate, by default 0.001.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Learning Rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="NerDLApproach.setPo"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setPo">[docs]</a>    <span class="k">def</span> <span class="nf">setPo</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets Learning rate decay coefficient, by default 0.005.</span>

<span class="sd">        Real Learning Rage is lr / (1 + po * epoch).</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Learning rate decay coefficient</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">po</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="NerDLApproach.setBatchSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setBatchSize">[docs]</a>    <span class="k">def</span> <span class="nf">setBatchSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets batch size, by default 64.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : int</span>
<span class="sd">            Batch size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">batchSize</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="NerDLApproach.setDropout"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setDropout">[docs]</a>    <span class="k">def</span> <span class="nf">setDropout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets dropout coefficient, by default 0.5.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Dropout coefficient</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">dropout</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">NerDLModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span>

<div class="viewcode-block" id="NerDLApproach.setValidationSplit"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setValidationSplit">[docs]</a>    <span class="k">def</span> <span class="nf">setValidationSplit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the proportion of training dataset to be validated against the</span>
<span class="sd">        model on each Epoch, by default it is 0.0 and off. The value should be</span>
<span class="sd">        between 0.0 and 1.0.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Proportion of training dataset to be validated</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">validationSplit</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="NerDLApproach.setEvaluationLogExtended"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setEvaluationLogExtended">[docs]</a>    <span class="k">def</span> <span class="nf">setEvaluationLogExtended</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether logs for validation to be extended, by default False.</span>
<span class="sd">        Displays time and evaluation of each label.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : bool</span>
<span class="sd">            Whether logs for validation to be extended</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">evaluationLogExtended</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="NerDLApproach.setTestDataset"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setTestDataset">[docs]</a>    <span class="k">def</span> <span class="nf">setTestDataset</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">SPARK</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;parquet&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets Path to test dataset. If set used to calculate statistic on it</span>
<span class="sd">        during training.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to test dataset</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the resource, by default ReadAs.SPARK</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options for reading the resource, by default {&quot;format&quot;: &quot;parquet&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">testDataset</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()))</span></div>

<div class="viewcode-block" id="NerDLApproach.setIncludeConfidence"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setIncludeConfidence">[docs]</a>    <span class="k">def</span> <span class="nf">setIncludeConfidence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to include confidence scores in annotation metadata, by</span>
<span class="sd">        default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to include the confidence value in the output.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">includeConfidence</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerDLApproach.setIncludeAllConfidenceScores"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setIncludeAllConfidenceScores">[docs]</a>    <span class="k">def</span> <span class="nf">setIncludeAllConfidenceScores</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to include all confidence scores in annotation metadata</span>
<span class="sd">        or just the score of the predicted tag, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to include all confidence scores in annotation metadata or</span>
<span class="sd">            just the score of the predicted tag</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">includeAllConfidenceScores</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerDLApproach.setEnableOutputLogs"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setEnableOutputLogs">[docs]</a>    <span class="k">def</span> <span class="nf">setEnableOutputLogs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to use stdout in addition to Spark logs, by default</span>
<span class="sd">        False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to use stdout in addition to Spark logs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">enableOutputLogs</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerDLApproach.setEnableMemoryOptimizer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setEnableMemoryOptimizer">[docs]</a>    <span class="k">def</span> <span class="nf">setEnableMemoryOptimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets Whether to optimize for large datasets or not, by default False.</span>
<span class="sd">        Enabling this option can slow down training.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to optimize for large datasets</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">enableMemoryOptimizer</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerDLApproach.setOutputLogsPath"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLApproach.html#sparknlp.annotator.NerDLApproach.setOutputLogsPath">[docs]</a>    <span class="k">def</span> <span class="nf">setOutputLogsPath</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets folder path to save training logs.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        p : str</span>
<span class="sd">            Folder path to save training logs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">outputLogsPath</span><span class="o">=</span><span class="n">p</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NerDLApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach&quot;</span><span class="p">)</span>
        <span class="n">uc</span> <span class="o">=</span> <span class="kc">False</span> <span class="k">if</span> <span class="n">sys</span><span class="o">.</span><span class="n">platform</span> <span class="o">==</span> <span class="s1">&#39;win32&#39;</span> <span class="k">else</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">minEpochs</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">maxEpochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.001</span><span class="p">),</span>
            <span class="n">po</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.005</span><span class="p">),</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">verbose</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">useContrib</span><span class="o">=</span><span class="n">uc</span><span class="p">,</span>
            <span class="n">validationSplit</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">evaluationLogExtended</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">includeConfidence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">includeAllConfidenceScores</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">enableOutputLogs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">enableMemoryOptimizer</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="NerDLModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLModel.html#sparknlp.annotator.NerDLModel">[docs]</a><span class="k">class</span> <span class="nc">NerDLModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasStorageRef</span><span class="p">,</span> <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This Named Entity recognition annotator is a generic NER model based on</span>
<span class="sd">    Neural Networks.</span>

<span class="sd">    Neural Network architecture is Char CNNs - BiLSTM - CRF that achieves</span>
<span class="sd">    state-of-the-art in most datasets.</span>

<span class="sd">    This is the instantiated model of the :class:`.NerDLApproach`. For training</span>
<span class="sd">    your own model, please see the documentation of that class.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; nerModel = NerDLModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;ner&quot;)</span>


<span class="sd">    The default model is ``&quot;ner_dl&quot;``, if no name is provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition&gt;`__.</span>
<span class="sd">    Additionally, pretrained pipelines are available for this module, see</span>
<span class="sd">    `Pipelines &lt;https://nlp.johnsnowlabs.com/docs/en/pipelines&gt;`__.</span>

<span class="sd">    Note that some pretrained models require specific types of embeddings,</span>
<span class="sd">    depending on which they were trained on. For example, the default model</span>
<span class="sd">    ``&quot;ner_dl&quot;`` requires the WordEmbeddings ``&quot;glove_100d&quot;``.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb&gt;`__.</span>

<span class="sd">    ==================================== ======================</span>
<span class="sd">    Input Annotation types               Output Annotation type</span>
<span class="sd">    ==================================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN, WORD_EMBEDDINGS`` ``NAMED_ENTITY``</span>
<span class="sd">    ==================================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Size of every batch, by default 8</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    includeConfidence</span>
<span class="sd">        Whether to include confidence scores in annotation metadata, by default</span>
<span class="sd">        False</span>
<span class="sd">    includeAllConfidenceScores</span>
<span class="sd">        Whether to include all confidence scores in annotation metadata or just</span>
<span class="sd">        the score of the predicted tag, by default False</span>
<span class="sd">    classes</span>
<span class="sd">        Tags used to trained this NerDLModel</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    First extract the prerequisites for the NerDLModel</span>

<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = WordEmbeddingsModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;bert&quot;)</span>

<span class="sd">    Then NER can be extracted</span>

<span class="sd">    &gt;&gt;&gt; nerTagger = NerDLModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;bert&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;ner&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     nerTagger</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;U.N. official Ekeus heads for Baghdad.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;ner.result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------+</span>
<span class="sd">    |result                              |</span>
<span class="sd">    +------------------------------------+</span>
<span class="sd">    |[B-ORG, O, O, B-PER, O, O, B-LOC, O]|</span>
<span class="sd">    +------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    NerCrfModel : for a generic CRF approach</span>
<span class="sd">    NerConverter : to further process the results</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;NerDLModel&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NerDLModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">includeConfidence</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">includeAllConfidenceScores</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span>
        <span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>
    <span class="n">includeConfidence</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;includeConfidence&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;whether to include confidence scores in annotation metadata&quot;</span><span class="p">,</span>
                              <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>
    <span class="n">includeAllConfidenceScores</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;includeAllConfidenceScores&quot;</span><span class="p">,</span>
                                       <span class="s2">&quot;whether to include all confidence scores in annotation metadata or just the score of the predicted tag&quot;</span><span class="p">,</span>
                                       <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;classes&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;get the tags used to trained this NerDLModel&quot;</span><span class="p">,</span>
                    <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="NerDLModel.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLModel.html#sparknlp.annotator.NerDLModel.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerDLModel.setIncludeConfidence"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLModel.html#sparknlp.annotator.NerDLModel.setIncludeConfidence">[docs]</a>    <span class="k">def</span> <span class="nf">setIncludeConfidence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to include confidence scores in annotation metadata, by</span>
<span class="sd">        default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to include the confidence value in the output.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">includeConfidence</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerDLModel.setIncludeAllConfidenceScores"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLModel.html#sparknlp.annotator.NerDLModel.setIncludeAllConfidenceScores">[docs]</a>    <span class="k">def</span> <span class="nf">setIncludeAllConfidenceScores</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to include all confidence scores in annotation metadata</span>
<span class="sd">        or just the score of the predicted tag, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to include all confidence scores in annotation metadata or</span>
<span class="sd">            just the score of the predicted tag</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">includeAllConfidenceScores</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerDLModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerDLModel.html#sparknlp.annotator.NerDLModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;ner_dl&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;ner_dl&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        NerDLModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">NerDLModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="NerConverter"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerConverter.html#sparknlp.annotator.NerConverter">[docs]</a><span class="k">class</span> <span class="nc">NerConverter</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts a IOB or IOB2 representation of NER to a user-friendly one, by</span>
<span class="sd">    associating the tokens of recognized entities and their label. Results in</span>
<span class="sd">    ``CHUNK`` Annotation type.</span>

<span class="sd">    NER chunks can then be filtered by setting a whitelist with</span>
<span class="sd">    ``setWhiteList``. Chunks with no associated entity (tagged &quot;O&quot;) are</span>
<span class="sd">    filtered.</span>

<span class="sd">    See also `Insideoutsidebeginning (tagging)</span>
<span class="sd">    &lt;https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)&gt;`__</span>
<span class="sd">    for more information.</span>

<span class="sd">    ================================= ======================</span>
<span class="sd">    Input Annotation types            Output Annotation type</span>
<span class="sd">    ================================= ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN, NAMED_ENTITY`` ``CHUNK``</span>
<span class="sd">    ================================= ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    whiteList</span>
<span class="sd">        If defined, list of entities to process. The rest will be ignored. Do</span>
<span class="sd">        not include IOB prefix on labels</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    This is a continuation of the example of the :class:`.NerDLModel`. See that</span>
<span class="sd">    class on how to extract the entities. The output of the NerDLModel follows</span>
<span class="sd">    the Annotator schema and can be converted like so:</span>

<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(ner)&quot;).show(truncate=False)</span>
<span class="sd">    +----------------------------------------------------+</span>
<span class="sd">    |col                                                 |</span>
<span class="sd">    +----------------------------------------------------+</span>
<span class="sd">    |[named_entity, 0, 2, B-ORG, [word -&gt; U.N], []]      |</span>
<span class="sd">    |[named_entity, 3, 3, O, [word -&gt; .], []]            |</span>
<span class="sd">    |[named_entity, 5, 12, O, [word -&gt; official], []]    |</span>
<span class="sd">    |[named_entity, 14, 18, B-PER, [word -&gt; Ekeus], []]  |</span>
<span class="sd">    |[named_entity, 20, 24, O, [word -&gt; heads], []]      |</span>
<span class="sd">    |[named_entity, 26, 28, O, [word -&gt; for], []]        |</span>
<span class="sd">    |[named_entity, 30, 36, B-LOC, [word -&gt; Baghdad], []]|</span>
<span class="sd">    |[named_entity, 37, 37, O, [word -&gt; .], []]          |</span>
<span class="sd">    +----------------------------------------------------+</span>

<span class="sd">    After the converter is used:</span>

<span class="sd">    &gt;&gt;&gt; converter = NerConverter() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;entities&quot;)</span>
<span class="sd">    &gt;&gt;&gt; converter.transform(result).selectExpr(&quot;explode(entities)&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------------------+</span>
<span class="sd">    |col                                                                     |</span>
<span class="sd">    +------------------------------------------------------------------------+</span>
<span class="sd">    |[chunk, 0, 2, U.N, [entity -&gt; ORG, sentence -&gt; 0, chunk -&gt; 0], []]      |</span>
<span class="sd">    |[chunk, 14, 18, Ekeus, [entity -&gt; PER, sentence -&gt; 0, chunk -&gt; 1], []]  |</span>
<span class="sd">    |[chunk, 30, 36, Baghdad, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 2], []]|</span>
<span class="sd">    +------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;NerConverter&#39;</span>

    <span class="n">whiteList</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span>
        <span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
        <span class="s2">&quot;whiteList&quot;</span><span class="p">,</span>
        <span class="s2">&quot;If defined, list of entities to process. The rest will be ignored. Do not include IOB prefix on labels&quot;</span><span class="p">,</span>
        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span>
    <span class="p">)</span>

<div class="viewcode-block" id="NerConverter.setWhiteList"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerConverter.html#sparknlp.annotator.NerConverter.setWhiteList">[docs]</a>    <span class="k">def</span> <span class="nf">setWhiteList</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">entities</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets list of entities to process. The rest will be ignored.</span>

<span class="sd">        Does not include IOB prefix on labels.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        entities : List[str]</span>
<span class="sd">            If defined, list of entities to process. The rest will be ignored.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">whiteList</span><span class="o">=</span><span class="n">entities</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NerConverter</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.ner.NerConverter&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="DependencyParserApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DependencyParserApproach.html#sparknlp.annotator.DependencyParserApproach">[docs]</a><span class="k">class</span> <span class="nc">DependencyParserApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains an unlabeled parser that finds a grammatical relations between two</span>
<span class="sd">    words in a sentence.</span>

<span class="sd">    For instantiated/pretrained models, see :class:`.DependencyParserModel`.</span>

<span class="sd">    Dependency parser provides information about word relationship. For example,</span>
<span class="sd">    dependency parsing can tell you what the subjects and objects of a verb are,</span>
<span class="sd">    as well as which words are modifying (describing) the subject. This can help</span>
<span class="sd">    you find precise answers to specific questions.</span>

<span class="sd">    The required training data can be set in two different ways (only one can be</span>
<span class="sd">    chosen for a particular model):</span>

<span class="sd">    - Dependency treebank in the</span>
<span class="sd">      `Penn Treebank format &lt;http://www.nltk.org/nltk_data/&gt;`__ set with</span>
<span class="sd">      ``setDependencyTreeBank``</span>
<span class="sd">    - Dataset in the</span>
<span class="sd">      `CoNLL-U format &lt;https://universaldependencies.org/format.html&gt;`__ set</span>
<span class="sd">      with ``setConllU``</span>

<span class="sd">    Apart from that, no additional training data is needed.</span>

<span class="sd">    ======================== ======================</span>
<span class="sd">    Input Annotation types   Output Annotation type</span>
<span class="sd">    ======================== ======================</span>
<span class="sd">    ``DOCUMENT, POS, TOKEN`` ``DEPENDENCY``</span>
<span class="sd">    ======================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dependencyTreeBank</span>
<span class="sd">        Dependency treebank source files</span>
<span class="sd">    conllU</span>
<span class="sd">        Universal Dependencies source files</span>
<span class="sd">    numberOfIterations</span>
<span class="sd">        Number of iterations in training, converges to better accuracy,</span>
<span class="sd">        by default 10</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; posTagger = PerceptronModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;pos&quot;)</span>
<span class="sd">    &gt;&gt;&gt; dependencyParserApproach = DependencyParserApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;dependency&quot;) \\</span>
<span class="sd">    ...     .setDependencyTreeBank(&quot;src/test/resources/parser/unlabeled/dependency_treebank&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     posTagger,</span>
<span class="sd">    ...     dependencyParserApproach</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; emptyDataSet = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(emptyDataSet)</span>

<span class="sd">    Additional training data is not needed, the dependency parser relies on the</span>
<span class="sd">    dependency tree bank / CoNLL-U only.</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    TypedDependencyParserApproach : to extract labels for the dependencies</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dependencyTreeBank</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                               <span class="s2">&quot;dependencyTreeBank&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;Dependency treebank source files&quot;</span><span class="p">,</span>
                               <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">conllU</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                   <span class="s2">&quot;conllU&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;Universal Dependencies source files&quot;</span><span class="p">,</span>
                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">numberOfIterations</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                               <span class="s2">&quot;numberOfIterations&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;Number of iterations in training, converges to better accuracy&quot;</span><span class="p">,</span>
                               <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DependencyParserApproach</span><span class="p">,</span>
              <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserApproach&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span><span class="n">numberOfIterations</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<div class="viewcode-block" id="DependencyParserApproach.setNumberOfIterations"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DependencyParserApproach.html#sparknlp.annotator.DependencyParserApproach.setNumberOfIterations">[docs]</a>    <span class="k">def</span> <span class="nf">setNumberOfIterations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets number of iterations in training, converges to better accuracy,</span>
<span class="sd">        by default 10.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Number of iterations</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">numberOfIterations</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="DependencyParserApproach.setDependencyTreeBank"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DependencyParserApproach.html#sparknlp.annotator.DependencyParserApproach.setDependencyTreeBank">[docs]</a>    <span class="k">def</span> <span class="nf">setDependencyTreeBank</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;value&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets dependency treebank source files.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the source files</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the file, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options to read the resource, by default {&quot;key&quot;: &quot;value&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">dependencyTreeBank</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">opts</span><span class="p">))</span></div>

<div class="viewcode-block" id="DependencyParserApproach.setConllU"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DependencyParserApproach.html#sparknlp.annotator.DependencyParserApproach.setConllU">[docs]</a>    <span class="k">def</span> <span class="nf">setConllU</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;value&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets Universal Dependencies source files.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the source files</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the file, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options to read the resource, by default {&quot;key&quot;: &quot;value&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">conllU</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">opts</span><span class="p">))</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">DependencyParserModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="DependencyParserModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DependencyParserModel.html#sparknlp.annotator.DependencyParserModel">[docs]</a><span class="k">class</span> <span class="nc">DependencyParserModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Unlabeled parser that finds a grammatical relation between two words in a</span>
<span class="sd">    sentence.</span>

<span class="sd">    Dependency parser provides information about word relationship. For example,</span>
<span class="sd">    dependency parsing can tell you what the subjects and objects of a verb are,</span>
<span class="sd">    as well as which words are modifying (describing) the subject. This can help</span>
<span class="sd">    you find precise answers to specific questions.</span>

<span class="sd">    This is the instantiated model of the :class:`.DependencyParserApproach`.</span>
<span class="sd">    For training your own model, please see the documentation of that class.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; dependencyParserApproach = DependencyParserModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;dependency&quot;)</span>


<span class="sd">    The default model is ``&quot;dependency_conllu&quot;``, if no name is provided.</span>
<span class="sd">    For available pretrained models please see the</span>
<span class="sd">    `Models Hub &lt;https://nlp.johnsnowlabs.com/models&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb&gt;`__.</span>

<span class="sd">    ================================ ======================</span>
<span class="sd">    Input Annotation types           Output Annotation type</span>
<span class="sd">    ================================ ======================</span>
<span class="sd">    ``[String]DOCUMENT, POS, TOKEN`` ``DEPENDENCY``</span>
<span class="sd">    ================================ ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    perceptron</span>
<span class="sd">        Dependency parsing perceptron features</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; posTagger = PerceptronModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;pos&quot;)</span>
<span class="sd">    &gt;&gt;&gt; dependencyParser = DependencyParserModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;dependency&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     posTagger,</span>
<span class="sd">    ...     dependencyParser</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[</span>
<span class="sd">    ...     &quot;Unions representing workers at Turner Newall say they are &#39;disappointed&#39; after talks with stricken parent &quot; +</span>
<span class="sd">    ...     &quot;firm Federal Mogul.&quot;</span>
<span class="sd">    ... ]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(arrays_zip(token.result, dependency.result)) as cols&quot;) \\</span>
<span class="sd">    ...     .selectExpr(&quot;cols[&#39;0&#39;] as token&quot;, &quot;cols[&#39;1&#39;] as dependency&quot;).show(8, truncate = False)</span>
<span class="sd">    +------------+------------+</span>
<span class="sd">    |token       |dependency  |</span>
<span class="sd">    +------------+------------+</span>
<span class="sd">    |Unions      |ROOT        |</span>
<span class="sd">    |representing|workers     |</span>
<span class="sd">    |workers     |Unions      |</span>
<span class="sd">    |at          |Turner      |</span>
<span class="sd">    |Turner      |workers     |</span>
<span class="sd">    |Newall      |say         |</span>
<span class="sd">    |say         |Unions      |</span>
<span class="sd">    |they        |disappointed|</span>
<span class="sd">    +------------+------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    TypedDependencyParserMdoel : to extract labels for the dependencies</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;DependencyParserModel&quot;</span>

    <span class="n">perceptron</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;perceptron&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;Dependency parsing perceptron features&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DependencyParserModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="DependencyParserModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DependencyParserModel.html#sparknlp.annotator.DependencyParserModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;dependency_conllu&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;dependency_conllu&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DependencyParserModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">DependencyParserModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="TypedDependencyParserApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TypedDependencyParserApproach.html#sparknlp.annotator.TypedDependencyParserApproach">[docs]</a><span class="k">class</span> <span class="nc">TypedDependencyParserApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Labeled parser that finds a grammatical relation between two words in a</span>
<span class="sd">    sentence. Its input is either a CoNLL2009 or ConllU dataset.</span>

<span class="sd">    For instantiated/pretrained models, see</span>
<span class="sd">    :class:`.TypedDependencyParserModel`.</span>

<span class="sd">    Dependency parsers provide information about word relationship. For example,</span>
<span class="sd">    dependency parsing can tell you what the subjects and objects of a verb are,</span>
<span class="sd">    as well as which words are modifying (describing) the subject. This can help</span>
<span class="sd">    you find precise answers to specific questions.</span>

<span class="sd">    The parser requires the dependant tokens beforehand with e.g.</span>
<span class="sd">    DependencyParser. The required training data can be set in two different</span>
<span class="sd">    ways (only one can be chosen for a particular model):</span>

<span class="sd">    - Dataset in the `CoNLL 2009 format</span>
<span class="sd">      &lt;https://ufal.mff.cuni.cz/conll2009-st/trial-data.html&gt;`__ set with</span>
<span class="sd">      :meth:`.setConll2009`</span>
<span class="sd">    - Dataset in the `CoNLL-U format</span>
<span class="sd">      &lt;https://universaldependencies.org/format.html&gt;`__ set with</span>
<span class="sd">      :meth:`.setConllU`</span>

<span class="sd">    Apart from that, no additional training data is needed.</span>

<span class="sd">    ========================== ======================</span>
<span class="sd">    Input Annotation types     Output Annotation type</span>
<span class="sd">    ========================== ======================</span>
<span class="sd">    ``TOKEN, POS, DEPENDENCY`` ``LABELED_DEPENDENCY``</span>
<span class="sd">    ========================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    conll2009</span>
<span class="sd">        Path to file with CoNLL 2009 format</span>
<span class="sd">    conllU</span>
<span class="sd">        Universal Dependencies source files</span>
<span class="sd">    numberOfIterations</span>
<span class="sd">        Number of iterations in training, converges to better accuracy</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; posTagger = PerceptronModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;pos&quot;)</span>
<span class="sd">    &gt;&gt;&gt; dependencyParser = DependencyParserModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;dependency&quot;)</span>
<span class="sd">    &gt;&gt;&gt; typedDependencyParser = TypedDependencyParserApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;dependency_type&quot;) \\</span>
<span class="sd">    ...     .setConllU(&quot;src/test/resources/parser/labeled/train_small.conllu.txt&quot;) \\</span>
<span class="sd">    ...     .setNumberOfIterations(1)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     posTagger,</span>
<span class="sd">    ...     dependencyParser,</span>
<span class="sd">    ...     typedDependencyParser</span>
<span class="sd">    ... ])</span>

<span class="sd">    Additional training data is not needed, the dependency parser relies on</span>
<span class="sd">    CoNLL-U only.</span>

<span class="sd">    &gt;&gt;&gt; emptyDataSet = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(emptyDataSet)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">conll2009</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;conll2009&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Path to file with CoNLL 2009 format&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">conllU</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                   <span class="s2">&quot;conllU&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;Universal Dependencies source files&quot;</span><span class="p">,</span>
                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">numberOfIterations</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                               <span class="s2">&quot;numberOfIterations&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;Number of iterations in training, converges to better accuracy&quot;</span><span class="p">,</span>
                               <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TypedDependencyParserApproach</span><span class="p">,</span>
              <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserApproach&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="TypedDependencyParserApproach.setConll2009"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TypedDependencyParserApproach.html#sparknlp.annotator.TypedDependencyParserApproach.setConll2009">[docs]</a>    <span class="k">def</span> <span class="nf">setConll2009</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;value&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets path to file with CoNLL 2009 format.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the resource</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the resource, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options for reading the resource, by default {&quot;key&quot;: &quot;value&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">conll2009</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">opts</span><span class="p">))</span></div>

<div class="viewcode-block" id="TypedDependencyParserApproach.setConllU"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TypedDependencyParserApproach.html#sparknlp.annotator.TypedDependencyParserApproach.setConllU">[docs]</a>    <span class="k">def</span> <span class="nf">setConllU</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;key&quot;</span><span class="p">:</span> <span class="s2">&quot;value&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets path to Universal Dependencies source files.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the resource</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to read the resource, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options for reading the resource, by default {&quot;key&quot;: &quot;value&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">opts</span> <span class="o">=</span> <span class="n">options</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">conllU</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">opts</span><span class="p">))</span></div>

<div class="viewcode-block" id="TypedDependencyParserApproach.setNumberOfIterations"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TypedDependencyParserApproach.html#sparknlp.annotator.TypedDependencyParserApproach.setNumberOfIterations">[docs]</a>    <span class="k">def</span> <span class="nf">setNumberOfIterations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets Number of iterations in training, converges to better accuracy.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Number of iterations in training</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        [type]</span>
<span class="sd">            [description]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">numberOfIterations</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">TypedDependencyParserModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="TypedDependencyParserModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TypedDependencyParserModel.html#sparknlp.annotator.TypedDependencyParserModel">[docs]</a><span class="k">class</span> <span class="nc">TypedDependencyParserModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Labeled parser that finds a grammatical relation between two words in a</span>
<span class="sd">    sentence. Its input is either a CoNLL2009 or ConllU dataset.</span>

<span class="sd">    Dependency parsers provide information about word relationship. For example,</span>
<span class="sd">    dependency parsing can tell you what the subjects and objects of a verb are,</span>
<span class="sd">    as well as which words are modifying (describing) the subject. This can help</span>
<span class="sd">    you find precise answers to specific questions.</span>

<span class="sd">    The parser requires the dependant tokens beforehand with e.g.</span>
<span class="sd">    DependencyParser.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; typedDependencyParser = TypedDependencyParserModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;dependency_type&quot;)</span>

<span class="sd">    The default model is ``&quot;dependency_typed_conllu&quot;``, if no name is provided.</span>
<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb&gt;`__.</span>

<span class="sd">    ========================== ======================</span>
<span class="sd">    Input Annotation types     Output Annotation type</span>
<span class="sd">    ========================== ======================</span>
<span class="sd">    ``TOKEN, POS, DEPENDENCY`` ``LABELED_DEPENDENCY``</span>
<span class="sd">    ========================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; posTagger = PerceptronModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;pos&quot;)</span>
<span class="sd">    &gt;&gt;&gt; dependencyParser = DependencyParserModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;dependency&quot;)</span>
<span class="sd">    &gt;&gt;&gt; typedDependencyParser = TypedDependencyParserModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;dependency_type&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     posTagger,</span>
<span class="sd">    ...     dependencyParser,</span>
<span class="sd">    ...     typedDependencyParser</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[</span>
<span class="sd">    ...     &quot;Unions representing workers at Turner Newall say they are &#39;disappointed&#39; after talks with stricken parent &quot; +</span>
<span class="sd">    ...       &quot;firm Federal Mogul.&quot;</span>
<span class="sd">    ... ]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(arrays_zip(token.result, dependency.result, dependency_type.result)) as cols&quot;) \\</span>
<span class="sd">    ...     .selectExpr(&quot;cols[&#39;0&#39;] as token&quot;, &quot;cols[&#39;1&#39;] as dependency&quot;, &quot;cols[&#39;2&#39;] as dependency_type&quot;) \\</span>
<span class="sd">    ...     .show(8, truncate = False)</span>
<span class="sd">    +------------+------------+---------------+</span>
<span class="sd">    |token       |dependency  |dependency_type|</span>
<span class="sd">    +------------+------------+---------------+</span>
<span class="sd">    |Unions      |ROOT        |root           |</span>
<span class="sd">    |representing|workers     |amod           |</span>
<span class="sd">    |workers     |Unions      |flat           |</span>
<span class="sd">    |at          |Turner      |case           |</span>
<span class="sd">    |Turner      |workers     |flat           |</span>
<span class="sd">    |Newall      |say         |nsubj          |</span>
<span class="sd">    |say         |Unions      |parataxis      |</span>
<span class="sd">    |they        |disappointed|nsubj          |</span>
<span class="sd">    +------------+------------+---------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;TypedDependencyParserModel&quot;</span>

    <span class="n">trainOptions</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;trainOptions&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;Training Options&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">trainParameters</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;trainParameters&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Training Parameters&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">trainDependencyPipe</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                <span class="s2">&quot;trainDependencyPipe&quot;</span><span class="p">,</span>
                                <span class="s2">&quot;Training dependency pipe&quot;</span><span class="p">,</span>
                                <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">conllFormat</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;conllFormat&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;CoNLL Format&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserModel&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TypedDependencyParserModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="TypedDependencyParserModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.TypedDependencyParserModel.html#sparknlp.annotator.TypedDependencyParserModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;dependency_typed_conllu&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;dependency_typed_conllu&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        TypedDependencyParserModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">TypedDependencyParserModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="WordEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordEmbeddings.html#sparknlp.annotator.WordEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">WordEmbeddings</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">,</span> <span class="n">HasEmbeddingsProperties</span><span class="p">,</span> <span class="n">HasStorage</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Word Embeddings lookup annotator that maps tokens to vectors.</span>

<span class="sd">    For instantiated/pretrained models, see :class:`.WordEmbeddingsModel`.</span>

<span class="sd">    A custom token lookup dictionary for embeddings can be set with</span>
<span class="sd">    :meth:`.setStoragePath`. Each line of the provided file needs to have a</span>
<span class="sd">    token, followed by their vector representation, delimited by a spaces::</span>

<span class="sd">        ...</span>
<span class="sd">        are 0.39658191506190343 0.630968081620067 0.5393722253731201 0.8428180123359783</span>
<span class="sd">        were 0.7535235923631415 0.9699218875629833 0.10397182122983872 0.11833962569383116</span>
<span class="sd">        stress 0.0492683418305907 0.9415954572751959 0.47624463167525755 0.16790967216778263</span>
<span class="sd">        induced 0.1535748762292387 0.33498936903209897 0.9235178224122094 0.1158772920395934</span>
<span class="sd">        ...</span>


<span class="sd">    If a token is not found in the dictionary, then the result will be a zero</span>
<span class="sd">    vector of the same dimension. Statistics about the rate of converted tokens,</span>
<span class="sd">    can be retrieved with :meth:`WordEmbeddingsModel.withCoverageColumn()</span>
<span class="sd">    &lt;sparknlp.annotator.WordEmbeddingsModel.withCoverageColumn&gt;` and</span>
<span class="sd">    :meth:`WordEmbeddingsModel.overallCoverage()</span>
<span class="sd">    &lt;sparknlp.annotator.WordEmbeddingsModel.overallCoverage&gt;`.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``WORD_EMBEDDINGS``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    writeBufferSize</span>
<span class="sd">        Buffer size limit before dumping to disk storage while writing, by</span>
<span class="sd">        default 10000</span>
<span class="sd">    readCacheSize</span>
<span class="sd">        Cache size for items retrieved from storage. Increase for performance</span>
<span class="sd">        but higher memory consumption</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    In this example, the file ``random_embeddings_dim4.txt`` has the form of the</span>
<span class="sd">    content above.</span>

<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = WordEmbeddings() \\</span>
<span class="sd">    ...     .setStoragePath(&quot;src/test/resources/random_embeddings_dim4.txt&quot;, ReadAs.TEXT) \\</span>
<span class="sd">    ...     .setStorageRef(&quot;glove_4d&quot;) \\</span>
<span class="sd">    ...     .setDimension(4) \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True) \\</span>
<span class="sd">    ...     .setCleanAnnotations(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       tokenizer,</span>
<span class="sd">    ...       embeddings,</span>
<span class="sd">    ...       embeddingsFinisher</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;The patient was diagnosed with diabetes.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(truncate=False)</span>
<span class="sd">    +----------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                            |</span>
<span class="sd">    +----------------------------------------------------------------------------------+</span>
<span class="sd">    |[0.9439099431037903,0.4707513153553009,0.806300163269043,0.16176554560661316]     |</span>
<span class="sd">    |[0.7966810464859009,0.5551124811172485,0.8861005902290344,0.28284206986427307]    |</span>
<span class="sd">    |[0.025029370561242104,0.35177749395370483,0.052506182342767715,0.1887107789516449]|</span>
<span class="sd">    |[0.08617766946554184,0.8399239182472229,0.5395117998123169,0.7864698767662048]    |</span>
<span class="sd">    |[0.6599600911140442,0.16109347343444824,0.6041093468666077,0.8913561105728149]    |</span>
<span class="sd">    |[0.5955275893211365,0.01899011991918087,0.4397728443145752,0.8911281824111938]    |</span>
<span class="sd">    |[0.9840458631515503,0.7599489092826843,0.9417727589607239,0.8624503016471863]     |</span>
<span class="sd">    +----------------------------------------------------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    SentenceEmbeddings : to combine embeddings into a sentence-level representation</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;WordEmbeddings&quot;</span>

    <span class="n">writeBufferSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;writeBufferSize&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;buffer size limit before dumping to disk storage while writing&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">readCacheSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;readCacheSize&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;cache size for items retrieved from storage. Increase for performance but higher memory consumption&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

<div class="viewcode-block" id="WordEmbeddings.setWriteBufferSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordEmbeddings.html#sparknlp.annotator.WordEmbeddings.setWriteBufferSize">[docs]</a>    <span class="k">def</span> <span class="nf">setWriteBufferSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets buffer size limit before dumping to disk storage while writing,</span>
<span class="sd">        by default 10000.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : int</span>
<span class="sd">            Buffer size limit</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">writeBufferSize</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

<div class="viewcode-block" id="WordEmbeddings.setReadCacheSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordEmbeddings.html#sparknlp.annotator.WordEmbeddings.setReadCacheSize">[docs]</a>    <span class="k">def</span> <span class="nf">setReadCacheSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets cache size for items retrieved from storage. Increase for</span>
<span class="sd">        performance but higher memory consumption.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : int</span>
<span class="sd">            Cache size for items retrieved from storage</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">readCacheSize</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WordEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.WordEmbeddings&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">writeBufferSize</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
            <span class="n">storageRef</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">uid</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">WordEmbeddingsModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="WordEmbeddingsModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordEmbeddingsModel.html#sparknlp.annotator.WordEmbeddingsModel">[docs]</a><span class="k">class</span> <span class="nc">WordEmbeddingsModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasEmbeddingsProperties</span><span class="p">,</span> <span class="n">HasStorageModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Word Embeddings lookup annotator that maps tokens to vectors</span>

<span class="sd">    This is the instantiated model of :class:`.WordEmbeddings`.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = WordEmbeddingsModel.pretrained() \\</span>
<span class="sd">    ...       .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...       .setOutputCol(&quot;embeddings&quot;)</span>

<span class="sd">    The default model is ``&quot;glove_100d&quot;``, if no name is provided. For available</span>
<span class="sd">    pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Embeddings&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``WORD_EMBEDDINGS``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions</span>
<span class="sd">    readCacheSize</span>
<span class="sd">        Cache size for items retrieved from storage. Increase for performance</span>
<span class="sd">        but higher memory consumption</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    There are also two convenient functions to retrieve the embeddings coverage</span>
<span class="sd">    with respect to the transformed dataset:</span>

<span class="sd">    - :meth:`.withCoverageColumn`: Adds a custom</span>
<span class="sd">      column with word coverage stats for the embedded field. This creates</span>
<span class="sd">      a new column with statistics for each row.</span>
<span class="sd">    - :meth:`.overallCoverage`: Calculates overall word</span>
<span class="sd">      coverage for the whole data in the embedded field. This returns a single</span>
<span class="sd">      coverage object considering all rows in the field.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = WordEmbeddingsModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True) \\</span>
<span class="sd">    ...     .setCleanAnnotations(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       tokenizer,</span>
<span class="sd">    ...       embeddings,</span>
<span class="sd">    ...       embeddingsFinisher</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[-0.570580005645752,0.44183000922203064,0.7010200023651123,-0.417129993438720...|</span>
<span class="sd">    |[-0.542639970779419,0.4147599935531616,1.0321999788284302,-0.4024400115013122...|</span>
<span class="sd">    |[-0.2708599865436554,0.04400600120425224,-0.020260000601410866,-0.17395000159...|</span>
<span class="sd">    |[0.6191999912261963,0.14650000631809235,-0.08592499792575836,-0.2629800140857...|</span>
<span class="sd">    |[-0.3397899866104126,0.20940999686717987,0.46347999572753906,-0.6479200124740...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    SentenceEmbeddings : to combine embeddings into a sentence-level representation</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;WordEmbeddingsModel&quot;</span>
    <span class="n">databases</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;EMBEDDINGS&#39;</span><span class="p">]</span>

    <span class="n">readCacheSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;readCacheSize&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;cache size for items retrieved from storage. Increase for performance but higher memory consumption&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

<div class="viewcode-block" id="WordEmbeddingsModel.setReadCacheSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordEmbeddingsModel.html#sparknlp.annotator.WordEmbeddingsModel.setReadCacheSize">[docs]</a>    <span class="k">def</span> <span class="nf">setReadCacheSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets cache size for items retrieved from storage. Increase for</span>
<span class="sd">        performance but higher memory consumption.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : int</span>
<span class="sd">            Cache size for items retrieved from storage</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">readCacheSize</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WordEmbeddingsModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="WordEmbeddingsModel.overallCoverage"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordEmbeddingsModel.html#sparknlp.annotator.WordEmbeddingsModel.overallCoverage">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">overallCoverage</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">embeddings_col</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Calculates overall word coverage for the whole data in the embedded</span>
<span class="sd">        field.</span>

<span class="sd">        This returns a single coverage object considering all rows in the</span>
<span class="sd">        field.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        dataset : :class:`pyspark.sql.DataFrame`</span>
<span class="sd">            The dataset with embeddings column</span>
<span class="sd">        embeddings_col : str</span>
<span class="sd">            Name of the embeddings column</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        :class:`.CoverageResult`</span>
<span class="sd">            CoverateResult object with extracted information</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; wordsOverallCoverage = WordEmbeddingsModel.overallCoverage(</span>
<span class="sd">        ...     resultDF,&quot;embeddings&quot;</span>
<span class="sd">        ... ).percentage</span>
<span class="sd">        1.0</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_EmbeddingsOverallCoverage</span>
        <span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="n">CoverageResult</span>
        <span class="k">return</span> <span class="n">CoverageResult</span><span class="p">(</span><span class="n">_EmbeddingsOverallCoverage</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">embeddings_col</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">())</span></div>

<div class="viewcode-block" id="WordEmbeddingsModel.withCoverageColumn"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordEmbeddingsModel.html#sparknlp.annotator.WordEmbeddingsModel.withCoverageColumn">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">withCoverageColumn</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">embeddings_col</span><span class="p">,</span> <span class="n">output_col</span><span class="o">=</span><span class="s1">&#39;coverage&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds a custom column with word coverage stats for the embedded field.</span>
<span class="sd">        This creates a new column with statistics for each row.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        dataset : :class:`pyspark.sql.DataFrame`</span>
<span class="sd">            The dataset with embeddings column</span>
<span class="sd">        embeddings_col : str</span>
<span class="sd">            Name of the embeddings column</span>
<span class="sd">        output_col : str, optional</span>
<span class="sd">            Name for the resulting column, by default &#39;coverage&#39;</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        :class:`pyspark.sql.DataFrame`</span>
<span class="sd">            Dataframe with calculated coverage</span>

<span class="sd">        Examples</span>
<span class="sd">        --------</span>
<span class="sd">        &gt;&gt;&gt; wordsCoverage = WordEmbeddingsModel.withCoverageColumn(resultDF, &quot;embeddings&quot;, &quot;cov_embeddings&quot;)</span>
<span class="sd">        &gt;&gt;&gt; wordsCoverage.select(&quot;text&quot;,&quot;cov_embeddings&quot;).show(truncate=False)</span>
<span class="sd">        +-------------------+--------------+</span>
<span class="sd">        |text               |cov_embeddings|</span>
<span class="sd">        +-------------------+--------------+</span>
<span class="sd">        |This is a sentence.|[5, 5, 1.0]   |</span>
<span class="sd">        +-------------------+--------------+</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_EmbeddingsCoverageColumn</span>
        <span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">DataFrame</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="n">_EmbeddingsCoverageColumn</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">embeddings_col</span><span class="p">,</span> <span class="n">output_col</span><span class="p">)</span><span class="o">.</span><span class="n">apply</span><span class="p">(),</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sql_ctx</span><span class="p">)</span></div>

<div class="viewcode-block" id="WordEmbeddingsModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordEmbeddingsModel.html#sparknlp.annotator.WordEmbeddingsModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;glove_100d&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;glove_100d&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        WordEmbeddingsModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">WordEmbeddingsModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div>

<div class="viewcode-block" id="WordEmbeddingsModel.loadStorage"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordEmbeddingsModel.html#sparknlp.annotator.WordEmbeddingsModel.loadStorage">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadStorage</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">spark</span><span class="p">,</span> <span class="n">storage_ref</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads the model from storage.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the model</span>
<span class="sd">        spark : :class:`pyspark.sql.SparkSession`</span>
<span class="sd">            The current SparkSession</span>
<span class="sd">        storage_ref : str</span>
<span class="sd">            Identifiers for the model parameters</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">HasStorageModel</span><span class="o">.</span><span class="n">loadStorages</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">spark</span><span class="p">,</span> <span class="n">storage_ref</span><span class="p">,</span> <span class="n">WordEmbeddingsModel</span><span class="o">.</span><span class="n">databases</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BertEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertEmbeddings.html#sparknlp.annotator.BertEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">BertEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                     <span class="n">HasEmbeddingsProperties</span><span class="p">,</span>
                     <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                     <span class="n">HasStorageRef</span><span class="p">,</span>
                     <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Token-level embeddings using BERT.</span>

<span class="sd">    BERT (Bidirectional Encoder Representations from Transformers) provides</span>
<span class="sd">    dense vector representations for natural language by using a deep,</span>
<span class="sd">    pre-trained neural network with the Transformer architecture.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = BertEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;bert_embeddings&quot;)</span>


<span class="sd">    The default model is ``&quot;small_bert_L2_768&quot;``, if no name is provided.</span>

<span class="sd">    For available pretrained models please see the</span>
<span class="sd">    `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Embeddings&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/blogposts/3.NER_with_BERT.ipynb&gt;`__.</span>
<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``WORD_EMBEDDINGS``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Size of every batch , by default 8</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions, by default 768</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default False</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding &lt;https://arxiv.org/abs/1810.04805&gt;`__</span>

<span class="sd">    https://github.com/google-research/bert</span>

<span class="sd">    **Paper abstract**</span>

<span class="sd">    *We introduce a new language representation model called BERT, which stands</span>
<span class="sd">    for Bidirectional Encoder Representations from Transformers. Unlike recent</span>
<span class="sd">    language representation models, BERT is designed to pre-train deep</span>
<span class="sd">    bidirectional representations from unlabeled text by jointly conditioning on</span>
<span class="sd">    both left and right context in all layers. As a result, the pre-trained BERT</span>
<span class="sd">    model can be fine-tuned with just one additional output layer to create</span>
<span class="sd">    state-of-the-art models for a wide range of tasks, such as question</span>
<span class="sd">    answering and language inference, without substantial task-specific</span>
<span class="sd">    architecture modifications. BERT is conceptually simple and empirically</span>
<span class="sd">    powerful. It obtains new state-of-the-art results on eleven natural language</span>
<span class="sd">    processing tasks, including pushing the GLUE score to 80.5% (7.7% point</span>
<span class="sd">    absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute</span>
<span class="sd">    improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point</span>
<span class="sd">    absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute</span>
<span class="sd">    improvement).*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = BertEmbeddings.pretrained(&quot;small_bert_L2_128&quot;, &quot;en&quot;) \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;bert_embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;bert_embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     embeddingsFinisher</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[-2.3497989177703857,0.480538547039032,-0.3238905668258667,-1.612930893898010...|</span>
<span class="sd">    |[-2.1357314586639404,0.32984697818756104,-0.6032363176345825,-1.6791689395904...|</span>
<span class="sd">    |[-1.8244884014129639,-0.27088963985443115,-1.059438943862915,-0.9817547798156...|</span>
<span class="sd">    |[-1.1648050546646118,-0.4725411534309387,-0.5938255786895752,-1.5780693292617...|</span>
<span class="sd">    |[-0.9125322699546814,0.4563939869403839,-0.3975459933280945,-1.81611204147338...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;BertEmbeddings&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="BertEmbeddings.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertEmbeddings.html#sparknlp.annotator.BertEmbeddings.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="BertEmbeddings.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertEmbeddings.html#sparknlp.annotator.BertEmbeddings.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.BertEmbeddings&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BertEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

<div class="viewcode-block" id="BertEmbeddings.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertEmbeddings.html#sparknlp.annotator.BertEmbeddings.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        BertEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_BertLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_BertLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">BertEmbeddings</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="BertEmbeddings.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertEmbeddings.html#sparknlp.annotator.BertEmbeddings.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;small_bert_L2_768&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;small_bert_L2_768&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        BertEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">BertEmbeddings</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="BertSentenceEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertSentenceEmbeddings.html#sparknlp.annotator.BertSentenceEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">BertSentenceEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                             <span class="n">HasEmbeddingsProperties</span><span class="p">,</span>
                             <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                             <span class="n">HasStorageRef</span><span class="p">,</span>
                             <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sentence-level embeddings using BERT. BERT (Bidirectional Encoder</span>
<span class="sd">    Representations from Transformers) provides dense vector representations for</span>
<span class="sd">    natural language by using a deep, pre-trained neural network with the</span>
<span class="sd">    Transformer architecture.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt;embeddings = BertSentenceEmbeddings.pretrained() \\</span>
<span class="sd">    ...    .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...    .setOutputCol(&quot;sentence_bert_embeddings&quot;)</span>


<span class="sd">    The default model is ``&quot;sent_small_bert_L2_768&quot;``, if no name is provided.</span>

<span class="sd">    For available pretrained models please see the</span>
<span class="sd">    `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Embeddings&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the</span>
<span class="sd">    `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BERT%20Sentence.ipynb&gt;`__.</span>

<span class="sd">    ====================== =======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== =======================</span>
<span class="sd">    ``DOCUMENT``           ``SENTENCE_EMBEDDINGS``</span>
<span class="sd">    ====================== =======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Size of every batch, by default 8</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        False</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions, by default 768</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>
<span class="sd">    isLong</span>
<span class="sd">        Use Long type instead of Int type for inputs buffer - Some Bert models</span>
<span class="sd">        require Long instead of Int.</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding &lt;https://arxiv.org/abs/1810.04805&gt;`__</span>

<span class="sd">    https://github.com/google-research/bert</span>

<span class="sd">    **Paper abstract**</span>

<span class="sd">    *We introduce a new language representation model called BERT, which stands</span>
<span class="sd">    for Bidirectional Encoder Representations from Transformers. Unlike recent</span>
<span class="sd">    language representation models, BERT is designed to pre-train deep</span>
<span class="sd">    bidirectional representations from unlabeled text by jointly conditioning on</span>
<span class="sd">    both left and right context in all layers. As a result, the pre-trained BERT</span>
<span class="sd">    model can be fine-tuned with just one additional output layer to create</span>
<span class="sd">    state-of-the-art models for a wide range of tasks, such as question</span>
<span class="sd">    answering and language inference, without substantial task-specific</span>
<span class="sd">    architecture modifications. BERT is conceptually simple and empirically</span>
<span class="sd">    powerful. It obtains new state-of-the-art results on eleven natural language</span>
<span class="sd">    processing tasks, including pushing the GLUE score to 80.5% (7.7% point</span>
<span class="sd">    absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute</span>
<span class="sd">    improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point</span>
<span class="sd">    absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute</span>
<span class="sd">    improvement).*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = BertSentenceEmbeddings.pretrained(&quot;sent_small_bert_L2_128&quot;) \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence_bert_embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence_bert_embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     embeddingsFinisher</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;John loves apples. Mary loves oranges. John loves Mary.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[-0.8951074481010437,0.13753940165042877,0.3108254075050354,-1.65693199634552...|</span>
<span class="sd">    |[-0.6180210709571838,-0.12179657071828842,-0.191165953874588,-1.4497021436691...|</span>
<span class="sd">    |[-0.822715163230896,0.7568016648292542,-0.1165061742067337,-1.59048593044281,...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;BertSentenceEmbeddings&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">isLong</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                   <span class="s2">&quot;isLong&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;Use Long type instead of Int type for inputs buffer - Some Bert models require Long instead of Int.&quot;</span><span class="p">,</span>
                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="BertSentenceEmbeddings.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertSentenceEmbeddings.html#sparknlp.annotator.BertSentenceEmbeddings.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="BertSentenceEmbeddings.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertSentenceEmbeddings.html#sparknlp.annotator.BertSentenceEmbeddings.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="BertSentenceEmbeddings.setIsLong"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertSentenceEmbeddings.html#sparknlp.annotator.BertSentenceEmbeddings.setIsLong">[docs]</a>    <span class="k">def</span> <span class="nf">setIsLong</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to use Long type instead of Int type for inputs buffer.</span>

<span class="sd">        Some Bert models require Long instead of Int.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to use Long type instead of Int type for inputs buffer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">isLong</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BertSentenceEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

<div class="viewcode-block" id="BertSentenceEmbeddings.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertSentenceEmbeddings.html#sparknlp.annotator.BertSentenceEmbeddings.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        BertSentenceEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_BertSentenceLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_BertSentenceLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">BertSentenceEmbeddings</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="BertSentenceEmbeddings.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertSentenceEmbeddings.html#sparknlp.annotator.BertSentenceEmbeddings.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;sent_small_bert_L2_768&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;sent_small_bert_L2_768&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        BertSentenceEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">BertSentenceEmbeddings</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="SentenceEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceEmbeddings.html#sparknlp.annotator.SentenceEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">SentenceEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasEmbeddingsProperties</span><span class="p">,</span> <span class="n">HasStorageRef</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Converts the results from WordEmbeddings, BertEmbeddings, or other word</span>
<span class="sd">    embeddings into sentence or document embeddings by either summing up or</span>
<span class="sd">    averaging all the word embeddings in a sentence or a document (depending on</span>
<span class="sd">    the inputCols).</span>

<span class="sd">    This can be configured with :meth:`.setPoolingStrategy`, which either be</span>
<span class="sd">    ``&quot;AVERAGE&quot;`` or ``&quot;SUM&quot;``.</span>

<span class="sd">    For more extended examples see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/5.1_Text_classification_examples_in_SparkML_SparkNLP.ipynb&gt;`__..</span>

<span class="sd">    ============================= =======================</span>
<span class="sd">    Input Annotation types        Output Annotation type</span>
<span class="sd">    ============================= =======================</span>
<span class="sd">    ``DOCUMENT, WORD_EMBEDDINGS`` ``SENTENCE_EMBEDDINGS``</span>
<span class="sd">    ============================= =======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions</span>
<span class="sd">    poolingStrategy</span>
<span class="sd">        Choose how you would like to aggregate Word Embeddings to Sentence</span>
<span class="sd">        Embeddings: AVERAGE or SUM, by default AVERAGE</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    If you choose document as your input for Tokenizer,</span>
<span class="sd">    WordEmbeddings/BertEmbeddings, and SentenceEmbeddings then it averages/sums</span>
<span class="sd">    all the embeddings into one array of embeddings. However, if you choose</span>
<span class="sd">    sentences as inputCols then for each sentence SentenceEmbeddings generates</span>
<span class="sd">    one array of embeddings.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = WordEmbeddingsModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsSentence = SentenceEmbeddings() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence_embeddings&quot;) \\</span>
<span class="sd">    ...     .setPoolingStrategy(&quot;AVERAGE&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence_embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True) \\</span>
<span class="sd">    ...     .setCleanAnnotations(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       tokenizer,</span>
<span class="sd">    ...       embeddings,</span>
<span class="sd">    ...       embeddingsSentence,</span>
<span class="sd">    ...       embeddingsFinisher</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[-0.22093398869037628,0.25130119919776917,0.41810303926467896,-0.380883991718...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;SentenceEmbeddings&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SentenceEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">poolingStrategy</span><span class="o">=</span><span class="s2">&quot;AVERAGE&quot;</span>
        <span class="p">)</span>

    <span class="n">poolingStrategy</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;poolingStrategy&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Choose how you would like to aggregate Word Embeddings to Sentence Embeddings: AVERAGE or SUM&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

<div class="viewcode-block" id="SentenceEmbeddings.setPoolingStrategy"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceEmbeddings.html#sparknlp.annotator.SentenceEmbeddings.setPoolingStrategy">[docs]</a>    <span class="k">def</span> <span class="nf">setPoolingStrategy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strategy</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets how to aggregate the word Embeddings to sentence embeddings, by</span>
<span class="sd">        default AVERAGE.</span>

<span class="sd">        Can either be AVERAGE or SUM.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        strategy : str</span>
<span class="sd">            Pooling Strategy, either be AVERAGE or SUM</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        [type]</span>
<span class="sd">            [description]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;AVERAGE&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">poolingStrategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;SUM&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">poolingStrategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">poolingStrategy</span><span class="o">=</span><span class="s2">&quot;AVERAGE&quot;</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="StopWordsCleaner"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.StopWordsCleaner.html#sparknlp.annotator.StopWordsCleaner">[docs]</a><span class="k">class</span> <span class="nc">StopWordsCleaner</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This annotator takes a sequence of strings (e.g. the output of a</span>
<span class="sd">    Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words</span>
<span class="sd">    from the input sequences.</span>

<span class="sd">    By default, it uses stop words from MLlibs `StopWordsRemover</span>
<span class="sd">    &lt;https://spark.apache.org/docs/latest/ml-features#stopwordsremover&gt;`__. Stop</span>
<span class="sd">    words can also be defined by explicitly setting them with</span>
<span class="sd">    :meth:`.setStopWords` or loaded from pretrained models using ``pretrained``</span>
<span class="sd">    of its companion object.</span>


<span class="sd">    &gt;&gt;&gt; stopWords = StopWordsCleaner.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;cleanTokens&quot;)</span>

<span class="sd">    This will load the default pretrained model ``&quot;stopwords_en&quot;``.</span>

<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Stop+Words+Removal&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    stopWords</span>
<span class="sd">        The words to be filtered out, by default english stopwords from Spark ML</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to do a case sensitive, by default False</span>
<span class="sd">    locale</span>
<span class="sd">        Locale of the input. ignored when case sensitive, by default locale of</span>
<span class="sd">        the JVM</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentenceDetector = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; stopWords = StopWordsCleaner() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;cleanTokens&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       sentenceDetector,</span>
<span class="sd">    ...       tokenizer,</span>
<span class="sd">    ...       stopWords</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([</span>
<span class="sd">    ...     [&quot;This is my first sentence. This is my second.&quot;],</span>
<span class="sd">    ...     [&quot;This is my third sentence. This is my forth.&quot;]</span>
<span class="sd">    ... ]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;cleanTokens.result&quot;).show(truncate=False)</span>
<span class="sd">    +-------------------------------+</span>
<span class="sd">    |result                         |</span>
<span class="sd">    +-------------------------------+</span>
<span class="sd">    |[first, sentence, ., second, .]|</span>
<span class="sd">    |[third, sentence, ., forth, .] |</span>
<span class="sd">    +-------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;StopWordsCleaner&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.StopWordsCleaner&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">StopWordsCleaner</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">stopWords</span><span class="o">=</span><span class="n">StopWordsCleaner</span><span class="o">.</span><span class="n">loadDefaultStopWords</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">),</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">locale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_java_obj</span><span class="o">.</span><span class="n">getLocale</span><span class="p">()</span>
        <span class="p">)</span>

    <span class="n">stopWords</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;stopWords&quot;</span><span class="p">,</span> <span class="s2">&quot;The words to be filtered out&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>
    <span class="n">caseSensitive</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;caseSensitive&quot;</span><span class="p">,</span> <span class="s2">&quot;whether to do a case sensitive &quot;</span> <span class="o">+</span>
                          <span class="s2">&quot;comparison over the stop words&quot;</span><span class="p">,</span> <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>
    <span class="n">locale</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;locale&quot;</span><span class="p">,</span> <span class="s2">&quot;locale of the input. ignored when case sensitive &quot;</span> <span class="o">+</span>
                   <span class="s2">&quot;is true&quot;</span><span class="p">,</span> <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

<div class="viewcode-block" id="StopWordsCleaner.setStopWords"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.StopWordsCleaner.html#sparknlp.annotator.StopWordsCleaner.setStopWords">[docs]</a>    <span class="k">def</span> <span class="nf">setStopWords</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the words to be filtered out, by default english stopwords from</span>
<span class="sd">        Spark ML.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            The words to be filtered out</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">stopWords</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="StopWordsCleaner.setCaseSensitive"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.StopWordsCleaner.html#sparknlp.annotator.StopWordsCleaner.setCaseSensitive">[docs]</a>    <span class="k">def</span> <span class="nf">setCaseSensitive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to do a case sensitive comparison, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to do a case sensitive comparison</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">caseSensitive</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="StopWordsCleaner.setLocale"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.StopWordsCleaner.html#sparknlp.annotator.StopWordsCleaner.setLocale">[docs]</a>    <span class="k">def</span> <span class="nf">setLocale</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets locale of the input. Ignored when case sensitive, by default</span>
<span class="sd">        locale of the JVM.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Locale of the input</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">locale</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="StopWordsCleaner.loadDefaultStopWords"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.StopWordsCleaner.html#sparknlp.annotator.StopWordsCleaner.loadDefaultStopWords">[docs]</a>    <span class="k">def</span> <span class="nf">loadDefaultStopWords</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads the default stop words for the given language.</span>

<span class="sd">        Supported languages: danish, dutch, english, finnish, french, german,</span>
<span class="sd">        hungarian, italian, norwegian, portuguese, russian, spanish, swedish,</span>
<span class="sd">        turkish</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        language : str, optional</span>
<span class="sd">            Language stopwords to load, by default &quot;english&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.ml.wrapper</span> <span class="kn">import</span> <span class="n">_jvm</span>
        <span class="n">stopWordsObj</span> <span class="o">=</span> <span class="n">_jvm</span><span class="p">()</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">ml</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">StopWordsRemover</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">stopWordsObj</span><span class="o">.</span><span class="n">loadDefaultStopWords</span><span class="p">(</span><span class="n">language</span><span class="p">))</span></div>

<div class="viewcode-block" id="StopWordsCleaner.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.StopWordsCleaner.html#sparknlp.annotator.StopWordsCleaner.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;stopwords_en&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;stopwords_en&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        StopWordsCleaner</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">StopWordsCleaner</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="NGramGenerator"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NGramGenerator.html#sparknlp.annotator.NGramGenerator">[docs]</a><span class="k">class</span> <span class="nc">NGramGenerator</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A feature transformer that converts the input array of strings</span>
<span class="sd">    (annotatorType ``TOKEN``) into an array of n-grams (annotatorType</span>
<span class="sd">    ``CHUNK``).</span>

<span class="sd">    Null values in the input array are ignored. It returns an array of n-grams</span>
<span class="sd">    where each n-gram is represented by a space-separated string of words.</span>

<span class="sd">    When the input is empty, an empty array is returned. When the input array</span>
<span class="sd">    length is less than n (number of elements per n-gram), no n-grams are</span>
<span class="sd">    returned.</span>

<span class="sd">    For more extended examples see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/annotation/english/chunking/NgramGenerator.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``CHUNK``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    n</span>
<span class="sd">        Number elements per n-gram (&gt;=1), by default 2</span>
<span class="sd">    enableCumulative</span>
<span class="sd">        Whether to calculate just the actual n-grams, by default False</span>
<span class="sd">    delimiter</span>
<span class="sd">        Character to use to join the tokens, by default &quot; &quot;</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; nGrams = NGramGenerator() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;ngrams&quot;) \\</span>
<span class="sd">    ...     .setN(2)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       sentence,</span>
<span class="sd">    ...       tokenizer,</span>
<span class="sd">    ...       nGrams</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is my sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; results = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; results.selectExpr(&quot;explode(ngrams) as result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------+</span>
<span class="sd">    |result                                                      |</span>
<span class="sd">    +------------------------------------------------------------+</span>
<span class="sd">    |[chunk, 0, 6, This is, [sentence -&gt; 0, chunk -&gt; 0], []]     |</span>
<span class="sd">    |[chunk, 5, 9, is my, [sentence -&gt; 0, chunk -&gt; 1], []]       |</span>
<span class="sd">    |[chunk, 8, 18, my sentence, [sentence -&gt; 0, chunk -&gt; 2], []]|</span>
<span class="sd">    |[chunk, 11, 19, sentence ., [sentence -&gt; 0, chunk -&gt; 3], []]|</span>
<span class="sd">    +------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;NGramGenerator&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NGramGenerator</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.NGramGenerator&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">enableCumulative</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

    <span class="n">n</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;n&quot;</span><span class="p">,</span> <span class="s2">&quot;number elements per n-gram (&gt;=1)&quot;</span><span class="p">,</span> <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">enableCumulative</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;enableCumulative&quot;</span><span class="p">,</span> <span class="s2">&quot;whether to calculate just the actual n-grams &quot;</span> <span class="o">+</span>
                             <span class="s2">&quot;or all n-grams from 1 through n&quot;</span><span class="p">,</span> <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">delimiter</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;delimiter&quot;</span><span class="p">,</span> <span class="s2">&quot;String to use to join the tokens &quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

<div class="viewcode-block" id="NGramGenerator.setN"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NGramGenerator.html#sparknlp.annotator.NGramGenerator.setN">[docs]</a>    <span class="k">def</span> <span class="nf">setN</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets number elements per n-gram (&gt;=1), by default 2.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Number elements per n-gram (&gt;=1), by default 2</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NGramGenerator.setEnableCumulative"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NGramGenerator.html#sparknlp.annotator.NGramGenerator.setEnableCumulative">[docs]</a>    <span class="k">def</span> <span class="nf">setEnableCumulative</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to calculate just the actual n-grams, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to calculate just the actual n-grams</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">enableCumulative</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NGramGenerator.setDelimiter"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NGramGenerator.html#sparknlp.annotator.NGramGenerator.setDelimiter">[docs]</a>    <span class="k">def</span> <span class="nf">setDelimiter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets character to use to join the tokens</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            character to use to join the tokens</span>

<span class="sd">        Raises</span>
<span class="sd">        ------</span>
<span class="sd">        Exception</span>
<span class="sd">            Delimiter should have length == 1</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Delimiter should have length == 1&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">delimiter</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ChunkEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ChunkEmbeddings.html#sparknlp.annotator.ChunkEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">ChunkEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This annotator utilizes WordEmbeddings, BertEmbeddings etc. to generate</span>
<span class="sd">    chunk embeddings from either Chunker, NGramGenerator, or NerConverter</span>
<span class="sd">    outputs.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb&gt;`__.</span>

<span class="sd">    ========================== ======================</span>
<span class="sd">    Input Annotation types     Output Annotation type</span>
<span class="sd">    ========================== ======================</span>
<span class="sd">    ``CHUNK, WORD_EMBEDDINGS`` ``WORD_EMBEDDINGS``</span>
<span class="sd">    ========================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    poolingStrategy</span>
<span class="sd">        Choose how you would like to aggregate Word Embeddings to Chunk</span>
<span class="sd">        Embeddings, by default AVERAGE.</span>
<span class="sd">        Possible Values: ``AVERAGE, SUM``</span>
<span class="sd">    skipOOV</span>
<span class="sd">        Whether to discard default vectors for OOV words from the</span>
<span class="sd">        aggregation/pooling.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    Extract the Embeddings from the NGrams</span>

<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; nGrams = NGramGenerator() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;chunk&quot;) \\</span>
<span class="sd">    ...     .setN(2)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = WordEmbeddingsModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(False)</span>

<span class="sd">    Convert the NGram chunks into Word Embeddings</span>

<span class="sd">    &gt;&gt;&gt; chunkEmbeddings = ChunkEmbeddings() \\</span>
<span class="sd">    ...     .setInputCols([&quot;chunk&quot;, &quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;chunk_embeddings&quot;) \\</span>
<span class="sd">    ...     .setPoolingStrategy(&quot;AVERAGE&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       sentence,</span>
<span class="sd">    ...       tokenizer,</span>
<span class="sd">    ...       nGrams,</span>
<span class="sd">    ...       embeddings,</span>
<span class="sd">    ...       chunkEmbeddings</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(chunk_embeddings) as result&quot;) \\</span>
<span class="sd">    ...     .select(&quot;result.annotatorType&quot;, &quot;result.result&quot;, &quot;result.embeddings&quot;) \\</span>
<span class="sd">    ...     .show(5, 80)</span>
<span class="sd">    +---------------+----------+--------------------------------------------------------------------------------+</span>
<span class="sd">    |  annotatorType|    result|                                                                      embeddings|</span>
<span class="sd">    +---------------+----------+--------------------------------------------------------------------------------+</span>
<span class="sd">    |word_embeddings|   This is|[-0.55661, 0.42829502, 0.86661, -0.409785, 0.06316501, 0.120775, -0.0732005, ...|</span>
<span class="sd">    |word_embeddings|      is a|[-0.40674996, 0.22938299, 0.50597, -0.288195, 0.555655, 0.465145, 0.140118, 0...|</span>
<span class="sd">    |word_embeddings|a sentence|[0.17417, 0.095253006, -0.0530925, -0.218465, 0.714395, 0.79860497, 0.0129999...|</span>
<span class="sd">    |word_embeddings|sentence .|[0.139705, 0.177955, 0.1887775, -0.45545, 0.20030999, 0.461557, -0.07891501, ...|</span>
<span class="sd">    +---------------+----------+--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;ChunkEmbeddings&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ChunkEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">poolingStrategy</span><span class="o">=</span><span class="s2">&quot;AVERAGE&quot;</span>
        <span class="p">)</span>

    <span class="n">poolingStrategy</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;poolingStrategy&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Choose how you would like to aggregate Word Embeddings to Chunk Embeddings:&quot;</span> <span class="o">+</span>
                            <span class="s2">&quot;AVERAGE or SUM&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>
    <span class="n">skipOOV</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;skipOOV&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;Whether to discard default vectors for OOV words from the aggregation / pooling &quot;</span><span class="p">,</span>
                    <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

<div class="viewcode-block" id="ChunkEmbeddings.setPoolingStrategy"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ChunkEmbeddings.html#sparknlp.annotator.ChunkEmbeddings.setPoolingStrategy">[docs]</a>    <span class="k">def</span> <span class="nf">setPoolingStrategy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strategy</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets how to aggregate Word Embeddings to Chunk Embeddings, by default</span>
<span class="sd">        AVERAGE.</span>

<span class="sd">        Possible Values: ``AVERAGE, SUM``</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        strategy : str</span>
<span class="sd">            Aggregation Strategy</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;AVERAGE&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">poolingStrategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">strategy</span> <span class="o">==</span> <span class="s2">&quot;SUM&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">poolingStrategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">poolingStrategy</span><span class="o">=</span><span class="s2">&quot;AVERAGE&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="ChunkEmbeddings.setSkipOOV"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ChunkEmbeddings.html#sparknlp.annotator.ChunkEmbeddings.setSkipOOV">[docs]</a>    <span class="k">def</span> <span class="nf">setSkipOOV</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to discard default vectors for OOV words from the</span>
<span class="sd">        aggregation/pooling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            whether to discard default vectors for OOV words from the</span>
<span class="sd">            aggregation/pooling.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">skipOOV</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="NerOverwriter"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerOverwriter.html#sparknlp.annotator.NerOverwriter">[docs]</a><span class="k">class</span> <span class="nc">NerOverwriter</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Overwrites entities of specified strings.</span>

<span class="sd">    The input for this Annotator have to be entities that are already extracted,</span>
<span class="sd">    Annotator type ``NAMED_ENTITY``. The strings specified with</span>
<span class="sd">    :meth:`.setStopWords` will have new entities assigned to, specified with</span>
<span class="sd">    :meth:`.setNewResult`.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``NAMED_ENTITY``       ``NAMED_ENTITY``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    stopWords</span>
<span class="sd">        The words to be overwritten</span>
<span class="sd">    newResult</span>
<span class="sd">        new NER class to apply to those stopwords, by default I-OVERWRITE</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    First extract the prerequisite Entities</span>

<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = WordEmbeddingsModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;bert&quot;)</span>
<span class="sd">    &gt;&gt;&gt; nerTagger = NerDLModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;bert&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;ner&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     nerTagger</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;Spark NLP Crosses Five Million Downloads, John Snow Labs Announces.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(ner)&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------+</span>
<span class="sd">    |col                                                   |</span>
<span class="sd">    +------------------------------------------------------+</span>
<span class="sd">    |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []]      |</span>
<span class="sd">    |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []]        |</span>
<span class="sd">    |[named_entity, 10, 16, O, [word -&gt; Crosses], []]      |</span>
<span class="sd">    |[named_entity, 18, 21, O, [word -&gt; Five], []]         |</span>
<span class="sd">    |[named_entity, 23, 29, O, [word -&gt; Million], []]      |</span>
<span class="sd">    |[named_entity, 31, 39, O, [word -&gt; Downloads], []]    |</span>
<span class="sd">    |[named_entity, 40, 40, O, [word -&gt; ,], []]            |</span>
<span class="sd">    |[named_entity, 42, 45, B-ORG, [word -&gt; John], []]     |</span>
<span class="sd">    |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []]     |</span>
<span class="sd">    |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []]     |</span>
<span class="sd">    |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []]|</span>
<span class="sd">    |[named_entity, 66, 66, O, [word -&gt; .], []]            |</span>
<span class="sd">    +------------------------------------------------------+</span>

<span class="sd">    The recognized entities can then be overwritten</span>

<span class="sd">    &gt;&gt;&gt; nerOverwriter = NerOverwriter() \\</span>
<span class="sd">    ...     .setInputCols([&quot;ner&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;ner_overwritten&quot;) \\</span>
<span class="sd">    ...     .setStopWords([&quot;Million&quot;]) \\</span>
<span class="sd">    ...     .setNewResult(&quot;B-CARDINAL&quot;)</span>
<span class="sd">    &gt;&gt;&gt; nerOverwriter.transform(result).selectExpr(&quot;explode(ner_overwritten)&quot;).show(truncate=False)</span>
<span class="sd">    +---------------------------------------------------------+</span>
<span class="sd">    |col                                                      |</span>
<span class="sd">    +---------------------------------------------------------+</span>
<span class="sd">    |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []]         |</span>
<span class="sd">    |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []]           |</span>
<span class="sd">    |[named_entity, 10, 16, O, [word -&gt; Crosses], []]         |</span>
<span class="sd">    |[named_entity, 18, 21, O, [word -&gt; Five], []]            |</span>
<span class="sd">    |[named_entity, 23, 29, B-CARDINAL, [word -&gt; Million], []]|</span>
<span class="sd">    |[named_entity, 31, 39, O, [word -&gt; Downloads], []]       |</span>
<span class="sd">    |[named_entity, 40, 40, O, [word -&gt; ,], []]               |</span>
<span class="sd">    |[named_entity, 42, 45, B-ORG, [word -&gt; John], []]        |</span>
<span class="sd">    |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []]        |</span>
<span class="sd">    |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []]        |</span>
<span class="sd">    |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []]   |</span>
<span class="sd">    |[named_entity, 66, 66, O, [word -&gt; .], []]               |</span>
<span class="sd">    +---------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;NerOverwriter&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NerOverwriter</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.ner.NerOverwriter&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">newResult</span><span class="o">=</span><span class="s2">&quot;I-OVERWRITE&quot;</span>
        <span class="p">)</span>

    <span class="n">stopWords</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;stopWords&quot;</span><span class="p">,</span> <span class="s2">&quot;The words to be overwritten&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>
    <span class="n">newResult</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;newResult&quot;</span><span class="p">,</span> <span class="s2">&quot;new NER class to apply to those stopwords&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

<div class="viewcode-block" id="NerOverwriter.setStopWords"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerOverwriter.html#sparknlp.annotator.NerOverwriter.setStopWords">[docs]</a>    <span class="k">def</span> <span class="nf">setStopWords</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the words to be overwritten.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            The words to be overwritten</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">stopWords</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="NerOverwriter.setNewResult"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.NerOverwriter.html#sparknlp.annotator.NerOverwriter.setNewResult">[docs]</a>    <span class="k">def</span> <span class="nf">setNewResult</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets new NER class to apply to those stopwords, by default</span>
<span class="sd">        I-OVERWRITE.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            NER class to apply the stopwords to</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">newResult</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="UniversalSentenceEncoder"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.UniversalSentenceEncoder.html#sparknlp.annotator.UniversalSentenceEncoder">[docs]</a><span class="k">class</span> <span class="nc">UniversalSentenceEncoder</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasEmbeddingsProperties</span><span class="p">,</span> <span class="n">HasStorageRef</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The Universal Sentence Encoder encodes text into high dimensional vectors</span>
<span class="sd">    that can be used for text classification, semantic similarity, clustering</span>
<span class="sd">    and other natural language tasks.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; useEmbeddings = UniversalSentenceEncoder.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence_embeddings&quot;)</span>


<span class="sd">    The default model is ``&quot;tfhub_use&quot;``, if no name is provided. For available</span>
<span class="sd">    pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Embeddings&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb&gt;`__.</span>

<span class="sd">    ====================== =======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== =======================</span>
<span class="sd">    ``DOCUMENT``           ``SENTENCE_EMBEDDINGS``</span>
<span class="sd">    ====================== =======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions</span>
<span class="sd">    loadSP</span>
<span class="sd">        Whether to load SentencePiece ops file which is required only by</span>
<span class="sd">        multi-lingual models, by default False</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `Universal Sentence Encoder &lt;https://arxiv.org/abs/1803.11175&gt;`__</span>

<span class="sd">    https://tfhub.dev/google/universal-sentence-encoder/2</span>

<span class="sd">    **Paper abstract:**</span>

<span class="sd">    *We present models for encoding sentences into embedding vectors that</span>
<span class="sd">    specifically target transfer learning to other NLP tasks. The models are</span>
<span class="sd">    efficient and result in accurate performance on diverse transfer tasks. Two</span>
<span class="sd">    variants of the encoding models allow for trade-offs between accuracy and</span>
<span class="sd">    compute resources. For both variants, we investigate and report the</span>
<span class="sd">    relationship between model complexity, resource consumption, the</span>
<span class="sd">    availability of transfer task training data, and task performance.</span>
<span class="sd">    Comparisons are made with baselines that use word level transfer learning</span>
<span class="sd">    via pretrained word embeddings as well as baselines do not use any transfer</span>
<span class="sd">    learning. We find that transfer learning using sentence embeddings tends to</span>
<span class="sd">    outperform word level transfer. With transfer learning via sentence</span>
<span class="sd">    embeddings, we observe surprisingly good performance with minimal amounts of</span>
<span class="sd">    supervised training data for a transfer task. We obtain encouraging results</span>
<span class="sd">    on Word Embedding Association Tests (WEAT) targeted at detecting model bias.</span>
<span class="sd">    Our pre-trained sentence encoding models are made freely available for</span>
<span class="sd">    download and on TF Hub.*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = UniversalSentenceEncoder.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence_embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence_embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True) \\</span>
<span class="sd">    ...     .setCleanAnnotations(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       sentence,</span>
<span class="sd">    ...       embeddings,</span>
<span class="sd">    ...       embeddingsFinisher</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[0.04616805538535118,0.022307956591248512,-0.044395286589860916,-0.0016493503...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;UniversalSentenceEncoder&quot;</span>

    <span class="n">loadSP</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;loadSP&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;Whether to load SentencePiece ops file which is required only by multi-lingual models. &quot;</span>
                   <span class="s2">&quot;This is not changeable after it&#39;s set with a pretrained model nor it is compatible with Windows.&quot;</span><span class="p">,</span>
                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="UniversalSentenceEncoder.setLoadSP"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.UniversalSentenceEncoder.html#sparknlp.annotator.UniversalSentenceEncoder.setLoadSP">[docs]</a>    <span class="k">def</span> <span class="nf">setLoadSP</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to load SentencePiece ops file which is required only by</span>
<span class="sd">        multi-lingual models, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to load SentencePiece ops file which is required only by</span>
<span class="sd">            multi-lingual models</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">loadSP</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="UniversalSentenceEncoder.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.UniversalSentenceEncoder.html#sparknlp.annotator.UniversalSentenceEncoder.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">UniversalSentenceEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">loadSP</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

<div class="viewcode-block" id="UniversalSentenceEncoder.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.UniversalSentenceEncoder.html#sparknlp.annotator.UniversalSentenceEncoder.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">,</span> <span class="n">loadsp</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        UniversalSentenceEncoder</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_USELoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_USELoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">,</span> <span class="n">loadsp</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">UniversalSentenceEncoder</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="UniversalSentenceEncoder.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.UniversalSentenceEncoder.html#sparknlp.annotator.UniversalSentenceEncoder.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;tfhub_use&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;tfhub_use&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        UniversalSentenceEncoder</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">UniversalSentenceEncoder</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ElmoEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ElmoEmbeddings.html#sparknlp.annotator.ElmoEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">ElmoEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasEmbeddingsProperties</span><span class="p">,</span> <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span> <span class="n">HasStorageRef</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Word embeddings from ELMo (Embeddings from Language Models), a language</span>
<span class="sd">    model trained on the 1 Billion Word Benchmark.</span>

<span class="sd">    Note that this is a very computationally expensive module compared to word</span>
<span class="sd">    embedding modules that only perform embedding lookups. The use of an</span>
<span class="sd">    accelerator is recommended.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = ElmoEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;elmo_embeddings&quot;)</span>


<span class="sd">    The default model is ``&quot;elmo&quot;``, if no name is provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Embeddings&gt;`__.</span>

<span class="sd">    The pooling layer can be set with :meth:`.setPoolingLayer` to the following</span>
<span class="sd">    values:</span>

<span class="sd">    - ``&quot;word_emb&quot;``: the character-based word representations with shape</span>
<span class="sd">      ``[batch_size, max_length, 512]``.</span>
<span class="sd">    - ``&quot;lstm_outputs1&quot;``: the first LSTM hidden state with shape</span>
<span class="sd">      ``[batch_size, max_length, 1024]``.</span>
<span class="sd">    - ``&quot;lstm_outputs2&quot;``: the second LSTM hidden state with shape</span>
<span class="sd">      ``[batch_size, max_length, 1024]``.</span>
<span class="sd">    - ``&quot;elmo&quot;``: the weighted sum of the 3 layers, where the weights are</span>
<span class="sd">      trainable. This tensor has shape ``[batch_size, max_length, 1024]``.</span>

<span class="sd">    For extended examples of usage, see the</span>
<span class="sd">    `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_elmo.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``WORD_EMBEDDINGS``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size. Large values allows faster processing but requires more</span>
<span class="sd">        memory, by default 32</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    poolingLayer</span>
<span class="sd">        Set ELMO pooling layer to: word_emb, lstm_outputs1, lstm_outputs2, or</span>
<span class="sd">        elmo, by default word_emb</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    https://tfhub.dev/google/elmo/3</span>

<span class="sd">    `Deep contextualized word representations &lt;https://arxiv.org/abs/1802.05365&gt;`__</span>

<span class="sd">    **Paper abstract:**</span>

<span class="sd">    *We introduce a new type of deep contextualized word representation that</span>
<span class="sd">    models both (1) complex characteristics of word use (e.g., syntax and</span>
<span class="sd">    semantics), and (2) how these uses vary across linguistic contexts (i.e.,</span>
<span class="sd">    to model polysemy). Our word vectors are learned functions of the internal</span>
<span class="sd">    states of a deep bidirectional language model (biLM), which is pre-trained</span>
<span class="sd">    on a large text corpus. We show that these representations can be easily</span>
<span class="sd">    added to existing models and significantly improve the state of the art</span>
<span class="sd">    across six challenging NLP problems, including question answering, textual</span>
<span class="sd">    entailment and sentiment analysis. We also present an analysis showing that</span>
<span class="sd">    exposing the deep internals of the pre-trained network is crucial, allowing</span>
<span class="sd">    downstream models to mix different types of semi-supervision signals.*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = ElmoEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setPoolingLayer(&quot;word_emb&quot;) \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True) \\</span>
<span class="sd">    ...     .setCleanAnnotations(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     embeddingsFinisher</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[6.662458181381226E-4,-0.2541114091873169,-0.6275503039360046,0.5787073969841...|</span>
<span class="sd">    |[0.19154725968837738,0.22998669743537903,-0.2894386649131775,0.21524395048618...|</span>
<span class="sd">    |[0.10400570929050446,0.12288510054349899,-0.07056470215320587,-0.246389418840...|</span>
<span class="sd">    |[0.49932169914245605,-0.12706467509269714,0.30969417095184326,0.2643227577209...|</span>
<span class="sd">    |[-0.8871506452560425,-0.20039963722229004,-1.0601330995559692,0.0348707810044...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;ElmoEmbeddings&quot;</span>

    <span class="n">batchSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;batchSize&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Batch size. Large values allows faster processing but requires more memory.&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">poolingLayer</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;poolingLayer&quot;</span><span class="p">,</span> <span class="s2">&quot;Set ELMO pooling layer to: word_emb, lstm_outputs1, lstm_outputs2, or elmo&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

<div class="viewcode-block" id="ElmoEmbeddings.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ElmoEmbeddings.html#sparknlp.annotator.ElmoEmbeddings.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="ElmoEmbeddings.setBatchSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ElmoEmbeddings.html#sparknlp.annotator.ElmoEmbeddings.setBatchSize">[docs]</a>    <span class="k">def</span> <span class="nf">setBatchSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets batch size, by default 32.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Batch size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">batchSize</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="ElmoEmbeddings.setPoolingLayer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ElmoEmbeddings.html#sparknlp.annotator.ElmoEmbeddings.setPoolingLayer">[docs]</a>    <span class="k">def</span> <span class="nf">setPoolingLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets ELMO pooling layer to: word_emb, lstm_outputs1, lstm_outputs2, or</span>
<span class="sd">        elmo, by default word_emb</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        layer : str</span>
<span class="sd">            ELMO pooling layer</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">layer</span> <span class="o">==</span> <span class="s2">&quot;word_emb&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">poolingLayer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">layer</span> <span class="o">==</span> <span class="s2">&quot;lstm_outputs1&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">poolingLayer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">layer</span> <span class="o">==</span> <span class="s2">&quot;lstm_outputs2&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">poolingLayer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">layer</span> <span class="o">==</span> <span class="s2">&quot;elmo&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">poolingLayer</span><span class="o">=</span><span class="n">layer</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">poolingLayer</span><span class="o">=</span><span class="s2">&quot;word_emb&quot;</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ElmoEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
            <span class="n">poolingLayer</span><span class="o">=</span><span class="s2">&quot;word_emb&quot;</span>
        <span class="p">)</span>

<div class="viewcode-block" id="ElmoEmbeddings.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ElmoEmbeddings.html#sparknlp.annotator.ElmoEmbeddings.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ElmoEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_ElmoLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_ElmoLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">ElmoEmbeddings</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="ElmoEmbeddings.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ElmoEmbeddings.html#sparknlp.annotator.ElmoEmbeddings.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;elmo&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;elmo&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ElmoEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">ElmoEmbeddings</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ClassifierDLApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLApproach.html#sparknlp.annotator.ClassifierDLApproach">[docs]</a><span class="k">class</span> <span class="nc">ClassifierDLApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains a ClassifierDL for generic Multi-class Text Classification.</span>

<span class="sd">    ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an</span>
<span class="sd">    input for text classifications.</span>
<span class="sd">    The ClassifierDL annotator uses a deep learning model (DNNs) we have built</span>
<span class="sd">    inside TensorFlow and supports up to 100 classes.</span>

<span class="sd">    For instantiated/pretrained models, see :class:`.ClassifierDLModel`.</span>

<span class="sd">    For extended examples of usage, see the Spark NLP Workshop</span>
<span class="sd">    `Spark NLP Workshop  &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/5.Text_Classification_with_ClassifierDL.ipynb&gt;`__.</span>

<span class="sd">    ======================= ======================</span>
<span class="sd">    Input Annotation types  Output Annotation type</span>
<span class="sd">    ======================= ======================</span>
<span class="sd">    ``SENTENCE_EMBEDDINGS`` ``CATEGORY``</span>
<span class="sd">    ======================= ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lr</span>
<span class="sd">        Learning Rate, by default 0.005</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size, by default 64</span>
<span class="sd">    dropout</span>
<span class="sd">        Dropout coefficient, by default 0.5</span>
<span class="sd">    maxEpochs</span>
<span class="sd">        Maximum number of epochs to train, by default 30</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    validationSplit</span>
<span class="sd">        Choose the proportion of training dataset to be validated against the</span>
<span class="sd">        model on each Epoch. The value should be between 0.0 and 1.0 and by</span>
<span class="sd">        default it is 0.0 and off.</span>
<span class="sd">    enableOutputLogs</span>
<span class="sd">        Whether to use stdout in addition to Spark logs, by default False</span>
<span class="sd">    outputLogsPath</span>
<span class="sd">        Folder path to save training logs</span>
<span class="sd">    labelColumn</span>
<span class="sd">        Column with label per each token</span>
<span class="sd">    verbose</span>
<span class="sd">        Level of verbosity during training</span>
<span class="sd">    randomSeed</span>
<span class="sd">        Random seed for shuffling</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    - This annotator accepts a label column of a single item in either type of</span>
<span class="sd">      String, Int, Float, or Double.</span>
<span class="sd">    - UniversalSentenceEncoder, Transformer based embeddings, or</span>
<span class="sd">      SentenceEmbeddings can be used for the ``inputCol``.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    In this example, the training data ``&quot;sentiment.csv&quot;`` has the form of::</span>

<span class="sd">        text,label</span>
<span class="sd">        This movie is the best movie I have wached ever! In my opinion this movie can win an award.,0</span>
<span class="sd">        This was a terrible movie! The acting was bad really bad!,1</span>
<span class="sd">        ...</span>

<span class="sd">    Then traning can be done like so:</span>

<span class="sd">    &gt;&gt;&gt; smallCorpus = spark.read.option(&quot;header&quot;,&quot;True&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;)</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; useEmbeddings = UniversalSentenceEncoder.pretrained() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence_embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; docClassifier = ClassifierDLApproach() \\</span>
<span class="sd">    ...     .setInputCols(&quot;sentence_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;category&quot;) \\</span>
<span class="sd">    ...     .setLabelColumn(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setBatchSize(64) \\</span>
<span class="sd">    ...     .setMaxEpochs(20) \\</span>
<span class="sd">    ...     .setLr(5e-3) \\</span>
<span class="sd">    ...     .setDropout(0.5)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     useEmbeddings,</span>
<span class="sd">    ...     docClassifier</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(smallCorpus)</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    MultiClassifierDLApproach : for multi-class classification</span>
<span class="sd">    SentimentDLApproach : for sentiment analysis</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lr</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="s2">&quot;Learning Rate&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">batchSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;batchSize&quot;</span><span class="p">,</span> <span class="s2">&quot;Batch size&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">dropout</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="s2">&quot;Dropout coefficient&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">maxEpochs</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;maxEpochs&quot;</span><span class="p">,</span> <span class="s2">&quot;Maximum number of epochs to train&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">validationSplit</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;validationSplit&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Choose the proportion of training dataset to be validated against the model on each Epoch. The value should be between 0.0 and 1.0 and by default it is 0.0 and off.&quot;</span><span class="p">,</span>
                            <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">enableOutputLogs</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;enableOutputLogs&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;Whether to use stdout in addition to Spark logs.&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">outputLogsPath</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;outputLogsPath&quot;</span><span class="p">,</span> <span class="s2">&quot;Folder path to save training logs&quot;</span><span class="p">,</span>
                           <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">labelColumn</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;labelColumn&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;Column with label per each token&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">verbose</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;verbose&quot;</span><span class="p">,</span> <span class="s2">&quot;Level of verbosity during training&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">randomSeed</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;randomSeed&quot;</span><span class="p">,</span> <span class="s2">&quot;Random seed&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

<div class="viewcode-block" id="ClassifierDLApproach.setVerbose"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLApproach.html#sparknlp.annotator.ClassifierDLApproach.setVerbose">[docs]</a>    <span class="k">def</span> <span class="nf">setVerbose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets level of verbosity during training</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Level of verbosity</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="ClassifierDLApproach.setRandomSeed"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLApproach.html#sparknlp.annotator.ClassifierDLApproach.setRandomSeed">[docs]</a>    <span class="k">def</span> <span class="nf">setRandomSeed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets random seed for shuffling</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        seed : int</span>
<span class="sd">            Random seed for shuffling</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">randomSeed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="ClassifierDLApproach.setLabelColumn"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLApproach.html#sparknlp.annotator.ClassifierDLApproach.setLabelColumn">[docs]</a>    <span class="k">def</span> <span class="nf">setLabelColumn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets name of column for data labels</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Column for data labels</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">labelColumn</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="ClassifierDLApproach.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLApproach.html#sparknlp.annotator.ClassifierDLApproach.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="ClassifierDLApproach.setLr"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLApproach.html#sparknlp.annotator.ClassifierDLApproach.setLr">[docs]</a>    <span class="k">def</span> <span class="nf">setLr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets Learning Rate, by default 0.005</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Learning Rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="ClassifierDLApproach.setBatchSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLApproach.html#sparknlp.annotator.ClassifierDLApproach.setBatchSize">[docs]</a>    <span class="k">def</span> <span class="nf">setBatchSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets batch size, by default 64.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : int</span>
<span class="sd">            Batch size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">batchSize</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="ClassifierDLApproach.setDropout"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLApproach.html#sparknlp.annotator.ClassifierDLApproach.setDropout">[docs]</a>    <span class="k">def</span> <span class="nf">setDropout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets dropout coefficient, by default 0.5</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Dropout coefficient</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">dropout</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="ClassifierDLApproach.setMaxEpochs"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLApproach.html#sparknlp.annotator.ClassifierDLApproach.setMaxEpochs">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxEpochs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets maximum number of epochs to train, by default 30</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        epochs : int</span>
<span class="sd">            Maximum number of epochs to train</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxEpochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ClassifierDLModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span>

<div class="viewcode-block" id="ClassifierDLApproach.setValidationSplit"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLApproach.html#sparknlp.annotator.ClassifierDLApproach.setValidationSplit">[docs]</a>    <span class="k">def</span> <span class="nf">setValidationSplit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the proportion of training dataset to be validated against the</span>
<span class="sd">        model on each Epoch, by default it is 0.0 and off. The value should be</span>
<span class="sd">        between 0.0 and 1.0.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Proportion of training dataset to be validated</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">validationSplit</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="ClassifierDLApproach.setEnableOutputLogs"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLApproach.html#sparknlp.annotator.ClassifierDLApproach.setEnableOutputLogs">[docs]</a>    <span class="k">def</span> <span class="nf">setEnableOutputLogs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to use stdout in addition to Spark logs, by default False</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to use stdout in addition to Spark logs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">enableOutputLogs</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="ClassifierDLApproach.setOutputLogsPath"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLApproach.html#sparknlp.annotator.ClassifierDLApproach.setOutputLogsPath">[docs]</a>    <span class="k">def</span> <span class="nf">setOutputLogsPath</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets folder path to save training logs</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        p : str</span>
<span class="sd">            Folder path to save training logs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">outputLogsPath</span><span class="o">=</span><span class="n">p</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ClassifierDLApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">maxEpochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.005</span><span class="p">),</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">enableOutputLogs</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="ClassifierDLModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLModel.html#sparknlp.annotator.ClassifierDLModel">[docs]</a><span class="k">class</span> <span class="nc">ClassifierDLModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasStorageRef</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;ClassifierDL for generic Multi-class Text Classification.</span>

<span class="sd">    ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an</span>
<span class="sd">    input for text classifications. The ClassifierDL annotator uses a deep</span>
<span class="sd">    learning model (DNNs) we have built inside TensorFlow and supports up to</span>
<span class="sd">    100 classes.</span>

<span class="sd">    This is the instantiated model of the :class:`.ClassifierDLApproach`.</span>
<span class="sd">    For training your own model, please see the documentation of that class.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; classifierDL = ClassifierDLModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence_embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;classification&quot;)</span>

<span class="sd">    The default model is ``&quot;classifierdl_use_trec6&quot;``, if no name is provided.</span>
<span class="sd">    It uses embeddings from the UniversalSentenceEncoder and is trained on the</span>
<span class="sd">    `TREC-6 &lt;https://deepai.org/dataset/trec-6#:~:text=The%20TREC%20dataset%20is%20dataset,50%20has%20finer%2Dgrained%20labels&gt;`__</span>
<span class="sd">    dataset.</span>

<span class="sd">    For available pretrained models please see the</span>
<span class="sd">    `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Text+Classification&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the</span>
<span class="sd">    `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/5.Text_Classification_with_ClassifierDL.ipynb&gt;`__.</span>

<span class="sd">    ======================= ======================</span>
<span class="sd">    Input Annotation types  Output Annotation type</span>
<span class="sd">    ======================= ======================</span>
<span class="sd">    ``SENTENCE_EMBEDDINGS`` ``CATEGORY``</span>
<span class="sd">    ======================= ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    classes</span>
<span class="sd">        Get the tags used to trained this ClassifierDLModel</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; useEmbeddings = UniversalSentenceEncoder.pretrained() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence_embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sarcasmDL = ClassifierDLModel.pretrained(&quot;classifierdl_use_sarcasm&quot;) \\</span>
<span class="sd">    ...     .setInputCols(&quot;sentence_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sarcasm&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       sentence,</span>
<span class="sd">    ...       useEmbeddings,</span>
<span class="sd">    ...       sarcasmDL</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([</span>
<span class="sd">    ...     [&quot;I&#39;m ready!&quot;],</span>
<span class="sd">    ...     [&quot;If I could put into words how much I love waking up at 6 am on Mondays I would.&quot;]</span>
<span class="sd">    ... ]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(arrays_zip(sentence, sarcasm)) as out&quot;) \\</span>
<span class="sd">    ...     .selectExpr(&quot;out.sentence.result as sentence&quot;, &quot;out.sarcasm.result as sarcasm&quot;) \\</span>
<span class="sd">    ...     .show(truncate=False)</span>
<span class="sd">    +-------------------------------------------------------------------------------+-------+</span>
<span class="sd">    |sentence                                                                       |sarcasm|</span>
<span class="sd">    +-------------------------------------------------------------------------------+-------+</span>
<span class="sd">    |I&#39;m ready!                                                                     |normal |</span>
<span class="sd">    |If I could put into words how much I love waking up at 6 am on Mondays I would.|sarcasm|</span>
<span class="sd">    +-------------------------------------------------------------------------------+-------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    MultiClassifierDLModel : for multi-class classification</span>
<span class="sd">    SentimentDLModel : for sentiment analysis</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;ClassifierDLModel&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ClassifierDLModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">classes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;classes&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;get the tags used to trained this ClassifierDLModel&quot;</span><span class="p">,</span>
                    <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="ClassifierDLModel.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLModel.html#sparknlp.annotator.ClassifierDLModel.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="ClassifierDLModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ClassifierDLModel.html#sparknlp.annotator.ClassifierDLModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;classifierdl_use_trec6&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;classifierdl_use_trec6&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ClassifierDLModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">ClassifierDLModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="AlbertEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.AlbertEmbeddings.html#sparknlp.annotator.AlbertEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">AlbertEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                       <span class="n">HasEmbeddingsProperties</span><span class="p">,</span>
                       <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                       <span class="n">HasStorageRef</span><span class="p">,</span>
                       <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;ALBERT: A Lite Bert For Self-Supervised Learning Of Language</span>
<span class="sd">    Representations - Google Research, Toyota Technological Institute at Chicago</span>

<span class="sd">    These word embeddings represent the outputs generated by the Albert model.</span>
<span class="sd">    All official Albert releases by google in TF-HUB are supported with this</span>
<span class="sd">    Albert Wrapper:</span>

<span class="sd">    **Ported TF-Hub Models:**</span>

<span class="sd">    ============================ ============================================================== =====================================================</span>
<span class="sd">    Model Name                   TF-Hub Model                                                   Model Properties</span>
<span class="sd">    ============================ ============================================================== =====================================================</span>
<span class="sd">    ``&quot;albert_base_uncased&quot;``    `albert_base &lt;https://tfhub.dev/google/albert_base/3&gt;`__       768-embed-dim,   12-layer,  12-heads, 12M parameters</span>
<span class="sd">    ``&quot;albert_large_uncased&quot;``   `albert_large &lt;https://tfhub.dev/google/albert_large/3&gt;`__     1024-embed-dim,  24-layer,  16-heads, 18M parameters</span>
<span class="sd">    ``&quot;albert_xlarge_uncased&quot;``  `albert_xlarge &lt;https://tfhub.dev/google/albert_xlarge/3&gt;`__   2048-embed-dim,  24-layer,  32-heads, 60M parameters</span>
<span class="sd">    ``&quot;albert_xxlarge_uncased&quot;`` `albert_xxlarge &lt;https://tfhub.dev/google/albert_xxlarge/3&gt;`__ 4096-embed-dim,  12-layer,  64-heads, 235M parameters</span>
<span class="sd">    ============================ ============================================================== =====================================================</span>

<span class="sd">    This model requires input tokenization with SentencePiece model, which is</span>
<span class="sd">    provided by Spark-NLP (See tokenizers package).</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = AlbertEmbeddings.pretrained() \\</span>
<span class="sd">    ...    .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...    .setOutputCol(&quot;embeddings&quot;)</span>


<span class="sd">    The default model is ``&quot;albert_base_uncased&quot;``, if no name is provided.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_albert.ipynb&gt;`__.</span>
<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``WORD_EMBEDDINGS``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Size of every batch, by default 8</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions, by default 768</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        False</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    ALBERT uses repeating layers which results in a small memory footprint,</span>
<span class="sd">    however the computational cost remains similar to a BERT-like architecture</span>
<span class="sd">    with the same number of hidden layers as it has to iterate through the same</span>
<span class="sd">    number of (repeating) layers.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS &lt;https://arxiv.org/pdf/1909.11942.pdf&gt;`__</span>

<span class="sd">    https://github.com/google-research/ALBERT</span>

<span class="sd">    https://tfhub.dev/s?q=albert</span>

<span class="sd">    **Paper abstract:**</span>

<span class="sd">    *Increasing model size when pretraining natural language representations</span>
<span class="sd">    often results in improved performance on downstream tasks. However, at some</span>
<span class="sd">    point further model increases become harder due to GPU/TPU memory</span>
<span class="sd">    limitations and longer training times. To address these problems, we present</span>
<span class="sd">    two parameter reduction techniques to lower memory consumption and increase</span>
<span class="sd">    the training speed of BERT (Devlin et al., 2019). Comprehensive empirical</span>
<span class="sd">    evidence shows that our proposed methods lead to models that scale much</span>
<span class="sd">    better compared to the original BERT. We also use a self-supervised loss</span>
<span class="sd">    that focuses on modeling inter-sentence coherence, and show it consistently</span>
<span class="sd">    helps downstream tasks with multi-sentence inputs. As a result, our best</span>
<span class="sd">    model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD</span>
<span class="sd">    benchmarks while having fewer parameters compared to BERT-large.*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    &gt;&gt;&gt; embeddings = AlbertEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True) \\</span>
<span class="sd">    ...     .setCleanAnnotations(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     embeddingsFinisher</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[1.1342473030090332,-1.3855540752410889,0.9818322062492371,-0.784737348556518...|</span>
<span class="sd">    |[0.847029983997345,-1.047153353691101,-0.1520637571811676,-0.6245765686035156...|</span>
<span class="sd">    |[-0.009860038757324219,-0.13450059294700623,2.707749128341675,1.2916892766952...|</span>
<span class="sd">    |[-0.04192575812339783,-0.5764210224151611,-0.3196685314178467,-0.527840495109...|</span>
<span class="sd">    |[0.15583214163780212,-0.1614152491092682,-0.28423872590065,-0.135491415858268...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    AlbertForTokenClassification : for  AlbertEmbeddings with a token classification layer on top</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;AlbertEmbeddings&quot;</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

<div class="viewcode-block" id="AlbertEmbeddings.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.AlbertEmbeddings.html#sparknlp.annotator.AlbertEmbeddings.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="AlbertEmbeddings.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.AlbertEmbeddings.html#sparknlp.annotator.AlbertEmbeddings.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AlbertEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

<div class="viewcode-block" id="AlbertEmbeddings.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.AlbertEmbeddings.html#sparknlp.annotator.AlbertEmbeddings.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        AlbertEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_AlbertLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_AlbertLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">AlbertEmbeddings</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="AlbertEmbeddings.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.AlbertEmbeddings.html#sparknlp.annotator.AlbertEmbeddings.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;albert_base_uncased&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;albert_base_uncased&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        AlbertEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">AlbertEmbeddings</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="XlnetEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlnetEmbeddings.html#sparknlp.annotator.XlnetEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">XlnetEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                      <span class="n">HasEmbeddingsProperties</span><span class="p">,</span>
                      <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                      <span class="n">HasStorageRef</span><span class="p">,</span>
                      <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;XlnetEmbeddings (XLNet): Generalized Autoregressive Pretraining for</span>
<span class="sd">    Language Understanding</span>

<span class="sd">    XLNet is a new unsupervised language representation learning method based on</span>
<span class="sd">    a novel generalized permutation language modeling objective. Additionally,</span>
<span class="sd">    XLNet employs Transformer-XL as the backbone model, exhibiting excellent</span>
<span class="sd">    performance for language tasks involving long context. Overall, XLNet</span>
<span class="sd">    achieves state-of-the-art (SOTA) results on various downstream language</span>
<span class="sd">    tasks including question answering, natural language inference, sentiment</span>
<span class="sd">    analysis, and document ranking.</span>

<span class="sd">    These word embeddings represent the outputs generated by the XLNet models.</span>

<span class="sd">    - ``&quot;xlnet_large_cased&quot;`` (`XLNet-Large</span>
<span class="sd">      &lt;https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip&gt;`__):</span>
<span class="sd">      24-layer, 1024-hidden, 16-heads</span>

<span class="sd">    - ``&quot;xlnet_base_cased&quot;`` (`XLNet-Base</span>
<span class="sd">      &lt;https://storage.googleapis.com/xlnet/released_models/cased_L-12_H-768_A-12.zip&gt;`__):</span>
<span class="sd">      12-layer, 768-hidden, 12-heads. This model is trained on full data</span>
<span class="sd">      (different from the one in the paper).</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = XlnetEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>

<span class="sd">    The default model is ``&quot;xlnet_base_cased&quot;``, if no name is provided.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_xlnet.ipynb&gt;`__.</span>
<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``WORD_EMBEDDINGS``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Size of every batch, by default 8</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions, by default 768</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This is a very computationally expensive module compared to word embedding</span>
<span class="sd">    modules that only perform embedding lookups. The use of an accelerator is</span>
<span class="sd">    recommended.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `XLNet: Generalized Autoregressive Pretraining for Language Understanding</span>
<span class="sd">    &lt;https://arxiv.org/abs/1906.08237&gt;`__</span>

<span class="sd">    https://github.com/zihangdai/xlnet</span>

<span class="sd">    **Paper abstract:**</span>

<span class="sd">    *With the capability of modeling bidirectional contexts, denoising</span>
<span class="sd">    autoencoding based pretraining like BERT achieves better performance than</span>
<span class="sd">    pretraining approaches based on autoregressive language modeling. However,</span>
<span class="sd">    relying on corrupting the input with masks, BERT neglects dependency between</span>
<span class="sd">    the masked positions and suffers from a pretrain-finetune discrepancy. In</span>
<span class="sd">    light of these pros and cons, we propose XLNet, a generalized autoregressive</span>
<span class="sd">    pretraining method that (1) enables learning bidirectional contexts by</span>
<span class="sd">    maximizing the expected likelihood over all permutations of the</span>
<span class="sd">    factorization order and (2) overcomes the limitations of BERT thanks to its</span>
<span class="sd">    autoregressive formulation. Furthermore, XLNet integrates ideas from</span>
<span class="sd">    Transformer-XL, the state-of-the-art autoregressive model, into pretraining.</span>
<span class="sd">    Empirically, under comparable experiment settings, XLNet outperforms BERT on</span>
<span class="sd">    20 tasks, often by a large margin, including question answering, natural</span>
<span class="sd">    language inference, sentiment analysis, and document ranking.*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = XlnetEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True) \\</span>
<span class="sd">    ...     .setCleanAnnotations(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     embeddingsFinisher</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[-0.6287205219268799,-0.4865287244319916,-0.186111718416214,0.234187275171279...|</span>
<span class="sd">    |[-1.1967450380325317,0.2746637463569641,0.9481253027915955,0.3431355059146881...|</span>
<span class="sd">    |[-1.0777631998062134,-2.092679977416992,-1.5331977605819702,-1.11190271377563...|</span>
<span class="sd">    |[-0.8349916934967041,-0.45627787709236145,-0.7890847325325012,-1.028069257736...|</span>
<span class="sd">    |[-0.134845569729805,-0.11672890186309814,0.4945235550403595,-0.66587203741073...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;XlnetEmbeddings&quot;</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

<div class="viewcode-block" id="XlnetEmbeddings.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlnetEmbeddings.html#sparknlp.annotator.XlnetEmbeddings.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="XlnetEmbeddings.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlnetEmbeddings.html#sparknlp.annotator.XlnetEmbeddings.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">XlnetEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="XlnetEmbeddings.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlnetEmbeddings.html#sparknlp.annotator.XlnetEmbeddings.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        XlnetEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_XlnetLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_XlnetLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">XlnetEmbeddings</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="XlnetEmbeddings.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlnetEmbeddings.html#sparknlp.annotator.XlnetEmbeddings.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;xlnet_base_cased&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;xlnet_base_cased&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        XlnetEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">XlnetEmbeddings</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ContextSpellCheckerApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach">[docs]</a><span class="k">class</span> <span class="nc">ContextSpellCheckerApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains a deep-learning based Noisy Channel Model Spell Algorithm.</span>

<span class="sd">    Correction candidates are extracted combining context information and word</span>
<span class="sd">    information.</span>

<span class="sd">    For instantiated/pretrained models, see :class:`.ContextSpellCheckerModel`.</span>

<span class="sd">    Spell Checking is a sequence to sequence mapping problem. Given an input</span>
<span class="sd">    sequence, potentially containing a certain number of errors,</span>
<span class="sd">    ``ContextSpellChecker`` will rank correction sequences according to three</span>
<span class="sd">    things:</span>

<span class="sd">    #. Different correction candidates for each word  **word level**.</span>
<span class="sd">    #. The surrounding text of each word, i.e. its context </span>
<span class="sd">       **sentence level**.</span>
<span class="sd">    #. The relative cost of different correction candidates according to the</span>
<span class="sd">       edit operations at the character level it requires  **subword level**.</span>

<span class="sd">    For extended examples of usage, see the article</span>
<span class="sd">    `Training a Contextual Spell Checker for Italian Language &lt;https://towardsdatascience.com/training-a-contextual-spell-checker-for-italian-language-66dda528e4bf&gt;`__,</span>
<span class="sd">    the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/blogposts/5.TrainingContextSpellChecker.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    languageModelClasses</span>
<span class="sd">        Number of classes to use during factorization of the softmax output in</span>
<span class="sd">        the LM.</span>
<span class="sd">    wordMaxDistance</span>
<span class="sd">        Maximum distance for the generated candidates for every word.</span>
<span class="sd">    maxCandidates</span>
<span class="sd">        Maximum number of candidates for every word.</span>
<span class="sd">    caseStrategy</span>
<span class="sd">        What case combinations to try when generating candidates, by default 2.</span>
<span class="sd">        Possible values are:</span>

<span class="sd">        - 0: All uppercase letters</span>
<span class="sd">        - 1: First letter capitalized</span>
<span class="sd">        - 2: All letters</span>
<span class="sd">    errorThreshold</span>
<span class="sd">        Threshold perplexity for a word to be considered as an error.</span>
<span class="sd">    epochs</span>
<span class="sd">        Number of epochs to train the language model.</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size for the training in NLM.</span>
<span class="sd">    initialRate</span>
<span class="sd">        Initial learning rate for the LM.</span>
<span class="sd">    finalRate</span>
<span class="sd">        Final learning rate for the LM.</span>
<span class="sd">    validationFraction</span>
<span class="sd">        Percentage of datapoints to use for validation.</span>
<span class="sd">    minCount</span>
<span class="sd">        Min number of times a token should appear to be included in vocab.</span>
<span class="sd">    compoundCount</span>
<span class="sd">        Min number of times a compound word should appear to be included in</span>
<span class="sd">        vocab.</span>
<span class="sd">    classCount</span>
<span class="sd">        Min number of times the word need to appear in corpus to not be</span>
<span class="sd">        considered of a special class.</span>
<span class="sd">    tradeoff</span>
<span class="sd">        Tradeoff between the cost of a word error and a transition in the</span>
<span class="sd">        language model.</span>
<span class="sd">    weightedDistPath</span>
<span class="sd">        The path to the file containing the weights for the levenshtein</span>
<span class="sd">        distance.</span>
<span class="sd">    maxWindowLen</span>
<span class="sd">        Maximum size for the window used to remember history prior to every</span>
<span class="sd">        correction.</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    For an in-depth explanation of the module see the article</span>
<span class="sd">    `Applying Context Aware Spell Checking in Spark NLP &lt;https://medium.com/spark-nlp/applying-context-aware-spell-checking-in-spark-nlp-3c29c46963bc&gt;`__.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    For this example, we use the first Sherlock Holmes book as the training dataset.</span>

<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spellChecker = ContextSpellCheckerApproach() \\</span>
<span class="sd">    ...     .setInputCols(&quot;token&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;corrected&quot;) \\</span>
<span class="sd">    ...     .setWordMaxDistance(3) \\</span>
<span class="sd">    ...     .setBatchSize(24) \\</span>
<span class="sd">    ...     .setEpochs(8) \\</span>
<span class="sd">    ...     .setLanguageModelClasses(1650)  # dependant on vocabulary size</span>
<span class="sd">    ...     # .addVocabClass(&quot;_NAME_&quot;, names) # Extra classes for correction could be added like this</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     spellChecker</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; path = &quot;sherlockholmes.txt&quot;</span>
<span class="sd">    &gt;&gt;&gt; dataset = spark.read.text(path) \\</span>
<span class="sd">    ...     .toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(dataset)</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    NorvigSweetingApproach, SymmetricDeleteApproach : For alternative approaches to spell checking</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;ContextSpellCheckerApproach&quot;</span>

    <span class="n">languageModelClasses</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                 <span class="s2">&quot;languageModelClasses&quot;</span><span class="p">,</span>
                                 <span class="s2">&quot;Number of classes to use during factorization of the softmax output in the LM.&quot;</span><span class="p">,</span>
                                 <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">wordMaxDistance</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;wordMaxDistance&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Maximum distance for the generated candidates for every word.&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">maxCandidates</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;maxCandidates&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;Maximum number of candidates for every word.&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">caseStrategy</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;caseStrategy&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;What case combinations to try when generating candidates.&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">errorThreshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;errorThreshold&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;Threshold perplexity for a word to be considered as an error.&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">epochs</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                   <span class="s2">&quot;epochs&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;Number of epochs to train the language model.&quot;</span><span class="p">,</span>
                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">batchSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;batchSize&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Batch size for the training in NLM.&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">initialRate</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;initialRate&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;Initial learning rate for the LM.&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">finalRate</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;finalRate&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Final learning rate for the LM.&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">validationFraction</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                               <span class="s2">&quot;validationFraction&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;Percentage of datapoints to use for validation.&quot;</span><span class="p">,</span>
                               <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">minCount</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;minCount&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;Min number of times a token should appear to be included in vocab.&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">compoundCount</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;compoundCount&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;Min number of times a compound word should appear to be included in vocab.&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">classCount</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;classCount&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;Min number of times the word need to appear in corpus to not be considered of a special class.&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">tradeoff</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;tradeoff&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;Tradeoff between the cost of a word error and a transition in the language model.&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">weightedDistPath</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;weightedDistPath&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;The path to the file containing the weights for the levenshtein distance.&quot;</span><span class="p">,</span>
                             <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">maxWindowLen</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;maxWindowLen&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;Maximum size for the window used to remember history prior to every correction.&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setLanguageModelClasses"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setLanguageModelClasses">[docs]</a>    <span class="k">def</span> <span class="nf">setLanguageModelClasses</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">count</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets number of classes to use during factorization of the softmax</span>
<span class="sd">        output in the Language Model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        count : int</span>
<span class="sd">            Number of classes</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">languageModelClasses</span><span class="o">=</span><span class="n">count</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setWordMaxDistance"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setWordMaxDistance">[docs]</a>    <span class="k">def</span> <span class="nf">setWordMaxDistance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dist</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets maximum distance for the generated candidates for every word.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        dist : int</span>
<span class="sd">            Maximum distance for the generated candidates for every word</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">wordMaxDistance</span><span class="o">=</span><span class="n">dist</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setMaxCandidates"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setMaxCandidates">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxCandidates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">candidates</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets maximum number of candidates for every word.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        candidates : int</span>
<span class="sd">            Maximum number of candidates for every word.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxCandidates</span><span class="o">=</span><span class="n">candidates</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setCaseStrategy"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setCaseStrategy">[docs]</a>    <span class="k">def</span> <span class="nf">setCaseStrategy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strategy</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets what case combinations to try when generating candidates.</span>

<span class="sd">        Possible values are:</span>

<span class="sd">        - 0: All uppercase letters</span>
<span class="sd">        - 1: First letter capitalized</span>
<span class="sd">        - 2: All letters</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        strategy : int</span>
<span class="sd">            Case combinations to try when generating candidates</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">caseStrategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setErrorThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setErrorThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setErrorThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets threshold perplexity for a word to be considered as an error.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        threshold : float</span>
<span class="sd">            Threshold perplexity for a word to be considered as an error</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">errorThreshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setEpochs"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setEpochs">[docs]</a>    <span class="k">def</span> <span class="nf">setEpochs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">count</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets number of epochs to train the language model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        count : int</span>
<span class="sd">            Number of epochs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">epochs</span><span class="o">=</span><span class="n">count</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setBatchSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setBatchSize">[docs]</a>    <span class="k">def</span> <span class="nf">setBatchSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets batch size.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        size : int</span>
<span class="sd">            Batch size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">batchSize</span><span class="o">=</span><span class="n">size</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setInitialRate"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setInitialRate">[docs]</a>    <span class="k">def</span> <span class="nf">setInitialRate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rate</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets initial learning rate for the LM.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        rate : float</span>
<span class="sd">            Initial learning rate for the LM</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">initialRate</span><span class="o">=</span><span class="n">rate</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setFinalRate"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setFinalRate">[docs]</a>    <span class="k">def</span> <span class="nf">setFinalRate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rate</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets final learning rate for the LM.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        rate : float</span>
<span class="sd">            Final learning rate for the LM</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">finalRate</span><span class="o">=</span><span class="n">rate</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setValidationFraction"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setValidationFraction">[docs]</a>    <span class="k">def</span> <span class="nf">setValidationFraction</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fraction</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets percentage of datapoints to use for validation.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        fraction : float</span>
<span class="sd">            Percentage of datapoints to use for validation</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">validationFraction</span><span class="o">=</span><span class="n">fraction</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setMinCount"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setMinCount">[docs]</a>    <span class="k">def</span> <span class="nf">setMinCount</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">count</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets min number of times a token should appear to be included in</span>
<span class="sd">        vocab.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        count : float</span>
<span class="sd">            Min number of times a token should appear to be included in vocab</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">minCount</span><span class="o">=</span><span class="n">count</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setCompoundCount"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setCompoundCount">[docs]</a>    <span class="k">def</span> <span class="nf">setCompoundCount</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">count</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets min number of times a compound word should appear to be included</span>
<span class="sd">        in vocab.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        count : int</span>
<span class="sd">            Min number of times a compound word should appear to be included in</span>
<span class="sd">            vocab.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">compoundCount</span><span class="o">=</span><span class="n">count</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setClassCount"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setClassCount">[docs]</a>    <span class="k">def</span> <span class="nf">setClassCount</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">count</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets min number of times the word need to appear in corpus to not be</span>
<span class="sd">        considered of a special class.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        count : float</span>
<span class="sd">            Min number of times the word need to appear in corpus to not be</span>
<span class="sd">            considered of a special class.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">classCount</span><span class="o">=</span><span class="n">count</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setTradeoff"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setTradeoff">[docs]</a>    <span class="k">def</span> <span class="nf">setTradeoff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets tradeoff between the cost of a word error and a transition in</span>
<span class="sd">        the language model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        alpha : float</span>
<span class="sd">            Tradeoff between the cost of a word error and a transition in the</span>
<span class="sd">            language model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">tradeoff</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">setWeightedDistPath</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the path to the file containing the weights for the levenshtein</span>
<span class="sd">        distance.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the file containing the weights for the levenshtein</span>
<span class="sd">            distance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">weightedDistPath</span><span class="o">=</span><span class="n">path</span><span class="p">)</span>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setWeightedDistPath"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setWeightedDistPath">[docs]</a>    <span class="k">def</span> <span class="nf">setWeightedDistPath</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the path to the file containing the weights for the levenshtein</span>
<span class="sd">        distance.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the file containing the weights for the levenshtein</span>
<span class="sd">            distance.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">weightedDistPath</span><span class="o">=</span><span class="n">path</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setMaxWindowLen"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setMaxWindowLen">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxWindowLen</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the maximum size for the window used to remember history prior</span>
<span class="sd">        to every correction.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        length : int</span>
<span class="sd">            Maximum size for the window used to remember history prior to</span>
<span class="sd">            every correction</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxWindowLen</span><span class="o">=</span><span class="n">length</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.addVocabClass"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.addVocabClass">[docs]</a>    <span class="k">def</span> <span class="nf">addVocabClass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">userdist</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds a new class of words to correct, based on a vocabulary.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        label : str</span>
<span class="sd">            Name of the class</span>
<span class="sd">        vocab : List[str]</span>
<span class="sd">            Vocabulary as a list</span>
<span class="sd">        userdist : int, optional</span>
<span class="sd">            Maximal distance to the word, by default 3</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s1">&#39;addVocabClass&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">userdist</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="ContextSpellCheckerApproach.addRegexClass"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerApproach.html#sparknlp.annotator.ContextSpellCheckerApproach.addRegexClass">[docs]</a>    <span class="k">def</span> <span class="nf">addRegexClass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">regex</span><span class="p">,</span> <span class="n">userdist</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Adds a new class of words to correct, based on regex.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        label : str</span>
<span class="sd">            Name of the class</span>
<span class="sd">        regex : str</span>
<span class="sd">            Regex to add</span>
<span class="sd">        userdist : int, optional</span>
<span class="sd">            Maximal distance to the word, by default 3</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s1">&#39;addRegexClass&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">regex</span><span class="p">,</span> <span class="n">userdist</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ContextSpellCheckerApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span> \
            <span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ContextSpellCheckerModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="ContextSpellCheckerModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel">[docs]</a><span class="k">class</span> <span class="nc">ContextSpellCheckerModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implements a deep-learning based Noisy Channel Model Spell Algorithm.</span>
<span class="sd">    Correction candidates are extracted combining context information and word</span>
<span class="sd">    information.</span>

<span class="sd">    Spell Checking is a sequence to sequence mapping problem. Given an input</span>
<span class="sd">    sequence, potentially containing a certain number of errors,</span>
<span class="sd">    ``ContextSpellChecker`` will rank correction sequences according to three</span>
<span class="sd">    things:</span>

<span class="sd">    #. Different correction candidates for each word  **word level**.</span>
<span class="sd">    #. The surrounding text of each word, i.e. its context </span>
<span class="sd">       **sentence level**.</span>
<span class="sd">    #. The relative cost of different correction candidates according to the</span>
<span class="sd">       edit operations at the character level it requires  **subword level**.</span>

<span class="sd">    This is the instantiated model of the :class:`.ContextSpellCheckerApproach`.</span>
<span class="sd">    For training your own model, please see the documentation of that class.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; spellChecker = ContextSpellCheckerModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;checked&quot;)</span>


<span class="sd">    The default model is ``&quot;spellcheck_dl&quot;``, if no name is provided.</span>
<span class="sd">    For available pretrained models please see the `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Spell+Check&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/SPELL_CHECKER_EN.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    wordMaxDistance</span>
<span class="sd">        Maximum distance for the generated candidates for every word.</span>
<span class="sd">    maxCandidates</span>
<span class="sd">        Maximum number of candidates for every word.</span>
<span class="sd">    caseStrategy</span>
<span class="sd">        What case combinations to try when generating candidates.</span>
<span class="sd">    errorThreshold</span>
<span class="sd">        Threshold perplexity for a word to be considered as an error.</span>
<span class="sd">    tradeoff</span>
<span class="sd">        Tradeoff between the cost of a word error and a transition in the</span>
<span class="sd">        language model.</span>
<span class="sd">    maxWindowLen</span>
<span class="sd">        Maximum size for the window used to remember history prior to every</span>
<span class="sd">        correction.</span>
<span class="sd">    gamma</span>
<span class="sd">        Controls the influence of individual word frequency in the decision.</span>
<span class="sd">    correctSymbols</span>
<span class="sd">        Whether to correct special symbols or skip spell checking for them</span>
<span class="sd">    compareLowcase</span>
<span class="sd">        If true will compare tokens in low case with vocabulary</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>


<span class="sd">    References</span>
<span class="sd">    -------------</span>
<span class="sd">    For an in-depth explanation of the module see the article `Applying Context</span>
<span class="sd">    Aware Spell Checking in Spark NLP</span>
<span class="sd">    &lt;https://medium.com/spark-nlp/applying-context-aware-spell-checking-in-spark-nlp-3c29c46963bc&gt;`__.</span>


<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;doc&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;doc&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; spellChecker = ContextSpellCheckerModel \\</span>
<span class="sd">    ...     .pretrained() \\</span>
<span class="sd">    ...     .setTradeoff(12.0) \\</span>
<span class="sd">    ...     .setInputCols(&quot;token&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;checked&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     spellChecker</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;It was a cold , dreary day and the country was white with smow .&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;checked.result&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                          |</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[It, was, a, cold, ,, dreary, day, and, the, country, was, white, with, snow, .]|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    NorvigSweetingModel, SymmetricDeleteModel: For alternative approaches to spell checking</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;ContextSpellCheckerModel&quot;</span>

    <span class="n">wordMaxDistance</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;wordMaxDistance&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Maximum distance for the generated candidates for every word.&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">maxCandidates</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;maxCandidates&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;Maximum number of candidates for every word.&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">caseStrategy</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;caseStrategy&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;What case combinations to try when generating candidates.&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">errorThreshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;errorThreshold&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;Threshold perplexity for a word to be considered as an error.&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">tradeoff</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;tradeoff&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;Tradeoff between the cost of a word error and a transition in the language model.&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">maxWindowLen</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;maxWindowLen&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;Maximum size for the window used to remember history prior to every correction.&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">gamma</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                  <span class="s2">&quot;gamma&quot;</span><span class="p">,</span>
                  <span class="s2">&quot;Controls the influence of individual word frequency in the decision.&quot;</span><span class="p">,</span>
                  <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">correctSymbols</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;correctSymbols&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;Whether to correct special symbols or skip spell checking for them&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">compareLowcase</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;compareLowcase&quot;</span><span class="p">,</span> <span class="s2">&quot;If true will compare tokens in low case with vocabulary&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="ContextSpellCheckerModel.setWordMaxDistance"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.setWordMaxDistance">[docs]</a>    <span class="k">def</span> <span class="nf">setWordMaxDistance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dist</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets maximum distance for the generated candidates for every word.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        dist : int</span>
<span class="sd">            Maximum distance for the generated candidates for every word.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">wordMaxDistance</span><span class="o">=</span><span class="n">dist</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.setMaxCandidates"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.setMaxCandidates">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxCandidates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">candidates</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets maximum number of candidates for every word.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        candidates : int</span>
<span class="sd">            Maximum number of candidates for every word.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxCandidates</span><span class="o">=</span><span class="n">candidates</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.setCaseStrategy"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.setCaseStrategy">[docs]</a>    <span class="k">def</span> <span class="nf">setCaseStrategy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">strategy</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets what case combinations to try when generating candidates.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        strategy : int</span>
<span class="sd">            Case combinations to try when generating candidates.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">caseStrategy</span><span class="o">=</span><span class="n">strategy</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.setErrorThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.setErrorThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setErrorThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets threshold perplexity for a word to be considered as an error.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        threshold : float</span>
<span class="sd">            Threshold perplexity for a word to be considered as an error</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">errorThreshold</span><span class="o">=</span><span class="n">threshold</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.setTradeoff"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.setTradeoff">[docs]</a>    <span class="k">def</span> <span class="nf">setTradeoff</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets tradeoff between the cost of a word error and a transition in the</span>
<span class="sd">        language model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        alpha : float</span>
<span class="sd">            Tradeoff between the cost of a word error and a transition in the</span>
<span class="sd">            language model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">tradeoff</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.setWeights"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.setWeights">[docs]</a>    <span class="k">def</span> <span class="nf">setWeights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets weights of each word for Levenshtein distance.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        weights : Dict[str, float]</span>
<span class="sd">            Weights for Levenshtein distance as a maping.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s1">&#39;setWeights&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.setMaxWindowLen"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.setMaxWindowLen">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxWindowLen</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">length</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the maximum size for the window used to remember history prior to</span>
<span class="sd">        every correction.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        length : int</span>
<span class="sd">            Maximum size for the window used to remember history prior to</span>
<span class="sd">            every correction</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxWindowLen</span><span class="o">=</span><span class="n">length</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.setGamma"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.setGamma">[docs]</a>    <span class="k">def</span> <span class="nf">setGamma</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">g</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the influence of individual word frequency in the decision.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        g : float</span>
<span class="sd">            Controls the influence of individual word frequency in the decision.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="n">g</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.getWordClasses"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.getWordClasses">[docs]</a>    <span class="k">def</span> <span class="nf">getWordClasses</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets the classes of words to be corrected.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[str]</span>
<span class="sd">            Classes of words to be corrected</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">it</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s1">&#39;getWordClasses&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">toIterator</span><span class="p">()</span>
        <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">while</span> <span class="p">(</span><span class="n">it</span><span class="o">.</span><span class="n">hasNext</span><span class="p">()):</span>
            <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">it</span><span class="o">.</span><span class="n">next</span><span class="p">()</span><span class="o">.</span><span class="n">toString</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">result</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.updateRegexClass"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.updateRegexClass">[docs]</a>    <span class="k">def</span> <span class="nf">updateRegexClass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">regex</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update existing class to correct, based on regex</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        label : str</span>
<span class="sd">            Label of the class</span>
<span class="sd">        regex : str</span>
<span class="sd">            Regex to parse the class</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s1">&#39;updateRegexClass&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">regex</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.updateVocabClass"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.updateVocabClass">[docs]</a>    <span class="k">def</span> <span class="nf">updateVocabClass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">append</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Update existing class to correct, based on a vocabulary.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        label : str</span>
<span class="sd">            Label of the class</span>
<span class="sd">        vocab : List[str]</span>
<span class="sd">            Vocabulary as a list</span>
<span class="sd">        append : bool, optional</span>
<span class="sd">            Whether to append to the existing vocabulary, by default True</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_call_java</span><span class="p">(</span><span class="s1">&#39;updateVocabClass&#39;</span><span class="p">,</span> <span class="n">label</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">append</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.setCorrectSymbols"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.setCorrectSymbols">[docs]</a>    <span class="k">def</span> <span class="nf">setCorrectSymbols</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to correct special symbols or skip spell checking for</span>
<span class="sd">        them.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to correct special symbols or skip spell checking for</span>
<span class="sd">            them</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">correctSymbols</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="ContextSpellCheckerModel.setCompareLowcase"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.setCompareLowcase">[docs]</a>    <span class="k">def</span> <span class="nf">setCompareLowcase</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to compare tokens in lower case with vocabulary.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to compare tokens in lower case with vocabulary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">compareLowcase</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerModel&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ContextSpellCheckerModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="ContextSpellCheckerModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.ContextSpellCheckerModel.html#sparknlp.annotator.ContextSpellCheckerModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;spellcheck_dl&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;spellcheck_dl&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        ContextSpellCheckerModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">ContextSpellCheckerModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="SentimentDLApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach">[docs]</a><span class="k">class</span> <span class="nc">SentimentDLApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains a SentimentDL, an annotator for multi-class sentiment analysis.</span>

<span class="sd">    In natural language processing, sentiment analysis is the task of</span>
<span class="sd">    classifying the affective state or subjective view of a text. A common</span>
<span class="sd">    example is if either a product review or tweet can be interpreted positively</span>
<span class="sd">    or negatively.</span>

<span class="sd">    For the instantiated/pretrained models, see :class:`.SentimentDLModel`.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/classification/SentimentDL_train_multiclass_sentiment_classifier.ipynb&gt;`__.</span>

<span class="sd">    ======================= ======================</span>
<span class="sd">    Input Annotation types  Output Annotation type</span>
<span class="sd">    ======================= ======================</span>
<span class="sd">    ``SENTENCE_EMBEDDINGS`` ``CATEGORY``</span>
<span class="sd">    ======================= ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lr</span>
<span class="sd">        Learning Rate, by default 0.005</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size, by default 64</span>
<span class="sd">    dropout</span>
<span class="sd">        Dropout coefficient, by default 0.5</span>
<span class="sd">    maxEpochs</span>
<span class="sd">        Maximum number of epochs to train, by default 30</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    validationSplit</span>
<span class="sd">        Choose the proportion of training dataset to be validated against the</span>
<span class="sd">        model on each Epoch. The value should be between 0.0 and 1.0 and by</span>
<span class="sd">        default it is 0.0 and off.</span>
<span class="sd">    enableOutputLogs</span>
<span class="sd">        Whether to use stdout in addition to Spark logs, by default False</span>
<span class="sd">    outputLogsPath</span>
<span class="sd">        Folder path to save training logs</span>
<span class="sd">    labelColumn</span>
<span class="sd">        Column with label per each token</span>
<span class="sd">    verbose</span>
<span class="sd">        Level of verbosity during training</span>
<span class="sd">    randomSeed</span>
<span class="sd">        Random seed</span>
<span class="sd">    threshold</span>
<span class="sd">        The minimum threshold for the final result otheriwse it will be neutral,</span>
<span class="sd">        by default 0.6</span>
<span class="sd">    thresholdLabel</span>
<span class="sd">        In case the score is less than threshold, what should be the label.</span>
<span class="sd">        Default is neutral, by default &quot;neutral&quot;</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    - This annotator accepts a label column of a single item in either type of</span>
<span class="sd">      String, Int, Float, or Double. So positive sentiment can be expressed as</span>
<span class="sd">      either ``&quot;positive&quot;`` or ``0``, negative sentiment as ``&quot;negative&quot;`` or</span>
<span class="sd">      ``1``.</span>
<span class="sd">    - UniversalSentenceEncoder, BertSentenceEmbeddings, or SentenceEmbeddings</span>
<span class="sd">      can be used for the ``inputCol``.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    In this example, ``sentiment.csv`` is in the form::</span>

<span class="sd">        text,label</span>
<span class="sd">        This movie is the best movie I have watched ever! In my opinion this movie can win an award.,0</span>
<span class="sd">        This was a terrible movie! The acting was bad really bad!,1</span>

<span class="sd">    The model can then be trained with</span>

<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; smallCorpus = spark.read.option(&quot;header&quot;, &quot;True&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;)</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; useEmbeddings = UniversalSentenceEncoder.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence_embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; docClassifier = SentimentDLApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence_embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentiment&quot;) \\</span>
<span class="sd">    ...     .setLabelColumn(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setBatchSize(32) \\</span>
<span class="sd">    ...     .setMaxEpochs(1) \\</span>
<span class="sd">    ...     .setLr(5e-3) \\</span>
<span class="sd">    ...     .setDropout(0.5)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...         documentAssembler,</span>
<span class="sd">    ...         useEmbeddings,</span>
<span class="sd">    ...         docClassifier</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(smallCorpus)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lr</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="s2">&quot;Learning Rate&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">batchSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;batchSize&quot;</span><span class="p">,</span> <span class="s2">&quot;Batch size&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">dropout</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="s2">&quot;Dropout coefficient&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">maxEpochs</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;maxEpochs&quot;</span><span class="p">,</span> <span class="s2">&quot;Maximum number of epochs to train&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">validationSplit</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;validationSplit&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Choose the proportion of training dataset to be validated against the model on each Epoch. The value should be between 0.0 and 1.0 and by default it is 0.0 and off.&quot;</span><span class="p">,</span>
                            <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">enableOutputLogs</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;enableOutputLogs&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;Whether to use stdout in addition to Spark logs.&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">outputLogsPath</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;outputLogsPath&quot;</span><span class="p">,</span> <span class="s2">&quot;Folder path to save training logs&quot;</span><span class="p">,</span>
                           <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">labelColumn</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;labelColumn&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;Column with label per each token&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">verbose</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;verbose&quot;</span><span class="p">,</span> <span class="s2">&quot;Level of verbosity during training&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">randomSeed</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;randomSeed&quot;</span><span class="p">,</span> <span class="s2">&quot;Random seed&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;threshold&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;The minimum threshold for the final result otheriwse it will be neutral&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>
    <span class="n">thresholdLabel</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;thresholdLabel&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;In case the score is less than threshold, what should be the label. Default is neutral.&quot;</span><span class="p">,</span>
                           <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

<div class="viewcode-block" id="SentimentDLApproach.setVerbose"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setVerbose">[docs]</a>    <span class="k">def</span> <span class="nf">setVerbose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets level of verbosity during training</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Level of verbosity</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentimentDLApproach.setRandomSeed"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setRandomSeed">[docs]</a>    <span class="k">def</span> <span class="nf">setRandomSeed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets random seed for shuffling</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        seed : int</span>
<span class="sd">            Random seed for shuffling</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">randomSeed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentimentDLApproach.setLabelColumn"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setLabelColumn">[docs]</a>    <span class="k">def</span> <span class="nf">setLabelColumn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets name of column for data labels</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Column for data labels</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">labelColumn</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentimentDLApproach.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentimentDLApproach.setLr"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setLr">[docs]</a>    <span class="k">def</span> <span class="nf">setLr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets Learning Rate, by default 0.005</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Learning Rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="SentimentDLApproach.setBatchSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setBatchSize">[docs]</a>    <span class="k">def</span> <span class="nf">setBatchSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets batch size, by default 64.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : int</span>
<span class="sd">            Batch size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">batchSize</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="SentimentDLApproach.setDropout"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setDropout">[docs]</a>    <span class="k">def</span> <span class="nf">setDropout</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets dropout coefficient, by default 0.5.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Dropout coefficient</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">dropout</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="SentimentDLApproach.setMaxEpochs"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setMaxEpochs">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxEpochs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets maximum number of epochs to train, by default 30.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        epochs : int</span>
<span class="sd">            Maximum number of epochs to train</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxEpochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">SentimentDLModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span>

<div class="viewcode-block" id="SentimentDLApproach.setValidationSplit"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setValidationSplit">[docs]</a>    <span class="k">def</span> <span class="nf">setValidationSplit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the proportion of training dataset to be validated against the</span>
<span class="sd">        model on each Epoch, by default it is 0.0 and off. The value should be</span>
<span class="sd">        between 0.0 and 1.0.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Proportion of training dataset to be validated</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">validationSplit</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="SentimentDLApproach.setEnableOutputLogs"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setEnableOutputLogs">[docs]</a>    <span class="k">def</span> <span class="nf">setEnableOutputLogs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to use stdout in addition to Spark logs, by default</span>
<span class="sd">        False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to use stdout in addition to Spark logs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">enableOutputLogs</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentimentDLApproach.setOutputLogsPath"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setOutputLogsPath">[docs]</a>    <span class="k">def</span> <span class="nf">setOutputLogsPath</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets folder path to save training logs.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        p : str</span>
<span class="sd">            Folder path to save training logs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">outputLogsPath</span><span class="o">=</span><span class="n">p</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentimentDLApproach.setThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the minimum threshold for the final result otheriwse it will be</span>
<span class="sd">        neutral, by default 0.6.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Minimum threshold for the final result</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="SentimentDLApproach.setThresholdLabel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLApproach.html#sparknlp.annotator.SentimentDLApproach.setThresholdLabel">[docs]</a>    <span class="k">def</span> <span class="nf">setThresholdLabel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets what the label should be, if the score is less than threshold,</span>
<span class="sd">        by default &quot;neutral&quot;.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        p : str</span>
<span class="sd">            The label, if the score is less than threshold</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">thresholdLabel</span><span class="o">=</span><span class="n">p</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SentimentDLApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.SentimentDLApproach&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">maxEpochs</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.005</span><span class="p">),</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">enableOutputLogs</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">threshold</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">thresholdLabel</span><span class="o">=</span><span class="s2">&quot;neutral&quot;</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="SentimentDLModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLModel.html#sparknlp.annotator.SentimentDLModel">[docs]</a><span class="k">class</span> <span class="nc">SentimentDLModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasStorageRef</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;SentimentDL, an annotator for multi-class sentiment analysis.</span>

<span class="sd">    In natural language processing, sentiment analysis is the task of</span>
<span class="sd">    classifying the affective state or subjective view of a text. A common</span>
<span class="sd">    example is if either a product review or tweet can be interpreted positively</span>
<span class="sd">    or negatively.</span>

<span class="sd">    This is the instantiated model of the :class:`.SentimentDLApproach`. For</span>
<span class="sd">    training your own model, please see the documentation of that class.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; sentiment = SentimentDLModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence_embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentiment&quot;)</span>


<span class="sd">    The default model is ``&quot;sentimentdl_use_imdb&quot;``, if no name is provided. It</span>
<span class="sd">    is english sentiment analysis trained on the IMDB dataset. For available</span>
<span class="sd">    pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Sentiment+Analysis&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/5.Text_Classification_with_ClassifierDL.ipynb&gt;`__.</span>

<span class="sd">    ======================= ======================</span>
<span class="sd">    Input Annotation types  Output Annotation type</span>
<span class="sd">    ======================= ======================</span>
<span class="sd">    ``SENTENCE_EMBEDDINGS`` ``CATEGORY``</span>
<span class="sd">    ======================= ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    threshold</span>
<span class="sd">        The minimum threshold for the final result otheriwse it will be neutral,</span>
<span class="sd">        by default 0.6</span>
<span class="sd">    thresholdLabel</span>
<span class="sd">        In case the score is less than threshold, what should be the label.</span>
<span class="sd">        Default is neutral, by default &quot;neutral&quot;</span>
<span class="sd">    classes</span>
<span class="sd">        Tags used to trained this SentimentDLModel</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; useEmbeddings = UniversalSentenceEncoder.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence_embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentiment = SentimentDLModel.pretrained(&quot;sentimentdl_use_twitter&quot;) \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence_embeddings&quot;]) \\</span>
<span class="sd">    ...     .setThreshold(0.7) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentiment&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     useEmbeddings,</span>
<span class="sd">    ...     sentiment</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([</span>
<span class="sd">    ...     [&quot;Wow, the new video is awesome!&quot;],</span>
<span class="sd">    ...     [&quot;bruh what a damn waste of time&quot;]</span>
<span class="sd">    ... ]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;text&quot;, &quot;sentiment.result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------+----------+</span>
<span class="sd">    |text                          |result    |</span>
<span class="sd">    +------------------------------+----------+</span>
<span class="sd">    |Wow, the new video is awesome!|[positive]|</span>
<span class="sd">    |bruh what a damn waste of time|[negative]|</span>
<span class="sd">    +------------------------------+----------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;SentimentDLModel&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.SentimentDLModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SentimentDLModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">threshold</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span>
            <span class="n">thresholdLabel</span><span class="o">=</span><span class="s2">&quot;neutral&quot;</span>
        <span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;threshold&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;The minimum threshold for the final result otheriwse it will be neutral&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>
    <span class="n">thresholdLabel</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;thresholdLabel&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;In case the score is less than threshold, what should be the label. Default is neutral.&quot;</span><span class="p">,</span>
                           <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;classes&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;get the tags used to trained this SentimentDLModel&quot;</span><span class="p">,</span>
                    <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="SentimentDLModel.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLModel.html#sparknlp.annotator.SentimentDLModel.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentimentDLModel.setThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLModel.html#sparknlp.annotator.SentimentDLModel.setThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the minimum threshold for the final result otheriwse it will be</span>
<span class="sd">        neutral, by default 0.6.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Minimum threshold for the final result</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="SentimentDLModel.setThresholdLabel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLModel.html#sparknlp.annotator.SentimentDLModel.setThresholdLabel">[docs]</a>    <span class="k">def</span> <span class="nf">setThresholdLabel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets what the label should be, if the score is less than threshold,</span>
<span class="sd">        by default &quot;neutral&quot;.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        p : str</span>
<span class="sd">            The label, if the score is less than threshold</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">thresholdLabel</span><span class="o">=</span><span class="n">p</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentimentDLModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentimentDLModel.html#sparknlp.annotator.SentimentDLModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;sentimentdl_use_imdb&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;sentimentdl_use_imdb&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SentimentDLModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">SentimentDLModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="LanguageDetectorDL"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LanguageDetectorDL.html#sparknlp.annotator.LanguageDetectorDL">[docs]</a><span class="k">class</span> <span class="nc">LanguageDetectorDL</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasStorageRef</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Language Identification and Detection by using CNN and RNN architectures</span>
<span class="sd">    in TensorFlow.</span>

<span class="sd">    ``LanguageDetectorDL`` is an annotator that detects the language of</span>
<span class="sd">    documents or sentences depending on the inputCols. The models are trained on</span>
<span class="sd">    large datasets such as Wikipedia and Tatoeba. Depending on the language</span>
<span class="sd">    (how similar the characters are), the LanguageDetectorDL works best with</span>
<span class="sd">    text longer than 140 characters. The output is a language code in</span>
<span class="sd">    `Wiki Code style &lt;https://en.wikipedia.org/wiki/List_of_Wikipedias&gt;`__.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; languageDetector = LanguageDetectorDL.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;language&quot;)</span>

<span class="sd">    The default model is ``&quot;ld_wiki_tatoeba_cnn_21&quot;``, default language is</span>
<span class="sd">    ``&quot;xx&quot;`` (meaning multi-lingual), if no values are provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Language+Detection&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/annotation/english/language-detection/Language_Detection_and_Indentification.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``LANGUAGE``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    threshold</span>
<span class="sd">        The minimum threshold for the final result otheriwse it will be either</span>
<span class="sd">        neutral or the value set in thresholdLabel, by default 0.5</span>
<span class="sd">    thresholdLabel</span>
<span class="sd">        In case the score is less than threshold, what should be the label, by</span>
<span class="sd">        default Unknown</span>
<span class="sd">    coalesceSentences</span>
<span class="sd">        If sets to true the output of all sentences will be averaged to one</span>
<span class="sd">        output instead of one output per sentence, by default True.</span>
<span class="sd">    languages</span>
<span class="sd">       The languages used to trained the model</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; languageDetector = LanguageDetectorDL.pretrained() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;language&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       languageDetector</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([</span>
<span class="sd">    ...     [&quot;Spark NLP is an open-source text processing library for advanced natural language processing for the Python, Java and Scala programming languages.&quot;],</span>
<span class="sd">    ...     [&quot;Spark NLP est une bibliothque de traitement de texte open source pour le traitement avanc du langage naturel pour les langages de programmation Python, Java et Scala.&quot;],</span>
<span class="sd">    ...     [&quot;Spark NLP ist eine Open-Source-Textverarbeitungsbibliothek fr fortgeschrittene natrliche Sprachverarbeitung fr die Programmiersprachen Python, Java und Scala.&quot;]</span>
<span class="sd">    ... ]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;language.result&quot;).show(truncate=False)</span>
<span class="sd">    +------+</span>
<span class="sd">    |result|</span>
<span class="sd">    +------+</span>
<span class="sd">    |[en]  |</span>
<span class="sd">    |[fr]  |</span>
<span class="sd">    |[de]  |</span>
<span class="sd">    +------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;LanguageDetectorDL&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.ld.dl.LanguageDetectorDL&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LanguageDetectorDL</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
            <span class="n">thresholdLabel</span><span class="o">=</span><span class="s2">&quot;Unknown&quot;</span><span class="p">,</span>
            <span class="n">coalesceSentences</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;threshold&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;The minimum threshold for the final result otheriwse it will be either neutral or the value set in thresholdLabel.&quot;</span><span class="p">,</span>
                      <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>
    <span class="n">thresholdLabel</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;thresholdLabel&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;In case the score is less than threshold, what should be the label. Default is neutral.&quot;</span><span class="p">,</span>
                           <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>
    <span class="n">coalesceSentences</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;coalesceSentences&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;If sets to true the output of all sentences will be averaged to one output instead of one output per sentence. Default to false.&quot;</span><span class="p">,</span>
                              <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>
    <span class="n">languages</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;languages&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;get the languages used to trained the model&quot;</span><span class="p">,</span>
                      <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="LanguageDetectorDL.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LanguageDetectorDL.html#sparknlp.annotator.LanguageDetectorDL.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="LanguageDetectorDL.setThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LanguageDetectorDL.html#sparknlp.annotator.LanguageDetectorDL.setThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the minimum threshold for the final result otherwise it will be</span>
<span class="sd">        either neutral or the value set in thresholdLabel, by default 0.5.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Minimum threshold for the final result</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="LanguageDetectorDL.setThresholdLabel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LanguageDetectorDL.html#sparknlp.annotator.LanguageDetectorDL.setThresholdLabel">[docs]</a>    <span class="k">def</span> <span class="nf">setThresholdLabel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets what should be the label in case the score is less than</span>
<span class="sd">        threshold, by default Unknown.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        p : str</span>
<span class="sd">            The replacement label.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">thresholdLabel</span><span class="o">=</span><span class="n">p</span><span class="p">)</span></div>

<div class="viewcode-block" id="LanguageDetectorDL.setCoalesceSentences"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LanguageDetectorDL.html#sparknlp.annotator.LanguageDetectorDL.setCoalesceSentences">[docs]</a>    <span class="k">def</span> <span class="nf">setCoalesceSentences</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets if the output of all sentences will be averaged to one output</span>
<span class="sd">        instead of one output per sentence, by default True.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            If the output of all sentences will be averaged to one output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">coalesceSentences</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="LanguageDetectorDL.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LanguageDetectorDL.html#sparknlp.annotator.LanguageDetectorDL.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;ld_wiki_tatoeba_cnn_21&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;xx&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;ld_wiki_tatoeba_cnn_21&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;xx&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        LanguageDetectorDL</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">LanguageDetectorDL</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="MultiClassifierDLApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLApproach.html#sparknlp.annotator.MultiClassifierDLApproach">[docs]</a><span class="k">class</span> <span class="nc">MultiClassifierDLApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains a MultiClassifierDL for Multi-label Text Classification.</span>

<span class="sd">    MultiClassifierDL uses a Bidirectional GRU with a convolutional model that</span>
<span class="sd">    we have built inside TensorFlow and supports up to 100 classes.</span>

<span class="sd">    In machine learning, multi-label classification and the strongly related</span>
<span class="sd">    problem of multi-output classification are variants of the classification</span>
<span class="sd">    problem where multiple labels may be assigned to each instance. Multi-label</span>
<span class="sd">    classification is a generalization of multiclass classification, which is</span>
<span class="sd">    the single-label problem of categorizing instances into precisely one of</span>
<span class="sd">    more than two classes; in the multi-label problem there is no constraint on</span>
<span class="sd">    how many of the classes the instance can be assigned to. Formally,</span>
<span class="sd">    multi-label classification is the problem of finding a model that maps</span>
<span class="sd">    inputs x to binary vectors y (assigning a value of 0 or 1 for each element</span>
<span class="sd">    (label) in y).</span>

<span class="sd">    For instantiated/pretrained models, see :class:`.MultiClassifierDLModel`.</span>

<span class="sd">    The input to `MultiClassifierDL` are Sentence Embeddings such as the</span>
<span class="sd">    state-of-the-art :class:`.UniversalSentenceEncoder`,</span>
<span class="sd">    :class:`.BertSentenceEmbeddings`, :class:`.SentenceEmbeddings` or other</span>
<span class="sd">    sentence embeddings.</span>


<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/classification/MultiClassifierDL_train_multi_label_E2E_challenge_classifier.ipynb&gt;`__.</span>

<span class="sd">    ======================= ======================</span>
<span class="sd">    Input Annotation types  Output Annotation type</span>
<span class="sd">    ======================= ======================</span>
<span class="sd">    ``SENTENCE_EMBEDDINGS`` ``CATEGORY``</span>
<span class="sd">    ======================= ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lr</span>
<span class="sd">        Learning Rate, by default 0.001</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size, by default 64</span>
<span class="sd">    maxEpochs</span>
<span class="sd">        Maximum number of epochs to train, by default 10</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    validationSplit</span>
<span class="sd">        Choose the proportion of training dataset to be validated against the</span>
<span class="sd">        model on each Epoch. The value should be between 0.0 and 1.0 and by</span>
<span class="sd">        default it is 0.0 and off, by default 0.0</span>
<span class="sd">    enableOutputLogs</span>
<span class="sd">        Whether to use stdout in addition to Spark logs, by default False</span>
<span class="sd">    outputLogsPath</span>
<span class="sd">        Folder path to save training logs</span>
<span class="sd">    labelColumn</span>
<span class="sd">        Column with label per each token</span>
<span class="sd">    verbose</span>
<span class="sd">        Level of verbosity during training</span>
<span class="sd">    randomSeed</span>
<span class="sd">        Random seed, by default 44</span>
<span class="sd">    shufflePerEpoch</span>
<span class="sd">        whether to shuffle the training data on each Epoch, by default False</span>
<span class="sd">    threshold</span>
<span class="sd">        The minimum threshold for each label to be accepted, by default 0.5</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    - This annotator requires an array of labels in type of String.</span>
<span class="sd">    - UniversalSentenceEncoder, BertSentenceEmbeddings, SentenceEmbeddings or</span>
<span class="sd">      other sentence embeddings can be used for the ``inputCol``.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    In this example, the training data has the form::</span>

<span class="sd">        +----------------+--------------------+--------------------+</span>
<span class="sd">        |              id|                text|              labels|</span>
<span class="sd">        +----------------+--------------------+--------------------+</span>
<span class="sd">        |ed58abb40640f983|PN NewsYou mean ... |             [toxic]|</span>
<span class="sd">        |a1237f726b5f5d89|Dude.  Place the ...|   [obscene, insult]|</span>
<span class="sd">        |24b0d6c8733c2abe|Thanks  - thanks ...|            [insult]|</span>
<span class="sd">        |8c4478fb239bcfc0|&quot; Gee, 5 minutes ...|[toxic, obscene, ...|</span>
<span class="sd">        +----------------+--------------------+--------------------+</span>

<span class="sd">    Process training data to create text with associated array of labels:</span>

<span class="sd">    &gt;&gt;&gt; trainDataset.printSchema()</span>
<span class="sd">    root</span>
<span class="sd">    |-- id: string (nullable = true)</span>
<span class="sd">    |-- text: string (nullable = true)</span>
<span class="sd">    |-- labels: array (nullable = true)</span>
<span class="sd">    |    |-- element: string (containsNull = true)</span>

<span class="sd">    Then create pipeline for training:</span>

<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setCleanupMode(&quot;shrink&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = UniversalSentenceEncoder.pretrained() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; docClassifier = MultiClassifierDLApproach() \\</span>
<span class="sd">    ...     .setInputCols(&quot;embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;category&quot;) \\</span>
<span class="sd">    ...     .setLabelColumn(&quot;labels&quot;) \\</span>
<span class="sd">    ...     .setBatchSize(128) \\</span>
<span class="sd">    ...     .setMaxEpochs(10) \\</span>
<span class="sd">    ...     .setLr(1e-3) \\</span>
<span class="sd">    ...     .setThreshold(0.5) \\</span>
<span class="sd">    ...     .setValidationSplit(0.1)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     docClassifier</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(trainDataset)</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    ClassifierDLApproach : for single-class classification</span>
<span class="sd">    SentimentDLApproach : for sentiment analysis</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">lr</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;lr&quot;</span><span class="p">,</span> <span class="s2">&quot;Learning Rate&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">batchSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;batchSize&quot;</span><span class="p">,</span> <span class="s2">&quot;Batch size&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">maxEpochs</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;maxEpochs&quot;</span><span class="p">,</span> <span class="s2">&quot;Maximum number of epochs to train&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">validationSplit</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;validationSplit&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Choose the proportion of training dataset to be validated against the model on each Epoch. The value should be between 0.0 and 1.0 and by default it is 0.0 and off.&quot;</span><span class="p">,</span>
                            <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">enableOutputLogs</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;enableOutputLogs&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;Whether to use stdout in addition to Spark logs.&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">outputLogsPath</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;outputLogsPath&quot;</span><span class="p">,</span> <span class="s2">&quot;Folder path to save training logs&quot;</span><span class="p">,</span>
                           <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">labelColumn</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;labelColumn&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;Column with label per each token&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">verbose</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;verbose&quot;</span><span class="p">,</span> <span class="s2">&quot;Level of verbosity during training&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">randomSeed</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;randomSeed&quot;</span><span class="p">,</span> <span class="s2">&quot;Random seed&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">shufflePerEpoch</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;shufflePerEpoch&quot;</span><span class="p">,</span> <span class="s2">&quot;whether to shuffle the training data on each Epoch&quot;</span><span class="p">,</span>
                            <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;threshold&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;The minimum threshold for each label to be accepted. Default is 0.5&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

<div class="viewcode-block" id="MultiClassifierDLApproach.setVerbose"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLApproach.html#sparknlp.annotator.MultiClassifierDLApproach.setVerbose">[docs]</a>    <span class="k">def</span> <span class="nf">setVerbose</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets level of verbosity during training.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : int</span>
<span class="sd">            Level of verbosity</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">verbose</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiClassifierDLApproach.setRandomSeed"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLApproach.html#sparknlp.annotator.MultiClassifierDLApproach.setRandomSeed">[docs]</a>    <span class="k">def</span> <span class="nf">setRandomSeed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets random seed for shuffling.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        seed : int</span>
<span class="sd">            Random seed for shuffling</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">randomSeed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiClassifierDLApproach.setLabelColumn"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLApproach.html#sparknlp.annotator.MultiClassifierDLApproach.setLabelColumn">[docs]</a>    <span class="k">def</span> <span class="nf">setLabelColumn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets name of column for data labels.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : str</span>
<span class="sd">            Column for data labels</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">labelColumn</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiClassifierDLApproach.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLApproach.html#sparknlp.annotator.MultiClassifierDLApproach.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiClassifierDLApproach.setLr"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLApproach.html#sparknlp.annotator.MultiClassifierDLApproach.setLr">[docs]</a>    <span class="k">def</span> <span class="nf">setLr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets Learning Rate, by default 0.001.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Learning Rate</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="MultiClassifierDLApproach.setBatchSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLApproach.html#sparknlp.annotator.MultiClassifierDLApproach.setBatchSize">[docs]</a>    <span class="k">def</span> <span class="nf">setBatchSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets batch size, by default 64.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : int</span>
<span class="sd">            Batch size</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">batchSize</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="MultiClassifierDLApproach.setMaxEpochs"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLApproach.html#sparknlp.annotator.MultiClassifierDLApproach.setMaxEpochs">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxEpochs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets maximum number of epochs to train, by default 10.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : int</span>
<span class="sd">            Maximum number of epochs to train</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxEpochs</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">ClassifierDLModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span>

<div class="viewcode-block" id="MultiClassifierDLApproach.setValidationSplit"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLApproach.html#sparknlp.annotator.MultiClassifierDLApproach.setValidationSplit">[docs]</a>    <span class="k">def</span> <span class="nf">setValidationSplit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the proportion of training dataset to be validated against the</span>
<span class="sd">        model on each Epoch, by default it is 0.0 and off. The value should be</span>
<span class="sd">        between 0.0 and 1.0.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            Proportion of training dataset to be validated</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">validationSplit</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="MultiClassifierDLApproach.setEnableOutputLogs"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLApproach.html#sparknlp.annotator.MultiClassifierDLApproach.setEnableOutputLogs">[docs]</a>    <span class="k">def</span> <span class="nf">setEnableOutputLogs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to use stdout in addition to Spark logs, by default</span>
<span class="sd">        False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : bool</span>
<span class="sd">            Whether to use stdout in addition to Spark logs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">enableOutputLogs</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiClassifierDLApproach.setOutputLogsPath"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLApproach.html#sparknlp.annotator.MultiClassifierDLApproach.setOutputLogsPath">[docs]</a>    <span class="k">def</span> <span class="nf">setOutputLogsPath</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets folder path to save training logs.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : str</span>
<span class="sd">            Folder path to save training logs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">outputLogsPath</span><span class="o">=</span><span class="n">v</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">setShufflePerEpoch</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">shufflePerEpoch</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>

<div class="viewcode-block" id="MultiClassifierDLApproach.setThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLApproach.html#sparknlp.annotator.MultiClassifierDLApproach.setThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets minimum threshold for each label to be accepted, by default 0.5.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            The minimum threshold for each label to be accepted, by default 0.5</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiClassifierDLApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.MultiClassifierDLApproach&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">maxEpochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
            <span class="n">lr</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.001</span><span class="p">),</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
            <span class="n">validationSplit</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span>
            <span class="n">threshold</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.5</span><span class="p">),</span>
            <span class="n">randomSeed</span><span class="o">=</span><span class="mi">44</span><span class="p">,</span>
            <span class="n">shufflePerEpoch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">enableOutputLogs</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="MultiClassifierDLModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLModel.html#sparknlp.annotator.MultiClassifierDLModel">[docs]</a><span class="k">class</span> <span class="nc">MultiClassifierDLModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasStorageRef</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;MultiClassifierDL for Multi-label Text Classification.</span>

<span class="sd">    MultiClassifierDL Bidirectional GRU with Convolution model we have built</span>
<span class="sd">    inside TensorFlow and supports up to 100 classes.</span>

<span class="sd">    In machine learning, multi-label classification and the strongly related</span>
<span class="sd">    problem of multi-output classification are variants of the classification</span>
<span class="sd">    problem where multiple labels may be assigned to each instance. Multi-label</span>
<span class="sd">    classification is a generalization of multiclass classification, which is</span>
<span class="sd">    the single-label problem of categorizing instances into precisely one of</span>
<span class="sd">    more than two classes; in the multi-label problem there is no constraint on</span>
<span class="sd">    how many of the classes the instance can be assigned to. Formally,</span>
<span class="sd">    multi-label classification is the problem of finding a model that maps</span>
<span class="sd">    inputs x to binary vectors y (assigning a value of 0 or 1 for each element</span>
<span class="sd">    (label) in y).</span>

<span class="sd">    The input to ``MultiClassifierDL`` are Sentence Embeddings such as the</span>
<span class="sd">    state-of-the-art :class:`.UniversalSentenceEncoder`,</span>
<span class="sd">    :class:`.BertSentenceEmbeddings`, :class:`.SentenceEmbeddings` or other</span>
<span class="sd">    sentence embeddings.</span>

<span class="sd">    This is the instantiated model of the :class:`.MultiClassifierDLApproach`.</span>
<span class="sd">    For training your own model, please see the documentation of that class.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; multiClassifier = MultiClassifierDLModel.pretrained() \\</span>
<span class="sd">    &gt;&gt;&gt;     .setInputCols([&quot;sentence_embeddings&quot;]) \\</span>
<span class="sd">    &gt;&gt;&gt;     .setOutputCol(&quot;categories&quot;)</span>

<span class="sd">    The default model is ``&quot;multiclassifierdl_use_toxic&quot;``, if no name is</span>
<span class="sd">    provided. It uses embeddings from the UniversalSentenceEncoder and</span>
<span class="sd">    classifies toxic comments.</span>

<span class="sd">    The data is based on the</span>
<span class="sd">    `Jigsaw Toxic Comment Classification Challenge &lt;https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview&gt;`__.</span>
<span class="sd">    For available pretrained models please see the `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Text+Classification&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/classification/MultiClassifierDL_train_multi_label_E2E_challenge_classifier.ipynb&gt;`__.</span>

<span class="sd">    ======================= ======================</span>
<span class="sd">    Input Annotation types  Output Annotation type</span>
<span class="sd">    ======================= ======================</span>
<span class="sd">    ``SENTENCE_EMBEDDINGS`` ``CATEGORY``</span>
<span class="sd">    ======================= ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    threshold</span>
<span class="sd">        The minimum threshold for each label to be accepted, by default 0.5</span>
<span class="sd">    classes</span>
<span class="sd">        Get the tags used to trained this MultiClassifierDLModel</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; useEmbeddings = UniversalSentenceEncoder.pretrained() \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence_embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; multiClassifierDl = MultiClassifierDLModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols(&quot;sentence_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;classifications&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...         documentAssembler,</span>
<span class="sd">    ...         useEmbeddings,</span>
<span class="sd">    ...         multiClassifierDl</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([</span>
<span class="sd">    ...     [&quot;This is pretty good stuff!&quot;],</span>
<span class="sd">    ...     [&quot;Wtf kind of crap is this&quot;]</span>
<span class="sd">    ... ]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;text&quot;, &quot;classifications.result&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------------+----------------+</span>
<span class="sd">    |text                      |result          |</span>
<span class="sd">    +--------------------------+----------------+</span>
<span class="sd">    |This is pretty good stuff!|[]              |</span>
<span class="sd">    |Wtf kind of crap is this  |[toxic, obscene]|</span>
<span class="sd">    +--------------------------+----------------+</span>
<span class="sd">    </span>
<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    ClassifierDLModel : for single-class classification</span>
<span class="sd">    SentimentDLModel : for sentiment analysis</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;MultiClassifierDLModel&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.MultiClassifierDLModel&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiClassifierDLModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">threshold</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;threshold&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;The minimum threshold for each label to be accepted. Default is 0.5&quot;</span><span class="p">,</span> <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;classes&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;get the tags used to trained this MultiClassifierDLModel&quot;</span><span class="p">,</span>
                    <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="MultiClassifierDLModel.setThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLModel.html#sparknlp.annotator.MultiClassifierDLModel.setThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets minimum threshold for each label to be accepted, by default 0.5.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        v : float</span>
<span class="sd">            The minimum threshold for each label to be accepted, by default 0.5</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>

<div class="viewcode-block" id="MultiClassifierDLModel.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLModel.html#sparknlp.annotator.MultiClassifierDLModel.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="MultiClassifierDLModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MultiClassifierDLModel.html#sparknlp.annotator.MultiClassifierDLModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;multiclassifierdl_use_toxic&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default</span>
<span class="sd">            &quot;multiclassifierdl_use_toxic&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        MultiClassifierDLModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">MultiClassifierDLModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="YakeKeywordExtraction"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.YakeKeywordExtraction.html#sparknlp.annotator.YakeKeywordExtraction">[docs]</a><span class="k">class</span> <span class="nc">YakeKeywordExtraction</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Yake is an Unsupervised, Corpus-Independent, Domain and</span>
<span class="sd">    Language-Independent and Single-Document keyword extraction algorithm.</span>

<span class="sd">    Extracting keywords from texts has become a challenge for individuals and</span>
<span class="sd">    organizations as the information grows in complexity and size. The need to</span>
<span class="sd">    automate this task so that text can be processed in a timely and adequate</span>
<span class="sd">    manner has led to the emergence of automatic keyword extraction tools. Yake</span>
<span class="sd">    is a novel feature-based system for multi-lingual keyword extraction, which</span>
<span class="sd">    supports texts of different sizes, domain or languages. Unlike other</span>
<span class="sd">    approaches, Yake does not rely on dictionaries nor thesauri, neither is</span>
<span class="sd">    trained against any corpora. Instead, it follows an unsupervised approach</span>
<span class="sd">    which builds upon features extracted from the text, making it thus</span>
<span class="sd">    applicable to documents written in different languages without the need for</span>
<span class="sd">    further knowledge. This can be beneficial for a large number of tasks and a</span>
<span class="sd">    plethora of situations where access to training corpora is either limited or</span>
<span class="sd">    restricted. The algorithm makes use of the position of a sentence and token.</span>
<span class="sd">    Therefore, to use the annotator, the text should be first sent through a</span>
<span class="sd">    Sentence Boundary Detector and then a tokenizer.</span>

<span class="sd">    See the parameters section for tweakable parameters to get the best result</span>
<span class="sd">    from the annotator.</span>

<span class="sd">    Note that each keyword will be given a keyword score greater than 0 (The</span>
<span class="sd">    lower the score better the keyword). Therefore to filter the keywords, an</span>
<span class="sd">    upper bound for the score can be set with :meth:`.setThreshold`.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/8.Keyword_Extraction_YAKE.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``TOKEN``              ``CHUNK``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    minNGrams</span>
<span class="sd">        Minimum N-grams a keyword should have, by default 2</span>
<span class="sd">    maxNGrams</span>
<span class="sd">        Maximum N-grams a keyword should have, by default 3</span>
<span class="sd">    threshold</span>
<span class="sd">        Keyword Score threshold, by default -1</span>
<span class="sd">    windowSize</span>
<span class="sd">        Window size for Co-Occurrence, by default 3</span>
<span class="sd">    nKeywords</span>
<span class="sd">        Number of Keywords to extract, by default 30</span>
<span class="sd">    stopWords</span>
<span class="sd">        the words to be filtered out, by default english stop words from Spark</span>
<span class="sd">        ML</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `Campos, R., Mangaravite, V., Pasquali, A., Jatowt, A., Jorge, A., Nunes, C.</span>
<span class="sd">    and Jatowt, A. (2020). YAKE! Keyword Extraction from Single Documents using</span>
<span class="sd">    Multiple Local Features. In Information Sciences Journal. Elsevier, Vol 509,</span>
<span class="sd">    pp 257-289</span>
<span class="sd">    &lt;https://www.sciencedirect.com/science/article/pii/S0020025519308588&gt;`__</span>

<span class="sd">    **Paper abstract:**</span>

<span class="sd">    *As the amount of generated information grows, reading and summarizing texts</span>
<span class="sd">    of large collections turns into a challenging task. Many documents do not</span>
<span class="sd">    come with descriptive terms, thus requiring humans to generate keywords</span>
<span class="sd">    on-the-fly. The need to automate this kind of task demands the development</span>
<span class="sd">    of keyword extraction systems with the ability to automatically identify</span>
<span class="sd">    keywords within the text. One approach is to resort to machine-learning</span>
<span class="sd">    algorithms. These, however, depend on large annotated text corpora, which</span>
<span class="sd">    are not always available. An alternative solution is to consider an</span>
<span class="sd">    unsupervised approach. In this article, we describe YAKE!, a light-weight</span>
<span class="sd">    unsupervised automatic keyword extraction method which rests on statistical</span>
<span class="sd">    text features extracted from single documents to select the most relevant</span>
<span class="sd">    keywords of a text. Our system does not need to be trained on a particular</span>
<span class="sd">    set of documents, nor does it depend on dictionaries, external corpora, text</span>
<span class="sd">    size, language, or domain. To demonstrate the merits and significance of</span>
<span class="sd">    YAKE!, we compare it against ten state-of-the-art unsupervised approaches</span>
<span class="sd">    and one supervised method. Experimental results carried out on top of twenty</span>
<span class="sd">    datasets show that YAKE! significantly outperforms other unsupervised</span>
<span class="sd">    methods on texts of different sizes, languages, and domains.*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentenceDetector = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; token = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;) \\</span>
<span class="sd">    ...     .setContextChars([&quot;(&quot;, &quot;]&quot;, &quot;?&quot;, &quot;!&quot;, &quot;.&quot;, &quot;,&quot;])</span>
<span class="sd">    &gt;&gt;&gt; keywords = YakeKeywordExtraction() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;keywords&quot;) \\</span>
<span class="sd">    ...     .setThreshold(0.6) \\</span>
<span class="sd">    ...     .setMinNGrams(2) \\</span>
<span class="sd">    ...     .setNKeywords(10)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentenceDetector,</span>
<span class="sd">    ...     token,</span>
<span class="sd">    ...     keywords</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[</span>
<span class="sd">    ...     &quot;Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud Next conference in San Francisco this week, the official announcement could come as early as tomorrow. Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. Google itself declined &#39;to comment on rumors&#39;. Kaggle, which has about half a million data scientists on its platform, was founded by Goldbloom  and Ben Hamner in 2010. The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, it has managed to stay well ahead of them by focusing on its specific niche. The service is basically the de facto home for running data science and machine learning competitions. With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow and other projects). Kaggle has a bit of a history with Google, too, but that&#39;s pretty recent. Earlier this month, Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google will keep the service running - likely under its current name. While the acquisition is probably more about Kaggle&#39;s community than technology, Kaggle did build some interesting tools for hosting its competition and &#39;kernels&#39;, too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can share this code on the platform (the company previously called them &#39;scripts&#39;). Like similar competition-centric sites, Kaggle also runs a job board, too. It&#39;s unclear what Google will do with that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it&#39;s $12.75) since its   launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, NaRavikant, Google chie economist Hal Varian, Khosla Ventures and Yuri Milner&quot;</span>
<span class="sd">    ... ]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>

<span class="sd">    Combine the result and score (contained in keywords.metadata)</span>

<span class="sd">    &gt;&gt;&gt; scores = result \\</span>
<span class="sd">    ...     .selectExpr(&quot;explode(arrays_zip(keywords.result, keywords.metadata)) as resultTuples&quot;) \\</span>
<span class="sd">    ...     .selectExpr(&quot;resultTuples[&#39;0&#39;] as keyword&quot;, &quot;resultTuples[&#39;1&#39;].score as score&quot;)</span>

<span class="sd">    Order ascending, as lower scores means higher importance</span>

<span class="sd">    &gt;&gt;&gt; scores.orderBy(&quot;score&quot;).show(5, truncate = False)</span>
<span class="sd">    +---------------------+-------------------+</span>
<span class="sd">    |keyword              |score              |</span>
<span class="sd">    +---------------------+-------------------+</span>
<span class="sd">    |google cloud         |0.32051516486864573|</span>
<span class="sd">    |google cloud platform|0.37786450577630676|</span>
<span class="sd">    |ceo anthony goldbloom|0.39922830978423146|</span>
<span class="sd">    |san francisco        |0.40224744669493756|</span>
<span class="sd">    |anthony goldbloom    |0.41584827825302534|</span>
<span class="sd">    +---------------------+-------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;YakeKeywordExtraction&quot;</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">YakeKeywordExtraction</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.keyword.yake.YakeKeywordExtraction&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">minNGrams</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">maxNGrams</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">nKeywords</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
            <span class="n">windowSize</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
            <span class="n">threshold</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">stopWords</span><span class="o">=</span><span class="n">YakeKeywordExtraction</span><span class="o">.</span><span class="n">loadDefaultStopWords</span><span class="p">(</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="n">minNGrams</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;minNGrams&quot;</span><span class="p">,</span> <span class="s2">&quot;Minimum N-grams a keyword should have&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">maxNGrams</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;maxNGrams&quot;</span><span class="p">,</span> <span class="s2">&quot;Maximum N-grams a keyword should have&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">threshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;threshold&quot;</span><span class="p">,</span> <span class="s2">&quot;Keyword Score threshold&quot;</span><span class="p">,</span> <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>
    <span class="n">windowSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;windowSize&quot;</span><span class="p">,</span> <span class="s2">&quot;Window size for Co-Occurrence&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">nKeywords</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;nKeywords&quot;</span><span class="p">,</span> <span class="s2">&quot;Number of Keywords to extract&quot;</span><span class="p">,</span> <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">stopWords</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;stopWords&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;the words to be filtered out. by default it&#39;s english stop words from Spark ML&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="YakeKeywordExtraction.setWindowSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.YakeKeywordExtraction.html#sparknlp.annotator.YakeKeywordExtraction.setWindowSize">[docs]</a>    <span class="k">def</span> <span class="nf">setWindowSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets window size for Co-Occurrence, by default 3.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Window size for Co-Occurrence</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">windowSize</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="YakeKeywordExtraction.setMinNGrams"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.YakeKeywordExtraction.html#sparknlp.annotator.YakeKeywordExtraction.setMinNGrams">[docs]</a>    <span class="k">def</span> <span class="nf">setMinNGrams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets minimum N-grams a keyword should have, by default 2.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Minimum N-grams a keyword should have</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">minNGrams</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="YakeKeywordExtraction.setMaxNGrams"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.YakeKeywordExtraction.html#sparknlp.annotator.YakeKeywordExtraction.setMaxNGrams">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxNGrams</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets maximum N-grams a keyword should have, by default 3.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Maximum N-grams a keyword should have</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxNGrams</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="YakeKeywordExtraction.setThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.YakeKeywordExtraction.html#sparknlp.annotator.YakeKeywordExtraction.setThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets keyword Score threshold, by default -1.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Keyword Score threshold, by default -1</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">threshold</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="YakeKeywordExtraction.setNKeywords"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.YakeKeywordExtraction.html#sparknlp.annotator.YakeKeywordExtraction.setNKeywords">[docs]</a>    <span class="k">def</span> <span class="nf">setNKeywords</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets number of Keywords to extract, by default 30.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Number of Keywords to extract</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">nKeywords</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="YakeKeywordExtraction.setStopWords"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.YakeKeywordExtraction.html#sparknlp.annotator.YakeKeywordExtraction.setStopWords">[docs]</a>    <span class="k">def</span> <span class="nf">setStopWords</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the words to be filtered out, by default english stop words from</span>
<span class="sd">        Spark ML.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            The words to be filtered out</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">stopWords</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="YakeKeywordExtraction.getStopWords"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.YakeKeywordExtraction.html#sparknlp.annotator.YakeKeywordExtraction.getStopWords">[docs]</a>    <span class="k">def</span> <span class="nf">getStopWords</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets the words to be filtered out, by default english stop words from</span>
<span class="sd">        Spark ML.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        List[str]</span>
<span class="sd">            The words to be filtered out</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stopWords</span><span class="p">)</span></div>

<div class="viewcode-block" id="YakeKeywordExtraction.loadDefaultStopWords"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.YakeKeywordExtraction.html#sparknlp.annotator.YakeKeywordExtraction.loadDefaultStopWords">[docs]</a>    <span class="k">def</span> <span class="nf">loadDefaultStopWords</span><span class="p">(</span><span class="n">language</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads the default stop words for the given language.</span>

<span class="sd">        Supported languages: danish, dutch, english, finnish, french, german,</span>
<span class="sd">        hungarian, italian, norwegian, portuguese, russian, spanish, swedish,</span>
<span class="sd">        turkish</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        language : str, optional</span>
<span class="sd">            Language stopwords to load, by default &quot;english&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">pyspark.ml.wrapper</span> <span class="kn">import</span> <span class="n">_jvm</span>
        <span class="n">stopWordsObj</span> <span class="o">=</span> <span class="n">_jvm</span><span class="p">()</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">ml</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">StopWordsRemover</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">stopWordsObj</span><span class="o">.</span><span class="n">loadDefaultStopWords</span><span class="p">(</span><span class="n">language</span><span class="p">))</span></div></div>


<div class="viewcode-block" id="SentenceDetectorDLModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetectorDLModel.html#sparknlp.annotator.SentenceDetectorDLModel">[docs]</a><span class="k">class</span> <span class="nc">SentenceDetectorDLModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Annotator that detects sentence boundaries using a deep learning approach.</span>

<span class="sd">    Instantiated Model of the :class:`.SentenceDetectorDLApproach`.</span>
<span class="sd">    Detects sentence boundaries using a deep learning approach.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; sentenceDL = SentenceDetectorDLModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentencesDL&quot;)</span>

<span class="sd">    The default model is ``&quot;sentence_detector_dl&quot;``, if no name is provided. For</span>
<span class="sd">    available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Sentence+Detection&gt;`__.</span>

<span class="sd">    Each extracted sentence can be returned in an Array or exploded to separate</span>
<span class="sd">    rows, if ``explodeSentences`` is set to ``true``.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``DOCUMENT``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    modelArchitecture</span>
<span class="sd">        Model architecture (CNN)</span>
<span class="sd">    explodeSentences</span>
<span class="sd">        whether to explode each sentence into a different row, for better</span>
<span class="sd">        parallelization. Defaults to false.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    In this example, the normal `SentenceDetector` is compared to the</span>
<span class="sd">    `SentenceDetectorDLModel`. In a pipeline, `SentenceDetectorDLModel` can be</span>
<span class="sd">    used as a replacement for the `SentenceDetector`.</span>

<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentences&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentenceDL = SentenceDetectorDLModel \\</span>
<span class="sd">    ...     .pretrained(&quot;sentence_detector_dl&quot;, &quot;en&quot;) \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentencesDL&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     sentenceDL</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[\&quot;\&quot;\&quot;John loves Mary.Mary loves Peter</span>
<span class="sd">    ...     Peter loves Helen .Helen loves John;</span>
<span class="sd">    ...     Total: four people involved.\&quot;\&quot;\&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(sentences.result) as sentences&quot;).show(truncate=False)</span>
<span class="sd">    +----------------------------------------------------------+</span>
<span class="sd">    |sentences                                                 |</span>
<span class="sd">    +----------------------------------------------------------+</span>
<span class="sd">    |John loves Mary.Mary loves Peter\\n     Peter loves Helen .|</span>
<span class="sd">    |Helen loves John;                                         |</span>
<span class="sd">    |Total: four people involved.                              |</span>
<span class="sd">    +----------------------------------------------------------+</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(sentencesDL.result) as sentencesDL&quot;).show(truncate=False)</span>
<span class="sd">    +----------------------------+</span>
<span class="sd">    |sentencesDL                 |</span>
<span class="sd">    +----------------------------+</span>
<span class="sd">    |John loves Mary.            |</span>
<span class="sd">    |Mary loves Peter            |</span>
<span class="sd">    |Peter loves Helen .         |</span>
<span class="sd">    |Helen loves John;           |</span>
<span class="sd">    |Total: four people involved.|</span>
<span class="sd">    +----------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;SentenceDetectorDLModel&quot;</span>

    <span class="n">modelArchitecture</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;modelArchitecture&quot;</span><span class="p">,</span> <span class="s2">&quot;Model architecture (CNN)&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">explodeSentences</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;explodeSentences&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;whether to explode each sentence into a different row, for better parallelization. Defaults to false.&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

<div class="viewcode-block" id="SentenceDetectorDLModel.setModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetectorDLModel.html#sparknlp.annotator.SentenceDetectorDLModel.setModel">[docs]</a>    <span class="k">def</span> <span class="nf">setModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modelArchitecture</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the Model architecture. Currently only ``&quot;cnn&quot;`` is available.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model_architecture : str</span>
<span class="sd">            Model architecture</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">modelArchitecture</span><span class="o">=</span><span class="n">modelArchitecture</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetectorDLModel.setExplodeSentences"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetectorDLModel.html#sparknlp.annotator.SentenceDetectorDLModel.setExplodeSentences">[docs]</a>    <span class="k">def</span> <span class="nf">setExplodeSentences</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to explode each sentence into a different row, for</span>
<span class="sd">        better parallelization, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to explode each sentence into a different row</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">explodeSentences</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLModel&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SentenceDetectorDLModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="SentenceDetectorDLModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetectorDLModel.html#sparknlp.annotator.SentenceDetectorDLModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;sentence_detector_dl&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;sentence_detector_dl&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        SentenceDetectorDLModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">SentenceDetectorDLModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="SentenceDetectorDLApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetectorDLApproach.html#sparknlp.annotator.SentenceDetectorDLApproach">[docs]</a><span class="k">class</span> <span class="nc">SentenceDetectorDLApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains an annotator that detects sentence boundaries using a deep</span>
<span class="sd">    learning approach.</span>

<span class="sd">    Currently, only the CNN model is supported for training, but in the future</span>
<span class="sd">    the architecture of the model can be set with :meth:`.setModel`.</span>

<span class="sd">    For pretrained models see :class:`.SentenceDetectorDLModel`.</span>

<span class="sd">    Each extracted sentence can be returned in an Array or exploded to separate</span>
<span class="sd">    rows, if ``explodeSentences`` is set to ``True``.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``DOCUMENT``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    modelArchitecture</span>
<span class="sd">        Model architecture (CNN)</span>
<span class="sd">    impossiblePenultimates</span>
<span class="sd">        Impossible penultimates - list of strings which a sentence can&#39;t end</span>
<span class="sd">        with</span>
<span class="sd">    validationSplit</span>
<span class="sd">        Choose the proportion of training dataset to be validated against the</span>
<span class="sd">        model on each</span>
<span class="sd">    epochsNumber</span>
<span class="sd">        Number of epochs for the optimization process</span>
<span class="sd">    outputLogsPath</span>
<span class="sd">        Path to folder where logs will be saved. If no path is specified, no</span>
<span class="sd">        logs are generated</span>
<span class="sd">    explodeSentences</span>
<span class="sd">        Whether to explode each sentence into a different row, for better</span>
<span class="sd">        parallelization. Defaults to False.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    The default model ``&quot;cnn&quot;`` is based on the paper `Deep-EOS: General-Purpose</span>
<span class="sd">    Neural Networks for Sentence Boundary Detection (2020, Stefan Schweter,</span>
<span class="sd">    Sajawel Ahmed)</span>
<span class="sd">    &lt;https://konvens.org/proceedings/2019/papers/KONVENS2019_paper_41.pdf&gt;`__</span>
<span class="sd">    using a CNN architecture. We also modified the original implementation a</span>
<span class="sd">    little bit to cover broken sentences and some impossible end of line chars.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    The training process needs data, where each data point is a sentence.</span>
<span class="sd">    In this example the ``train.txt`` file has the form of::</span>

<span class="sd">        ...</span>
<span class="sd">        Slightly more moderate language would make our present situation  namely the lack of progress  a little easier.</span>
<span class="sd">        His political successors now have great responsibilities to history and to the heritage of values bequeathed to them by Nelson Mandela.</span>
<span class="sd">        ...</span>

<span class="sd">    where each line is one sentence.</span>

<span class="sd">    Training can then be started like so:</span>

<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; trainingData = spark.read.text(&quot;train.txt&quot;).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentenceDetector = SentenceDetectorDLApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentences&quot;) \\</span>
<span class="sd">    ...     .setEpochsNumber(100)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([documentAssembler, sentenceDetector])</span>
<span class="sd">    &gt;&gt;&gt; model = pipeline.fit(trainingData)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;SentenceDetectorDLApproach&quot;</span>

    <span class="n">modelArchitecture</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;modelArchitecture&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Model architecture (CNN)&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">impossiblePenultimates</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                   <span class="s2">&quot;impossiblePenultimates&quot;</span><span class="p">,</span>
                                   <span class="s2">&quot;Impossible penultimates - list of strings which a sentence can&#39;t end with&quot;</span><span class="p">,</span>
                                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">validationSplit</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;validationSplit&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Choose the proportion of training dataset to be validated against the model on each &quot;</span>
                            <span class="s2">&quot;Epoch. The value should be between 0.0 and 1.0 and by default it is 0.0 and off.&quot;</span><span class="p">,</span>
                            <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">epochsNumber</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;epochsNumber&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;Number of epochs for the optimization process&quot;</span><span class="p">,</span>
                         <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">outputLogsPath</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                           <span class="s2">&quot;outputLogsPath&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;Path to folder where logs will be saved. If no path is specified, no logs are generated&quot;</span><span class="p">,</span>
                           <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">explodeSentences</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;explodeSentences&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;whether to explode each sentence into a different row, for better parallelization. Defaults to false.&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

<div class="viewcode-block" id="SentenceDetectorDLApproach.setModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetectorDLApproach.html#sparknlp.annotator.SentenceDetectorDLApproach.setModel">[docs]</a>    <span class="k">def</span> <span class="nf">setModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model_architecture</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the Model architecture. Currently only ``&quot;cnn&quot;`` is available.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        model_architecture : str</span>
<span class="sd">            Model architecture</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">modelArchitecture</span><span class="o">=</span><span class="n">model_architecture</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetectorDLApproach.setValidationSplit"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetectorDLApproach.html#sparknlp.annotator.SentenceDetectorDLApproach.setValidationSplit">[docs]</a>    <span class="k">def</span> <span class="nf">setValidationSplit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">validation_split</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the proportion of training dataset to be validated against the</span>
<span class="sd">        model on each Epoch, by default it is 0.0 and off. The value should be</span>
<span class="sd">        between 0.0 and 1.0.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        validation_split : float</span>
<span class="sd">            Proportion of training dataset to be validated</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">validationSplit</span><span class="o">=</span><span class="n">validation_split</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetectorDLApproach.setEpochsNumber"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetectorDLApproach.html#sparknlp.annotator.SentenceDetectorDLApproach.setEpochsNumber">[docs]</a>    <span class="k">def</span> <span class="nf">setEpochsNumber</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epochs_number</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets number of epochs to train.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        epochs_number : int</span>
<span class="sd">            Number of epochs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">epochsNumber</span><span class="o">=</span><span class="n">epochs_number</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetectorDLApproach.setOutputLogsPath"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetectorDLApproach.html#sparknlp.annotator.SentenceDetectorDLApproach.setOutputLogsPath">[docs]</a>    <span class="k">def</span> <span class="nf">setOutputLogsPath</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output_logs_path</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets folder path to save training logs.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        output_logs_path : str</span>
<span class="sd">            Folder path to save training logs</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">outputLogsPath</span><span class="o">=</span><span class="n">output_logs_path</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetectorDLApproach.setImpossiblePenultimates"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetectorDLApproach.html#sparknlp.annotator.SentenceDetectorDLApproach.setImpossiblePenultimates">[docs]</a>    <span class="k">def</span> <span class="nf">setImpossiblePenultimates</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">impossible_penultimates</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets impossible penultimates - list of strings which a sentence can&#39;t</span>
<span class="sd">        end with.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        impossible_penultimates : List[str]</span>
<span class="sd">            List of strings which a sentence can&#39;t end with</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">impossiblePenultimates</span><span class="o">=</span><span class="n">impossible_penultimates</span><span class="p">)</span></div>

<div class="viewcode-block" id="SentenceDetectorDLApproach.setExplodeSentences"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.SentenceDetectorDLApproach.html#sparknlp.annotator.SentenceDetectorDLApproach.setExplodeSentences">[docs]</a>    <span class="k">def</span> <span class="nf">setExplodeSentences</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to explode each sentence into a different row, for</span>
<span class="sd">        better parallelization, by default False.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to explode each sentence into a different row</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">explodeSentences</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">SentenceDetectorDLModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLApproach&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SentenceDetectorDLApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">)</span></div>


<div class="viewcode-block" id="WordSegmenterApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordSegmenterApproach.html#sparknlp.annotator.WordSegmenterApproach">[docs]</a><span class="k">class</span> <span class="nc">WordSegmenterApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains a WordSegmenter which tokenizes non-english or non-whitespace</span>
<span class="sd">    separated texts.</span>

<span class="sd">    Many languages are not whitespace separated and their sentences are a</span>
<span class="sd">    concatenation of many symbols, like Korean, Japanese or Chinese. Without</span>
<span class="sd">    understanding the language, splitting the words into their corresponding</span>
<span class="sd">    tokens is impossible. The WordSegmenter is trained to understand these</span>
<span class="sd">    languages and split them into semantically correct parts.</span>

<span class="sd">    For instantiated/pretrained models, see :class:`.WordSegmenterModel`.</span>

<span class="sd">    To train your own model, a training dataset consisting of `Part-Of-Speech</span>
<span class="sd">    tags &lt;https://en.wikipedia.org/wiki/Part-of-speech_tagging&gt;`__ is required.</span>
<span class="sd">    The data has to be loaded into a dataframe, where the column is an</span>
<span class="sd">    Annotation of type ``POS``. This can be set with :meth:`.setPosColumn`.</span>

<span class="sd">    **Tip**:</span>
<span class="sd">    The helper class :class:`.POS` might be useful to read training data into</span>
<span class="sd">    data frames.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/jupyter/annotation/chinese/word_segmentation&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    posCol</span>
<span class="sd">        column of Array of POS tags that match tokens</span>
<span class="sd">    nIterations</span>
<span class="sd">        Number of iterations in training, converges to better accuracy, by</span>
<span class="sd">        default 5</span>
<span class="sd">    frequencyThreshold</span>
<span class="sd">        How many times at least a tag on a word to be marked as frequent, by</span>
<span class="sd">        default 5</span>
<span class="sd">    ambiguityThreshold</span>
<span class="sd">        How much percentage of total amount of words are covered to be marked as</span>
<span class="sd">        frequent, by default 0.97</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    In this example, ``&quot;chinese_train.utf8&quot;`` is in the form of::</span>

<span class="sd">        |LL |RR |LL |RR |LL |RR</span>

<span class="sd">    and is loaded with the `POS` class to create a dataframe of ``POS`` type</span>
<span class="sd">    Annotations.</span>

<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; wordSegmenter = WordSegmenterApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;) \\</span>
<span class="sd">    ...     .setPosColumn(&quot;tags&quot;) \\</span>
<span class="sd">    ...     .setNIterations(5)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     wordSegmenter</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; trainingDataSet = POS().readDataset(</span>
<span class="sd">    ...     spark,</span>
<span class="sd">    ...     &quot;src/test/resources/word-segmenter/chinese_train.utf8&quot;</span>
<span class="sd">    ... )</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(trainingDataSet)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;WordSegmenterApproach&quot;</span>

    <span class="n">posCol</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                   <span class="s2">&quot;posCol&quot;</span><span class="p">,</span>
                   <span class="s2">&quot;column of Array of POS tags that match tokens&quot;</span><span class="p">,</span>
                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">nIterations</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;nIterations&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;Number of iterations in training, converges to better accuracy&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">frequencyThreshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                               <span class="s2">&quot;frequencyThreshold&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;How many times at least a tag on a word to be marked as frequent&quot;</span><span class="p">,</span>
                               <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">ambiguityThreshold</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                               <span class="s2">&quot;ambiguityThreshold&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;How much percentage of total amount of words are covered to be marked as frequent&quot;</span><span class="p">,</span>
                               <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WordSegmenterApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.ws.WordSegmenterApproach&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">nIterations</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">frequencyThreshold</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">ambiguityThreshold</span><span class="o">=</span><span class="mf">0.97</span>
        <span class="p">)</span>

<div class="viewcode-block" id="WordSegmenterApproach.setPosColumn"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordSegmenterApproach.html#sparknlp.annotator.WordSegmenterApproach.setPosColumn">[docs]</a>    <span class="k">def</span> <span class="nf">setPosColumn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets column name for array of POS tags that match tokens.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Name of the column</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">posCol</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="WordSegmenterApproach.setNIterations"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordSegmenterApproach.html#sparknlp.annotator.WordSegmenterApproach.setNIterations">[docs]</a>    <span class="k">def</span> <span class="nf">setNIterations</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets number of iterations in training, converges to better accuracy,</span>
<span class="sd">        by default 5.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Number of iterations</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">nIterations</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="WordSegmenterApproach.setFrequencyThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordSegmenterApproach.html#sparknlp.annotator.WordSegmenterApproach.setFrequencyThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setFrequencyThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets how many times at least a tag on a word to be marked as</span>
<span class="sd">        frequent, by default 5.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Frequency threshold to be marked as frequent</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">frequencyThreshold</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="WordSegmenterApproach.setAmbiguityThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordSegmenterApproach.html#sparknlp.annotator.WordSegmenterApproach.setAmbiguityThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">setAmbiguityThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the percentage of total amount of words are covered to be</span>
<span class="sd">        marked as frequent, by default 0.97.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : float</span>
<span class="sd">            Percentage of total amount of words are covered to be</span>
<span class="sd">            marked as frequent</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">ambiguityThreshold</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="WordSegmenterApproach.getNIterations"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordSegmenterApproach.html#sparknlp.annotator.WordSegmenterApproach.getNIterations">[docs]</a>    <span class="k">def</span> <span class="nf">getNIterations</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Gets number of iterations in training, converges to better accuracy.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Number of iterations</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">nIterations</span><span class="p">)</span></div>

<div class="viewcode-block" id="WordSegmenterApproach.getFrequencyThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordSegmenterApproach.html#sparknlp.annotator.WordSegmenterApproach.getFrequencyThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">getFrequencyThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets How many times at least a tag on a word to be marked as</span>
<span class="sd">        frequent.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        int</span>
<span class="sd">            Frequency threshold to be marked as frequent</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">frequencyThreshold</span><span class="p">)</span></div>

<div class="viewcode-block" id="WordSegmenterApproach.getAmbiguityThreshold"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordSegmenterApproach.html#sparknlp.annotator.WordSegmenterApproach.getAmbiguityThreshold">[docs]</a>    <span class="k">def</span> <span class="nf">getAmbiguityThreshold</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets How much percentage of total amount of words are covered to be</span>
<span class="sd">        marked as frequent.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        float</span>
<span class="sd">            Percentage of total amount of words are covered to be</span>
<span class="sd">            marked as frequent</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">getOrDefault</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ambiguityThreshold</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">WordSegmenterModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="WordSegmenterModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordSegmenterModel.html#sparknlp.annotator.WordSegmenterModel">[docs]</a><span class="k">class</span> <span class="nc">WordSegmenterModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;WordSegmenter which tokenizes non-english or non-whitespace separated</span>
<span class="sd">    texts.</span>

<span class="sd">    Many languages are not whitespace separated and their sentences are a</span>
<span class="sd">    concatenation of many symbols, like Korean, Japanese or Chinese. Without</span>
<span class="sd">    understanding the language, splitting the words into their corresponding</span>
<span class="sd">    tokens is impossible. The WordSegmenter is trained to understand these</span>
<span class="sd">    languages and plit them into semantically correct parts.</span>

<span class="sd">    This is the instantiated model of the :class:`.WordSegmenterApproach`. For</span>
<span class="sd">    training your own model, please see the documentation of that class.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; wordSegmenter = WordSegmenterModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;words_segmented&quot;)</span>

<span class="sd">    The default model is ``&quot;wordseg_pku&quot;``, default language is ``&quot;zh&quot;``, if no</span>
<span class="sd">    values are provided. For available pretrained models please see the `Models</span>
<span class="sd">    Hub &lt;https://nlp.johnsnowlabs.com/models?task=Word+Segmentation&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/annotation/chinese/word_segmentation/words_segmenter_demo.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``TOKEN``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    None</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; wordSegmenter = WordSegmenterModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     wordSegmenter</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;token.result&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------------------------------------------+</span>
<span class="sd">    |result                                                  |</span>
<span class="sd">    +--------------------------------------------------------+</span>
<span class="sd">    |[, , , , , , , , , ,       ]|</span>
<span class="sd">    +--------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;WordSegmenterModel&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.ws.WordSegmenterModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WordSegmenterModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

<div class="viewcode-block" id="WordSegmenterModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.WordSegmenterModel.html#sparknlp.annotator.WordSegmenterModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;wordseg_pku&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;zh&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;wordseg_pku&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        WordSegmenterModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">WordSegmenterModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="T5Transformer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer">[docs]</a><span class="k">class</span> <span class="nc">T5Transformer</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;T5: the Text-To-Text Transfer Transformer</span>

<span class="sd">    T5 reconsiders all NLP tasks into a unified text-to-text-format where the</span>
<span class="sd">    input and output are always text strings, in contrast to BERT-style models</span>
<span class="sd">    that can only output either a class label or a span of the input. The</span>
<span class="sd">    text-to-text framework is able to use the same model, loss function, and</span>
<span class="sd">    hyper-parameters on any NLP task, including machine translation, document</span>
<span class="sd">    summarization, question answering, and classification tasks (e.g., sentiment</span>
<span class="sd">    analysis). T5 can even apply to regression tasks by training it to predict</span>
<span class="sd">    the string representation of a number instead of the number itself.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; t5 = T5Transformer.pretrained() \\</span>
<span class="sd">    ...     .setTask(&quot;summarize:&quot;) \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;summaries&quot;)</span>


<span class="sd">    The default model is ``&quot;t5_small&quot;``, if no name is provided. For available</span>
<span class="sd">    pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?q=t5&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/10.Question_Answering_and_Summarization_with_T5.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``DOCUMENT``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    task</span>
<span class="sd">        Transformer&#39;s task, e.g. ``summarize:``</span>
<span class="sd">    minOutputLength</span>
<span class="sd">        Minimum length of the sequence to be generated</span>
<span class="sd">    maxOutputLength</span>
<span class="sd">        Maximum length of output text</span>
<span class="sd">    doSample</span>
<span class="sd">        Whether or not to use sampling; use greedy decoding otherwise</span>
<span class="sd">    temperature</span>
<span class="sd">        The value used to module the next token probabilities</span>
<span class="sd">    topK</span>
<span class="sd">        The number of highest probability vocabulary tokens to keep for</span>
<span class="sd">        top-k-filtering</span>
<span class="sd">    topP</span>
<span class="sd">        Top cumulative probability for vocabulary tokens</span>

<span class="sd">        If set to float &lt; 1, only the most probable tokens with probabilities</span>
<span class="sd">        that add up to ``topP`` or higher are kept for generation.</span>
<span class="sd">    repetitionPenalty</span>
<span class="sd">        The parameter for repetition penalty. 1.0 means no penalty.</span>
<span class="sd">    noRepeatNgramSize</span>
<span class="sd">        If set to int &gt; 0, all ngrams of that size can only occur once</span>
<span class="sd">    ignoreTokenIds</span>
<span class="sd">       A list of token ids which are ignored in the decoder&#39;s output</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This is a very computationally expensive module especially on larger</span>
<span class="sd">    sequence. The use of an accelerator such as GPU is recommended.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    - `Exploring Transfer Learning with T5: the Text-To-Text Transfer</span>
<span class="sd">      Transformer</span>
<span class="sd">      &lt;https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html&gt;`__</span>
<span class="sd">    - `Exploring the Limits of Transfer Learning with a Unified Text-to-Text</span>
<span class="sd">      Transformer &lt;https://arxiv.org/abs/1910.10683&gt;`__</span>
<span class="sd">    - https://github.com/google-research/text-to-text-transfer-transformer</span>

<span class="sd">    **Paper Abstract:**</span>

<span class="sd">    *Transfer learning, where a model is first pre-trained on a data-rich task</span>
<span class="sd">    before being fine-tuned on a downstream task, has emerged as a powerful</span>
<span class="sd">    technique in natural language processing (NLP). The effectiveness of</span>
<span class="sd">    transfer learning has given rise to a diversity of approaches, methodology,</span>
<span class="sd">    and practice. In this paper, we explore the landscape of transfer learning</span>
<span class="sd">    techniques for NLP by introducing a unified framework that converts all</span>
<span class="sd">    text-based language problems into a text-to-text format. Our systematic</span>
<span class="sd">    study compares pre-training objectives, architectures, unlabeled data sets,</span>
<span class="sd">    transfer approaches, and other factors on dozens of language understanding</span>
<span class="sd">    tasks. By combining the insights from our exploration with scale and our new</span>
<span class="sd">    Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many</span>
<span class="sd">    benchmarks covering summarization, question answering, text classification,</span>
<span class="sd">    and more. To facilitate future work on transfer learning for NLP, we release</span>
<span class="sd">    our data set, pre-trained models, and code.*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;documents&quot;)</span>
<span class="sd">    &gt;&gt;&gt; t5 = T5Transformer.pretrained(&quot;t5_small&quot;) \\</span>
<span class="sd">    ...     .setTask(&quot;summarize:&quot;) \\</span>
<span class="sd">    ...     .setInputCols([&quot;documents&quot;]) \\</span>
<span class="sd">    ...     .setMaxOutputLength(200) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;summaries&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([documentAssembler, t5])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[</span>
<span class="sd">    ...     &quot;Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a &quot; +</span>
<span class="sd">    ...     &quot;downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness&quot; +</span>
<span class="sd">    ...     &quot; of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this &quot; +</span>
<span class="sd">    ...     &quot;paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework &quot; +</span>
<span class="sd">    ...     &quot;that converts all text-based language problems into a text-to-text format. Our systematic study compares &quot; +</span>
<span class="sd">    ...     &quot;pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens &quot; +</span>
<span class="sd">    ...     &quot;of language understanding tasks. By combining the insights from our exploration with scale and our new &quot; +</span>
<span class="sd">    ...     &quot;Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering &quot; +</span>
<span class="sd">    ...     &quot;summarization, question answering, text classification, and more. To facilitate future work on transfer &quot; +</span>
<span class="sd">    ...     &quot;learning for NLP, we release our data set, pre-trained models, and code.&quot;</span>
<span class="sd">    ... ]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;summaries.result&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                                                                                                                                                        |</span>
<span class="sd">    +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="sd">    |[transfer learning has emerged as a powerful technique in natural language processing (NLP) the effectiveness of transfer learning has given rise to a diversity of approaches, methodologies, and practice .]|</span>
<span class="sd">    --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;T5Transformer&quot;</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">task</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;task&quot;</span><span class="p">,</span> <span class="s2">&quot;Transformer&#39;s task, e.g. summarize&gt;&quot;</span><span class="p">,</span> <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">minOutputLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;minOutputLength&quot;</span><span class="p">,</span> <span class="s2">&quot;Minimum length of the sequence to be generated&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">maxOutputLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;maxOutputLength&quot;</span><span class="p">,</span> <span class="s2">&quot;Maximum length of output text&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">doSample</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;doSample&quot;</span><span class="p">,</span> <span class="s2">&quot;Whether or not to use sampling; use greedy decoding otherwise&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>
    <span class="n">temperature</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;temperature&quot;</span><span class="p">,</span> <span class="s2">&quot;The value used to module the next token probabilities&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>
    <span class="n">topK</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;topK&quot;</span><span class="p">,</span>
                 <span class="s2">&quot;The number of highest probability vocabulary tokens to keep for top-k-filtering&quot;</span><span class="p">,</span>
                 <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>
    <span class="n">topP</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;topP&quot;</span><span class="p">,</span>
                 <span class="s2">&quot;If set to float &lt; 1, only the most probable tokens with probabilities that add up to ``top_p`` or higher are kept for generation&quot;</span><span class="p">,</span>
                 <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>
    <span class="n">repetitionPenalty</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;repetitionPenalty&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;The parameter for repetition penalty. 1.0 means no penalty. See `this paper &lt;https://arxiv.org/pdf/1909.05858.pdf&gt;`__ for more details&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>
    <span class="n">noRepeatNgramSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;noRepeatNgramSize&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;If set to int &gt; 0, all ngrams of that size can only occur once&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">ignoreTokenIds</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;ignoreTokenIds&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;A list of token ids which are ignored in the decoder&#39;s output&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListInt</span><span class="p">)</span>

<div class="viewcode-block" id="T5Transformer.setIgnoreTokenIds"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.setIgnoreTokenIds">[docs]</a>    <span class="k">def</span> <span class="nf">setIgnoreTokenIds</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A list of token ids which are ignored in the decoder&#39;s output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[int]</span>
<span class="sd">            The words to be filtered out</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">ignoreTokenIds</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="T5Transformer.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="T5Transformer.setTask"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.setTask">[docs]</a>    <span class="k">def</span> <span class="nf">setTask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the transformer&#39;s task, e.g. ``summarize:``.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            The transformer&#39;s task</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="T5Transformer.setMinOutputLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.setMinOutputLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMinOutputLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets minimum length of the sequence to be generated.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Minimum length of the sequence to be generated</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">minOutputLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="T5Transformer.setMaxOutputLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.setMaxOutputLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxOutputLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets maximum length of output text.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Maximum length of output text</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxOutputLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="T5Transformer.setDoSample"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.setDoSample">[docs]</a>    <span class="k">def</span> <span class="nf">setDoSample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether or not to use sampling, use greedy decoding otherwise.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether or not to use sampling; use greedy decoding otherwise</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">doSample</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="T5Transformer.setTemperature"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.setTemperature">[docs]</a>    <span class="k">def</span> <span class="nf">setTemperature</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the value used to module the next token probabilities.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : float</span>
<span class="sd">            The value used to module the next token probabilities</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">temperature</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="T5Transformer.setTopK"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.setTopK">[docs]</a>    <span class="k">def</span> <span class="nf">setTopK</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the number of highest probability vocabulary tokens to keep for</span>
<span class="sd">        top-k-filtering.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Number of highest probability vocabulary tokens to keep</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">topK</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="T5Transformer.setTopP"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.setTopP">[docs]</a>    <span class="k">def</span> <span class="nf">setTopP</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the top cumulative probability for vocabulary tokens.</span>

<span class="sd">        If set to float &lt; 1, only the most probable tokens with probabilities</span>
<span class="sd">        that add up to ``topP`` or higher are kept for generation.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : float</span>
<span class="sd">            Cumulative probability for vocabulary tokens</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">topP</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="T5Transformer.setRepetitionPenalty"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.setRepetitionPenalty">[docs]</a>    <span class="k">def</span> <span class="nf">setRepetitionPenalty</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the parameter for repetition penalty. 1.0 means no penalty.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : float</span>
<span class="sd">            The repetition penalty</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        See `Ctrl: A Conditional Transformer Language Model For Controllable</span>
<span class="sd">        Generation &lt;https://arxiv.org/pdf/1909.05858.pdf&gt;`__ for more details.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">repetitionPenalty</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="T5Transformer.setNoRepeatNgramSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.setNoRepeatNgramSize">[docs]</a>    <span class="k">def</span> <span class="nf">setNoRepeatNgramSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets size of n-grams that can only occur once.</span>

<span class="sd">        If set to int &gt; 0, all ngrams of that size can only occur once.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            N-gram size can only occur once</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">noRepeatNgramSize</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.seq2seq.T5Transformer&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">T5Transformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">task</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="n">minOutputLength</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">maxOutputLength</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
            <span class="n">doSample</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">temperature</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="n">topK</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
            <span class="n">topP</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="n">repetitionPenalty</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
            <span class="n">noRepeatNgramSize</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">ignoreTokenIds</span><span class="o">=</span><span class="p">[]</span>
        <span class="p">)</span>

<div class="viewcode-block" id="T5Transformer.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T5Transformer</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_T5Loader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_T5Loader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">T5Transformer</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="T5Transformer.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.T5Transformer.html#sparknlp.annotator.T5Transformer.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;t5_small&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;t5_small&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T5Transformer</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">T5Transformer</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="MarianTransformer"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MarianTransformer.html#sparknlp.annotator.MarianTransformer">[docs]</a><span class="k">class</span> <span class="nc">MarianTransformer</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;MarianTransformer: Fast Neural Machine Translation</span>

<span class="sd">    Marian is an efficient, free Neural Machine Translation framework written in</span>
<span class="sd">    pure C++ with minimal dependencies. It is mainly being developed by the</span>
<span class="sd">    Microsoft Translator team. Many academic (most notably the University of</span>
<span class="sd">    Edinburgh and in the past the Adam Mickiewicz University in Pozna) and</span>
<span class="sd">    commercial contributors help with its development. MarianTransformer uses</span>
<span class="sd">    the models trained by MarianNMT.</span>

<span class="sd">    It is currently the engine behind the Microsoft Translator Neural Machine</span>
<span class="sd">    Translation services and being deployed by many companies, organizations and</span>
<span class="sd">    research projects.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; marian = MarianTransformer.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;translation&quot;)</span>

<span class="sd">    The default model is ``&quot;opus_mt_en_fr&quot;``, default language is ``&quot;xx&quot;``</span>
<span class="sd">    (meaning multi-lingual), if no values are provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Translation&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/TRANSLATION_MARIAN.ipynb&gt;`__.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT``           ``DOCUMENT``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Size of every batch, by default 8</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    langId</span>
<span class="sd">        Transformer&#39;s task, e.g. &quot;summarize&gt;&quot;, by default &quot;&quot;</span>
<span class="sd">    maxInputLength</span>
<span class="sd">        Controls the maximum length for encoder inputs (source language texts),</span>
<span class="sd">        by default 40</span>
<span class="sd">    maxOutputLength</span>
<span class="sd">        Controls the maximum length for decoder outputs (target language texts),</span>
<span class="sd">        by default 40</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    This is a very computationally expensive module especially on larger</span>
<span class="sd">    sequence. The use of an accelerator such as GPU is recommended.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `MarianNMT at GitHub &lt;https://marian-nmt.github.io/&gt;`__</span>

<span class="sd">    `Marian: Fast Neural Machine Translation in C++  &lt;https://www.aclweb.org/anthology/P18-4020/&gt;`__</span>

<span class="sd">    **Paper Abstract:**</span>

<span class="sd">    *We present Marian, an efficient and self-contained Neural Machine</span>
<span class="sd">    Translation framework with an integrated automatic differentiation</span>
<span class="sd">    engine based on dynamic computation graphs. Marian is written entirely in</span>
<span class="sd">    C++. We describe the design of the encoder-decoder framework and</span>
<span class="sd">    demonstrate that a research-friendly toolkit can achieve high training</span>
<span class="sd">    and translation speed.*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;xx&quot;) \\</span>
<span class="sd">    ...     .setInputCols(&quot;document&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; marian = MarianTransformer.pretrained() \\</span>
<span class="sd">    ...     .setInputCols(&quot;sentence&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;translation&quot;) \\</span>
<span class="sd">    ...     .setMaxInputLength(30)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       sentence,</span>
<span class="sd">    ...       marian</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;What is the capital of France? We should know this in french.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(translation.result) as result&quot;).show(truncate=False)</span>
<span class="sd">    +-------------------------------------+</span>
<span class="sd">    |result                               |</span>
<span class="sd">    +-------------------------------------+</span>
<span class="sd">    |Quelle est la capitale de la France ?|</span>
<span class="sd">    |On devrait le savoir en franais.    |</span>
<span class="sd">    +-------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;MarianTransformer&quot;</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">langId</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;langId&quot;</span><span class="p">,</span> <span class="s2">&quot;Transformer&#39;s task, e.g. summarize&gt;&quot;</span><span class="p">,</span>
                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">maxInputLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;maxInputLength&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;Controls the maximum length for encoder inputs (source language texts)&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">maxOutputLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;maxOutputLength&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Controls the maximum length for decoder outputs (target language texts)&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">ignoreTokenIds</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;ignoreTokenIds&quot;</span><span class="p">,</span>
                           <span class="s2">&quot;A list of token ids which are ignored in the decoder&#39;s output&quot;</span><span class="p">,</span>
                           <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListInt</span><span class="p">)</span>

<div class="viewcode-block" id="MarianTransformer.setIgnoreTokenIds"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MarianTransformer.html#sparknlp.annotator.MarianTransformer.setIgnoreTokenIds">[docs]</a>    <span class="k">def</span> <span class="nf">setIgnoreTokenIds</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;A list of token ids which are ignored in the decoder&#39;s output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[int]</span>
<span class="sd">            The words to be filtered out</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">ignoreTokenIds</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="MarianTransformer.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MarianTransformer.html#sparknlp.annotator.MarianTransformer.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="MarianTransformer.setLangId"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MarianTransformer.html#sparknlp.annotator.MarianTransformer.setLangId">[docs]</a>    <span class="k">def</span> <span class="nf">setLangId</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets transformer&#39;s task, e.g. &quot;summarize&gt;&quot;, by default &quot;&quot;.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Transformer&#39;s task, e.g. &quot;summarize&gt;&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">langId</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="MarianTransformer.setMaxInputLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MarianTransformer.html#sparknlp.annotator.MarianTransformer.setMaxInputLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxInputLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the maximum length for encoder inputs (source language texts),</span>
<span class="sd">        by default 40.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            The maximum length for encoder inputs (source language texts)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxInputLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="MarianTransformer.setMaxOutputLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MarianTransformer.html#sparknlp.annotator.MarianTransformer.setMaxOutputLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxOutputLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets the maximum length for decoder outputs (target language texts),</span>
<span class="sd">        by default 40.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            The maximum length for decoder outputs (target language texts)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxOutputLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.seq2seq.MarianTransformer&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MarianTransformer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">maxInputLength</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
            <span class="n">maxOutputLength</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
            <span class="n">langId</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span>
            <span class="n">ignoreTokenIds</span><span class="o">=</span><span class="p">[]</span>
        <span class="p">)</span>

<div class="viewcode-block" id="MarianTransformer.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MarianTransformer.html#sparknlp.annotator.MarianTransformer.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        MarianTransformer</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_MarianLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_MarianLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">MarianTransformer</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="MarianTransformer.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.MarianTransformer.html#sparknlp.annotator.MarianTransformer.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;opus_mt_en_fr&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;xx&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;opus_mt_en_fr&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;xx&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        MarianTransformer</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">MarianTransformer</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="DistilBertEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertEmbeddings.html#sparknlp.annotator.DistilBertEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">DistilBertEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                           <span class="n">HasEmbeddingsProperties</span><span class="p">,</span>
                           <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                           <span class="n">HasStorageRef</span><span class="p">,</span>
                           <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;DistilBERT is a small, fast, cheap and light Transformer model trained by</span>
<span class="sd">    distilling BERT base. It has 40% less parameters than ``bert-base-uncased``,</span>
<span class="sd">    runs 60% faster while preserving over 95% of BERT&#39;s performances as measured</span>
<span class="sd">    on the GLUE language understanding benchmark.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = DistilBertEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>


<span class="sd">    The default model is ``&quot;distilbert_base_cased&quot;``, if no name is provided.</span>
<span class="sd">    For available pretrained models please see the</span>
<span class="sd">    `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Embeddings&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20DistilBERT.ipynb&gt;`__.</span>
<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``WORD_EMBEDDINGS``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Size of every batch, by default 8</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions, by default 768</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        False</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    - DistilBERT doesn&#39;t have ``token_type_ids``, you don&#39;t need to</span>
<span class="sd">      indicate which token belongs to which segment. Just separate your segments</span>
<span class="sd">      with the separation token ``tokenizer.sep_token`` (or ``[SEP]``).</span>
<span class="sd">    - DistilBERT doesn&#39;t have options to select the input positions</span>
<span class="sd">      (``position_ids`` input). This could be added if necessary though,</span>
<span class="sd">      just let us know if you need this option.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    The DistilBERT model was proposed in the paper</span>
<span class="sd">    `DistilBERT, a distilled version of BERT: smaller, faster, cheaper and</span>
<span class="sd">    lighter &lt;https://arxiv.org/abs/1910.01108&gt;`__.</span>

<span class="sd">    **Paper Abstract:**</span>

<span class="sd">    *As Transfer Learning from large-scale pre-trained models becomes more</span>
<span class="sd">    prevalent  in Natural Language Processing (NLP), operating these</span>
<span class="sd">    large models in on-the- edge and/or under constrained computational</span>
<span class="sd">    training or inference budgets  remains challenging. In this work, we</span>
<span class="sd">    propose a method to pre-train a smaller  general-purpose language</span>
<span class="sd">    representation model, called DistilBERT, which can  then be</span>
<span class="sd">    fine-tuned with good performances on a wide range of tasks like its</span>
<span class="sd">    larger counterparts. While most prior work investigated the use of</span>
<span class="sd">    distillation  for building task-specific models, we leverage</span>
<span class="sd">    knowledge distillation during  the pretraining phase and show that it</span>
<span class="sd">    is possible to reduce the size of a BERT  model by 40%, while</span>
<span class="sd">    retaining 97% of its language understanding capabilities  and being</span>
<span class="sd">    60% faster. To leverage the inductive biases learned by larger</span>
<span class="sd">    models  during pretraining, we introduce a triple loss combining</span>
<span class="sd">    language modeling,  distillation and cosine-distance losses. Our</span>
<span class="sd">    smaller, faster and lighter model  is cheaper to pre-train and we</span>
<span class="sd">    demonstrate its capabilities for on-device  computations in a</span>
<span class="sd">    proof-of-concept experiment and a comparative on-device study.*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = DistilBertEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True) \\</span>
<span class="sd">    ...     .setCleanAnnotations(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       tokenizer,</span>
<span class="sd">    ...       embeddings,</span>
<span class="sd">    ...       embeddingsFinisher</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[0.1127224713563919,-0.1982710212469101,0.5360898375511169,-0.272536993026733...|</span>
<span class="sd">    |[0.35534414649009705,0.13215228915214539,0.40981462597846985,0.14036104083061...|</span>
<span class="sd">    |[0.328085333108902,-0.06269335001707077,-0.017595693469047546,-0.024373905733...|</span>
<span class="sd">    |[0.15617232024669647,0.2967822253704071,0.22324979305267334,-0.04568954557180...|</span>
<span class="sd">    |[0.45411425828933716,0.01173491682857275,0.190129816532135,0.1178255230188369...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;DistilBertEmbeddings&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="DistilBertEmbeddings.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertEmbeddings.html#sparknlp.annotator.DistilBertEmbeddings.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="DistilBertEmbeddings.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertEmbeddings.html#sparknlp.annotator.DistilBertEmbeddings.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistilBertEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

<div class="viewcode-block" id="DistilBertEmbeddings.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertEmbeddings.html#sparknlp.annotator.DistilBertEmbeddings.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DistilBertEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_DistilBertLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_DistilBertLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">DistilBertEmbeddings</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="DistilBertEmbeddings.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertEmbeddings.html#sparknlp.annotator.DistilBertEmbeddings.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;distilbert_base_cased&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;distilbert_base_cased&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DistilBertEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">DistilBertEmbeddings</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="RoBertaEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaEmbeddings.html#sparknlp.annotator.RoBertaEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">RoBertaEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                        <span class="n">HasEmbeddingsProperties</span><span class="p">,</span>
                        <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                        <span class="n">HasStorageRef</span><span class="p">,</span>
                        <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Creates word embeddings using RoBERTa.</span>

<span class="sd">    The RoBERTa model was proposed in `RoBERTa: A Robustly Optimized BERT</span>
<span class="sd">    Pretraining Approach` by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,</span>
<span class="sd">    Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin</span>
<span class="sd">    Stoyanov. It is based on Google&#39;s BERT model released in 2018.</span>

<span class="sd">    It builds on BERT and modifies key hyperparameters, removing the</span>
<span class="sd">    next-sentence pretraining objective and training with much larger</span>
<span class="sd">    mini-batches and learning rates.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = RoBertaEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>

<span class="sd">    The default model is ``&quot;roberta_base&quot;``, if no name is provided. For</span>
<span class="sd">    available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Embeddings&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20RoBERTa.ipynb&gt;`__.</span>
<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>


<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``WORD_EMBEDDINGS``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Size of every batch, by default 8</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions, by default 768</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    - RoBERTa has the same architecture as BERT, but uses a byte-level BPE as</span>
<span class="sd">      a tokenizer (same as GPT-2) and uses a different pretraining scheme.</span>
<span class="sd">    - RoBERTa doesn&#39;t have ``token_type_ids``, you don&#39;t need to indicate</span>
<span class="sd">      which token belongs to which segment. Just separate your segments with</span>
<span class="sd">      the separation token ``tokenizer.sep_token`` (or ``&lt;/s&gt;``)</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `RoBERTa: A Robustly Optimized BERT</span>
<span class="sd">    Pretraining Approach &lt;https://arxiv.org/abs/1907.11692&gt;`__</span>

<span class="sd">    **Paper Abstract:**</span>

<span class="sd">    *Language model pretraining has led to significant performance gains but</span>
<span class="sd">    careful comparison between different approaches is challenging. Training is</span>
<span class="sd">    computationally expensive, often done on private datasets of different</span>
<span class="sd">    sizes, and, as we will show, hyperparameter choices have significant impact</span>
<span class="sd">    on the final results. We present a replication study of BERT pretraining</span>
<span class="sd">    (Devlin et al., 2019) that carefully measures the impact of many key</span>
<span class="sd">    hyperparameters and training data size. We find that BERT was significantly</span>
<span class="sd">    undertrained, and can match or exceed the performance of every model</span>
<span class="sd">    published after it. Our best model achieves state-of-the-art results on</span>
<span class="sd">    GLUE, RACE and SQuAD. These results highlight the importance of previously</span>
<span class="sd">    overlooked design choices, and raise questions about the source of recently</span>
<span class="sd">    reported improvements. We release our models and code.*</span>

<span class="sd">    Source of the original code: `RoBERTa: A Robustly Optimized BERT Pretraining</span>
<span class="sd">    Approach on GitHub</span>
<span class="sd">    &lt;https://github.com/pytorch/fairseq/tree/master/examples/roberta&gt;`__.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = RoBertaEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True) \\</span>
<span class="sd">    ...     .setCleanAnnotations(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       tokenizer,</span>
<span class="sd">    ...       embeddings,</span>
<span class="sd">    ...       embeddingsFinisher</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[0.18792399764060974,-0.14591649174690247,0.20547787845134735,0.1468472778797...|</span>
<span class="sd">    |[0.22845706343650818,0.18073144555091858,0.09725798666477203,-0.0417917296290...|</span>
<span class="sd">    |[0.07037967443466187,-0.14801117777824402,-0.03603338822722435,-0.17893412709...|</span>
<span class="sd">    |[-0.08734266459941864,0.2486150562763214,-0.009067727252840996,-0.24408400058...|</span>
<span class="sd">    |[0.22409197688102722,-0.4312366545200348,0.1401449590921402,0.356410235166549...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;RoBertaEmbeddings&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="RoBertaEmbeddings.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaEmbeddings.html#sparknlp.annotator.RoBertaEmbeddings.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="RoBertaEmbeddings.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaEmbeddings.html#sparknlp.annotator.RoBertaEmbeddings.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RoBertaEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="RoBertaEmbeddings.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaEmbeddings.html#sparknlp.annotator.RoBertaEmbeddings.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        RoBertaEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_RoBertaLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_RoBertaLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">RoBertaEmbeddings</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="RoBertaEmbeddings.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaEmbeddings.html#sparknlp.annotator.RoBertaEmbeddings.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;roberta_base&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;roberta_base&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        RoBertaEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">RoBertaEmbeddings</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="XlmRoBertaEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaEmbeddings.html#sparknlp.annotator.XlmRoBertaEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">XlmRoBertaEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                           <span class="n">HasEmbeddingsProperties</span><span class="p">,</span>
                           <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                           <span class="n">HasStorageRef</span><span class="p">,</span>
                           <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;The XLM-RoBERTa model was proposed in `Unsupervised Cross-lingual</span>
<span class="sd">    Representation Learning at Scale` by Alexis Conneau, Kartikay Khandelwal,</span>
<span class="sd">    Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard</span>
<span class="sd">    Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov.</span>

<span class="sd">    It is based on Facebook&#39;s RoBERTa model released in 2019. It is a large</span>
<span class="sd">    multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = XlmRoBertaEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>

<span class="sd">    The default model is ``&quot;xlm_roberta_base&quot;``, default language is ``&quot;xx&quot;``</span>
<span class="sd">    (meaning multi-lingual), if no values are provided. For available pretrained</span>
<span class="sd">    models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Embeddings&gt;`__.</span>

<span class="sd">    For extended examples of usage, see the `Spark NLP Workshop</span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20XLM-RoBERTa.ipynb&gt;`__.</span>
<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``WORD_EMBEDDINGS``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Size of every batch, by default 8</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions, by default 768</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>

<span class="sd">    Notes</span>
<span class="sd">    -----</span>
<span class="sd">    - XLM-RoBERTa is a multilingual model trained on 100 different languages.</span>
<span class="sd">      Unlike some XLM multilingual models, it does not require **lang**</span>
<span class="sd">      parameter to understand which language is used, and should be able to</span>
<span class="sd">      determine the correct language from the input ids.</span>
<span class="sd">    - This implementation is the same as RoBERTa. Refer to</span>
<span class="sd">      :class:`.RoBertaEmbeddings` for usage examples as well as the information</span>
<span class="sd">      relative to the inputs and outputs.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `Unsupervised Cross-lingual</span>
<span class="sd">    Representation Learning at Scale &lt;https://arxiv.org/abs/1911.02116&gt;`__</span>

<span class="sd">    **Paper Abstract:**</span>

<span class="sd">    *This paper shows that pretraining multilingual language models at scale</span>
<span class="sd">    leads to significant performance gains for a wide range of cross-lingual</span>
<span class="sd">    transfer tasks. We train a Transformer-based masked language model on one</span>
<span class="sd">    hundred languages, using more than two terabytes of filtered CommonCrawl</span>
<span class="sd">    data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT</span>
<span class="sd">    (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average</span>
<span class="sd">    accuracy on XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1</span>
<span class="sd">    score on NER. XLM-R performs particularly well on low-resource languages,</span>
<span class="sd">    improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the</span>
<span class="sd">    previous XLM model. We also present a detailed empirical evaluation of the</span>
<span class="sd">    key factors that are required to achieve these gains, including the</span>
<span class="sd">    trade-offs between (1) positive transfer and capacity dilution and (2) the</span>
<span class="sd">    performance of high and low resource languages at scale. Finally, we show,</span>
<span class="sd">    for the first time, the possibility of multilingual modeling without</span>
<span class="sd">    sacrificing per-language performance; XLM-Ris very competitive with strong</span>
<span class="sd">    monolingual models on the GLUE and XNLI benchmarks. We will make XLM-R code,</span>
<span class="sd">    data, and models publicly available.*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = XlmRoBertaEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True) \\</span>
<span class="sd">    ...     .setCleanAnnotations(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       tokenizer,</span>
<span class="sd">    ...       embeddings,</span>
<span class="sd">    ...       embeddingsFinisher</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[-0.05969233065843582,-0.030789051204919815,0.04443822056055069,0.09564960747...|</span>
<span class="sd">    |[-0.038839809596538544,0.011712731793522835,0.019954433664679527,0.0667808502...|</span>
<span class="sd">    |[-0.03952755779027939,-0.03455188870429993,0.019103847444057465,0.04311436787...|</span>
<span class="sd">    |[-0.09579929709434509,0.02494969218969345,-0.014753809198737144,0.10259044915...|</span>
<span class="sd">    |[0.004710011184215546,-0.022148698568344116,0.011723337695002556,-0.013356896...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;XlmRoBertaEmbeddings&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="XlmRoBertaEmbeddings.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaEmbeddings.html#sparknlp.annotator.XlmRoBertaEmbeddings.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="XlmRoBertaEmbeddings.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaEmbeddings.html#sparknlp.annotator.XlmRoBertaEmbeddings.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">XlmRoBertaEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="XlmRoBertaEmbeddings.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaEmbeddings.html#sparknlp.annotator.XlmRoBertaEmbeddings.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        XlmRoBertaEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_XlmRoBertaLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_XlmRoBertaLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">XlmRoBertaEmbeddings</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="XlmRoBertaEmbeddings.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaEmbeddings.html#sparknlp.annotator.XlmRoBertaEmbeddings.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;xlm_roberta_base&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;xx&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;xlm_roberta_base&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;xx&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        XlmRoBertaEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">XlmRoBertaEmbeddings</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="GraphExtraction"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction">[docs]</a><span class="k">class</span> <span class="nc">GraphExtraction</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Extracts a dependency graph between entities.</span>

<span class="sd">    The GraphExtraction class takes e.g. extracted entities from a</span>
<span class="sd">    :class:`.NerDLModel` and creates a dependency tree which describes how the</span>
<span class="sd">    entities relate to each other. For that a triple store format is used. Nodes</span>
<span class="sd">    represent the entities and the edges represent the relations between those</span>
<span class="sd">    entities. The graph can then be used to find relevant relationships between</span>
<span class="sd">    words.</span>

<span class="sd">    Both the :class:`.DependencyParserModel` and</span>
<span class="sd">    :class:`.TypedDependencyParserModel` need to be</span>
<span class="sd">    present in the pipeline. There are two ways to set them:</span>

<span class="sd">    #. Both Annotators are present in the pipeline already. The dependencies are</span>
<span class="sd">       taken implicitly from these two Annotators.</span>
<span class="sd">    #. Setting :meth:`.setMergeEntities` to ``True`` will download the</span>
<span class="sd">       default pretrained models for those two Annotators automatically. The</span>
<span class="sd">       specific models can also be set with :meth:`.setDependencyParserModel`</span>
<span class="sd">       and :meth:`.setTypedDependencyParserModel`:</span>

<span class="sd">        &gt;&gt;&gt; graph_extraction = GraphExtraction() \\</span>
<span class="sd">        ...     .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;]) \\</span>
<span class="sd">        ...     .setOutputCol(&quot;graph&quot;) \\</span>
<span class="sd">        ...     .setRelationshipTypes([&quot;prefer-LOC&quot;]) \\</span>
<span class="sd">        ...     .setMergeEntities(True)</span>
<span class="sd">        &gt;&gt;&gt;     #.setDependencyParserModel([&quot;dependency_conllu&quot;, &quot;en&quot;,  &quot;public/models&quot;])</span>
<span class="sd">        &gt;&gt;&gt;     #.setTypedDependencyParserModel([&quot;dependency_typed_conllu&quot;, &quot;en&quot;,  &quot;public/models&quot;])</span>

<span class="sd">    ================================= ======================</span>
<span class="sd">    Input Annotation types            Output Annotation type</span>
<span class="sd">    ================================= ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN, NAMED_ENTITY`` ``NODE``</span>
<span class="sd">    ================================= ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    relationshipTypes</span>
<span class="sd">        Paths to find between a pair of token and entity</span>
<span class="sd">    entityTypes</span>
<span class="sd">        Paths to find between a pair of entities</span>
<span class="sd">    explodeEntities</span>
<span class="sd">        When set to true find paths between entities</span>
<span class="sd">    rootTokens</span>
<span class="sd">        Tokens to be consider as root to start traversing the paths. Use it</span>
<span class="sd">        along with explodeEntities</span>
<span class="sd">    maxSentenceSize</span>
<span class="sd">        Maximum sentence size that the annotator will process, by default 1000.</span>
<span class="sd">        Above this, the sentence is skipped</span>
<span class="sd">    minSentenceSize</span>
<span class="sd">        Minimum sentence size that the annotator will process, by default 2.</span>
<span class="sd">        Below this, the sentence is skipped.</span>
<span class="sd">    mergeEntities</span>
<span class="sd">        Merge same neighboring entities as a single token</span>
<span class="sd">    includeEdges</span>
<span class="sd">        Whether to include edges when building paths</span>
<span class="sd">    delimiter</span>
<span class="sd">        Delimiter symbol used for path output</span>
<span class="sd">    posModel</span>
<span class="sd">        Coordinates (name, lang, remoteLoc) to a pretrained POS model</span>
<span class="sd">    dependencyParserModel</span>
<span class="sd">        Coordinates (name, lang, remoteLoc) to a pretrained Dependency Parser</span>
<span class="sd">        model</span>
<span class="sd">    typedDependencyParserModel</span>
<span class="sd">        Coordinates (name, lang, remoteLoc) to a pretrained Typed Dependency</span>
<span class="sd">        Parser model</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = WordEmbeddingsModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; nerTagger = NerDLModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;ner&quot;)</span>
<span class="sd">    &gt;&gt;&gt; posTagger = PerceptronModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;pos&quot;)</span>
<span class="sd">    &gt;&gt;&gt; dependencyParser = DependencyParserModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;dependency&quot;)</span>
<span class="sd">    &gt;&gt;&gt; typedDependencyParser = TypedDependencyParserModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;dependency_type&quot;)</span>
<span class="sd">    &gt;&gt;&gt; graph_extraction = GraphExtraction() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;graph&quot;) \\</span>
<span class="sd">    ...     .setRelationshipTypes([&quot;prefer-LOC&quot;])</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     nerTagger,</span>
<span class="sd">    ...     posTagger,</span>
<span class="sd">    ...     dependencyParser,</span>
<span class="sd">    ...     typedDependencyParser,</span>
<span class="sd">    ...     graph_extraction</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;You and John prefer the morning flight through Denver&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;graph&quot;).show(truncate=False)</span>
<span class="sd">    +-----------------------------------------------------------------------------------------------------------------+</span>
<span class="sd">    |graph                                                                                                            |</span>
<span class="sd">    +-----------------------------------------------------------------------------------------------------------------+</span>
<span class="sd">    |[[node, 13, 18, prefer, [relationship -&gt; prefer,LOC, path1 -&gt; prefer,nsubj,morning,flat,flight,flat,Denver], []]]|</span>
<span class="sd">    +-----------------------------------------------------------------------------------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    GraphFinisher : to output the paths in a more generic format, like RDF</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;GraphExtraction&quot;</span>

    <span class="n">relationshipTypes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;relationshipTypes&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Find paths between a pair of token and entity&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">entityTypes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                        <span class="s2">&quot;entityTypes&quot;</span><span class="p">,</span>
                        <span class="s2">&quot;Find paths between a pair of entities&quot;</span><span class="p">,</span>
                        <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">explodeEntities</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;explodeEntities&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;When set to true find paths between entities&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">rootTokens</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;rootTokens&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;Tokens to be consider as root to start traversing the paths. Use it along with explodeEntities&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">maxSentenceSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;maxSentenceSize&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Maximum sentence size that the annotator will process. Above this, the sentence is skipped&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">minSentenceSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                            <span class="s2">&quot;minSentenceSize&quot;</span><span class="p">,</span>
                            <span class="s2">&quot;Minimum sentence size that the annotator will process. Above this, the sentence is skipped&quot;</span><span class="p">,</span>
                            <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">mergeEntities</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;mergeEntities&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;Merge same neighboring entities as a single token&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">mergeEntitiesIOBFormat</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                   <span class="s2">&quot;mergeEntitiesIOBFormat&quot;</span><span class="p">,</span>
                                   <span class="s2">&quot;IOB format to apply when merging entities&quot;</span><span class="p">,</span>
                                   <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">includeEdges</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                         <span class="s2">&quot;includeEdges&quot;</span><span class="p">,</span>
                         <span class="s2">&quot;Whether to include edges when building paths&quot;</span><span class="p">,</span>
                         <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">delimiter</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                      <span class="s2">&quot;delimiter&quot;</span><span class="p">,</span>
                      <span class="s2">&quot;Delimiter symbol used for path output&quot;</span><span class="p">,</span>
                      <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toString</span><span class="p">)</span>

    <span class="n">posModel</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;posModel&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;Coordinates (name, lang, remoteLoc) to a pretrained POS model&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">dependencyParserModel</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                  <span class="s2">&quot;dependencyParserModel&quot;</span><span class="p">,</span>
                                  <span class="s2">&quot;Coordinates (name, lang, remoteLoc) to a pretrained Dependency Parser model&quot;</span><span class="p">,</span>
                                  <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">typedDependencyParserModel</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                                       <span class="s2">&quot;typedDependencyParserModel&quot;</span><span class="p">,</span>
                                       <span class="s2">&quot;Coordinates (name, lang, remoteLoc) to a pretrained Typed Dependency Parser model&quot;</span><span class="p">,</span>
                                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="GraphExtraction.setRelationshipTypes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setRelationshipTypes">[docs]</a>    <span class="k">def</span> <span class="nf">setRelationshipTypes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets paths to find between a pair of token and entity.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Paths to find between a pair of token and entity</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">relationshipTypes</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="GraphExtraction.setEntityTypes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setEntityTypes">[docs]</a>    <span class="k">def</span> <span class="nf">setEntityTypes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets paths to find between a pair of entities.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Paths to find between a pair of entities</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">entityTypes</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="GraphExtraction.setExplodeEntities"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setExplodeEntities">[docs]</a>    <span class="k">def</span> <span class="nf">setExplodeEntities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to find paths between entities.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to find paths between entities.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">explodeEntities</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="GraphExtraction.setRootTokens"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setRootTokens">[docs]</a>    <span class="k">def</span> <span class="nf">setRootTokens</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets tokens to be considered as the root to start traversing the paths.</span>

<span class="sd">        Use it along with explodeEntities.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Sets Tokens to be consider as root to start traversing the paths.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">rootTokens</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="GraphExtraction.setMaxSentenceSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setMaxSentenceSize">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets Maximum sentence size that the annotator will process, by</span>
<span class="sd">        default 1000.</span>

<span class="sd">        Above this, the sentence is skipped.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Maximum sentence size that the annotator will process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceSize</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="GraphExtraction.setMinSentenceSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setMinSentenceSize">[docs]</a>    <span class="k">def</span> <span class="nf">setMinSentenceSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets Minimum sentence size that the annotator will process, by</span>
<span class="sd">        default 2.</span>

<span class="sd">        Below this, the sentence is skipped.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Minimum sentence size that the annotator will process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">minSentenceSize</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="GraphExtraction.setMergeEntities"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setMergeEntities">[docs]</a>    <span class="k">def</span> <span class="nf">setMergeEntities</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to merge same neighboring entities as a single token.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to merge same neighboring entities as a single token.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">mergeEntities</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="GraphExtraction.setMergeEntitiesIOBFormat"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setMergeEntitiesIOBFormat">[docs]</a>    <span class="k">def</span> <span class="nf">setMergeEntitiesIOBFormat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets IOB format to apply when merging entities.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            IOB format to apply when merging entities. Values IOB or IOB2</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">mergeEntitiesIOBFormat</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="GraphExtraction.setIncludeEdges"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setIncludeEdges">[docs]</a>    <span class="k">def</span> <span class="nf">setIncludeEdges</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to include edges when building paths.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to include edges when building paths</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">includeEdges</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="GraphExtraction.setDelimiter"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setDelimiter">[docs]</a>    <span class="k">def</span> <span class="nf">setDelimiter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets delimiter symbol used for path output.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : str</span>
<span class="sd">            Delimiter symbol used for path output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">delimiter</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="GraphExtraction.setPosModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setPosModel">[docs]</a>    <span class="k">def</span> <span class="nf">setPosModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets Coordinates (name, lang, remoteLoc) to a pretrained POS model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Coordinates (name, lang, remoteLoc) to a pretrained POS model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">posModel</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="GraphExtraction.setDependencyParserModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setDependencyParserModel">[docs]</a>    <span class="k">def</span> <span class="nf">setDependencyParserModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets Coordinates (name, lang, remoteLoc) to a pretrained Dependency</span>
<span class="sd">        Parser model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Coordinates (name, lang, remoteLoc) to a pretrained Dependency</span>
<span class="sd">            Parser model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">dependencyParserModel</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="GraphExtraction.setTypedDependencyParserModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.GraphExtraction.html#sparknlp.annotator.GraphExtraction.setTypedDependencyParserModel">[docs]</a>    <span class="k">def</span> <span class="nf">setTypedDependencyParserModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets Coordinates (name, lang, remoteLoc) to a pretrained Typed</span>
<span class="sd">        Dependency Parser model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : List[str]</span>
<span class="sd">            Coordinates (name, lang, remoteLoc) to a pretrained Typed Dependency</span>
<span class="sd">            Parser model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">typedDependencyParserModel</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.GraphExtraction&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GraphExtraction</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">maxSentenceSize</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
            <span class="n">minSentenceSize</span><span class="o">=</span><span class="mi">2</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="BertForTokenClassification"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertForTokenClassification.html#sparknlp.annotator.BertForTokenClassification">[docs]</a><span class="k">class</span> <span class="nc">BertForTokenClassification</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                                 <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                                 <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;BertForTokenClassification can load Bert Models with a token</span>
<span class="sd">    classification head on top (a linear layer on top of the hidden-states</span>
<span class="sd">    output) e.g. for Named-Entity-Recognition (NER) tasks.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = BertForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;)</span>

<span class="sd">    The default model is ``&quot;bert_base_token_classifier_conll03&quot;``, if no name is</span>
<span class="sd">    provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition&gt;`__.</span>

<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``NAMED_ENTITY``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size. Large values allows faster processing but requires more</span>
<span class="sd">        memory, by default 8</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenClassifier = BertForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     tokenClassifier</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;John Lenon was born in London and lived in Paris. My name is Sarah and I live in London&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;label.result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |result</span>
<span class="sd">    |</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O,</span>
<span class="sd">    O, B-LOC]|</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;BertForTokenClassification&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="BertForTokenClassification.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertForTokenClassification.html#sparknlp.annotator.BertForTokenClassification.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="BertForTokenClassification.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertForTokenClassification.html#sparknlp.annotator.BertForTokenClassification.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process, by default 128.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.BertForTokenClassification&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BertForTokenClassification</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="BertForTokenClassification.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertForTokenClassification.html#sparknlp.annotator.BertForTokenClassification.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">            spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        BertForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_BertTokenClassifierLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_BertTokenClassifierLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">BertForTokenClassification</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="BertForTokenClassification.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertForTokenClassification.html#sparknlp.annotator.BertForTokenClassification.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;bert_base_token_classifier_conll03&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default</span>
<span class="sd">            &quot;bert_base_token_classifier_conll03&quot;</span>
<span class="sd">            lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">            remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        BertForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">BertForTokenClassification</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="DistilBertForTokenClassification"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertForTokenClassification.html#sparknlp.annotator.DistilBertForTokenClassification">[docs]</a><span class="k">class</span> <span class="nc">DistilBertForTokenClassification</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                                       <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                                       <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;DistilBertForTokenClassification can load Bert Models with a token</span>
<span class="sd">    classification head on top (a linear layer on top of the hidden-states</span>
<span class="sd">    output) e.g. for Named-Entity-Recognition (NER) tasks.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; labels = DistilBertForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;)</span>

<span class="sd">    The default model is ``&quot;distilbert_base_token_classifier_conll03&quot;``, if no</span>
<span class="sd">    name is provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition&gt;`__.</span>

<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``NAMED_ENTITY``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size. Large values allows faster processing but requires more</span>
<span class="sd">        memory, by default 8</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenClassifier = DistilBertForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     tokenClassifier</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;John Lenon was born in London and lived in Paris. My name is Sarah and I live in London&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;label.result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                              |</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]|</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;DistilBertForTokenClassification&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="DistilBertForTokenClassification.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertForTokenClassification.html#sparknlp.annotator.DistilBertForTokenClassification.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="DistilBertForTokenClassification.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertForTokenClassification.html#sparknlp.annotator.DistilBertForTokenClassification.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process, by default 128.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.DistilBertForTokenClassification&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistilBertForTokenClassification</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="DistilBertForTokenClassification.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertForTokenClassification.html#sparknlp.annotator.DistilBertForTokenClassification.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DistilBertForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_DistilBertTokenClassifierLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_DistilBertTokenClassifierLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">DistilBertForTokenClassification</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="DistilBertForTokenClassification.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertForTokenClassification.html#sparknlp.annotator.DistilBertForTokenClassification.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;distilbert_base_token_classifier_conll03&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default</span>
<span class="sd">            &quot;distilbert_base_token_classifier_conll03&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DistilBertForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">DistilBertForTokenClassification</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="LongformerEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LongformerEmbeddings.html#sparknlp.annotator.LongformerEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">LongformerEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                           <span class="n">HasEmbeddingsProperties</span><span class="p">,</span>
                           <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                           <span class="n">HasStorageRef</span><span class="p">,</span>
                           <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Longformer is a transformer model for long documents. The Longformer</span>
<span class="sd">    model was presented in `Longformer: The Long-Document Transformer` by Iz</span>
<span class="sd">    Beltagy, Matthew E. Peters, Arman Cohan. longformer-base-4096 is a BERT-like</span>
<span class="sd">    model started from the RoBERTa checkpoint and pretrained for MLM on long</span>
<span class="sd">    documents. It supports sequences of length up to 4,096.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = LongformerEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>


<span class="sd">    The default model is ``&quot;longformer_base_4096&quot;``, if no name is provided. For</span>
<span class="sd">    available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Embeddings&gt;`__.</span>

<span class="sd">    Models from the HuggingFace  Transformers library are compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``WORD_EMBEDDINGS``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Size of every batch, by default 8</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions, by default 768</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 1024</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `Longformer: The Long-Document Transformer</span>
<span class="sd">    &lt;https://arxiv.org/pdf/2004.05150.pdf&gt;`__</span>


<span class="sd">    **Paper Abstract:**</span>

<span class="sd">    *Transformer-based models are unable to process long sequences due to their</span>
<span class="sd">    self-attention operation, which scales quadratically with the sequence</span>
<span class="sd">    length. To address this limitation, we introduce the Longformer with an</span>
<span class="sd">    attention mechanism that scales linearly with sequence length, making it</span>
<span class="sd">    easy to process documents of thousands of tokens or longer. Longformer&#39;s</span>
<span class="sd">    attention mechanism is a drop-in replacement for the standard self-attention</span>
<span class="sd">    and combines a local windowed attention with a task motivated global</span>
<span class="sd">    attention. Following prior work on long-sequence transformers, we evaluate</span>
<span class="sd">    Longformer on character-level language modeling and achieve state-of-the-art</span>
<span class="sd">    results on text8 and enwik8. In contrast to most prior work, we also</span>
<span class="sd">    pretrain Longformer and finetune it on a variety of downstream tasks. Our</span>
<span class="sd">    pretrained Longformer consistently outperforms RoBERTa on long document</span>
<span class="sd">    tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We</span>
<span class="sd">    finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant</span>
<span class="sd">    for supporting long document generative sequence-to-sequence tasks, and</span>
<span class="sd">    demonstrate its effectiveness on the arXiv summarization dataset.*</span>

<span class="sd">    The original code can be found at `Longformer: The Long-Document Transformer</span>
<span class="sd">    &lt;https://github.com/allenai/longformer&gt;`__.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = LongformerEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    &gt;&gt;&gt;     .setInputCols([&quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True) \\</span>
<span class="sd">    ...     .setCleanAnnotations(False)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...         documentAssembler,</span>
<span class="sd">    ...         tokenizer,</span>
<span class="sd">    ...         embeddings,</span>
<span class="sd">    ...         embeddingsFinisher</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[0.18792399764060974,-0.14591649174690247,0.20547787845134735,0.1468472778797...|</span>
<span class="sd">    |[0.22845706343650818,0.18073144555091858,0.09725798666477203,-0.0417917296290...|</span>
<span class="sd">    |[0.07037967443466187,-0.14801117777824402,-0.03603338822722435,-0.17893412709...|</span>
<span class="sd">    |[-0.08734266459941864,0.2486150562763214,-0.009067727252840996,-0.24408400058...|</span>
<span class="sd">    |[0.22409197688102722,-0.4312366545200348,0.1401449590921402,0.356410235166549...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;LongformerEmbeddings&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="LongformerEmbeddings.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LongformerEmbeddings.html#sparknlp.annotator.LongformerEmbeddings.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="LongformerEmbeddings.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LongformerEmbeddings.html#sparknlp.annotator.LongformerEmbeddings.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process, by default 1024.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LongformerEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="LongformerEmbeddings.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LongformerEmbeddings.html#sparknlp.annotator.LongformerEmbeddings.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        LongformerEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_LongformerLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_LongformerLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">LongformerEmbeddings</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="LongformerEmbeddings.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LongformerEmbeddings.html#sparknlp.annotator.LongformerEmbeddings.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;longformer_base_4096&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;longformer_base_4096&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        LongformerEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">LongformerEmbeddings</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="RoBertaSentenceEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaSentenceEmbeddings.html#sparknlp.annotator.RoBertaSentenceEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">RoBertaSentenceEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                                <span class="n">HasEmbeddingsProperties</span><span class="p">,</span>
                                <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                                <span class="n">HasStorageRef</span><span class="p">,</span>
                                <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sentence-level embeddings using RoBERTa. The RoBERTa model was proposed in RoBERTa: A Robustly Optimized BERT</span>
<span class="sd">    Pretraining Approach  by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,</span>
<span class="sd">    Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov. It is based on Google&#39;s BERT model released in 2018. It builds on</span>
<span class="sd">    BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much</span>
<span class="sd">    larger mini-batches and learning rates. Pretrained models can be loaded with pretrained of the companion object:</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = RoBertaSentenceEmbeddings.pretrained() \\</span>
<span class="sd">    ...    .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...    .setOutputCol(&quot;sentence_embeddings&quot;)</span>


<span class="sd">    The default model is ``&quot;sent_roberta_base&quot;``, if no name is provided.</span>

<span class="sd">    For available pretrained models please see the</span>
<span class="sd">    `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Embeddings&gt;`__.</span>

<span class="sd">    ====================== =======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== =======================</span>
<span class="sd">    ``DOCUMENT``           ``SENTENCE_EMBEDDINGS``</span>
<span class="sd">    ====================== =======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Size of every batch, by default 8</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        False</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions, by default 768</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `RoBERTa: A Robustly Optimized BERT Pretraining Approach &lt;https://arxiv.org/abs/1907.11692&gt;`__</span>

<span class="sd">    **Paper abstract:**</span>

<span class="sd">    *Language model pretraining has led to significant performance gains but careful comparison between different</span>
<span class="sd">    approaches is challenging. Training is computationally expensive, often done on private datasets of different</span>
<span class="sd">    sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a</span>
<span class="sd">    replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key</span>
<span class="sd">    hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed</span>
<span class="sd">    the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE,</span>
<span class="sd">    RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise</span>
<span class="sd">    questions about the source of recently reported improvements. We release our models and code.*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = RoBertaSentenceEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence_embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence_embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     embeddingsFinisher</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;John loves apples. Mary loves oranges. John loves Mary.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[-0.8951074481010437,0.13753940165042877,0.3108254075050354,-1.65693199634552...|</span>
<span class="sd">    |[-0.6180210709571838,-0.12179657071828842,-0.191165953874588,-1.4497021436691...|</span>
<span class="sd">    |[-0.822715163230896,0.7568016648292542,-0.1165061742067337,-1.59048593044281,...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;RoBertaSentenceEmbeddings&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="RoBertaSentenceEmbeddings.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaSentenceEmbeddings.html#sparknlp.annotator.RoBertaSentenceEmbeddings.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="RoBertaSentenceEmbeddings.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaSentenceEmbeddings.html#sparknlp.annotator.RoBertaSentenceEmbeddings.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RoBertaSentenceEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="RoBertaSentenceEmbeddings.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaSentenceEmbeddings.html#sparknlp.annotator.RoBertaSentenceEmbeddings.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        BertSentenceEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_RoBertaSentenceLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_RoBertaSentenceLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">RoBertaSentenceEmbeddings</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="RoBertaSentenceEmbeddings.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaSentenceEmbeddings.html#sparknlp.annotator.RoBertaSentenceEmbeddings.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;sent_roberta_base&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;sent_roberta_base&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        RoBertaSentenceEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">RoBertaSentenceEmbeddings</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="XlmRoBertaSentenceEmbeddings"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaSentenceEmbeddings.html#sparknlp.annotator.XlmRoBertaSentenceEmbeddings">[docs]</a><span class="k">class</span> <span class="nc">XlmRoBertaSentenceEmbeddings</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                                   <span class="n">HasEmbeddingsProperties</span><span class="p">,</span>
                                   <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                                   <span class="n">HasStorageRef</span><span class="p">,</span>
                                   <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sentence-level embeddings using XLM-RoBERTa. The XLM-RoBERTa model was proposed in Unsupervised Cross-lingual</span>
<span class="sd">    Representation Learning at Scale  by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary,</span>
<span class="sd">    Guillaume Wenzek, Francisco Guzmn, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based</span>
<span class="sd">    on Facebook&#39;s RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of</span>
<span class="sd">    filtered CommonCrawl data. Pretrained models can be loaded with pretrained of the companion object:</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = XlmRoBertaSentenceEmbeddings.pretrained() \\</span>
<span class="sd">    ...    .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...    .setOutputCol(&quot;sentence_embeddings&quot;)</span>


<span class="sd">    The default model is ``&quot;sent_xlm_roberta_base&quot;``, if no name is provided.</span>

<span class="sd">    For available pretrained models please see the</span>
<span class="sd">    `Models Hub &lt;https://nlp.johnsnowlabs.com/models?task=Embeddings&gt;`__.</span>

<span class="sd">    ====================== =======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== =======================</span>
<span class="sd">    ``DOCUMENT``           ``SENTENCE_EMBEDDINGS``</span>
<span class="sd">    ====================== =======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Size of every batch, by default 8</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        False</span>
<span class="sd">    dimension</span>
<span class="sd">        Number of embedding dimensions, by default 768</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    `Unsupervised Cross-lingual Representation Learning at Scale &lt;https://arxiv.org/pdf/1911.02116.pdf&gt;`__</span>

<span class="sd">    **Paper abstract:**</span>

<span class="sd">    *This paper shows that pretraining multilingual language models at scale leads to significant performance gains</span>
<span class="sd">    for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one</span>
<span class="sd">    hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R,</span>
<span class="sd">    significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8%</span>
<span class="sd">    average accuracy on XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs</span>
<span class="sd">    particularly well on low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over</span>
<span class="sd">    the previous XLM model. We also present a detailed empirical evaluation of the key factors that are required to</span>
<span class="sd">    achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the</span>
<span class="sd">    performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of</span>
<span class="sd">    multilingual modeling without sacrificing per-language performance; XLM-Ris very competitive with strong</span>
<span class="sd">    monolingual models on the GLUE and XNLI benchmarks. We will make XLM-R code, data, and models publicly available.*</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sentence = SentenceDetector() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = XlmRoBertaSentenceEmbeddings.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;sentence_embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;sentence_embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     sentence,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     embeddingsFinisher</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;John loves apples. Mary loves oranges. John loves Mary.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[-0.8951074481010437,0.13753940165042877,0.3108254075050354,-1.65693199634552...|</span>
<span class="sd">    |[-0.6180210709571838,-0.12179657071828842,-0.191165953874588,-1.4497021436691...|</span>
<span class="sd">    |[-0.822715163230896,0.7568016648292542,-0.1165061742067337,-1.59048593044281,...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;XlmRoBertaSentenceEmbeddings&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="XlmRoBertaSentenceEmbeddings.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaSentenceEmbeddings.html#sparknlp.annotator.XlmRoBertaSentenceEmbeddings.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="XlmRoBertaSentenceEmbeddings.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaSentenceEmbeddings.html#sparknlp.annotator.XlmRoBertaSentenceEmbeddings.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.XlmRoBertaSentenceEmbeddings&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">XlmRoBertaSentenceEmbeddings</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">dimension</span><span class="o">=</span><span class="mi">768</span><span class="p">,</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="XlmRoBertaSentenceEmbeddings.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaSentenceEmbeddings.html#sparknlp.annotator.XlmRoBertaSentenceEmbeddings.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        BertSentenceEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_XlmRoBertaSentenceLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_XlmRoBertaSentenceLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">XlmRoBertaSentenceEmbeddings</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="XlmRoBertaSentenceEmbeddings.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaSentenceEmbeddings.html#sparknlp.annotator.XlmRoBertaSentenceEmbeddings.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;sent_xlm_roberta_base&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;xx&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;sent_xlm_roberta_base&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;xx&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        XlmRoBertaSentenceEmbeddings</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">XlmRoBertaSentenceEmbeddings</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="RoBertaForTokenClassification"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaForTokenClassification.html#sparknlp.annotator.RoBertaForTokenClassification">[docs]</a><span class="k">class</span> <span class="nc">RoBertaForTokenClassification</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                                    <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                                    <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;RoBertaForTokenClassification can load RoBerta Models with a token</span>
<span class="sd">    classification head on top (a linear layer on top of the hidden-states</span>
<span class="sd">    output) e.g. for Named-Entity-Recognition (NER) tasks.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; token_classifier = RoBertaForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;)</span>

<span class="sd">    The default model is ``&quot;roberta_base_token_classifier_conll03&quot;``, if no name</span>
<span class="sd">    is provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition&gt;`__.</span>

<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``NAMED_ENTITY``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size. Large values allows faster processing but requires more</span>
<span class="sd">        memory, by default 8</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenClassifier = RoBertaForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     tokenClassifier</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;John Lenon was born in London and lived in Paris. My name is Sarah and I live in London&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;label.result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                              |</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]|</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;RoBertaForTokenClassification&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="RoBertaForTokenClassification.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaForTokenClassification.html#sparknlp.annotator.RoBertaForTokenClassification.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="RoBertaForTokenClassification.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaForTokenClassification.html#sparknlp.annotator.RoBertaForTokenClassification.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process, by default 128.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.RoBertaForTokenClassification&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RoBertaForTokenClassification</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="RoBertaForTokenClassification.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaForTokenClassification.html#sparknlp.annotator.RoBertaForTokenClassification.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        RoBertaForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_RoBertaTokenClassifierLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_RoBertaTokenClassifierLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">RoBertaForTokenClassification</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="RoBertaForTokenClassification.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.RoBertaForTokenClassification.html#sparknlp.annotator.RoBertaForTokenClassification.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;roberta_base_token_classifier_conll03&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default</span>
<span class="sd">            &quot;roberta_base_token_classifier_conll03&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        RoBertaForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">RoBertaForTokenClassification</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="XlmRoBertaForTokenClassification"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaForTokenClassification.html#sparknlp.annotator.XlmRoBertaForTokenClassification">[docs]</a><span class="k">class</span> <span class="nc">XlmRoBertaForTokenClassification</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                                       <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                                       <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;XlmRoBertaForTokenClassification can load XLM-RoBERTa Models with a token</span>
<span class="sd">    classification head on top (a linear layer on top of the hidden-states</span>
<span class="sd">    output) e.g. for Named-Entity-Recognition (NER) tasks.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; token_classifier = XlmRoBertaForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;)</span>
<span class="sd">    The default model is ``&quot;xlm_roberta_base_token_classifier_conll03&quot;``, if no</span>
<span class="sd">    name is provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition&gt;`__.</span>
<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``NAMED_ENTITY``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size. Large values allows faster processing but requires more</span>
<span class="sd">        memory, by default 8</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenClassifier = XlmRoBertaForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     tokenClassifier</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;John Lenon was born in London and lived in Paris. My name is Sarah and I live in London&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;label.result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                              |</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]|</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;XlmRoBertaForTokenClassification&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="XlmRoBertaForTokenClassification.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaForTokenClassification.html#sparknlp.annotator.XlmRoBertaForTokenClassification.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="XlmRoBertaForTokenClassification.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaForTokenClassification.html#sparknlp.annotator.XlmRoBertaForTokenClassification.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process, by default 128.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.XlmRoBertaForTokenClassification&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">XlmRoBertaForTokenClassification</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="XlmRoBertaForTokenClassification.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaForTokenClassification.html#sparknlp.annotator.XlmRoBertaForTokenClassification.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        XlmRoBertaForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_XlmRoBertaTokenClassifierLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_XlmRoBertaTokenClassifierLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">XlmRoBertaForTokenClassification</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="XlmRoBertaForTokenClassification.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlmRoBertaForTokenClassification.html#sparknlp.annotator.XlmRoBertaForTokenClassification.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;xlm_roberta_base_token_classifier_conll03&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default</span>
<span class="sd">            &quot;xlm_roberta_base_token_classifier_conll03&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        XlmRoBertaForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">XlmRoBertaForTokenClassification</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="AlbertForTokenClassification"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.AlbertForTokenClassification.html#sparknlp.annotator.AlbertForTokenClassification">[docs]</a><span class="k">class</span> <span class="nc">AlbertForTokenClassification</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                                   <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                                   <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;AlbertForTokenClassification can load ALBERT Models with a token</span>
<span class="sd">    classification head on top (a linear layer on top of the hidden-states</span>
<span class="sd">    output) e.g. for Named-Entity-Recognition (NER) tasks.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; token_classifier = AlbertForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;)</span>

<span class="sd">    The default model is ``&quot;albert_base_token_classifier_conll03&quot;``, if no name</span>
<span class="sd">    is provided.</span>
<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition&gt;`__.</span>

<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``NAMED_ENTITY``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size. Large values allows faster processing but requires more</span>
<span class="sd">        memory, by default 8</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenClassifier = AlbertForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     tokenClassifier</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;John Lenon was born in London and lived in Paris. My name is Sarah and I live in London&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;label.result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                              |</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]|</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>

<span class="sd">    See Also</span>
<span class="sd">    --------</span>
<span class="sd">    AlbertEmbeddings : for token-level embeddings</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;AlbertForTokenClassification&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="AlbertForTokenClassification.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.AlbertForTokenClassification.html#sparknlp.annotator.AlbertForTokenClassification.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="AlbertForTokenClassification.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.AlbertForTokenClassification.html#sparknlp.annotator.AlbertForTokenClassification.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process, by default 128.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.AlbertForTokenClassification&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AlbertForTokenClassification</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>

<div class="viewcode-block" id="AlbertForTokenClassification.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.AlbertForTokenClassification.html#sparknlp.annotator.AlbertForTokenClassification.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        AlbertForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_AlbertTokenClassifierLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_AlbertTokenClassifierLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">AlbertForTokenClassification</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="AlbertForTokenClassification.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.AlbertForTokenClassification.html#sparknlp.annotator.AlbertForTokenClassification.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;albert_base_token_classifier_conll03&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default</span>
<span class="sd">            &quot;albert_base_token_classifier_conll03&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        AlbertForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">AlbertForTokenClassification</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="XlnetForTokenClassification"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlnetForTokenClassification.html#sparknlp.annotator.XlnetForTokenClassification">[docs]</a><span class="k">class</span> <span class="nc">XlnetForTokenClassification</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                                  <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                                  <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;XlnetForTokenClassification can load XLNet Models with a token</span>
<span class="sd">    classification head on top (a linear layer on top of the hidden-states</span>
<span class="sd">    output) e.g. for Named-Entity-Recognition (NER) tasks.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; token_classifier = XlnetForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;)</span>

<span class="sd">    The default model is ``&quot;xlnet_base_token_classifier_conll03&quot;``, if no name is</span>
<span class="sd">    provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition&gt;`__.</span>

<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``NAMED_ENTITY``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size. Large values allows faster processing but requires more</span>
<span class="sd">        memory, by default 8</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenClassifier = XlnetForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     tokenClassifier</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;John Lenon was born in London and lived in Paris. My name is Sarah and I live in London&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;label.result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                              |</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]|</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;XlnetForTokenClassification&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="XlnetForTokenClassification.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlnetForTokenClassification.html#sparknlp.annotator.XlnetForTokenClassification.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="XlnetForTokenClassification.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlnetForTokenClassification.html#sparknlp.annotator.XlnetForTokenClassification.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process, by default 128.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.XlnetForTokenClassification&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">XlnetForTokenClassification</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="XlnetForTokenClassification.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlnetForTokenClassification.html#sparknlp.annotator.XlnetForTokenClassification.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        XlnetForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_XlnetTokenClassifierLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_XlnetTokenClassifierLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">XlnetForTokenClassification</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="XlnetForTokenClassification.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.XlnetForTokenClassification.html#sparknlp.annotator.XlnetForTokenClassification.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;xlnet_base_token_classifier_conll03&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default</span>
<span class="sd">            &quot;xlnet_base_token_classifier_conll03&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        XlnetForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">XlnetForTokenClassification</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="LongformerForTokenClassification"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LongformerForTokenClassification.html#sparknlp.annotator.LongformerForTokenClassification">[docs]</a><span class="k">class</span> <span class="nc">LongformerForTokenClassification</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                                       <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                                       <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;LongformerForTokenClassification can load Longformer Models with a token</span>
<span class="sd">    classification head on top (a linear layer on top of the hidden-states</span>
<span class="sd">    output) e.g. for Named-Entity-Recognition (NER) tasks.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; token_classifier = LongformerForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;)</span>

<span class="sd">    The default model is ``&quot;xlnet_base_token_classifier_conll03&quot;``, if no name is</span>
<span class="sd">    provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition&gt;`__.</span>

<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``NAMED_ENTITY``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size. Large values allows faster processing but requires more</span>
<span class="sd">        memory, by default 8</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenClassifier = LongformerForTokenClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     tokenClassifier</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;John Lenon was born in London and lived in Paris. My name is Sarah and I live in London&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;label.result&quot;).show(truncate=False)</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |result                                                                              |</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]|</span>
<span class="sd">    +------------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;LongformerForTokenClassification&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

<div class="viewcode-block" id="LongformerForTokenClassification.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LongformerForTokenClassification.html#sparknlp.annotator.LongformerForTokenClassification.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="LongformerForTokenClassification.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LongformerForTokenClassification.html#sparknlp.annotator.LongformerForTokenClassification.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process, by default 128.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.LongformerForTokenClassification&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LongformerForTokenClassification</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="LongformerForTokenClassification.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LongformerForTokenClassification.html#sparknlp.annotator.LongformerForTokenClassification.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        LongformerForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_LongformerTokenClassifierLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_LongformerTokenClassifierLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">LongformerForTokenClassification</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="LongformerForTokenClassification.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.LongformerForTokenClassification.html#sparknlp.annotator.LongformerForTokenClassification.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;longformer_base_token_classifier_conll03&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default</span>
<span class="sd">            &quot;longformer_base_token_classifier_conll03&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        LongformerForTokenClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">LongformerForTokenClassification</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="EntityRulerApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.EntityRulerApproach.html#sparknlp.annotator.EntityRulerApproach">[docs]</a><span class="k">class</span> <span class="nc">EntityRulerApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">,</span> <span class="n">HasStorage</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fits an Annotator to match exact strings or regex patterns provided in a</span>
<span class="sd">    file against a Document and assigns them an named entity. The definitions</span>
<span class="sd">    can contain any number of named entities.</span>

<span class="sd">    There are multiple ways and formats to set the extraction resource. It is</span>
<span class="sd">    possible to set it either as a &quot;JSON&quot;, &quot;JSONL&quot; or &quot;CSV&quot; file. A path to the</span>
<span class="sd">    file needs to be provided to ``setPatternsResource``. The file format needs</span>
<span class="sd">    to be set as the &quot;format&quot; field in the ``option`` parameter map and</span>
<span class="sd">    depending on the file type, additional parameters might need to be set.</span>

<span class="sd">    To enable regex extraction, ``setEnablePatternRegex(True)`` needs to be</span>
<span class="sd">    called.</span>

<span class="sd">    If the file is in a JSON format, then the rule definitions need to be given</span>
<span class="sd">    in a list with the fields &quot;id&quot;, &quot;label&quot; and &quot;patterns&quot;::</span>

<span class="sd">         [</span>
<span class="sd">            {</span>
<span class="sd">              &quot;id&quot;: &quot;person-regex&quot;,</span>
<span class="sd">              &quot;label&quot;: &quot;PERSON&quot;,</span>
<span class="sd">              &quot;patterns&quot;: [&quot;\\w+\\s\\w+&quot;, &quot;\\w+-\\w+&quot;]</span>
<span class="sd">            },</span>
<span class="sd">            {</span>
<span class="sd">              &quot;id&quot;: &quot;locations-words&quot;,</span>
<span class="sd">              &quot;label&quot;: &quot;LOCATION&quot;,</span>
<span class="sd">              &quot;patterns&quot;: [&quot;Winterfell&quot;]</span>
<span class="sd">            }</span>
<span class="sd">        ]</span>

<span class="sd">    The same fields also apply to a file in the JSONL format::</span>

<span class="sd">        {&quot;id&quot;: &quot;names-with-j&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Jon&quot;, &quot;John&quot;, &quot;John Snow&quot;]}</span>
<span class="sd">        {&quot;id&quot;: &quot;names-with-s&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Stark&quot;, &quot;Snow&quot;]}</span>
<span class="sd">        {&quot;id&quot;: &quot;names-with-e&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Eddard&quot;, &quot;Eddard Stark&quot;]}</span>

<span class="sd">    In order to use a CSV file, an additional parameter &quot;delimiter&quot; needs to be</span>
<span class="sd">    set. In this case, the delimiter might be set by using</span>
<span class="sd">    ``.setPatternsResource(&quot;patterns.csv&quot;, ReadAs.TEXT, {&quot;format&quot;: &quot;csv&quot;, &quot;delimiter&quot;: &quot;|&quot;)})``::</span>

<span class="sd">        PERSON|Jon</span>
<span class="sd">        PERSON|John</span>
<span class="sd">        PERSON|John Snow</span>
<span class="sd">        LOCATION|Winterfell</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``CHUNK``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    patternsResource</span>
<span class="sd">        Resource in JSON or CSV format to map entities to patterns</span>
<span class="sd">    enablePatternRegex</span>
<span class="sd">        Enables regex pattern match</span>
<span class="sd">    useStorage</span>
<span class="sd">        Whether to use RocksDB storage to serialize patterns</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.common import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>

<span class="sd">    In this example, the entities file as the form of::</span>

<span class="sd">        PERSON|Jon</span>
<span class="sd">        PERSON|John</span>
<span class="sd">        PERSON|John Snow</span>
<span class="sd">        LOCATION|Winterfell</span>

<span class="sd">    where each line represents an entity and the associated string delimited by &quot;|&quot;.</span>

<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; entityRuler = EntityRulerApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;, &quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;entities&quot;) \\</span>
<span class="sd">    ...     .setPatternsResource(</span>
<span class="sd">    ...       &quot;patterns.csv&quot;,</span>
<span class="sd">    ...       ReadAs.TEXT,</span>
<span class="sd">    ...       {&quot;format&quot;: &quot;csv&quot;, &quot;delimiter&quot;: &quot;\\\\|&quot;}</span>
<span class="sd">    ...     ) \\</span>
<span class="sd">    ...     .setEnablePatternRegex(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     entityRuler</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;Jon Snow wants to be lord of Winterfell.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(entities)&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------------------------------------------------------+</span>
<span class="sd">    |col                                                                 |</span>
<span class="sd">    +--------------------------------------------------------------------+</span>
<span class="sd">    |[chunk, 0, 2, Jon, [entity -&gt; PERSON, sentence -&gt; 0], []]           |</span>
<span class="sd">    |[chunk, 29, 38, Winterfell, [entity -&gt; LOCATION, sentence -&gt; 0], []]|</span>
<span class="sd">    +--------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;EntityRulerApproach&quot;</span>

    <span class="n">patternsResource</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;patternsResource&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;Resource in JSON or CSV format to map entities to patterns&quot;</span><span class="p">,</span>
                             <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">identity</span><span class="p">)</span>

    <span class="n">enablePatternRegex</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                               <span class="s2">&quot;enablePatternRegex&quot;</span><span class="p">,</span>
                               <span class="s2">&quot;Enables regex pattern match&quot;</span><span class="p">,</span>
                               <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="n">useStorage</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;useStorage&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;Whether to use RocksDB storage to serialize patterns&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EntityRulerApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.er.EntityRulerApproach&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="EntityRulerApproach.setPatternsResource"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.EntityRulerApproach.html#sparknlp.annotator.EntityRulerApproach.setPatternsResource">[docs]</a>    <span class="k">def</span> <span class="nf">setPatternsResource</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="o">=</span><span class="n">ReadAs</span><span class="o">.</span><span class="n">TEXT</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;format&quot;</span><span class="p">:</span> <span class="s2">&quot;JSON&quot;</span><span class="p">}):</span>
        <span class="sd">&quot;&quot;&quot;Sets Resource in JSON or CSV format to map entities to patterns.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        path : str</span>
<span class="sd">            Path to the resource</span>
<span class="sd">        read_as : str, optional</span>
<span class="sd">            How to interpret the resource, by default ReadAs.TEXT</span>
<span class="sd">        options : dict, optional</span>
<span class="sd">            Options for parsing the resource, by default {&quot;format&quot;: &quot;JSON&quot;}</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">patternsResource</span><span class="o">=</span><span class="n">ExternalResource</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">read_as</span><span class="p">,</span> <span class="n">options</span><span class="p">))</span></div>

<div class="viewcode-block" id="EntityRulerApproach.setEnablePatternRegex"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.EntityRulerApproach.html#sparknlp.annotator.EntityRulerApproach.setEnablePatternRegex">[docs]</a>    <span class="k">def</span> <span class="nf">setEnablePatternRegex</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to enable regex pattern matching.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to enable regex pattern matching.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">enablePatternRegex</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="EntityRulerApproach.setUseStorage"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.EntityRulerApproach.html#sparknlp.annotator.EntityRulerApproach.setUseStorage">[docs]</a>    <span class="k">def</span> <span class="nf">setUseStorage</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets whether to use RocksDB storage to serialize patterns.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            Whether to use RocksDB storage to serialize patterns.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">useStorage</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">EntityRulerModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="EntityRulerModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.EntityRulerModel.html#sparknlp.annotator.EntityRulerModel">[docs]</a><span class="k">class</span> <span class="nc">EntityRulerModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasStorageModel</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Instantiated model of the EntityRulerApproach.</span>
<span class="sd">    For usage and examples see the documentation of the main class.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``CHUNK``</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;EntityRulerModel&quot;</span>
    <span class="n">database</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ENTITY_PATTERNS&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.er.EntityRulerModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EntityRulerModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">EntityRulerModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadStorage</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">spark</span><span class="p">,</span> <span class="n">storage_ref</span><span class="p">):</span>
        <span class="n">HasStorageModel</span><span class="o">.</span><span class="n">loadStorages</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">spark</span><span class="p">,</span> <span class="n">storage_ref</span><span class="p">,</span> <span class="n">EntityRulerModel</span><span class="o">.</span><span class="n">databases</span><span class="p">)</span></div>


<div class="viewcode-block" id="BertForSequenceClassification"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertForSequenceClassification.html#sparknlp.annotator.BertForSequenceClassification">[docs]</a><span class="k">class</span> <span class="nc">BertForSequenceClassification</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                                    <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                                    <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;BertForSequenceClassification can load Bert Models with sequence classification/regression head on top</span>
<span class="sd">    (a linear layer on top of the pooled output) e.g. for multi-class document classification tasks.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; sequenceClassifier = BertForSequenceClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;)</span>

<span class="sd">    The default model is ``&quot;bert_base_sequence_classifier_imdb&quot;``, if no name is</span>
<span class="sd">    provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Text+Classification&gt;`__.</span>

<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``CATEGORY``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size. Large values allows faster processing but requires more</span>
<span class="sd">        memory, by default 8</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>
<span class="sd">    coalesceSentences</span>
<span class="sd">        Instead of 1 class per sentence (if inputCols is &#39;&#39;&#39;sentence&#39;&#39;&#39;) output 1 class per document by averaging probabilities in all sentences.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sequenceClassifier = BertForSequenceClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     sequenceClassifier</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[\&quot;\&quot;\&quot;John Lenon was born in London and lived</span>
<span class="sd">    ... in Paris. My name is Sarah and I live in London\&quot;\&quot;\&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;label.result&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------+</span>
<span class="sd">    |result              |</span>
<span class="sd">    +--------------------+</span>
<span class="sd">    |[neg, neg]          |</span>
<span class="sd">    |[pos, pos, pos, pos]|</span>
<span class="sd">    +--------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;BertForSequenceClassification&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">coalesceSentences</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;coalesceSentences&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Instead of 1 class per sentence (if inputCols is &#39;&#39;&#39;sentence&#39;&#39;&#39;) output 1 class per document by averaging probabilities in all sentences.&quot;</span><span class="p">,</span>
                              <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

<div class="viewcode-block" id="BertForSequenceClassification.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertForSequenceClassification.html#sparknlp.annotator.BertForSequenceClassification.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="BertForSequenceClassification.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertForSequenceClassification.html#sparknlp.annotator.BertForSequenceClassification.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process, by default 128.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="BertForSequenceClassification.setCoalesceSentences"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertForSequenceClassification.html#sparknlp.annotator.BertForSequenceClassification.setCoalesceSentences">[docs]</a>    <span class="k">def</span> <span class="nf">setCoalesceSentences</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Instead of 1 class per sentence (if inputCols is &#39;&#39;&#39;sentence&#39;&#39;&#39;) output 1 class per document by averaging probabilities in all sentences.</span>
<span class="sd">        Due to max sequence length limit in almost all transformer models such as BERT (512 tokens), this parameter helps feeding all the sentences</span>
<span class="sd">        into the model and averaging all the probabilities for the entire document instead of probabilities per sentence. (Default: true)</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            If the output of all sentences will be averaged to one output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">coalesceSentences</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.BertForSequenceClassification&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="BertForSequenceClassification.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertForSequenceClassification.html#sparknlp.annotator.BertForSequenceClassification.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">            spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        BertForSequenceClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_BertSequenceClassifierLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_BertSequenceClassifierLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">BertForSequenceClassification</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="BertForSequenceClassification.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.BertForSequenceClassification.html#sparknlp.annotator.BertForSequenceClassification.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;bert_base_sequence_classifier_imdb&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default</span>
<span class="sd">            &quot;bert_base_sequence_classifier_imdb&quot;</span>
<span class="sd">            lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">            remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        BertForSequenceClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">BertForSequenceClassification</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Doc2VecApproach"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Doc2VecApproach.html#sparknlp.annotator.Doc2VecApproach">[docs]</a><span class="k">class</span> <span class="nc">Doc2VecApproach</span><span class="p">(</span><span class="n">AnnotatorApproach</span><span class="p">,</span> <span class="n">HasStorageRef</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Trains a Word2Vec model that creates vector representations of words in a</span>
<span class="sd">    text corpus.</span>

<span class="sd">    The algorithm first constructs a vocabulary from the corpus and then learns</span>
<span class="sd">    vector representation of words in the vocabulary. The vector representation</span>
<span class="sd">    can be used as features in natural language processing and machine learning</span>
<span class="sd">    algorithms.</span>

<span class="sd">    We use Word2Vec implemented in Spark ML. It uses skip-gram model in our</span>
<span class="sd">    implementation and a hierarchical softmax method to train the model. The</span>
<span class="sd">    variable names in the implementation match the original C implementation.</span>

<span class="sd">    For instantiated/pretrained models, see :class:`.Doc2VecModel`.</span>

<span class="sd">    For available pretrained models please see the `Models Hub &lt;https://nlp.johnsnowlabs.com/models&gt;`__.</span>

<span class="sd">    ====================== =======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== =======================</span>
<span class="sd">    ``TOKEN``              ``SENTENCE_EMBEDDINGS``</span>
<span class="sd">    ====================== =======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vectorSize</span>
<span class="sd">        The dimension of codes after transforming from words (&gt; 0), by default</span>
<span class="sd">        100</span>
<span class="sd">    windowSize</span>
<span class="sd">        The window size (context words from [-window, window]) (&gt; 0), by default</span>
<span class="sd">        5</span>
<span class="sd">    numPartitions</span>
<span class="sd">        Number of partitions for sentences of words (&gt; 0), by default 1</span>
<span class="sd">    minCount</span>
<span class="sd">        The minimum number of times a token must appear to be included in the</span>
<span class="sd">        word2vec model&#39;s vocabulary (&gt;= 0), by default 1</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        The window size (Maximum length (in words) of each sentence in the input</span>
<span class="sd">        data. Any sentence longer than this threshold will be divided into</span>
<span class="sd">        chunks up to the size (&gt; 0), by default 1000</span>
<span class="sd">    stepSize</span>
<span class="sd">        Step size (learning rate) to be used for each iteration of optimization</span>
<span class="sd">        (&gt; 0), by default 0.025</span>
<span class="sd">    maxIter</span>
<span class="sd">        Maximum number of iterations (&gt;= 0), by default 1</span>
<span class="sd">    seed</span>
<span class="sd">        Random seed, by default 44</span>


<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    For the original C implementation, see https://code.google.com/p/word2vec/</span>

<span class="sd">    For the research paper, see `Efficient Estimation of Word Representations in</span>
<span class="sd">    Vector Space &lt;https://arxiv.org/abs/1301.3781&gt;`__ and `Distributed</span>
<span class="sd">    Representations of Words and Phrases and their Compositionality</span>
<span class="sd">    &lt;https://arxiv.org/pdf/1310.4546v1.pdf&gt;`__.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = Doc2VecApproach() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline() \\</span>
<span class="sd">    ...     .setStages([</span>
<span class="sd">    ...       documentAssembler,</span>
<span class="sd">    ...       tokenizer,</span>
<span class="sd">    ...       embeddings</span>
<span class="sd">    ...     ])</span>
<span class="sd">    &gt;&gt;&gt; path = &quot;sherlockholmes.txt&quot;</span>
<span class="sd">    &gt;&gt;&gt; dataset = spark.read.text(path).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; pipelineModel = pipeline.fit(dataset)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">vectorSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;vectorSize&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;the dimension of codes after transforming from words (&gt; 0)&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">windowSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;windowSize&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;the window size (context words from [-window, window]) (&gt; 0)&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">numPartitions</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                          <span class="s2">&quot;numPartitions&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;number of partitions for sentences of words (&gt; 0)&quot;</span><span class="p">,</span>
                          <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">minCount</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;minCount&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;the minimum number of times a token must &quot;</span> <span class="o">+</span>
                     <span class="s2">&quot;appear to be included in the word2vec model&#39;s vocabulary (&gt;= 0)&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;the window size (Maximum length (in words) of each sentence in the input data. Any sentence longer than this threshold will &quot;</span> <span class="o">+</span>
                              <span class="s2">&quot;be divided into chunks up to the size (&gt; 0)&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">stepSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                     <span class="s2">&quot;stepSize&quot;</span><span class="p">,</span>
                     <span class="s2">&quot;Step size (learning rate) to be used for each iteration of optimization (&gt; 0)&quot;</span><span class="p">,</span>
                     <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toFloat</span><span class="p">)</span>

    <span class="n">maxIter</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                    <span class="s2">&quot;maxIter&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;maximum number of iterations (&gt;= 0)&quot;</span><span class="p">,</span>
                    <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">seed</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                 <span class="s2">&quot;seed&quot;</span><span class="p">,</span>
                 <span class="s2">&quot;Random seed&quot;</span><span class="p">,</span>
                 <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

<div class="viewcode-block" id="Doc2VecApproach.setVectorSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Doc2VecApproach.html#sparknlp.annotator.Doc2VecApproach.setVectorSize">[docs]</a>    <span class="k">def</span> <span class="nf">setVectorSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vectorSize</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets vector size (default: 100).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">vectorSize</span><span class="o">=</span><span class="n">vectorSize</span><span class="p">)</span></div>

<div class="viewcode-block" id="Doc2VecApproach.setWindowSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Doc2VecApproach.html#sparknlp.annotator.Doc2VecApproach.setWindowSize">[docs]</a>    <span class="k">def</span> <span class="nf">setWindowSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">windowSize</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets window size (default: 5).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">windowSize</span><span class="o">=</span><span class="n">windowSize</span><span class="p">)</span></div>

<div class="viewcode-block" id="Doc2VecApproach.setStepSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Doc2VecApproach.html#sparknlp.annotator.Doc2VecApproach.setStepSize">[docs]</a>    <span class="k">def</span> <span class="nf">setStepSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stepSize</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets initial learning rate (default: 0.025).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">stepSize</span><span class="o">=</span><span class="n">stepSize</span><span class="p">)</span></div>

<div class="viewcode-block" id="Doc2VecApproach.setNumPartitions"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Doc2VecApproach.html#sparknlp.annotator.Doc2VecApproach.setNumPartitions">[docs]</a>    <span class="k">def</span> <span class="nf">setNumPartitions</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">numPartitions</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets number of partitions (default: 1). Use a small number for</span>
<span class="sd">        accuracy.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">numPartitions</span><span class="o">=</span><span class="n">numPartitions</span><span class="p">)</span></div>

<div class="viewcode-block" id="Doc2VecApproach.setMaxIter"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Doc2VecApproach.html#sparknlp.annotator.Doc2VecApproach.setMaxIter">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxIter</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">numIterations</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets number of iterations (default: 1), which should be smaller</span>
<span class="sd">        than or equal to number of partitions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxIter</span><span class="o">=</span><span class="n">numIterations</span><span class="p">)</span></div>

<div class="viewcode-block" id="Doc2VecApproach.setSeed"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Doc2VecApproach.html#sparknlp.annotator.Doc2VecApproach.setSeed">[docs]</a>    <span class="k">def</span> <span class="nf">setSeed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets random seed.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span></div>

<div class="viewcode-block" id="Doc2VecApproach.setMinCount"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Doc2VecApproach.html#sparknlp.annotator.Doc2VecApproach.setMinCount">[docs]</a>    <span class="k">def</span> <span class="nf">setMinCount</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">minCount</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets minCount, the minimum number of times a token must appear</span>
<span class="sd">        to be included in the word2vec model&#39;s vocabulary (default: 5).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">minCount</span><span class="o">=</span><span class="n">minCount</span><span class="p">)</span></div>

<div class="viewcode-block" id="Doc2VecApproach.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Doc2VecApproach.html#sparknlp.annotator.Doc2VecApproach.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">maxSentenceLength</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Maximum length (in words) of each sentence in the input data.</span>
<span class="sd">        Any sentence longer than this threshold will be divided into</span>
<span class="sd">        chunks up to the size (&gt; 0)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">maxSentenceLength</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Doc2VecApproach</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.Doc2VecApproach&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">vectorSize</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
            <span class="n">windowSize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
            <span class="n">numPartitions</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">minCount</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
            <span class="n">stepSize</span><span class="o">=</span><span class="mf">0.025</span><span class="p">,</span>
            <span class="n">maxIter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="mi">44</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_create_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">java_model</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">Doc2VecModel</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span><span class="p">)</span></div>


<div class="viewcode-block" id="Doc2VecModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Doc2VecModel.html#sparknlp.annotator.Doc2VecModel">[docs]</a><span class="k">class</span> <span class="nc">Doc2VecModel</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span> <span class="n">HasStorageRef</span><span class="p">,</span> <span class="n">HasEmbeddingsProperties</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Word2Vec model that creates vector representations of words in a text</span>
<span class="sd">    corpus.</span>

<span class="sd">    The algorithm first constructs a vocabulary from the corpus and then learns</span>
<span class="sd">    vector representation of words in the vocabulary. The vector representation</span>
<span class="sd">    can be used as features in natural language processing and machine learning</span>
<span class="sd">    algorithms.</span>

<span class="sd">    We use Word2Vec implemented in Spark ML. It uses skip-gram model in our</span>
<span class="sd">    implementation and a hierarchical softmax method to train the model. The</span>
<span class="sd">    variable names in the implementation match the original C implementation.</span>

<span class="sd">    This is the instantiated model of the :class:`.Doc2VecApproach`. For</span>
<span class="sd">    training your own model, please see the documentation of that class.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; embeddings = Doc2VecModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>

<span class="sd">    The default model is `&quot;doc2vec_gigaword_300&quot;`, if no name is provided.</span>

<span class="sd">    ====================== =======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== =======================</span>
<span class="sd">    ``TOKEN``              ``SENTENCE_EMBEDDINGS``</span>
<span class="sd">    ====================== =======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    vectorSize</span>
<span class="sd">        The dimension of codes after transforming from words (&gt; 0) , by default</span>
<span class="sd">        100</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    For the original C implementation, see https://code.google.com/p/word2vec/</span>

<span class="sd">    For the research paper, see `Efficient Estimation of Word Representations in</span>
<span class="sd">    Vector Space &lt;https://arxiv.org/abs/1301.3781&gt;`__ and `Distributed</span>
<span class="sd">    Representations of Words and Phrases and their Compositionality</span>
<span class="sd">    &lt;https://arxiv.org/pdf/1310.4546v1.pdf&gt;`__.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddings = Doc2VecModel.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;embeddings&quot;)</span>
<span class="sd">    &gt;&gt;&gt; embeddingsFinisher = EmbeddingsFinisher() \\</span>
<span class="sd">    ...     .setInputCols([&quot;embeddings&quot;]) \\</span>
<span class="sd">    ...     .setOutputCols(&quot;finished_embeddings&quot;) \\</span>
<span class="sd">    ...     .setOutputAsVector(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     embeddings,</span>
<span class="sd">    ...     embeddingsFinisher</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80)</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |                                                                          result|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...|</span>
<span class="sd">    +--------------------------------------------------------------------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Doc2VecModel&quot;</span>

    <span class="n">vectorSize</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                       <span class="s2">&quot;vectorSize&quot;</span><span class="p">,</span>
                       <span class="s2">&quot;the dimension of codes after transforming from words (&gt; 0)&quot;</span><span class="p">,</span>
                       <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

<div class="viewcode-block" id="Doc2VecModel.setVectorSize"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Doc2VecModel.html#sparknlp.annotator.Doc2VecModel.setVectorSize">[docs]</a>    <span class="k">def</span> <span class="nf">setVectorSize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vectorSize</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets vector size (default: 100).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">vectorSize</span><span class="o">=</span><span class="n">vectorSize</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.embeddings.Doc2VecModel&quot;</span><span class="p">,</span> <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Doc2VecModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">vectorSize</span><span class="o">=</span><span class="mi">100</span>
        <span class="p">)</span>

<div class="viewcode-block" id="Doc2VecModel.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.Doc2VecModel.html#sparknlp.annotator.Doc2VecModel.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;doc2vec_gigaword_300&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default &quot;doc2vec_wiki&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        Doc2VecModel</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">Doc2VecModel</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="DistilBertForSequenceClassification"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertForSequenceClassification.html#sparknlp.annotator.DistilBertForSequenceClassification">[docs]</a><span class="k">class</span> <span class="nc">DistilBertForSequenceClassification</span><span class="p">(</span><span class="n">AnnotatorModel</span><span class="p">,</span>
                                          <span class="n">HasCaseSensitiveProperties</span><span class="p">,</span>
                                          <span class="n">HasBatchedAnnotate</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;DistilBertForSequenceClassification can load DistilBERT Models with sequence classification/regression head on</span>
<span class="sd">    top (a linear layer on top of the pooled output) e.g. for multi-class document classification tasks.</span>

<span class="sd">    Pretrained models can be loaded with :meth:`.pretrained` of the companion</span>
<span class="sd">    object:</span>

<span class="sd">    &gt;&gt;&gt; sequenceClassifier = DistilBertForSequenceClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;)</span>

<span class="sd">    The default model is ``&quot;distilbert_base_sequence_classifier_imdb&quot;``, if no name is</span>
<span class="sd">    provided.</span>

<span class="sd">    For available pretrained models please see the `Models Hub</span>
<span class="sd">    &lt;https://nlp.johnsnowlabs.com/models?task=Text+Classification&gt;`__.</span>

<span class="sd">    Models from the HuggingFace  Transformers library are also compatible with</span>
<span class="sd">    Spark NLP . To see which models are compatible and how to import them see</span>
<span class="sd">    `Import Transformers into Spark NLP </span>
<span class="sd">    &lt;https://github.com/JohnSnowLabs/spark-nlp/discussions/5669&gt;`_.</span>

<span class="sd">    ====================== ======================</span>
<span class="sd">    Input Annotation types Output Annotation type</span>
<span class="sd">    ====================== ======================</span>
<span class="sd">    ``DOCUMENT, TOKEN``    ``CATEGORY``</span>
<span class="sd">    ====================== ======================</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    batchSize</span>
<span class="sd">        Batch size. Large values allows faster processing but requires more</span>
<span class="sd">        memory, by default 8</span>
<span class="sd">    caseSensitive</span>
<span class="sd">        Whether to ignore case in tokens for embeddings matching, by default</span>
<span class="sd">        True</span>
<span class="sd">    configProtoBytes</span>
<span class="sd">        ConfigProto from tensorflow, serialized into byte array.</span>
<span class="sd">    maxSentenceLength</span>
<span class="sd">        Max sentence length to process, by default 128</span>
<span class="sd">    coalesceSentences</span>
<span class="sd">        Instead of 1 class per sentence (if inputCols is &#39;&#39;&#39;sentence&#39;&#39;&#39;) output 1 class per document by averaging</span>
<span class="sd">        probabilities in all sentences.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import sparknlp</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.base import *</span>
<span class="sd">    &gt;&gt;&gt; from sparknlp.annotator import *</span>
<span class="sd">    &gt;&gt;&gt; from pyspark.ml import Pipeline</span>
<span class="sd">    &gt;&gt;&gt; documentAssembler = DocumentAssembler() \\</span>
<span class="sd">    ...     .setInputCol(&quot;text&quot;) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;document&quot;)</span>
<span class="sd">    &gt;&gt;&gt; tokenizer = Tokenizer() \\</span>
<span class="sd">    ...     .setInputCols([&quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;token&quot;)</span>
<span class="sd">    &gt;&gt;&gt; sequenceClassifier = DistilBertForSequenceClassification.pretrained() \\</span>
<span class="sd">    ...     .setInputCols([&quot;token&quot;, &quot;document&quot;]) \\</span>
<span class="sd">    ...     .setOutputCol(&quot;label&quot;) \\</span>
<span class="sd">    ...     .setCaseSensitive(True)</span>
<span class="sd">    &gt;&gt;&gt; pipeline = Pipeline().setStages([</span>
<span class="sd">    ...     documentAssembler,</span>
<span class="sd">    ...     tokenizer,</span>
<span class="sd">    ...     sequenceClassifier</span>
<span class="sd">    ... ])</span>
<span class="sd">    &gt;&gt;&gt; data = spark.createDataFrame([[\&quot;\&quot;\&quot;John Lenon was born in London and lived</span>
<span class="sd">    ... in Paris. My name is Sarah and I live in London\&quot;\&quot;\&quot;]]).toDF(&quot;text&quot;)</span>
<span class="sd">    &gt;&gt;&gt; result = pipeline.fit(data).transform(data)</span>
<span class="sd">    &gt;&gt;&gt; result.select(&quot;label.result&quot;).show(truncate=False)</span>
<span class="sd">    +--------------------+</span>
<span class="sd">    |result              |</span>
<span class="sd">    +--------------------+</span>
<span class="sd">    |[neg, neg]          |</span>
<span class="sd">    |[pos, pos, pos, pos]|</span>
<span class="sd">    +--------------------+</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;DistilBertForSequenceClassification&quot;</span>

    <span class="n">maxSentenceLength</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                              <span class="s2">&quot;maxSentenceLength&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Max sentence length to process&quot;</span><span class="p">,</span>
                              <span class="n">typeConverter</span><span class="o">=</span><span class="n">TypeConverters</span><span class="o">.</span><span class="n">toInt</span><span class="p">)</span>

    <span class="n">configProtoBytes</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span>
                             <span class="s2">&quot;configProtoBytes&quot;</span><span class="p">,</span>
                             <span class="s2">&quot;ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()&quot;</span><span class="p">,</span>
                             <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toListString</span><span class="p">)</span>

    <span class="n">coalesceSentences</span> <span class="o">=</span> <span class="n">Param</span><span class="p">(</span><span class="n">Params</span><span class="o">.</span><span class="n">_dummy</span><span class="p">(),</span> <span class="s2">&quot;coalesceSentences&quot;</span><span class="p">,</span>
                              <span class="s2">&quot;Instead of 1 class per sentence (if inputCols is &#39;&#39;&#39;sentence&#39;&#39;&#39;) output 1 class per document by averaging probabilities in all sentences.&quot;</span><span class="p">,</span>
                              <span class="n">TypeConverters</span><span class="o">.</span><span class="n">toBoolean</span><span class="p">)</span>

<div class="viewcode-block" id="DistilBertForSequenceClassification.setConfigProtoBytes"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertForSequenceClassification.html#sparknlp.annotator.DistilBertForSequenceClassification.setConfigProtoBytes">[docs]</a>    <span class="k">def</span> <span class="nf">setConfigProtoBytes</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets configProto from tensorflow, serialized into byte array.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        b : List[str]</span>
<span class="sd">            ConfigProto from tensorflow, serialized into byte array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">configProtoBytes</span><span class="o">=</span><span class="n">b</span><span class="p">)</span></div>

<div class="viewcode-block" id="DistilBertForSequenceClassification.setMaxSentenceLength"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertForSequenceClassification.html#sparknlp.annotator.DistilBertForSequenceClassification.setMaxSentenceLength">[docs]</a>    <span class="k">def</span> <span class="nf">setMaxSentenceLength</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sets max sentence length to process, by default 128.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : int</span>
<span class="sd">            Max sentence length to process</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">maxSentenceLength</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

<div class="viewcode-block" id="DistilBertForSequenceClassification.setCoalesceSentences"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertForSequenceClassification.html#sparknlp.annotator.DistilBertForSequenceClassification.setCoalesceSentences">[docs]</a>    <span class="k">def</span> <span class="nf">setCoalesceSentences</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Instead of 1 class per sentence (if inputCols is &#39;&#39;&#39;sentence&#39;&#39;&#39;) output 1 class per document by averaging probabilities in all sentences.</span>
<span class="sd">        Due to max sequence length limit in almost all transformer models such as BERT (512 tokens), this parameter helps feeding all the sentences</span>
<span class="sd">        into the model and averaging all the probabilities for the entire document instead of probabilities per sentence. (Default: true)</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        value : bool</span>
<span class="sd">            If the output of all sentences will be averaged to one output</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_set</span><span class="p">(</span><span class="n">coalesceSentences</span><span class="o">=</span><span class="n">value</span><span class="p">)</span></div>

    <span class="nd">@keyword_only</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classname</span><span class="o">=</span><span class="s2">&quot;com.johnsnowlabs.nlp.annotators.classifier.dl.DistilBertForSequenceClassification&quot;</span><span class="p">,</span>
                 <span class="n">java_model</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DistilBertForSequenceClassification</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">classname</span><span class="o">=</span><span class="n">classname</span><span class="p">,</span>
            <span class="n">java_model</span><span class="o">=</span><span class="n">java_model</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_setDefault</span><span class="p">(</span>
            <span class="n">batchSize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
            <span class="n">maxSentenceLength</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
            <span class="n">caseSensitive</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

<div class="viewcode-block" id="DistilBertForSequenceClassification.loadSavedModel"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertForSequenceClassification.html#sparknlp.annotator.DistilBertForSequenceClassification.loadSavedModel">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">loadSavedModel</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Loads a locally saved model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        folder : str</span>
<span class="sd">            Folder of the saved model</span>
<span class="sd">        spark_session : pyspark.sql.SparkSession</span>
<span class="sd">            The current SparkSession</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DistilBertForSequenceClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.internal</span> <span class="kn">import</span> <span class="n">_DistilBertSequenceClassifierLoader</span>
        <span class="n">jModel</span> <span class="o">=</span> <span class="n">_DistilBertSequenceClassifierLoader</span><span class="p">(</span><span class="n">folder</span><span class="p">,</span> <span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="p">)</span><span class="o">.</span><span class="n">_java_obj</span>
        <span class="k">return</span> <span class="n">DistilBertForSequenceClassification</span><span class="p">(</span><span class="n">java_model</span><span class="o">=</span><span class="n">jModel</span><span class="p">)</span></div>

<div class="viewcode-block" id="DistilBertForSequenceClassification.pretrained"><a class="viewcode-back" href="../../reference/autosummary/sparknlp.annotator.DistilBertForSequenceClassification.html#sparknlp.annotator.DistilBertForSequenceClassification.pretrained">[docs]</a>    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;distilbert_base_sequence_classifier_imdb&quot;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">remote_loc</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Downloads and loads a pretrained model.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        name : str, optional</span>
<span class="sd">            Name of the pretrained model, by default</span>
<span class="sd">            &quot;distilbert_base_sequence_classifier_imdb&quot;</span>
<span class="sd">        lang : str, optional</span>
<span class="sd">            Language of the pretrained model, by default &quot;en&quot;</span>
<span class="sd">        remote_loc : str, optional</span>
<span class="sd">            Optional remote address of the resource, by default None. Will use</span>
<span class="sd">            Spark NLPs repositories otherwise.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        DistilBertForSequenceClassification</span>
<span class="sd">            The restored model</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">from</span> <span class="nn">sparknlp.pretrained</span> <span class="kn">import</span> <span class="n">ResourceDownloader</span>
        <span class="k">return</span> <span class="n">ResourceDownloader</span><span class="o">.</span><span class="n">downloadModel</span><span class="p">(</span><span class="n">DistilBertForSequenceClassification</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">lang</span><span class="p">,</span> <span class="n">remote_loc</span><span class="p">)</span></div></div>
</pre></div>

              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2021, John Snow Labs.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.3.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>