---
layout: model
title: Word Embeddings for Urdu (urduvec_140M_300d)
author: John Snow Labs
name: urduvec_140M_300d
date: 2020-12-01
tags: [embeddings, ur, open_source]
article_header:
  type: cover
use_language_switcher: "Python-Scala-Java"
---

## Description

This model is trained using word2vec approach on a corpora of 140 Million tokens, has a vocabulary of 100k unique tokens, and gives 300 dimensional vector outputs per token. The output vectors map words into a meaningful space where the distance between the vectors is related to semantic similarity of words.

These embeddings can be used in multiple tasks like semantic word similarity, named entity recognition, sentiment analysis, and classification.

{:.btn-box}
<button class="button button-orange" disabled>Live Demo</button>
<button class="button button-orange" disabled>Open in Colab</button>
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/urduvec_140M_300d_ur_2.7.0_2.4_1606810614734.zip){:.button.button-orange.button-orange-trans.arr.button-icon}

## How to use



<div class="tabs-box" markdown="1">
{% include programmingLanguageSelectScalaPython.html %}
```python
embeddings = WordEmbeddingsModel.pretrained("urduvec_140M_300d", "ur") \
        .setInputCols(["document", "token"]) \
        .setOutputCol("embeddings")

```

</div>

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|urduvec_140M_300d|
|Type:|embeddings|
|Compatibility:|Spark NLP 2.7.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[word_embeddings]|
|Language:|ur|
|Case sensitive:|false|
|Dimension:|300|

## Data Source

The model is imported from http://www.lrec-conf.org/proceedings/lrec2018/pdf/148.pdf