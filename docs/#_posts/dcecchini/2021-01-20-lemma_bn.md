---
layout: model
title: Bengali Lemmatizer
author: John Snow Labs
name: lemma
date: 2021-01-20
task: Lemmatization
language: bn
edition: Spark NLP 2.7.0
spark_version: 2.4
tags: [bn, lemmatizer, open_source]
supported: true
annotator: LemmatizerModel
article_header:
type: cover
use_language_switcher: "Python-Scala-Java"
---

## Description

This model uses context and language knowledge to assign all forms and inflections of a word to a single root. This enables the pipeline to treat the past and present tense of a verb, for example, as the same word instead of two completely different words. The lemmatizer takes into consideration the context surrounding a word to determine which root is correct when the word form alone is ambiguous.

{:.btn-box}
[Live Demo](https://demo.johnsnowlabs.com/public/TEXT_PREPROCESSING/){:.button.button-orange}
[Open in Colab](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/TEXT_PREPROCESSING.ipynb){:.button.button-orange.button-orange-trans.co.button-icon}
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/lemma_bn_2.7.0_2.4_1611163691269.zip){:.button.button-orange.button-orange-trans.arr.button-icon}

## How to use



<div class="tabs-box" markdown="1">
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
document_assembler = DocumentAssembler() \
.setInputCol("text") \
.setOutputCol("document")

tokenizer = Tokenizer()\
.setInputCols(["document"]) \
.setOutputCol("token")

lemmatizer = LemmatizerModel.pretrained("lemma", "bn") \
.setInputCols(["token"]) \
.setOutputCol("lemma")

nlp_pipeline = Pipeline(stages=[document_assembler, tokenizer, lemmatizer])
light_pipeline = LightPipeline(nlp_pipeline.fit(spark.createDataFrame([[""]]).toDF("text")))
results = light_pipeline.fullAnnotate(["একদিন প্রাতে বৈদ্যনাথের মার্বলমণ্ডিত দালানে একটি স্থূলোদর সন্ন্যাসী দুইসের মোহনভোগ এবং দেড়সের দুগ্ধ সেবায় নিযুক্ত আছে বৈদ্যনাথ গায়ে একখানি চাদর দিয়া জোড়করে একান্ত বিনীতভাবে ভূতলে বসিয়া ভক্তিভরে পবিত্র ভোজনব্যাপার নিরীক্ষণ করিতেছিলেন এমন সময় কোনোমতে দ্বারীদের দৃষ্টি এড়াইয়া জীর্ণদেহ বালক সহিত একটি অতি শীর্ণকায়া রমণী গৃহে প্রবেশ করিয়া ক্ষীণস্বরে কহিল বাবু দুটি খেতে দাও"])
```
```scala
val document_assembler = DocumentAssembler()
.setInputCol("text")
.setOutputCol("document")

val tokenizer = Tokenizer()
.setInputCols(["document"])
.setOutputCol("token")

val lemmatizer = LemmatizerModel.pretrained("lemma", "bn")
.setInputCols(["token"])
.setOutputCol("lemma")

val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, lemmatizer))
val data = Seq("একদিন প্রাতে বৈদ্যনাথের মার্বলমণ্ডিত দালানে একটি স্থূলোদর সন্ন্যাসী দুইসের মোহনভোগ এবং দেড়সের দুগ্ধ সেবায় নিযুক্ত আছে বৈদ্যনাথ গায়ে একখানি চাদর দিয়া জোড়করে একান্ত বিনীতভাবে ভূতলে বসিয়া ভক্তিভরে পবিত্র ভোজনব্যাপার নিরীক্ষণ করিতেছিলেন এমন সময় কোনোমতে দ্বারীদের দৃষ্টি এড়াইয়া জীর্ণদেহ বালক সহিত একটি অতি শীর্ণকায়া রমণী গৃহে প্রবেশ করিয়া ক্ষীণস্বরে কহিল বাবু দুটি খেতে দাও").toDF("text")
val result = pipeline.fit(data).transform(data)
```

{:.nlu-block}
```python
import nlu

text = ["একদিন প্রাতে বৈদ্যনাথের মার্বলমণ্ডিত দালানে একটি স্থূলোদর সন্ন্যাসী দুইসের মোহনভোগ এবং দেড়সের দুগ্ধ সেবায় নিযুক্ত আছে বৈদ্যনাথ গায়ে একখানি চাদর দিয়া জোড়করে একান্ত বিনীতভাবে ভূতলে বসিয়া ভক্তিভরে পবিত্র ভোজনব্যাপার নিরীক্ষণ করিতেছিলেন এমন সময় কোনোমতে দ্বারীদের দৃষ্টি এড়াইয়া জীর্ণদেহ বালক সহিত একটি অতি শীর্ণকায়া রমণী গৃহে প্রবেশ করিয়া ক্ষীণস্বরে কহিল বাবু দুটি খেতে দাও"]
lemma_df = nlu.load('bn.lemma').predict(text, output_level = "document")
lemma_df.lemma.values[0]
```

</div>

## Results

```bash
{'lemma': [Annotation(token, 0, 4, একদিন, {'sentence': '0'}),
Annotation(token, 6, 11, প্রাতঃ, {'sentence': '0'}),
Annotation(token, 13, 22, বৈদ্যনাথ, {'sentence': '0'}),
Annotation(token, 24, 35, মার্বলমণ্ডিত, {'sentence': '0'}),
Annotation(token, 37, 42, দালান, {'sentence': '0'}),
Annotation(token, 44, 47, এক, {'sentence': '0'}),
Annotation(token, 49, 56, স্থূলউদর, {'sentence': '0'}),
Annotation(token, 58, 66, সন্ন্যাসী, {'sentence': '0'}),
Annotation(token, 68, 73, দুইসের, {'sentence': '0'}),
Annotation(token, 75, 81, মোহনভোগ, {'sentence': '0'}),
Annotation(token, 83, 85, এবং, {'sentence': '0'}),
Annotation(token, 87, 93, দেড়সের, {'sentence': '0'}),
Annotation(token, 95, 99, দুগ্ধ, {'sentence': '0'}),
Annotation(token, 101, 105, সেবা, {'sentence': '0'}),
Annotation(token, 107, 113, নিযুক্ত, {'sentence': '0'}),
Annotation(token, 115, 117, আছে, {'sentence': '0'}),
Annotation(token, 119, 126, বৈদ্যনাথ, {'sentence': '0'}),
Annotation(token, 128, 131, গা, {'sentence': '0'}),
Annotation(token, 133, 138, একখান, {'sentence': '0'}),
Annotation(token, 140, 143, চাদর, {'sentence': '0'}),
Annotation(token, 145, 148, দেওয়া, {'sentence': '0'}),
Annotation(token, 150, 156, জোড়কর, {'sentence': '0'}),
Annotation(token, 158, 163, একান্ত, {'sentence': '0'}),
Annotation(token, 165, 173, বিনীতভাব, {'sentence': '0'}),
Annotation(token, 175, 179, ভূতল, {'sentence': '0'}),
Annotation(token, 181, 185, বসা, {'sentence': '0'}),
Annotation(token, 187, 194, ভক্তিভরা, {'sentence': '0'}),
Annotation(token, 196, 201, পবিত্র, {'sentence': '0'}),
Annotation(token, 203, 213, ভোজনব্যাপার, {'sentence': '0'}),
Annotation(token, 215, 222, নিরীক্ষণ, {'sentence': '0'}),
Annotation(token, 224, 233, করা, {'sentence': '0'}),
Annotation(token, 235, 237, এমন, {'sentence': '0'}),
Annotation(token, 239, 241, সময়, {'sentence': '0'}),
Annotation(token, 243, 249, কোনোমত, {'sentence': '0'}),
Annotation(token, 251, 259, দ্বারী, {'sentence': '0'}),
Annotation(token, 261, 266, দৃষ্টি, {'sentence': '0'}),
Annotation(token, 268, 274, এড়ানো, {'sentence': '0'}),
Annotation(token, 276, 283, জীর্ণদেহ, {'sentence': '0'}),
Annotation(token, 285, 288, বালক, {'sentence': '0'}),
Annotation(token, 290, 293, সহিত, {'sentence': '0'}),
Annotation(token, 295, 298, এক, {'sentence': '0'}),
Annotation(token, 300, 302, অতি, {'sentence': '0'}),
Annotation(token, 304, 312, শীর্ণকায়া, {'sentence': '0'}),
Annotation(token, 314, 317, রমণী, {'sentence': '0'}),
Annotation(token, 319, 322, গৃহ, {'sentence': '0'}),
Annotation(token, 324, 329, প্রবেশ, {'sentence': '0'}),
Annotation(token, 331, 335, বিশ্বাস, {'sentence': '0'}),
Annotation(token, 337, 346, ক্ষীণস্বর, {'sentence': '0'}),
Annotation(token, 348, 351, কহা, {'sentence': '0'}),
Annotation(token, 353, 356, বাবু, {'sentence': '0'}),
Annotation(token, 358, 361, দুই, {'sentence': '0'}),
Annotation(token, 363, 366, খাওয়া, {'sentence': '0'}),
Annotation(token, 368, 370, দাওয়া, {'sentence': '0'})]}
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|lemma|
|Compatibility:|Spark NLP 2.7.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[document]|
|Output Labels:|[token]|
|Language:|bn|

## Data Source

The model was trained on the annotated Bengali data set from the [Indian Statistics Institute](https://www.isical.ac.in).

Reference:

- A. Chakrabarty, O.A. Pandit, and U. Garain (2017): Context Sensitive Lemmatization Using Two Successive Bidirectional Gated Recurrent Networks, in ACL 2017:1481-1491.