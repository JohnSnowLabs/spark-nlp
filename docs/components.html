<!DOCTYPE html>
<!--[if IE 8]>
<html lang="en" class="ie8"> <![endif]-->
<!--[if IE 9]>
<html lang="en" class="ie9"> <![endif]-->
<!--[if !IE]><!-->
<html lang="en"> <!--<![endif]-->
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-70312582-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-70312582-2');
    </script>
    <title>Spark NLP - Documentation and Reference</title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="JOSE" >
    <link rel="shortcut icon" href="fav.ico">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800'
          rel='stylesheet' type='text/css'>
    <!-- Global CSS -->
    <link rel="stylesheet" href="assets/plugins/bootstrap/css/bootstrap.min.css">
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="assets/plugins/font-awesome/css/font-awesome.css">
    <link rel="stylesheet" href="assets/plugins/prism/prism.css">
    <link rel="stylesheet" href="assets/plugins/lightbox/dist/ekko-lightbox.min.css">
    <link rel="stylesheet" href="assets/plugins/elegant_font/css/style.css">

    <!-- Theme CSS -->
    <link id="theme-style" rel="stylesheet" href="assets/css/styles.css">
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!--   <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css" rel="stylesheet"/>
       <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
       <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
   -->
    <script type="text/javascript" src="assets/plugins/jquery-1.12.3.min.js"></script>
    <script>
        function getTasks() {
            $.get("https://api.github.com/repos/JohnSnowLabs/spark-nlp/commits?path=docs/components.html",
                    function (data) {
                        var dateObject = new Date(data[0].commit.author.date);
                        $(".wrapper").html(dateObject.toDateString());
                    });
        }
        getTasks();
    </script>
</head>

<body class="body-pink">
<script>
    $(function () {
        $("#includedHeader").load("header.html");
        $("#includedFooter").load("footer.html");
    });
</script>
<div class="page-wrapper">
    <!-- ******Header****** -->
    <header id="header" class="header">
        <div class="container">
            <div id="includedHeader"></div>
            <ol class="breadcrumb">
                <li><a href="index.html">Home</a></li>
                <li class="active">Documentation and Reference</li>
            </ol>
        </div>
    </header>
    <div style="border:1px solid #e7e7e7;"></div>
    <div class="doc-wrapper">
        <div class="container">
            <div id="doc-header" class="doc-header text-center">
                <h1 class="doc-title"><span aria-hidden="true" class="icon icon_puzzle_alt"></span> SparkNLP - Documentation and Reference
                </h1>
                <div class="meta"><i class="fa fa-clock-o"></i> Last updated: <span class="wrapper"></span></div>
            </div><!--//doc-header-->
            <div class="doc-body">
                <div class="doc-content">
                    <div class="content-inner">
                        <section id="code-section" class="doc-section" style="padding-left: 20px">
                            <h2 class="section-title" id="concepts">Concepts</h2>
                            <div>
                                <h4 id="Imports" class="section-block"> Spark NLP Imports: Getting ready to work</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Since version 1.5.0 we are making necessary imports easy to reach, <b>base</b>
                                                will include general Spark NLP transformers and concepts, while <b>annotator</b> will include
                                                all annotators that we currently provide. We also need SparkML pipelines.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.base import *
from sparknlp.annotator import *</code></pre>
                                        </div>
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Since version 1.5.0 we are making necessary imports easy to reach, <b>base._</b>
                                                will include general Spark NLP transformers and concepts, while <b>annotator._</b> will include
                                                all annotators that we currently provide. We also need SparkML pipelines.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.base._
import com.johnsnowlabs.nlp.annotator._</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="MLPipeline" class="section-block"> Spark ML Pipelines</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                SparkML Pipelines are a uniform structure that helps creating and tuning practical machine learning pipelines.
                                                Spark NLP integrates with them seamlessly so it is important to have this concept handy.
                                                Once a <b>Pipeline</b> is trained with <b>fit()</b>, this becomes a <b>PipelineModel</b>
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from pyspark.ml import Pipeline
pipeline = Pipeline().setStages([...])</code></pre>
                                        </div>
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                SparkML Pipelines are a uniform structure that helps creating and tuning practical machine learning pipelines.
                                                Spark NLP integrates with them seamlessly so it is important to have this concept handy.
                                                Once a <b>Pipeline</b> is trained with <b>fit()</b>, this becomes a <b>PipelineModel</b>
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import org.apache.spark.ml.Pipeline
new Pipeline().setStages(Array(...))</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="LightPipeline" class="section-block"> LightPipeline: A super-fast Spark-NLP pipeline for small data</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                LightPipelines are Spark ML pipelines converted into a single
                                                machine but multithreaded task, becoming more than 10x times
                                                faster for smaller amounts of data (50k lines of text or below).
                                                To use them, simply plug in a trained (fitted) pipeline.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.base import LightPipeline
LightPipeline(someTrainedPipeline).annotate(someStringOrArray)</code></pre>
                                            <b>Functions:</b>
                                            <ul>
                                                <li>
                                                    annotate(string or string[]): returns dictionary list of annotation results
                                                </li>
                                                <li>
                                                    fullAnnotate(string or string[]): returns dictionary list of entire annotations content
                                                </li>
                                            </ul>
                                        </div>
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                LightPipelines are Spark ML pipelines converted into a single
                                                machine but multithreaded task, becoming more than 10x times
                                                faster for smaller amounts of data (50k lines of text or below).
                                                To use them, simply plug in a trained (fitted) pipeline.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.LightPipeline
new LightPipeline(somePipelineModel).annotate(someStringOrArray))</code></pre>
                                            <b>Functions:</b>
                                            <ul>
                                                <li>
                                                    annotate(string or string[]): returns dictionary list of annotation results
                                                </li>
                                                <li>
                                                    fullAnnotate(string or string[]): returns dictionary list of entire annotations content
                                                </li>
                                            </ul>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="RecursivePipeline" class="section-block"> RecursivePipeline: A smarter Spark-NLP pipeline</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Recursive pipelines are SparkNLP specific pipelines that
                                                allow a Spark ML Pipeline to know about itself on every
                                                Pipeline Stage task, allowing annotators to utilize this
                                                same pipeline against external resources to process them in the same way
                                                the user decides. Only some of our annotators take advantage of this.
                                                RecursivePipeline behaves exactly the same than normal Spark ML pipelines,
                                                so they can be used with the same intention.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.annotator import *
recursivePipeline = RecursivePipeline(stages=[
        documentAssembler,
        sentenceDetector,
        tokenizer,
        lemmatizer,
        finisher
      ])</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Recursive pipelines are SparkNLP specific pipelines that
                                                allow a Spark ML Pipeline to know about itself on every
                                                Pipeline Stage task, allowing annotators to utilize to use this
                                                same pipeline against external resources to process them in the same way
                                                the user decides. Only some of our annotators take advantage of this.
                                                RecursivePipeline behaves exactly the same than normal Spark ML pipelines,
                                                so they can be used with the same intention.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.RecursivePipeline
val recursivePipeline = new RecursivePipeline()
      .setStages(Array(
        documentAssembler,
        sentenceDetector,
        tokenizer,
        lemmatizer,
        finisher
      ))</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="ExternalResource" class="section-block"> ExternalResource: Data properties outside the pipeline</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                ExternalResource represent the properties of external data to be read,
                                                usually by the ResourceHelper (which is explained below). It contains
                                                information into how such external source may be read, and allows
                                                different protocols (hdfs, s3, etc) and formats (csv, text, parquet, etc).
                                                User does not usually need to create explicitly External Resources, but function
                                                parameters usually ask for elements used by it.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">regex_matcher = RegexMatcher() \
    .setStrategy("MATCH_ALL") \
    .setExternalRules(path="/some/path", delimiter=",", read_as=ReadAs.LINE_BY_LINE, options={"format": "parquet"}) \
    .setOutputCol("regex")</code></pre>
                                            <b>Arguments:</b>
                                            <ul>
                                                <li>
                                                    path -> Takes a path with protocol of desintation file or folder
                                                </li>
                                                <li>
                                                    ReadAs -> "LINE_BY_LINE" or "SPARK_DATASET" will tell SparkNLP to use spark or not for this file or folder
                                                </li>
                                                <li>
                                                    options -> Contains information passed to Spark reader (e.g. format: "text") and other useful options for annotators (e.g. delimiter)
                                                </li>
                                            </ul>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                ExternalResource represent the properties of external data to be read,
                                                usually by the ResourceHelper (which is explained below). It contains
                                                information into how such external source may be read, and allows
                                                different protocols (hdfs, s3, etc) and formats (csv, text, parquet, etc).
                                                User does not usually need to create explicitly External Resources, but function
                                                parameters usually ask for elements used by it.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">posTagger = new PerceptronApproach()
    .setInputCols(Array("sentence", "token"))
    .setOutputCol("pos")
    .setNIterations(1)
    .setCorpus(ExternalResource("/some/path/110CYL067.txt", ReadAs.LINE_BY_LINE, Map("delimiter" -> "|"))))</code></pre>

                                            <b>Arguments:</b>
                                            <ul>
                                                <li>
                                                    path -> Takes a path with protocol of desintation file or folder
                                                </li>
                                                <li>
                                                    ReadAs -> "LINE_BY_LINE" or "SPARK_DATASET" will tell SparkNLP to use spark or not for this file or folder
                                                </li>
                                                <li>
                                                    options -> Contains information passed to Spark reader (e.g. format: "text") and other useful options for annotators (e.g. delimiter)
                                                </li>
                                            </ul>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="ResourceHelper" class="section-block"> ResourceHelper: Deal with data outside the pipeline</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                When working with external resources, like training data that is not
                                                part of the pipeline process, our annotators use the ResourceHelper
                                                to efficiently parse and extract data into specific formats. This
                                                class may be utilized for other purposes by the user (Only in Scala)
                                            </p>
                                        </div><!--//code-block--></div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                When working with external resources, like training data that is not
                                                part of the pipeline process, our annotators use the ResourceHelper
                                                to efficiently parse and extract data into specific formats. This
                                                class may be utilized for other purposes by the user.
                                            </p>
                                            <b>Functions (not all of them listed):</b>
                                            <ul>
                                                <li>
                                                    createDatasetFromText(path, includeFilename, includeRowNumber, aggregateByFile) -> Takes file or folder and builds up an aggregated dataset
                                                </li>
                                                <li>
                                                    parseKeyValueText(externalResource) -> Parses delimited text with delimiter
                                                </li>
                                                <li>
                                                    parseLines(externalResource) -> Parses line by line text
                                                </li>
                                                <li>
                                                    parseTupleText(externalResource) -> Parses a text as a delimited tuple
                                                </li>
                                                <li>
                                                    parseTupleSentences(externalResource) -> Parses tagged tokens with a specific delimiter
                                                </li>
                                                <li>
                                                    wordCount(externalResources) -> Counts appearance of each word in text
                                                </li>
                                            </ul>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="EmbeddingsHelper" class="section-block"> EmbeddingsHelper: Deal with word embeddings</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Allows loading, saving and setting word embeddings for annotators
                                            </p>
                                            <b>Functions (not all of them listed):</b>
                                            <ul>
                                                <li>
                                                    load(path, spark, format, reference, dims, caseSensitive) -> Loads embeddings from disk in any format possible: 'TEXT', 'BINARY', 'SPARKNLP'. Makes embeddings available for Annotators without included embeddings.
                                                </li>
                                                <li>
                                                    save(path, embeddings, spark) -> Saves provided embeddings to path, using current SparkSession
                                                </li>
                                            </ul>
                                        </div><!--//code-block--></div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Allows loading, saving and setting word embeddings for annotators
                                            </p>
                                            <b>Functions (not all of them listed):</b>
                                            <ul>
                                                <li>
                                                    load(path, spark, format, reference, dims, caseSensitive) -> Loads embeddings from disk in any format possible: 'TEXT', 'BINARY', 'SPARKNLP'. Makes embeddings available for Annotators without included embeddings.
                                                </li>
                                                <li>
                                                    save(path, embeddings, spark) -> Saves provided embeddings to path, using current SparkSession
                                                </li>
                                            </ul>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="AnnotatorEmbeddings" class="section-block"> Annotator with Word Embeddings</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Some annotators use word embeddings. This is a common functionality within them.
                                            </p>
                                            <b>Functions (not all of them listed):</b>
                                            <ul>
                                                <li>
                                                    setIncludeEmbeddings(bool) -> Param to define whether or not to include word embeddings when saving this annotator to disk (single or within pipeline)
                                                </li>
                                                <li>
                                                    setEmbeddingsRef(ref) -> Set whether to use annotators under the provided name. This means these embeddings will be lookup from the cache by the ref name. This allows multiple annotators to utilize same word embeddings by ref name.
                                                </li>
                                            </ul>
                                        </div><!--//code-block--></div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Some annotators use word embeddings. This is a common functionality within them.
                                            </p>
                                            <b>Functions (not all of them listed):</b>
                                            <ul>
                                                <li>
                                                    setIncludeEmbeddings(bool) -> Param to define whether or not to include word embeddings when saving this annotator to disk (single or within pipeline)
                                                </li>
                                                <li>
                                                    setEmbeddingsRef(ref) -> Set whether to use annotators under the provided name. This means these embeddings will be lookup from the cache by the ref name. This allows multiple annotators to utilize same word embeddings by ref name.
                                                </li>
                                            </ul>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="ParamsFeatures" class="section-block"> Params and Features: Annotator parameters</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                SparkML uses ML Params to store pipeline parameter maps. In SparkNLP,
                                                we also use Features, which are a way to store parameter maps that
                                                are larger than just a string or a boolean. These features are
                                                serialized as either Parquet or RDD objects, allowing much faster
                                                and scalable annotator information. Features are also broadcasted
                                                among executors for better performance.
                                                <br><b>Example:</b><br>
                                            </p>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                SparkML uses ML Params to store pipeline parameter maps. In SparkNLP,
                                                we also use Features, which are a way to store parameter maps that
                                                are larger than just a string or a boolean. These features are
                                                serialized as either Parquet or RDD objects, allowing much faster
                                                and scalable annotator information. Features are also broadcasted
                                                among executors for better performance.
                                                <br><b>Example:</b><br>
                                            </p>
                                        </div><!--//code-block--></div>
                                </div>
                            </div>
                            <h2 class="section-title" id="pipelines">Pretrained Pipelines</h2>
                            <div>
                                <h4 id="BasicPipeline" class="section-block"> BasicPipeline: common NLP annotations</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                BasicPipeline will easily return to you tokens, normalized tokens,
                                                lemmas and part of speech tags. It can take either a Spark dataset
                                                or a string or array of strings (LightPipelines behind). It will
                                                require internet connection to download it from our servers.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.pretrained.pipeline.en import BasicPipeline
#Annotate with pipeline
BasicPipeline().annotate("Please parse this sentence. Thanks")
BasicPipeline().annotate(["This is a first sentence", "This is another one"])
BasicPipeline().annotate(dataset, "textColumn")
#Just get the pipeline
BasicPipeline().pretrained()</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                BasicPipeline will easily return to you tokens, normalized tokens,
                                                lemmas and part of speech tags. It can take either a Spark dataset
                                                or a string or array of strings (LightPipelines behind). It will
                                                require internet connection to download it from our servers.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.pretrained.pipelines.en.BasicPipeline
//Annotate with pipeline
BasicPipeline().annotate("Please parse this sentence. Thanks")
BasicPipeline().annotate(["This is a first sentence", "This is another one"])
BasicPipeline().annotate(dataset, "textColumn")
//Just get the pipeline
BasicPipeline().pretrained()</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="AdvancedPipeline" class="section-block"> AdvancedPipeline: All NLP annotations</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Advanced pipelines will return the same than the BasicPipeline, plus
                                                Stems, Spell Checked tokens and NER entities using the CRF model.
                                                It requires an internet connection to download it from our servers.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.pretrained.pipeline.en import AdvancedPipeline
#Annotate with pipeline
AdvancedPipeline().annotate("Please parse this sentence. Thanks")
AdvancedPipeline().annotate(["This is a first sentence", "This is another one"])
AdvancedPipeline().annotate(dataset, "textColumn")
#Just get the pipeline
AdvancedPipeline().pretrained()</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Advanced pipelines will return the same than the BasicPipeline, plus
                                                Stems, Spell Checked tokens and NER entities using the CRF model.
                                                It requires an internet connection to download it from our servers.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.pretrained.pipelines.en.AdvancedPipeline
//Annotate with pipeline
AdvancedPipeline().annotate("Please parse this sentence. Thanks")
AdvancedPipeline().annotate(["This is a first sentence", "This is another one"])
AdvancedPipeline().annotate(dataset, "textColumn")
//Just get the pipeline
AdvancedPipeline().pretrained()</code></pre>
                                </div><!--//code-block--></div>
                                </div>
                                <h4 id="SentimentPipeline" class="section-block"> SentimentPipeline: Vivekn Sentiment analysis</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                This pipeline takes a dataset or text or array of text and computes
                                                sentiment analysis with spell checking included
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.pretrained.pipeline.en import SentimentPipeline
#Annotate with pipeline
AdvancedPipeline().annotate("Please parse this sentence. Thanks")
AdvancedPipeline().annotate(["This is a first sentence", "This is another one"])
AdvancedPipeline().annotate(dataset, "textColumn")
#Just get the pipeline
SentimentPipeline().pretrained()</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                This pipeline takes a dataset or text or array of text and computes
                                                sentiment analysis with spell checking included
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.pretrained.pipelines.en.SentimentPipeline
//Annotate with pipeline
SentimentPipeline().annotate(data, "textColumn")
//Just get the pipeline
SentimentPipeline().pretrained()</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                            </div>
                            <h2 class="section-title" id="models">Pretrained Models</h2>
                            <div>
                                <h4 id="LemmaFast" class="section-block"> Fast lemmatizer</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Lemmatizer trained with AntBNC Free corpus
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.annotator import LemmatizerModel
LemmatizerModel.pretrained()</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Lemmatizer trained with AntBNC Free corpus
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.annotator.LemmatizerModel
LemmatizerModel.pretrained()</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="SpellFast" class="section-block"> Fast Norvig Spell Checker</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Spell Checker trained with Wikipedia corpus
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.annotator import NorvigSweetingModel
NorvigSweetingModel.pretrained()</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Spell Checker trained with Wikipedia corpus
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.annotator.NorvigSweetingModel
NorvigSweetingModel.pretrained()</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="SymmSpellFast" class="section-block"> Fast SymmDelete Spell Checker</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Symmetric delete Spell Checker trained with Wikipedia corpus
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.annotator import SymmetricDeleteModel
SymmetricDeleteModel.pretrained()</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Symmetric delete Spell Checker trained with Wikipedia corpus
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.annotator.SymmetricDeleteModel
SymmetricDeleteModel.pretrained()</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="ContextSpell" class="section-block"> Contextual Spell Checker</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Context Spell Checker trained on Gutenberg project books.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.annotator import ContextSpellCheckerModel
ContextSpellCheckerModel.pretrained()</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Context Spell Checker trained on Gutenberg project books.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.annotator.ContextSpellCheckerModel
ContextSpellCheckerModel.pretrained()</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="PosFast" class="section-block"> Fast Part of Speech</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Part of Speech trained with ANC American Corpus
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.annotator import PerceptronModel
PerceptronModel.pretrained()</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Part of Speech trained with ANC American Corpus
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.annotator.PerceptronModel
PerceptronModel.pretrained()</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="NerFast" class="section-block"> Fast CRF Named Entity Recognition</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Named entity recognition model trained with Glove embeddings
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.annotator import NerCrfModel
NerCrfModel.pretrained()
</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Named entity recognition model trained with Glove embeddings
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.annotator.NerCrfModel
NerCrfModel.pretrained()</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="FastNerDL" class="section-block"> Fast Deep Learning NER</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Named entity recognition deep learning model trained with Glove embeddings
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">from sparknlp.annotator import NerDLModel
NerDLModel.pretrained()
</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Named entity recognition deep learning model trained with Glove embeddings
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">import com.johnsnowlabs.nlp.annotator.NerDLModel
NerDLModel.pretrained()</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                            </div>
                            <h2 class="section-title" id="transformers">Transformers</h2>
                            <div>
                              <h4 id="DocumentAssembler" class="section-block"> DocumentAssembler: Getting data
                                in</h4>
                              <ul class="nav nav-tabs" role="tablist">
                                <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                          role="tab" data-toggle="tab">Python</a>
                                </li>
                                <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                           data-toggle="tab">Scala</a></li>
                              </ul>
                              <div class="tab-content">
                                <div role="tabpanel" class="tab-pane active" id="python">
                                  <div class="code-block">
                                    <p>

                                      In order to get through the NLP process, we need to get raw data
                                      annotated. There is a special transformer that does this for us:
                                        it creates the first annotation of type Document
                                      which may be used by annotators down the road. It can read either
                                        a String column or an Array[String]
                                      <br><b>Example:</b><br>
                                    </p>
                                    <pre><code class="language-python">from sparknlp.annotator import *
from sparknlp.common import *
from sparknlp.base import *
from pyspark.ml import Pipeline
documentAssembler = new DocumentAssembler() \
.setInputCol("text") \
.setOutputCol("document")</code></pre>

                                    <b>Settable parameters are:</b>
                                    <ul>
                                      <li>
                                        setInputCol()
                                      </li>
                                      <li>
                                        setOutputCol()
                                      </li>
                                      <li>
                                        setIdCol() -> OPTIONAL: Sring type column with id information
                                      </li>
                                      <li>
                                        setMetadataCol() -> OPTIONAL: Map type column with metadata
                                        information
                                      </li>
                                        <li>
                                            setTrimAndClearNewLines(bool) -> Whether to remove new line characters and trim strings. Defaults to true. Useful for later sentence detection if contains multiple lines.
                                        </li>
                                    </ul>
                                  </div><!--//code-block-->
                                </div>
                                <div role="tabpanel" class="tab-pane" id="scala">
                                  <div class="code-block">
                                    <p>

                                        In order to get through the NLP process, we need to get raw data
                                        annotated. There is a special transformer that does this for us:
                                        it creates the first annotation of type Document
                                        which may be used by annotators down the road. It can read either
                                        a String column or an Array[String]
                                      <br><b>Example:</b><br>
                                    </p>
                                    <pre><code class="language-javascript">import com.johnsnowlabs.nlp._
import com.johnsnowlabs.nlp.annotators._
import org.apache.spark.ml.Pipeline
documentAssembler = new DocumentAssembler()
.setInputCol("text")
.setOutputCol("document")</code></pre>

                                    <b>Settable parameters are:</b>
                                    <ul>
                                      <li>
                                        setInputCol()
                                      </li>
                                      <li>
                                        setOutputCol()
                                      </li>
                                      <li>
                                        setIdCol() -> OPTIONAL: Sring type column with id information
                                      </li>
                                      <li>
                                        setMetadataCol() -> OPTIONAL: Map type column with metadata
                                        information
                                      </li>
                                        <li>
                                            setTrimAndClearNewLines(bool) -> Whether to remove new line characters and trim strings. Defaults to true. Useful for later sentence detection if contains multiple lines.
                                        </li>
                                    </ul>
                                  </div><!--//code-block--></div>
                              </div>
                              <h4 id="TokenAssembler" class="section-block"> TokenAssembler: Getting data reshaped </h4>
                              <ul class="nav nav-tabs" role="tablist">
                                <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                          role="tab" data-toggle="tab">Python</a>
                                </li>
                                <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                           data-toggle="tab">Scala</a></li>
                              </ul>
                              <div class="tab-content">
                                <div role="tabpanel" class="tab-pane active" id="python">
                                  <div class="code-block">
                                    <p>


                                      This transformer reconstructs a Document type annotation from tokens, usually after these
                                      have been normalized, lemmatized, normalized, spell checked, etc, in order to use this
                                      document annotation in further annotators.
                                      <br><b>Example:</b><br>
                                    </p>
                                    <pre><code class="language-python">token_assembler = TokenAssembler() \
  .setInputCols(["normalized"]) \
  .setOutputCol("assembled")
</code></pre>

                                    <b>Settable parameters are:</b>
                                    <ul>
                                      <li>
                                        setInputCol()
                                      </li>
                                      <li>
                                        setOutputCol()
                                      </li>
                                    </ul>
                                  </div><!--//code-block-->
                                </div>
                                <div role="tabpanel" class="tab-pane" id="scala">
                                  <div class="code-block">
                                    <p>


                                      This transformer reconstructs a Document type annotation from tokens, usually after these
                                      have been normalized, lemmatized, spell checked, etc, in order to use this
                                      document annotation in further annotators.
                                      <br><b>Example:</b><br>
                                    </p>
                                    <pre><code class="language-javascript">token_assembler = TokenAssembler()
  .setInputCols("normalized")
  .setOutputCol("assembled")
</code></pre>

                                    <b>Settable parameters are:</b>
                                    <ul>
                                      <li>
                                        setInputCol()
                                      </li>
                                      <li>
                                        setOutputCol()
                                      </li>
                                    </ul>
                                  </div><!--//code-block--></div>
                              </div>
                                <h4 id="Doc2Chunk" class="section-block"> Doc2Chunk</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol.
                                                Chunk text must be contained within input DOCUMENT. May be either StringType or ArrayType[StringType] (using isArray Param)
                                                Useful for annotators that require a CHUNK type input.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">chunker = Doc2Chunk()\
  .setInputCols(["document"])\
  .setOutputCol("chunk")\
  .setIsArray(False)\
  .setChunkCol("some_column")
</code></pre>

                                            <b>Settable parameters are:</b>
                                            <ul>
                                                <li>
                                                    setInputCol()
                                                </li>
                                                <li>
                                                    setOutputCol()
                                                </li>
                                                <li>
                                                    setIsArray()
                                                </li>
                                                <li>
                                                    setChunkCol
                                                </li>
                                            </ul>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Converts document type annotations into CHUNK type with the contents of a chunkCol.
                                                Chunk text must be contained within input DOCUMENT. May be either StringType or ArrayType[StringType] (using isArray Param)
                                                Useful for annotators that require a CHUNK type input.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val chunker = new Doc2Chunk()
  .setInputCols("document")
  .setOutputCol("chunk")
  .setIsArray(false)
  .setChunkCol("some_column")
</code></pre>

                                            <b>Settable parameters are:</b>
                                            <ul>
                                                <li>
                                                    setInputCol()
                                                </li>
                                                <li>
                                                    setOutputCol()
                                                </li>
                                                <li>
                                                    setIsArray()
                                                </li>
                                                <li>
                                                    setChunkCol
                                                </li>
                                            </ul>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="Chunk2Doc" class="section-block"> Chunk2Doc</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">chunk_doc = Chunk2Doc()\
  .setInputCols(["chunk_output"])\
  .setOutputCol("new_document")\
</code></pre>

                                            <b>Settable parameters are:</b>
                                            <ul>
                                                <li>
                                                    setInputCol()
                                                </li>
                                                <li>
                                                    setOutputCol()
                                                </li>
                                            </ul>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result.
                                                <br><b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val chunk_doc = new Chunk2Doc()
  .setInputCols("chunk_output")
  .setOutputCol("new_document")
</code></pre>

                                            <b>Settable parameters are:</b>
                                            <ul>
                                                <li>
                                                    setInputCol()
                                                </li>
                                                <li>
                                                    setOutputCol()
                                                </li>
                                            </ul>
                                        </div><!--//code-block--></div>
                                </div>
                              <h4 id="Finisher" class="section-block"> Finisher: Getting data out </h4>
                              <ul class="nav nav-tabs" role="tablist">
                                <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                          role="tab" data-toggle="tab">Python</a>
                                </li>
                                <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                           data-toggle="tab">Scala</a></li>
                              </ul>
                              <div class="tab-content">
                                <div role="tabpanel" class="tab-pane active" id="python">
                                  <div class="code-block">
                                    <p>
                                      Once we have our NLP pipeline ready to go, we might want to use our
                                      annotation results somewhere else where it is easy to use. The Finisher
                                      outputs annotation(s) values into string.
                                      <br><b>Example:</b><br>
                                    </p>
                                    <pre><code class="language-python">finisher = Finisher() \
  .setInputCols(["sentiment"]) \
  .setIncludeMetadata(True)
</code></pre>

                                    <b>Settable parameters are:</b>
                                    <ul>
                                      <li>
                                        setInputCols()
                                      </li>
                                      <li>
                                        setOutputCols()
                                      </li>
                                      <li>
                                        setCleanAnnotations(True) -> Whether to remove intermediate
                                        annotations
                                      </li>
                                      <li>
                                        setValueSplitSymbol("#") -> split values within an annotation
                                        character
                                      </li>
                                      <li>
                                        setAnnotationSplitSymbol("@") -> split values between annotations
                                        character
                                      </li>
                                      <li>
                                        setIncludeMetadata(False) -> Whether to include metadata keys. Sometimes
                                        useful in some annotations
                                      </li>
                                      <li>
                                        setOutputAsArray(False) -> Whether to output as Array. Useful as input for other Spark transformers.
                                      </li>
                                    </ul>
                                  </div><!--//code-block-->
                                </div>
                                <div role="tabpanel" class="tab-pane" id="scala">
                                  <div class="code-block">
                                    <p>


                                      Once we have our NLP pipeline ready to go, we might want to use our
                                      annotation results somewhere else where it is easy to use. The Finisher
                                      outputs annotation(s) values into string.
                                      <br><b>Example:</b><br>
                                    </p>
                                    <pre><code class="language-javascript">val finisher = new Finisher()
  .setInputCols("token")
  .setIncludeMetadata(true)
</code></pre>

                                    <b>Settable parameters are:</b>
                                    <ul>
                                      <li>
                                        setInputCols()
                                      </li>
                                      <li>
                                        setOutputCols()
                                      </li>
                                      <li>
                                        setCleanAnnotations(true) -> Whether to remove intermediate
                                        annotations
                                      </li>
                                      <li>
                                        setValueSplitSymbol("#") -> split values within an annotation
                                        character
                                      </li>
                                      <li>
                                        setAnnotationSplitSymbol("@") -> split values between annotations
                                        character
                                      </li>
                                        <li>
                                            setIncludeMetadata(False) -> Whether to include metadata keys. Sometimes
                                            useful in some annotations
                                        </li>
                                      <li>
                                        setOutputAsArray(false) -> Whether to output as Array. Useful as input for other Spark transformers.
                                      </li>
                                    </ul>
                                  </div><!--//code-block--></div>
                              </div>
                            </div>
                            <h2 class="section-title" id="annotators">Annotators</h2>
                            <div>
                                <h4 id="Tokenizer" class="section-block">Tokenizer: Word tokens</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs.<br>
                                                <b>Type:</b> Token<br>
                                                <b>Requires:</b> Document<br>
                                                <b>Functions:</b>
                                            <ul>
                                          <li>
                                            setTargetPattern: Basic regex rule to identify a candidate for tokenization. Defaults to \S+ which means anything not a space
                                          </li>
                                          <li>
                                            setSuffixPattern: Regex to identify subtokens that are in the end of the token. Regex has to end with \z and must contain groups (). Each group will become a separate token within the prefix. Defaults to non-letter characters. e.g. quotes or parenthesis
                                          </li>
                                          <li>
                                            setPrefixPattern: Regex to identify subtokens that come in the beginning of the token. Regex has to start with \A and must contain groups (). Each group will become a separate token within the prefix. Defaults to non-letter characters. e.g. quotes or parenthesis
                                          </li>
                                          <li>
                                            setExtensionPatterns: Array of Regex with groups () to match subtokens within the target pattern. Every group () will become its own separate token. Order matters (later rules will apply first). Its default rules should cover most cases, e.g. part-of-speech as single token
                                          </li>
                                          <li>
                                            addInfixPattern: Add an extension pattern regex with groups to the top of the rules (will target first, from more specific to the more general).
                                          </li>
                                          <li>
                                            setCompositeTokensPatterns: Adds a list of compound words to mark for ignore. E.g., adding "New York" so it doesn't get split into "New" and "York".
                                          </li>
                                            </ul>
<b>Note:</b> all these APIs receive regular expressions so please make sure that you escape special characters according to Java conventions.
                                            <br>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">tokenizer = Tokenizer() \
  .setInputCols(["sentences"]) \
  .setOutputCol("token") \
  .addInfixPattern("(\p{L}+)(n't\b)")</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                          <p>
                                            Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs.<br>
                                            <b>Type:</b> Token<br>
                                            <b>Requires:</b> Document<br>
                                            <b>Functions:</b>
                                          <ul>
                                            <li>
                                              setTargetPattern: Basic regex rule to identify a candidate for tokenization. Defaults to \S+ which means anything not a space
                                            </li>
                                            <li>
                                              setSuffixPattern: Regex to identify subtokens that are in the end of the token. Regex has to end with \z and must contain groups (). Each group will become a separate token within the prefix. Defaults to non-letter characters. e.g. quotes or parenthesis
                                            </li>
                                            <li>
                                              setPrefixPattern: Regex to identify subtokens that come in the beginning of the token. Regex has to start with \A and must contain groups (). Each group will become a separate token within the prefix. Defaults to non-letter characters. e.g. quotes or parenthesis
                                            </li>
                                            <li>
                                              setExtensionPatterns: Array of Regex with groups () to match subtokens within the target pattern. Every group () will become its own separate token. Order matters (later rules will apply first). Its default rules should cover most cases, e.g. part-of-speech as single token
                                            </li>
                                            <li>
                                              addInfixPattern: Add an extension pattern regex with groups to the top of the rules (will target first, from more specific to the more general).
                                            </li>
                                            <li>
                                              setCompositeTokensPatterns: Adds a list of compound words to mark for ignore. E.g., adding "New York" so it doesn't get split into "New" and "York".
                                            </li>
                                          </ul>
<b>Note:</b> all these APIs receive regular expressions so please make sure that you escape special characters according to Java conventions.
                                            <br>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val tokenizer = new Tokenizer()
  .setInputCols("sentence")
  .setOutputCol("token")
  .addInfixPattern("(\p{L}+)(n't\b)")</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                </div>
                                <h4 id="Normalizer" class="section-block">Normalizer: Text cleaning</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Removes all dirty characters from text following a regex pattern and
                                                transforms words based on a provided dictionary<br>
                                                <b>Type:</b> Token<br>
                                                <b>Requires:</b> Token<br>
                                                <b>Functions:</b>
                                            <ul>
                                              <li>
                                                setPatterns(patterns): Regular expressions list for normalization, defaults [^A-Za-z]
                                              </li>
                                              <li>
                                                setLowercase(value): lowercase tokens, default true
                                              </li>
                                                <li>
                                                    setSlangDictionary(path): txt file with delimited words to be transformed into something else
                                                </li>
                                            </ul>
                                                <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">normalizer = Normalizer() \
  .setInputCols(["token"]) \
  .setOutputCol("normalized")</code></pre>
                                        </div><!--//code-block-->

                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Removes all dirty characters from text following a regex pattern and
                                                transforms words based on a provided dictionary<br>
                                                <b>Type:</b> Token<br>
                                                <b>Requires:</b> Token<br>
                                                <b>Functions:</b>
                                            <ul>
                                                <li>
                                                    setPatterns(patterns): Regular expressions list for normalization, defaults [^A-Za-z]
                                                </li>
                                                <li>
                                                    setLowercase(value): lowercase tokens, default true
                                                </li>
                                                <li>
                                                    setSlangDictionary(path): txt file with delimited words to be transformed into something else
                                                </li>
                                            </ul>
                                                <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val normalizer = new Normalizer()
  .setInputCols(Array("token"))
  .setOutputCol("normalized")</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="Stemmer" class="section-block"> Stemmer: Hard stems</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Returns hard-stems out of words with the objective of retrieving the
                                                meaningful
                                                part of the word<br>
                                                <b>Type:</b> Token<br>
                                                <b>Requires:</b> Token<br>
                                                <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">stemmer = Stemmer() \
  .setInputCols(["token"]) \
  .setOutputCol("stem")</code></pre>
                                        </div>
                                    </div>
                                    <div role="tabpanel" class="tab-pane active" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Returns hard-stems out of words with the objective of retrieving the
                                                meaningful
                                                part of the word<br>
                                                <b>Type:</b> Token<br>
                                                <b>Requires:</b> Token<br>
                                                <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val stemmer = new Stemmer()
  .setInputCols(Array("token"))
  .setOutputCol("stem")</code></pre>
                                        </div>
                                    </div>
                                </div>
                                <h4 id="Lemmatizer" class="section-block"> Lemmatizer: Lemmas</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Retrieves lemmas out of words with the objective of returning a base
                                                dictionary
                                                word<br>
                                                <b>Type:</b> Token<br>
                                                <b>Requires:</b> Token<br>
                                                <b>Input:</b> abduct -> abducted abducting abduct abducts<br>
                                                <b>Functions:</b> --<br>
                                            <ul>
                                                <li>
                                                    setDictionary(path, keyDelimiter, valueDelimiter, readAs, options): Path and options to lemma dictionary, in lemma vs possible words format.
                                                    readAs can be LINE_BY_LINE or SPARK_DATASET. options contain option passed to spark reader if readAs is SPARK_DATASET.
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">lemmatizer = Lemmatizer() \
  .setInputCols(["token"]) \
  .setOutputCol("lemma") \
  .setDictionary("./lemmas001.txt")</code></pre>
                                        </div>
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Retrieves lemmas out of words with the objective of returning a base
                                                dictionary
                                                word<br>
                                                <b>Type:</b> Token<br>
                                                <b>Requires:</b> None<br>
                                                <b>Input:</b> abduct -> abducted abducting abduct abducts<br>
                                                <b>Functions:</b> --<br>
                                            <ul>
                                              <li>
                                                setDictionary(path, keyDelimiter, valueDelimiter, readAs, options): Path and options to lemma dictionary, in lemma vs possible words format.
                                                readAs can be LINE_BY_LINE or SPARK_DATASET. options contain option passed to spark reader if readAs is SPARK_DATASET.
                                              </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val lemmatizer = new Lemmatizer()
  .setInputCols(Array("token"))
  .setOutputCol("lemma")
  .setDictionary("./lemmas001.txt")</code></pre>
                                        </div>
                                    </div>
                                </div>
                                <h4 id="RegexMatcher" class="section-block"> RegexMatcher: Rule matching</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Uses a reference file to match a set of regular expressions and put them
                                                inside
                                                a provided key. File must be comma separated.<br>
                                                <b>Type:</b> Regex<br>
                                                <b>Requires:</b> Document<br>
                                                <b>Input:</b> "the\s\w+", "followed by 'the'"<br>
                                                <b>Functions:</b> <br>
                                            <ul>
                                                <li>
                                                    setStrategy(strategy): Can be any of
                                                    MATCH_FIRST|MATCH_ALL|MATCH_COMPLETE
                                                </li>
                                                <li>
                                                    setRulesPath(path, delimiter, readAs, options): Path to file containing a set of regex,key pair.
                                                    readAs can be LINE_BY_LINE or SPARK_DATASET. options contain option passed to spark reader if readAs is SPARK_DATASET.
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">regex_matcher = RegexMatcher() \
  .setStrategy("MATCH_ALL") \
  .setOutputCol("regex")</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Uses a reference file to match a set of regular expressions and put them
                                                inside
                                                a provided key. File must be comma separated.<br>
                                                <b>Type:</b> Regex<br>
                                                <b>Requires:</b> Document<br>
                                                <b>Input:</b> "the\s\w+", "followed by 'the'"<br>
                                                <b>Functions:</b> <br>
                                            <ul>
                                              <li>
                                                setStrategy(strategy): Can be any of
                                                MATCH_FIRST|MATCH_ALL|MATCH_COMPLETE
                                              </li>
                                              <li>
                                                setRulesPath(path, delimiter, readAs, options): Path to file containing a set of regex,key pair.
                                                readAs can be LINE_BY_LINE or SPARK_DATASET. options contain option passed to spark reader if readAs is SPARK_DATASET.
                                              </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val regexMatcher = new RegexMatcher()
  .setStrategy(strategy)
  .setInputCols(Array("document"))
  .setOutputCol("regex")</code></pre>
                                        </div>
                                    </div>
                                </div>

                                <h4 id="TextMatcher" class="section-block"> TextMatcher: Phrase matching</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Annotator to match entire phrases provided in a file against a
                                                Document<br>
                                                <b>Type:</b> Entity<br>
                                                <b>Requires:</b> Document<br>
                                                <b>Input:</b> hello world, I am looking for you<br>
                                                <b>Functions:</b> <br>
                                            <ul>
                                                <li>
                                                    setEntities(path, format, options): Provides a file with phrases to match. Default: Looks up path in configuration.</br>
                                                    path: a path to a file that contains the entities in the specified format.</br>
                                                    readAs: the format of the file, can be one of {ReadAs.LINE_BY_LINE, ReadAs.SPARK_DATASET}. Defaults to LINE_BY_LINE.</br>
                                                    options: a map of additional parameters. Defaults to {"format": "text"}.
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">entity_extractor = TextMatcher() \
 .setInputCols(["inputCol"])\
 .setOutputCol("entity")\
 .setEntities("/path/to/file/myentities.txt")
</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Annotator to match entire phrases provided in a file against a
                                                Document<br>
                                                <b>Type:</b> Entity<br>
                                                <b>Requires:</b> Document<br>
                                                <b>Input:</b> hello world, I am looking for you<br>
                                                <b>Functions:</b> <br>
                                            <ul>
                                              <li>
                                                setEntities(path, format, options): Provides a file with phrases to match. Default: Looks up path in configuration.</br>
                                                path: a path to a file that contains the entities in the specified format.</br>
                                                readAs: the format of the file, can be one of {ReadAs.LINE_BY_LINE, ReadAs.SPARK_DATASET}. Defaults to LINE_BY_LINE.</br>
                                                options: a map of additional parameters. Defaults to {"format": "text"}.
                                              </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val entityExtractor = new TextMatcher()
 .setInputCols("inputCol")
 .setOutputCol("entity")
 .setEntities("/path/to/file/myentities.txt")
</code></pre>
                                        </div>
                                    </div>
                                </div>

                                <h4 id="Chunker" class="section-block"> Chunker: Meaningful phrase matching</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                This annotator matches a pattern of part-of-speech tags in order to return meaningul phrases from document<br>
                                            </p>
                                                <b>Type:</b> Document<br>
                                                <b>Requires:</b> Document<br>
                                                <b>Functions:</b>
                                            <ul>
                                                <li>
                                                    setRegexParsers(patterns): A list of regex patterns to match chunks, for example: Array("&lsaquo;DT&rsaquo;?&lsaquo;JJ&rsaquo;*&lsaquo;NN&rsaquo;")
                                                </li>
                                                <li>
                                                    addRegexParser(patterns): adds a pattern to the current list of chunk patterns, for example: "&lsaquo;DT&rsaquo;?&lsaquo;JJ&rsaquo;*&lsaquo;NN&rsaquo;"
                                                </li>
                                            </ul>
                                            <b>Example:</b>
                                            <pre><code class="language-python">chunker = Chunker() \
    .setInputCols(["pos"]) \
    .setOutputCol("chunk") \
    .setRegexParsers(["&lsaquo;NNP&rsaquo;+", "&lsaquo;DT|PP\\$&rsaquo;?&lsaquo;JJ&rsaquo;*&lsaquo;NN&rsaquo;"])
</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                This annotator matches a pattern of part-of-speech tags in order to return meaningul phrases from document<br>
                                            </p>
                                            <b>Type:</b> Document<br>
                                            <b>Requires:</b> Document<br>
                                            <b>Functions:</b>
                                            <ul>
                                                <li>
                                                    setRegexParsers(patterns): A list of regex patterns to match chunks, for example: Array("&lsaquo;DT&rsaquo;?&lsaquo;JJ&rsaquo;*&lsaquo;NN&rsaquo;")
                                                </li>
                                                <li>
                                                    addRegexParser(patterns): adds a pattern to the current list of chunk patterns, for example: "&lsaquo;DT&rsaquo;?&lsaquo;JJ&rsaquo;*&lsaquo;NN&rsaquo;"
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val chunker = new Chunker()
  .setInputCols(Array("pos"))
  .setOutputCol("chunks")
  .setRegexParsers(Array("&lsaquo;NNP&rsaquo;+", "&lsaquo;DT|PP\\$&rsaquo;?&lsaquo;JJ&rsaquo;*&lsaquo;NN&rsaquo;"))
</code></pre>
                                        </div>
                                    </div>
                                </div>

                                <h4 id="DateMatcher" class="section-block"> DateMatcher: Date-time parsing</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Reads from different forms of date and time expressions and converts
                                                them to a provided date format. Extracts only ONE date per sentence.
                                                Use with sentence detector for more matches.<br>
                                                <b>Type:</b> Date<br>
                                                <b>Requires:</b> Document<br>
                                                <b>Reads the following kind of dates:</b>
                                        <ul>
                                            <li>1978-01-28</li>
                                            <li>1984/04/02</li>
                                            <li>1/02/1980</li>
                                            <li>2/28/79</li>
                                            <li>The 31st of April in the year 2008</li>
                                            <li>Fri, 21 Nov 1997</li>
                                            <li>Jan 21, '97</li>
                                            <li>Sun, Nov 21</li>
                                            <li>jan 1st</li>
                                            <li>next thursday</li>
                                            <li>last wednesday</li>
                                            <li>today</li>
                                            <li>tomorrow</li>
                                            <li>yesterday</li>
                                            <li>next week</li>
                                            <li>next month</li>
                                            <li>next year</li>
                                            <li>day after</li>
                                            <li>the day before</li>
                                            <li>0600h</li>
                                            <li>06:00 hours</li>
                                            <li>6pm</li>
                                            <li>5:30 a.m.</li>
                                            <li>at 5</li>
                                            <li>12:59</li>
                                            <li>23:59</li>
                                            <li>1988/11/23 6pm</li>
                                            <li>next week at 7.30</li>
                                            <li>5 am tomorrow</li>
                                        </ul>
                                                <b>Functions:</b> <br>
                                            <ul>
                                                <li>
                                                    setDateFormat(format): SimpleDateFormat standard date formatting.
                                                    Defaults to yyyy/MM/dd
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">date_matcher = DateMatcher() \
  .setOutputCol("date") \
  .setDateFormat("yyyyMM")</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Reads from different forms of date and time expressions and converts
                                                them to a provided date format. Extracts only ONE date per sentence.
                                                Use with sentence detector for more matches.<br>
                                                <b>Type:</b> Date<br>
                                                <b>Requires:</b> Document<br>
                                                <b>Reads the following kind of dates:</b>
                                            <ul>
                                                <li>1978-01-28</li>
                                                <li>1984/04/02</li>
                                                <li>1/02/1980</li>
                                                <li>2/28/79</li>
                                                <li>The 31st of April in the year 2008</li>
                                                <li>Fri, 21 Nov 1997</li>
                                                <li>Jan 21, '97</li>
                                                <li>Sun, Nov 21</li>
                                                <li>jan 1st</li>
                                                <li>next thursday</li>
                                                <li>last wednesday</li>
                                                <li>today</li>
                                                <li>tomorrow</li>
                                                <li>yesterday</li>
                                                <li>next week</li>
                                                <li>next month</li>
                                                <li>next year</li>
                                                <li>day after</li>
                                                <li>the day before</li>
                                                <li>0600h</li>
                                                <li>06:00 hours</li>
                                                <li>6pm</li>
                                                <li>5:30 a.m.</li>
                                                <li>at 5</li>
                                                <li>12:59</li>
                                                <li>23:59</li>
                                                <li>1988/11/23 6pm</li>
                                                <li>next week at 7.30</li>
                                                <li>5 am tomorrow</li>
                                            </ul>
                                                <b>Functions:</b> <br>
                                            <ul>
                                                <li>
                                                    setDateFormat(format): SimpleDateFormat standard date formatting.
                                                    Defaults
                                                    to yyyy/MM/dd
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val dateMatcher = new DateMatcher()
  .setFormat("yyyyMM")
  .setOutputCol("date")</code></pre>
                                        </div><!--//code-block--></div>
                                </div>

                                <h4 id="SentenceDetector" class="section-block"> SentenceDetector: Sentence Boundary
                                    Detector</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Finds sentence bounds in raw text. Applies rules from Pragmatic Segmenter.<br>
                                                <b>Type:</b> Document<br>
                                                <b>Requires:</b> Document<br>
                                                <b>Functions:</b> <br>
                                            <ul>
                                              <li>
                                                  setCustomBounds(string): Custom sentence separator text
                                              </li>
                                              <li>
                                                  setUseCustomOnly(bool): Use only custom bounds without considering those of Pragmatic Segmenter. Defaults to false. Needs customBounds.
                                              </li>
                                              <li>
                                                  setUseAbbreviations(bool): Whether to consider abbreviation strategies for better accuracy but slower performance. Defaults to true.
                                              </li>
                                            <li>
                                                setExplodeSentences(bool): Whether to split sentences into different Dataset rows. Useful for higher parallelism in fat rows. Defaults to false.
                                            </li>
                                            </ul>
                                                <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">sentence_detector = SentenceDetector() \
  .setInputCols(["document"]) \
  .setOutputCol("sentence")</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Finds sentence bounds in raw text. Applies rules from Pragmatic Segmenter.<br>
                                                <b>Type:</b> Document<br>
                                                <b>Requires:</b> Document<br>
                                              <b>Functions:</b> <br>
                                          <ul>
                                            <li>
                                                setCustomBounds(bounds): Custom sentence separator text
                                            </li>
                                            <li>
                                                setUseCustomOnly(False): Use only custom bounds without considering those of Pragmatic Segmenter
                                            </li>
                                            <li>
                                                setUseAbbreviations(False): Whether to consider abbreviation strategies for better accuracy but slower performance
                                            </li>
                                            <li>
                                                setExplodeSentences(bool): Whether to split sentences into different Dataset rows. Useful for higher parallelism in fat rows. Defaults to false.
                                            </li>
                                          </ul>
                                                <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val sentenceDetector = new SentenceDetector()
  .setInputCols("document")
  .setOutputCol("sentence")</code></pre>
                                        </div>
                                    </div>
                                </div>
                                
                                <!--Begin Deep Sentence Detector-->
                                <h4 id="DeepSentenceDetector" class="section-block"> DeepSentenceDetector: Sentence Boundary Detector with Machine Learning</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Finds sentence bounds in raw text. Applies a Named Entity Recognition DL model.<br>
                                                <b>Type:</b> Document<br>
                                                <b>Requires:</b> Document, Token, Chunk<br>
                                                <b>Functions:</b> <br>
                                            <ul>
                                              <li>
                                                setIncludePragmaticSegmenter(bool): Whether to include rule-based sentence detector as first filter. Defaults to false.
                                              </li>
                                              <li>
                                                setEndPunctuation(patterns): An array of symbols that deep sentence detector will consider as an end of sentence punctuation. Defaults to ".", "!", "?"
                                              </li>
                                            </ul>
                                                <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">deep_sentence_detector = DeepSentenceDetector() \
  .setInputCols(["document", "token", "ner_con"]) \
  .setOutputCol("sentence") \
  .setIncludePragmaticSegmenter(True) \
  .setEndPunctuation([".", "?"])</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Finds sentence bounds in raw text. Applies a Named Entity Recognition DL model.<br>
                                                <b>Type:</b> Document<br>
                                                <b>Requires:</b> Document, Token, Chunk<br>
                                              <b>Functions:</b> <br>
                                          <ul>
                                            <li>
                                                setIncludePragmaticSegmenter(bool): Whether to include rule-based sentence detector as first filter. Defaults to false.
                                            </li>
                                            <li>
                                                setEndPunctuation(patterns): An array of symbols that deep sentence detector will consider as an end of sentence punctuation. Defaults to ".", "!", "?"
                                            </li>
                                          </ul>
                                                <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val deepSentenceDetector = new DeepSentenceDetector()
  .setInputCols(Array("document", "token", "ner_con"))
  .setOutputCol("sentence")
  .setIncludePragmaticSegmenter(true)
  .setEndPunctuation(Array(".", "?"))</code></pre>
                                        </div>
                                    </div>
                                </div>
                                <!--End Deep Sentence Detector-->

                                <h4 id="POSTagger" class="section-block"> POSTagger: Part of speech tagger</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Sets a POS tag to each word within a sentence<br>
                                                <b>Type:</b> POS<br>
                                                <b>Input:</b> A|DT few|JJ months|NNS ago|RB you|PRP received|VBD a|DT
                                                letter|NN<br>
                                                <b>Requires:</b> Document, Token<br>
                                                <b>Functions:</b> <br>
                                            <ul>
                                                <li>
                                                    setCorpus(path, delimiter, readAs, options): path to file, delimiter of
                                                    token and postag, readAs either LINE_BY_LINE or SPARK_DATASET, options
                                                    passed to reader if SPARK_DATASET
                                                </li>
                                                <li>
                                                    setNIterations(number): Number of iterations for training. May improve
                                                    accuracy but
                                                    takes longer. Default 5.
                                                </li>
                                                <li>
                                                    setPosColumn(colname): Column containing an array of POS Tags matching
                                                    every token on the line. If set, this column will be used during fit()
                                                    stage to train from it instead of external corpora
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">pos_tagger = PerceptronApproach() \
  .setInputCols(["token", "sentence"]) \
  .setOutputCol("pos") \
  .setCorpusPath("./src/main/resources/anc-pos-corpus") \
  .setIterations(2) \
  .fit(data)</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Sets a POS tag to each word within a sentence<br>
                                                <b>Type:</b> POS<br>
                                                <b>Input:</b> A|DT few|JJ months|NNS ago|RB you|PRP received|VBD a|DT
                                                letter|NN<br>
                                                <b>Requires:</b> Document, Token<br>
                                                <b>Functions:</b> <br>
                                            <ul>
                                              <li>
                                                setCorpus(path, delimiter, readAs, options): path to file, delimiter of
                                                token and postag, readAs either LINE_BY_LINE or SPARK_DATASET, options
                                                passed to reader if SPARK_DATASET
                                              </li>
                                              <li>
                                                setNIterations(number): Number of iterations for training. May improve
                                                accuracy but
                                                takes longer. Default 5.
                                              </li>
                                              <li>
                                                setPosColumn(colname): Column containing an array of POS Tags matching
                                                every token on the line. If set, this column will be used during fit()
                                                stage to train from it instead of external corpora
                                              </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val posTagger = new PerceptronApproach()
  .setInputCols(Array("sentence", "token"))
  .setOutputCol("pos")
  .fit(data)</code></pre>
                                        </div><!--//code-block--></div>
                                </div>

                              <h4 id="ViveknSentimentDetector" class="section-block"> ViveknSentimentDetector:
                                Sentiment
                                analysis</h4>
                              <ul class="nav nav-tabs" role="tablist">
                                <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                          role="tab" data-toggle="tab">Python</a>
                                </li>
                                <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                           data-toggle="tab">Scala</a></li>
                              </ul>
                              <div class="tab-content">
                                <div role="tabpanel" class="tab-pane active" id="python">
                                  <div class="code-block">
                                    <p>
                                      Scores a sentence for a sentiment<br>
                                      <b>Type:</b> sentiment<br>
                                      <b>Requires:</b> Document, Token<br>
                                      <b>Functions:</b>
                                    <ul>
                                      <li>
                                        setSentimentCol(colname): Column with sentiment analysis row's result for training. If not set, external sources need to be set instead.
                                      </li>
                                      <li>
                                        setPositiveSource(path, tokenPattern, readAs, options): Path to file or folder with positive sentiment text, with tokenPattern the regex pattern to match tokens in source. readAs either LINE_BY_LINE or as SPARK_DATASET. If latter is set, options is passed to reader
                                      </li>
                                      <li>
                                        setNegativeSource(path, tokenPattern, readAs, options): Path to file or folder with positive sentiment text, with tokenPattern the regex pattern to match tokens in source. readAs either LINE_BY_LINE or as SPARK_DATASET. If latter is set, options is passed to reader
                                      </li>
                                      <li>
                                        setPruneCorpus(true): when training on small data you may want
                                        to disable this to not cut off unfrequent words
                                      </li>
                                    </ul>
                                    <br>
                                    <b>Input:</b> File or folder of text files of positive and negative data<br>
                                    <b>Example:</b><br>
                                    </p>
                                    <pre><code class="language-python">sentiment_detector = SentimentDetector() \
    .setInputCols(["lemma", "sentence"]) \
    .setOutputCol("sentiment")</code></pre>
                                  </div><!--//code-block-->
                                </div>
                                <div role="tabpanel" class="tab-pane" id="scala">
                                  <div class="code-block">
                                    <p>
                                      Scores a sentence for a sentiment<br>
                                      <b>Type:</b> sentiment<br>
                                      <b>Requires:</b> Document, Token<br>
                                      <b>Functions:</b>
                                    <ul>
                                      <li>
                                        setSentimentCol(colname): Column with sentiment analysis row's result for training. If not set, external sources need to be set instead.
                                      </li>
                                      <li>
                                        setPositiveSource(path, tokenPattern, readAs, options): Path to file or folder with positive sentiment text, with tokenPattern the regex pattern to match tokens in source. readAs either LINE_BY_LINE or as SPARK_DATASET. If latter is set, options is passed to reader
                                      </li>
                                      <li>
                                        setNegativeSource(path, tokenPattern, readAs, options): Path to file or folder with positive sentiment text, with tokenPattern the regex pattern to match tokens in source. readAs either LINE_BY_LINE or as SPARK_DATASET. If latter is set, options is passed to reader
                                      </li>
                                      <li>
                                        setPruneCorpus(true): when training on small data you may want
                                        to disable this to not cut off unfrequent words
                                      </li>
                                    </ul>
                                    <br>
                                    <b>Input:</b> File or folder of text files of positive and negative data<br>
                                    <b>Example:</b><br>
                                    </p>
                                    <pre><code class="language-javascript">new ViveknSentimentApproach()
      .setInputCols(Array("token", "sentence"))
      .setOutputCol("vivekn")
      .setPositiveSourcePath("./positive/1.txt")
      .setNegativeSourcePath("./negative/1.txt")
      .setCorpusPrune(false)</code></pre>
                                  </div>
                                </div>
                              </div>

                                <h4 id="SentimentDetector" class="section-block"> SentimentDetector: Sentiment
                                    analysis</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Scores a sentence for a sentiment<br>
                                                <b>Type:</b> sentiment<br>
                                                <b>Requires:</b> Document, Token<br>
                                                <b>Functions:</b>
                                            <ul>
                                                <li>
                                                    setDictionary(path, delimiter, readAs, options): path to file with list of inputs and their content, with such delimiter, readAs LINE_BY_LINE or as SPARK_DATASET. If latter is set, options is passed to spark reader.
                                                </li>
                                            <li>
                                                setPositiveMultiplier(double): Defaults to 1.0
                                            </li>
                                            <li>
                                                setNegativeMultiplier(double): Defaults to -1.0
                                            </li>
                                            <li>
                                                setIncrementMultiplier(double): Defaults to 2.0
                                            </li>
                                            <li>
                                                setDecrementMultiplier(double): Defaults to -2.0
                                            </li>
                                            <li>
                                                setReverseMultiplier(double): Defaults to -1.0
                                            </li>
                                            </ul>
                                            <br>
                                            <b>Input:</b>
                                            <ul>
                                                <li>superb,positive</li>
                                                <li>bad,negative</li>
                                                <li>lack of,revert</li>
                                                <li>very,increment</li>
                                                <li>barely,decrement</li>
                                            </ul>
                                            <br>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">sentiment_detector = SentimentDetector() \
  .setInputCols(["lemma", "sentence"]) \
  .setOutputCol("sentiment")</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Scores a sentence for a sentiment<br>
                                                <b>Type:</b> sentiment<br>
                                                <b>Requires:</b> Document, Token<br>
                                                <b>Functions:</b>
                                            <ul>
                                              <li>
                                                setDictionary(path, delimiter, readAs, options): path to file with list of inputs and their content, with such delimiter, readAs LINE_BY_LINE or as SPARK_DATASET. If latter is set, options is passed to spark reader.
                                              </li>
                                            <li>
                                                setPositiveMultiplier(double): Defaults to 1.0
                                            </li>
                                            <li>
                                                setNegativeMultiplier(double): Defaults to -1.0
                                            </li>
                                            <li>
                                                setIncrementMultiplier(double): Defaults to 2.0
                                            </li>
                                            <li>
                                                setDecrementMultiplier(double): Defaults to -2.0
                                            </li>
                                            <li>
                                                setReverseMultiplier(double): Defaults to -1.0
                                            </li>
                                            </ul>
                                            <br>
                                            <b>Input:</b>
                                            <ul>
                                                <li>superb,positive</li>
                                                <li>bad,negative</li>
                                                <li>lack of,revert</li>
                                                <li>very,increment</li>
                                                <li>barely,decrement</li>
                                            </ul>
                                            <br>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val sentimentDetector = new SentimentDetector
  .setInputCols(Array("token", "sentence"))
  .setOutputCol("sentiment")</code></pre>
                                        </div><!--//code-block--></div>
                                </div>

                                <h4 id="NERCRF" class="section-block"> Named Entity Recognition CRF
                                    annotator</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                              This Named Entity recognition annotator allows for a generic model
                                              to be trained by utilizing a CRF machine learning algorithm. Its inputs are either a labeled dataset with an Annotations column or an external CoNLL 2003 IOB based dataset, and optionally
                                              the user can provide both an entities dictionary and a word embeddings file for better accuracy<br>
                                                <b>Type:</b> named_entity<br>
                                                <b>Requires:</b> Document, token, pos<br>
                                                <b>Functions:</b> <br>
                                          <ul>
                                            <li>
                                                setExternalDataset(path, readAs, options): Path to a <a href="https://www.clips.uantwerpen.be/conll2003/ner">CoNLL 2003 IOB NER and POS annotated file</a>. If this is provided. label column is not needed. readAs can be LINE_BY_LINE or SPARK_DATASET, with options if latter is used
                                            </li>
                                            <li>
                                              setLabelColumn: If DatasetPath is not provided, this Seq[Annotation] type of column should have labeled data per token
                                            </li>
                                            <li>
                                              setMinEpochs: Minimum number of epochs to train
                                            </li>
                                            <li>
                                              setMaxEpochs: Maximum number of epochs to train
                                            </li>
                                            <li>
                                              setL2: L2 regularization coefficient for CRF
                                            </li>
                                            <li>
                                              setC0: c0 defines decay speed for gradient
                                            </li>
                                            <li>
                                              setLossEps: If epoch relative improvement lass than this vallue, training is stopped
                                            </li>
                                            <li>
                                              setMinW: Features with less weights than this value will be filtered out
                                            </li>
                                            <li>
                                                setEmbeddingsSource:(path, nDims, format) - sets <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> options.
                                                path - word embeddings file
                                                nDims - number of word embeddings dimensions
                                                format - format of word embeddings files:<br>
                                                1 - spark-nlp format. <br>
                                                2 - text. This format is usually used by <a href="https://nlp.stanford.edu/projects/glove/">Glove</a><br>
                                                3 - binary. This format is usually used by <a href="https://code.google.com/archive/p/word2vec/">Word2Vec</a>
                                            </li>
                                            <li>
                                              setExternalFeatures(path, delimiter, readAs, options): Path to file or folder of line separated file that has something like this: Volvo:ORG with such delimiter, readAs LINE_BY_LINE or SPARK_DATASET with options passed to the latter.
                                            </li>
                                            <li>
                                              setEntities: Array of entities to recognize
                                            </li>
                                            <li>
                                              setVerbose: Verbosity level
                                            </li>
                                            <li>
                                              setRandomSeed: Random seed
                                            </li>
                                          </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">nerTagger = NerCrfApproach()\
  .setInputCols(["sentence", "token", "pos"])\
  .setLabelColumn("label")\
  .setOutputCol("ner")\
  .setMinEpochs(1)\
  .setMaxEpochs(20)\
  .setLossEps(1e-3)\
  .setDicts(["ner-corpus/dict.txt"])\
  .setDatasetPath("eng.train")\
  .setEmbeddingsSource("glove.6B.100d.txt", 100, 2)\
  .setL2(1)\
  .setC0(1250000)\
  .setRandomSeed(0)\
  .setVerbose(2)</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                              This Named Entity recognition annotator allows for a generic model
                                              to be trained by utilizing a CRF machine learning algorithm. Its inputs are either a labeled dataset with an Annotations column or an external CoNLL 2003 IOB based dataset, and optionally
                                              the user can provide both an entities dictionary and a word embeddings file for better accuracy<br>
                                              <b>Type:</b> named_entity<br>
                                              <b>Requires:</b> Document, token, pos<br>
                                                <b>Functions:</b> <br>
                                            <ul>
                                              <li>
                                                  setExternalDataset(path, readAs, options): Path to a <a href="https://www.clips.uantwerpen.be/conll2003/ner">CoNLL 2003 IOB NER and POS annotated file</a>. If this is provided. label column is not needed. readAs can be LINE_BY_LINE or SPARK_DATASET, with options if latter is used
                                              </li>
                                              <li>
                                                setLabelColumn: If DatasetPath is not provided, this Seq[Annotation] type of column should have labeled data per token
                                              </li>
                                              <li>
                                                setMinEpochs: Minimum number of epochs to train
                                              </li>
                                              <li>
                                                setMaxEpochs: Maximum number of epochs to train
                                              </li>
                                              <li>
                                                setL2: L2 regularization coefficient for CRF
                                              </li>
                                              <li>
                                                setC0: c0 defines decay speed for gradient
                                              </li>
                                              <li>
                                                setLossEps: If epoch relative improvement lass than this vallue, training is stopped
                                              </li>
                                              <li>
                                                setMinW: Features with less weights than this value will be filtered out
                                              </li>
                                              <li>
                                                setEmbeddingsSource:(path, nDims, format) - sets <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> options.
                                                path - word embeddings file
                                                nDims - number of word embeddings dimensions
                                                format - format of word embeddings files:<br>
                                                1 - spark-nlp format. <br>
                                                  2 - text. This format is usually used by <a href="https://nlp.stanford.edu/projects/glove/">Glove</a><br>
                                                  3 - binary. This format is usually used by <a href="https://code.google.com/archive/p/word2vec/">Word2Vec</a>
                                              </li>
                                              <li>
                                                setExternalFeatures(path, delimiter, readAs, options): Path to file or folder of line separated file that has something like this: Volvo:ORG with such delimiter, readAs LINE_BY_LINE or SPARK_DATASET with options passed to the latter.
                                              </li>
                                              <li>
                                                setEntities: Array of entities to recognize
                                              </li>
                                              <li>
                                                setVerbose: Verbosity level
                                              </li>
                                              <li>
                                                setRandomSeed: Random seed
                                              </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">new NerCrfApproach()
  .setInputCols("sentence", "token", "pos")
  .setLabelColumn("label")
  .setMinEpochs(1)
  .setMaxEpochs(3)
  .setDatasetPath("src/test/resources/ner-corpus/test_ner_dataset.txt")
  .setEmbeddingsSource("src/test/resources/ner-corpus/test_embeddings.txt", 3, WordEmbeddingsFormat.Text)
  .setC0(34)
  .setL2(3.0)
  .setOutputCol("ner")
  .fit(df)</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="NerDL" class="section-block"> Named Entity Recognition Deep Learning
                                    annotator</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                This Named Entity recognition annotator allows to train generic NER model
                                                based on a Neural Networks. Its inputs are either a labeled dataset with an Annotations column or an external CoNLL 2003 IOB based dataset,
                                                Also the user have to provide word embeddings file<br>
                                                Neural Network architecture is Char CNN - BLSTM that achieves state-of-the-art in most datasets.<br>
                                                <b>Type:</b> named_entity<br>
                                                <b>Requires:</b> Document, token<br>
                                                <b>Functions:</b> <br>
                                            <ul>
                                                <li>
                                                    setExternalDataset(path, readAs, options): Path to a <a href="https://www.clips.uantwerpen.be/conll2003/ner">CoNLL 2003 IOB NER file</a>. If this is provided. label column is not needed. readAs can be LINE_BY_LINE or SPARK_DATASET, with options if latter is used
                                                </li>
                                                <li>
                                                    setLabelColumn: If DatasetPath is not provided, this Seq[Annotation] type of column should have labeled data per token
                                                </li>
                                                <li>
                                                    setMaxEpochs: Maximum number of epochs to train
                                                </li>
                                                <li>
                                                    setLr: Initial learning rate
                                                </li>
                                                <li>
                                                    setPo: Learning rate decay coefficient. Real Learning Rate: lr / (1 + po * epoch)
                                                </li>
                                                <li>
                                                    setBatchSize: Batch size for training
                                                </li>
                                                <li>
                                                    setDropout: Dropout coefficient
                                                </li>
                                                <li>
                                                    setEmbeddingsSource:(path, nDims, format) - sets <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> options.
                                                    path - word embeddings file
                                                    nDims - number of word embeddings dimensions
                                                    format - format of word embeddings files:<br>
                                                    1 - spark-nlp format. <br>
                                                    2 - text. This format is usually used by <a href="https://nlp.stanford.edu/projects/glove/">Glove</a><br>
                                                    3 - binary. This format is usually used by <a href="https://code.google.com/archive/p/word2vec/">Word2Vec</a>
                                                </li>
                                                <li>
                                                    setValidationDataset(path, readAs, options): Path to a <a href="https://www.clips.uantwerpen.be/conll2003/ner">CoNLL 2003 IOB NER file</a>. If provided than quality will be logged after every epoch. readAs can be LINE_BY_LINE or SPARK_DATASET, with options if latter is used
                                                </li>
                                                <li>
                                                    setTestDataset(path, readAs, options): Path to a <a href="https://www.clips.uantwerpen.be/conll2003/ner">CoNLL 2003 IOB NER file</a>. If provided than quality will be logged after every epoch. readAs can be LINE_BY_LINE or SPARK_DATASET, with options if latter is used
                                                </li>
                                                <li>
                                                    setVerbose: Verbosity level
                                                </li>
                                                <li>
                                                    setRandomSeed: Random seed
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">nerTagger = NerDLApproach()\
  .setInputCols(["sentence", "token"])\
  .setLabelColumn("label")\
  .setOutputCol("ner")\
  .setMaxEpochs(10)\
  .setExternalDataset("file://conll2013/eng.train")\
  .setValidationDataset("file://conll2013/eng.testa")\
  .setTestDataset("file://conll2013/eng.testb")\
  .setEmbeddingsSource("file://glove.6B/glove.6B.100d.txt", 100, 2)\
  .setRandomSeed(0)\
  .setVerbose(2)</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                            <p>
                                                This Named Entity recognition annotator allows to train generic NER model
                                                based on a Neural Networks. Its inputs are either a labeled dataset with an Annotations column or an external CoNLL 2003 IOB based dataset,
                                                Also the user have to provide word embeddings file<br>
                                                Neural Network architecture is Char CNN - BLSTM that achieves state-of-the-art in most datasets.<br>
                                                <b>Type:</b> named_entity<br>
                                                <b>Requires:</b> Document, token<br>
                                                <b>Functions:</b> <br>
                                            <ul>
                                                <li>
                                                    setExternalDataset(path, readAs, options): Path to a <a href="https://www.clips.uantwerpen.be/conll2003/ner">CoNLL 2003 IOB NER file</a>. If this is provided. label column is not needed. readAs can be LINE_BY_LINE or SPARK_DATASET, with options if latter is used
                                                </li>
                                                <li>
                                                    setLabelColumn: If DatasetPath is not provided, this Seq[Annotation] type of column should have labeled data per token
                                                </li>
                                                <li>
                                                    setMaxEpochs: Maximum number of epochs to train
                                                </li>

                                                <li>
                                                    setLr: Initial learning rate
                                                </li>
                                                <li>
                                                    setPo: Learning rate decay coefficient. Real Learning Rate: lr / (1 + po * epoch)
                                                </li>
                                                <li>
                                                    setBatchSize: Batch size for training
                                                </li>
                                                <li>
                                                    setDropout: Dropout coefficient
                                                </li>
                                                <li>
                                                    setEmbeddingsSource:(path, nDims, format) - sets <a href="https://en.wikipedia.org/wiki/Word_embedding">word embeddings</a> options.
                                                    path - word embeddings file
                                                    nDims - number of word embeddings dimensions
                                                    format - format of word embeddings files:<br>
                                                    1 - spark-nlp format. <br>
                                                    2 - text. This format is usually used by <a href="https://nlp.stanford.edu/projects/glove/">Glove</a><br>
                                                    3 - binary. This format is usually used by <a href="https://code.google.com/archive/p/word2vec/">Word2Vec</a>
                                                </li>
                                                <li>
                                                    setValidationDataset(path, readAs, options): Path to a <a href="https://www.clips.uantwerpen.be/conll2003/ner">CoNLL 2003 IOB NER file</a>. If provided than quality will be logged after every epoch. readAs can be LINE_BY_LINE or SPARK_DATASET, with options if latter is used
                                                </li>
                                                <li>
                                                    setTestDataset(path, readAs, options): Path to a <a href="https://www.clips.uantwerpen.be/conll2003/ner">CoNLL 2003 IOB NER file</a>. If provided than quality will be logged after every epoch. readAs can be LINE_BY_LINE or SPARK_DATASET, with options if latter is used
                                                </li>

                                                <li>
                                                    setVerbose: Verbosity level
                                                </li>
                                                <li>
                                                    setRandomSeed: Random seed
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val nerTagger = new NerDLApproach()
      .setInputCols("sentence", "token")
      .setOutputCol("ner")
      .setLabelColumn("label")
      .setMaxEpochs(120)
      .setRandomSeed(0)
      .setPo(0.03f)
      .setLr(0.2f)
      .setDropout(0.5f)
      .setBatchSize(9)
      .setEmbeddingsSource("glove.6B.100d.txt", 100, WordEmbeddingsFormat.TEXT)
      .setExternalDataset("conll2013/eng.train", ReadAs.LINE_BY_LINE, Map.empty[String, String])
      .setValidationDataset("conll2013/eng.testa", ReadAs.LINE_BY_LINE, Map.empty[String, String])
      .setTestDataset("conll2013/eng.testb", ReadAs.LINE_BY_LINE, Map.empty[String, String])
      .setVerbose(Verbose.Epochs)</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="SpellChecker" class="section-block"> SpellChecker: Norvig algorithm</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                This annotator retrieves tokens and makes corrections automatically if
                                                not found
                                                on an english dictionary<br>
                                                <b>Type:</b> Token<br>
                                                <b>Inputs:</b> Any text for corpus. A list of words for dictionary. A
                                                comma
                                                separated custom dictionary.<br>
                                                <b>Requires:</b> Tokenizer<br>
                                                <b>Functions:</b><br>
                                            <ul>
                                            <li>
                                                setDictionary(path, tokenPattern, readAs, options): path to file with properly spelled words, tokenPattern is the regex pattern to identify them in text, readAs LINE_BY_LINE or SPARK_DATASET, with options passed to Spark reader if the latter is set.
                                            </li>
                                            <li>
                                                setCorpus(path, tokenPattern, readAs, options): path to training corpus folder or file. If not set, content available in Dataset received in fit() stage will be used. tokenPattern is the regex pattern to identify them in text, readAs LINE_BY_LINE or SPARK_DATASET, with options passed to Spark reader if the latter is set.
                                            </li>
                                            <li>
                                                setSlangDictionary(path, delimiter, readAs, options): path to custom word mapping for spell checking. e.g. gr8 -> great. Uses provided delimiter, readAs LINE_BY_LINE or SPARK_DATASET with options passed to reader if the latter.
                                            </li>
                                            <li>
                                                setCaseSensitive(boolean): defaults to false. Might affect accuracy
                                            </li>
                                            <li>
                                                setDoubleVariants(boolean): enables extra check for word combinations, more
                                                accuracy
                                                at performance
                                            </li>
                                            <li>
                                                setShortCircuit(boolean): faster but less accurate mode
                                            </li>
                                            <li>
                                                setWordSizeIgnore(int): Minimum size of word before moving on. Defaults to 3.
                                            </li>
                                            <li>
                                                setDupsLimit(int): Maximum duplicate of characters to account for. Defaults to 2.
                                            </li>
                                            <li>
                                                setReductLimit(int): Word reduction limit. Defaults to 3
                                            </li>
                                            <li>
                                                setIntersections(int): Hamming intersections to attempt. Defaults to 10.
                                            </li>
                                            <li>
                                                setVowelSwapLimit(int): Vowel swap attempts. Defaults to 6.
                                            </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">spell_checker = NorvigSweetingApproach() \
  .setInputCols(["token"]) \
  .setOutputCol("spell") \
  .setCorpus("./sherlockholmes.txt")</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                This annotator retrieves tokens and makes corrections automatically if
                                                not found
                                                on an english dictionary<br>
                                                <b>Type:</b> Token<br>
                                                <b>Inputs:</b> Any text for corpus. A list of words for dictionary. A
                                                comma
                                                separated custom dictionary.<br>
                                                <b>Requires:</b> Tokenizer<br>
                                                <b>Functions:</b><br>
                                            <ul>
                                              <li>
                                                setDictionary(path, tokenPattern, readAs, options): path to file with properly spelled words, tokenPattern is the regex pattern to identify them in text, readAs LINE_BY_LINE or SPARK_DATASET, with options passed to Spark reader if the latter is set.
                                              </li>
                                              <li>
                                                setCorpus(path, tokenPattern, readAs, options): path to training corpus folder or file. If not set, content available in Dataset received in fit() stage will be used. tokenPattern is the regex pattern to identify them in text, readAs LINE_BY_LINE or SPARK_DATASET, with options passed to Spark reader if the latter is set.
                                              </li>
                                              <li>
                                                setSlangDictionary(path, delimiter, readAs, options): path to custom word mapping for spell checking. e.g. gr8 -> great. Uses provided delimiter, readAs LINE_BY_LINE or SPARK_DATASET with options passed to reader if the latter.
                                              </li>
                                              <li>
                                                setCaseSensitive(boolean): defaults to false. Might affect accuracy
                                              </li>
                                              <li>
                                                setDoubleVariants(boolean): enables extra check for word combinations, more
                                                accuracy
                                                at performance
                                              </li>
                                              <li>
                                                setShortCircuit(boolean): faster but less accurate mode
                                              </li>
                                            <li>
                                                setWordSizeIgnore(int): Minimum size of word before moving on. Defaults to 3.
                                            </li>
                                            <li>
                                                setDupsLimit(int): Maximum duplicate of characters to account for. Defaults to 2.
                                            </li>
                                            <li>
                                                setReductLimit(int): Word reduction limit. Defaults to 3
                                            </li>
                                            <li>
                                                setIntersections(int): Hamming intersections to attempt. Defaults to 10.
                                            </li>
                                            <li>
                                                setVowelSwapLimit(int): Vowel swap attempts. Defaults to 6.
                                            </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val spellChecker = new NorvigSweetingApproach()
  .setInputCols(Array("normalized"))
  .setOutputCol("spell")
  .setCorpus("./sherlockholmes.txt")</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <h4 id="SymmChecker" class="section-block"> SpellChecker: Symmetric delete</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                This spell checker is inspired on Symmetric Delete algorithm. It retrieves tokens
                                                and utilizes distance metrics to compute possible derived words<br>
                                                <b>Type:</b> Token<br>
                                                <b>Inputs:</b> Any text for corpus. A list of words for dictionary. A
                                                comma
                                                separated custom dictionary.<br>
                                                <b>Requires:</b> Tokenizer<br>
                                                <b>Functions:</b><br>
                                            <ul>
                                                <li>
                                                    setCorpus(path, tokenPattern, readAs, options): path to training corpus folder or file. If not set, content available in Dataset received in fit() stage will be used. tokenPattern is the regex pattern to identify them in text, readAs LINE_BY_LINE or SPARK_DATASET, with options passed to Spark reader if the latter is set.
                                                </li>
                                                <li>
                                                    setDictionary(path, tokenPattern, readAs, options): Optional dictionary of properly written words. If provided, significantly boosts spell checking performance
                                                </li>
                                                <li>
                                                    setMaxEditDistance(distance): Maximum edit distance to calculate possible derived words. Defaults to 3.
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">spell_checker = SymmetricDeleteApproach() \
  .setInputCols(["token"]) \
  .setOutputCol("spell") \
  .setCorpus("./sherlockholmes.txt")</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                This spell checker is inspired on Symmetric Delete algorithm. It retrieves tokens
                                                and utilizes distance metrics to compute possible derived words<br>
                                                <b>Type:</b> Token<br>
                                                <b>Inputs:</b> Any text for corpus. A list of words for dictionary. A
                                                comma
                                                separated custom dictionary.<br>
                                                <b>Requires:</b> Tokenizer<br>
                                                <b>Functions:</b><br>
                                            <ul>
                                                <li>
                                                    setCorpus(path, tokenPattern, readAs, options): path to training corpus folder or file. If not set, content available in Dataset received in fit() stage will be used. tokenPattern is the regex pattern to identify them in text, readAs LINE_BY_LINE or SPARK_DATASET, with options passed to Spark reader if the latter is set.
                                                </li>
                                                <li>
                                                    setDictionary(path, tokenPattern, readAs, options): Optional dictionary of properly written words. If provided, significantly boosts spell checking performance
                                                </li>
                                                <li>
                                                    setMaxEditDistance(distance): Maximum edit distance to calculate possible derived words. Defaults to 3.
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val spellChecker = new SymmetricDeleteApproach()
  .setInputCols(Array("normalized"))
  .setOutputCol("spell")
  .setCorpus("./sherlockholmes.txt")</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <!--Begin Dependency Parser Doc -->
                                <h4 id="DependencyParser" class="section-block">Dependency Parser: Unlabeled grammatical relation</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Unlabeled parser that finds a grammatical relation between two words in a sentence. Its input is a directory with dependency treebank files.<br>
                                                <b>Type:</b> Dependency<br>
                                                <b>Requires:</b> Document, POS, Token<br>
                                                <b>Functions:</b>
                                            <ul>
                                                <li>
                                                    setNumberOfIterations: Number of iterations in training, converges to better accuracy
                                                </li>
                                                <li>
                                                    setDependencyTreeBank: Dependency treebank source files in <a href="http://www.nltk.org/nltk_data/"> Penn Treebank format</a>
                                                </li>
                                            </ul>
                                            <br>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">dependency_parser = DependencyParserApproach() \
            .setInputCols(["sentence", "pos", "token"]) \
            .setOutputCol("dependency") \
            .setDependencyTreeBank("file://parser/dependency_treebank") \
            .setNumberOfIterations(10)</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Unlabeled parser that finds a grammatical relation between two words in a sentence. Its input is a directory with dependency treebank files.<br>
                                                <b>Type:</b> Dependency<br>
                                                <b>Requires:</b> Document, POS, Token<br>
                                            <ul>
                                                <li>
                                                    setNumberOfIterations: Number of iterations in training, converges to better accuracy
                                                </li>
                                                <li>
                                                    setDependencyTreeBank: Dependency treebank source files in <a href="http://www.nltk.org/nltk_data/"> Penn Treebank format</a>
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val dependencyParser = new DependencyParserApproach()
    .setInputCols(Array("sentence", "pos", "token"))
    .setOutputCol("dependency")
    .setDependencyTreeBank("parser/dependency_treebank")
    .setNumberOfIterations(10)</code></pre>
                                        </div><!--//code-block--></div>
                                </div>

                                <!--Begin Typed Dependency Parser Doc -->
                                <h4 id="TypedDependencyParser" class="section-block">Typed Dependency Parser: Labeled grammatical relation</h4>
                                <ul class="nav nav-tabs" role="tablist">
                                    <li role="presentation" class="active"><a href="#python" aria-controls="home"
                                                                              role="tab" data-toggle="tab">Python</a>
                                    </li>
                                    <li role="presentation"><a href="#scala" aria-controls="profile" role="tab"
                                                               data-toggle="tab">Scala</a></li>
                                </ul>
                                <div class="tab-content">
                                    <div role="tabpanel" class="tab-pane active" id="python">
                                        <div class="code-block">
                                            <p>
                                                Labeled parser that finds a grammatical relation between two words in a sentence. Its input is a CoNLL2009 dataset.<br>
                                                <b>Type:</b> Labeled Dependency<br>
                                                <b>Requires:</b> Token, POS, Dependency<br>
                                                <b>Functions:</b>
                                            <ul>
                                                <li>
                                                    setNumberOfIterations: Number of iterations in training, converges to better accuracy
                                                </li>
                                                <li>
                                                    setConll2009FilePath: Path to a file in <a href="https://ufal.mff.cuni.cz/conll2009-st/trial-data.html"> CoNLL 2009 format</a>
                                                </li>
                                            </ul>
                                            <br>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-python">typed_dependency_parser = TypedDependencyParserApproach() \
            .setInputCols(["token", "pos", "dependency"]) \
            .setOutputCol("labdep") \
            .setConll2009FilePath("file://conll2009/eng.train") \
            .setNumberOfIterations(10)</code></pre>
                                        </div><!--//code-block-->
                                    </div>
                                    <div role="tabpanel" class="tab-pane" id="scala">
                                        <div class="code-block">
                                            <p>
                                                Unlabeled parser that finds a grammatical relation between two words in a sentence. Its input is a directory with dependency treebank files<br>
                                                <b>Type:</b> Labeled Dependency<br>
                                                <b>Requires:</b> Token, POS, Dependency<br>
                                            <ul>
                                                <li>
                                                    setNumberOfIterations: Number of iterations in training, converges to better accuracy
                                                </li>
                                                <li>
                                                    setDependencyTreeBank: Dependency treebank source files in <a href="http://www.nltk.org/nltk_data/"> Penn Treebank format</a>
                                                </li>
                                            </ul>
                                            <b>Example:</b><br>
                                            </p>
                                            <pre><code class="language-javascript">val typedDependencyParser = new TypedDependencyParserApproach()
    .setInputCols(Array("token", "pos", "dependency"))
    .setOutputCol("labdep")
    .setConll2009FilePath("conll2009/eng.train"))</code></pre>
                                        </div><!--//code-block--></div>
                                </div>
                                <!--End Annotators-->
                            </div>
                        </section>
                    </div>
                </div>
                <div class="doc-sidebar hidden-xs">
                    <nav id="doc-nav">
                        <ul id="doc-menu" class="nav doc-menu" style="overflow-y: auto;height: 100%;padding-top: 20px;padding-bottom: 30px" data-spy="affix">
                            <li style="font-size: 25px"><b>Index</b></li>
                            <li>
                                <input type="checkbox" class="collapsable" id="ii1" checked>
                                <label class="hover-label" for="ii1">Concepts</label>
                                <ul class="nav doc-sub-menu">
                                    <li><a class="scrollto" href="#Imports">Spark NLP Imports</a></li>
                                    <li><a class="scrollto" href="#MLPipeline">Spark ML Pipelines</a></li>
                                    <li><a class="scrollto" href="#LightPipeline">Light Pipelines</a></li>
                                    <li><a class="scrollto" href="#RecursivePipeline">Recursive Pipelines</a></li>
                                    <li><a class="scrollto" href="#ExternalResource">External Resources</a></li>
                                    <li><a class="scrollto" href="#ResourceHelper">ResourceHelper</a></li>
                                    <li><a class="scrollto" href="#EmbeddingsHelper">EmbeddingsHelper</a></li>
                                    <li><a class="scrollto" href="#AnnotatorEmbeddings">Annotators with Embeddings</a></li>
                                    <li><a class="scrollto" href="#ParamsFeatures">Params and Features</a></li>

                                </ul><!--//nav-->
                            </li>
                            <li>
                                <input type="checkbox"  class="collapsable" id="ii2">
                                <label class="hover-label" for="ii2">Pretrained Pipelines</label>
                                <ul class="nav doc-sub-menu">
                                    <li><a class="scrollto" href="#BasicPipeline">Basic Pipeline</a></li>
                                    <li><a class="scrollto" href="#AdvancedPipeline">Advanced Pipeline</a></li>
                                    <li><a class="scrollto" href="#SentimentPipeline">Sentiment Pipeline</a></li>

                                </ul><!--//nav-->
                            </li>
                            <li>
                                <input type="checkbox"  class="collapsable" id="ii3">
                                <label class="hover-label" for="ii3">Pretrained Models</label>
                                <ul class="nav doc-sub-menu">
                                    <li><a class="scrollto" href="#LemmaFast">Fast Lemmatizer</a></li>
                                    <li><a class="scrollto" href="#SpellFast">Fast Norvig Spell Checker</a></li>
                                    <li><a class="scrollto" href="#SymmSpellFast">Fast SymmDelete Spell Checker</a></li>
                                    <li><a class="scrollto" href="#ContextSpell">Context Spell Checker</a></li>
                                    <li><a class="scrollto" href="#PosFast">Fast Part of Speech</a></li>
                                    <li><a class="scrollto" href="#NerFast">Fast CRF NER</a></li>
                                    <li><a class="scrollto" href="#NerDL">Fast DL NER</a></li>


                                </ul><!--//nav-->
                            </li>
                            <li>
                              <input type="checkbox"  class="collapsable" id="ii4">
                                <label class="hover-label" for="ii4">Transformers</label>
                              <ul class="nav doc-sub-menu">
                                <li><a class="scrollto" href="#DocumentAssembler">Document Assembler</a></li>
                                <li><a class="scrollto" href="#TokenAssembler">Token Assembler</a></li>
                                <li><a class="scrollto" href="#Finisher">Finisher</a></li>

                              </ul><!--//nav-->
                            </li>
                            <li>
                                <input type="checkbox"  class="collapsable" id="ii5">
                                <label class="hover-label" for="ii5">Annotators</label>
                                <ul class="nav doc-sub-menu">
                                    <li><a class="scrollto" href="#Tokenizer">Tokenizer</a></li>
                                    <li><a class="scrollto" href="#Normalizer">Normalizer</a></li>
                                    <li><a class="scrollto" href="#Stemmer">Stemmer</a></li>
                                    <li><a class="scrollto" href="#Lemmatizer">Lemmatizer</a></li>
                                    <li><a class="scrollto" href="#RegexMatcher">Regex Matcher</a></li>
                                    <li><a class="scrollto" href="#TextMatcher">Text Matcher</a></li>
                                    <li><a class="scrollto" href="#Chunker">Chunker</a></li>
                                    <li><a class="scrollto" href="#DateMatcher">Date Matcher</a></li>
                                    <li><a class="scrollto" href="#SentenceDetector">Sentence Detector</a></li>
                                    <li><a class="scrollto" href="#SentenceDetector">Deep Sentence Detector</a></li>
                                    <li><a class="scrollto" href="#POSTagger">Part of Speech Tagger</a></li>
                                    <li><a class="scrollto" href="#ViveknSentimentDetector">Vivekn Sentiment Detector</a></li>
                                    <li><a class="scrollto" href="#SentimentDetector">Rule based Sentiment Detector</a></li>
                                    <li><a class="scrollto" href="#NERCRF">Named Entity Recognition CRF</a></li>
                                    <li><a class="scrollto" href="#NerDL">Named Entity Recognition DL</a></li>
                                    <li><a class="scrollto" href="#SpellChecker">Norvig Spell Checker</a></li>
                                    <li><a class="scrollto" href="#SymmChecker">Symmetric Spell Checker</a></li>
                                    <li><a class="scrollto" href="#DependencyParser">Dependency Parser</a></li>
                                    <li><a class="scrollto" href="#TypedDependencyParser">Typed Dependency Parser</a></li>
                                </ul><!--//nav-->
                            </li>
                            <li style="padding-bottom: 75px">
                                <input type="checkbox" class="collapsable" id="ii6">
                                <label class="hover-label" for="ii6">License and feedback</label>
                                <ul class="nav doc-sub-menu">
                                    <li><a class="link" href="contribute.html">Feedback & Contribute</a></li>
                                    <li><a class="link" href="license.html">Licence information</a></li>
                                </ul><!--//nav-->
                            </li>
                        </ul>
                    </nav>
                </div>
            </div>
        </div>
    </div>
</div>
<footer id="footer" class="footer text-center">
    <div id="includedFooter"></div>
</footer>
<!-- Main Javascript -->
<script type="text/javascript" src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="assets/plugins/prism/prism.js"></script>
<script type="text/javascript" src="assets/plugins/jquery-scrollTo/jquery.scrollTo.min.js"></script>
<script type="text/javascript" src="assets/plugins/lightbox/dist/ekko-lightbox.min.js"></script>
<script type="text/javascript" src="assets/plugins/jquery-match-height/jquery.matchHeight-min.js"></script>
<script type="text/javascript" src="assets/js/main.js"></script>
<script>

    $('.nav-tabs li a').click(function (e) {
        //get selected href
        var href = $(this).attr('href');

        //set all nav tabs to inactive
        $('.nav-tabs li').removeClass('active');

        //get all nav tabs matching the href and set to active
        $('.nav-tabs li a[href="' + href + '"]').closest('li').addClass('active');

        //active tab
        $('.tab-pane').removeClass('active');
        $('.tab-pane' + href).addClass('active');
    })

</script>
</body>
</html>
