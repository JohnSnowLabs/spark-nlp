{%- capture title -%}
GPT2Transformer
{%- endcapture -%}

{%- capture description -%}
GPT-2: the OpenAI Text-To-Text Transformer

GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million
web pages. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within
some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of
many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained
on more than 10X the amount of data.

GPT-2 displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of
unprecedented quality, where we prime the model with an input and have it generate a lengthy continuation. In
addition, GPT-2 outperforms other language models trained on specific domains (like Wikipedia, news, or books)
without needing to use these domain-specific training datasets. On language tasks like question answering, reading
comprehension, summarization, and translation, GPT-2 begins to learn these tasks from the raw text, using no
task-specific training data. While scores on these downstream tasks are far from state-of-the-art, they suggest
that the tasks can benefit from unsupervised techniques, given sufficient (unlabeled) data and compute.

Pretrained models can be loaded with `pretrained` of the companion object:
```
val gpt2 = GPT2Transformer.pretrained()
  .setInputCols("document")
  .setOutputCol("generation")
```
The default model is `"gpt2"`, if no name is provided.
For available pretrained models please see the [Models Hub](https://nlp.johnsnowlabs.com/models?q=gpt2).

For extended examples of usage, see [GPT2TestSpec](https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/seq2seq/GPT2TestSpec.scala).

**Sources:**
 - [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
 - https://github.com/openai/gpt-2

**Paper Abstract:**

*Natural language processing tasks, such as question answering, machine translation, reading comprehension, and
summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that
language models begin to learn these tasks without any explicit supervision when trained on a new dataset
of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by
the language model reach F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline
systems without using the 127,000+ training examples. The capacity of the language model is essential to the
success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks.
Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8
tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model
reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path
towards building language processing systems which learn to perform tasks from their naturally occurring
demonstrations.*

**Note:**

This is a very computationally expensive module especially on larger sequence.
The use of an accelerator such as GPU is recommended.
{%- endcapture -%}

{%- capture input_anno -%}
DOCUMENT
{%- endcapture -%}

{%- capture output_anno -%}
DOCUMENT
{%- endcapture -%}

{%- capture python_example -%}
import sparknlp
from sparknlp.base import *
from sparknlp.annotator import *
from pyspark.ml import Pipeline

documentAssembler = DocumentAssembler() \
    .setInputCol("text") \
    .setOutputCol("documents")

gpt2 = GPT2Transformer.pretrained("gpt2") \
    .setInputCols(["documents"]) \
    .setMaxOutputLength(50) \
    .setOutputCol("generation")

pipeline = Pipeline().setStages([documentAssembler, gpt2])
data = spark.createDataFrame([["My name is Leonardo."]]).toDF("text")
result = pipeline.fit(data).transform(data)
result.select("summaries.generation").show(truncate=False)
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|result                                                                                                                                                                                              |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|[My name is Leonardo. I am a man of letters. I have been a man for many years. I was born in the year 1776. I came to the United States in 1776, and I have lived in the United Kingdom since 1776.]|
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
{%- endcapture -%}

{%- capture scala_example -%}
import spark.implicits._
import com.johnsnowlabs.nlp.base.DocumentAssembler
import com.johnsnowlabs.nlp.annotators.seq2seq.GPT2Transformer
import org.apache.spark.ml.Pipeline

val documentAssembler = new DocumentAssembler()
  .setInputCol("text")
  .setOutputCol("documents")

val gpt2 = GPT2Transformer.pretrained("gpt2")
  .setInputCols(Array("documents"))
  .setMinOutputLength(10)
  .setMaxOutputLength(50)
  .setDoSample(false)
  .setTopK(50)
  .setNoRepeatNgramSize(3)
  .setOutputCol("generation")

val pipeline = new Pipeline().setStages(Array(documentAssembler, gpt2))

val data = Seq(
  "My name is Leonardo."
).toDF("text")
val result = pipeline.fit(data).transform(data)

results.select("generation.result").show(truncate = false)
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|result                                                                                                                                                                                              |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|[ My name is Leonardo. I am a man of letters. I have been a man for many years. I was born in the year 1776. I came to the United States in 1776, and I have lived in the United Kingdom since 1776]|
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

{%- endcapture -%}

{%- capture api_link -%}
[GPT2Transformer](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/seq2seq/GPT2Transformer)
{%- endcapture -%}

{%- capture python_api_link -%}
[GPT2Transformer](/api/python/reference/autosummary/python/sparknlp/annotator/seq2seq/gpt2_transformer/index.html#sparknlp.annotator.seq2seq.gpt2_transformer.GPT2Transformer)
{%- endcapture -%}

{%- capture source_link -%}
[GPT2Transformer](https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/seq2seq/GPT2Transformer.scala)
{%- endcapture -%}

{% include templates/anno_template.md
title=title
description=description
input_anno=input_anno
output_anno=output_anno
python_example=python_example
scala_example=scala_example
api_link=api_link
python_api_link=python_api_link
source_link=source_link
%}