<!DOCTYPE html><html lang="en">
  <head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMNWXW7PZM"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-NMNWXW7PZM');
</script><title>Spark NLP -Training</title><meta property="og:title" content="Spark NLP - Training"/>

<meta name="description" content="High Performance NLP with Apache Spark
">
<link rel="canonical" href="/docs/en/training"><link rel="alternate" type="application/rss+xml" title="Spark NLP" href="/feed.xml"><!-- start favicons snippet, use https://realfavicongenerator.net/ -->
<!---->
<!-- <link rel="apple-touch-icon" sizes="180x180" href="/fav.ico"> -->

<!---->
<!-- <link rel="icon" type="image/png" sizes="32x32" href="/fav.ico"> -->

<!---->
<!-- <link rel="icon" type="image/png" sizes="16x16" href="/fav.ico"> -->

<!---->
<!-- <link rel="manifest" href="/fav.ico"> --><link rel="mask-icon" href="/fav.ico" color="#fc4d50"><link rel="shortcut icon" href="/fav.ico">

<meta name="msapplication-TileColor" content="#ffc40d"><meta name="msapplication-config" content="/assets/browserconfig.xml">

<meta name="theme-color" content="#ffffff">
<!-- end favicons snippet --><link rel="stylesheet" href="/assets/css/main.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" >
<link rel="stylesheet" href="/static/models.css" /><!-- start custom head snippets -->
 <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700;800&display=swap" rel="stylesheet"> 
 <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
<!-- end custom head snippets -->
<script>(function() {
  window.isArray = function(val) {
    return Object.prototype.toString.call(val) === '[object Array]';
  };
  window.isString = function(val) {
    return typeof val === 'string';
  };

  window.decodeUrl = function(str) {
    return str ? decodeURIComponent(str.replace(/\+/g, '%20')) : '';
  };

  window.hasEvent = function(event) {
    return 'on'.concat(event) in window.document;
  };

  window.isOverallScroller = function(node) {
    return node === document.documentElement || node === document.body || node === window;
  };

  window.isFormElement = function(node) {
    var tagName = node.tagName;
    return tagName === 'INPUT' || tagName === 'SELECT' || tagName === 'TEXTAREA';
  };

  window.pageLoad = (function () {
    var loaded = false, cbs = [];
    window.addEventListener('load', function () {
      var i;
      loaded = true;
      if (cbs.length > 0) {
        for (i = 0; i < cbs.length; i++) {
          cbs[i]();
        }
      }
    });
    return {
      then: function(cb) {
        cb && (loaded ? cb() : (cbs.push(cb)));
      }
    };
  })();
})();
(function() {
  window.throttle = function(func, wait) {
    var args, result, thisArg, timeoutId, lastCalled = 0;

    function trailingCall() {
      lastCalled = new Date;
      timeoutId = null;
      result = func.apply(thisArg, args);
    }
    return function() {
      var now = new Date,
        remaining = wait - (now - lastCalled);

      args = arguments;
      thisArg = this;

      if (remaining <= 0) {
        clearTimeout(timeoutId);
        timeoutId = null;
        lastCalled = now;
        result = func.apply(thisArg, args);
      } else if (!timeoutId) {
        timeoutId = setTimeout(trailingCall, remaining);
      }
      return result;
    };
  };
})();
(function() {
  var Set = (function() {
    var add = function(item) {
      var i, data = this._data;
      for (i = 0; i < data.length; i++) {
        if (data[i] === item) {
          return;
        }
      }
      this.size ++;
      data.push(item);
      return data;
    };

    var Set = function(data) {
      this.size = 0;
      this._data = [];
      var i;
      if (data.length > 0) {
        for (i = 0; i < data.length; i++) {
          add.call(this, data[i]);
        }
      }
    };
    Set.prototype.add = add;
    Set.prototype.get = function(index) { return this._data[index]; };
    Set.prototype.has = function(item) {
      var i, data = this._data;
      for (i = 0; i < data.length; i++) {
        if (this.get(i) === item) {
          return true;
        }
      }
      return false;
    };
    Set.prototype.is = function(map) {
      if (map._data.length !== this._data.length) { return false; }
      var i, j, flag, tData = this._data, mData = map._data;
      for (i = 0; i < tData.length; i++) {
        for (flag = false, j = 0; j < mData.length; j++) {
          if (tData[i] === mData[j]) {
            flag = true;
            break;
          }
        }
        if (!flag) { return false; }
      }
      return true;
    };
    Set.prototype.values = function() {
      return this._data;
    };
    return Set;
  })();

  window.Lazyload = (function(doc) {
    var queue = {js: [], css: []}, sources = {js: {}, css: {}}, context = this;
    var createNode = function(name, attrs) {
      var node = doc.createElement(name), attr;
      for (attr in attrs) {
        if (attrs.hasOwnProperty(attr)) {
          node.setAttribute(attr, attrs[attr]);
        }
      }
      return node;
    };
    var end = function(type, url) {
      var s, q, qi, cbs, i, j, cur, val, flag;
      if (type === 'js' || type ==='css') {
        s = sources[type], q = queue[type];
        s[url] = true;
        for (i = 0; i < q.length; i++) {
          cur = q[i];
          if (cur.urls.has(url)) {
            qi = cur, val = qi.urls.values();
            qi && (cbs = qi.callbacks);
            for (flag = true, j = 0; j < val.length; j++) {
              cur = val[j];
              if (!s[cur]) {
                flag = false;
              }
            }
            if (flag && cbs && cbs.length > 0) {
              for (j = 0; j < cbs.length; j++) {
                cbs[j].call(context);
              }
              qi.load = true;
            }
          }
        }
      }
    };
    var load = function(type, urls, callback) {
      var s, q, qi, node, i, cur,
        _urls = typeof urls === 'string' ? new Set([urls]) : new Set(urls), val, url;
      if (type === 'js' || type ==='css') {
        s = sources[type], q = queue[type];
        for (i = 0; i < q.length; i++) {
          cur = q[i];
          if (_urls.is(cur.urls)) {
            qi = cur;
            break;
          }
        }
        val = _urls.values();
        if (qi) {
          callback && (qi.load || qi.callbacks.push(callback));
          callback && (qi.load && callback());
        } else {
          q.push({
            urls: _urls,
            callbacks: callback ? [callback] : [],
            load: false
          });
          for (i = 0; i < val.length; i++) {
            node = null, url = val[i];
            if (s[url] === undefined) {
              (type === 'js' ) && (node = createNode('script', { src: url }));
              (type === 'css') && (node = createNode('link', { rel: 'stylesheet', href: url }));
              if (node) {
                node.onload = (function(type, url) {
                  return function() {
                    end(type, url);
                  };
                })(type, url);
                (doc.head || doc.body).appendChild(node);
                s[url] = false;
              }
            }
          }
        }
      }
    };
    return {
      js: function(url, callback) {
        load('js', url, callback);
      },
      css: function(url, callback) {
        load('css', url, callback);
      }
    };
  })(this.document);
})();
</script><script>
  (function() {
    var TEXT_VARIABLES = {
      version: '2.2.4',
      sources: {
        font_awesome: 'https://use.fontawesome.com/releases/v5.0.13/css/all.css',
        jquery: 'https://cdn.bootcss.com/jquery/3.1.1/jquery.min.js',
        leancloud_js_sdk: '//cdn1.lncld.net/static/js/3.4.1/av-min.js',
        chart: 'https://cdn.bootcss.com/Chart.js/2.7.2/Chart.bundle.min.js',
        gitalk: {
          js: 'https://cdn.bootcss.com/gitalk/1.2.2/gitalk.min.js',
          css: 'https://cdn.bootcss.com/gitalk/1.2.2/gitalk.min.css'
        },
        valine: 'https://unpkg.com/valine/dist/Valine.min.js',
        mathjax: 'https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML',
        mermaid: 'https://cdn.bootcss.com/mermaid/8.0.0-rc.8/mermaid.min.js'
      },
      site: {
        toc: {
          selectors: 'h1,h2,h3'
        }
      },
      paths: {
        search_js: '/assets/search.js'
      }
    };
    window.TEXT_VARIABLES = TEXT_VARIABLES;
  })();
</script></head>
  <body>
    <div class="root" data-is-touch="false">
      <div class="layout--page layout--page--sidebar clearfix js-page-root&nbsp; layout--page--aside">
  <div class="page__mask d-print-none js-page-mask js-sidebar-hide"></div>
  <div class="page__viewport">
    <div class="page__actions d-print-none">
      <div class="js-sidebar-show">
        <i class="fas fa-bars icon--show"></i>
      </div>
    </div>

    <div class="grid page__grid">

      <div class="page__sidebar d-print-none"><a title="High Performance NLP with Apache Spark
" href="/">
    <!--<svg width="187" height="50" viewBox="0 0 187 50" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M38.6212 18.6877H42.3588V29.0697C42.3588 33.7209 40.1163 35.382 36.5448 35.382C35.7143 35.382 34.5515 35.2159 33.804 34.9668L34.2192 31.9767C34.7176 32.1428 35.382 32.3089 36.1295 32.3089C37.7076 32.3089 38.6212 31.6445 38.6212 29.0697V18.6877Z" fill="#3E4095"/>
<path d="M55.2325 28.9867C55.2325 33.3056 52.1594 35.299 48.9202 35.299C45.4319 35.299 42.774 32.9734 42.774 29.1528C42.774 25.3322 45.2657 22.8405 49.0863 22.8405C52.7408 22.8405 55.2325 25.4153 55.2325 28.9867ZM46.5946 29.0698C46.5946 31.1462 47.4252 32.6412 49.0033 32.6412C50.4152 32.6412 51.3289 31.2292 51.3289 29.0698C51.3289 27.3256 50.6644 25.4983 49.0033 25.4983C47.2591 25.4983 46.5946 27.3256 46.5946 29.0698Z" fill="#3E4095"/>
<path d="M55.6478 17.774H59.3854V24.5847H59.4684C59.8837 24.0863 60.382 23.6711 60.9634 23.3388C61.4618 23.0066 62.2093 22.8405 62.8737 22.8405C65.1993 22.8405 67.0266 24.5016 67.0266 28.0731V35.0498H63.289V28.4883C63.289 26.9103 62.7907 25.8305 61.3787 25.8305C60.382 25.8305 59.8006 26.495 59.5515 27.1594C59.4684 27.4086 59.4684 27.7408 59.4684 27.99V35.0498H55.6478V17.774Z" fill="#3E4095"/>
<path d="M68.1064 26.9103C68.1064 25.4153 68.0233 24.1694 68.0233 23.0897H71.2625L71.4286 24.7508C71.927 24.0033 73.0898 22.8405 75.0831 22.8405C77.4917 22.8405 79.319 24.4186 79.319 27.907V34.9668H75.5814V28.4053C75.5814 26.9103 75.0831 25.8305 73.6711 25.8305C72.6745 25.8305 72.01 26.495 71.7609 27.2425C71.6778 27.4917 71.5947 27.8239 71.5947 28.1561V35.0498H68.1064V26.9103Z" fill="#3E4095"/>
<path d="M83.887 31.2292C84.8836 31.7275 86.3787 32.2259 87.9567 32.2259C89.6179 32.2259 90.5315 31.5614 90.5315 30.4817C90.5315 29.485 89.784 28.9036 87.7906 28.1561C85.0497 27.2425 83.3056 25.6644 83.3056 23.3388C83.3056 20.5149 85.6311 18.4385 89.5348 18.4385C91.362 18.4385 92.774 18.8538 93.6876 19.269L92.8571 22.2591C92.1926 21.9268 91.0298 21.5116 89.4517 21.5116C87.8737 21.5116 87.0431 22.2591 87.0431 23.0896C87.0431 24.1694 87.9567 24.5847 90.1162 25.4152C93.0232 26.495 94.3521 27.99 94.3521 30.3156C94.3521 33.0564 92.2757 35.382 87.7076 35.382C85.7973 35.382 83.97 34.8837 83.0564 34.3853L83.887 31.2292Z" fill="#3E4095"/>
<path d="M94.9336 26.9103C94.9336 25.4153 94.8505 24.1694 94.8505 23.0897H98.0897L98.2558 24.7508H98.3389C98.8372 24.0033 100 22.8405 101.993 22.8405C104.402 22.8405 106.229 24.4186 106.229 27.907V34.9668H102.492V28.4053C102.492 26.9103 101.993 25.8305 100.581 25.8305C99.5847 25.8305 98.9203 26.495 98.6711 27.2425C98.5881 27.4917 98.505 27.8239 98.505 28.1561V35.0498H94.7675V26.9103H94.9336Z" fill="#3E4095"/>
<path d="M119.103 28.9867C119.103 33.3056 116.03 35.299 112.791 35.299C109.302 35.299 106.645 32.9734 106.645 29.1528C106.645 25.3322 109.136 22.8405 112.957 22.8405C116.694 22.8405 119.103 25.4153 119.103 28.9867ZM110.465 29.0698C110.465 31.1462 111.296 32.6412 112.874 32.6412C114.286 32.6412 115.199 31.2292 115.199 29.0698C115.199 27.3256 114.535 25.4983 112.874 25.4983C111.13 25.4983 110.465 27.3256 110.465 29.0698Z" fill="#3E4095"/>
<path d="M121.927 23.1727L122.841 28.0731C123.09 29.3189 123.339 30.6478 123.505 31.9767H123.588C123.837 30.6478 124.17 29.2359 124.502 28.0731L125.748 23.1727H128.655L129.817 27.9069C130.15 29.2359 130.482 30.5648 130.731 31.9767H130.814C130.98 30.6478 131.229 29.2359 131.478 27.9069L132.475 23.1727H136.13L132.475 35.0498H128.987L127.907 30.897C127.575 29.7342 127.409 28.6545 127.16 27.1594H127.076C126.827 28.6545 126.578 29.7342 126.329 30.897L125.166 35.0498H121.678L118.189 23.1727H121.927Z" fill="#3E4095"/>
<path d="M143.023 18.9369H145.1V32.8073H152.575V34.5515H143.023V18.9369Z" fill="#0098DA"/>
<path d="M155.399 29.5681L153.571 34.5515H151.329L157.226 18.9369H159.801L165.781 34.5515H163.455L161.545 29.5681H155.399ZM161.213 27.99L159.468 23.3389C159.136 22.3422 158.804 21.5116 158.555 20.6811H158.472C158.223 21.5116 157.973 22.3422 157.641 23.2558L155.897 27.99H161.213Z" fill="#0098DA"/>
<path d="M165.864 19.186C166.777 19.0199 168.355 18.8538 169.933 18.8538C172.176 18.8538 173.505 19.186 174.502 20.0166C175.332 20.6811 175.914 21.5947 175.914 22.8405C175.914 24.3355 174.834 25.6644 173.173 26.2458V26.3289C174.502 26.6611 176.495 27.8239 176.495 30.2326C176.495 31.5615 175.914 32.6412 175.083 33.3887C173.92 34.3854 172.093 34.8837 169.269 34.8837C167.774 34.8837 166.611 34.8007 165.864 34.7176V19.186ZM168.023 25.5814H170.183C172.508 25.5814 173.754 24.5017 173.754 23.0066C173.754 21.0963 172.176 20.4319 170.1 20.4319C169.02 20.4319 168.355 20.5149 168.023 20.598V25.5814ZM168.023 32.9734C168.521 33.0565 169.103 33.0565 169.933 33.0565C172.093 33.0565 174.252 32.392 174.252 29.9834C174.252 27.8239 172.342 26.9934 169.933 26.9934H167.94V32.9734H168.023Z" fill="#0098DA"/>
<path d="M176.91 31.9768C177.907 32.6412 179.402 33.1396 180.98 33.1396C183.223 33.1396 184.468 32.0598 184.468 30.4818C184.468 28.9867 183.638 28.1562 181.229 27.4087C178.239 26.495 176.661 25.1661 176.661 22.9236C176.661 20.4319 178.821 18.6047 182.06 18.6047C183.887 18.6047 185.133 19.02 185.963 19.4352L185.382 21.0964C184.884 20.7641 183.638 20.2658 182.06 20.2658C179.734 20.2658 178.821 21.5947 178.821 22.5914C178.821 24.0033 179.817 24.7509 182.226 25.4984C185.133 26.412 186.628 27.6578 186.628 30.1495C186.628 32.4751 184.884 34.7176 180.814 34.7176C179.153 34.7176 177.325 34.2193 176.412 33.6379L176.91 31.9768Z" fill="#0098DA"/>
<path d="M22.5083 35.6312C22.5083 40.1163 18.8538 43.7708 14.3688 43.7708C9.88372 43.7708 6.22924 40.1163 6.22924 35.6312V12.2093L0 11.4618V35.6312C0 43.6047 6.4784 50 14.3688 50C22.2591 50 28.7375 43.5216 28.7375 35.6312V11.4618L22.5083 12.2093V35.6312Z" fill="#0098DA"/>
<path d="M16.1129 17.7741H8.63786C8.13952 17.7741 7.72424 17.3588 7.72424 16.8604V9.38536C7.72424 8.88702 8.13952 8.47174 8.63786 8.47174H16.1129C16.6113 8.47174 17.0266 8.88702 17.0266 9.38536V16.8604C17.0266 17.3588 16.6113 17.7741 16.1129 17.7741Z" fill="#3E4095"/>
<path d="M20.515 22.7575H15.2824C14.7841 22.7575 14.3688 22.3422 14.3688 21.8439V16.6113C14.3688 16.113 14.7841 15.6977 15.2824 15.6977H20.515C21.0133 15.6977 21.4286 16.113 21.4286 16.6113V21.8439C21.4286 22.4253 21.0133 22.7575 20.515 22.7575Z" fill="#3E4095"/>
<path d="M19.8505 9.71762H16.113C15.6146 9.71762 15.1993 9.30233 15.1993 8.80399V5.06645C15.1993 4.56811 15.6146 4.15283 16.113 4.15283H19.8505C20.3488 4.15283 20.7641 4.56811 20.7641 5.06645V8.80399C20.6811 9.30233 20.3488 9.71762 19.8505 9.71762Z" fill="#3E4095"/>
<path d="M13.6213 3.48837H11.8771C11.3788 3.48837 10.9635 3.07309 10.9635 2.57475V0.913621C10.9635 0.415282 11.3788 0 11.8771 0H13.6213C14.1196 0 14.5349 0.415282 14.5349 0.913621V2.65781C14.5349 3.15615 14.1196 3.48837 13.6213 3.48837Z" fill="#3E4095"/>
<path d="M20.2658 41.196H8.38867V41.3622H20.2658V41.196Z" fill="#ECF9FF"/>
<path d="M20.2658 40.9469H8.38867V41.113H20.2658V40.9469Z" fill="#EBF9FF"/>
<path d="M20.2658 40.7808H8.38867V40.9469H20.2658V40.7808Z" fill="#EAF8FF"/>
<path d="M20.2658 40.6146H8.38867V40.7807H20.2658V40.6146Z" fill="#E9F8FF"/>
<path d="M20.2658 40.3655H8.38867V40.5316H20.2658V40.3655Z" fill="#E8F8FF"/>
<path d="M20.2658 40.1993H8.38867V40.3655H20.2658V40.1993Z" fill="#E7F7FF"/>
<path d="M20.2658 40.0333H8.38867V40.1994H20.2658V40.0333Z" fill="#E6F7FF"/>
<path d="M20.2658 39.8671H8.38867V40.0332H20.2658V39.8671Z" fill="#E5F7FF"/>
<path d="M20.2658 39.618H8.38867V39.7841H20.2658V39.618Z" fill="#E4F6FE"/>
<path d="M20.2658 39.4518H8.38867V39.618H20.2658V39.4518Z" fill="#E3F6FE"/>
<path d="M20.2658 39.2858H8.38867V39.4519H20.2658V39.2858Z" fill="#E2F5FE"/>
<path d="M20.2658 39.0366H8.38867V39.2027H20.2658V39.0366Z" fill="#E1F5FE"/>
<path d="M20.2658 38.8705H8.38867V39.0366H20.2658V38.8705Z" fill="#E0F5FE"/>
<path d="M20.2658 38.7043H8.38867V38.8705H20.2658V38.7043Z" fill="#DFF4FE"/>
<path d="M20.2658 38.4552H8.38867V38.6213H20.2658V38.4552Z" fill="#DEF4FE"/>
<path d="M20.2658 38.2891H8.38867V38.4552H20.2658V38.2891Z" fill="#DDF4FE"/>
<path d="M20.2658 38.1229H8.38867V38.289H20.2658V38.1229Z" fill="#DCF3FE"/>
<path d="M20.2658 37.8738H8.38867V38.0399H20.2658V37.8738Z" fill="#DBF3FE"/>
<path d="M20.2658 37.7077H8.38867V37.8738H20.2658V37.7077Z" fill="#DAF3FE"/>
<path d="M20.2658 37.5416H8.38867V37.7077H20.2658V37.5416Z" fill="#D9F2FE"/>
<path d="M20.2658 37.3754H8.38867V37.5415H20.2658V37.3754Z" fill="#D8F2FE"/>
<path d="M20.2658 37.1263H8.38867V37.2924H20.2658V37.1263Z" fill="#D7F2FE"/>
<path d="M20.2658 36.9601H8.38867V37.1263H20.2658V36.9601Z" fill="#D6F1FE"/>
<path d="M20.2658 36.7941H8.38867V36.9602H20.2658V36.7941Z" fill="#D5F1FE"/>
<path d="M20.2658 36.5449H8.38867V36.711H20.2658V36.5449Z" fill="#D4F1FD"/>
<path d="M20.2658 36.3788H8.38867V36.5449H20.2658V36.3788Z" fill="#D3F0FD"/>
<path d="M20.2658 36.2126H8.38867V36.3788H20.2658V36.2126Z" fill="#D2F0FD"/>
<path d="M20.2658 35.9635H8.38867V36.1296H20.2658V35.9635Z" fill="#D1F0FD"/>
<path d="M20.2658 35.7974H8.38867V35.9635H20.2658V35.7974Z" fill="#D0EFFD"/>
<path d="M20.2658 35.6313H8.38867V35.7974H20.2658V35.6313Z" fill="#CFEFFD"/>
<path d="M20.2658 35.3821H8.38867V35.5482H20.2658V35.3821Z" fill="#CEEEFD"/>
<path d="M20.2658 35.216H8.38867V35.3821H20.2658V35.216Z" fill="#CDEEFD"/>
<path d="M20.2658 35.0499H8.38867V35.216H20.2658V35.0499Z" fill="#CCEEFD"/>
<path d="M20.2658 34.8837H8.38867V35.0498H20.2658V34.8837Z" fill="#CBEDFD"/>
<path d="M20.2658 34.6346H8.38867V34.8007H20.2658V34.6346Z" fill="#CAEDFD"/>
<path d="M20.2658 34.4684H8.38867V34.6346H20.2658V34.4684Z" fill="#C9EDFD"/>
<path d="M20.2658 34.3024H8.38867V34.4685H20.2658V34.3024Z" fill="#C8ECFD"/>
<path d="M20.2658 34.0532H8.38867V34.2193H20.2658V34.0532Z" fill="#C7ECFD"/>
<path d="M20.2658 33.8871H8.38867V34.0532H20.2658V33.8871Z" fill="#C6ECFD"/>
<path d="M20.2658 33.7209H8.38867V33.8871H20.2658V33.7209Z" fill="#C4EBFC"/>
<path d="M20.2658 33.4718H8.38867V33.6379H20.2658V33.4718Z" fill="#C3EBFC"/>
<path d="M20.2658 33.3057H8.38867V33.4718H20.2658V33.3057Z" fill="#C2EBFC"/>
<path d="M20.2658 33.1396H8.38867V33.3057H20.2658V33.1396Z" fill="#C1EAFC"/>
<path d="M20.2658 32.8904H8.38867V33.0565H20.2658V32.8904Z" fill="#C0EAFC"/>
<path d="M20.2658 32.7242H8.38867V32.8904H20.2658V32.7242Z" fill="#BFEAFC"/>
<path d="M20.2658 32.5582H8.38867V32.7243H20.2658V32.5582Z" fill="#BEE9FC"/>
<path d="M20.2658 32.392H8.38867V32.5581H20.2658V32.392Z" fill="#BDE9FC"/>
<path d="M20.2658 32.1429H8.38867V32.309H20.2658V32.1429Z" fill="#BCE9FC"/>
<path d="M20.2658 31.9768H8.38867V32.1429H20.2658V31.9768Z" fill="#BBE8FC"/>
<path d="M20.2658 31.8107H8.38867V31.9768H20.2658V31.8107Z" fill="#BAE8FC"/>
<path d="M20.2658 31.5615H8.38867V31.7276H20.2658V31.5615Z" fill="#B9E7FC"/>
<path d="M20.2658 31.3954H8.38867V31.5615H20.2658V31.3954Z" fill="#B8E7FC"/>
<path d="M20.2658 31.2292H8.38867V31.3954H20.2658V31.2292Z" fill="#B7E7FC"/>
<path d="M20.2658 30.9801H8.38867V31.1462H20.2658V30.9801Z" fill="#B6E6FC"/>
<path d="M20.2658 30.814H8.38867V30.9801H20.2658V30.814Z" fill="#B5E6FB"/>
<path d="M20.2658 30.6479H8.38867V30.814H20.2658V30.6479Z" fill="#B4E6FB"/>
<path d="M20.2658 30.3987H8.38867V30.5648H20.2658V30.3987Z" fill="#B3E5FB"/>
<path d="M20.2658 30.2326H8.38867V30.3987H20.2658V30.2326Z" fill="#B2E5FB"/>
<path d="M20.2658 30.0665H8.38867V30.2326H20.2658V30.0665Z" fill="#B1E5FB"/>
<path d="M20.2658 29.9004H8.38867V30.0665H20.2658V29.9004Z" fill="#B0E4FB"/>
<path d="M20.2658 29.6512H8.38867V29.8173H20.2658V29.6512Z" fill="#AFE4FB"/>
<path d="M20.2658 29.4851H8.38867V29.6512H20.2658V29.4851Z" fill="#AEE4FB"/>
<path d="M20.2658 29.319H8.38867V29.4851H20.2658V29.319Z" fill="#ADE3FB"/>
<path d="M20.2658 29.0698H8.38867V29.2359H20.2658V29.0698Z" fill="#ACE3FB"/>
<path d="M20.2658 28.9037H8.38867V29.0698H20.2658V28.9037Z" fill="#ABE3FB"/>
<path d="M20.2658 28.7375H8.38867V28.9037H20.2658V28.7375Z" fill="#AAE2FB"/>
<path d="M20.2658 28.4884H8.38867V28.6545H20.2658V28.4884Z" fill="#A9E2FB"/>
<path d="M20.2658 28.3223H8.38867V28.4884H20.2658V28.3223Z" fill="#A8E2FB"/>
<path d="M20.2658 28.1562H8.38867V28.3223H20.2658V28.1562Z" fill="#A7E1FB"/>
<path d="M20.2658 27.907H8.38867V28.0731H20.2658V27.907Z" fill="#A6E1FB"/>
<path d="M20.2658 27.7409H8.38867V27.907H20.2658V27.7409Z" fill="#A5E0FA"/>
<path d="M20.2658 27.5748H8.38867V27.7409H20.2658V27.5748Z" fill="#A4E0FA"/>
<path d="M20.2658 27.4087H8.38867V27.5748H20.2658V27.4087Z" fill="#A3E0FA"/>
<path d="M20.2658 27.1595H8.38867V27.3256H20.2658V27.1595Z" fill="#A2DFFA"/>
<path d="M20.2658 26.9934H8.38867V27.1595H20.2658V26.9934Z" fill="#A1DFFA"/>
<path d="M20.2658 26.8273H8.38867V26.9934H20.2658V26.8273Z" fill="#A0DFFA"/>
<path d="M20.2658 26.5781H8.38867V26.7442H20.2658V26.5781Z" fill="#9FDEFA"/>
<path d="M20.2658 26.412H8.38867V26.5781H20.2658V26.412Z" fill="#9EDEFA"/>
</svg>
-->
</a><div class="sidebar-toc"><ul class="toc toc--navigator"><li class="toc-h1">Spark NLP</li><li class="toc-h2"><a href="/docs/en/quickstart">Spark NLP Getting Started</a></li><li class="toc-h2"><a href="/docs/en/install">Install Spark NLP</a></li><li class="toc-h2"><a href="/docs/en/concepts">General Concepts</a></li><li class="toc-h2"><a href="/docs/en/annotators">Annotators</a></li><li class="toc-h2"><a href="/docs/en/transformers">Transformers</a></li><li class="toc-h2 active"><a href="/docs/en/training">Training</a></li><li class="toc-h2"><a href="/docs/en/display">Spark NLP Display</a></li><li class="toc-h2"><a href="/docs/en/mlflow">Experiment Tracking</a></li><li class="toc-h2"><a href="/docs/en/serving_spark_nlp_via_api_databricks_mlflow">Serving Spark NLP&#58 MLFlow on Databricks</a></li><li class="toc-h2"><a href="/docs/en/hardware_acceleration">Hardware Acceleration</a></li><li class="toc-h2"><a href="/docs/en/CPUvsGPUbenchmark">GPU vs CPU Training</a></li><li class="toc-h2"><a href="/docs/en/auxiliary">Helpers</a></li><li class="toc-h2"><a href="/api/">Scala API (Scaladoc)</a></li><li class="toc-h2"><a href="/api/python/">Python API (Sphinx)</a></li><li class="toc-h2"><a href="/docs/en/developers">Developers</a></li><li class="toc-h2"><a href="/docs/en/third-party-projects">Third Party Projects</a></li><li class="toc-h2"><a href="/docs/en/release_notes">Release Notes</a></li><li class="toc-h2"><a href="/docs/en/faq">Spark NLP FAQ</a></li></ul></div></div><div class="page__main js-page-main has-aside cell cell--auto">

      <div class="page__main-inner"><div class="page__header d-print-none"><header class="header"><div class="main">
      <div class="header__title">
        <a class="responsive_btn" href="#" id="responsive_menu">
        <i class="fas fa-bars"></i>
        <i class="fas fa-times"></i>
        </a>
        <div class="header__brand">
          <a title="High Performance NLP with Apache Spark
" href="/"><svg width="187" height="50" viewBox="0 0 187 50" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M38.6212 18.6877H42.3588V29.0697C42.3588 33.7209 40.1163 35.382 36.5448 35.382C35.7143 35.382 34.5515 35.2159 33.804 34.9668L34.2192 31.9767C34.7176 32.1428 35.382 32.3089 36.1295 32.3089C37.7076 32.3089 38.6212 31.6445 38.6212 29.0697V18.6877Z" fill="#3E4095"/>
<path d="M55.2325 28.9867C55.2325 33.3056 52.1594 35.299 48.9202 35.299C45.4319 35.299 42.774 32.9734 42.774 29.1528C42.774 25.3322 45.2657 22.8405 49.0863 22.8405C52.7408 22.8405 55.2325 25.4153 55.2325 28.9867ZM46.5946 29.0698C46.5946 31.1462 47.4252 32.6412 49.0033 32.6412C50.4152 32.6412 51.3289 31.2292 51.3289 29.0698C51.3289 27.3256 50.6644 25.4983 49.0033 25.4983C47.2591 25.4983 46.5946 27.3256 46.5946 29.0698Z" fill="#3E4095"/>
<path d="M55.6478 17.774H59.3854V24.5847H59.4684C59.8837 24.0863 60.382 23.6711 60.9634 23.3388C61.4618 23.0066 62.2093 22.8405 62.8737 22.8405C65.1993 22.8405 67.0266 24.5016 67.0266 28.0731V35.0498H63.289V28.4883C63.289 26.9103 62.7907 25.8305 61.3787 25.8305C60.382 25.8305 59.8006 26.495 59.5515 27.1594C59.4684 27.4086 59.4684 27.7408 59.4684 27.99V35.0498H55.6478V17.774Z" fill="#3E4095"/>
<path d="M68.1064 26.9103C68.1064 25.4153 68.0233 24.1694 68.0233 23.0897H71.2625L71.4286 24.7508C71.927 24.0033 73.0898 22.8405 75.0831 22.8405C77.4917 22.8405 79.319 24.4186 79.319 27.907V34.9668H75.5814V28.4053C75.5814 26.9103 75.0831 25.8305 73.6711 25.8305C72.6745 25.8305 72.01 26.495 71.7609 27.2425C71.6778 27.4917 71.5947 27.8239 71.5947 28.1561V35.0498H68.1064V26.9103Z" fill="#3E4095"/>
<path d="M83.887 31.2292C84.8836 31.7275 86.3787 32.2259 87.9567 32.2259C89.6179 32.2259 90.5315 31.5614 90.5315 30.4817C90.5315 29.485 89.784 28.9036 87.7906 28.1561C85.0497 27.2425 83.3056 25.6644 83.3056 23.3388C83.3056 20.5149 85.6311 18.4385 89.5348 18.4385C91.362 18.4385 92.774 18.8538 93.6876 19.269L92.8571 22.2591C92.1926 21.9268 91.0298 21.5116 89.4517 21.5116C87.8737 21.5116 87.0431 22.2591 87.0431 23.0896C87.0431 24.1694 87.9567 24.5847 90.1162 25.4152C93.0232 26.495 94.3521 27.99 94.3521 30.3156C94.3521 33.0564 92.2757 35.382 87.7076 35.382C85.7973 35.382 83.97 34.8837 83.0564 34.3853L83.887 31.2292Z" fill="#3E4095"/>
<path d="M94.9336 26.9103C94.9336 25.4153 94.8505 24.1694 94.8505 23.0897H98.0897L98.2558 24.7508H98.3389C98.8372 24.0033 100 22.8405 101.993 22.8405C104.402 22.8405 106.229 24.4186 106.229 27.907V34.9668H102.492V28.4053C102.492 26.9103 101.993 25.8305 100.581 25.8305C99.5847 25.8305 98.9203 26.495 98.6711 27.2425C98.5881 27.4917 98.505 27.8239 98.505 28.1561V35.0498H94.7675V26.9103H94.9336Z" fill="#3E4095"/>
<path d="M119.103 28.9867C119.103 33.3056 116.03 35.299 112.791 35.299C109.302 35.299 106.645 32.9734 106.645 29.1528C106.645 25.3322 109.136 22.8405 112.957 22.8405C116.694 22.8405 119.103 25.4153 119.103 28.9867ZM110.465 29.0698C110.465 31.1462 111.296 32.6412 112.874 32.6412C114.286 32.6412 115.199 31.2292 115.199 29.0698C115.199 27.3256 114.535 25.4983 112.874 25.4983C111.13 25.4983 110.465 27.3256 110.465 29.0698Z" fill="#3E4095"/>
<path d="M121.927 23.1727L122.841 28.0731C123.09 29.3189 123.339 30.6478 123.505 31.9767H123.588C123.837 30.6478 124.17 29.2359 124.502 28.0731L125.748 23.1727H128.655L129.817 27.9069C130.15 29.2359 130.482 30.5648 130.731 31.9767H130.814C130.98 30.6478 131.229 29.2359 131.478 27.9069L132.475 23.1727H136.13L132.475 35.0498H128.987L127.907 30.897C127.575 29.7342 127.409 28.6545 127.16 27.1594H127.076C126.827 28.6545 126.578 29.7342 126.329 30.897L125.166 35.0498H121.678L118.189 23.1727H121.927Z" fill="#3E4095"/>
<path d="M143.023 18.9369H145.1V32.8073H152.575V34.5515H143.023V18.9369Z" fill="#0098DA"/>
<path d="M155.399 29.5681L153.571 34.5515H151.329L157.226 18.9369H159.801L165.781 34.5515H163.455L161.545 29.5681H155.399ZM161.213 27.99L159.468 23.3389C159.136 22.3422 158.804 21.5116 158.555 20.6811H158.472C158.223 21.5116 157.973 22.3422 157.641 23.2558L155.897 27.99H161.213Z" fill="#0098DA"/>
<path d="M165.864 19.186C166.777 19.0199 168.355 18.8538 169.933 18.8538C172.176 18.8538 173.505 19.186 174.502 20.0166C175.332 20.6811 175.914 21.5947 175.914 22.8405C175.914 24.3355 174.834 25.6644 173.173 26.2458V26.3289C174.502 26.6611 176.495 27.8239 176.495 30.2326C176.495 31.5615 175.914 32.6412 175.083 33.3887C173.92 34.3854 172.093 34.8837 169.269 34.8837C167.774 34.8837 166.611 34.8007 165.864 34.7176V19.186ZM168.023 25.5814H170.183C172.508 25.5814 173.754 24.5017 173.754 23.0066C173.754 21.0963 172.176 20.4319 170.1 20.4319C169.02 20.4319 168.355 20.5149 168.023 20.598V25.5814ZM168.023 32.9734C168.521 33.0565 169.103 33.0565 169.933 33.0565C172.093 33.0565 174.252 32.392 174.252 29.9834C174.252 27.8239 172.342 26.9934 169.933 26.9934H167.94V32.9734H168.023Z" fill="#0098DA"/>
<path d="M176.91 31.9768C177.907 32.6412 179.402 33.1396 180.98 33.1396C183.223 33.1396 184.468 32.0598 184.468 30.4818C184.468 28.9867 183.638 28.1562 181.229 27.4087C178.239 26.495 176.661 25.1661 176.661 22.9236C176.661 20.4319 178.821 18.6047 182.06 18.6047C183.887 18.6047 185.133 19.02 185.963 19.4352L185.382 21.0964C184.884 20.7641 183.638 20.2658 182.06 20.2658C179.734 20.2658 178.821 21.5947 178.821 22.5914C178.821 24.0033 179.817 24.7509 182.226 25.4984C185.133 26.412 186.628 27.6578 186.628 30.1495C186.628 32.4751 184.884 34.7176 180.814 34.7176C179.153 34.7176 177.325 34.2193 176.412 33.6379L176.91 31.9768Z" fill="#0098DA"/>
<path d="M22.5083 35.6312C22.5083 40.1163 18.8538 43.7708 14.3688 43.7708C9.88372 43.7708 6.22924 40.1163 6.22924 35.6312V12.2093L0 11.4618V35.6312C0 43.6047 6.4784 50 14.3688 50C22.2591 50 28.7375 43.5216 28.7375 35.6312V11.4618L22.5083 12.2093V35.6312Z" fill="#0098DA"/>
<path d="M16.1129 17.7741H8.63786C8.13952 17.7741 7.72424 17.3588 7.72424 16.8604V9.38536C7.72424 8.88702 8.13952 8.47174 8.63786 8.47174H16.1129C16.6113 8.47174 17.0266 8.88702 17.0266 9.38536V16.8604C17.0266 17.3588 16.6113 17.7741 16.1129 17.7741Z" fill="#3E4095"/>
<path d="M20.515 22.7575H15.2824C14.7841 22.7575 14.3688 22.3422 14.3688 21.8439V16.6113C14.3688 16.113 14.7841 15.6977 15.2824 15.6977H20.515C21.0133 15.6977 21.4286 16.113 21.4286 16.6113V21.8439C21.4286 22.4253 21.0133 22.7575 20.515 22.7575Z" fill="#3E4095"/>
<path d="M19.8505 9.71762H16.113C15.6146 9.71762 15.1993 9.30233 15.1993 8.80399V5.06645C15.1993 4.56811 15.6146 4.15283 16.113 4.15283H19.8505C20.3488 4.15283 20.7641 4.56811 20.7641 5.06645V8.80399C20.6811 9.30233 20.3488 9.71762 19.8505 9.71762Z" fill="#3E4095"/>
<path d="M13.6213 3.48837H11.8771C11.3788 3.48837 10.9635 3.07309 10.9635 2.57475V0.913621C10.9635 0.415282 11.3788 0 11.8771 0H13.6213C14.1196 0 14.5349 0.415282 14.5349 0.913621V2.65781C14.5349 3.15615 14.1196 3.48837 13.6213 3.48837Z" fill="#3E4095"/>
<path d="M20.2658 41.196H8.38867V41.3622H20.2658V41.196Z" fill="#ECF9FF"/>
<path d="M20.2658 40.9469H8.38867V41.113H20.2658V40.9469Z" fill="#EBF9FF"/>
<path d="M20.2658 40.7808H8.38867V40.9469H20.2658V40.7808Z" fill="#EAF8FF"/>
<path d="M20.2658 40.6146H8.38867V40.7807H20.2658V40.6146Z" fill="#E9F8FF"/>
<path d="M20.2658 40.3655H8.38867V40.5316H20.2658V40.3655Z" fill="#E8F8FF"/>
<path d="M20.2658 40.1993H8.38867V40.3655H20.2658V40.1993Z" fill="#E7F7FF"/>
<path d="M20.2658 40.0333H8.38867V40.1994H20.2658V40.0333Z" fill="#E6F7FF"/>
<path d="M20.2658 39.8671H8.38867V40.0332H20.2658V39.8671Z" fill="#E5F7FF"/>
<path d="M20.2658 39.618H8.38867V39.7841H20.2658V39.618Z" fill="#E4F6FE"/>
<path d="M20.2658 39.4518H8.38867V39.618H20.2658V39.4518Z" fill="#E3F6FE"/>
<path d="M20.2658 39.2858H8.38867V39.4519H20.2658V39.2858Z" fill="#E2F5FE"/>
<path d="M20.2658 39.0366H8.38867V39.2027H20.2658V39.0366Z" fill="#E1F5FE"/>
<path d="M20.2658 38.8705H8.38867V39.0366H20.2658V38.8705Z" fill="#E0F5FE"/>
<path d="M20.2658 38.7043H8.38867V38.8705H20.2658V38.7043Z" fill="#DFF4FE"/>
<path d="M20.2658 38.4552H8.38867V38.6213H20.2658V38.4552Z" fill="#DEF4FE"/>
<path d="M20.2658 38.2891H8.38867V38.4552H20.2658V38.2891Z" fill="#DDF4FE"/>
<path d="M20.2658 38.1229H8.38867V38.289H20.2658V38.1229Z" fill="#DCF3FE"/>
<path d="M20.2658 37.8738H8.38867V38.0399H20.2658V37.8738Z" fill="#DBF3FE"/>
<path d="M20.2658 37.7077H8.38867V37.8738H20.2658V37.7077Z" fill="#DAF3FE"/>
<path d="M20.2658 37.5416H8.38867V37.7077H20.2658V37.5416Z" fill="#D9F2FE"/>
<path d="M20.2658 37.3754H8.38867V37.5415H20.2658V37.3754Z" fill="#D8F2FE"/>
<path d="M20.2658 37.1263H8.38867V37.2924H20.2658V37.1263Z" fill="#D7F2FE"/>
<path d="M20.2658 36.9601H8.38867V37.1263H20.2658V36.9601Z" fill="#D6F1FE"/>
<path d="M20.2658 36.7941H8.38867V36.9602H20.2658V36.7941Z" fill="#D5F1FE"/>
<path d="M20.2658 36.5449H8.38867V36.711H20.2658V36.5449Z" fill="#D4F1FD"/>
<path d="M20.2658 36.3788H8.38867V36.5449H20.2658V36.3788Z" fill="#D3F0FD"/>
<path d="M20.2658 36.2126H8.38867V36.3788H20.2658V36.2126Z" fill="#D2F0FD"/>
<path d="M20.2658 35.9635H8.38867V36.1296H20.2658V35.9635Z" fill="#D1F0FD"/>
<path d="M20.2658 35.7974H8.38867V35.9635H20.2658V35.7974Z" fill="#D0EFFD"/>
<path d="M20.2658 35.6313H8.38867V35.7974H20.2658V35.6313Z" fill="#CFEFFD"/>
<path d="M20.2658 35.3821H8.38867V35.5482H20.2658V35.3821Z" fill="#CEEEFD"/>
<path d="M20.2658 35.216H8.38867V35.3821H20.2658V35.216Z" fill="#CDEEFD"/>
<path d="M20.2658 35.0499H8.38867V35.216H20.2658V35.0499Z" fill="#CCEEFD"/>
<path d="M20.2658 34.8837H8.38867V35.0498H20.2658V34.8837Z" fill="#CBEDFD"/>
<path d="M20.2658 34.6346H8.38867V34.8007H20.2658V34.6346Z" fill="#CAEDFD"/>
<path d="M20.2658 34.4684H8.38867V34.6346H20.2658V34.4684Z" fill="#C9EDFD"/>
<path d="M20.2658 34.3024H8.38867V34.4685H20.2658V34.3024Z" fill="#C8ECFD"/>
<path d="M20.2658 34.0532H8.38867V34.2193H20.2658V34.0532Z" fill="#C7ECFD"/>
<path d="M20.2658 33.8871H8.38867V34.0532H20.2658V33.8871Z" fill="#C6ECFD"/>
<path d="M20.2658 33.7209H8.38867V33.8871H20.2658V33.7209Z" fill="#C4EBFC"/>
<path d="M20.2658 33.4718H8.38867V33.6379H20.2658V33.4718Z" fill="#C3EBFC"/>
<path d="M20.2658 33.3057H8.38867V33.4718H20.2658V33.3057Z" fill="#C2EBFC"/>
<path d="M20.2658 33.1396H8.38867V33.3057H20.2658V33.1396Z" fill="#C1EAFC"/>
<path d="M20.2658 32.8904H8.38867V33.0565H20.2658V32.8904Z" fill="#C0EAFC"/>
<path d="M20.2658 32.7242H8.38867V32.8904H20.2658V32.7242Z" fill="#BFEAFC"/>
<path d="M20.2658 32.5582H8.38867V32.7243H20.2658V32.5582Z" fill="#BEE9FC"/>
<path d="M20.2658 32.392H8.38867V32.5581H20.2658V32.392Z" fill="#BDE9FC"/>
<path d="M20.2658 32.1429H8.38867V32.309H20.2658V32.1429Z" fill="#BCE9FC"/>
<path d="M20.2658 31.9768H8.38867V32.1429H20.2658V31.9768Z" fill="#BBE8FC"/>
<path d="M20.2658 31.8107H8.38867V31.9768H20.2658V31.8107Z" fill="#BAE8FC"/>
<path d="M20.2658 31.5615H8.38867V31.7276H20.2658V31.5615Z" fill="#B9E7FC"/>
<path d="M20.2658 31.3954H8.38867V31.5615H20.2658V31.3954Z" fill="#B8E7FC"/>
<path d="M20.2658 31.2292H8.38867V31.3954H20.2658V31.2292Z" fill="#B7E7FC"/>
<path d="M20.2658 30.9801H8.38867V31.1462H20.2658V30.9801Z" fill="#B6E6FC"/>
<path d="M20.2658 30.814H8.38867V30.9801H20.2658V30.814Z" fill="#B5E6FB"/>
<path d="M20.2658 30.6479H8.38867V30.814H20.2658V30.6479Z" fill="#B4E6FB"/>
<path d="M20.2658 30.3987H8.38867V30.5648H20.2658V30.3987Z" fill="#B3E5FB"/>
<path d="M20.2658 30.2326H8.38867V30.3987H20.2658V30.2326Z" fill="#B2E5FB"/>
<path d="M20.2658 30.0665H8.38867V30.2326H20.2658V30.0665Z" fill="#B1E5FB"/>
<path d="M20.2658 29.9004H8.38867V30.0665H20.2658V29.9004Z" fill="#B0E4FB"/>
<path d="M20.2658 29.6512H8.38867V29.8173H20.2658V29.6512Z" fill="#AFE4FB"/>
<path d="M20.2658 29.4851H8.38867V29.6512H20.2658V29.4851Z" fill="#AEE4FB"/>
<path d="M20.2658 29.319H8.38867V29.4851H20.2658V29.319Z" fill="#ADE3FB"/>
<path d="M20.2658 29.0698H8.38867V29.2359H20.2658V29.0698Z" fill="#ACE3FB"/>
<path d="M20.2658 28.9037H8.38867V29.0698H20.2658V28.9037Z" fill="#ABE3FB"/>
<path d="M20.2658 28.7375H8.38867V28.9037H20.2658V28.7375Z" fill="#AAE2FB"/>
<path d="M20.2658 28.4884H8.38867V28.6545H20.2658V28.4884Z" fill="#A9E2FB"/>
<path d="M20.2658 28.3223H8.38867V28.4884H20.2658V28.3223Z" fill="#A8E2FB"/>
<path d="M20.2658 28.1562H8.38867V28.3223H20.2658V28.1562Z" fill="#A7E1FB"/>
<path d="M20.2658 27.907H8.38867V28.0731H20.2658V27.907Z" fill="#A6E1FB"/>
<path d="M20.2658 27.7409H8.38867V27.907H20.2658V27.7409Z" fill="#A5E0FA"/>
<path d="M20.2658 27.5748H8.38867V27.7409H20.2658V27.5748Z" fill="#A4E0FA"/>
<path d="M20.2658 27.4087H8.38867V27.5748H20.2658V27.4087Z" fill="#A3E0FA"/>
<path d="M20.2658 27.1595H8.38867V27.3256H20.2658V27.1595Z" fill="#A2DFFA"/>
<path d="M20.2658 26.9934H8.38867V27.1595H20.2658V26.9934Z" fill="#A1DFFA"/>
<path d="M20.2658 26.8273H8.38867V26.9934H20.2658V26.8273Z" fill="#A0DFFA"/>
<path d="M20.2658 26.5781H8.38867V26.7442H20.2658V26.5781Z" fill="#9FDEFA"/>
<path d="M20.2658 26.412H8.38867V26.5781H20.2658V26.412Z" fill="#9EDEFA"/>
</svg>
</a><!---->
            <!-- <a title="High Performance NLP with Apache Spark
" href="/">Spark NLP</a> -->
          <!---->
        </div></div><nav class="navigation top_navigation">
        <ul class="top-menu"><li class="navigation__item "><a href="/">Home</a></li><li class="navigation__item navigation__item--active"><a href="/docs/en/quickstart">Docs</a></li><li class="navigation__item "><a href="/models">Models</a></li><li class="navigation__item "><a href="/infer_meaning_intent">Demo</a></li><li class="navigation__item "><a href="https://www.johnsnowlabs.com/spark-nlp-blog/"><a href="https://www.johnsnowlabs.com/spark-nlp-blog/" target="_blank">Blog</a></a></li><li class="f_inner"><a class="github-button" href="https://github.com/JohnSnowLabs/spark-nlp" data-color-scheme="no-preference: light; light: light; dark: dark;" data-size="large" data-show-count="true" aria-label="Star spark-nlp on GitHub">Star on GitHub</a></li>
        </ul>
      </nav><a class="responsive_btn" href="#" id="aside_menu">
        <i class="fas fa-bars"></i>
        <i class="fas fa-times"></i>
        </a>
    </div>
  </header>
</div><div class="page__content "><div class ="main"><div class="grid grid--reverse">

              <div class="col-aside d-print-none js-col-aside"><aside class="page__aside js-page-aside"><div class="toc-aside js-toc-root"></div></aside></div>

              <div class="col-main cell cell--auto"><!-- start custom main top snippet -->

<!-- end custom main top snippet --><article itemscope itemtype="http://schema.org/Article"><script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script><div class="article__header"><div class="header-nav"><div class="main-docs">
  <ul class="breadcrambs">
    <li><a href="/docs">Documentation</a></li>
    <li>Spark NLP -Training</li>
  </ul>
</div></div><header class="main-docs have_subtitle">
          <h1>Spark NLP -Training</h1><div class="top-subtitle mont"></div></header><span class="split-space">&nbsp;</span>
          <a class="edit-on-github"
            title="Edit on Github"
            href="https://github.com/johnsnowlabs/spark-nlp/tree/master/docs/en/training.md">
            <i class="far fa-edit"></i></a></div><meta itemprop="headline" content="Spark NLP -Training"><meta itemprop="author" content=""/><div class="js-article-content"><div class="docs-wrapper">
<div class="layout--article"><!-- start custom article top snippet -->

<!-- end custom article top snippet --><div class="article__content" itemprop="articleBody"><div class="h3-box">

  <h2 id="training-datasets">Training Datasets</h2>
  <p>These are classes to load common datasets to train annotators for tasks such as
part-of-speech tagging, named entity recognition, spell checking and more.</p>

  <div class="h3-box">

    <h3 id="pos-dataset">POS Dataset</h3>

    <p>In order to train a Part of Speech Tagger annotator, we need to get corpus data as a Spark dataframe. There is a component that does this for us: it reads a plain text file and transforms it to a Spark dataset.</p>

    <p><strong>Input File Format:</strong></p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A|DT few|JJ months|NNS ago|RB you|PRP received|VBD a|DT letter|NN
</code></pre></div>    </div>

    <p><strong>Constructor Parameters:</strong></p>

    <p>None</p>

    <p><strong>Parameters for <code class="language-plaintext highlighter-rouge">readDataset</code>:</strong></p>

    <ul>
      <li><strong>spark</strong>: Initiated Spark Session with Spark NLP</li>
      <li><strong>path</strong>: Path to the resource</li>
      <li><strong>delimiter</strong>: Delimiter of word and POS, by default “|”</li>
      <li><strong>outputPosCol</strong>: Name of the output POS column, by default “tags”</li>
      <li><strong>outputDocumentCol</strong>: Name of the output document column, by default “document”</li>
      <li><strong>outputTextCol</strong>: Name of the output text column, by default “text”</li>
    </ul>

    <p>Refer to the documentation for more details on the API:</p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/training/pos/index.html#sparknlp.training.pos.POS">POS</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/training/POS.html">POS</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/training/POS.scala">POS.scala</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="n">POS</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">POS</span><span class="p">()</span>
<span class="n">path</span> <span class="o">=</span> <span class="s">"src/test/resources/anc-pos-corpus-small/test-training.txt"</span>
<span class="n">posDf</span> <span class="o">=</span> <span class="n">pos</span><span class="p">.</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="s">"|"</span><span class="p">,</span> <span class="s">"tags"</span><span class="p">)</span>
<span class="n">posDf</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(tags) as tags"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+---------------------------------------------+</span>
<span class="o">|</span><span class="n">tags</span>                                         <span class="o">|</span>
<span class="o">+---------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">NNP</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="n">Pierre</span><span class="p">],</span> <span class="p">[]]</span>       <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="n">NNP</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="n">Vinken</span><span class="p">],</span> <span class="p">[]]</span>      <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="p">,,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="p">,],</span> <span class="p">[]]</span>            <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="n">CD</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="mi">61</span><span class="p">],</span> <span class="p">[]]</span>          <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">23</span><span class="p">,</span> <span class="n">NNS</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="n">years</span><span class="p">],</span> <span class="p">[]]</span>      <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">27</span><span class="p">,</span> <span class="n">JJ</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="n">old</span><span class="p">],</span> <span class="p">[]]</span>         <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="p">,,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="p">,],</span> <span class="p">[]]</span>            <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">31</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="n">MD</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="n">will</span><span class="p">],</span> <span class="p">[]]</span>        <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">36</span><span class="p">,</span> <span class="mi">39</span><span class="p">,</span> <span class="n">VB</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="n">join</span><span class="p">],</span> <span class="p">[]]</span>        <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">41</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="n">DT</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="n">the</span><span class="p">],</span> <span class="p">[]]</span>         <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">45</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="n">NN</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="n">board</span><span class="p">],</span> <span class="p">[]]</span>       <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mi">52</span><span class="p">,</span> <span class="n">IN</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="k">as</span><span class="p">],</span> <span class="p">[]]</span>          <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">47</span><span class="p">,</span> <span class="mi">47</span><span class="p">,</span> <span class="n">DT</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="n">a</span><span class="p">],</span> <span class="p">[]]</span>           <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">56</span><span class="p">,</span> <span class="mi">67</span><span class="p">,</span> <span class="n">JJ</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="n">nonexecutive</span><span class="p">],</span> <span class="p">[]]</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">69</span><span class="p">,</span> <span class="mi">76</span><span class="p">,</span> <span class="n">NN</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="n">director</span><span class="p">],</span> <span class="p">[]]</span>    <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">78</span><span class="p">,</span> <span class="mi">81</span><span class="p">,</span> <span class="n">NNP</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="n">Nov</span><span class="p">.],</span> <span class="p">[]]</span>       <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">83</span><span class="p">,</span> <span class="mi">84</span><span class="p">,</span> <span class="n">CD</span><span class="p">,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="mi">29</span><span class="p">],</span> <span class="p">[]]</span>          <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="mi">81</span><span class="p">,</span> <span class="mi">81</span><span class="p">,</span> <span class="p">.,</span> <span class="p">[</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="p">.],</span> <span class="p">[]]</span>            <span class="o">|</span>
<span class="o">+---------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.POS</span>

<span class="k">val</span> <span class="nv">pos</span> <span class="k">=</span> <span class="nc">POS</span><span class="o">()</span>
<span class="k">val</span> <span class="nv">path</span> <span class="k">=</span> <span class="s">"src/test/resources/anc-pos-corpus-small/test-training.txt"</span>
<span class="k">val</span> <span class="nv">posDf</span> <span class="k">=</span> <span class="nv">pos</span><span class="o">.</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="n">path</span><span class="o">,</span> <span class="s">"|"</span><span class="o">,</span> <span class="s">"tags"</span><span class="o">)</span>

<span class="nv">posDf</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(tags) as tags"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+---------------------------------------------+</span>
<span class="o">|</span><span class="n">tags</span>                                         <span class="o">|</span>
<span class="o">+---------------------------------------------+</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">0</span>, <span class="err">5</span>, <span class="kt">NNP</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">Pierre</span><span class="o">]</span>, <span class="o">[]]</span>       <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">7</span>, <span class="err">12</span>, <span class="kt">NNP</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">Vinken</span><span class="o">]</span>, <span class="o">[]]</span>      <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">14</span>, <span class="err">14</span>, ,, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> ,<span class="o">]</span>, <span class="o">[]]</span>            <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">16</span>, <span class="err">17</span>, <span class="kt">CD</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="err">61</span><span class="o">]</span>, <span class="o">[]]</span>          <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">19</span>, <span class="err">23</span>, <span class="kt">NNS</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">years</span><span class="o">]</span>, <span class="o">[]]</span>      <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">25</span>, <span class="err">27</span>, <span class="kt">JJ</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">old</span><span class="o">]</span>, <span class="o">[]]</span>         <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">29</span>, <span class="err">29</span>, ,, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> ,<span class="o">]</span>, <span class="o">[]]</span>            <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">31</span>, <span class="err">34</span>, <span class="kt">MD</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">will</span><span class="o">]</span>, <span class="o">[]]</span>        <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">36</span>, <span class="err">39</span>, <span class="kt">VB</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">join</span><span class="o">]</span>, <span class="o">[]]</span>        <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">41</span>, <span class="err">43</span>, <span class="kt">DT</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">the</span><span class="o">]</span>, <span class="o">[]]</span>         <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">45</span>, <span class="err">49</span>, <span class="kt">NN</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">board</span><span class="o">]</span>, <span class="o">[]]</span>       <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">51</span>, <span class="err">52</span>, <span class="kt">IN</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">as</span><span class="o">]</span>, <span class="o">[]]</span>          <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">47</span>, <span class="err">47</span>, <span class="kt">DT</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">a</span><span class="o">]</span>, <span class="o">[]]</span>           <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">56</span>, <span class="err">67</span>, <span class="kt">JJ</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">nonexecutive</span><span class="o">]</span>, <span class="o">[]]|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">69</span>, <span class="err">76</span>, <span class="kt">NN</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">director</span><span class="o">]</span>, <span class="o">[]]</span>    <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">78</span>, <span class="err">81</span>, <span class="kt">NNP</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">Nov.</span><span class="o">]</span>, <span class="o">[]]</span>       <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">83</span>, <span class="err">84</span>, <span class="kt">CD</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="err">29</span><span class="o">]</span>, <span class="o">[]]</span>          <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="err">81</span>, <span class="err">81</span>, <span class="kt">.</span>, <span class="o">[</span><span class="kt">word</span> <span class="kt">-&gt;</span> <span class="kt">.</span><span class="o">]</span>, <span class="o">[]]</span>            <span class="o">|</span>
<span class="o">+---------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box">

    <h3 id="conll-dataset">CoNLL Dataset</h3>

    <p>In order to train a Named Entity Recognition DL annotator, we need to get CoNLL format data as a spark dataframe. There is a component that does this for us: it reads a plain text file and transforms it to a spark dataset.</p>

    <p>The dataset should be in the format of <a href="https://www.clips.uantwerpen.be/conll2003/ner/">CoNLL 2003</a> and needs to be specified with <code class="language-plaintext highlighter-rouge">readDataset()</code>, which will create a dataframe with the data.</p>

    <p><strong>Input File Format:</strong></p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-DOCSTART- -X- -X- O

EU NNP B-NP B-ORG
rejects VBZ B-VP O
German JJ B-NP B-MISC
call NN I-NP O
to TO B-VP O
boycott VB I-VP O
British JJ B-NP B-MISC
lamb NN I-NP O
. . O O
</code></pre></div>    </div>

    <p><strong>Constructor Parameters:</strong></p>

    <ul>
      <li><strong>documentCol:</strong> Name of the DocumentAssembler column, by default ‘document’</li>
      <li><strong>sentenceCol:</strong> Name of the SentenceDetector column, by default ‘sentence’</li>
      <li><strong>tokenCol:</strong> Name of the Tokenizer column, by default ‘token’</li>
      <li><strong>posCol:</strong> Name of the part-of-speech tag column, by default ‘pos’</li>
      <li><strong>conllLabelIndex:</strong> Index of the label column in the dataset, by default 3</li>
      <li><strong>conllPosIndex:</strong> Index of the POS tags in the dataset, by default 1</li>
      <li><strong>textCol:</strong> Index of the text column in the dataset, by default ‘text’</li>
      <li><strong>labelCol:</strong> Name of the label column, by default ‘label’</li>
      <li><strong>explodeSentences:</strong> Whether to explode sentences to separate rows, by default True</li>
      <li><strong>delimiter:</strong> Delimiter used to separate columns inside CoNLL file</li>
    </ul>

    <p><strong>Parameters for <code class="language-plaintext highlighter-rouge">readDataset</code>:</strong></p>

    <ul>
      <li><strong>spark</strong>: Initiated Spark Session with Spark NLP</li>
      <li><strong>path</strong>: Path to the resource</li>
      <li><strong>read_as</strong>: How to read the resource, by default ReadAs.TEXT</li>
    </ul>

    <p>Refer to the documentation for more details on the API:</p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/training/conll/index.html#sparknlp.training.conll.CoNLL">CoNLL</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/training/CoNLL.html">CoNLL</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/training/CoNLL.scala">CoNLL.scala</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="n">CoNLL</span>
<span class="n">trainingData</span> <span class="o">=</span> <span class="n">CoNLL</span><span class="p">().</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="p">)</span>
<span class="n">trainingData</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span>
    <span class="s">"text"</span><span class="p">,</span>
    <span class="s">"token.result as tokens"</span><span class="p">,</span>
    <span class="s">"pos.result as pos"</span><span class="p">,</span>
    <span class="s">"label.result as label"</span>
<span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------------------+----------------------------------------------------------+-------------------------------------+-----------------------------------------+</span>
<span class="o">|</span><span class="n">text</span>                                            <span class="o">|</span><span class="n">tokens</span>                                                    <span class="o">|</span><span class="n">pos</span>                                  <span class="o">|</span><span class="n">label</span>                                    <span class="o">|</span>
<span class="o">+------------------------------------------------+----------------------------------------------------------+-------------------------------------+-----------------------------------------+</span>
<span class="o">|</span><span class="n">EU</span> <span class="n">rejects</span> <span class="n">German</span> <span class="n">call</span> <span class="n">to</span> <span class="n">boycott</span> <span class="n">British</span> <span class="n">lamb</span> <span class="p">.</span><span class="o">|</span><span class="p">[</span><span class="n">EU</span><span class="p">,</span> <span class="n">rejects</span><span class="p">,</span> <span class="n">German</span><span class="p">,</span> <span class="n">call</span><span class="p">,</span> <span class="n">to</span><span class="p">,</span> <span class="n">boycott</span><span class="p">,</span> <span class="n">British</span><span class="p">,</span> <span class="n">lamb</span><span class="p">,</span> <span class="p">.]</span><span class="o">|</span><span class="p">[</span><span class="n">NNP</span><span class="p">,</span> <span class="n">VBZ</span><span class="p">,</span> <span class="n">JJ</span><span class="p">,</span> <span class="n">NN</span><span class="p">,</span> <span class="n">TO</span><span class="p">,</span> <span class="n">VB</span><span class="p">,</span> <span class="n">JJ</span><span class="p">,</span> <span class="n">NN</span><span class="p">,</span> <span class="p">.]</span><span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">ORG</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">MISC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">MISC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">]</span><span class="o">|</span>
<span class="o">|</span><span class="n">Peter</span> <span class="n">Blackburn</span>                                 <span class="o">|</span><span class="p">[</span><span class="n">Peter</span><span class="p">,</span> <span class="n">Blackburn</span><span class="p">]</span>                                        <span class="o">|</span><span class="p">[</span><span class="n">NNP</span><span class="p">,</span> <span class="n">NNP</span><span class="p">]</span>                           <span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">]</span>                           <span class="o">|</span>
<span class="o">|</span><span class="n">BRUSSELS</span> <span class="mi">1996</span><span class="o">-</span><span class="mi">08</span><span class="o">-</span><span class="mi">22</span>                             <span class="o">|</span><span class="p">[</span><span class="n">BRUSSELS</span><span class="p">,</span> <span class="mi">1996</span><span class="o">-</span><span class="mi">08</span><span class="o">-</span><span class="mi">22</span><span class="p">]</span>                                    <span class="o">|</span><span class="p">[</span><span class="n">NNP</span><span class="p">,</span> <span class="n">CD</span><span class="p">]</span>                            <span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">]</span>                               <span class="o">|</span>
<span class="o">+------------------------------------------------+----------------------------------------------------------+-------------------------------------+-----------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">trainingData</span> <span class="k">=</span> <span class="nc">CoNLL</span><span class="o">().</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="o">)</span>
<span class="nv">trainingData</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"text"</span><span class="o">,</span> <span class="s">"token.result as tokens"</span><span class="o">,</span> <span class="s">"pos.result as pos"</span><span class="o">,</span> <span class="s">"label.result as label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">show</span><span class="o">(</span><span class="mi">3</span><span class="o">,</span> <span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------------------+----------------------------------------------------------+-------------------------------------+-----------------------------------------+</span>
<span class="o">|</span><span class="n">text</span>                                            <span class="o">|</span><span class="n">tokens</span>                                                    <span class="o">|</span><span class="n">pos</span>                                  <span class="o">|</span><span class="n">label</span>                                    <span class="o">|</span>
<span class="o">+------------------------------------------------+----------------------------------------------------------+-------------------------------------+-----------------------------------------+</span>
<span class="o">|</span><span class="nc">EU</span> <span class="n">rejects</span> <span class="nc">German</span> <span class="n">call</span> <span class="n">to</span> <span class="n">boycott</span> <span class="nc">British</span> <span class="n">lamb</span> <span class="o">.|[</span><span class="kt">EU</span>, <span class="kt">rejects</span>, <span class="kt">German</span>, <span class="kt">call</span>, <span class="kt">to</span>, <span class="kt">boycott</span>, <span class="kt">British</span>, <span class="kt">lamb</span>, <span class="kt">.</span><span class="o">]|[</span><span class="kt">NNP</span>, <span class="kt">VBZ</span>, <span class="kt">JJ</span>, <span class="kt">NN</span>, <span class="kt">TO</span>, <span class="kt">VB</span>, <span class="kt">JJ</span>, <span class="kt">NN</span>, <span class="kt">.</span><span class="o">]|[</span><span class="kt">B-ORG</span>, <span class="kt">O</span>, <span class="kt">B-MISC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-MISC</span>, <span class="kt">O</span>, <span class="kt">O</span><span class="o">]|</span>
<span class="o">|</span><span class="nc">Peter</span> <span class="nc">Blackburn</span>                                 <span class="o">|[</span><span class="kt">Peter</span>, <span class="kt">Blackburn</span><span class="o">]</span>                                        <span class="o">|[</span><span class="kt">NNP</span>, <span class="kt">NNP</span><span class="o">]</span>                           <span class="o">|[</span><span class="kt">B-PER</span>, <span class="kt">I-PER</span><span class="o">]</span>                           <span class="o">|</span>
<span class="o">|</span><span class="nc">BRUSSELS</span> <span class="mi">1996</span><span class="o">-</span><span class="mi">08</span><span class="o">-</span><span class="mi">22</span>                             <span class="o">|[</span><span class="kt">BRUSSELS</span>, <span class="err">1996</span><span class="kt">-</span><span class="err">08</span><span class="kt">-</span><span class="err">22</span><span class="o">]</span>                                    <span class="o">|[</span><span class="kt">NNP</span>, <span class="kt">CD</span><span class="o">]</span>                            <span class="o">|[</span><span class="kt">B-LOC</span>, <span class="kt">O</span><span class="o">]</span>                               <span class="o">|</span>
<span class="o">+------------------------------------------------+----------------------------------------------------------+-------------------------------------+-----------------------------------------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box">

    <h3 id="conll-u-dataset">CoNLL-U Dataset</h3>

    <p>In order to train a DependencyParserApproach annotator, we need to get CoNLL-U format data as a spark dataframe. There is a component that does this for us: it reads a plain text file and transforms it to a spark dataset.</p>

    <p>The dataset should be in the format of <a href="https://universaldependencies.org/format.html">CoNLL-U</a> and needs to be specified with <code class="language-plaintext highlighter-rouge">readDataset()</code>, which will create a dataframe with the data.</p>

    <p><strong>Input File Format:</strong></p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># sent_id = 1
# text = They buy and sell books.
1   They     they    PRON    PRP    Case=Nom|Number=Plur               2   nsubj   2:nsubj|4:nsubj   _
2   buy      buy     VERB    VBP    Number=Plur|Person=3|Tense=Pres    0   root    0:root            _
3   and      and     CONJ    CC     _                                  4   cc      4:cc              _
4   sell     sell    VERB    VBP    Number=Plur|Person=3|Tense=Pres    2   conj    0:root|2:conj     _
5   books    book    NOUN    NNS    Number=Plur                        2   obj     2:obj|4:obj       SpaceAfter=No
6   .        .       PUNCT   .      _                                  2   punct   2:punct           _
</code></pre></div>    </div>

    <p><strong>Constructor Parameters:</strong></p>

    <ul>
      <li><strong>explodeSentences</strong>: Whether to explode each sentence to a separate row</li>
    </ul>

    <p><strong>Parameters for <code class="language-plaintext highlighter-rouge">readDataset</code>:</strong></p>

    <ul>
      <li><strong>spark</strong>: Initiated Spark Session with Spark NLP</li>
      <li><strong>path</strong>: Path to the resource</li>
      <li><strong>read_as</strong>: How to read the resource, by default ReadAs.TEXT</li>
    </ul>

    <p>Refer to the documentation for more details on the API:</p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/training/conllu/index.html#sparknlp.training.conllu.CoNLLU">CoNLLU</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/training/CoNLLU.html">CoNLLU</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/training/CoNLLU.scala">CoNLLU.scala</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="n">CoNLLU</span>
<span class="n">conlluFile</span> <span class="o">=</span> <span class="s">"src/test/resources/conllu/en.test.conllu"</span>
<span class="n">conllDataSet</span> <span class="o">=</span> <span class="n">CoNLLU</span><span class="p">(</span><span class="bp">False</span><span class="p">).</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="n">conlluFile</span><span class="p">)</span>
<span class="n">conllDataSet</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span>
    <span class="s">"text"</span><span class="p">,</span>
    <span class="s">"form.result as form"</span><span class="p">,</span>
    <span class="s">"upos.result as upos"</span><span class="p">,</span>
    <span class="s">"xpos.result as xpos"</span><span class="p">,</span>
    <span class="s">"lemma.result as lemma"</span>
<span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">False</span><span class="p">)</span>
<span class="o">+---------------------------------------+----------------------------------------------+---------------------------------------------+------------------------------+--------------------------------------------+</span>
<span class="o">|</span><span class="n">text</span>                                   <span class="o">|</span><span class="n">form</span>                                          <span class="o">|</span><span class="n">upos</span>                                         <span class="o">|</span><span class="n">xpos</span>                          <span class="o">|</span><span class="n">lemma</span>                                       <span class="o">|</span>
<span class="o">+---------------------------------------+----------------------------------------------+---------------------------------------------+------------------------------+--------------------------------------------+</span>
<span class="o">|</span><span class="n">What</span> <span class="k">if</span> <span class="n">Google</span> <span class="n">Morphed</span> <span class="n">Into</span> <span class="n">GoogleOS</span><span class="err">?</span>  <span class="o">|</span><span class="p">[</span><span class="n">What</span><span class="p">,</span> <span class="k">if</span><span class="p">,</span> <span class="n">Google</span><span class="p">,</span> <span class="n">Morphed</span><span class="p">,</span> <span class="n">Into</span><span class="p">,</span> <span class="n">GoogleOS</span><span class="p">,</span> <span class="err">?</span><span class="p">]</span><span class="o">|</span><span class="p">[</span><span class="n">PRON</span><span class="p">,</span> <span class="n">SCONJ</span><span class="p">,</span> <span class="n">PROPN</span><span class="p">,</span> <span class="n">VERB</span><span class="p">,</span> <span class="n">ADP</span><span class="p">,</span> <span class="n">PROPN</span><span class="p">,</span> <span class="n">PUNCT</span><span class="p">]</span><span class="o">|</span><span class="p">[</span><span class="n">WP</span><span class="p">,</span> <span class="n">IN</span><span class="p">,</span> <span class="n">NNP</span><span class="p">,</span> <span class="n">VBD</span><span class="p">,</span> <span class="n">IN</span><span class="p">,</span> <span class="n">NNP</span><span class="p">,</span> <span class="p">.]</span><span class="o">|</span><span class="p">[</span><span class="n">what</span><span class="p">,</span> <span class="k">if</span><span class="p">,</span> <span class="n">Google</span><span class="p">,</span> <span class="n">morph</span><span class="p">,</span> <span class="n">into</span><span class="p">,</span> <span class="n">GoogleOS</span><span class="p">,</span> <span class="err">?</span><span class="p">]</span><span class="o">|</span>
<span class="o">+---------------------------------------+----------------------------------------------+---------------------------------------------+------------------------------+--------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.CoNLLU</span>

<span class="k">val</span> <span class="nv">conlluFile</span> <span class="k">=</span> <span class="s">"src/test/resources/conllu/en.test.conllu"</span>
<span class="k">val</span> <span class="nv">conllDataSet</span> <span class="k">=</span> <span class="nc">CoNLLU</span><span class="o">(</span><span class="kc">false</span><span class="o">).</span><span class="py">readDataset</span><span class="o">(</span><span class="nv">ResourceHelper</span><span class="o">.</span><span class="py">spark</span><span class="o">,</span> <span class="n">conlluFile</span><span class="o">)</span>
<span class="nv">conllDataSet</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"text"</span><span class="o">,</span> <span class="s">"form.result as form"</span><span class="o">,</span> <span class="s">"upos.result as upos"</span><span class="o">,</span> <span class="s">"xpos.result as xpos"</span><span class="o">,</span> <span class="s">"lemma.result as lemma"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span> <span class="kc">false</span><span class="o">)</span>
<span class="o">+---------------------------------------+----------------------------------------------+---------------------------------------------+------------------------------+--------------------------------------------+</span>
<span class="o">|</span><span class="n">text</span>                                   <span class="o">|</span><span class="n">form</span>                                          <span class="o">|</span><span class="n">upos</span>                                         <span class="o">|</span><span class="n">xpos</span>                          <span class="o">|</span><span class="n">lemma</span>                                       <span class="o">|</span>
<span class="o">+---------------------------------------+----------------------------------------------+---------------------------------------------+------------------------------+--------------------------------------------+</span>
<span class="o">|</span><span class="nc">What</span> <span class="k">if</span> <span class="nc">Google</span> <span class="nc">Morphed</span> <span class="nc">Into</span> <span class="nc">GoogleOS</span><span class="o">?</span>  <span class="o">|[</span><span class="kt">What</span>, <span class="kt">if</span>, <span class="kt">Google</span>, <span class="kt">Morphed</span>, <span class="kt">Into</span>, <span class="kt">GoogleOS</span>, <span class="kt">?</span><span class="o">]|[</span><span class="kt">PRON</span>, <span class="kt">SCONJ</span>, <span class="kt">PROPN</span>, <span class="kt">VERB</span>, <span class="kt">ADP</span>, <span class="kt">PROPN</span>, <span class="kt">PUNCT</span><span class="o">]|[</span><span class="kt">WP</span>, <span class="kt">IN</span>, <span class="kt">NNP</span>, <span class="kt">VBD</span>, <span class="kt">IN</span>, <span class="kt">NNP</span>, <span class="kt">.</span><span class="o">]|[</span><span class="kt">what</span>, <span class="kt">if</span>, <span class="kt">Google</span>, <span class="kt">morph</span>, <span class="kt">into</span>, <span class="kt">GoogleOS</span>, <span class="kt">?</span><span class="o">]|</span>
<span class="o">+---------------------------------------+----------------------------------------------+---------------------------------------------+------------------------------+--------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box">

    <h3 id="pubtator-dataset">PubTator Dataset</h3>

    <p>The PubTator format includes medical papers’ titles, abstracts, and tagged chunks (see <a href="http://bioportal.bioontology.org/ontologies/EDAM?p=classes&amp;conceptid=format_3783">PubTator Docs</a> and <a href="http://github.com/chanzuckerberg/MedMentions">MedMentions Docs</a> for more information). We can create a Spark DataFrame from a PubTator text file.</p>

    <p><strong>Input File Format:</strong></p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>25763772        0       5       DCTN4   T116,T123       C4308010
25763772        23      63      chronic Pseudomonas aeruginosa infection        T047    C0854135
25763772        67      82      cystic fibrosis T047    C0010674
25763772        83      120     Pseudomonas aeruginosa (Pa) infection   T047    C0854135
25763772        124     139     cystic fibrosis T047    C0010674
</code></pre></div>    </div>

    <p><strong>Constructor Parameters:</strong></p>

    <p>None</p>

    <p><strong>Parameters for <code class="language-plaintext highlighter-rouge">readDataset</code>:</strong></p>

    <ul>
      <li><strong>spark</strong>: Initiated Spark Session with Spark NLP</li>
      <li><strong>path</strong>: Path to the resource</li>
      <li><strong>isPaddedToken</strong>: Whether tokens are padded</li>
    </ul>

    <p>Refer to the documentation for more details on the API:</p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/training/pub_tator/index.html#sparknlp.training.pub_tator.PubTator">PubTator</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/training/PubTator.html">PubTator</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/training/PubTator.scala">PubTator.scala</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="n">PubTator</span>
<span class="n">pubTatorFile</span> <span class="o">=</span> <span class="s">"./src/test/resources/corpus_pubtator_sample.txt"</span>
<span class="n">pubTatorDataSet</span> <span class="o">=</span> <span class="n">PubTator</span><span class="p">().</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="n">pubTatorFile</span><span class="p">)</span>
<span class="n">pubTatorDataSet</span><span class="p">.</span><span class="n">show</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="o">+--------+--------------------+--------------------+--------------------+-----------------------+---------------------+-----------------------+</span>
<span class="o">|</span>  <span class="n">doc_id</span><span class="o">|</span>      <span class="n">finished_token</span><span class="o">|</span>        <span class="n">finished_pos</span><span class="o">|</span>        <span class="n">finished_ner</span><span class="o">|</span><span class="n">finished_token_metadata</span><span class="o">|</span><span class="n">finished_pos_metadata</span><span class="o">|</span><span class="n">finished_label_metadata</span><span class="o">|</span>
<span class="o">+--------+--------------------+--------------------+--------------------+-----------------------+---------------------+-----------------------+</span>
<span class="o">|</span><span class="mi">25763772</span><span class="o">|</span><span class="p">[</span><span class="n">DCTN4</span><span class="p">,</span> <span class="k">as</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">mo</span><span class="p">...</span><span class="o">|</span><span class="p">[</span><span class="n">NNP</span><span class="p">,</span> <span class="n">IN</span><span class="p">,</span> <span class="n">DT</span><span class="p">,</span> <span class="n">NN</span><span class="p">,...</span><span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">T116</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,...</span><span class="o">|</span>   <span class="p">[[</span><span class="n">sentence</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[...</span><span class="o">|</span> <span class="p">[[</span><span class="n">word</span><span class="p">,</span> <span class="n">DCTN4</span><span class="p">],</span> <span class="p">[...</span><span class="o">|</span>   <span class="p">[[</span><span class="n">word</span><span class="p">,</span> <span class="n">DCTN4</span><span class="p">],</span> <span class="p">[...</span><span class="o">|</span>
<span class="o">+--------+--------------------+--------------------+--------------------+-----------------------+---------------------+-----------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.PubTator</span>

<span class="k">val</span> <span class="nv">pubTatorFile</span> <span class="k">=</span> <span class="s">"./src/test/resources/corpus_pubtator_sample.txt"</span>
<span class="k">val</span> <span class="nv">pubTatorDataSet</span> <span class="k">=</span> <span class="nc">PubTator</span><span class="o">().</span><span class="py">readDataset</span><span class="o">(</span><span class="nv">ResourceHelper</span><span class="o">.</span><span class="py">spark</span><span class="o">,</span> <span class="n">pubTatorFile</span><span class="o">)</span>
<span class="nv">pubTatorDataSet</span><span class="o">.</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
<span class="o">+--------+--------------------+--------------------+--------------------+-----------------------+---------------------+-----------------------+</span>
<span class="o">|</span>  <span class="n">doc_id</span><span class="o">|</span>      <span class="n">finished_token</span><span class="o">|</span>        <span class="n">finished_pos</span><span class="o">|</span>        <span class="n">finished_ner</span><span class="o">|</span><span class="n">finished_token_metadata</span><span class="o">|</span><span class="n">finished_pos_metadata</span><span class="o">|</span><span class="n">finished_label_metadata</span><span class="o">|</span>
<span class="o">+--------+--------------------+--------------------+--------------------+-----------------------+---------------------+-----------------------+</span>
<span class="o">|</span><span class="mi">25763772</span><span class="o">|[</span><span class="kt">DCTN4</span>, <span class="kt">as</span>, <span class="kt">a</span>, <span class="kt">mo...|</span><span class="o">[</span><span class="kt">NNP</span>, <span class="kt">IN</span>, <span class="kt">DT</span>, <span class="kt">NN</span>,<span class="kt">...|</span><span class="o">[</span><span class="kt">B-T116</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>,<span class="kt">...|</span>   <span class="o">[[</span><span class="kt">sentence</span>, <span class="err">0</span><span class="o">]</span>, <span class="o">[</span><span class="kt">...|</span> <span class="o">[[</span><span class="kt">word</span>, <span class="kt">DCTN4</span><span class="o">]</span>, <span class="o">[</span><span class="kt">...|</span>   <span class="o">[[</span><span class="kt">word</span>, <span class="kt">DCTN4</span><span class="o">]</span>, <span class="o">[</span><span class="kt">...|</span>
<span class="kt">+--------+--------------------+--------------------+--------------------+-----------------------+---------------------+-----------------------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

</div>
<div class="h3-box">

  <h3 id="spell-checkers-dataset-corpus">Spell Checkers Dataset (Corpus)</h3>

  <p>In order to train a Norvig or Symmetric Spell Checkers, we need to get corpus data as a spark dataframe. We can read a plain text file and transforms it to a spark dataset.</p>

  <p><strong>Example:</strong></p>

  <div class="tabs-box tabs-new">

    <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_corpus</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span> \
    <span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="s">"./sherlockholmes.txt"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="s">"value"</span><span class="p">,</span> <span class="s">"text"</span><span class="p">)</span>
</code></pre></div>    </div>

    <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">val</span> <span class="nv">trainCorpus</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span>
    <span class="o">.</span><span class="py">text</span><span class="o">(</span><span class="s">"./sherlockholmes.txt"</span><span class="o">)</span>
    <span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="nv">trainCorpus</span><span class="o">.</span><span class="py">col</span><span class="o">(</span><span class="s">"value"</span><span class="o">).</span><span class="py">as</span><span class="o">(</span><span class="s">"text"</span><span class="o">))</span>
</code></pre></div>    </div>

  </div>
</div>
<div class="h3-box">

  <h2 id="text-processing">Text Processing</h2>
  <p>These are annotators that can be trained to process text for tasks such as
dependency parsing, lemmatisation, part-of-speech tagging, sentence detection
and word segmentation.</p>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="dependencyparserapproach">DependencyParserApproach</h3>

    <p>Trains an unlabeled parser that finds a grammatical relations between two words in a sentence.</p>

    <p>Dependency parser provides information about word relationship. For example, dependency parsing can tell you what
the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help
you find precise answers to specific questions.</p>

    <p>The required training data can be set in two different ways (only one can be chosen for a particular model):</p>
    <ul>
      <li>Dependency treebank in the <a href="http://www.nltk.org/nltk_data/">Penn Treebank format</a> set with <code class="language-plaintext highlighter-rouge">setDependencyTreeBank</code>. Data Format:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(S
(S-TPC-1
  (NP-SBJ
    (NP (NP (DT A) (NN form)) (PP (IN of) (NP (NN asbestos))))
    (RRC ...)...)...)
...
(VP (VBD reported) (SBAR (-NONE- 0) (S (-NONE- *T*-1))))
(. .))
</code></pre></div>        </div>
      </li>
      <li>Dataset in the <a href="https://universaldependencies.org/format.html">CoNLL-U format</a> set with <code class="language-plaintext highlighter-rouge">setConllU</code>.
Data Format:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-DOCSTART- -X- -X- O

EU NNP B-NP B-ORG
rejects VBZ B-VP O
German JJ B-NP B-MISC
</code></pre></div>        </div>
      </li>
    </ul>

    <p>Apart from that, no additional training data is needed.</p>

    <p>See <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/parser/dep/DependencyParserApproachTestSpec.scala">DependencyParserApproachTestSpec</a> for further reference on how to use this API.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, POS, TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">DEPENDENCY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/dependency/dependency_parser/index.html#sparknlp.annotator.dependency.dependency_parser.DependencyParserApproach">DependencyParserApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/parser/dep/DependencyParserApproach">DependencyParserApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/parser/dep/DependencyParserApproach.scala">DependencyParserApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">posTagger</span> <span class="o">=</span> <span class="n">PerceptronModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"pos"</span><span class="p">)</span>

<span class="n">dependencyParserApproach</span> <span class="o">=</span> <span class="n">DependencyParserApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"pos"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"dependency"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setDependencyTreeBank</span><span class="p">(</span><span class="s">"src/test/resources/parser/unlabeled/dependency_treebank"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">posTagger</span><span class="p">,</span>
    <span class="n">dependencyParserApproach</span>
<span class="p">])</span>

<span class="c1"># Additional training data is not needed, the dependency parser relies on the dependency tree bank / CoNLL-U only.
</span><span class="n">emptyDataSet</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">""</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">emptyDataSet</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserApproach</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">posTagger</span> <span class="k">=</span> <span class="nv">PerceptronModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"pos"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">dependencyParserApproach</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DependencyParserApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"pos"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"dependency"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setDependencyTreeBank</span><span class="o">(</span><span class="s">"src/test/resources/parser/unlabeled/dependency_treebank"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">posTagger</span><span class="o">,</span>
  <span class="n">dependencyParserApproach</span>
<span class="o">))</span>

<span class="c1">// Additional training data is not needed, the dependency parser relies on the dependency tree bank / CoNLL-U only.</span>
<span class="k">val</span> <span class="nv">emptyDataSet</span> <span class="k">=</span> <span class="nv">Seq</span><span class="o">.</span><span class="py">empty</span><span class="o">[</span><span class="kt">String</span><span class="o">].</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">emptyDataSet</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="lemmatizer">Lemmatizer</h3>

    <p>Class to find lemmas out of words with the objective of returning a base dictionary word.
Retrieves the significant part of a word. A dictionary of predefined lemmas must be provided with <code class="language-plaintext highlighter-rouge">setDictionary</code>.
The dictionary can be set as a delimited text file.
Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">LemmatizerModel.pretrained</code>.</p>

    <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/italian/Train-Lemmatizer-Italian.ipynb">Examples</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">TOKEN</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/lemmatizer/index.html#sparknlp.annotator.lemmatizer.Lemmatizer">Lemmatizer</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/Lemmatizer">Lemmatizer</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/Lemmatizer.scala">Lemmatizer</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In this example, the lemma dictionary `lemmas_small.txt` has the form of
#
# ...
# pick	-&gt;	pick	picks	picking	picked
# peck	-&gt;	peck	pecking	pecked	pecks
# pickle	-&gt;	pickle	pickles	pickled	pickling
# pepper	-&gt;	pepper	peppers	peppered	peppering
# ...
#
# where each key is delimited by `-&gt;` and values are delimited by `\t`
</span>
<span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentenceDetector</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">lemmatizer</span> <span class="o">=</span> <span class="n">Lemmatizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"lemma"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setDictionary</span><span class="p">(</span><span class="s">"src/test/resources/lemma-corpus-small/lemmas_small.txt"</span><span class="p">,</span> <span class="s">"-&gt;"</span><span class="p">,</span> <span class="s">"</span><span class="se">\t</span><span class="s">"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">([</span>
      <span class="n">documentAssembler</span><span class="p">,</span>
      <span class="n">sentenceDetector</span><span class="p">,</span>
      <span class="n">tokenizer</span><span class="p">,</span>
      <span class="n">lemmatizer</span>
    <span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"Peter Pipers employees are picking pecks of pickled peppers."</span><span class="p">]])</span> \
    <span class="p">.</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"lemma.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                            <span class="o">|</span>
<span class="o">+------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">Peter</span><span class="p">,</span> <span class="n">Pipers</span><span class="p">,</span> <span class="n">employees</span><span class="p">,</span> <span class="n">are</span><span class="p">,</span> <span class="n">pick</span><span class="p">,</span> <span class="n">peck</span><span class="p">,</span> <span class="n">of</span><span class="p">,</span> <span class="n">pickle</span><span class="p">,</span> <span class="n">pepper</span><span class="p">,</span> <span class="p">.]</span><span class="o">|</span>
<span class="o">+------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// In this example, the lemma dictionary `lemmas_small.txt` has the form of</span>
<span class="c1">//</span>
<span class="c1">// ...</span>
<span class="c1">// pick	-&gt;	pick	picks	picking	picked</span>
<span class="c1">// peck	-&gt;	peck	pecking	pecked	pecks</span>
<span class="c1">// pickle	-&gt;	pickle	pickles	pickled	pickling</span>
<span class="c1">// pepper	-&gt;	pepper	peppers	peppered	peppering</span>
<span class="c1">// ...</span>
<span class="c1">//</span>
<span class="c1">// where each key is delimited by `-&gt;` and values are delimited by `\t`</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Lemmatizer</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentenceDetector</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">lemmatizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Lemmatizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"token"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"lemma"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setDictionary</span><span class="o">(</span><span class="s">"src/test/resources/lemma-corpus-small/lemmas_small.txt"</span><span class="o">,</span> <span class="s">"-&gt;"</span><span class="o">,</span> <span class="s">"\t"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
    <span class="n">documentAssembler</span><span class="o">,</span>
    <span class="n">sentenceDetector</span><span class="o">,</span>
    <span class="n">tokenizer</span><span class="o">,</span>
    <span class="n">lemmatizer</span>
  <span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"Peter Pipers employees are picking pecks of pickled peppers."</span><span class="o">)</span>
  <span class="o">.</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>
<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"lemma.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                            <span class="o">|</span>
<span class="o">+------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">Peter</span>, <span class="kt">Pipers</span>, <span class="kt">employees</span>, <span class="kt">are</span>, <span class="kt">pick</span>, <span class="kt">peck</span>, <span class="kt">of</span>, <span class="kt">pickle</span>, <span class="kt">pepper</span>, <span class="kt">.</span><span class="o">]|</span>
<span class="o">+------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="perceptronapproach-part-of-speech-tagger">PerceptronApproach (Part of speech tagger)</h3>

    <p>Trains an averaged Perceptron model to tag words part-of-speech.
Sets a POS tag to each word within a sentence.</p>

    <p>The training data needs to be in a Spark DataFrame, where the column needs to consist of
<a href="/api/com/johnsnowlabs/nlp/Annotation">Annotations</a> of type <code class="language-plaintext highlighter-rouge">POS</code>. The <code class="language-plaintext highlighter-rouge">Annotation</code> needs to have member <code class="language-plaintext highlighter-rouge">result</code>
set to the POS tag and have a <code class="language-plaintext highlighter-rouge">"word"</code> mapping to its word inside of member <code class="language-plaintext highlighter-rouge">metadata</code>.
This DataFrame for training can easily created by the helper class <a href="#pos-dataset">POS</a>.</p>

    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>POS().readDataset(spark, datasetPath).selectExpr("explode(tags) as tags").show(false)
+---------------------------------------------+
|tags                                         |
+---------------------------------------------+
|[pos, 0, 5, NNP, [word -&gt; Pierre], []]       |
|[pos, 7, 12, NNP, [word -&gt; Vinken], []]      |
|[pos, 14, 14, ,, [word -&gt; ,], []]            |
|[pos, 31, 34, MD, [word -&gt; will], []]        |
|[pos, 36, 39, VB, [word -&gt; join], []]        |
|[pos, 41, 43, DT, [word -&gt; the], []]         |
|[pos, 45, 49, NN, [word -&gt; board], []]       |
                      ...
</code></pre></div>    </div>

    <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/french/Train-Perceptron-French.ipynb">Examples</a>
and <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/annotators/pos/perceptron">PerceptronApproach tests</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">TOKEN, DOCUMENT</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">POS</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/pos/perceptron/index.html#sparknlp.annotator.pos.perceptron.PerceptronApproach">PerceptronApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/pos/perceptron/PerceptronApproach">PerceptronApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/pos/perceptron/PerceptronApproach.scala">PerceptronApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">datasetPath</span> <span class="o">=</span> <span class="s">"src/test/resources/anc-pos-corpus-small/test-training.txt"</span>
<span class="n">trainingPerceptronDF</span> <span class="o">=</span> <span class="n">POS</span><span class="p">().</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="n">datasetPath</span><span class="p">)</span>

<span class="n">trainedPos</span> <span class="o">=</span> <span class="n">PerceptronApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"pos"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setPosColumn</span><span class="p">(</span><span class="s">"tags"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingPerceptronDF</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">trainedPos</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"To be or not to be, is this the question?"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"pos.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+--------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                            <span class="o">|</span>
<span class="o">+--------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">NNP</span><span class="p">,</span> <span class="n">NNP</span><span class="p">,</span> <span class="n">CD</span><span class="p">,</span> <span class="n">JJ</span><span class="p">,</span> <span class="n">NNP</span><span class="p">,</span> <span class="n">NNP</span><span class="p">,</span> <span class="p">,,</span> <span class="n">MD</span><span class="p">,</span> <span class="n">VB</span><span class="p">,</span> <span class="n">DT</span><span class="p">,</span> <span class="n">CD</span><span class="p">,</span> <span class="p">.]</span><span class="o">|</span>
<span class="o">+--------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.POS</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronApproach</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">datasetPath</span> <span class="k">=</span> <span class="s">"src/test/resources/anc-pos-corpus-small/test-training.txt"</span>
<span class="k">val</span> <span class="nv">trainingPerceptronDF</span> <span class="k">=</span> <span class="nc">POS</span><span class="o">().</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="n">datasetPath</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">trainedPos</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">PerceptronApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"pos"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setPosColumn</span><span class="o">(</span><span class="s">"tags"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingPerceptronDF</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">trainedPos</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"To be or not to be, is this the question?"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"pos.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+--------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                            <span class="o">|</span>
<span class="o">+--------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">NNP</span>, <span class="kt">NNP</span>, <span class="kt">CD</span>, <span class="kt">JJ</span>, <span class="kt">NNP</span>, <span class="kt">NNP</span>, ,, <span class="kt">MD</span>, <span class="kt">VB</span>, <span class="kt">DT</span>, <span class="kt">CD</span>, <span class="kt">.</span><span class="o">]|</span>
<span class="o">+--------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="sentencedetectordlapproach">SentenceDetectorDLApproach</h3>

    <p>Trains an annotator that detects sentence boundaries using a deep learning approach.</p>

    <p>For pretrained models see SentenceDetectorDLModel.</p>

    <p>Currently, only the CNN model is supported for training, but in the future the architecture of the model can
be set with <code class="language-plaintext highlighter-rouge">setModelArchitecture</code>.</p>

    <p>The default model <code class="language-plaintext highlighter-rouge">"cnn"</code> is based on the paper
<a href="https://konvens.org/proceedings/2019/papers/KONVENS2019_paper_41.pdf">Deep-EOS: General-Purpose Neural Networks for Sentence Boundary Detection (2020, Stefan Schweter, Sajawel Ahmed)</a>
using a CNN architecture. We also modified the original implementation a little bit to cover broken sentences and some impossible end of line chars.</p>

    <p>Each extracted sentence can be returned in an Array or exploded to separate rows,
if <code class="language-plaintext highlighter-rouge">explodeSentences</code> is set to <code class="language-plaintext highlighter-rouge">true</code>.</p>

    <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/multilingual/SentenceDetectorDL.ipynb">Examples</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/sentence/sentence_detector_dl/index.html#sparknlp.annotator.sentence.sentence_detector_dl.SentenceDetectorDLApproach">SentenceDetectorDLApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/sentence_detector_dl/SentenceDetectorDLApproach">SentenceDetectorDLApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/sentence_detector_dl/SentenceDetectorDLApproach.scala">SentenceDetectorDLApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The training process needs data, where each data point is a sentence.
# In this example the `train.txt` file has the form of
#
# ...
# Slightly more moderate language would make our present situation – namely the lack of progress – a little easier.
# His political successors now have great responsibilities to history and to the heritage of values bequeathed to them by Nelson Mandela.
# ...
#
# where each line is one sentence.
# Training can then be started like so:
</span>
<span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">trainingData</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="s">"train.txt"</span><span class="p">).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentenceDetector</span> <span class="o">=</span> <span class="n">SentenceDetectorDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentences"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setEpochsNumber</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span><span class="n">documentAssembler</span><span class="p">,</span> <span class="n">sentenceDetector</span><span class="p">])</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// The training process needs data, where each data point is a sentence.</span>
<span class="c1">// In this example the `train.txt` file has the form of</span>
<span class="c1">//</span>
<span class="c1">// ...</span>
<span class="c1">// Slightly more moderate language would make our present situation – namely the lack of progress – a little easier.</span>
<span class="c1">// His political successors now have great responsibilities to history and to the heritage of values bequeathed to them by Nelson Mandela.</span>
<span class="c1">// ...</span>
<span class="c1">//</span>
<span class="c1">// where each line is one sentence.</span>
<span class="c1">// Training can then be started like so:</span>

<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLApproach</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">trainingData</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">text</span><span class="o">(</span><span class="s">"train.txt"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentenceDetector</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetectorDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentences"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setEpochsNumber</span><span class="o">(</span><span class="mi">100</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="n">documentAssembler</span><span class="o">,</span> <span class="n">sentenceDetector</span><span class="o">))</span>

<span class="k">val</span> <span class="nv">model</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="typeddependencyparser">TypedDependencyParser</h3>

    <p>Labeled parser that finds a grammatical relation between two words in a sentence.
Its input is either a CoNLL2009 or ConllU dataset.</p>

    <p>For instantiated/pretrained models, see TypedDependencyParserModel.</p>

    <p>Dependency parsers provide information about word relationship. For example, dependency parsing can tell you what
the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help
you find precise answers to specific questions.</p>

    <p>The parser requires the dependant tokens beforehand with e.g. <a href="/docs/en/annotators#dependencyparser">DependencyParser</a>.
The required training data can be set in two different ways (only one can be chosen for a particular model):</p>
    <ul>
      <li>Dataset in the <a href="https://ufal.mff.cuni.cz/conll2009-st/trial-data.html">CoNLL 2009 format</a> set with <code class="language-plaintext highlighter-rouge">setConll2009</code>. Data format:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1	The	the	the	DT	DT	_	_	4	4	NMOD	NMOD	_	_	_	_
2	most	most	most	RBS	RBS	_	_	3	3	AMOD	AMOD	_	_	_	_
3	troublesome	troublesome	troublesome	JJ	JJ	_	_	4	4	NMOD	NMOD	_	_	_	_
</code></pre></div>        </div>
      </li>
      <li>Dataset in the <a href="https://universaldependencies.org/format.html">CoNLL-U format</a> set with <code class="language-plaintext highlighter-rouge">setConllU</code>
Data format:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-DOCSTART- -X- -X- O

EU NNP B-NP B-ORG
rejects VBZ B-VP O
German JJ B-NP B-MISC
</code></pre></div>        </div>
      </li>
    </ul>

    <p>Apart from that, no additional training data is needed.</p>

    <p>See <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/annotators/parser/typdep/TypedDependencyParserApproachTestSpec.scala">TypedDependencyParserApproachTestSpec</a> for further reference on this API.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">TOKEN, POS, DEPENDENCY</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">LABELED_DEPENDENCY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/dependency/typed_dependency_parser/index.html#sparknlp.annotator.dependency.typed_dependency_parser.TypedDependencyParserApproach">TypedDependencyParserApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/parser/typdep/TypedDependencyParserApproach">TypedDependencyParserApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/parser/typdep/TypedDependencyParserApproach.scala">TypedDependencyParserApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">posTagger</span> <span class="o">=</span> <span class="n">PerceptronModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"pos"</span><span class="p">)</span>

<span class="n">dependencyParser</span> <span class="o">=</span> <span class="n">DependencyParserModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"pos"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"dependency"</span><span class="p">)</span>

<span class="n">typedDependencyParser</span> <span class="o">=</span> <span class="n">TypedDependencyParserApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"dependency"</span><span class="p">,</span> <span class="s">"pos"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"dependency_type"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setConllU</span><span class="p">(</span><span class="s">"src/test/resources/parser/labeled/train_small.conllu.txt"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setNumberOfIterations</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">posTagger</span><span class="p">,</span>
    <span class="n">dependencyParser</span><span class="p">,</span>
    <span class="n">typedDependencyParser</span>
<span class="p">])</span>

<span class="c1"># Additional training data is not needed, the dependency parser relies on CoNLL-U only.
</span><span class="n">emptyDataSet</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">""</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">emptyDataSet</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserApproach</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">posTagger</span> <span class="k">=</span> <span class="nv">PerceptronModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"pos"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">dependencyParser</span> <span class="k">=</span> <span class="nv">DependencyParserModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"pos"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"dependency"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">typedDependencyParser</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">TypedDependencyParserApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"dependency"</span><span class="o">,</span> <span class="s">"pos"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"dependency_type"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setConllU</span><span class="o">(</span><span class="s">"src/test/resources/parser/labeled/train_small.conllu.txt"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setNumberOfIterations</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">posTagger</span><span class="o">,</span>
  <span class="n">dependencyParser</span><span class="o">,</span>
  <span class="n">typedDependencyParser</span>
<span class="o">))</span>

<span class="c1">// Additional training data is not needed, the dependency parser relies on CoNLL-U only.</span>
<span class="k">val</span> <span class="nv">emptyDataSet</span> <span class="k">=</span> <span class="nv">Seq</span><span class="o">.</span><span class="py">empty</span><span class="o">[</span><span class="kt">String</span><span class="o">].</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">emptyDataSet</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="wordsegmenterapproach">WordSegmenterApproach</h3>

    <p>Trains a WordSegmenter which tokenizes non-english or non-whitespace separated texts.</p>

    <p>Many languages are not whitespace separated and their sentences are a concatenation of many symbols, like Korean,
Japanese or Chinese. Without understanding the language, splitting the words into their corresponding tokens is
impossible. The WordSegmenter is trained to understand these languages and split them into semantically correct parts.</p>

    <p>To train your own model, a training dataset consisting of
<a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">Part-Of-Speech tags</a> is required. The data has to be loaded
into a dataframe, where the column is an <a href="/api/com/johnsnowlabs/nlp/Annotation">Annotation</a> of type <code class="language-plaintext highlighter-rouge">"POS"</code>. This can be
set with <code class="language-plaintext highlighter-rouge">setPosColumn</code>.</p>

    <p><strong>Tip</strong>: The helper class <a href="#pos-dataset">POS</a> might be useful to read training data into data frames.</p>

    <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/annotation/text/chinese/word_segmentation">Examples</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">TOKEN</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/ws/word_segmenter/index.html#sparknlp.annotator.ws.word_segmenter.WordSegmenterApproach">WordSegmenterApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/ws/WordSegmenterApproach">WordSegmenterApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/ws/WordSegmenterApproach.scala">WordSegmenterApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In this example, `"chinese_train.utf8"` is in the form of
#
# 十|LL 四|RR 不|LL 是|RR 四|LL 十|RR
#
# and is loaded with the `POS` class to create a dataframe of `"POS"` type Annotations.
</span>
<span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">wordSegmenter</span> <span class="o">=</span> <span class="n">WordSegmenterApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setPosColumn</span><span class="p">(</span><span class="s">"tags"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setNIterations</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">wordSegmenter</span>
<span class="p">])</span>

<span class="n">trainingDataSet</span> <span class="o">=</span> <span class="n">POS</span><span class="p">().</span><span class="n">readDataset</span><span class="p">(</span>
    <span class="n">spark</span><span class="p">,</span>
    <span class="s">"src/test/resources/word-segmenter/chinese_train.utf8"</span>
<span class="p">)</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingDataSet</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// In this example, `"chinese_train.utf8"` is in the form of</span>
<span class="c1">//</span>
<span class="c1">// 十|LL 四|RR 不|LL 是|RR 四|LL 十|RR</span>
<span class="c1">//</span>
<span class="c1">// and is loaded with the `POS` class to create a dataframe of `"POS"` type Annotations.</span>

<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ws.WordSegmenterApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.POS</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">wordSegmenter</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">WordSegmenterApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setPosColumn</span><span class="o">(</span><span class="s">"tags"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setNIterations</span><span class="o">(</span><span class="mi">5</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">wordSegmenter</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">trainingDataSet</span> <span class="k">=</span> <span class="nc">POS</span><span class="o">().</span><span class="py">readDataset</span><span class="o">(</span>
  <span class="nv">ResourceHelper</span><span class="o">.</span><span class="py">spark</span><span class="o">,</span>
  <span class="s">"src/test/resources/word-segmenter/chinese_train.utf8"</span>
<span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingDataSet</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

</div>
<div class="h3-box">

  <h2 id="spell-checkers">Spell Checkers</h2>
  <p>These are annotators that can be trained to correct text.</p>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="contextspellcheckerapproach">ContextSpellCheckerApproach</h3>

    <p>Trains a deep-learning based Noisy Channel Model Spell Algorithm.
Correction candidates are extracted combining context information and word information.</p>

    <p>Spell Checking is a sequence to sequence mapping problem. Given an input sequence, potentially containing a
certain number of errors, <code class="language-plaintext highlighter-rouge">ContextSpellChecker</code> will rank correction sequences according to three things:</p>
    <ol>
      <li>Different correction candidates for each word — <strong>word level</strong>.</li>
      <li>The surrounding text of each word, i.e. it’s context — <strong>sentence level</strong>.</li>
      <li>The relative cost of different correction candidates according to the edit operations at the character level it requires — <strong>subword level</strong>.</li>
    </ol>

    <p>For an in-depth explanation of the module see the article <a href="https://medium.com/spark-nlp/applying-context-aware-spell-checking-in-spark-nlp-3c29c46963bc">Applying Context Aware Spell Checking in Spark NLP</a>.</p>

    <p>For extended examples of usage, see the article <a href="https://towardsdatascience.com/training-a-contextual-spell-checker-for-italian-language-66dda528e4bf">Training a Contextual Spell Checker for Italian Language</a>,
the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/italian/Training_Context_Spell_Checker_Italian.ipynb">Examples</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">TOKEN</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/spell_check/context_spell_checker/index.html#sparknlp.annotator.spell_check.context_spell_checker.ContextSpellCheckerApproach">ContextSpellCheckerApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/spell/context/ContextSpellCheckerApproach">ContextSpellCheckerApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/spell/context/ContextSpellCheckerApproach.scala">ContextSpellCheckerApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># For this example, we use the first Sherlock Holmes book as the training dataset.
</span>
<span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>


<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">spellChecker</span> <span class="o">=</span> <span class="n">ContextSpellCheckerApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"corrected"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setWordMaxDistance</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setBatchSize</span><span class="p">(</span><span class="mi">24</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setEpochs</span><span class="p">(</span><span class="mi">8</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLanguageModelClasses</span><span class="p">(</span><span class="mi">1650</span><span class="p">)</span>  <span class="c1"># dependant on vocabulary size
</span>    <span class="c1"># .addVocabClass("_NAME_", names) # Extra classes for correction could be added like this
</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">spellChecker</span>
<span class="p">])</span>

<span class="n">path</span> <span class="o">=</span> <span class="s">"sherlockholmes.txt"</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// For this example, we use the first Sherlock Holmes book as the training dataset.</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach</span>

<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>


<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">spellChecker</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ContextSpellCheckerApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"corrected"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setWordMaxDistance</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setBatchSize</span><span class="o">(</span><span class="mi">24</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setEpochs</span><span class="o">(</span><span class="mi">8</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLanguageModelClasses</span><span class="o">(</span><span class="mi">1650</span><span class="o">)</span>  <span class="c1">// dependant on vocabulary size</span>
  <span class="c1">// .addVocabClass("_NAME_", names) // Extra classes for correction could be added like this</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">spellChecker</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">path</span> <span class="k">=</span> <span class="s">"src/test/resources/spell/sherlockholmes.txt"</span>
<span class="k">val</span> <span class="nv">dataset</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">sparkContext</span><span class="o">.</span><span class="py">textFile</span><span class="o">(</span><span class="n">path</span><span class="o">)</span>
  <span class="o">.</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">dataset</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="norvigsweeting-spellchecker">NorvigSweeting Spellchecker</h3>

    <p>Trains annotator, that retrieves tokens and makes corrections automatically if not found in an English dictionary.</p>

    <p>The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and
dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster
(than the standard approach with deletes + transposes + replaces + inserts) and language independent.
A dictionary of correct spellings must be provided with <code class="language-plaintext highlighter-rouge">setDictionary</code> as a text file, where each word is parsed by a regex pattern.</p>

    <p>For Example a file <code class="language-plaintext highlighter-rouge">"words.txt"</code>:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>...
gummy
gummic
gummier
gummiest
gummiferous
...
</code></pre></div>    </div>
    <p>can be parsed with the regular expression <code class="language-plaintext highlighter-rouge">\S+</code>, which is the default for <code class="language-plaintext highlighter-rouge">setDictionary</code>.</p>

    <p>This dictionary is then set to be the basis of the spell checker.</p>

    <p>Inspired by Norvig model and <a href="https://github.com/wolfgarbe/SymSpell">SymSpell</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">TOKEN</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/spell_check/norvig_sweeting/index.html#sparknlp.annotator.spell_check.norvig_sweeting.NorvigSweetingApproach">NorvigSweetingApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/spell/norvig/NorvigSweetingApproach">NorvigSweetingApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/spell/norvig/NorvigSweetingApproach.scala">NorvigSweetingApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">spellChecker</span> <span class="o">=</span> <span class="n">NorvigSweetingApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"spell"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setDictionary</span><span class="p">(</span><span class="s">"src/test/resources/spell/words.txt"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">spellChecker</span>
<span class="p">])</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.spell.norvig.NorvigSweetingApproach</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">spellChecker</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NorvigSweetingApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"spell"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setDictionary</span><span class="o">(</span><span class="s">"src/test/resources/spell/words.txt"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">spellChecker</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="symmetricdelete-spellchecker">SymmetricDelete Spellchecker</h3>

    <p>Trains a Symmetric Delete spelling correction algorithm.
Retrieves tokens and utilizes distance metrics to compute possible derived words.</p>

    <p>A dictionary of correct spellings must be provided with <code class="language-plaintext highlighter-rouge">setDictionary</code> as a text file, where each word is parsed by a regex pattern.</p>

    <p>For Example a file <code class="language-plaintext highlighter-rouge">"words.txt"</code>:</p>
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>...
gummy
gummic
gummier
gummiest
gummiferous
...
</code></pre></div>    </div>
    <p>can be parsed with the regular expression <code class="language-plaintext highlighter-rouge">\S+</code>, which is the default for <code class="language-plaintext highlighter-rouge">setDictionary</code>.</p>

    <p>This dictionary is then set to be the basis of the spell checker.</p>

    <p>Inspired by <a href="https://github.com/wolfgarbe/SymSpell">SymSpell</a>.</p>

    <p>For instantiated/pretrained models, see SymmetricDeleteModel.</p>

    <p>See <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/spell/symmetric/SymmetricDeleteModelTestSpec.scala">SymmetricDeleteModelTestSpec</a> for further reference.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">TOKEN</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/spell_check/symmetric_delete/index.html#sparknlp.annotator.spell_check.symmetric_delete.SymmetricDeleteApproach">SymmetricDeleteApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/spell/symmetric/SymmetricDeleteApproach">SymmetricDeleteApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/spell/symmetric/SymmetricDeleteApproach.scala">SymmetricDeleteApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">spellChecker</span> <span class="o">=</span> <span class="n">SymmetricDeleteApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"spell"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setDictionary</span><span class="p">(</span><span class="s">"src/test/resources/spell/words.txt"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">spellChecker</span>
<span class="p">])</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteApproach</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">spellChecker</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SymmetricDeleteApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"spell"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setDictionary</span><span class="o">(</span><span class="s">"src/test/resources/spell/words.txt"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">spellChecker</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

</div>
<div class="h3-box">

  <h2 id="token-classification">Token Classification</h2>
  <p>These are annotators that can be trained to recognize named entities in text.</p>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="nercrfapproach">NerCrfApproach</h3>

    <p>Algorithm for training a Named Entity Recognition Model</p>

    <p>This Named Entity recognition annotator allows for a generic model to be trained by utilizing a CRF machine learning
algorithm. The training data should be a labeled Spark Dataset, e.g. <a href="/docs/en/training#conll-dataset">CoNLL</a> 2003 IOB with
<code class="language-plaintext highlighter-rouge">Annotation</code> type columns. The data should have columns of type <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS</code> and an
additional label column of annotator type <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code>.
Excluding the label, this can be done with for example</p>
    <ul>
      <li>a <a href="/docs/en/annotators#sentencedetector">SentenceDetector</a>,</li>
      <li>a <a href="/docs/en/annotators#tokenizer">Tokenizer</a> and</li>
      <li>a <a href="/docs/en/annotators#postagger-part-of-speech-tagger">PerceptronModel</a> and</li>
      <li>a <a href="/docs/en/annotators#wordembeddings">WordEmbeddingsModel</a>
  (any word embeddings can be chosen, e.g. <a href="/docs/en/transformers#bertembeddings">BertEmbeddings</a> for BERT based embeddings).</li>
    </ul>

    <p>Optionally the user can provide an entity dictionary file with <code class="language-plaintext highlighter-rouge">setExternalFeatures</code> for better accuracy.</p>

    <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/crf-ner/ner_dl_crf.ipynb">Examples</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/ner/ner_crf/index.html#sparknlp.annotator.ner.ner_crf.NerCrfApproach">NerCrfApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/ner/crf/NerCrfApproach">NerCrfApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/ner/crf/NerCrfApproach.scala">NerCrfApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># This CoNLL dataset already includes a sentence, token, POS tags and label
# column with their respective annotator types. If a custom dataset is used,
# these need to be defined with for example:
</span>
<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \\
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \\
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \\
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \\
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \\
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \\
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">posTagger</span> <span class="o">=</span> <span class="n">PerceptronModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \\
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \\
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"pos"</span><span class="p">)</span>

<span class="n">Then</span> <span class="n">training</span> <span class="n">can</span> <span class="n">start</span><span class="p">:</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">WordEmbeddingsModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \\
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \\
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span> \\
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">nerTagger</span> <span class="o">=</span> <span class="n">NerCrfApproach</span><span class="p">()</span> \\
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"pos"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \\
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \\
    <span class="p">.</span><span class="n">setMinEpochs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> \\
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> \\
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">nerTagger</span>
<span class="p">])</span>

<span class="c1"># We use the sentences, tokens, POS tags and labels from the CoNLL dataset.
</span>
<span class="n">conll</span> <span class="o">=</span> <span class="n">CoNLL</span><span class="p">()</span>
<span class="n">trainingData</span> <span class="o">=</span> <span class="n">conll</span><span class="p">.</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="p">)</span>
<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.CoNLL</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator.NerCrfApproach</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// This CoNLL dataset already includes a sentence, token, POS tags and label</span>
<span class="c1">// column with their respective annotator types. If a custom dataset is used,</span>
<span class="c1">// these need to be defined with for example:</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">posTagger</span> <span class="k">=</span> <span class="nv">PerceptronModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"pos"</span><span class="o">)</span>

<span class="c1">// Then the training can start</span>
<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">WordEmbeddingsModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">nerTagger</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NerCrfApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"pos"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMinEpochs</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">3</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerTagger</span>
<span class="o">))</span>

<span class="c1">// We use the sentences, tokens, POS tags and labels from the CoNLL dataset.</span>
<span class="k">val</span> <span class="nv">conll</span> <span class="k">=</span> <span class="nc">CoNLL</span><span class="o">()</span>
<span class="k">val</span> <span class="nv">trainingData</span> <span class="k">=</span> <span class="nv">conll</span><span class="o">.</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="nerdlapproach">NerDLApproach</h3>

    <p>This Named Entity recognition annotator allows to train generic NER model based on Neural Networks.</p>

    <p>The architecture of the neural network is a Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets.</p>

    <p>The training data should be a labeled Spark Dataset, in the format of <a href="/docs/en/training#conll-dataset">CoNLL</a>
2003 IOB with <code class="language-plaintext highlighter-rouge">Annotation</code> type columns. The data should have columns of type <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN, WORD_EMBEDDINGS</code> and an
additional label column of annotator type <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code>.
Excluding the label, this can be done with for example</p>
    <ul>
      <li>a <a href="/docs/en/annotators#sentencedetector">SentenceDetector</a>,</li>
      <li>a <a href="/docs/en/annotators#tokenizer">Tokenizer</a> and</li>
      <li>a <a href="/docs/en/annotators#wordembeddings">WordEmbeddingsModel</a>
  (any word embeddings can be chosen, e.g. <a href="/docs/en/transformers#bertembeddings">BertEmbeddings</a> for BERT based embeddings).</li>
    </ul>

    <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/dl-ner">Examples</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN, WORD_EMBEDDINGS</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/ner/ner_dl/index.html#sparknlp.annotator.ner.ner_dl.NerDLApproach">NerDLApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/ner/dl/NerDLApproach">NerDLApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/ner/dl/NerDLApproach.scala">NerDLApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># This CoNLL dataset already includes a sentence, token and label
# column with their respective annotator types. If a custom dataset is used,
# these need to be defined with for example:
</span>
<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \\
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \\
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \\
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \\
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \\
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \\
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">Then</span> <span class="n">the</span> <span class="n">training</span> <span class="n">can</span> <span class="n">start</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">BertEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \\
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \\
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="n">nerTagger</span> <span class="o">=</span> <span class="n">NerDLApproach</span><span class="p">()</span> \\
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \\
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \\
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \\
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> \\
    <span class="p">.</span><span class="n">setRandomSeed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> \\
    <span class="p">.</span><span class="n">setVerbose</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">nerTagger</span>
<span class="p">])</span>

<span class="c1"># We use the sentences, tokens, and labels from the CoNLL dataset.
</span>
<span class="n">conll</span> <span class="o">=</span> <span class="n">CoNLL</span><span class="p">()</span>
<span class="n">trainingData</span> <span class="o">=</span> <span class="n">conll</span><span class="p">.</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="p">)</span>
<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.BertEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.CoNLL</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// This CoNLL dataset already includes a sentence, token and label</span>
<span class="c1">// column with their respective annotator types. If a custom dataset is used,</span>
<span class="c1">// these need to be defined with for example:</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="c1">// Then the training can start</span>
<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">BertEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">nerTagger</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NerDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setRandomSeed</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setVerbose</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerTagger</span>
<span class="o">))</span>

<span class="c1">// We use the sentences, tokens and labels from the CoNLL dataset</span>
<span class="k">val</span> <span class="nv">conll</span> <span class="k">=</span> <span class="nc">CoNLL</span><span class="o">()</span>
<span class="k">val</span> <span class="nv">trainingData</span> <span class="k">=</span> <span class="nv">conll</span><span class="o">.</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

</div>
<div class="h3-box">

  <h2 id="text-classification">Text Classification</h2>
  <p>These are annotators that can be trained to classify text into different
classes, such as sentiment.</p>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="classifierdlapproach">ClassifierDLApproach</h3>

    <p>Trains a ClassifierDL for generic Multi-class Text Classification.</p>

    <p>ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an input for text classifications.
The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to
100 classes.</p>

    <p>For extended examples of usage, see the Examples
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/scala/training/Train%20Multi-Class%20Text%20Classification%20on%20News%20Articles.scala">[1] </a>
<a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/classification/ClassifierDL_Train_multi_class_news_category_classifier.ipynb">[2] </a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">SENTENCE_EMBEDDINGS</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">CATEGORY</code></p>

    <blockquote>
      <p><strong>Note:</strong> This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. UniversalSentenceEncoder, BertSentenceEmbeddings, or SentenceEmbeddings can be used for the inputCol</p>
    </blockquote>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/classifier_dl/classifier_dl/index.html#sparknlp.annotator.classifier_dl.classifier_dl.ClassifierDLApproach">ClassifierDLApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/classifier/dl/ClassifierDLApproach">ClassifierDLApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/ClassifierDLApproach.scala">ClassifierDLApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In this example, the training data `"sentiment.csv"` has the form of
#
# text,label
# This movie is the best movie I have wached ever! In my opinion this movie can win an award.,0
# This was a terrible movie! The acting was bad really bad!,1
# ...
#
# Then traning can be done like so:
</span>
<span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">smallCorpus</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span><span class="s">"True"</span><span class="p">).</span><span class="n">csv</span><span class="p">(</span><span class="s">"src/test/resources/classifier/sentiment.csv"</span><span class="p">)</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">useEmbeddings</span> <span class="o">=</span> <span class="n">UniversalSentenceEncoder</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span>

<span class="n">docClassifier</span> <span class="o">=</span> <span class="n">ClassifierDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"category"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setBatchSize</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLr</span><span class="p">(</span><span class="mf">5e-3</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setDropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">(</span>
      <span class="p">[</span>
        <span class="n">documentAssembler</span><span class="p">,</span>
        <span class="n">useEmbeddings</span><span class="p">,</span>
        <span class="n">docClassifier</span>
      <span class="p">]</span>
    <span class="p">)</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">smallCorpus</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// In this example, the training data `"sentiment.csv"` has the form of</span>
<span class="c1">//</span>
<span class="c1">// text,label</span>
<span class="c1">// This movie is the best movie I have wached ever! In my opinion this movie can win an award.,0</span>
<span class="c1">// This was a terrible movie! The acting was bad really bad!,1</span>
<span class="c1">// ...</span>
<span class="c1">//</span>
<span class="c1">// Then traning can be done like so:</span>

<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">smallCorpus</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span><span class="s">"true"</span><span class="o">).</span><span class="py">csv</span><span class="o">(</span><span class="s">"src/test/resources/classifier/sentiment.csv"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">useEmbeddings</span> <span class="k">=</span> <span class="nv">UniversalSentenceEncoder</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">docClassifier</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ClassifierDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"category"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setBatchSize</span><span class="o">(</span><span class="mi">64</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">20</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLr</span><span class="o">(</span><span class="mi">5</span><span class="n">e</span><span class="o">-</span><span class="mf">3f</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setDropout</span><span class="o">(</span><span class="mf">0.5f</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span>
    <span class="nc">Array</span><span class="o">(</span>
      <span class="n">documentAssembler</span><span class="o">,</span>
      <span class="n">useEmbeddings</span><span class="o">,</span>
      <span class="n">docClassifier</span>
    <span class="o">)</span>
  <span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">smallCorpus</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="multiclassifierdlapproach">MultiClassifierDLApproach</h3>

    <p>Trains a MultiClassifierDL for Multi-label Text Classification.</p>

    <p>MultiClassifierDL uses a Bidirectional GRU with a convolutional model that we have built inside TensorFlow and supports
up to 100 classes.</p>

    <p>The input to MultiClassifierDL is Sentence Embeddings such as state-of-the-art
<a href="docs/en/transformers#universalsentenceencoder">UniversalSentenceEncoder</a>,
<a href="/docs/en/transformers#bertsentenceembeddings">BertSentenceEmbeddings</a>, or
<a href="/docs/en/annotators#sentenceembeddings">SentenceEmbeddings</a>.</p>

    <p>In machine learning, multi-label classification and the strongly related problem of multi-output classification are
variants of the classification problem where multiple labels may be assigned to each instance. Multi-label
classification is a generalization of multiclass classification, which is the single-label problem of categorizing
instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many
of the classes the instance can be assigned to.
Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y
(assigning a value of 0 or 1 for each element (label) in y).</p>

    <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/classification/MultiClassifierDL_train_multi_label_E2E_challenge_classifier.ipynb">Examples</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">SENTENCE_EMBEDDINGS</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">CATEGORY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/classifier_dl/multi_classifier_dl/index.html#sparknlp.annotator.classifier_dl.multi_classifier_dl.MultiClassifierDLApproach">MultiClassifierDLApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/classifier/dl/MultiClassifierDLApproach">MultiClassifierDLApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/MultiClassifierDLApproach.scala">MultiClassifierDLApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In this example, the training data has the form
#
# +----------------+--------------------+--------------------+
# |              id|                text|              labels|
# +----------------+--------------------+--------------------+
# |ed58abb40640f983|PN NewsYou mean ... |             [toxic]|
# |a1237f726b5f5d89|Dude.  Place the ...|   [obscene, insult]|
# |24b0d6c8733c2abe|Thanks  - thanks ...|            [insult]|
# |8c4478fb239bcfc0|" Gee, 5 minutes ...|[toxic, obscene, ...|
# +----------------+--------------------+--------------------+
</span>
<span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># Process training data to create text with associated array of labels
</span>
<span class="n">trainDataset</span><span class="p">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="c1"># root
#  |-- id: string (nullable = true)
#  |-- text: string (nullable = true)
#  |-- labels: array (nullable = true)
#  |    |-- element: string (containsNull = true)
</span>

<span class="c1"># Then create pipeline for training
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCleanupMode</span><span class="p">(</span><span class="s">"shrink"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">UniversalSentenceEncoder</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="n">docClassifier</span> <span class="o">=</span> <span class="n">MultiClassifierDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"category"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"labels"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setBatchSize</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLr</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setThreshold</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setValidationSplit</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">(</span>
      <span class="p">[</span>
        <span class="n">documentAssembler</span><span class="p">,</span>
        <span class="n">embeddings</span><span class="p">,</span>
        <span class="n">docClassifier</span>
      <span class="p">]</span>
    <span class="p">)</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainDataset</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// In this example, the training data has the form (Note: labels can be arbitrary)</span>
<span class="c1">//</span>
<span class="c1">// mr,ref</span>
<span class="c1">// "name[Alimentum], area[city centre], familyFriendly[no], near[Burger King]",Alimentum is an adult establish found in the city centre area near Burger King.</span>
<span class="c1">// "name[Alimentum], area[city centre], familyFriendly[yes]",Alimentum is a family-friendly place in the city centre.</span>
<span class="c1">// ...</span>
<span class="c1">//</span>
<span class="c1">// It needs some pre-processing first, so the labels are of type `Array[String]`. This can be done like so:</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.classifier.dl.MultiClassifierDLApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.functions.</span><span class="o">{</span><span class="n">col</span><span class="o">,</span> <span class="n">udf</span><span class="o">}</span>

<span class="c1">// Process training data to create text with associated array of labels</span>
<span class="k">def</span> <span class="nf">splitAndTrim</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">labels</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span>
  <span class="nv">labels</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">", "</span><span class="o">).</span><span class="py">map</span><span class="o">(</span><span class="n">x</span><span class="k">=&gt;</span><span class="nv">x</span><span class="o">.</span><span class="py">trim</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">val</span> <span class="nv">smallCorpus</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span>
  <span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"inferSchema"</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"mode"</span><span class="o">,</span> <span class="s">"DROPMALFORMED"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">csv</span><span class="o">(</span><span class="s">"src/test/resources/classifier/e2e.csv"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">withColumn</span><span class="o">(</span><span class="s">"labels"</span><span class="o">,</span> <span class="nf">splitAndTrim</span><span class="o">(</span><span class="nf">col</span><span class="o">(</span><span class="s">"mr"</span><span class="o">)))</span>
  <span class="o">.</span><span class="py">withColumn</span><span class="o">(</span><span class="s">"text"</span><span class="o">,</span> <span class="nf">col</span><span class="o">(</span><span class="s">"ref"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">drop</span><span class="o">(</span><span class="s">"mr"</span><span class="o">)</span>

<span class="nv">smallCorpus</span><span class="o">.</span><span class="py">printSchema</span><span class="o">()</span>
<span class="c1">// root</span>
<span class="c1">// |-- ref: string (nullable = true)</span>
<span class="c1">// |-- labels: array (nullable = true)</span>
<span class="c1">// |    |-- element: string (containsNull = true)</span>

<span class="c1">// Then create pipeline for training</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCleanupMode</span><span class="o">(</span><span class="s">"shrink"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">UniversalSentenceEncoder</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">docClassifier</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MultiClassifierDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"category"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"labels"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setBatchSize</span><span class="o">(</span><span class="mi">128</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">10</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLr</span><span class="o">(</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mf">3f</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setThreshold</span><span class="o">(</span><span class="mf">0.5f</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setValidationSplit</span><span class="o">(</span><span class="mf">0.1f</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span>
    <span class="nc">Array</span><span class="o">(</span>
      <span class="n">documentAssembler</span><span class="o">,</span>
      <span class="n">embeddings</span><span class="o">,</span>
      <span class="n">docClassifier</span>
    <span class="o">)</span>
  <span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">smallCorpus</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="sentimentdlapproach">SentimentDLApproach</h3>

    <p>Trains a MultiClassifierDL for Multi-label Text Classification.</p>

    <p>MultiClassifierDL uses a Bidirectional GRU with a convolutional model that we have built inside TensorFlow and supports
up to 100 classes.</p>

    <p>The input to MultiClassifierDL is Sentence Embeddings such as state-of-the-art
<a href="docs/en/transformers#universalsentenceencoder">UniversalSentenceEncoder</a>,
<a href="/docs/en/transformers#bertsentenceembeddings">BertSentenceEmbeddings</a>, or
<a href="/docs/en/annotators#sentenceembeddings">SentenceEmbeddings</a>.</p>

    <p>In machine learning, multi-label classification and the strongly related problem of multi-output classification are
variants of the classification problem where multiple labels may be assigned to each instance. Multi-label
classification is a generalization of multiclass classification, which is the single-label problem of categorizing
instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many
of the classes the instance can be assigned to.
Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y
(assigning a value of 0 or 1 for each element (label) in y).</p>

    <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/classification/MultiClassifierDL_train_multi_label_E2E_challenge_classifier.ipynb">Examples</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">SENTENCE_EMBEDDINGS</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">CATEGORY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/classifier_dl/multi_classifier_dl/index.html#sparknlp.annotator.classifier_dl.multi_classifier_dl.MultiClassifierDLApproach">MultiClassifierDLApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/classifier/dl/MultiClassifierDLApproach">MultiClassifierDLApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/MultiClassifierDLApproach.scala">MultiClassifierDLApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># In this example, the training data has the form
#
# +----------------+--------------------+--------------------+
# |              id|                text|              labels|
# +----------------+--------------------+--------------------+
# |ed58abb40640f983|PN NewsYou mean ... |             [toxic]|
# |a1237f726b5f5d89|Dude.  Place the ...|   [obscene, insult]|
# |24b0d6c8733c2abe|Thanks  - thanks ...|            [insult]|
# |8c4478fb239bcfc0|" Gee, 5 minutes ...|[toxic, obscene, ...|
# +----------------+--------------------+--------------------+
</span>
<span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># Process training data to create text with associated array of labels
</span>
<span class="n">trainDataset</span><span class="p">.</span><span class="n">printSchema</span><span class="p">()</span>
<span class="c1"># root
#  |-- id: string (nullable = true)
#  |-- text: string (nullable = true)
#  |-- labels: array (nullable = true)
#  |    |-- element: string (containsNull = true)
</span>

<span class="c1"># Then create pipeline for training
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCleanupMode</span><span class="p">(</span><span class="s">"shrink"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">UniversalSentenceEncoder</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="n">docClassifier</span> <span class="o">=</span> <span class="n">MultiClassifierDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"category"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"labels"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setBatchSize</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLr</span><span class="p">(</span><span class="mf">1e-3</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setThreshold</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setValidationSplit</span><span class="p">(</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">(</span>
      <span class="p">[</span>
        <span class="n">documentAssembler</span><span class="p">,</span>
        <span class="n">embeddings</span><span class="p">,</span>
        <span class="n">docClassifier</span>
      <span class="p">]</span>
    <span class="p">)</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainDataset</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// In this example, the training data has the form (Note: labels can be arbitrary)</span>
<span class="c1">//</span>
<span class="c1">// mr,ref</span>
<span class="c1">// "name[Alimentum], area[city centre], familyFriendly[no], near[Burger King]",Alimentum is an adult establish found in the city centre area near Burger King.</span>
<span class="c1">// "name[Alimentum], area[city centre], familyFriendly[yes]",Alimentum is a family-friendly place in the city centre.</span>
<span class="c1">// ...</span>
<span class="c1">//</span>
<span class="c1">// It needs some pre-processing first, so the labels are of type `Array[String]`. This can be done like so:</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.classifier.dl.MultiClassifierDLApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>
<span class="k">import</span> <span class="nn">org.apache.spark.sql.functions.</span><span class="o">{</span><span class="n">col</span><span class="o">,</span> <span class="n">udf</span><span class="o">}</span>

<span class="c1">// Process training data to create text with associated array of labels</span>
<span class="k">def</span> <span class="nf">splitAndTrim</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">labels</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span>
  <span class="nv">labels</span><span class="o">.</span><span class="py">split</span><span class="o">(</span><span class="s">", "</span><span class="o">).</span><span class="py">map</span><span class="o">(</span><span class="n">x</span><span class="k">=&gt;</span><span class="nv">x</span><span class="o">.</span><span class="py">trim</span><span class="o">)</span>
<span class="o">}</span>

<span class="k">val</span> <span class="nv">smallCorpus</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span>
  <span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"inferSchema"</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"mode"</span><span class="o">,</span> <span class="s">"DROPMALFORMED"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">csv</span><span class="o">(</span><span class="s">"src/test/resources/classifier/e2e.csv"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">withColumn</span><span class="o">(</span><span class="s">"labels"</span><span class="o">,</span> <span class="nf">splitAndTrim</span><span class="o">(</span><span class="nf">col</span><span class="o">(</span><span class="s">"mr"</span><span class="o">)))</span>
  <span class="o">.</span><span class="py">withColumn</span><span class="o">(</span><span class="s">"text"</span><span class="o">,</span> <span class="nf">col</span><span class="o">(</span><span class="s">"ref"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">drop</span><span class="o">(</span><span class="s">"mr"</span><span class="o">)</span>

<span class="nv">smallCorpus</span><span class="o">.</span><span class="py">printSchema</span><span class="o">()</span>
<span class="c1">// root</span>
<span class="c1">// |-- ref: string (nullable = true)</span>
<span class="c1">// |-- labels: array (nullable = true)</span>
<span class="c1">// |    |-- element: string (containsNull = true)</span>

<span class="c1">// Then create pipeline for training</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCleanupMode</span><span class="o">(</span><span class="s">"shrink"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">UniversalSentenceEncoder</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">docClassifier</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">MultiClassifierDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"category"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"labels"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setBatchSize</span><span class="o">(</span><span class="mi">128</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">10</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLr</span><span class="o">(</span><span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mf">3f</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setThreshold</span><span class="o">(</span><span class="mf">0.5f</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setValidationSplit</span><span class="o">(</span><span class="mf">0.1f</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span>
    <span class="nc">Array</span><span class="o">(</span>
      <span class="n">documentAssembler</span><span class="o">,</span>
      <span class="n">embeddings</span><span class="o">,</span>
      <span class="n">docClassifier</span>
    <span class="o">)</span>
  <span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">smallCorpus</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="viveknsentimentapproach">ViveknSentimentApproach</h3>

    <p>Trains a sentiment analyser inspired by the algorithm by Vivek Narayanan https://github.com/vivekn/sentiment/.</p>

    <p>The algorithm is based on the paper
<a href="https://arxiv.org/abs/1305.6143">“Fast and accurate sentiment classification using an enhanced Naive Bayes model”</a>.</p>

    <p>The analyzer requires sentence boundaries to give a score in context.
Tokenization is needed to make sure tokens are within bounds. Transitivity requirements are also required.</p>

    <p>The training data needs to consist of a column for normalized text and a label column (either <code class="language-plaintext highlighter-rouge">"positive"</code> or <code class="language-plaintext highlighter-rouge">"negative"</code>).</p>

    <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/training/english/vivekn-sentiment/VivekNarayanSentimentApproach.ipynb">Examples</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">TOKEN, DOCUMENT</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">SENTIMENT</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/sentiment/vivekn_sentiment/index.html#sparknlp.annotator.sentiment.vivekn_sentiment.ViveknSentimentApproach">ViveknSentimentApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/sda/vivekn/ViveknSentimentApproach">ViveknSentimentApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/sda/vivekn/ViveknSentimentApproach.scala">ViveknSentimentApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">document</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">token</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">normalizer</span> <span class="o">=</span> <span class="n">Normalizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"normal"</span><span class="p">)</span>

<span class="n">vivekn</span> <span class="o">=</span> <span class="n">ViveknSentimentApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"normal"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setSentimentCol</span><span class="p">(</span><span class="s">"train_sentiment"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"result_sentiment"</span><span class="p">)</span>

<span class="n">finisher</span> <span class="o">=</span> <span class="n">Finisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"result_sentiment"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"final_sentiment"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span><span class="n">document</span><span class="p">,</span> <span class="n">token</span><span class="p">,</span> <span class="n">normalizer</span><span class="p">,</span> <span class="n">vivekn</span><span class="p">,</span> <span class="n">finisher</span><span class="p">])</span>

<span class="n">training</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([</span>
    <span class="p">(</span><span class="s">"I really liked this movie!"</span><span class="p">,</span> <span class="s">"positive"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"The cast was horrible"</span><span class="p">,</span> <span class="s">"negative"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"Never going to watch this again or recommend it to anyone"</span><span class="p">,</span> <span class="s">"negative"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"It's a waste of time"</span><span class="p">,</span> <span class="s">"negative"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"I loved the protagonist"</span><span class="p">,</span> <span class="s">"positive"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"The music was really really good"</span><span class="p">,</span> <span class="s">"positive"</span><span class="p">)</span>
<span class="p">]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">,</span> <span class="s">"train_sentiment"</span><span class="p">)</span>
<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([</span>
    <span class="p">[</span><span class="s">"I recommend this movie"</span><span class="p">],</span>
    <span class="p">[</span><span class="s">"Dont waste your time!!!"</span><span class="p">]</span>
<span class="p">]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipelineModel</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"final_sentiment"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+---------------+</span>
<span class="o">|</span><span class="n">final_sentiment</span><span class="o">|</span>
<span class="o">+---------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">positive</span><span class="p">]</span>     <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">negative</span><span class="p">]</span>     <span class="o">|</span>
<span class="o">+---------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Normalizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.Finisher</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">document</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">token</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">normalizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Normalizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"normal"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">vivekn</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ViveknSentimentApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"normal"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setSentimentCol</span><span class="o">(</span><span class="s">"train_sentiment"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"result_sentiment"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">finisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Finisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"result_sentiment"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"final_sentiment"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="n">document</span><span class="o">,</span> <span class="n">token</span><span class="o">,</span> <span class="n">normalizer</span><span class="o">,</span> <span class="n">vivekn</span><span class="o">,</span> <span class="n">finisher</span><span class="o">))</span>

<span class="k">val</span> <span class="nv">training</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
  <span class="o">(</span><span class="s">"I really liked this movie!"</span><span class="o">,</span> <span class="s">"positive"</span><span class="o">),</span>
  <span class="o">(</span><span class="s">"The cast was horrible"</span><span class="o">,</span> <span class="s">"negative"</span><span class="o">),</span>
  <span class="o">(</span><span class="s">"Never going to watch this again or recommend it to anyone"</span><span class="o">,</span> <span class="s">"negative"</span><span class="o">),</span>
  <span class="o">(</span><span class="s">"It's a waste of time"</span><span class="o">,</span> <span class="s">"negative"</span><span class="o">),</span>
  <span class="o">(</span><span class="s">"I loved the protagonist"</span><span class="o">,</span> <span class="s">"positive"</span><span class="o">),</span>
  <span class="o">(</span><span class="s">"The music was really really good"</span><span class="o">,</span> <span class="s">"positive"</span><span class="o">)</span>
<span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">,</span> <span class="s">"train_sentiment"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">training</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
  <span class="s">"I recommend this movie"</span><span class="o">,</span>
  <span class="s">"Dont waste your time!!!"</span>
<span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipelineModel</span><span class="o">.</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"final_sentiment"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+---------------+</span>
<span class="o">|</span><span class="n">final_sentiment</span><span class="o">|</span>
<span class="o">+---------------+</span>
<span class="o">|[</span><span class="kt">positive</span><span class="o">]</span>     <span class="o">|</span>
<span class="o">|[</span><span class="kt">negative</span><span class="o">]</span>     <span class="o">|</span>
<span class="o">+---------------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

</div>
<div class="h3-box">

  <h2 id="text-representation">Text Representation</h2>
  <p>These are annotators that can be trained to turn text into a numerical
representation.</p>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="doc2vecapproach">Doc2VecApproach</h3>

    <p>Trains a Word2Vec model that creates vector representations of words in a text corpus.</p>

    <p>The algorithm first constructs a vocabulary from the corpus
and then learns vector representation of words in the vocabulary.
The vector representation can be used as features in
natural language processing and machine learning algorithms.</p>

    <p>We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax
method to train the model. The variable names in the implementation match the original C implementation.</p>

    <p>For instantiated/pretrained models, see Doc2VecModel.</p>

    <p><strong>Sources</strong> :</p>

    <p>For the original C implementation, see <a href="https://code.google.com/p/word2vec/">https://code.google.com/p/word2vec/</a></p>

    <p>For the research paper, see
<a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a>
and <a href="https://arxiv.org/pdf/1310.4546v1.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">SENTENCE_EMBEDDINGS</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/embeddings/doc2vec/index.html#sparknlp.annotator.embeddings.doc2vec.Doc2VecApproach">Doc2VecApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/embeddings/Doc2VecApproach">Doc2VecApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/Doc2VecApproach.scala">Doc2VecApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">Doc2VecApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">([</span>
      <span class="n">documentAssembler</span><span class="p">,</span>
      <span class="n">tokenizer</span><span class="p">,</span>
      <span class="n">embeddings</span>
    <span class="p">])</span>

<span class="n">path</span> <span class="o">=</span> <span class="s">"sherlockholmes.txt"</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="n">path</span><span class="p">).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator.</span><span class="o">{</span><span class="nc">Tokenizer</span><span class="o">,</span> <span class="nc">Doc2VecApproach</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Doc2VecApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
    <span class="n">documentAssembler</span><span class="o">,</span>
    <span class="n">tokenizer</span><span class="o">,</span>
    <span class="n">embeddings</span>
  <span class="o">))</span>

<span class="k">val</span> <span class="nv">path</span> <span class="k">=</span> <span class="s">"src/test/resources/spell/sherlockholmes.txt"</span>
<span class="k">val</span> <span class="nv">dataset</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">sparkContext</span><span class="o">.</span><span class="py">textFile</span><span class="o">(</span><span class="n">path</span><span class="o">)</span>
  <span class="o">.</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">dataset</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="word2vecapproach">Word2VecApproach</h3>

    <p>Trains a Word2Vec model that creates vector representations of words in a text corpus.</p>

    <p>The algorithm first constructs a vocabulary from the corpus
and then learns vector representation of words in the vocabulary.
The vector representation can be used as features in
natural language processing and machine learning algorithms.</p>

    <p>We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax
method to train the model. The variable names in the implementation match the original C implementation.</p>

    <p>For instantiated/pretrained models, see Word2VecModel.</p>

    <p><strong>Sources</strong> :</p>

    <p>For the original C implementation, see <a href="https://code.google.com/p/word2vec/">https://code.google.com/p/word2vec/</a></p>

    <p>For the research paper, see
<a href="https://arxiv.org/abs/1301.3781">Efficient Estimation of Word Representations in Vector Space</a>
and <a href="https://arxiv.org/pdf/1310.4546v1.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>.</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">WORD_EMBEDDINGS</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/embeddings/word2vec/index.html#sparknlp.annotator.embeddings.word2vec.Word2VecApproach">Word2VecApproach</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/embeddings/Word2VecApproach">Word2VecApproach</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/Word2VecApproach.scala">Word2VecApproach</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">Word2VecApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">([</span>
      <span class="n">documentAssembler</span><span class="p">,</span>
      <span class="n">tokenizer</span><span class="p">,</span>
      <span class="n">embeddings</span>
    <span class="p">])</span>

<span class="n">path</span> <span class="o">=</span> <span class="s">"sherlockholmes.txt"</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="n">path</span><span class="p">).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator.</span><span class="o">{</span><span class="nc">Tokenizer</span><span class="o">,</span> <span class="nc">Word2VecApproach</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Word2VecApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
    <span class="n">documentAssembler</span><span class="o">,</span>
    <span class="n">tokenizer</span><span class="o">,</span>
    <span class="n">embeddings</span>
  <span class="o">))</span>

<span class="k">val</span> <span class="nv">path</span> <span class="k">=</span> <span class="s">"src/test/resources/spell/sherlockholmes.txt"</span>
<span class="k">val</span> <span class="nv">dataset</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">sparkContext</span><span class="o">.</span><span class="py">textFile</span><span class="o">(</span><span class="n">path</span><span class="o">)</span>
  <span class="o">.</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">dataset</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

</div>
<div class="h3-box">

  <h2 id="external-trainable-models">External Trainable Models</h2>
  <p>These are annotators that are trained in an external library, which are then
loaded into Spark NLP.</p>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="albertfortokenclassification">AlbertForTokenClassification</h3>

    <p>AlbertForTokenClassification can load Albert Models with a token classification head
on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.</p>

    <p>Since Spark NLP 3.2.x, this annotator needs to be trained externally using the
transformers library. After the training process is done, the model checkpoint
can be loaded by this annotator. This is done with <code class="language-plaintext highlighter-rouge">loadSavedModel</code> (for loading
the transformers model) and <code class="language-plaintext highlighter-rouge">load</code> for the saved Spark NLP model.</p>

    <p>For an extended example see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/HuggingFace%20in%20Spark%20NLP%20-%20AlbertForTokenClassification.ipynb">Examples</a>.</p>

    <p>Example for loading a saved transformers model:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Installing prerequisites
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">transformers</span><span class="o">==</span><span class="mf">4.10</span><span class="p">.</span><span class="mi">0</span> <span class="n">tensorflow</span><span class="o">==</span><span class="mf">2.4</span><span class="p">.</span><span class="mi">1</span> <span class="n">sentencepiece</span>

<span class="c1"># Loading the external transformers model
</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFAlbertForTokenClassification</span><span class="p">,</span> <span class="n">AlbertTokenizer</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'HooshvareLab/albert-fa-zwnj-base-v2-ner'</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AlbertTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">'./{}_tokenizer/'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="c1"># just in case if there is no TF/Keras file provided in the model
# we can just use `from_pt` and convert PyTorch to TensorFlow
</span><span class="k">try</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'try downloading TF weights'</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">TFAlbertForTokenClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'try downloading PyTorch weights'</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">TFAlbertForTokenClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">"./{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span> <span class="n">saved_model</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Extracting the tokenizer resources
</span><span class="n">asset_path</span> <span class="o">=</span> <span class="s">'{}/saved_model/1/assets'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

<span class="err">!</span><span class="n">cp</span> <span class="p">{</span><span class="n">MODEL_NAME</span><span class="p">}</span><span class="n">_tokenizer</span><span class="o">/</span><span class="n">spiece</span><span class="p">.</span><span class="n">model</span> <span class="p">{</span><span class="n">asset_path</span><span class="p">}</span>

<span class="c1"># Get label2id dictionary
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">label2id</span>
<span class="c1"># Sort the dictionary based on the id
</span><span class="n">labels</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">labels</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">asset_path</span><span class="o">+</span><span class="s">'/labels.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
</code></pre></div>    </div>

    <p>Then the model can be loaded and used into Spark NLP in the following examples:</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/classifier_dl/albert_for_token_classification/index.html#sparknlp.annotator.classifier_dl.albert_for_token_classification.AlbertForTokenClassification">AlbertForTokenClassification</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/classifier/dl/AlbertForTokenClassification">AlbertForTokenClassification</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/AlbertForTokenClassification.scala">AlbertForTokenClassification</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'HooshvareLab/albert-fa-zwnj-base-v2-ner'</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">AlbertForTokenClassification</span>\
  <span class="p">.</span><span class="n">loadSavedModel</span><span class="p">(</span><span class="s">'{}/saved_model/1'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span> <span class="n">spark</span><span class="p">)</span>\
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span>\
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span>\
  <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>\
  <span class="p">.</span><span class="n">setMaxSentenceLength</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

<span class="c1"># Optionally the classifier can be saved to load it more conveniently into Spark NLP
</span><span class="n">tokenClassifier</span><span class="p">.</span><span class="n">write</span><span class="p">().</span><span class="n">overwrite</span><span class="p">().</span><span class="n">save</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="n">tokenClassifier_loaded</span> <span class="o">=</span> <span class="n">AlbertForTokenClassification</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>\
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span>\
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"دفتر مرکزی شرکت کامیکو در شهر ساسکاتون ساسکاچوان قرار دارد."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"ner.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+-----------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                               <span class="o">|</span>
<span class="o">+-----------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">ORG</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">ORG</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">]</span><span class="o">|</span>
<span class="o">+-----------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// The model needs to be trained with the transformers library.</span>
<span class="c1">// Afterwards it can be loaded into the scala version of Spark NLP.</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">ModelName</span> <span class="k">=</span> <span class="s">"HooshvareLab/albert-fa-zwnj-base-v2-ner"</span>
<span class="k">val</span> <span class="nv">tokenClassifier</span> <span class="k">=</span> <span class="nc">AlbertForTokenClassification</span>
  <span class="o">.</span><span class="py">loadSavedModel</span><span class="o">(</span><span class="n">s</span><span class="s">"$ModelName/saved_model/1"</span><span class="o">,</span> <span class="n">spark</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxSentenceLength</span><span class="o">(</span><span class="mi">128</span><span class="o">)</span>

<span class="c1">// Optionally the classifier can be saved to load it more conveniently into Spark NLP</span>
<span class="nv">tokenClassifier</span><span class="o">.</span><span class="py">write</span><span class="o">.</span><span class="py">overwrite</span><span class="o">().</span><span class="py">save</span><span class="o">(</span><span class="n">s</span><span class="s">"${ModelName}_spark_nlp"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenClassifierLoaded</span> <span class="k">=</span> <span class="nv">AlbertForTokenClassification</span><span class="o">.</span><span class="py">load</span><span class="o">(</span><span class="n">s</span><span class="s">"${ModelName}_spark_nlp"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifierLoaded</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"دفتر مرکزی شرکت کامیکو در شهر ساسکاتون ساسکاچوان قرار دارد."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="n">truncate</span> <span class="k">=</span> <span class="kc">false</span><span class="o">)</span>
<span class="o">+-----------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                               <span class="o">|</span>
<span class="o">+-----------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-ORG</span>, <span class="kt">I-ORG</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">I-LOC</span>, <span class="kt">I-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span><span class="o">]|</span>
<span class="o">+-----------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="bertforsequenceclassification">BertForSequenceClassification</h3>

    <p>BertForSequenceClassification can load Bert Models with sequence
classification/regression head on top (a linear layer on top of the pooled
output) e.g. for multi-class document classification tasks.</p>

    <p>Since Spark NLP 3.2.x, this annotator needs to be trained externally using the
transformers library. After the training process is done, the model checkpoint
can be loaded by this annotator. This is done with <code class="language-plaintext highlighter-rouge">loadSavedModel</code> (for loading
the transformers model) and <code class="language-plaintext highlighter-rouge">load</code> for the saved Spark NLP model.</p>

    <p>For an extended example see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BertForSequenceClassification.ipynb">Examples</a>.</p>

    <p>Example for loading a saved transformers model:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Installing prerequisites
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">transformers</span><span class="o">==</span><span class="mf">4.8</span><span class="p">.</span><span class="mi">1</span> <span class="n">tensorflow</span><span class="o">==</span><span class="mf">2.4</span><span class="p">.</span><span class="mi">1</span>

<span class="c1"># Loading the external transformers model
</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFBertForSequenceClassification</span><span class="p">,</span> <span class="n">BertTokenizer</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'finiteautomata/beto-sentiment-analysis'</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">'./{}_tokenizer/'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">TFBertForSequenceClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">TFBertForSequenceClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">"./{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span> <span class="n">saved_model</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Extracting the tokenizer resources
</span><span class="n">asset_path</span> <span class="o">=</span> <span class="s">'{}/saved_model/1/assets'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

<span class="err">!</span><span class="n">cp</span> <span class="p">{</span><span class="n">MODEL_NAME</span><span class="p">}</span><span class="n">_tokenizer</span><span class="o">/</span><span class="n">vocab</span><span class="p">.</span><span class="n">txt</span> <span class="p">{</span><span class="n">asset_path</span><span class="p">}</span>

<span class="c1"># Get label2id dictionary
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">label2id</span>
<span class="c1"># Sort the dictionary based on the id
</span><span class="n">labels</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">labels</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">asset_path</span><span class="o">+</span><span class="s">'/labels.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
</code></pre></div>    </div>

    <p>Then the model can be loaded and used into Spark NLP in the following examples:</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/classifier_dl/bert_for_sequence_classification/index.html#sparknlp.annotator.classifier_dl.bert_for_sequence_classification.BertForSequenceClassification">BertForSequenceClassification</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/classifier/dl/BertForSequenceClassification">BertForSequenceClassification</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/BertForSequenceClassification.scala">BertForSequenceClassification</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'finiteautomata/beto-sentiment-analysis'</span>
<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="p">.</span><span class="n">loadSavedModel</span><span class="p">(</span>
    <span class="s">'{}/saved_model/1'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span>
    <span class="n">spark</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxSentenceLength</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

<span class="c1"># Optionally the classifier can be saved to load it more conveniently into Spark NLP
</span><span class="n">tokenClassifier</span><span class="p">.</span><span class="n">write</span><span class="p">().</span><span class="n">overwrite</span><span class="p">().</span><span class="n">save</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>\
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span>\
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"¡La película fue genial!"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------+</span>
<span class="o">|</span><span class="n">result</span><span class="o">|</span>
<span class="o">+------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">POS</span><span class="p">]</span> <span class="o">|</span>
<span class="o">+------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// The model needs to be trained with the transformers library.</span>
<span class="c1">// Afterwards it can be loaded into the scala version of Spark NLP.</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">ModelName</span> <span class="k">=</span> <span class="s">"finiteautomata/beto-sentiment-analysis"</span>
<span class="k">val</span> <span class="nv">tokenClassifier</span> <span class="k">=</span> <span class="nc">BertForSequenceClassification</span>
  <span class="o">.</span><span class="py">loadSavedModel</span><span class="o">(</span><span class="n">s</span><span class="s">"$ModelName/saved_model/1"</span><span class="o">,</span> <span class="n">spark</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxSentenceLength</span><span class="o">(</span><span class="mi">128</span><span class="o">)</span>

<span class="c1">// Optionally the classifier can be saved to load it more conveniently into Spark NLP</span>
<span class="nv">tokenClassifier</span><span class="o">.</span><span class="py">write</span><span class="o">.</span><span class="py">overwrite</span><span class="o">().</span><span class="py">save</span><span class="o">(</span><span class="n">s</span><span class="s">"${ModelName}_spark_nlp"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenClassifierLoaded</span> <span class="k">=</span> <span class="nv">BertForSequenceClassification</span><span class="o">.</span><span class="py">load</span><span class="o">(</span><span class="n">s</span><span class="s">"${ModelName}_spark_nlp"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifierLoaded</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"¡La película fue genial!"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="n">truncate</span> <span class="k">=</span> <span class="kc">false</span><span class="o">)</span>
<span class="o">+------+</span>
<span class="o">|</span><span class="n">result</span><span class="o">|</span>
<span class="o">+------+</span>
<span class="o">|[</span><span class="kt">POS</span><span class="o">]</span> <span class="o">|</span>
<span class="o">+------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="bertfortokenclassification">BertForTokenClassification</h3>

    <p>BertForTokenClassification can load Bert Models with a token classification head
on top (a linear layer on top of the hidden-states output) e.g. for
Named-Entity-Recognition (NER) tasks.</p>

    <p>Since Spark NLP 3.2.x, this annotator needs to be trained externally using the
transformers library. After the training process is done, the model checkpoint
can be loaded by this annotator. This is done with <code class="language-plaintext highlighter-rouge">loadSavedModel</code> (for loading
the transformers model) and <code class="language-plaintext highlighter-rouge">load</code> for the saved Spark NLP model.</p>

    <p>For an extended example see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BertForTokenClassification.ipynb">Examples</a>.</p>

    <p>Example for loading a saved transformers model:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Installing prerequisites
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">transformers</span><span class="o">==</span><span class="mf">4.8</span><span class="p">.</span><span class="mi">1</span> <span class="n">tensorflow</span><span class="o">==</span><span class="mf">2.4</span><span class="p">.</span><span class="mi">1</span> <span class="n">spark</span><span class="o">-</span><span class="n">nlp</span><span class="o">&gt;=</span><span class="mf">3.2</span><span class="p">.</span><span class="mi">0</span>

<span class="c1"># Loading the external transformers model
</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFBertForTokenClassification</span><span class="p">,</span> <span class="n">BertTokenizer</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'dslim/bert-base-NER'</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">'./{}_tokenizer/'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TFBertForTokenClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">"./{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span> <span class="n">saved_model</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Extracting the tokenizer resources
</span><span class="n">asset_path</span> <span class="o">=</span> <span class="s">'{}/saved_model/1/assets'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

<span class="err">!</span><span class="n">cp</span> <span class="p">{</span><span class="n">MODEL_NAME</span><span class="p">}</span><span class="n">_tokenizer</span><span class="o">/</span><span class="n">vocab</span><span class="p">.</span><span class="n">txt</span> <span class="p">{</span><span class="n">asset_path</span><span class="p">}</span>

<span class="c1"># Get label2id dictionary
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">label2id</span>
<span class="c1"># Sort the dictionary based on the id
</span><span class="n">labels</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">labels</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">asset_path</span><span class="o">+</span><span class="s">'/labels.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
</code></pre></div>    </div>

    <p>Then the model can be loaded and used into Spark NLP in the following examples:</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/classifier_dl/bert_for_token_classification/index.html#sparknlp.annotator.classifier_dl.bert_for_token_classification.BertForTokenClassification">BertForTokenClassification</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/classifier/dl/BertForTokenClassification">BertForTokenClassification</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/BertForTokenClassification.scala">BertForTokenClassification</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'dslim/bert-base-NER'</span>
<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">BertForTokenClassification</span><span class="p">.</span><span class="n">loadSavedModel</span><span class="p">(</span>
    <span class="s">'{}/saved_model/1'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span>
    <span class="n">spark</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxSentenceLength</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

<span class="c1"># Optionally the classifier can be saved to load it more conveniently into Spark NLP
</span><span class="n">tokenClassifier</span><span class="p">.</span><span class="n">write</span><span class="p">().</span><span class="n">overwrite</span><span class="p">().</span><span class="n">save</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">BertForTokenClassification</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>\
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span>\
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// The model needs to be trained with the transformers library.</span>
<span class="c1">// Afterwards it can be loaded into the scala version of Spark NLP.</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">modelName</span> <span class="k">=</span> <span class="s">"dslim/bert-base-NER"</span>
<span class="k">var</span> <span class="n">tokenClassifier</span> <span class="k">=</span> <span class="nv">BertForTokenClassification</span><span class="o">.</span><span class="py">loadSavedModel</span><span class="o">(</span><span class="n">s</span><span class="s">"$modelName/saved_model/1"</span><span class="o">,</span> <span class="n">spark</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxSentenceLength</span><span class="o">(</span><span class="mi">128</span><span class="o">)</span>

<span class="c1">// Optionally the classifier can be saved to load it more conveniently into Spark NLP</span>
<span class="nv">tokenClassifier</span><span class="o">.</span><span class="py">write</span><span class="o">.</span><span class="py">overwrite</span><span class="o">.</span><span class="py">save</span><span class="o">(</span><span class="n">s</span><span class="s">"${modelName}_spark_nlp"</span><span class="o">)</span>

<span class="n">tokenClassifier</span> <span class="k">=</span> <span class="nv">BertForTokenClassification</span><span class="o">.</span><span class="py">load</span><span class="o">(</span><span class="n">s</span><span class="s">"${modelName}_spark_nlp"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"label.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">B-PER</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span><span class="o">]|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="distilbertforsequenceclassification">DistilBertForSequenceClassification</h3>

    <p>DistilBertForSequenceClassification can load Bert Models with sequence
classification/regression head on top (a linear layer on top of the pooled
output) e.g. for multi-class document classification tasks.</p>

    <p>Since Spark NLP 3.2.x, this annotator needs to be trained externally using the
transformers library. After the training process is done, the model checkpoint
can be loaded by this annotator. This is done with <code class="language-plaintext highlighter-rouge">loadSavedModel</code> (for loading
the transformers model) and <code class="language-plaintext highlighter-rouge">load</code> for the saved Spark NLP model.</p>

    <p>For an extended example see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/HuggingFace%20in%20Spark%20NLP%20-%20DistilBertForSequenceClassification.ipynb">Examples</a>.</p>

    <p>Example for loading a saved transformers model:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Installing prerequisites
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">transformers</span><span class="o">==</span><span class="mf">4.8</span><span class="p">.</span><span class="mi">1</span> <span class="n">tensorflow</span><span class="o">==</span><span class="mf">2.4</span><span class="p">.</span><span class="mi">1</span>

<span class="c1"># Loading the external transformers model
</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFDistilBertForSequenceClassification</span><span class="p">,</span> <span class="n">DistilBertTokenizer</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'distilbert-base-uncased-finetuned-sst-2-english'</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">'./{}_tokenizer/'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForSequenceClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForSequenceClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">"./{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span> <span class="n">saved_model</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Extracting the tokenizer resources
</span><span class="n">asset_path</span> <span class="o">=</span> <span class="s">'{}/saved_model/1/assets'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

<span class="err">!</span><span class="n">cp</span> <span class="p">{</span><span class="n">MODEL_NAME</span><span class="p">}</span><span class="n">_tokenizer</span><span class="o">/</span><span class="n">vocab</span><span class="p">.</span><span class="n">txt</span> <span class="p">{</span><span class="n">asset_path</span><span class="p">}</span>

<span class="c1"># Get label2id dictionary
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">label2id</span>
<span class="c1"># Sort the dictionary based on the id
</span><span class="n">labels</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">labels</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">asset_path</span><span class="o">+</span><span class="s">'/labels.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
</code></pre></div>    </div>

    <p>Then the model can be loaded and used into Spark NLP in the following examples:</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/classifier_dl/distil_bert_for_sequence_classification/index.html#sparknlp.annotator.classifier_dl.distil_bert_for_sequence_classification.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/classifier/dl/DistilBertForSequenceClassification">DistilBertForSequenceClassification</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/DistilBertForSequenceClassification.scala">DistilBertForSequenceClassification</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'distilbert-base-uncased-finetuned-sst-2-english'</span>
<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">DistilBertForSequenceClassification</span><span class="p">.</span><span class="n">loadSavedModel</span><span class="p">(</span>
    <span class="s">'{}/saved_model/1'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span>
    <span class="n">spark</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxSentenceLength</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

<span class="c1"># Optionally the classifier can be saved to load it more conveniently into Spark NLP
</span><span class="n">tokenClassifier</span><span class="p">.</span><span class="n">write</span><span class="p">().</span><span class="n">overwrite</span><span class="p">().</span><span class="n">save</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">DistilBertForSequenceClassification</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>\
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span>\
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"The movie was great!"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+----------+</span>
<span class="o">|</span><span class="n">result</span>    <span class="o">|</span>
<span class="o">+----------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">POSITIVE</span><span class="p">]</span><span class="o">|</span>
<span class="o">+----------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// The model needs to be trained with the transformers library.</span>
<span class="c1">// Afterwards it can be loaded into the scala version of Spark NLP.</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">ModelName</span> <span class="k">=</span> <span class="s">"distilbert-base-uncased-finetuned-sst-2-english"</span>
<span class="k">val</span> <span class="nv">tokenClassifier</span> <span class="k">=</span> <span class="nc">DistilBertForSequenceClassification</span>
  <span class="o">.</span><span class="py">loadSavedModel</span><span class="o">(</span><span class="n">s</span><span class="s">"$ModelName/saved_model/1"</span><span class="o">,</span> <span class="n">spark</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxSentenceLength</span><span class="o">(</span><span class="mi">128</span><span class="o">)</span>

<span class="c1">// Optionally the classifier can be saved to load it more conveniently into Spark NLP</span>
<span class="nv">tokenClassifier</span><span class="o">.</span><span class="py">write</span><span class="o">.</span><span class="py">overwrite</span><span class="o">().</span><span class="py">save</span><span class="o">(</span><span class="n">s</span><span class="s">"${ModelName}_spark_nlp"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenClassifierLoaded</span> <span class="k">=</span> <span class="nv">DistilBertForSequenceClassification</span><span class="o">.</span><span class="py">load</span><span class="o">(</span><span class="n">s</span><span class="s">"${ModelName}_spark_nlp"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifierLoaded</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"The movie was great!"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="n">truncate</span> <span class="k">=</span> <span class="kc">false</span><span class="o">)</span>
<span class="o">+----------+</span>
<span class="o">|</span><span class="n">result</span>    <span class="o">|</span>
<span class="o">+----------+</span>
<span class="o">|[</span><span class="kt">POSITIVE</span><span class="o">]|</span>
<span class="o">+----------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="distilbertfortokenclassification">DistilBertForTokenClassification</h3>

    <p>DistilBertForTokenClassification can load Bert Models with a token classification head on top (a linear layer on top of the hidden-states output)
e.g. for Named-Entity-Recognition (NER) tasks.</p>

    <p>Since Spark NLP 3.2.x, this annotator needs to be trained externally using the
transformers library. After the training process is done, the model checkpoint
can be loaded by this annotator. This is done with <code class="language-plaintext highlighter-rouge">loadSavedModel</code> (for loading
the transformers model) and <code class="language-plaintext highlighter-rouge">load</code> for the saved Spark NLP model.</p>

    <p>For an extended example see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/HuggingFace%20in%20Spark%20NLP%20-%20DistilBertForTokenClassification.ipynb">Examples</a>.</p>

    <p>Example for loading a saved transformers model:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Installing prerequisites
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">transformers</span><span class="o">==</span><span class="mf">4.8</span><span class="p">.</span><span class="mi">1</span> <span class="n">tensorflow</span><span class="o">==</span><span class="mf">2.4</span><span class="p">.</span><span class="mi">1</span> <span class="n">spark</span><span class="o">-</span><span class="n">nlp</span><span class="o">&gt;=</span><span class="mf">3.2</span><span class="p">.</span><span class="mi">0</span>

<span class="c1"># Loading the external transformers model
</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFDistilBertForTokenClassification</span><span class="p">,</span> <span class="n">DistilBertTokenizer</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'elastic/distilbert-base-cased-finetuned-conll03-english'</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">'./{}_tokenizer/'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="c1"># let's add from_pt=True since there is no TF weights available for this model
# from_pt=True will convert the pytorch model to tf model
</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForTokenClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">"./{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span> <span class="n">saved_model</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Extracting the tokenizer resources
</span><span class="n">asset_path</span> <span class="o">=</span> <span class="s">'{}/saved_model/1/assets'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

<span class="err">!</span><span class="n">cp</span> <span class="p">{</span><span class="n">MODEL_NAME</span><span class="p">}</span><span class="n">_tokenizer</span><span class="o">/</span><span class="n">vocab</span><span class="p">.</span><span class="n">txt</span> <span class="p">{</span><span class="n">asset_path</span><span class="p">}</span>

<span class="c1"># Get label2id dictionary
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">label2id</span>
<span class="c1"># Sort the dictionary based on the id
</span><span class="n">labels</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">labels</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">asset_path</span><span class="o">+</span><span class="s">'/labels.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
</code></pre></div>    </div>

    <p>Then the model can be loaded and used into Spark NLP in the following examples:</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/classifier_dl/distil_bert_for_token_classification/index.html#sparknlp.annotator.classifier_dl.distil_bert_for_token_classification.DistilBertForTokenClassification">DistilBertForTokenClassification</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/classifier/dl/DistilBertForTokenClassification">DistilBertForTokenClassification</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/DistilBertForTokenClassification.scala">DistilBertForTokenClassification</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">sparknlp</span><span class="p">.</span><span class="n">start</span><span class="p">()</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'elastic/distilbert-base-cased-finetuned-conll03-english'</span>
<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">DistilBertForTokenClassification</span><span class="p">.</span><span class="n">loadSavedModel</span><span class="p">(</span>
    <span class="s">'{}/saved_model/1'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span>
    <span class="n">spark</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxSentenceLength</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

<span class="c1"># Optionally the classifier can be saved to load it more conveniently into Spark NLP
</span><span class="n">tokenClassifier</span><span class="p">.</span><span class="n">write</span><span class="p">().</span><span class="n">overwrite</span><span class="p">().</span><span class="n">save</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">DistilBertForTokenClassification</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>\
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span>\
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// The model needs to be trained with the transformers library.</span>
<span class="c1">// Afterwards it can be loaded into the scala version of Spark NLP.</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">modelName</span> <span class="k">=</span> <span class="s">"elastic/distilbert-base-cased-finetuned-conll03-english"</span>
<span class="k">var</span> <span class="n">tokenClassifier</span> <span class="k">=</span> <span class="nv">DistilBertForTokenClassification</span><span class="o">.</span><span class="py">loadSavedModel</span><span class="o">(</span><span class="n">s</span><span class="s">"$modelName/saved_model/1"</span><span class="o">,</span> <span class="n">spark</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxSentenceLength</span><span class="o">(</span><span class="mi">128</span><span class="o">)</span>

<span class="c1">// Optionally the classifier can be saved to load it more conveniently into Spark NLP</span>
<span class="nv">tokenClassifier</span><span class="o">.</span><span class="py">write</span><span class="o">.</span><span class="py">overwrite</span><span class="o">.</span><span class="py">save</span><span class="o">(</span><span class="n">s</span><span class="s">"${modelName}_spark_nlp"</span><span class="o">)</span>

<span class="n">tokenClassifier</span> <span class="k">=</span> <span class="nv">DistilBertForTokenClassification</span><span class="o">.</span><span class="py">load</span><span class="o">(</span><span class="n">s</span><span class="s">"${modelName}_spark_nlp"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"label.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">B-PER</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span><span class="o">]|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="robertafortokenclassification">RoBertaForTokenClassification</h3>

    <p>RoBertaForTokenClassification can load RoBERTa Models with a token
classification head on top (a linear layer on top of the hidden-states output)
e.g. for Named-Entity-Recognition (NER) tasks.</p>

    <p>Since Spark NLP 3.2.x, this annotator needs to be trained externally using the
transformers library. After the training process is done, the model checkpoint
can be loaded by this annotator. This is done with <code class="language-plaintext highlighter-rouge">loadSavedModel</code> (for loading
the transformers model) and <code class="language-plaintext highlighter-rouge">load</code> for the saved Spark NLP model.</p>

    <p>For an extended example see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/HuggingFace%20in%20Spark%20NLP%20-%20RoBertaForTokenClassification.ipynb">Examples</a>.</p>

    <p>Example for loading a saved transformers model:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Installing prerequisites
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">transformers</span><span class="o">==</span><span class="mf">4.10</span><span class="p">.</span><span class="mi">0</span> <span class="n">tensorflow</span><span class="o">==</span><span class="mf">2.4</span><span class="p">.</span><span class="mi">1</span>

<span class="c1"># Loading the external transformers model
</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFRobertaForTokenClassification</span><span class="p">,</span> <span class="n">RobertaTokenizer</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'philschmid/distilroberta-base-ner-wikiann-conll2003-3-class'</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">RobertaTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">'./{}_tokenizer/'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="c1"># just in case if there is no TF/Keras file provided in the model
# we can just use `from_pt` and convert PyTorch to TensorFlow
</span><span class="k">try</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'try downloading TF weights'</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">TFRobertaForTokenClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'try downloading PyTorch weights'</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">TFRobertaForTokenClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">"./{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span> <span class="n">saved_model</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># get assets
</span><span class="n">asset_path</span> <span class="o">=</span> <span class="s">'{}/saved_model/1/assets'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

<span class="c1"># let's save the vocab as txt file
</span><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'{}_tokenizer/vocab.txt'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">get_vocab</span><span class="p">().</span><span class="n">keys</span><span class="p">():</span>
        <span class="n">f</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">"%s</span><span class="se">\n</span><span class="s">"</span> <span class="o">%</span> <span class="n">item</span><span class="p">)</span>

<span class="c1"># let's copy both vocab.txt and merges.txt files to saved_model/1/assets
</span><span class="err">!</span><span class="n">cp</span> <span class="p">{</span><span class="n">MODEL_NAME</span><span class="p">}</span><span class="n">_tokenizer</span><span class="o">/</span><span class="n">vocab</span><span class="p">.</span><span class="n">txt</span> <span class="p">{</span><span class="n">asset_path</span><span class="p">}</span>
<span class="err">!</span><span class="n">cp</span> <span class="p">{</span><span class="n">MODEL_NAME</span><span class="p">}</span><span class="n">_tokenizer</span><span class="o">/</span><span class="n">merges</span><span class="p">.</span><span class="n">txt</span> <span class="p">{</span><span class="n">asset_path</span><span class="p">}</span>

<span class="c1"># get label2id dictionary
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">label2id</span>
<span class="c1"># sort the dictionary based on the id
</span><span class="n">labels</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">labels</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">asset_path</span><span class="o">+</span><span class="s">'/labels.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
</code></pre></div>    </div>

    <p>Then the model can be loaded and used into Spark NLP in the following examples:</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/classifier_dl/roberta_for_token_classification/index.html#sparknlp.annotator.classifier_dl.roberta_for_token_classification.RoBertaForTokenClassification">RoBertaForTokenClassification</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/classifier/dl/RoBertaForTokenClassification">RoBertaForTokenClassification</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/RoBertaForTokenClassification.scala">RoBertaForTokenClassification</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'philschmid/distilroberta-base-ner-wikiann-conll2003-3-class'</span>
<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">RoBertaForTokenClassification</span><span class="p">.</span><span class="n">loadSavedModel</span><span class="p">(</span>
    <span class="s">'{}/saved_model/1'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span>
    <span class="n">spark</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxSentenceLength</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

<span class="c1"># Optionally the classifier can be saved to load it more conveniently into Spark NLP
</span><span class="n">tokenClassifier</span><span class="p">.</span><span class="n">write</span><span class="p">().</span><span class="n">overwrite</span><span class="p">().</span><span class="n">save</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">RoBertaForTokenClassification</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>\
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span>\
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// The model needs to be trained with the transformers library.</span>
<span class="c1">// Afterwards it can be loaded into the scala version of Spark NLP.</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">modelName</span> <span class="k">=</span> <span class="s">"philschmid/distilroberta-base-ner-wikiann-conll2003-3-class"</span>
<span class="k">var</span> <span class="n">tokenClassifier</span> <span class="k">=</span> <span class="nv">RoBertaForTokenClassification</span><span class="o">.</span><span class="py">loadSavedModel</span><span class="o">(</span><span class="n">s</span><span class="s">"$modelName/saved_model/1"</span><span class="o">,</span> <span class="n">spark</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxSentenceLength</span><span class="o">(</span><span class="mi">128</span><span class="o">)</span>

<span class="c1">// Optionally the classifier can be saved to load it more conveniently into Spark NLP</span>
<span class="nv">tokenClassifier</span><span class="o">.</span><span class="py">write</span><span class="o">.</span><span class="py">overwrite</span><span class="o">.</span><span class="py">save</span><span class="o">(</span><span class="n">s</span><span class="s">"${modelName}_spark_nlp"</span><span class="o">)</span>

<span class="n">tokenClassifier</span> <span class="k">=</span> <span class="nv">RoBertaForTokenClassification</span><span class="o">.</span><span class="py">load</span><span class="o">(</span><span class="n">s</span><span class="s">"${modelName}_spark_nlp"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"label.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">B-PER</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span><span class="o">]|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

  <div class="h3-box tabs-python-scala-box">

    <h3 id="xlmrobertafortokenclassification">XlmRoBertaForTokenClassification</h3>

    <p>XlmRoBertaForTokenClassification can load XLM-RoBERTa Models with a token
classification head on top (a linear layer on top of the hidden-states output)
e.g. for Named-Entity-Recognition (NER) tasks.</p>

    <p>Since Spark NLP 3.2.x, this annotator needs to be trained externally using the
transformers library. After the training process is done, the model checkpoint
can be loaded by this annotator. This is done with <code class="language-plaintext highlighter-rouge">loadSavedModel</code> (for loading
the transformers model) and <code class="language-plaintext highlighter-rouge">load</code> for the saved Spark NLP model.</p>

    <p>For an extended example see the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/examples/python/transformers/HuggingFace%20in%20Spark%20NLP%20-%20XlmRoBertaForTokenClassification.ipynb">Examples</a>.</p>

    <p>Example for loading a saved transformers model:</p>
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Installing prerequisites
</span><span class="err">!</span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="n">transformers</span><span class="o">==</span><span class="mf">4.10</span><span class="p">.</span><span class="mi">0</span> <span class="n">tensorflow</span><span class="o">==</span><span class="mf">2.4</span><span class="p">.</span><span class="mi">1</span>

<span class="c1"># Loading the external transformers model
</span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFXLMRobertaForTokenClassification</span><span class="p">,</span> <span class="n">XLMRobertaTokenizer</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'wpnbos/xlm-roberta-base-conll2002-dutch'</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">XLMRobertaTokenizer</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">'./{}_tokenizer/'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="c1"># just in case if there is no TF/Keras file provided in the model
# we can just use `from_pt` and convert PyTorch to TensorFlow
</span><span class="k">try</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'try downloading TF weights'</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">TFXLMRobertaForTokenClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>
<span class="k">except</span><span class="p">:</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'try downloading PyTorch weights'</span><span class="p">)</span>
  <span class="n">model</span> <span class="o">=</span> <span class="n">TFXLMRobertaForTokenClassification</span><span class="p">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">,</span> <span class="n">from_pt</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s">"./{}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span> <span class="n">saved_model</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># get assets
</span><span class="n">asset_path</span> <span class="o">=</span> <span class="s">'{}/saved_model/1/assets'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

<span class="c1"># let's copy sentencepiece.bpe.model file to saved_model/1/assets
</span><span class="err">!</span><span class="n">cp</span> <span class="p">{</span><span class="n">MODEL_NAME</span><span class="p">}</span><span class="n">_tokenizer</span><span class="o">/</span><span class="n">sentencepiece</span><span class="p">.</span><span class="n">bpe</span><span class="p">.</span><span class="n">model</span> <span class="p">{</span><span class="n">asset_path</span><span class="p">}</span>

<span class="c1"># get label2id dictionary
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">label2id</span>
<span class="c1"># sort the dictionary based on the id
</span><span class="n">labels</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">labels</span><span class="p">.</span><span class="n">get</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">asset_path</span><span class="o">+</span><span class="s">'/labels.txt'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">f</span><span class="p">.</span><span class="n">write</span><span class="p">(</span><span class="s">'</span><span class="se">\n</span><span class="s">'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>
</code></pre></div>    </div>

    <p>Then the model can be loaded and used into Spark NLP in the following examples:</p>

    <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

    <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

    <table>
      <tbody>
        <tr>
          <td><strong>Python API:</strong> <a href="/api/python/reference/autosummary/sparknlp/annotator/classifier_dl/xlm_roberta_for_token_classification/index.html#sparknlp.annotator.classifier_dl.xlm_roberta_for_token_classification.XlmRoBertaForTokenClassification">XlmRoBertaForTokenClassification</a></td>
          <td><strong>Scala API:</strong> <a href="/api/com/johnsnowlabs/nlp/annotators/classifier/dl/XlmRoBertaForTokenClassification">XlmRoBertaForTokenClassification</a></td>
          <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/XlmRoBertaForTokenClassification.scala">XlmRoBertaForTokenClassification</a></td>
        </tr>
      </tbody>
    </table>

    <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box tabs-new">

        <div class="tabs-python-scala-head"><button class="tab-python-scala-li tabheader_active">Python</button><button class="tab-python-scala-li">Scala</button></div>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s">'wpnbos/xlm-roberta-base-conll2002-dutch'</span>
<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">XlmRoBertaForTokenClassification</span><span class="p">.</span><span class="n">loadSavedModel</span><span class="p">(</span>
    <span class="s">'{}/saved_model/1'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">),</span>
    <span class="n">spark</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxSentenceLength</span><span class="p">(</span><span class="mi">128</span><span class="p">)</span>

<span class="c1"># Optionally the classifier can be saved to load it more conveniently into Spark NLP
</span><span class="n">tokenClassifier</span><span class="p">.</span><span class="n">write</span><span class="p">().</span><span class="n">overwrite</span><span class="p">().</span><span class="n">save</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">XlmRoBertaForTokenClassification</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">"./{}_spark_nlp"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">))</span>\
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span><span class="s">'token'</span><span class="p">])</span>\
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"John Lenon is geboren in Londen en heeft in Parijs gewoond. Mijn naam is Sarah en ik woon in Londen"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                                                                                                                                       <span class="o">|</span>
<span class="o">+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">LABEL_1</span><span class="p">,</span> <span class="n">LABEL_2</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_5</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_5</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_1</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_0</span><span class="p">,</span> <span class="n">LABEL_5</span><span class="p">]</span><span class="o">|</span>
<span class="o">+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// The model needs to be trained with the transformers library.</span>
<span class="c1">// Afterwards it can be loaded into the scala version of Spark NLP.</span>

<span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">modelName</span> <span class="k">=</span> <span class="s">"wpnbos/xlm-roberta-base-conll2002-dutch"</span>
<span class="k">var</span> <span class="n">tokenClassifier</span> <span class="k">=</span> <span class="nv">XlmRoBertaForTokenClassification</span><span class="o">.</span><span class="py">loadSavedModel</span><span class="o">(</span><span class="n">s</span><span class="s">"$modelName/saved_model/1"</span><span class="o">,</span> <span class="n">spark</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxSentenceLength</span><span class="o">(</span><span class="mi">128</span><span class="o">)</span>

<span class="c1">// Optionally the classifier can be saved to load it more conveniently into Spark NLP</span>
<span class="nv">tokenClassifier</span><span class="o">.</span><span class="py">write</span><span class="o">.</span><span class="py">overwrite</span><span class="o">.</span><span class="py">save</span><span class="o">(</span><span class="n">s</span><span class="s">"${modelName}_spark_nlp"</span><span class="o">)</span>

<span class="n">tokenClassifier</span> <span class="k">=</span> <span class="nv">XlmRoBertaForTokenClassification</span><span class="o">.</span><span class="py">load</span><span class="o">(</span><span class="n">s</span><span class="s">"${modelName}_spark_nlp"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"label.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                                                                                                                                       <span class="o">|</span>
<span class="o">+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">LABEL_1</span>, <span class="kt">LABEL_2</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_5</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_5</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_1</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_0</span>, <span class="kt">LABEL_5</span><span class="o">]|</span>
<span class="o">+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

</details>

  </div>

</div>
<div class="h3-box">

  <h2 id="tensorflow-graphs">TensorFlow Graphs</h2>
  <p>NER DL uses Char CNNs - BiLSTM - CRF Neural Network architecture. Spark NLP defines this architecture through a Tensorflow graph, which requires the following parameters:</p>

  <ul>
    <li>Tags</li>
    <li>Embeddings Dimension</li>
    <li>Number of Chars</li>
  </ul>

  <p>Spark NLP infers these values from the training dataset used in <a href="annotators.md#ner-dl">NerDLApproach annotator</a> and tries to load the graph embedded on spark-nlp package.
Currently, Spark NLP has graphs for the most common combination of tags, embeddings, and number of chars values:</p>

  <table class="table-model-big w7">
    <thead>
      <tr>
        <th style="text-align: left">Tags</th>
        <th style="text-align: center">Embeddings Dimension</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left">10</td>
        <td style="text-align: center">100</td>
      </tr>
      <tr>
        <td style="text-align: left">10</td>
        <td style="text-align: center">200</td>
      </tr>
      <tr>
        <td style="text-align: left">10</td>
        <td style="text-align: center">300</td>
      </tr>
      <tr>
        <td style="text-align: left">10</td>
        <td style="text-align: center">768</td>
      </tr>
      <tr>
        <td style="text-align: left">10</td>
        <td style="text-align: center">1024</td>
      </tr>
      <tr>
        <td style="text-align: left">25</td>
        <td style="text-align: center">300</td>
      </tr>
    </tbody>
  </table>

  <p>All of these graphs use an LSTM of size 128 and number of chars 100</p>

  <p>In case, your train dataset has a different number of tags, embeddings dimension, number of chars and LSTM size combinations shown in the table above, <code class="language-plaintext highlighter-rouge">NerDLApproach</code> will raise an <strong>IllegalArgumentException</strong> exception during runtime with the message below:</p>

  <p><em>Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Check https://sparknlp.org/docs/en/graph for instructions to generate the required graph.</em></p>

  <p>To overcome this exception message we have to follow these steps:</p>

  <ol>
    <li>
      <p>Clone <a href="https://github.com/JohnSnowLabs/spark-nlp">spark-nlp github repo</a></p>
    </li>
    <li>
      <p>Run python file <code class="language-plaintext highlighter-rouge">create_models</code> with number of tags, embeddings dimension and number of char values mentioned on your exception message error.</p>

      <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="nb">cd </span>spark-nlp/python/tensorflow
 <span class="nb">export </span><span class="nv">PYTHONPATH</span><span class="o">=</span>lib/ner
 python create_models.py <span class="o">[</span>number_of_tags] <span class="o">[</span>embeddings_dimension] <span class="o">[</span>number_of_chars] <span class="o">[</span>output_path]
</code></pre></div>      </div>
    </li>
    <li>
      <p>This will generate a graph on the directory defined on `output_path argument.</p>
    </li>
    <li>
      <p>Retry training with <code class="language-plaintext highlighter-rouge">NerDLApproach</code> annotator but this time use the parameter <code class="language-plaintext highlighter-rouge">setGraphFolder</code> with the path of your graph.</p>
    </li>
  </ol>

  <p><strong>Note:</strong>  Make sure that you have Python 3 and Tensorflow 1.15.0 installed on your system since <code class="language-plaintext highlighter-rouge">create_models</code> requires those versions to generate the graph successfully.
<strong>Note:</strong>  We also have a notebook in the same directory if you prefer Jupyter notebook to cerate your custom graph.</p>

</div>
</div><div class="d-print-none"><footer class="article__footer"><span class="footer_date">Last updated
      <time itemprop="dateModified" datetime="2019-10-23T00:00:00+00:00">Oct 23, 2019</time>
    </span><!-- start custom article footer snippet -->

<!-- end custom article footer snippet --></footer>

<script>

'use strict';

function tabs({tabsWrapperSelector, tabsParentSelector, tabsSelector, tabsContentSelector, activeClass}) {
    //Tabs

    const tabsWrapper = document.querySelectorAll(tabsWrapperSelector);

    //Detecting all tabs
    tabsWrapper.forEach(tab => {
        const tabsParent = tab.querySelector(tabsParentSelector),
                tabsLi = tab.querySelectorAll(tabsSelector),
                tabsContent = tab.querySelectorAll(tabsContentSelector);

        //Hiding all tabs
        function hideTabsContent() {
            if(Array.from(tabsLi).length != 0) {
                tabsContent.forEach(item => {
                    item.style.display = 'none';
                }); 
            }
            
            if(Array.from(tabsLi).length != 0) {
                tabsLi.forEach(item => {
                    item.classList.remove(activeClass);
                }); 
            }
        }

        //Show active tabs
        function showTabContent(i = 0) {
            if(Array.from(tabsContent).length != 0) {
                tabsContent[i].style.display = "block";
            }
            if(Array.from(tabsLi).length != 0) {
                tabsLi[i].classList.add(activeClass);            
            }
        }

        //Changing the tabs
        if(tabsParent != null) {
            tabsParent.addEventListener('click', (event) => {
                const target = event.target;
    
                if(target && target.classList.contains(tabsSelector.slice(1))) {
                    tabsLi.forEach((item, i) => {
                        if(target == item) {
                            hideTabsContent();
                            try{showTabContent(i);}catch(e){}
                        }
                    });
                }
            });
        }        
        
        hideTabsContent();
        showTabContent();
    });
}

tabs({
    tabsWrapperSelector: '.tabs-model-aproach', 
    tabsParentSelector: '.tabs-model-aproach-head', 
    tabsSelector: '.tab-li-model-aproach', 
    tabsContentSelector: '.tabs-python-scala-box', 
    activeClass: 'tabheader_active'
});
tabs({
    tabsWrapperSelector: '.tabs-python-scala-box', 
    tabsParentSelector: '.tabs-python-scala-head', 
    tabsSelector: '.tab-python-scala-li', 
    tabsContentSelector: '.tabs-mfl-box', 
    activeClass: 'tabheader_active'
});
tabs({
    tabsWrapperSelector: '.tabs-mfl-box', 
    tabsParentSelector: '.tabs-mfl-head', 
    tabsSelector: '.tab-mfl-li', 
    tabsContentSelector: '.tab-mfl-content', 
    activeClass: 'tabheader_active'
});
tabs({
    tabsWrapperSelector: '.tabs-box', 
    tabsParentSelector: '.tabs-python-scala-head', 
    tabsSelector: '.tab-python-scala-li', 
    tabsContentSelector: '.tabs-box .highlighter-rouge', 
    activeClass: 'tabheader_active'
});
tabs({
    tabsWrapperSelector: '.tabs-box', 
    tabsParentSelector: '.tabs-model-aproach-head', 
    tabsSelector: '.tab-li-model-aproach', 
    tabsContentSelector: '.tabs-python-scala-box', 
    activeClass: 'tabheader_active'
});
tabs({
    tabsWrapperSelector: '.tabs-box', 
    tabsParentSelector: '.tabs-model-aproach-head', 
    tabsSelector: '.tab-li-model-aproach', 
    tabsContentSelector: '.tabs-box .highlighter-rouge', 
    activeClass: 'tabheader_active'
});

</script>


<style>
  /* Remove Scrollbar from Code Segments */
.article__content .highlighter-rouge > .highlight > pre > code, .article__content figure.highlight > pre > code  {
    overflow: auto;
}



button.code-selector-active {
 background-color: white;
 color: #08c;
 font-weight: bold;
 border-width: 1px;
 padding-left: 12px;
 padding-right: 12px;
 width: 90px;
 padding-top: 6px;
 margin-right: 2px;

 border-bottom: none;

 position: relative;
 z-index: 2;
}

button.code-selector-un-active {
    background-color: white;
    padding-left: 12px;
    padding-right: 12px;
    width: 90px;
    margin-right: 2px;
    padding-top: 8px;
    position: relative;
    border-bottom: none;

   }

hr.code-selector-underlie {
    border-top: 1px solid;
    background-color: black;
    width: fill;
    height: 1px;
    margin-top: -3px;
    position: relative;

}

</style><div class="article__section-navigator clearfix"><div class="previous nav_link"><span>PREVIOUS</span><a href="/docs/en/transformers">Transformers</a></div><div class="next nav_link"><span>NEXT</span><a href="/docs/en/display">Spark NLP Display</a></div></div></div>

</div>
</div>

<script>/*! jQuery v1.12.3 | (c) jQuery Foundation | jquery.org/license */
!function(a,b){"object"==typeof module&&"object"==typeof module.exports?module.exports=a.document?b(a,!0):function(a){if(!a.document)throw new Error("jQuery requires a window with a document");return b(a)}:b(a)}("undefined"!=typeof window?window:this,function(a,b){var c=[],d=a.document,e=c.slice,f=c.concat,g=c.push,h=c.indexOf,i={},j=i.toString,k=i.hasOwnProperty,l={},m="1.12.3",n=function(a,b){return new n.fn.init(a,b)},o=/^[\s\uFEFF\xA0]+|[\s\uFEFF\xA0]+$/g,p=/^-ms-/,q=/-([\da-z])/gi,r=function(a,b){return b.toUpperCase()};n.fn=n.prototype={jquery:m,constructor:n,selector:"",length:0,toArray:function(){return e.call(this)},get:function(a){return null!=a?0>a?this[a+this.length]:this[a]:e.call(this)},pushStack:function(a){var b=n.merge(this.constructor(),a);return b.prevObject=this,b.context=this.context,b},each:function(a){return n.each(this,a)},map:function(a){return this.pushStack(n.map(this,function(b,c){return a.call(b,c,b)}))},slice:function(){return this.pushStack(e.apply(this,arguments))},first:function(){return this.eq(0)},last:function(){return this.eq(-1)},eq:function(a){var b=this.length,c=+a+(0>a?b:0);return this.pushStack(c>=0&&b>c?[this[c]]:[])},end:function(){return this.prevObject||this.constructor()},push:g,sort:c.sort,splice:c.splice},n.extend=n.fn.extend=function(){var a,b,c,d,e,f,g=arguments[0]||{},h=1,i=arguments.length,j=!1;for("boolean"==typeof g&&(j=g,g=arguments[h]||{},h++),"object"==typeof g||n.isFunction(g)||(g={}),h===i&&(g=this,h--);i>h;h++)if(null!=(e=arguments[h]))for(d in e)a=g[d],c=e[d],g!==c&&(j&&c&&(n.isPlainObject(c)||(b=n.isArray(c)))?(b?(b=!1,f=a&&n.isArray(a)?a:[]):f=a&&n.isPlainObject(a)?a:{},g[d]=n.extend(j,f,c)):void 0!==c&&(g[d]=c));return g},n.extend({expando:"jQuery"+(m+Math.random()).replace(/\D/g,""),isReady:!0,error:function(a){throw new Error(a)},noop:function(){},isFunction:function(a){return"function"===n.type(a)},isArray:Array.isArray||function(a){return"array"===n.type(a)},isWindow:function(a){return null!=a&&a==a.window},isNumeric:function(a){var b=a&&a.toString();return!n.isArray(a)&&b-parseFloat(b)+1>=0},isEmptyObject:function(a){var b;for(b in a)return!1;return!0},isPlainObject:function(a){var b;if(!a||"object"!==n.type(a)||a.nodeType||n.isWindow(a))return!1;try{if(a.constructor&&!k.call(a,"constructor")&&!k.call(a.constructor.prototype,"isPrototypeOf"))return!1}catch(c){return!1}if(!l.ownFirst)for(b in a)return k.call(a,b);for(b in a);return void 0===b||k.call(a,b)},type:function(a){return null==a?a+"":"object"==typeof a||"function"==typeof a?i[j.call(a)]||"object":typeof a},globalEval:function(b){b&&n.trim(b)&&(a.execScript||function(b){a.eval.call(a,b)})(b)},camelCase:function(a){return a.replace(p,"ms-").replace(q,r)},nodeName:function(a,b){return a.nodeName&&a.nodeName.toLowerCase()===b.toLowerCase()},each:function(a,b){var c,d=0;if(s(a)){for(c=a.length;c>d;d++)if(b.call(a[d],d,a[d])===!1)break}else for(d in a)if(b.call(a[d],d,a[d])===!1)break;return a},trim:function(a){return null==a?"":(a+"").replace(o,"")},makeArray:function(a,b){var c=b||[];return null!=a&&(s(Object(a))?n.merge(c,"string"==typeof a?[a]:a):g.call(c,a)),c},inArray:function(a,b,c){var d;if(b){if(h)return h.call(b,a,c);for(d=b.length,c=c?0>c?Math.max(0,d+c):c:0;d>c;c++)if(c in b&&b[c]===a)return c}return-1},merge:function(a,b){var c=+b.length,d=0,e=a.length;while(c>d)a[e++]=b[d++];if(c!==c)while(void 0!==b[d])a[e++]=b[d++];return a.length=e,a},grep:function(a,b,c){for(var d,e=[],f=0,g=a.length,h=!c;g>f;f++)d=!b(a[f],f),d!==h&&e.push(a[f]);return e},map:function(a,b,c){var d,e,g=0,h=[];if(s(a))for(d=a.length;d>g;g++)e=b(a[g],g,c),null!=e&&h.push(e);else for(g in a)e=b(a[g],g,c),null!=e&&h.push(e);return f.apply([],h)},guid:1,proxy:function(a,b){var c,d,f;return"string"==typeof b&&(f=a[b],b=a,a=f),n.isFunction(a)?(c=e.call(arguments,2),d=function(){return a.apply(b||this,c.concat(e.call(arguments)))},d.guid=a.guid=a.guid||n.guid++,d):void 0},now:function(){return+new Date},support:l}),"function"==typeof Symbol&&(n.fn[Symbol.iterator]=c[Symbol.iterator]),n.each("Boolean Number String Function Array Date RegExp Object Error Symbol".split(" "),function(a,b){i["[object "+b+"]"]=b.toLowerCase()});function s(a){var b=!!a&&"length"in a&&a.length,c=n.type(a);return"function"===c||n.isWindow(a)?!1:"array"===c||0===b||"number"==typeof b&&b>0&&b-1 in a}var t=function(a){var b,c,d,e,f,g,h,i,j,k,l,m,n,o,p,q,r,s,t,u="sizzle"+1*new Date,v=a.document,w=0,x=0,y=ga(),z=ga(),A=ga(),B=function(a,b){return a===b&&(l=!0),0},C=1<<31,D={}.hasOwnProperty,E=[],F=E.pop,G=E.push,H=E.push,I=E.slice,J=function(a,b){for(var c=0,d=a.length;d>c;c++)if(a[c]===b)return c;return-1},K="checked|selected|async|autofocus|autoplay|controls|defer|disabled|hidden|ismap|loop|multiple|open|readonly|required|scoped",L="[\\x20\\t\\r\\n\\f]",M="(?:\\\\.|[\\w-]|[^\\x00-\\xa0])+",N="\\["+L+"*("+M+")(?:"+L+"*([*^$|!~]?=)"+L+"*(?:'((?:\\\\.|[^\\\\'])*)'|\"((?:\\\\.|[^\\\\\"])*)\"|("+M+"))|)"+L+"*\\]",O=":("+M+")(?:\\((('((?:\\\\.|[^\\\\'])*)'|\"((?:\\\\.|[^\\\\\"])*)\")|((?:\\\\.|[^\\\\()[\\]]|"+N+")*)|.*)\\)|)",P=new RegExp(L+"+","g"),Q=new RegExp("^"+L+"+|((?:^|[^\\\\])(?:\\\\.)*)"+L+"+$","g"),R=new RegExp("^"+L+"*,"+L+"*"),S=new RegExp("^"+L+"*([>+~]|"+L+")"+L+"*"),T=new RegExp("="+L+"*([^\\]'\"]*?)"+L+"*\\]","g"),U=new RegExp(O),V=new RegExp("^"+M+"$"),W={ID:new RegExp("^#("+M+")"),CLASS:new RegExp("^\\.("+M+")"),TAG:new RegExp("^("+M+"|[*])"),ATTR:new RegExp("^"+N),PSEUDO:new RegExp("^"+O),CHILD:new RegExp("^:(only|first|last|nth|nth-last)-(child|of-type)(?:\\("+L+"*(even|odd|(([+-]|)(\\d*)n|)"+L+"*(?:([+-]|)"+L+"*(\\d+)|))"+L+"*\\)|)","i"),bool:new RegExp("^(?:"+K+")$","i"),needsContext:new RegExp("^"+L+"*[>+~]|:(even|odd|eq|gt|lt|nth|first|last)(?:\\("+L+"*((?:-\\d)?\\d*)"+L+"*\\)|)(?=[^-]|$)","i")},X=/^(?:input|select|textarea|button)$/i,Y=/^h\d$/i,Z=/^[^{]+\{\s*\[native \w/,$=/^(?:#([\w-]+)|(\w+)|\.([\w-]+))$/,_=/[+~]/,aa=/'|\\/g,ba=new RegExp("\\\\([\\da-f]{1,6}"+L+"?|("+L+")|.)","ig"),ca=function(a,b,c){var d="0x"+b-65536;return d!==d||c?b:0>d?String.fromCharCode(d+65536):String.fromCharCode(d>>10|55296,1023&d|56320)},da=function(){m()};try{H.apply(E=I.call(v.childNodes),v.childNodes),E[v.childNodes.length].nodeType}catch(ea){H={apply:E.length?function(a,b){G.apply(a,I.call(b))}:function(a,b){var c=a.length,d=0;while(a[c++]=b[d++]);a.length=c-1}}}function fa(a,b,d,e){var f,h,j,k,l,o,r,s,w=b&&b.ownerDocument,x=b?b.nodeType:9;if(d=d||[],"string"!=typeof a||!a||1!==x&&9!==x&&11!==x)return d;if(!e&&((b?b.ownerDocument||b:v)!==n&&m(b),b=b||n,p)){if(11!==x&&(o=$.exec(a)))if(f=o[1]){if(9===x){if(!(j=b.getElementById(f)))return d;if(j.id===f)return d.push(j),d}else if(w&&(j=w.getElementById(f))&&t(b,j)&&j.id===f)return d.push(j),d}else{if(o[2])return H.apply(d,b.getElementsByTagName(a)),d;if((f=o[3])&&c.getElementsByClassName&&b.getElementsByClassName)return H.apply(d,b.getElementsByClassName(f)),d}if(c.qsa&&!A[a+" "]&&(!q||!q.test(a))){if(1!==x)w=b,s=a;else if("object"!==b.nodeName.toLowerCase()){(k=b.getAttribute("id"))?k=k.replace(aa,"\\$&"):b.setAttribute("id",k=u),r=g(a),h=r.length,l=V.test(k)?"#"+k:"[id='"+k+"']";while(h--)r[h]=l+" "+qa(r[h]);s=r.join(","),w=_.test(a)&&oa(b.parentNode)||b}if(s)try{return H.apply(d,w.querySelectorAll(s)),d}catch(y){}finally{k===u&&b.removeAttribute("id")}}}return i(a.replace(Q,"$1"),b,d,e)}function ga(){var a=[];function b(c,e){return a.push(c+" ")>d.cacheLength&&delete b[a.shift()],b[c+" "]=e}return b}function ha(a){return a[u]=!0,a}function ia(a){var b=n.createElement("div");try{return!!a(b)}catch(c){return!1}finally{b.parentNode&&b.parentNode.removeChild(b),b=null}}function ja(a,b){var c=a.split("|"),e=c.length;while(e--)d.attrHandle[c[e]]=b}function ka(a,b){var c=b&&a,d=c&&1===a.nodeType&&1===b.nodeType&&(~b.sourceIndex||C)-(~a.sourceIndex||C);if(d)return d;if(c)while(c=c.nextSibling)if(c===b)return-1;return a?1:-1}function la(a){return function(b){var c=b.nodeName.toLowerCase();return"input"===c&&b.type===a}}function ma(a){return function(b){var c=b.nodeName.toLowerCase();return("input"===c||"button"===c)&&b.type===a}}function na(a){return ha(function(b){return b=+b,ha(function(c,d){var e,f=a([],c.length,b),g=f.length;while(g--)c[e=f[g]]&&(c[e]=!(d[e]=c[e]))})})}function oa(a){return a&&"undefined"!=typeof a.getElementsByTagName&&a}c=fa.support={},f=fa.isXML=function(a){var b=a&&(a.ownerDocument||a).documentElement;return b?"HTML"!==b.nodeName:!1},m=fa.setDocument=function(a){var b,e,g=a?a.ownerDocument||a:v;return g!==n&&9===g.nodeType&&g.documentElement?(n=g,o=n.documentElement,p=!f(n),(e=n.defaultView)&&e.top!==e&&(e.addEventListener?e.addEventListener("unload",da,!1):e.attachEvent&&e.attachEvent("onunload",da)),c.attributes=ia(function(a){return a.className="i",!a.getAttribute("className")}),c.getElementsByTagName=ia(function(a){return a.appendChild(n.createComment("")),!a.getElementsByTagName("*").length}),c.getElementsByClassName=Z.test(n.getElementsByClassName),c.getById=ia(function(a){return o.appendChild(a).id=u,!n.getElementsByName||!n.getElementsByName(u).length}),c.getById?(d.find.ID=function(a,b){if("undefined"!=typeof b.getElementById&&p){var c=b.getElementById(a);return c?[c]:[]}},d.filter.ID=function(a){var b=a.replace(ba,ca);return function(a){return a.getAttribute("id")===b}}):(delete d.find.ID,d.filter.ID=function(a){var b=a.replace(ba,ca);return function(a){var c="undefined"!=typeof a.getAttributeNode&&a.getAttributeNode("id");return c&&c.value===b}}),d.find.TAG=c.getElementsByTagName?function(a,b){return"undefined"!=typeof b.getElementsByTagName?b.getElementsByTagName(a):c.qsa?b.querySelectorAll(a):void 0}:function(a,b){var c,d=[],e=0,f=b.getElementsByTagName(a);if("*"===a){while(c=f[e++])1===c.nodeType&&d.push(c);return d}return f},d.find.CLASS=c.getElementsByClassName&&function(a,b){return"undefined"!=typeof b.getElementsByClassName&&p?b.getElementsByClassName(a):void 0},r=[],q=[],(c.qsa=Z.test(n.querySelectorAll))&&(ia(function(a){o.appendChild(a).innerHTML="<a id='"+u+"'></a><select id='"+u+"-\r\\' msallowcapture=''><option selected=''></option></select>",a.querySelectorAll("[msallowcapture^='']").length&&q.push("[*^$]="+L+"*(?:''|\"\")"),a.querySelectorAll("[selected]").length||q.push("\\["+L+"*(?:value|"+K+")"),a.querySelectorAll("[id~="+u+"-]").length||q.push("~="),a.querySelectorAll(":checked").length||q.push(":checked"),a.querySelectorAll("a#"+u+"+*").length||q.push(".#.+[+~]")}),ia(function(a){var b=n.createElement("input");b.setAttribute("type","hidden"),a.appendChild(b).setAttribute("name","D"),a.querySelectorAll("[name=d]").length&&q.push("name"+L+"*[*^$|!~]?="),a.querySelectorAll(":enabled").length||q.push(":enabled",":disabled"),a.querySelectorAll("*,:x"),q.push(",.*:")})),(c.matchesSelector=Z.test(s=o.matches||o.webkitMatchesSelector||o.mozMatchesSelector||o.oMatchesSelector||o.msMatchesSelector))&&ia(function(a){c.disconnectedMatch=s.call(a,"div"),s.call(a,"[s!='']:x"),r.push("!=",O)}),q=q.length&&new RegExp(q.join("|")),r=r.length&&new RegExp(r.join("|")),b=Z.test(o.compareDocumentPosition),t=b||Z.test(o.contains)?function(a,b){var c=9===a.nodeType?a.documentElement:a,d=b&&b.parentNode;return a===d||!(!d||1!==d.nodeType||!(c.contains?c.contains(d):a.compareDocumentPosition&&16&a.compareDocumentPosition(d)))}:function(a,b){if(b)while(b=b.parentNode)if(b===a)return!0;return!1},B=b?function(a,b){if(a===b)return l=!0,0;var d=!a.compareDocumentPosition-!b.compareDocumentPosition;return d?d:(d=(a.ownerDocument||a)===(b.ownerDocument||b)?a.compareDocumentPosition(b):1,1&d||!c.sortDetached&&b.compareDocumentPosition(a)===d?a===n||a.ownerDocument===v&&t(v,a)?-1:b===n||b.ownerDocument===v&&t(v,b)?1:k?J(k,a)-J(k,b):0:4&d?-1:1)}:function(a,b){if(a===b)return l=!0,0;var c,d=0,e=a.parentNode,f=b.parentNode,g=[a],h=[b];if(!e||!f)return a===n?-1:b===n?1:e?-1:f?1:k?J(k,a)-J(k,b):0;if(e===f)return ka(a,b);c=a;while(c=c.parentNode)g.unshift(c);c=b;while(c=c.parentNode)h.unshift(c);while(g[d]===h[d])d++;return d?ka(g[d],h[d]):g[d]===v?-1:h[d]===v?1:0},n):n},fa.matches=function(a,b){return fa(a,null,null,b)},fa.matchesSelector=function(a,b){if((a.ownerDocument||a)!==n&&m(a),b=b.replace(T,"='$1']"),c.matchesSelector&&p&&!A[b+" "]&&(!r||!r.test(b))&&(!q||!q.test(b)))try{var d=s.call(a,b);if(d||c.disconnectedMatch||a.document&&11!==a.document.nodeType)return d}catch(e){}return fa(b,n,null,[a]).length>0},fa.contains=function(a,b){return(a.ownerDocument||a)!==n&&m(a),t(a,b)},fa.attr=function(a,b){(a.ownerDocument||a)!==n&&m(a);var e=d.attrHandle[b.toLowerCase()],f=e&&D.call(d.attrHandle,b.toLowerCase())?e(a,b,!p):void 0;return void 0!==f?f:c.attributes||!p?a.getAttribute(b):(f=a.getAttributeNode(b))&&f.specified?f.value:null},fa.error=function(a){throw new Error("Syntax error, unrecognized expression: "+a)},fa.uniqueSort=function(a){var b,d=[],e=0,f=0;if(l=!c.detectDuplicates,k=!c.sortStable&&a.slice(0),a.sort(B),l){while(b=a[f++])b===a[f]&&(e=d.push(f));while(e--)a.splice(d[e],1)}return k=null,a},e=fa.getText=function(a){var b,c="",d=0,f=a.nodeType;if(f){if(1===f||9===f||11===f){if("string"==typeof a.textContent)return a.textContent;for(a=a.firstChild;a;a=a.nextSibling)c+=e(a)}else if(3===f||4===f)return a.nodeValue}else while(b=a[d++])c+=e(b);return c},d=fa.selectors={cacheLength:50,createPseudo:ha,match:W,attrHandle:{},find:{},relative:{">":{dir:"parentNode",first:!0}," ":{dir:"parentNode"},"+":{dir:"previousSibling",first:!0},"~":{dir:"previousSibling"}},preFilter:{ATTR:function(a){return a[1]=a[1].replace(ba,ca),a[3]=(a[3]||a[4]||a[5]||"").replace(ba,ca),"~="===a[2]&&(a[3]=" "+a[3]+" "),a.slice(0,4)},CHILD:function(a){return a[1]=a[1].toLowerCase(),"nth"===a[1].slice(0,3)?(a[3]||fa.error(a[0]),a[4]=+(a[4]?a[5]+(a[6]||1):2*("even"===a[3]||"odd"===a[3])),a[5]=+(a[7]+a[8]||"odd"===a[3])):a[3]&&fa.error(a[0]),a},PSEUDO:function(a){var b,c=!a[6]&&a[2];return W.CHILD.test(a[0])?null:(a[3]?a[2]=a[4]||a[5]||"":c&&U.test(c)&&(b=g(c,!0))&&(b=c.indexOf(")",c.length-b)-c.length)&&(a[0]=a[0].slice(0,b),a[2]=c.slice(0,b)),a.slice(0,3))}},filter:{TAG:function(a){var b=a.replace(ba,ca).toLowerCase();return"*"===a?function(){return!0}:function(a){return a.nodeName&&a.nodeName.toLowerCase()===b}},CLASS:function(a){var b=y[a+" "];return b||(b=new RegExp("(^|"+L+")"+a+"("+L+"|$)"))&&y(a,function(a){return b.test("string"==typeof a.className&&a.className||"undefined"!=typeof a.getAttribute&&a.getAttribute("class")||"")})},ATTR:function(a,b,c){return function(d){var e=fa.attr(d,a);return null==e?"!="===b:b?(e+="","="===b?e===c:"!="===b?e!==c:"^="===b?c&&0===e.indexOf(c):"*="===b?c&&e.indexOf(c)>-1:"$="===b?c&&e.slice(-c.length)===c:"~="===b?(" "+e.replace(P," ")+" ").indexOf(c)>-1:"|="===b?e===c||e.slice(0,c.length+1)===c+"-":!1):!0}},CHILD:function(a,b,c,d,e){var f="nth"!==a.slice(0,3),g="last"!==a.slice(-4),h="of-type"===b;return 1===d&&0===e?function(a){return!!a.parentNode}:function(b,c,i){var j,k,l,m,n,o,p=f!==g?"nextSibling":"previousSibling",q=b.parentNode,r=h&&b.nodeName.toLowerCase(),s=!i&&!h,t=!1;if(q){if(f){while(p){m=b;while(m=m[p])if(h?m.nodeName.toLowerCase()===r:1===m.nodeType)return!1;o=p="only"===a&&!o&&"nextSibling"}return!0}if(o=[g?q.firstChild:q.lastChild],g&&s){m=q,l=m[u]||(m[u]={}),k=l[m.uniqueID]||(l[m.uniqueID]={}),j=k[a]||[],n=j[0]===w&&j[1],t=n&&j[2],m=n&&q.childNodes[n];while(m=++n&&m&&m[p]||(t=n=0)||o.pop())if(1===m.nodeType&&++t&&m===b){k[a]=[w,n,t];break}}else if(s&&(m=b,l=m[u]||(m[u]={}),k=l[m.uniqueID]||(l[m.uniqueID]={}),j=k[a]||[],n=j[0]===w&&j[1],t=n),t===!1)while(m=++n&&m&&m[p]||(t=n=0)||o.pop())if((h?m.nodeName.toLowerCase()===r:1===m.nodeType)&&++t&&(s&&(l=m[u]||(m[u]={}),k=l[m.uniqueID]||(l[m.uniqueID]={}),k[a]=[w,t]),m===b))break;return t-=e,t===d||t%d===0&&t/d>=0}}},PSEUDO:function(a,b){var c,e=d.pseudos[a]||d.setFilters[a.toLowerCase()]||fa.error("unsupported pseudo: "+a);return e[u]?e(b):e.length>1?(c=[a,a,"",b],d.setFilters.hasOwnProperty(a.toLowerCase())?ha(function(a,c){var d,f=e(a,b),g=f.length;while(g--)d=J(a,f[g]),a[d]=!(c[d]=f[g])}):function(a){return e(a,0,c)}):e}},pseudos:{not:ha(function(a){var b=[],c=[],d=h(a.replace(Q,"$1"));return d[u]?ha(function(a,b,c,e){var f,g=d(a,null,e,[]),h=a.length;while(h--)(f=g[h])&&(a[h]=!(b[h]=f))}):function(a,e,f){return b[0]=a,d(b,null,f,c),b[0]=null,!c.pop()}}),has:ha(function(a){return function(b){return fa(a,b).length>0}}),contains:ha(function(a){return a=a.replace(ba,ca),function(b){return(b.textContent||b.innerText||e(b)).indexOf(a)>-1}}),lang:ha(function(a){return V.test(a||"")||fa.error("unsupported lang: "+a),a=a.replace(ba,ca).toLowerCase(),function(b){var c;do if(c=p?b.lang:b.getAttribute("xml:lang")||b.getAttribute("lang"))return c=c.toLowerCase(),c===a||0===c.indexOf(a+"-");while((b=b.parentNode)&&1===b.nodeType);return!1}}),target:function(b){var c=a.location&&a.location.hash;return c&&c.slice(1)===b.id},root:function(a){return a===o},focus:function(a){return a===n.activeElement&&(!n.hasFocus||n.hasFocus())&&!!(a.type||a.href||~a.tabIndex)},enabled:function(a){return a.disabled===!1},disabled:function(a){return a.disabled===!0},checked:function(a){var b=a.nodeName.toLowerCase();return"input"===b&&!!a.checked||"option"===b&&!!a.selected},selected:function(a){return a.parentNode&&a.parentNode.selectedIndex,a.selected===!0},empty:function(a){for(a=a.firstChild;a;a=a.nextSibling)if(a.nodeType<6)return!1;return!0},parent:function(a){return!d.pseudos.empty(a)},header:function(a){return Y.test(a.nodeName)},input:function(a){return X.test(a.nodeName)},button:function(a){var b=a.nodeName.toLowerCase();return"input"===b&&"button"===a.type||"button"===b},text:function(a){var b;return"input"===a.nodeName.toLowerCase()&&"text"===a.type&&(null==(b=a.getAttribute("type"))||"text"===b.toLowerCase())},first:na(function(){return[0]}),last:na(function(a,b){return[b-1]}),eq:na(function(a,b,c){return[0>c?c+b:c]}),even:na(function(a,b){for(var c=0;b>c;c+=2)a.push(c);return a}),odd:na(function(a,b){for(var c=1;b>c;c+=2)a.push(c);return a}),lt:na(function(a,b,c){for(var d=0>c?c+b:c;--d>=0;)a.push(d);return a}),gt:na(function(a,b,c){for(var d=0>c?c+b:c;++d<b;)a.push(d);return a})}},d.pseudos.nth=d.pseudos.eq;for(b in{radio:!0,checkbox:!0,file:!0,password:!0,image:!0})d.pseudos[b]=la(b);for(b in{submit:!0,reset:!0})d.pseudos[b]=ma(b);function pa(){}pa.prototype=d.filters=d.pseudos,d.setFilters=new pa,g=fa.tokenize=function(a,b){var c,e,f,g,h,i,j,k=z[a+" "];if(k)return b?0:k.slice(0);h=a,i=[],j=d.preFilter;while(h){c&&!(e=R.exec(h))||(e&&(h=h.slice(e[0].length)||h),i.push(f=[])),c=!1,(e=S.exec(h))&&(c=e.shift(),f.push({value:c,type:e[0].replace(Q," ")}),h=h.slice(c.length));for(g in d.filter)!(e=W[g].exec(h))||j[g]&&!(e=j[g](e))||(c=e.shift(),f.push({value:c,type:g,matches:e}),h=h.slice(c.length));if(!c)break}return b?h.length:h?fa.error(a):z(a,i).slice(0)};function qa(a){for(var b=0,c=a.length,d="";c>b;b++)d+=a[b].value;return d}function ra(a,b,c){var d=b.dir,e=c&&"parentNode"===d,f=x++;return b.first?function(b,c,f){while(b=b[d])if(1===b.nodeType||e)return a(b,c,f)}:function(b,c,g){var h,i,j,k=[w,f];if(g){while(b=b[d])if((1===b.nodeType||e)&&a(b,c,g))return!0}else while(b=b[d])if(1===b.nodeType||e){if(j=b[u]||(b[u]={}),i=j[b.uniqueID]||(j[b.uniqueID]={}),(h=i[d])&&h[0]===w&&h[1]===f)return k[2]=h[2];if(i[d]=k,k[2]=a(b,c,g))return!0}}}function sa(a){return a.length>1?function(b,c,d){var e=a.length;while(e--)if(!a[e](b,c,d))return!1;return!0}:a[0]}function ta(a,b,c){for(var d=0,e=b.length;e>d;d++)fa(a,b[d],c);return c}function ua(a,b,c,d,e){for(var f,g=[],h=0,i=a.length,j=null!=b;i>h;h++)(f=a[h])&&(c&&!c(f,d,e)||(g.push(f),j&&b.push(h)));return g}function va(a,b,c,d,e,f){return d&&!d[u]&&(d=va(d)),e&&!e[u]&&(e=va(e,f)),ha(function(f,g,h,i){var j,k,l,m=[],n=[],o=g.length,p=f||ta(b||"*",h.nodeType?[h]:h,[]),q=!a||!f&&b?p:ua(p,m,a,h,i),r=c?e||(f?a:o||d)?[]:g:q;if(c&&c(q,r,h,i),d){j=ua(r,n),d(j,[],h,i),k=j.length;while(k--)(l=j[k])&&(r[n[k]]=!(q[n[k]]=l))}if(f){if(e||a){if(e){j=[],k=r.length;while(k--)(l=r[k])&&j.push(q[k]=l);e(null,r=[],j,i)}k=r.length;while(k--)(l=r[k])&&(j=e?J(f,l):m[k])>-1&&(f[j]=!(g[j]=l))}}else r=ua(r===g?r.splice(o,r.length):r),e?e(null,g,r,i):H.apply(g,r)})}function wa(a){for(var b,c,e,f=a.length,g=d.relative[a[0].type],h=g||d.relative[" "],i=g?1:0,k=ra(function(a){return a===b},h,!0),l=ra(function(a){return J(b,a)>-1},h,!0),m=[function(a,c,d){var e=!g&&(d||c!==j)||((b=c).nodeType?k(a,c,d):l(a,c,d));return b=null,e}];f>i;i++)if(c=d.relative[a[i].type])m=[ra(sa(m),c)];else{if(c=d.filter[a[i].type].apply(null,a[i].matches),c[u]){for(e=++i;f>e;e++)if(d.relative[a[e].type])break;return va(i>1&&sa(m),i>1&&qa(a.slice(0,i-1).concat({value:" "===a[i-2].type?"*":""})).replace(Q,"$1"),c,e>i&&wa(a.slice(i,e)),f>e&&wa(a=a.slice(e)),f>e&&qa(a))}m.push(c)}return sa(m)}function xa(a,b){var c=b.length>0,e=a.length>0,f=function(f,g,h,i,k){var l,o,q,r=0,s="0",t=f&&[],u=[],v=j,x=f||e&&d.find.TAG("*",k),y=w+=null==v?1:Math.random()||.1,z=x.length;for(k&&(j=g===n||g||k);s!==z&&null!=(l=x[s]);s++){if(e&&l){o=0,g||l.ownerDocument===n||(m(l),h=!p);while(q=a[o++])if(q(l,g||n,h)){i.push(l);break}k&&(w=y)}c&&((l=!q&&l)&&r--,f&&t.push(l))}if(r+=s,c&&s!==r){o=0;while(q=b[o++])q(t,u,g,h);if(f){if(r>0)while(s--)t[s]||u[s]||(u[s]=F.call(i));u=ua(u)}H.apply(i,u),k&&!f&&u.length>0&&r+b.length>1&&fa.uniqueSort(i)}return k&&(w=y,j=v),t};return c?ha(f):f}return h=fa.compile=function(a,b){var c,d=[],e=[],f=A[a+" "];if(!f){b||(b=g(a)),c=b.length;while(c--)f=wa(b[c]),f[u]?d.push(f):e.push(f);f=A(a,xa(e,d)),f.selector=a}return f},i=fa.select=function(a,b,e,f){var i,j,k,l,m,n="function"==typeof a&&a,o=!f&&g(a=n.selector||a);if(e=e||[],1===o.length){if(j=o[0]=o[0].slice(0),j.length>2&&"ID"===(k=j[0]).type&&c.getById&&9===b.nodeType&&p&&d.relative[j[1].type]){if(b=(d.find.ID(k.matches[0].replace(ba,ca),b)||[])[0],!b)return e;n&&(b=b.parentNode),a=a.slice(j.shift().value.length)}i=W.needsContext.test(a)?0:j.length;while(i--){if(k=j[i],d.relative[l=k.type])break;if((m=d.find[l])&&(f=m(k.matches[0].replace(ba,ca),_.test(j[0].type)&&oa(b.parentNode)||b))){if(j.splice(i,1),a=f.length&&qa(j),!a)return H.apply(e,f),e;break}}}return(n||h(a,o))(f,b,!p,e,!b||_.test(a)&&oa(b.parentNode)||b),e},c.sortStable=u.split("").sort(B).join("")===u,c.detectDuplicates=!!l,m(),c.sortDetached=ia(function(a){return 1&a.compareDocumentPosition(n.createElement("div"))}),ia(function(a){return a.innerHTML="<a href='#'></a>","#"===a.firstChild.getAttribute("href")})||ja("type|href|height|width",function(a,b,c){return c?void 0:a.getAttribute(b,"type"===b.toLowerCase()?1:2)}),c.attributes&&ia(function(a){return a.innerHTML="<input/>",a.firstChild.setAttribute("value",""),""===a.firstChild.getAttribute("value")})||ja("value",function(a,b,c){return c||"input"!==a.nodeName.toLowerCase()?void 0:a.defaultValue}),ia(function(a){return null==a.getAttribute("disabled")})||ja(K,function(a,b,c){var d;return c?void 0:a[b]===!0?b.toLowerCase():(d=a.getAttributeNode(b))&&d.specified?d.value:null}),fa}(a);n.find=t,n.expr=t.selectors,n.expr[":"]=n.expr.pseudos,n.uniqueSort=n.unique=t.uniqueSort,n.text=t.getText,n.isXMLDoc=t.isXML,n.contains=t.contains;var u=function(a,b,c){var d=[],e=void 0!==c;while((a=a[b])&&9!==a.nodeType)if(1===a.nodeType){if(e&&n(a).is(c))break;d.push(a)}return d},v=function(a,b){for(var c=[];a;a=a.nextSibling)1===a.nodeType&&a!==b&&c.push(a);return c},w=n.expr.match.needsContext,x=/^<([\w-]+)\s*\/?>(?:<\/\1>|)$/,y=/^.[^:#\[\.,]*$/;function z(a,b,c){if(n.isFunction(b))return n.grep(a,function(a,d){return!!b.call(a,d,a)!==c});if(b.nodeType)return n.grep(a,function(a){return a===b!==c});if("string"==typeof b){if(y.test(b))return n.filter(b,a,c);b=n.filter(b,a)}return n.grep(a,function(a){return n.inArray(a,b)>-1!==c})}n.filter=function(a,b,c){var d=b[0];return c&&(a=":not("+a+")"),1===b.length&&1===d.nodeType?n.find.matchesSelector(d,a)?[d]:[]:n.find.matches(a,n.grep(b,function(a){return 1===a.nodeType}))},n.fn.extend({find:function(a){var b,c=[],d=this,e=d.length;if("string"!=typeof a)return this.pushStack(n(a).filter(function(){for(b=0;e>b;b++)if(n.contains(d[b],this))return!0}));for(b=0;e>b;b++)n.find(a,d[b],c);return c=this.pushStack(e>1?n.unique(c):c),c.selector=this.selector?this.selector+" "+a:a,c},filter:function(a){return this.pushStack(z(this,a||[],!1))},not:function(a){return this.pushStack(z(this,a||[],!0))},is:function(a){return!!z(this,"string"==typeof a&&w.test(a)?n(a):a||[],!1).length}});var A,B=/^(?:\s*(<[\w\W]+>)[^>]*|#([\w-]*))$/,C=n.fn.init=function(a,b,c){var e,f;if(!a)return this;if(c=c||A,"string"==typeof a){if(e="<"===a.charAt(0)&&">"===a.charAt(a.length-1)&&a.length>=3?[null,a,null]:B.exec(a),!e||!e[1]&&b)return!b||b.jquery?(b||c).find(a):this.constructor(b).find(a);if(e[1]){if(b=b instanceof n?b[0]:b,n.merge(this,n.parseHTML(e[1],b&&b.nodeType?b.ownerDocument||b:d,!0)),x.test(e[1])&&n.isPlainObject(b))for(e in b)n.isFunction(this[e])?this[e](b[e]):this.attr(e,b[e]);return this}if(f=d.getElementById(e[2]),f&&f.parentNode){if(f.id!==e[2])return A.find(a);this.length=1,this[0]=f}return this.context=d,this.selector=a,this}return a.nodeType?(this.context=this[0]=a,this.length=1,this):n.isFunction(a)?"undefined"!=typeof c.ready?c.ready(a):a(n):(void 0!==a.selector&&(this.selector=a.selector,this.context=a.context),n.makeArray(a,this))};C.prototype=n.fn,A=n(d);var D=/^(?:parents|prev(?:Until|All))/,E={children:!0,contents:!0,next:!0,prev:!0};n.fn.extend({has:function(a){var b,c=n(a,this),d=c.length;return this.filter(function(){for(b=0;d>b;b++)if(n.contains(this,c[b]))return!0})},closest:function(a,b){for(var c,d=0,e=this.length,f=[],g=w.test(a)||"string"!=typeof a?n(a,b||this.context):0;e>d;d++)for(c=this[d];c&&c!==b;c=c.parentNode)if(c.nodeType<11&&(g?g.index(c)>-1:1===c.nodeType&&n.find.matchesSelector(c,a))){f.push(c);break}return this.pushStack(f.length>1?n.uniqueSort(f):f)},index:function(a){return a?"string"==typeof a?n.inArray(this[0],n(a)):n.inArray(a.jquery?a[0]:a,this):this[0]&&this[0].parentNode?this.first().prevAll().length:-1},add:function(a,b){return this.pushStack(n.uniqueSort(n.merge(this.get(),n(a,b))))},addBack:function(a){return this.add(null==a?this.prevObject:this.prevObject.filter(a))}});function F(a,b){do a=a[b];while(a&&1!==a.nodeType);return a}n.each({parent:function(a){var b=a.parentNode;return b&&11!==b.nodeType?b:null},parents:function(a){return u(a,"parentNode")},parentsUntil:function(a,b,c){return u(a,"parentNode",c)},next:function(a){return F(a,"nextSibling")},prev:function(a){return F(a,"previousSibling")},nextAll:function(a){return u(a,"nextSibling")},prevAll:function(a){return u(a,"previousSibling")},nextUntil:function(a,b,c){return u(a,"nextSibling",c)},prevUntil:function(a,b,c){return u(a,"previousSibling",c)},siblings:function(a){return v((a.parentNode||{}).firstChild,a)},children:function(a){return v(a.firstChild)},contents:function(a){return n.nodeName(a,"iframe")?a.contentDocument||a.contentWindow.document:n.merge([],a.childNodes)}},function(a,b){n.fn[a]=function(c,d){var e=n.map(this,b,c);return"Until"!==a.slice(-5)&&(d=c),d&&"string"==typeof d&&(e=n.filter(d,e)),this.length>1&&(E[a]||(e=n.uniqueSort(e)),D.test(a)&&(e=e.reverse())),this.pushStack(e)}});var G=/\S+/g;function H(a){var b={};return n.each(a.match(G)||[],function(a,c){b[c]=!0}),b}n.Callbacks=function(a){a="string"==typeof a?H(a):n.extend({},a);var b,c,d,e,f=[],g=[],h=-1,i=function(){for(e=a.once,d=b=!0;g.length;h=-1){c=g.shift();while(++h<f.length)f[h].apply(c[0],c[1])===!1&&a.stopOnFalse&&(h=f.length,c=!1)}a.memory||(c=!1),b=!1,e&&(f=c?[]:"")},j={add:function(){return f&&(c&&!b&&(h=f.length-1,g.push(c)),function d(b){n.each(b,function(b,c){n.isFunction(c)?a.unique&&j.has(c)||f.push(c):c&&c.length&&"string"!==n.type(c)&&d(c)})}(arguments),c&&!b&&i()),this},remove:function(){return n.each(arguments,function(a,b){var c;while((c=n.inArray(b,f,c))>-1)f.splice(c,1),h>=c&&h--}),this},has:function(a){return a?n.inArray(a,f)>-1:f.length>0},empty:function(){return f&&(f=[]),this},disable:function(){return e=g=[],f=c="",this},disabled:function(){return!f},lock:function(){return e=!0,c||j.disable(),this},locked:function(){return!!e},fireWith:function(a,c){return e||(c=c||[],c=[a,c.slice?c.slice():c],g.push(c),b||i()),this},fire:function(){return j.fireWith(this,arguments),this},fired:function(){return!!d}};return j},n.extend({Deferred:function(a){var b=[["resolve","done",n.Callbacks("once memory"),"resolved"],["reject","fail",n.Callbacks("once memory"),"rejected"],["notify","progress",n.Callbacks("memory")]],c="pending",d={state:function(){return c},always:function(){return e.done(arguments).fail(arguments),this},then:function(){var a=arguments;return n.Deferred(function(c){n.each(b,function(b,f){var g=n.isFunction(a[b])&&a[b];e[f[1]](function(){var a=g&&g.apply(this,arguments);a&&n.isFunction(a.promise)?a.promise().progress(c.notify).done(c.resolve).fail(c.reject):c[f[0]+"With"](this===d?c.promise():this,g?[a]:arguments)})}),a=null}).promise()},promise:function(a){return null!=a?n.extend(a,d):d}},e={};return d.pipe=d.then,n.each(b,function(a,f){var g=f[2],h=f[3];d[f[1]]=g.add,h&&g.add(function(){c=h},b[1^a][2].disable,b[2][2].lock),e[f[0]]=function(){return e[f[0]+"With"](this===e?d:this,arguments),this},e[f[0]+"With"]=g.fireWith}),d.promise(e),a&&a.call(e,e),e},when:function(a){var b=0,c=e.call(arguments),d=c.length,f=1!==d||a&&n.isFunction(a.promise)?d:0,g=1===f?a:n.Deferred(),h=function(a,b,c){return function(d){b[a]=this,c[a]=arguments.length>1?e.call(arguments):d,c===i?g.notifyWith(b,c):--f||g.resolveWith(b,c)}},i,j,k;if(d>1)for(i=new Array(d),j=new Array(d),k=new Array(d);d>b;b++)c[b]&&n.isFunction(c[b].promise)?c[b].promise().progress(h(b,j,i)).done(h(b,k,c)).fail(g.reject):--f;return f||g.resolveWith(k,c),g.promise()}});var I;n.fn.ready=function(a){return n.ready.promise().done(a),this},n.extend({isReady:!1,readyWait:1,holdReady:function(a){a?n.readyWait++:n.ready(!0)},ready:function(a){(a===!0?--n.readyWait:n.isReady)||(n.isReady=!0,a!==!0&&--n.readyWait>0||(I.resolveWith(d,[n]),n.fn.triggerHandler&&(n(d).triggerHandler("ready"),n(d).off("ready"))))}});function J(){d.addEventListener?(d.removeEventListener("DOMContentLoaded",K),a.removeEventListener("load",K)):(d.detachEvent("onreadystatechange",K),a.detachEvent("onload",K))}function K(){(d.addEventListener||"load"===a.event.type||"complete"===d.readyState)&&(J(),n.ready())}n.ready.promise=function(b){if(!I)if(I=n.Deferred(),"complete"===d.readyState||"loading"!==d.readyState&&!d.documentElement.doScroll)a.setTimeout(n.ready);else if(d.addEventListener)d.addEventListener("DOMContentLoaded",K),a.addEventListener("load",K);else{d.attachEvent("onreadystatechange",K),a.attachEvent("onload",K);var c=!1;try{c=null==a.frameElement&&d.documentElement}catch(e){}c&&c.doScroll&&!function f(){if(!n.isReady){try{c.doScroll("left")}catch(b){return a.setTimeout(f,50)}J(),n.ready()}}()}return I.promise(b)},n.ready.promise();var L;for(L in n(l))break;l.ownFirst="0"===L,l.inlineBlockNeedsLayout=!1,n(function(){var a,b,c,e;c=d.getElementsByTagName("body")[0],c&&c.style&&(b=d.createElement("div"),e=d.createElement("div"),e.style.cssText="position:absolute;border:0;width:0;height:0;top:0;left:-9999px",c.appendChild(e).appendChild(b),"undefined"!=typeof b.style.zoom&&(b.style.cssText="display:inline;margin:0;border:0;padding:1px;width:1px;zoom:1",l.inlineBlockNeedsLayout=a=3===b.offsetWidth,a&&(c.style.zoom=1)),c.removeChild(e))}),function(){var a=d.createElement("div");l.deleteExpando=!0;try{delete a.test}catch(b){l.deleteExpando=!1}a=null}();var M=function(a){var b=n.noData[(a.nodeName+" ").toLowerCase()],c=+a.nodeType||1;return 1!==c&&9!==c?!1:!b||b!==!0&&a.getAttribute("classid")===b},N=/^(?:\{[\w\W]*\}|\[[\w\W]*\])$/,O=/([A-Z])/g;function P(a,b,c){if(void 0===c&&1===a.nodeType){var d="data-"+b.replace(O,"-$1").toLowerCase();if(c=a.getAttribute(d),"string"==typeof c){try{c="true"===c?!0:"false"===c?!1:"null"===c?null:+c+""===c?+c:N.test(c)?n.parseJSON(c):c}catch(e){}n.data(a,b,c)}else c=void 0;
}return c}function Q(a){var b;for(b in a)if(("data"!==b||!n.isEmptyObject(a[b]))&&"toJSON"!==b)return!1;return!0}function R(a,b,d,e){if(M(a)){var f,g,h=n.expando,i=a.nodeType,j=i?n.cache:a,k=i?a[h]:a[h]&&h;if(k&&j[k]&&(e||j[k].data)||void 0!==d||"string"!=typeof b)return k||(k=i?a[h]=c.pop()||n.guid++:h),j[k]||(j[k]=i?{}:{toJSON:n.noop}),"object"!=typeof b&&"function"!=typeof b||(e?j[k]=n.extend(j[k],b):j[k].data=n.extend(j[k].data,b)),g=j[k],e||(g.data||(g.data={}),g=g.data),void 0!==d&&(g[n.camelCase(b)]=d),"string"==typeof b?(f=g[b],null==f&&(f=g[n.camelCase(b)])):f=g,f}}function S(a,b,c){if(M(a)){var d,e,f=a.nodeType,g=f?n.cache:a,h=f?a[n.expando]:n.expando;if(g[h]){if(b&&(d=c?g[h]:g[h].data)){n.isArray(b)?b=b.concat(n.map(b,n.camelCase)):b in d?b=[b]:(b=n.camelCase(b),b=b in d?[b]:b.split(" ")),e=b.length;while(e--)delete d[b[e]];if(c?!Q(d):!n.isEmptyObject(d))return}(c||(delete g[h].data,Q(g[h])))&&(f?n.cleanData([a],!0):l.deleteExpando||g!=g.window?delete g[h]:g[h]=void 0)}}}n.extend({cache:{},noData:{"applet ":!0,"embed ":!0,"object ":"clsid:D27CDB6E-AE6D-11cf-96B8-444553540000"},hasData:function(a){return a=a.nodeType?n.cache[a[n.expando]]:a[n.expando],!!a&&!Q(a)},data:function(a,b,c){return R(a,b,c)},removeData:function(a,b){return S(a,b)},_data:function(a,b,c){return R(a,b,c,!0)},_removeData:function(a,b){return S(a,b,!0)}}),n.fn.extend({data:function(a,b){var c,d,e,f=this[0],g=f&&f.attributes;if(void 0===a){if(this.length&&(e=n.data(f),1===f.nodeType&&!n._data(f,"parsedAttrs"))){c=g.length;while(c--)g[c]&&(d=g[c].name,0===d.indexOf("data-")&&(d=n.camelCase(d.slice(5)),P(f,d,e[d])));n._data(f,"parsedAttrs",!0)}return e}return"object"==typeof a?this.each(function(){n.data(this,a)}):arguments.length>1?this.each(function(){n.data(this,a,b)}):f?P(f,a,n.data(f,a)):void 0},removeData:function(a){return this.each(function(){n.removeData(this,a)})}}),n.extend({queue:function(a,b,c){var d;return a?(b=(b||"fx")+"queue",d=n._data(a,b),c&&(!d||n.isArray(c)?d=n._data(a,b,n.makeArray(c)):d.push(c)),d||[]):void 0},dequeue:function(a,b){b=b||"fx";var c=n.queue(a,b),d=c.length,e=c.shift(),f=n._queueHooks(a,b),g=function(){n.dequeue(a,b)};"inprogress"===e&&(e=c.shift(),d--),e&&("fx"===b&&c.unshift("inprogress"),delete f.stop,e.call(a,g,f)),!d&&f&&f.empty.fire()},_queueHooks:function(a,b){var c=b+"queueHooks";return n._data(a,c)||n._data(a,c,{empty:n.Callbacks("once memory").add(function(){n._removeData(a,b+"queue"),n._removeData(a,c)})})}}),n.fn.extend({queue:function(a,b){var c=2;return"string"!=typeof a&&(b=a,a="fx",c--),arguments.length<c?n.queue(this[0],a):void 0===b?this:this.each(function(){var c=n.queue(this,a,b);n._queueHooks(this,a),"fx"===a&&"inprogress"!==c[0]&&n.dequeue(this,a)})},dequeue:function(a){return this.each(function(){n.dequeue(this,a)})},clearQueue:function(a){return this.queue(a||"fx",[])},promise:function(a,b){var c,d=1,e=n.Deferred(),f=this,g=this.length,h=function(){--d||e.resolveWith(f,[f])};"string"!=typeof a&&(b=a,a=void 0),a=a||"fx";while(g--)c=n._data(f[g],a+"queueHooks"),c&&c.empty&&(d++,c.empty.add(h));return h(),e.promise(b)}}),function(){var a;l.shrinkWrapBlocks=function(){if(null!=a)return a;a=!1;var b,c,e;return c=d.getElementsByTagName("body")[0],c&&c.style?(b=d.createElement("div"),e=d.createElement("div"),e.style.cssText="position:absolute;border:0;width:0;height:0;top:0;left:-9999px",c.appendChild(e).appendChild(b),"undefined"!=typeof b.style.zoom&&(b.style.cssText="-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box;display:block;margin:0;border:0;padding:1px;width:1px;zoom:1",b.appendChild(d.createElement("div")).style.width="5px",a=3!==b.offsetWidth),c.removeChild(e),a):void 0}}();var T=/[+-]?(?:\d*\.|)\d+(?:[eE][+-]?\d+|)/.source,U=new RegExp("^(?:([+-])=|)("+T+")([a-z%]*)$","i"),V=["Top","Right","Bottom","Left"],W=function(a,b){return a=b||a,"none"===n.css(a,"display")||!n.contains(a.ownerDocument,a)};function X(a,b,c,d){var e,f=1,g=20,h=d?function(){return d.cur()}:function(){return n.css(a,b,"")},i=h(),j=c&&c[3]||(n.cssNumber[b]?"":"px"),k=(n.cssNumber[b]||"px"!==j&&+i)&&U.exec(n.css(a,b));if(k&&k[3]!==j){j=j||k[3],c=c||[],k=+i||1;do f=f||".5",k/=f,n.style(a,b,k+j);while(f!==(f=h()/i)&&1!==f&&--g)}return c&&(k=+k||+i||0,e=c[1]?k+(c[1]+1)*c[2]:+c[2],d&&(d.unit=j,d.start=k,d.end=e)),e}var Y=function(a,b,c,d,e,f,g){var h=0,i=a.length,j=null==c;if("object"===n.type(c)){e=!0;for(h in c)Y(a,b,h,c[h],!0,f,g)}else if(void 0!==d&&(e=!0,n.isFunction(d)||(g=!0),j&&(g?(b.call(a,d),b=null):(j=b,b=function(a,b,c){return j.call(n(a),c)})),b))for(;i>h;h++)b(a[h],c,g?d:d.call(a[h],h,b(a[h],c)));return e?a:j?b.call(a):i?b(a[0],c):f},Z=/^(?:checkbox|radio)$/i,$=/<([\w:-]+)/,_=/^$|\/(?:java|ecma)script/i,aa=/^\s+/,ba="abbr|article|aside|audio|bdi|canvas|data|datalist|details|dialog|figcaption|figure|footer|header|hgroup|main|mark|meter|nav|output|picture|progress|section|summary|template|time|video";function ca(a){var b=ba.split("|"),c=a.createDocumentFragment();if(c.createElement)while(b.length)c.createElement(b.pop());return c}!function(){var a=d.createElement("div"),b=d.createDocumentFragment(),c=d.createElement("input");a.innerHTML="  <link/><table></table><a href='/a'>a</a><input type='checkbox'/>",l.leadingWhitespace=3===a.firstChild.nodeType,l.tbody=!a.getElementsByTagName("tbody").length,l.htmlSerialize=!!a.getElementsByTagName("link").length,l.html5Clone="<:nav></:nav>"!==d.createElement("nav").cloneNode(!0).outerHTML,c.type="checkbox",c.checked=!0,b.appendChild(c),l.appendChecked=c.checked,a.innerHTML="<textarea>x</textarea>",l.noCloneChecked=!!a.cloneNode(!0).lastChild.defaultValue,b.appendChild(a),c=d.createElement("input"),c.setAttribute("type","radio"),c.setAttribute("checked","checked"),c.setAttribute("name","t"),a.appendChild(c),l.checkClone=a.cloneNode(!0).cloneNode(!0).lastChild.checked,l.noCloneEvent=!!a.addEventListener,a[n.expando]=1,l.attributes=!a.getAttribute(n.expando)}();var da={option:[1,"<select multiple='multiple'>","</select>"],legend:[1,"<fieldset>","</fieldset>"],area:[1,"<map>","</map>"],param:[1,"<object>","</object>"],thead:[1,"<table>","</table>"],tr:[2,"<table><tbody>","</tbody></table>"],col:[2,"<table><tbody></tbody><colgroup>","</colgroup></table>"],td:[3,"<table><tbody><tr>","</tr></tbody></table>"],_default:l.htmlSerialize?[0,"",""]:[1,"X<div>","</div>"]};da.optgroup=da.option,da.tbody=da.tfoot=da.colgroup=da.caption=da.thead,da.th=da.td;function ea(a,b){var c,d,e=0,f="undefined"!=typeof a.getElementsByTagName?a.getElementsByTagName(b||"*"):"undefined"!=typeof a.querySelectorAll?a.querySelectorAll(b||"*"):void 0;if(!f)for(f=[],c=a.childNodes||a;null!=(d=c[e]);e++)!b||n.nodeName(d,b)?f.push(d):n.merge(f,ea(d,b));return void 0===b||b&&n.nodeName(a,b)?n.merge([a],f):f}function fa(a,b){for(var c,d=0;null!=(c=a[d]);d++)n._data(c,"globalEval",!b||n._data(b[d],"globalEval"))}var ga=/<|&#?\w+;/,ha=/<tbody/i;function ia(a){Z.test(a.type)&&(a.defaultChecked=a.checked)}function ja(a,b,c,d,e){for(var f,g,h,i,j,k,m,o=a.length,p=ca(b),q=[],r=0;o>r;r++)if(g=a[r],g||0===g)if("object"===n.type(g))n.merge(q,g.nodeType?[g]:g);else if(ga.test(g)){i=i||p.appendChild(b.createElement("div")),j=($.exec(g)||["",""])[1].toLowerCase(),m=da[j]||da._default,i.innerHTML=m[1]+n.htmlPrefilter(g)+m[2],f=m[0];while(f--)i=i.lastChild;if(!l.leadingWhitespace&&aa.test(g)&&q.push(b.createTextNode(aa.exec(g)[0])),!l.tbody){g="table"!==j||ha.test(g)?"<table>"!==m[1]||ha.test(g)?0:i:i.firstChild,f=g&&g.childNodes.length;while(f--)n.nodeName(k=g.childNodes[f],"tbody")&&!k.childNodes.length&&g.removeChild(k)}n.merge(q,i.childNodes),i.textContent="";while(i.firstChild)i.removeChild(i.firstChild);i=p.lastChild}else q.push(b.createTextNode(g));i&&p.removeChild(i),l.appendChecked||n.grep(ea(q,"input"),ia),r=0;while(g=q[r++])if(d&&n.inArray(g,d)>-1)e&&e.push(g);else if(h=n.contains(g.ownerDocument,g),i=ea(p.appendChild(g),"script"),h&&fa(i),c){f=0;while(g=i[f++])_.test(g.type||"")&&c.push(g)}return i=null,p}!function(){var b,c,e=d.createElement("div");for(b in{submit:!0,change:!0,focusin:!0})c="on"+b,(l[b]=c in a)||(e.setAttribute(c,"t"),l[b]=e.attributes[c].expando===!1);e=null}();var ka=/^(?:input|select|textarea)$/i,la=/^key/,ma=/^(?:mouse|pointer|contextmenu|drag|drop)|click/,na=/^(?:focusinfocus|focusoutblur)$/,oa=/^([^.]*)(?:\.(.+)|)/;function pa(){return!0}function qa(){return!1}function ra(){try{return d.activeElement}catch(a){}}function sa(a,b,c,d,e,f){var g,h;if("object"==typeof b){"string"!=typeof c&&(d=d||c,c=void 0);for(h in b)sa(a,h,c,d,b[h],f);return a}if(null==d&&null==e?(e=c,d=c=void 0):null==e&&("string"==typeof c?(e=d,d=void 0):(e=d,d=c,c=void 0)),e===!1)e=qa;else if(!e)return a;return 1===f&&(g=e,e=function(a){return n().off(a),g.apply(this,arguments)},e.guid=g.guid||(g.guid=n.guid++)),a.each(function(){n.event.add(this,b,e,d,c)})}n.event={global:{},add:function(a,b,c,d,e){var f,g,h,i,j,k,l,m,o,p,q,r=n._data(a);if(r){c.handler&&(i=c,c=i.handler,e=i.selector),c.guid||(c.guid=n.guid++),(g=r.events)||(g=r.events={}),(k=r.handle)||(k=r.handle=function(a){return"undefined"==typeof n||a&&n.event.triggered===a.type?void 0:n.event.dispatch.apply(k.elem,arguments)},k.elem=a),b=(b||"").match(G)||[""],h=b.length;while(h--)f=oa.exec(b[h])||[],o=q=f[1],p=(f[2]||"").split(".").sort(),o&&(j=n.event.special[o]||{},o=(e?j.delegateType:j.bindType)||o,j=n.event.special[o]||{},l=n.extend({type:o,origType:q,data:d,handler:c,guid:c.guid,selector:e,needsContext:e&&n.expr.match.needsContext.test(e),namespace:p.join(".")},i),(m=g[o])||(m=g[o]=[],m.delegateCount=0,j.setup&&j.setup.call(a,d,p,k)!==!1||(a.addEventListener?a.addEventListener(o,k,!1):a.attachEvent&&a.attachEvent("on"+o,k))),j.add&&(j.add.call(a,l),l.handler.guid||(l.handler.guid=c.guid)),e?m.splice(m.delegateCount++,0,l):m.push(l),n.event.global[o]=!0);a=null}},remove:function(a,b,c,d,e){var f,g,h,i,j,k,l,m,o,p,q,r=n.hasData(a)&&n._data(a);if(r&&(k=r.events)){b=(b||"").match(G)||[""],j=b.length;while(j--)if(h=oa.exec(b[j])||[],o=q=h[1],p=(h[2]||"").split(".").sort(),o){l=n.event.special[o]||{},o=(d?l.delegateType:l.bindType)||o,m=k[o]||[],h=h[2]&&new RegExp("(^|\\.)"+p.join("\\.(?:.*\\.|)")+"(\\.|$)"),i=f=m.length;while(f--)g=m[f],!e&&q!==g.origType||c&&c.guid!==g.guid||h&&!h.test(g.namespace)||d&&d!==g.selector&&("**"!==d||!g.selector)||(m.splice(f,1),g.selector&&m.delegateCount--,l.remove&&l.remove.call(a,g));i&&!m.length&&(l.teardown&&l.teardown.call(a,p,r.handle)!==!1||n.removeEvent(a,o,r.handle),delete k[o])}else for(o in k)n.event.remove(a,o+b[j],c,d,!0);n.isEmptyObject(k)&&(delete r.handle,n._removeData(a,"events"))}},trigger:function(b,c,e,f){var g,h,i,j,l,m,o,p=[e||d],q=k.call(b,"type")?b.type:b,r=k.call(b,"namespace")?b.namespace.split("."):[];if(i=m=e=e||d,3!==e.nodeType&&8!==e.nodeType&&!na.test(q+n.event.triggered)&&(q.indexOf(".")>-1&&(r=q.split("."),q=r.shift(),r.sort()),h=q.indexOf(":")<0&&"on"+q,b=b[n.expando]?b:new n.Event(q,"object"==typeof b&&b),b.isTrigger=f?2:3,b.namespace=r.join("."),b.rnamespace=b.namespace?new RegExp("(^|\\.)"+r.join("\\.(?:.*\\.|)")+"(\\.|$)"):null,b.result=void 0,b.target||(b.target=e),c=null==c?[b]:n.makeArray(c,[b]),l=n.event.special[q]||{},f||!l.trigger||l.trigger.apply(e,c)!==!1)){if(!f&&!l.noBubble&&!n.isWindow(e)){for(j=l.delegateType||q,na.test(j+q)||(i=i.parentNode);i;i=i.parentNode)p.push(i),m=i;m===(e.ownerDocument||d)&&p.push(m.defaultView||m.parentWindow||a)}o=0;while((i=p[o++])&&!b.isPropagationStopped())b.type=o>1?j:l.bindType||q,g=(n._data(i,"events")||{})[b.type]&&n._data(i,"handle"),g&&g.apply(i,c),g=h&&i[h],g&&g.apply&&M(i)&&(b.result=g.apply(i,c),b.result===!1&&b.preventDefault());if(b.type=q,!f&&!b.isDefaultPrevented()&&(!l._default||l._default.apply(p.pop(),c)===!1)&&M(e)&&h&&e[q]&&!n.isWindow(e)){m=e[h],m&&(e[h]=null),n.event.triggered=q;try{e[q]()}catch(s){}n.event.triggered=void 0,m&&(e[h]=m)}return b.result}},dispatch:function(a){a=n.event.fix(a);var b,c,d,f,g,h=[],i=e.call(arguments),j=(n._data(this,"events")||{})[a.type]||[],k=n.event.special[a.type]||{};if(i[0]=a,a.delegateTarget=this,!k.preDispatch||k.preDispatch.call(this,a)!==!1){h=n.event.handlers.call(this,a,j),b=0;while((f=h[b++])&&!a.isPropagationStopped()){a.currentTarget=f.elem,c=0;while((g=f.handlers[c++])&&!a.isImmediatePropagationStopped())a.rnamespace&&!a.rnamespace.test(g.namespace)||(a.handleObj=g,a.data=g.data,d=((n.event.special[g.origType]||{}).handle||g.handler).apply(f.elem,i),void 0!==d&&(a.result=d)===!1&&(a.preventDefault(),a.stopPropagation()))}return k.postDispatch&&k.postDispatch.call(this,a),a.result}},handlers:function(a,b){var c,d,e,f,g=[],h=b.delegateCount,i=a.target;if(h&&i.nodeType&&("click"!==a.type||isNaN(a.button)||a.button<1))for(;i!=this;i=i.parentNode||this)if(1===i.nodeType&&(i.disabled!==!0||"click"!==a.type)){for(d=[],c=0;h>c;c++)f=b[c],e=f.selector+" ",void 0===d[e]&&(d[e]=f.needsContext?n(e,this).index(i)>-1:n.find(e,this,null,[i]).length),d[e]&&d.push(f);d.length&&g.push({elem:i,handlers:d})}return h<b.length&&g.push({elem:this,handlers:b.slice(h)}),g},fix:function(a){if(a[n.expando])return a;var b,c,e,f=a.type,g=a,h=this.fixHooks[f];h||(this.fixHooks[f]=h=ma.test(f)?this.mouseHooks:la.test(f)?this.keyHooks:{}),e=h.props?this.props.concat(h.props):this.props,a=new n.Event(g),b=e.length;while(b--)c=e[b],a[c]=g[c];return a.target||(a.target=g.srcElement||d),3===a.target.nodeType&&(a.target=a.target.parentNode),a.metaKey=!!a.metaKey,h.filter?h.filter(a,g):a},props:"altKey bubbles cancelable ctrlKey currentTarget detail eventPhase metaKey relatedTarget shiftKey target timeStamp view which".split(" "),fixHooks:{},keyHooks:{props:"char charCode key keyCode".split(" "),filter:function(a,b){return null==a.which&&(a.which=null!=b.charCode?b.charCode:b.keyCode),a}},mouseHooks:{props:"button buttons clientX clientY fromElement offsetX offsetY pageX pageY screenX screenY toElement".split(" "),filter:function(a,b){var c,e,f,g=b.button,h=b.fromElement;return null==a.pageX&&null!=b.clientX&&(e=a.target.ownerDocument||d,f=e.documentElement,c=e.body,a.pageX=b.clientX+(f&&f.scrollLeft||c&&c.scrollLeft||0)-(f&&f.clientLeft||c&&c.clientLeft||0),a.pageY=b.clientY+(f&&f.scrollTop||c&&c.scrollTop||0)-(f&&f.clientTop||c&&c.clientTop||0)),!a.relatedTarget&&h&&(a.relatedTarget=h===a.target?b.toElement:h),a.which||void 0===g||(a.which=1&g?1:2&g?3:4&g?2:0),a}},special:{load:{noBubble:!0},focus:{trigger:function(){if(this!==ra()&&this.focus)try{return this.focus(),!1}catch(a){}},delegateType:"focusin"},blur:{trigger:function(){return this===ra()&&this.blur?(this.blur(),!1):void 0},delegateType:"focusout"},click:{trigger:function(){return n.nodeName(this,"input")&&"checkbox"===this.type&&this.click?(this.click(),!1):void 0},_default:function(a){return n.nodeName(a.target,"a")}},beforeunload:{postDispatch:function(a){void 0!==a.result&&a.originalEvent&&(a.originalEvent.returnValue=a.result)}}},simulate:function(a,b,c){var d=n.extend(new n.Event,c,{type:a,isSimulated:!0});n.event.trigger(d,null,b),d.isDefaultPrevented()&&c.preventDefault()}},n.removeEvent=d.removeEventListener?function(a,b,c){a.removeEventListener&&a.removeEventListener(b,c)}:function(a,b,c){var d="on"+b;a.detachEvent&&("undefined"==typeof a[d]&&(a[d]=null),a.detachEvent(d,c))},n.Event=function(a,b){return this instanceof n.Event?(a&&a.type?(this.originalEvent=a,this.type=a.type,this.isDefaultPrevented=a.defaultPrevented||void 0===a.defaultPrevented&&a.returnValue===!1?pa:qa):this.type=a,b&&n.extend(this,b),this.timeStamp=a&&a.timeStamp||n.now(),void(this[n.expando]=!0)):new n.Event(a,b)},n.Event.prototype={constructor:n.Event,isDefaultPrevented:qa,isPropagationStopped:qa,isImmediatePropagationStopped:qa,preventDefault:function(){var a=this.originalEvent;this.isDefaultPrevented=pa,a&&(a.preventDefault?a.preventDefault():a.returnValue=!1)},stopPropagation:function(){var a=this.originalEvent;this.isPropagationStopped=pa,a&&!this.isSimulated&&(a.stopPropagation&&a.stopPropagation(),a.cancelBubble=!0)},stopImmediatePropagation:function(){var a=this.originalEvent;this.isImmediatePropagationStopped=pa,a&&a.stopImmediatePropagation&&a.stopImmediatePropagation(),this.stopPropagation()}},n.each({mouseenter:"mouseover",mouseleave:"mouseout",pointerenter:"pointerover",pointerleave:"pointerout"},function(a,b){n.event.special[a]={delegateType:b,bindType:b,handle:function(a){var c,d=this,e=a.relatedTarget,f=a.handleObj;return e&&(e===d||n.contains(d,e))||(a.type=f.origType,c=f.handler.apply(this,arguments),a.type=b),c}}}),l.submit||(n.event.special.submit={setup:function(){return n.nodeName(this,"form")?!1:void n.event.add(this,"click._submit keypress._submit",function(a){var b=a.target,c=n.nodeName(b,"input")||n.nodeName(b,"button")?n.prop(b,"form"):void 0;c&&!n._data(c,"submit")&&(n.event.add(c,"submit._submit",function(a){a._submitBubble=!0}),n._data(c,"submit",!0))})},postDispatch:function(a){a._submitBubble&&(delete a._submitBubble,this.parentNode&&!a.isTrigger&&n.event.simulate("submit",this.parentNode,a))},teardown:function(){return n.nodeName(this,"form")?!1:void n.event.remove(this,"._submit")}}),l.change||(n.event.special.change={setup:function(){return ka.test(this.nodeName)?("checkbox"!==this.type&&"radio"!==this.type||(n.event.add(this,"propertychange._change",function(a){"checked"===a.originalEvent.propertyName&&(this._justChanged=!0)}),n.event.add(this,"click._change",function(a){this._justChanged&&!a.isTrigger&&(this._justChanged=!1),n.event.simulate("change",this,a)})),!1):void n.event.add(this,"beforeactivate._change",function(a){var b=a.target;ka.test(b.nodeName)&&!n._data(b,"change")&&(n.event.add(b,"change._change",function(a){!this.parentNode||a.isSimulated||a.isTrigger||n.event.simulate("change",this.parentNode,a)}),n._data(b,"change",!0))})},handle:function(a){var b=a.target;return this!==b||a.isSimulated||a.isTrigger||"radio"!==b.type&&"checkbox"!==b.type?a.handleObj.handler.apply(this,arguments):void 0},teardown:function(){return n.event.remove(this,"._change"),!ka.test(this.nodeName)}}),l.focusin||n.each({focus:"focusin",blur:"focusout"},function(a,b){var c=function(a){n.event.simulate(b,a.target,n.event.fix(a))};n.event.special[b]={setup:function(){var d=this.ownerDocument||this,e=n._data(d,b);e||d.addEventListener(a,c,!0),n._data(d,b,(e||0)+1)},teardown:function(){var d=this.ownerDocument||this,e=n._data(d,b)-1;e?n._data(d,b,e):(d.removeEventListener(a,c,!0),n._removeData(d,b))}}}),n.fn.extend({on:function(a,b,c,d){return sa(this,a,b,c,d)},one:function(a,b,c,d){return sa(this,a,b,c,d,1)},off:function(a,b,c){var d,e;if(a&&a.preventDefault&&a.handleObj)return d=a.handleObj,n(a.delegateTarget).off(d.namespace?d.origType+"."+d.namespace:d.origType,d.selector,d.handler),this;if("object"==typeof a){for(e in a)this.off(e,b,a[e]);return this}return b!==!1&&"function"!=typeof b||(c=b,b=void 0),c===!1&&(c=qa),this.each(function(){n.event.remove(this,a,c,b)})},trigger:function(a,b){return this.each(function(){n.event.trigger(a,b,this)})},triggerHandler:function(a,b){var c=this[0];return c?n.event.trigger(a,b,c,!0):void 0}});var ta=/ jQuery\d+="(?:null|\d+)"/g,ua=new RegExp("<(?:"+ba+")[\\s/>]","i"),va=/<(?!area|br|col|embed|hr|img|input|link|meta|param)(([\w:-]+)[^>]*)\/>/gi,wa=/<script|<style|<link/i,xa=/checked\s*(?:[^=]|=\s*.checked.)/i,ya=/^true\/(.*)/,za=/^\s*<!(?:\[CDATA\[|--)|(?:\]\]|--)>\s*$/g,Aa=ca(d),Ba=Aa.appendChild(d.createElement("div"));function Ca(a,b){return n.nodeName(a,"table")&&n.nodeName(11!==b.nodeType?b:b.firstChild,"tr")?a.getElementsByTagName("tbody")[0]||a.appendChild(a.ownerDocument.createElement("tbody")):a}function Da(a){return a.type=(null!==n.find.attr(a,"type"))+"/"+a.type,a}function Ea(a){var b=ya.exec(a.type);return b?a.type=b[1]:a.removeAttribute("type"),a}function Fa(a,b){if(1===b.nodeType&&n.hasData(a)){var c,d,e,f=n._data(a),g=n._data(b,f),h=f.events;if(h){delete g.handle,g.events={};for(c in h)for(d=0,e=h[c].length;e>d;d++)n.event.add(b,c,h[c][d])}g.data&&(g.data=n.extend({},g.data))}}function Ga(a,b){var c,d,e;if(1===b.nodeType){if(c=b.nodeName.toLowerCase(),!l.noCloneEvent&&b[n.expando]){e=n._data(b);for(d in e.events)n.removeEvent(b,d,e.handle);b.removeAttribute(n.expando)}"script"===c&&b.text!==a.text?(Da(b).text=a.text,Ea(b)):"object"===c?(b.parentNode&&(b.outerHTML=a.outerHTML),l.html5Clone&&a.innerHTML&&!n.trim(b.innerHTML)&&(b.innerHTML=a.innerHTML)):"input"===c&&Z.test(a.type)?(b.defaultChecked=b.checked=a.checked,b.value!==a.value&&(b.value=a.value)):"option"===c?b.defaultSelected=b.selected=a.defaultSelected:"input"!==c&&"textarea"!==c||(b.defaultValue=a.defaultValue)}}function Ha(a,b,c,d){b=f.apply([],b);var e,g,h,i,j,k,m=0,o=a.length,p=o-1,q=b[0],r=n.isFunction(q);if(r||o>1&&"string"==typeof q&&!l.checkClone&&xa.test(q))return a.each(function(e){var f=a.eq(e);r&&(b[0]=q.call(this,e,f.html())),Ha(f,b,c,d)});if(o&&(k=ja(b,a[0].ownerDocument,!1,a,d),e=k.firstChild,1===k.childNodes.length&&(k=e),e||d)){for(i=n.map(ea(k,"script"),Da),h=i.length;o>m;m++)g=k,m!==p&&(g=n.clone(g,!0,!0),h&&n.merge(i,ea(g,"script"))),c.call(a[m],g,m);if(h)for(j=i[i.length-1].ownerDocument,n.map(i,Ea),m=0;h>m;m++)g=i[m],_.test(g.type||"")&&!n._data(g,"globalEval")&&n.contains(j,g)&&(g.src?n._evalUrl&&n._evalUrl(g.src):n.globalEval((g.text||g.textContent||g.innerHTML||"").replace(za,"")));k=e=null}return a}function Ia(a,b,c){for(var d,e=b?n.filter(b,a):a,f=0;null!=(d=e[f]);f++)c||1!==d.nodeType||n.cleanData(ea(d)),d.parentNode&&(c&&n.contains(d.ownerDocument,d)&&fa(ea(d,"script")),d.parentNode.removeChild(d));return a}n.extend({htmlPrefilter:function(a){return a.replace(va,"<$1></$2>")},clone:function(a,b,c){var d,e,f,g,h,i=n.contains(a.ownerDocument,a);if(l.html5Clone||n.isXMLDoc(a)||!ua.test("<"+a.nodeName+">")?f=a.cloneNode(!0):(Ba.innerHTML=a.outerHTML,Ba.removeChild(f=Ba.firstChild)),!(l.noCloneEvent&&l.noCloneChecked||1!==a.nodeType&&11!==a.nodeType||n.isXMLDoc(a)))for(d=ea(f),h=ea(a),g=0;null!=(e=h[g]);++g)d[g]&&Ga(e,d[g]);if(b)if(c)for(h=h||ea(a),d=d||ea(f),g=0;null!=(e=h[g]);g++)Fa(e,d[g]);else Fa(a,f);return d=ea(f,"script"),d.length>0&&fa(d,!i&&ea(a,"script")),d=h=e=null,f},cleanData:function(a,b){for(var d,e,f,g,h=0,i=n.expando,j=n.cache,k=l.attributes,m=n.event.special;null!=(d=a[h]);h++)if((b||M(d))&&(f=d[i],g=f&&j[f])){if(g.events)for(e in g.events)m[e]?n.event.remove(d,e):n.removeEvent(d,e,g.handle);j[f]&&(delete j[f],k||"undefined"==typeof d.removeAttribute?d[i]=void 0:d.removeAttribute(i),c.push(f))}}}),n.fn.extend({domManip:Ha,detach:function(a){return Ia(this,a,!0)},remove:function(a){return Ia(this,a)},text:function(a){return Y(this,function(a){return void 0===a?n.text(this):this.empty().append((this[0]&&this[0].ownerDocument||d).createTextNode(a))},null,a,arguments.length)},append:function(){return Ha(this,arguments,function(a){if(1===this.nodeType||11===this.nodeType||9===this.nodeType){var b=Ca(this,a);b.appendChild(a)}})},prepend:function(){return Ha(this,arguments,function(a){if(1===this.nodeType||11===this.nodeType||9===this.nodeType){var b=Ca(this,a);b.insertBefore(a,b.firstChild)}})},before:function(){return Ha(this,arguments,function(a){this.parentNode&&this.parentNode.insertBefore(a,this)})},after:function(){return Ha(this,arguments,function(a){this.parentNode&&this.parentNode.insertBefore(a,this.nextSibling)})},empty:function(){for(var a,b=0;null!=(a=this[b]);b++){1===a.nodeType&&n.cleanData(ea(a,!1));while(a.firstChild)a.removeChild(a.firstChild);a.options&&n.nodeName(a,"select")&&(a.options.length=0)}return this},clone:function(a,b){return a=null==a?!1:a,b=null==b?a:b,this.map(function(){return n.clone(this,a,b)})},html:function(a){return Y(this,function(a){var b=this[0]||{},c=0,d=this.length;if(void 0===a)return 1===b.nodeType?b.innerHTML.replace(ta,""):void 0;if("string"==typeof a&&!wa.test(a)&&(l.htmlSerialize||!ua.test(a))&&(l.leadingWhitespace||!aa.test(a))&&!da[($.exec(a)||["",""])[1].toLowerCase()]){a=n.htmlPrefilter(a);try{for(;d>c;c++)b=this[c]||{},1===b.nodeType&&(n.cleanData(ea(b,!1)),b.innerHTML=a);b=0}catch(e){}}b&&this.empty().append(a)},null,a,arguments.length)},replaceWith:function(){var a=[];return Ha(this,arguments,function(b){var c=this.parentNode;n.inArray(this,a)<0&&(n.cleanData(ea(this)),c&&c.replaceChild(b,this))},a)}}),n.each({appendTo:"append",prependTo:"prepend",insertBefore:"before",insertAfter:"after",replaceAll:"replaceWith"},function(a,b){n.fn[a]=function(a){for(var c,d=0,e=[],f=n(a),h=f.length-1;h>=d;d++)c=d===h?this:this.clone(!0),n(f[d])[b](c),g.apply(e,c.get());return this.pushStack(e)}});var Ja,Ka={HTML:"block",BODY:"block"};function La(a,b){var c=n(b.createElement(a)).appendTo(b.body),d=n.css(c[0],"display");return c.detach(),d}function Ma(a){var b=d,c=Ka[a];return c||(c=La(a,b),"none"!==c&&c||(Ja=(Ja||n("<iframe frameborder='0' width='0' height='0'/>")).appendTo(b.documentElement),b=(Ja[0].contentWindow||Ja[0].contentDocument).document,b.write(),b.close(),c=La(a,b),Ja.detach()),Ka[a]=c),c}var Na=/^margin/,Oa=new RegExp("^("+T+")(?!px)[a-z%]+$","i"),Pa=function(a,b,c,d){var e,f,g={};for(f in b)g[f]=a.style[f],a.style[f]=b[f];e=c.apply(a,d||[]);for(f in b)a.style[f]=g[f];return e},Qa=d.documentElement;!function(){var b,c,e,f,g,h,i=d.createElement("div"),j=d.createElement("div");if(j.style){j.style.cssText="float:left;opacity:.5",l.opacity="0.5"===j.style.opacity,l.cssFloat=!!j.style.cssFloat,j.style.backgroundClip="content-box",j.cloneNode(!0).style.backgroundClip="",l.clearCloneStyle="content-box"===j.style.backgroundClip,i=d.createElement("div"),i.style.cssText="border:0;width:8px;height:0;top:0;left:-9999px;padding:0;margin-top:1px;position:absolute",j.innerHTML="",i.appendChild(j),l.boxSizing=""===j.style.boxSizing||""===j.style.MozBoxSizing||""===j.style.WebkitBoxSizing,n.extend(l,{reliableHiddenOffsets:function(){return null==b&&k(),f},boxSizingReliable:function(){return null==b&&k(),e},pixelMarginRight:function(){return null==b&&k(),c},pixelPosition:function(){return null==b&&k(),b},reliableMarginRight:function(){return null==b&&k(),g},reliableMarginLeft:function(){return null==b&&k(),h}});function k(){var k,l,m=d.documentElement;m.appendChild(i),j.style.cssText="-webkit-box-sizing:border-box;box-sizing:border-box;position:relative;display:block;margin:auto;border:1px;padding:1px;top:1%;width:50%",b=e=h=!1,c=g=!0,a.getComputedStyle&&(l=a.getComputedStyle(j),b="1%"!==(l||{}).top,h="2px"===(l||{}).marginLeft,e="4px"===(l||{width:"4px"}).width,j.style.marginRight="50%",c="4px"===(l||{marginRight:"4px"}).marginRight,k=j.appendChild(d.createElement("div")),k.style.cssText=j.style.cssText="-webkit-box-sizing:content-box;-moz-box-sizing:content-box;box-sizing:content-box;display:block;margin:0;border:0;padding:0",k.style.marginRight=k.style.width="0",j.style.width="1px",g=!parseFloat((a.getComputedStyle(k)||{}).marginRight),j.removeChild(k)),j.style.display="none",f=0===j.getClientRects().length,f&&(j.style.display="",j.innerHTML="<table><tr><td></td><td>t</td></tr></table>",k=j.getElementsByTagName("td"),k[0].style.cssText="margin:0;border:0;padding:0;display:none",f=0===k[0].offsetHeight,f&&(k[0].style.display="",k[1].style.display="none",f=0===k[0].offsetHeight)),m.removeChild(i)}}}();var Ra,Sa,Ta=/^(top|right|bottom|left)$/;a.getComputedStyle?(Ra=function(b){var c=b.ownerDocument.defaultView;return c&&c.opener||(c=a),c.getComputedStyle(b)},Sa=function(a,b,c){var d,e,f,g,h=a.style;return c=c||Ra(a),g=c?c.getPropertyValue(b)||c[b]:void 0,""!==g&&void 0!==g||n.contains(a.ownerDocument,a)||(g=n.style(a,b)),c&&!l.pixelMarginRight()&&Oa.test(g)&&Na.test(b)&&(d=h.width,e=h.minWidth,f=h.maxWidth,h.minWidth=h.maxWidth=h.width=g,g=c.width,h.width=d,h.minWidth=e,h.maxWidth=f),void 0===g?g:g+""}):Qa.currentStyle&&(Ra=function(a){return a.currentStyle},Sa=function(a,b,c){var d,e,f,g,h=a.style;return c=c||Ra(a),g=c?c[b]:void 0,null==g&&h&&h[b]&&(g=h[b]),Oa.test(g)&&!Ta.test(b)&&(d=h.left,e=a.runtimeStyle,f=e&&e.left,f&&(e.left=a.currentStyle.left),h.left="fontSize"===b?"1em":g,g=h.pixelLeft+"px",h.left=d,f&&(e.left=f)),void 0===g?g:g+""||"auto"});function Ua(a,b){return{get:function(){return a()?void delete this.get:(this.get=b).apply(this,arguments)}}}var Va=/alpha\([^)]*\)/i,Wa=/opacity\s*=\s*([^)]*)/i,Xa=/^(none|table(?!-c[ea]).+)/,Ya=new RegExp("^("+T+")(.*)$","i"),Za={position:"absolute",visibility:"hidden",display:"block"},$a={letterSpacing:"0",fontWeight:"400"},_a=["Webkit","O","Moz","ms"],ab=d.createElement("div").style;function bb(a){if(a in ab)return a;var b=a.charAt(0).toUpperCase()+a.slice(1),c=_a.length;while(c--)if(a=_a[c]+b,a in ab)return a}function cb(a,b){for(var c,d,e,f=[],g=0,h=a.length;h>g;g++)d=a[g],d.style&&(f[g]=n._data(d,"olddisplay"),c=d.style.display,b?(f[g]||"none"!==c||(d.style.display=""),""===d.style.display&&W(d)&&(f[g]=n._data(d,"olddisplay",Ma(d.nodeName)))):(e=W(d),(c&&"none"!==c||!e)&&n._data(d,"olddisplay",e?c:n.css(d,"display"))));for(g=0;h>g;g++)d=a[g],d.style&&(b&&"none"!==d.style.display&&""!==d.style.display||(d.style.display=b?f[g]||"":"none"));return a}function db(a,b,c){var d=Ya.exec(b);return d?Math.max(0,d[1]-(c||0))+(d[2]||"px"):b}function eb(a,b,c,d,e){for(var f=c===(d?"border":"content")?4:"width"===b?1:0,g=0;4>f;f+=2)"margin"===c&&(g+=n.css(a,c+V[f],!0,e)),d?("content"===c&&(g-=n.css(a,"padding"+V[f],!0,e)),"margin"!==c&&(g-=n.css(a,"border"+V[f]+"Width",!0,e))):(g+=n.css(a,"padding"+V[f],!0,e),"padding"!==c&&(g+=n.css(a,"border"+V[f]+"Width",!0,e)));return g}function fb(b,c,e){var f=!0,g="width"===c?b.offsetWidth:b.offsetHeight,h=Ra(b),i=l.boxSizing&&"border-box"===n.css(b,"boxSizing",!1,h);if(d.msFullscreenElement&&a.top!==a&&b.getClientRects().length&&(g=Math.round(100*b.getBoundingClientRect()[c])),0>=g||null==g){if(g=Sa(b,c,h),(0>g||null==g)&&(g=b.style[c]),Oa.test(g))return g;f=i&&(l.boxSizingReliable()||g===b.style[c]),g=parseFloat(g)||0}return g+eb(b,c,e||(i?"border":"content"),f,h)+"px"}n.extend({cssHooks:{opacity:{get:function(a,b){if(b){var c=Sa(a,"opacity");return""===c?"1":c}}}},cssNumber:{animationIterationCount:!0,columnCount:!0,fillOpacity:!0,flexGrow:!0,flexShrink:!0,fontWeight:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,widows:!0,zIndex:!0,zoom:!0},cssProps:{"float":l.cssFloat?"cssFloat":"styleFloat"},style:function(a,b,c,d){if(a&&3!==a.nodeType&&8!==a.nodeType&&a.style){var e,f,g,h=n.camelCase(b),i=a.style;if(b=n.cssProps[h]||(n.cssProps[h]=bb(h)||h),g=n.cssHooks[b]||n.cssHooks[h],void 0===c)return g&&"get"in g&&void 0!==(e=g.get(a,!1,d))?e:i[b];if(f=typeof c,"string"===f&&(e=U.exec(c))&&e[1]&&(c=X(a,b,e),f="number"),null!=c&&c===c&&("number"===f&&(c+=e&&e[3]||(n.cssNumber[h]?"":"px")),l.clearCloneStyle||""!==c||0!==b.indexOf("background")||(i[b]="inherit"),!(g&&"set"in g&&void 0===(c=g.set(a,c,d)))))try{i[b]=c}catch(j){}}},css:function(a,b,c,d){var e,f,g,h=n.camelCase(b);return b=n.cssProps[h]||(n.cssProps[h]=bb(h)||h),g=n.cssHooks[b]||n.cssHooks[h],g&&"get"in g&&(f=g.get(a,!0,c)),void 0===f&&(f=Sa(a,b,d)),"normal"===f&&b in $a&&(f=$a[b]),""===c||c?(e=parseFloat(f),c===!0||isFinite(e)?e||0:f):f}}),n.each(["height","width"],function(a,b){n.cssHooks[b]={get:function(a,c,d){return c?Xa.test(n.css(a,"display"))&&0===a.offsetWidth?Pa(a,Za,function(){return fb(a,b,d)}):fb(a,b,d):void 0},set:function(a,c,d){var e=d&&Ra(a);return db(a,c,d?eb(a,b,d,l.boxSizing&&"border-box"===n.css(a,"boxSizing",!1,e),e):0)}}}),l.opacity||(n.cssHooks.opacity={get:function(a,b){return Wa.test((b&&a.currentStyle?a.currentStyle.filter:a.style.filter)||"")?.01*parseFloat(RegExp.$1)+"":b?"1":""},set:function(a,b){var c=a.style,d=a.currentStyle,e=n.isNumeric(b)?"alpha(opacity="+100*b+")":"",f=d&&d.filter||c.filter||"";c.zoom=1,(b>=1||""===b)&&""===n.trim(f.replace(Va,""))&&c.removeAttribute&&(c.removeAttribute("filter"),""===b||d&&!d.filter)||(c.filter=Va.test(f)?f.replace(Va,e):f+" "+e)}}),n.cssHooks.marginRight=Ua(l.reliableMarginRight,function(a,b){return b?Pa(a,{display:"inline-block"},Sa,[a,"marginRight"]):void 0}),n.cssHooks.marginLeft=Ua(l.reliableMarginLeft,function(a,b){
return b?(parseFloat(Sa(a,"marginLeft"))||(n.contains(a.ownerDocument,a)?a.getBoundingClientRect().left-Pa(a,{marginLeft:0},function(){return a.getBoundingClientRect().left}):0))+"px":void 0}),n.each({margin:"",padding:"",border:"Width"},function(a,b){n.cssHooks[a+b]={expand:function(c){for(var d=0,e={},f="string"==typeof c?c.split(" "):[c];4>d;d++)e[a+V[d]+b]=f[d]||f[d-2]||f[0];return e}},Na.test(a)||(n.cssHooks[a+b].set=db)}),n.fn.extend({css:function(a,b){return Y(this,function(a,b,c){var d,e,f={},g=0;if(n.isArray(b)){for(d=Ra(a),e=b.length;e>g;g++)f[b[g]]=n.css(a,b[g],!1,d);return f}return void 0!==c?n.style(a,b,c):n.css(a,b)},a,b,arguments.length>1)},show:function(){return cb(this,!0)},hide:function(){return cb(this)},toggle:function(a){return"boolean"==typeof a?a?this.show():this.hide():this.each(function(){W(this)?n(this).show():n(this).hide()})}});function gb(a,b,c,d,e){return new gb.prototype.init(a,b,c,d,e)}n.Tween=gb,gb.prototype={constructor:gb,init:function(a,b,c,d,e,f){this.elem=a,this.prop=c,this.easing=e||n.easing._default,this.options=b,this.start=this.now=this.cur(),this.end=d,this.unit=f||(n.cssNumber[c]?"":"px")},cur:function(){var a=gb.propHooks[this.prop];return a&&a.get?a.get(this):gb.propHooks._default.get(this)},run:function(a){var b,c=gb.propHooks[this.prop];return this.options.duration?this.pos=b=n.easing[this.easing](a,this.options.duration*a,0,1,this.options.duration):this.pos=b=a,this.now=(this.end-this.start)*b+this.start,this.options.step&&this.options.step.call(this.elem,this.now,this),c&&c.set?c.set(this):gb.propHooks._default.set(this),this}},gb.prototype.init.prototype=gb.prototype,gb.propHooks={_default:{get:function(a){var b;return 1!==a.elem.nodeType||null!=a.elem[a.prop]&&null==a.elem.style[a.prop]?a.elem[a.prop]:(b=n.css(a.elem,a.prop,""),b&&"auto"!==b?b:0)},set:function(a){n.fx.step[a.prop]?n.fx.step[a.prop](a):1!==a.elem.nodeType||null==a.elem.style[n.cssProps[a.prop]]&&!n.cssHooks[a.prop]?a.elem[a.prop]=a.now:n.style(a.elem,a.prop,a.now+a.unit)}}},gb.propHooks.scrollTop=gb.propHooks.scrollLeft={set:function(a){a.elem.nodeType&&a.elem.parentNode&&(a.elem[a.prop]=a.now)}},n.easing={linear:function(a){return a},swing:function(a){return.5-Math.cos(a*Math.PI)/2},_default:"swing"},n.fx=gb.prototype.init,n.fx.step={};var hb,ib,jb=/^(?:toggle|show|hide)$/,kb=/queueHooks$/;function lb(){return a.setTimeout(function(){hb=void 0}),hb=n.now()}function mb(a,b){var c,d={height:a},e=0;for(b=b?1:0;4>e;e+=2-b)c=V[e],d["margin"+c]=d["padding"+c]=a;return b&&(d.opacity=d.width=a),d}function nb(a,b,c){for(var d,e=(qb.tweeners[b]||[]).concat(qb.tweeners["*"]),f=0,g=e.length;g>f;f++)if(d=e[f].call(c,b,a))return d}function ob(a,b,c){var d,e,f,g,h,i,j,k,m=this,o={},p=a.style,q=a.nodeType&&W(a),r=n._data(a,"fxshow");c.queue||(h=n._queueHooks(a,"fx"),null==h.unqueued&&(h.unqueued=0,i=h.empty.fire,h.empty.fire=function(){h.unqueued||i()}),h.unqueued++,m.always(function(){m.always(function(){h.unqueued--,n.queue(a,"fx").length||h.empty.fire()})})),1===a.nodeType&&("height"in b||"width"in b)&&(c.overflow=[p.overflow,p.overflowX,p.overflowY],j=n.css(a,"display"),k="none"===j?n._data(a,"olddisplay")||Ma(a.nodeName):j,"inline"===k&&"none"===n.css(a,"float")&&(l.inlineBlockNeedsLayout&&"inline"!==Ma(a.nodeName)?p.zoom=1:p.display="inline-block")),c.overflow&&(p.overflow="hidden",l.shrinkWrapBlocks()||m.always(function(){p.overflow=c.overflow[0],p.overflowX=c.overflow[1],p.overflowY=c.overflow[2]}));for(d in b)if(e=b[d],jb.exec(e)){if(delete b[d],f=f||"toggle"===e,e===(q?"hide":"show")){if("show"!==e||!r||void 0===r[d])continue;q=!0}o[d]=r&&r[d]||n.style(a,d)}else j=void 0;if(n.isEmptyObject(o))"inline"===("none"===j?Ma(a.nodeName):j)&&(p.display=j);else{r?"hidden"in r&&(q=r.hidden):r=n._data(a,"fxshow",{}),f&&(r.hidden=!q),q?n(a).show():m.done(function(){n(a).hide()}),m.done(function(){var b;n._removeData(a,"fxshow");for(b in o)n.style(a,b,o[b])});for(d in o)g=nb(q?r[d]:0,d,m),d in r||(r[d]=g.start,q&&(g.end=g.start,g.start="width"===d||"height"===d?1:0))}}function pb(a,b){var c,d,e,f,g;for(c in a)if(d=n.camelCase(c),e=b[d],f=a[c],n.isArray(f)&&(e=f[1],f=a[c]=f[0]),c!==d&&(a[d]=f,delete a[c]),g=n.cssHooks[d],g&&"expand"in g){f=g.expand(f),delete a[d];for(c in f)c in a||(a[c]=f[c],b[c]=e)}else b[d]=e}function qb(a,b,c){var d,e,f=0,g=qb.prefilters.length,h=n.Deferred().always(function(){delete i.elem}),i=function(){if(e)return!1;for(var b=hb||lb(),c=Math.max(0,j.startTime+j.duration-b),d=c/j.duration||0,f=1-d,g=0,i=j.tweens.length;i>g;g++)j.tweens[g].run(f);return h.notifyWith(a,[j,f,c]),1>f&&i?c:(h.resolveWith(a,[j]),!1)},j=h.promise({elem:a,props:n.extend({},b),opts:n.extend(!0,{specialEasing:{},easing:n.easing._default},c),originalProperties:b,originalOptions:c,startTime:hb||lb(),duration:c.duration,tweens:[],createTween:function(b,c){var d=n.Tween(a,j.opts,b,c,j.opts.specialEasing[b]||j.opts.easing);return j.tweens.push(d),d},stop:function(b){var c=0,d=b?j.tweens.length:0;if(e)return this;for(e=!0;d>c;c++)j.tweens[c].run(1);return b?(h.notifyWith(a,[j,1,0]),h.resolveWith(a,[j,b])):h.rejectWith(a,[j,b]),this}}),k=j.props;for(pb(k,j.opts.specialEasing);g>f;f++)if(d=qb.prefilters[f].call(j,a,k,j.opts))return n.isFunction(d.stop)&&(n._queueHooks(j.elem,j.opts.queue).stop=n.proxy(d.stop,d)),d;return n.map(k,nb,j),n.isFunction(j.opts.start)&&j.opts.start.call(a,j),n.fx.timer(n.extend(i,{elem:a,anim:j,queue:j.opts.queue})),j.progress(j.opts.progress).done(j.opts.done,j.opts.complete).fail(j.opts.fail).always(j.opts.always)}n.Animation=n.extend(qb,{tweeners:{"*":[function(a,b){var c=this.createTween(a,b);return X(c.elem,a,U.exec(b),c),c}]},tweener:function(a,b){n.isFunction(a)?(b=a,a=["*"]):a=a.match(G);for(var c,d=0,e=a.length;e>d;d++)c=a[d],qb.tweeners[c]=qb.tweeners[c]||[],qb.tweeners[c].unshift(b)},prefilters:[ob],prefilter:function(a,b){b?qb.prefilters.unshift(a):qb.prefilters.push(a)}}),n.speed=function(a,b,c){var d=a&&"object"==typeof a?n.extend({},a):{complete:c||!c&&b||n.isFunction(a)&&a,duration:a,easing:c&&b||b&&!n.isFunction(b)&&b};return d.duration=n.fx.off?0:"number"==typeof d.duration?d.duration:d.duration in n.fx.speeds?n.fx.speeds[d.duration]:n.fx.speeds._default,null!=d.queue&&d.queue!==!0||(d.queue="fx"),d.old=d.complete,d.complete=function(){n.isFunction(d.old)&&d.old.call(this),d.queue&&n.dequeue(this,d.queue)},d},n.fn.extend({fadeTo:function(a,b,c,d){return this.filter(W).css("opacity",0).show().end().animate({opacity:b},a,c,d)},animate:function(a,b,c,d){var e=n.isEmptyObject(a),f=n.speed(b,c,d),g=function(){var b=qb(this,n.extend({},a),f);(e||n._data(this,"finish"))&&b.stop(!0)};return g.finish=g,e||f.queue===!1?this.each(g):this.queue(f.queue,g)},stop:function(a,b,c){var d=function(a){var b=a.stop;delete a.stop,b(c)};return"string"!=typeof a&&(c=b,b=a,a=void 0),b&&a!==!1&&this.queue(a||"fx",[]),this.each(function(){var b=!0,e=null!=a&&a+"queueHooks",f=n.timers,g=n._data(this);if(e)g[e]&&g[e].stop&&d(g[e]);else for(e in g)g[e]&&g[e].stop&&kb.test(e)&&d(g[e]);for(e=f.length;e--;)f[e].elem!==this||null!=a&&f[e].queue!==a||(f[e].anim.stop(c),b=!1,f.splice(e,1));!b&&c||n.dequeue(this,a)})},finish:function(a){return a!==!1&&(a=a||"fx"),this.each(function(){var b,c=n._data(this),d=c[a+"queue"],e=c[a+"queueHooks"],f=n.timers,g=d?d.length:0;for(c.finish=!0,n.queue(this,a,[]),e&&e.stop&&e.stop.call(this,!0),b=f.length;b--;)f[b].elem===this&&f[b].queue===a&&(f[b].anim.stop(!0),f.splice(b,1));for(b=0;g>b;b++)d[b]&&d[b].finish&&d[b].finish.call(this);delete c.finish})}}),n.each(["toggle","show","hide"],function(a,b){var c=n.fn[b];n.fn[b]=function(a,d,e){return null==a||"boolean"==typeof a?c.apply(this,arguments):this.animate(mb(b,!0),a,d,e)}}),n.each({slideDown:mb("show"),slideUp:mb("hide"),slideToggle:mb("toggle"),fadeIn:{opacity:"show"},fadeOut:{opacity:"hide"},fadeToggle:{opacity:"toggle"}},function(a,b){n.fn[a]=function(a,c,d){return this.animate(b,a,c,d)}}),n.timers=[],n.fx.tick=function(){var a,b=n.timers,c=0;for(hb=n.now();c<b.length;c++)a=b[c],a()||b[c]!==a||b.splice(c--,1);b.length||n.fx.stop(),hb=void 0},n.fx.timer=function(a){n.timers.push(a),a()?n.fx.start():n.timers.pop()},n.fx.interval=13,n.fx.start=function(){ib||(ib=a.setInterval(n.fx.tick,n.fx.interval))},n.fx.stop=function(){a.clearInterval(ib),ib=null},n.fx.speeds={slow:600,fast:200,_default:400},n.fn.delay=function(b,c){return b=n.fx?n.fx.speeds[b]||b:b,c=c||"fx",this.queue(c,function(c,d){var e=a.setTimeout(c,b);d.stop=function(){a.clearTimeout(e)}})},function(){var a,b=d.createElement("input"),c=d.createElement("div"),e=d.createElement("select"),f=e.appendChild(d.createElement("option"));c=d.createElement("div"),c.setAttribute("className","t"),c.innerHTML="  <link/><table></table><a href='/a'>a</a><input type='checkbox'/>",a=c.getElementsByTagName("a")[0],b.setAttribute("type","checkbox"),c.appendChild(b),a=c.getElementsByTagName("a")[0],a.style.cssText="top:1px",l.getSetAttribute="t"!==c.className,l.style=/top/.test(a.getAttribute("style")),l.hrefNormalized="/a"===a.getAttribute("href"),l.checkOn=!!b.value,l.optSelected=f.selected,l.enctype=!!d.createElement("form").enctype,e.disabled=!0,l.optDisabled=!f.disabled,b=d.createElement("input"),b.setAttribute("value",""),l.input=""===b.getAttribute("value"),b.value="t",b.setAttribute("type","radio"),l.radioValue="t"===b.value}();var rb=/\r/g,sb=/[\x20\t\r\n\f]+/g;n.fn.extend({val:function(a){var b,c,d,e=this[0];{if(arguments.length)return d=n.isFunction(a),this.each(function(c){var e;1===this.nodeType&&(e=d?a.call(this,c,n(this).val()):a,null==e?e="":"number"==typeof e?e+="":n.isArray(e)&&(e=n.map(e,function(a){return null==a?"":a+""})),b=n.valHooks[this.type]||n.valHooks[this.nodeName.toLowerCase()],b&&"set"in b&&void 0!==b.set(this,e,"value")||(this.value=e))});if(e)return b=n.valHooks[e.type]||n.valHooks[e.nodeName.toLowerCase()],b&&"get"in b&&void 0!==(c=b.get(e,"value"))?c:(c=e.value,"string"==typeof c?c.replace(rb,""):null==c?"":c)}}}),n.extend({valHooks:{option:{get:function(a){var b=n.find.attr(a,"value");return null!=b?b:n.trim(n.text(a)).replace(sb," ")}},select:{get:function(a){for(var b,c,d=a.options,e=a.selectedIndex,f="select-one"===a.type||0>e,g=f?null:[],h=f?e+1:d.length,i=0>e?h:f?e:0;h>i;i++)if(c=d[i],(c.selected||i===e)&&(l.optDisabled?!c.disabled:null===c.getAttribute("disabled"))&&(!c.parentNode.disabled||!n.nodeName(c.parentNode,"optgroup"))){if(b=n(c).val(),f)return b;g.push(b)}return g},set:function(a,b){var c,d,e=a.options,f=n.makeArray(b),g=e.length;while(g--)if(d=e[g],n.inArray(n.valHooks.option.get(d),f)>-1)try{d.selected=c=!0}catch(h){d.scrollHeight}else d.selected=!1;return c||(a.selectedIndex=-1),e}}}}),n.each(["radio","checkbox"],function(){n.valHooks[this]={set:function(a,b){return n.isArray(b)?a.checked=n.inArray(n(a).val(),b)>-1:void 0}},l.checkOn||(n.valHooks[this].get=function(a){return null===a.getAttribute("value")?"on":a.value})});var tb,ub,vb=n.expr.attrHandle,wb=/^(?:checked|selected)$/i,xb=l.getSetAttribute,yb=l.input;n.fn.extend({attr:function(a,b){return Y(this,n.attr,a,b,arguments.length>1)},removeAttr:function(a){return this.each(function(){n.removeAttr(this,a)})}}),n.extend({attr:function(a,b,c){var d,e,f=a.nodeType;if(3!==f&&8!==f&&2!==f)return"undefined"==typeof a.getAttribute?n.prop(a,b,c):(1===f&&n.isXMLDoc(a)||(b=b.toLowerCase(),e=n.attrHooks[b]||(n.expr.match.bool.test(b)?ub:tb)),void 0!==c?null===c?void n.removeAttr(a,b):e&&"set"in e&&void 0!==(d=e.set(a,c,b))?d:(a.setAttribute(b,c+""),c):e&&"get"in e&&null!==(d=e.get(a,b))?d:(d=n.find.attr(a,b),null==d?void 0:d))},attrHooks:{type:{set:function(a,b){if(!l.radioValue&&"radio"===b&&n.nodeName(a,"input")){var c=a.value;return a.setAttribute("type",b),c&&(a.value=c),b}}}},removeAttr:function(a,b){var c,d,e=0,f=b&&b.match(G);if(f&&1===a.nodeType)while(c=f[e++])d=n.propFix[c]||c,n.expr.match.bool.test(c)?yb&&xb||!wb.test(c)?a[d]=!1:a[n.camelCase("default-"+c)]=a[d]=!1:n.attr(a,c,""),a.removeAttribute(xb?c:d)}}),ub={set:function(a,b,c){return b===!1?n.removeAttr(a,c):yb&&xb||!wb.test(c)?a.setAttribute(!xb&&n.propFix[c]||c,c):a[n.camelCase("default-"+c)]=a[c]=!0,c}},n.each(n.expr.match.bool.source.match(/\w+/g),function(a,b){var c=vb[b]||n.find.attr;yb&&xb||!wb.test(b)?vb[b]=function(a,b,d){var e,f;return d||(f=vb[b],vb[b]=e,e=null!=c(a,b,d)?b.toLowerCase():null,vb[b]=f),e}:vb[b]=function(a,b,c){return c?void 0:a[n.camelCase("default-"+b)]?b.toLowerCase():null}}),yb&&xb||(n.attrHooks.value={set:function(a,b,c){return n.nodeName(a,"input")?void(a.defaultValue=b):tb&&tb.set(a,b,c)}}),xb||(tb={set:function(a,b,c){var d=a.getAttributeNode(c);return d||a.setAttributeNode(d=a.ownerDocument.createAttribute(c)),d.value=b+="","value"===c||b===a.getAttribute(c)?b:void 0}},vb.id=vb.name=vb.coords=function(a,b,c){var d;return c?void 0:(d=a.getAttributeNode(b))&&""!==d.value?d.value:null},n.valHooks.button={get:function(a,b){var c=a.getAttributeNode(b);return c&&c.specified?c.value:void 0},set:tb.set},n.attrHooks.contenteditable={set:function(a,b,c){tb.set(a,""===b?!1:b,c)}},n.each(["width","height"],function(a,b){n.attrHooks[b]={set:function(a,c){return""===c?(a.setAttribute(b,"auto"),c):void 0}}})),l.style||(n.attrHooks.style={get:function(a){return a.style.cssText||void 0},set:function(a,b){return a.style.cssText=b+""}});var zb=/^(?:input|select|textarea|button|object)$/i,Ab=/^(?:a|area)$/i;n.fn.extend({prop:function(a,b){return Y(this,n.prop,a,b,arguments.length>1)},removeProp:function(a){return a=n.propFix[a]||a,this.each(function(){try{this[a]=void 0,delete this[a]}catch(b){}})}}),n.extend({prop:function(a,b,c){var d,e,f=a.nodeType;if(3!==f&&8!==f&&2!==f)return 1===f&&n.isXMLDoc(a)||(b=n.propFix[b]||b,e=n.propHooks[b]),void 0!==c?e&&"set"in e&&void 0!==(d=e.set(a,c,b))?d:a[b]=c:e&&"get"in e&&null!==(d=e.get(a,b))?d:a[b]},propHooks:{tabIndex:{get:function(a){var b=n.find.attr(a,"tabindex");return b?parseInt(b,10):zb.test(a.nodeName)||Ab.test(a.nodeName)&&a.href?0:-1}}},propFix:{"for":"htmlFor","class":"className"}}),l.hrefNormalized||n.each(["href","src"],function(a,b){n.propHooks[b]={get:function(a){return a.getAttribute(b,4)}}}),l.optSelected||(n.propHooks.selected={get:function(a){var b=a.parentNode;return b&&(b.selectedIndex,b.parentNode&&b.parentNode.selectedIndex),null},set:function(a){var b=a.parentNode;b&&(b.selectedIndex,b.parentNode&&b.parentNode.selectedIndex)}}),n.each(["tabIndex","readOnly","maxLength","cellSpacing","cellPadding","rowSpan","colSpan","useMap","frameBorder","contentEditable"],function(){n.propFix[this.toLowerCase()]=this}),l.enctype||(n.propFix.enctype="encoding");var Bb=/[\t\r\n\f]/g;function Cb(a){return n.attr(a,"class")||""}n.fn.extend({addClass:function(a){var b,c,d,e,f,g,h,i=0;if(n.isFunction(a))return this.each(function(b){n(this).addClass(a.call(this,b,Cb(this)))});if("string"==typeof a&&a){b=a.match(G)||[];while(c=this[i++])if(e=Cb(c),d=1===c.nodeType&&(" "+e+" ").replace(Bb," ")){g=0;while(f=b[g++])d.indexOf(" "+f+" ")<0&&(d+=f+" ");h=n.trim(d),e!==h&&n.attr(c,"class",h)}}return this},removeClass:function(a){var b,c,d,e,f,g,h,i=0;if(n.isFunction(a))return this.each(function(b){n(this).removeClass(a.call(this,b,Cb(this)))});if(!arguments.length)return this.attr("class","");if("string"==typeof a&&a){b=a.match(G)||[];while(c=this[i++])if(e=Cb(c),d=1===c.nodeType&&(" "+e+" ").replace(Bb," ")){g=0;while(f=b[g++])while(d.indexOf(" "+f+" ")>-1)d=d.replace(" "+f+" "," ");h=n.trim(d),e!==h&&n.attr(c,"class",h)}}return this},toggleClass:function(a,b){var c=typeof a;return"boolean"==typeof b&&"string"===c?b?this.addClass(a):this.removeClass(a):n.isFunction(a)?this.each(function(c){n(this).toggleClass(a.call(this,c,Cb(this),b),b)}):this.each(function(){var b,d,e,f;if("string"===c){d=0,e=n(this),f=a.match(G)||[];while(b=f[d++])e.hasClass(b)?e.removeClass(b):e.addClass(b)}else void 0!==a&&"boolean"!==c||(b=Cb(this),b&&n._data(this,"__className__",b),n.attr(this,"class",b||a===!1?"":n._data(this,"__className__")||""))})},hasClass:function(a){var b,c,d=0;b=" "+a+" ";while(c=this[d++])if(1===c.nodeType&&(" "+Cb(c)+" ").replace(Bb," ").indexOf(b)>-1)return!0;return!1}}),n.each("blur focus focusin focusout load resize scroll unload click dblclick mousedown mouseup mousemove mouseover mouseout mouseenter mouseleave change select submit keydown keypress keyup error contextmenu".split(" "),function(a,b){n.fn[b]=function(a,c){return arguments.length>0?this.on(b,null,a,c):this.trigger(b)}}),n.fn.extend({hover:function(a,b){return this.mouseenter(a).mouseleave(b||a)}});var Db=a.location,Eb=n.now(),Fb=/\?/,Gb=/(,)|(\[|{)|(}|])|"(?:[^"\\\r\n]|\\["\\\/bfnrt]|\\u[\da-fA-F]{4})*"\s*:?|true|false|null|-?(?!0\d)\d+(?:\.\d+|)(?:[eE][+-]?\d+|)/g;n.parseJSON=function(b){if(a.JSON&&a.JSON.parse)return a.JSON.parse(b+"");var c,d=null,e=n.trim(b+"");return e&&!n.trim(e.replace(Gb,function(a,b,e,f){return c&&b&&(d=0),0===d?a:(c=e||b,d+=!f-!e,"")}))?Function("return "+e)():n.error("Invalid JSON: "+b)},n.parseXML=function(b){var c,d;if(!b||"string"!=typeof b)return null;try{a.DOMParser?(d=new a.DOMParser,c=d.parseFromString(b,"text/xml")):(c=new a.ActiveXObject("Microsoft.XMLDOM"),c.async="false",c.loadXML(b))}catch(e){c=void 0}return c&&c.documentElement&&!c.getElementsByTagName("parsererror").length||n.error("Invalid XML: "+b),c};var Hb=/#.*$/,Ib=/([?&])_=[^&]*/,Jb=/^(.*?):[ \t]*([^\r\n]*)\r?$/gm,Kb=/^(?:about|app|app-storage|.+-extension|file|res|widget):$/,Lb=/^(?:GET|HEAD)$/,Mb=/^\/\//,Nb=/^([\w.+-]+:)(?:\/\/(?:[^\/?#]*@|)([^\/?#:]*)(?::(\d+)|)|)/,Ob={},Pb={},Qb="*/".concat("*"),Rb=Db.href,Sb=Nb.exec(Rb.toLowerCase())||[];function Tb(a){return function(b,c){"string"!=typeof b&&(c=b,b="*");var d,e=0,f=b.toLowerCase().match(G)||[];if(n.isFunction(c))while(d=f[e++])"+"===d.charAt(0)?(d=d.slice(1)||"*",(a[d]=a[d]||[]).unshift(c)):(a[d]=a[d]||[]).push(c)}}function Ub(a,b,c,d){var e={},f=a===Pb;function g(h){var i;return e[h]=!0,n.each(a[h]||[],function(a,h){var j=h(b,c,d);return"string"!=typeof j||f||e[j]?f?!(i=j):void 0:(b.dataTypes.unshift(j),g(j),!1)}),i}return g(b.dataTypes[0])||!e["*"]&&g("*")}function Vb(a,b){var c,d,e=n.ajaxSettings.flatOptions||{};for(d in b)void 0!==b[d]&&((e[d]?a:c||(c={}))[d]=b[d]);return c&&n.extend(!0,a,c),a}function Wb(a,b,c){var d,e,f,g,h=a.contents,i=a.dataTypes;while("*"===i[0])i.shift(),void 0===e&&(e=a.mimeType||b.getResponseHeader("Content-Type"));if(e)for(g in h)if(h[g]&&h[g].test(e)){i.unshift(g);break}if(i[0]in c)f=i[0];else{for(g in c){if(!i[0]||a.converters[g+" "+i[0]]){f=g;break}d||(d=g)}f=f||d}return f?(f!==i[0]&&i.unshift(f),c[f]):void 0}function Xb(a,b,c,d){var e,f,g,h,i,j={},k=a.dataTypes.slice();if(k[1])for(g in a.converters)j[g.toLowerCase()]=a.converters[g];f=k.shift();while(f)if(a.responseFields[f]&&(c[a.responseFields[f]]=b),!i&&d&&a.dataFilter&&(b=a.dataFilter(b,a.dataType)),i=f,f=k.shift())if("*"===f)f=i;else if("*"!==i&&i!==f){if(g=j[i+" "+f]||j["* "+f],!g)for(e in j)if(h=e.split(" "),h[1]===f&&(g=j[i+" "+h[0]]||j["* "+h[0]])){g===!0?g=j[e]:j[e]!==!0&&(f=h[0],k.unshift(h[1]));break}if(g!==!0)if(g&&a["throws"])b=g(b);else try{b=g(b)}catch(l){return{state:"parsererror",error:g?l:"No conversion from "+i+" to "+f}}}return{state:"success",data:b}}n.extend({active:0,lastModified:{},etag:{},ajaxSettings:{url:Rb,type:"GET",isLocal:Kb.test(Sb[1]),global:!0,processData:!0,async:!0,contentType:"application/x-www-form-urlencoded; charset=UTF-8",accepts:{"*":Qb,text:"text/plain",html:"text/html",xml:"application/xml, text/xml",json:"application/json, text/javascript"},contents:{xml:/\bxml\b/,html:/\bhtml/,json:/\bjson\b/},responseFields:{xml:"responseXML",text:"responseText",json:"responseJSON"},converters:{"* text":String,"text html":!0,"text json":n.parseJSON,"text xml":n.parseXML},flatOptions:{url:!0,context:!0}},ajaxSetup:function(a,b){return b?Vb(Vb(a,n.ajaxSettings),b):Vb(n.ajaxSettings,a)},ajaxPrefilter:Tb(Ob),ajaxTransport:Tb(Pb),ajax:function(b,c){"object"==typeof b&&(c=b,b=void 0),c=c||{};var d,e,f,g,h,i,j,k,l=n.ajaxSetup({},c),m=l.context||l,o=l.context&&(m.nodeType||m.jquery)?n(m):n.event,p=n.Deferred(),q=n.Callbacks("once memory"),r=l.statusCode||{},s={},t={},u=0,v="canceled",w={readyState:0,getResponseHeader:function(a){var b;if(2===u){if(!k){k={};while(b=Jb.exec(g))k[b[1].toLowerCase()]=b[2]}b=k[a.toLowerCase()]}return null==b?null:b},getAllResponseHeaders:function(){return 2===u?g:null},setRequestHeader:function(a,b){var c=a.toLowerCase();return u||(a=t[c]=t[c]||a,s[a]=b),this},overrideMimeType:function(a){return u||(l.mimeType=a),this},statusCode:function(a){var b;if(a)if(2>u)for(b in a)r[b]=[r[b],a[b]];else w.always(a[w.status]);return this},abort:function(a){var b=a||v;return j&&j.abort(b),y(0,b),this}};if(p.promise(w).complete=q.add,w.success=w.done,w.error=w.fail,l.url=((b||l.url||Rb)+"").replace(Hb,"").replace(Mb,Sb[1]+"//"),l.type=c.method||c.type||l.method||l.type,l.dataTypes=n.trim(l.dataType||"*").toLowerCase().match(G)||[""],null==l.crossDomain&&(d=Nb.exec(l.url.toLowerCase()),l.crossDomain=!(!d||d[1]===Sb[1]&&d[2]===Sb[2]&&(d[3]||("http:"===d[1]?"80":"443"))===(Sb[3]||("http:"===Sb[1]?"80":"443")))),l.data&&l.processData&&"string"!=typeof l.data&&(l.data=n.param(l.data,l.traditional)),Ub(Ob,l,c,w),2===u)return w;i=n.event&&l.global,i&&0===n.active++&&n.event.trigger("ajaxStart"),l.type=l.type.toUpperCase(),l.hasContent=!Lb.test(l.type),f=l.url,l.hasContent||(l.data&&(f=l.url+=(Fb.test(f)?"&":"?")+l.data,delete l.data),l.cache===!1&&(l.url=Ib.test(f)?f.replace(Ib,"$1_="+Eb++):f+(Fb.test(f)?"&":"?")+"_="+Eb++)),l.ifModified&&(n.lastModified[f]&&w.setRequestHeader("If-Modified-Since",n.lastModified[f]),n.etag[f]&&w.setRequestHeader("If-None-Match",n.etag[f])),(l.data&&l.hasContent&&l.contentType!==!1||c.contentType)&&w.setRequestHeader("Content-Type",l.contentType),w.setRequestHeader("Accept",l.dataTypes[0]&&l.accepts[l.dataTypes[0]]?l.accepts[l.dataTypes[0]]+("*"!==l.dataTypes[0]?", "+Qb+"; q=0.01":""):l.accepts["*"]);for(e in l.headers)w.setRequestHeader(e,l.headers[e]);if(l.beforeSend&&(l.beforeSend.call(m,w,l)===!1||2===u))return w.abort();v="abort";for(e in{success:1,error:1,complete:1})w[e](l[e]);if(j=Ub(Pb,l,c,w)){if(w.readyState=1,i&&o.trigger("ajaxSend",[w,l]),2===u)return w;l.async&&l.timeout>0&&(h=a.setTimeout(function(){w.abort("timeout")},l.timeout));try{u=1,j.send(s,y)}catch(x){if(!(2>u))throw x;y(-1,x)}}else y(-1,"No Transport");function y(b,c,d,e){var k,s,t,v,x,y=c;2!==u&&(u=2,h&&a.clearTimeout(h),j=void 0,g=e||"",w.readyState=b>0?4:0,k=b>=200&&300>b||304===b,d&&(v=Wb(l,w,d)),v=Xb(l,v,w,k),k?(l.ifModified&&(x=w.getResponseHeader("Last-Modified"),x&&(n.lastModified[f]=x),x=w.getResponseHeader("etag"),x&&(n.etag[f]=x)),204===b||"HEAD"===l.type?y="nocontent":304===b?y="notmodified":(y=v.state,s=v.data,t=v.error,k=!t)):(t=y,!b&&y||(y="error",0>b&&(b=0))),w.status=b,w.statusText=(c||y)+"",k?p.resolveWith(m,[s,y,w]):p.rejectWith(m,[w,y,t]),w.statusCode(r),r=void 0,i&&o.trigger(k?"ajaxSuccess":"ajaxError",[w,l,k?s:t]),q.fireWith(m,[w,y]),i&&(o.trigger("ajaxComplete",[w,l]),--n.active||n.event.trigger("ajaxStop")))}return w},getJSON:function(a,b,c){return n.get(a,b,c,"json")},getScript:function(a,b){return n.get(a,void 0,b,"script")}}),n.each(["get","post"],function(a,b){n[b]=function(a,c,d,e){return n.isFunction(c)&&(e=e||d,d=c,c=void 0),n.ajax(n.extend({url:a,type:b,dataType:e,data:c,success:d},n.isPlainObject(a)&&a))}}),n._evalUrl=function(a){return n.ajax({url:a,type:"GET",dataType:"script",cache:!0,async:!1,global:!1,"throws":!0})},n.fn.extend({wrapAll:function(a){if(n.isFunction(a))return this.each(function(b){n(this).wrapAll(a.call(this,b))});if(this[0]){var b=n(a,this[0].ownerDocument).eq(0).clone(!0);this[0].parentNode&&b.insertBefore(this[0]),b.map(function(){var a=this;while(a.firstChild&&1===a.firstChild.nodeType)a=a.firstChild;return a}).append(this)}return this},wrapInner:function(a){return n.isFunction(a)?this.each(function(b){n(this).wrapInner(a.call(this,b))}):this.each(function(){var b=n(this),c=b.contents();c.length?c.wrapAll(a):b.append(a)})},wrap:function(a){var b=n.isFunction(a);return this.each(function(c){n(this).wrapAll(b?a.call(this,c):a)})},unwrap:function(){return this.parent().each(function(){n.nodeName(this,"body")||n(this).replaceWith(this.childNodes)}).end()}});function Yb(a){return a.style&&a.style.display||n.css(a,"display")}function Zb(a){while(a&&1===a.nodeType){if("none"===Yb(a)||"hidden"===a.type)return!0;a=a.parentNode}return!1}n.expr.filters.hidden=function(a){return l.reliableHiddenOffsets()?a.offsetWidth<=0&&a.offsetHeight<=0&&!a.getClientRects().length:Zb(a)},n.expr.filters.visible=function(a){return!n.expr.filters.hidden(a)};var $b=/%20/g,_b=/\[\]$/,ac=/\r?\n/g,bc=/^(?:submit|button|image|reset|file)$/i,cc=/^(?:input|select|textarea|keygen)/i;function dc(a,b,c,d){var e;if(n.isArray(b))n.each(b,function(b,e){c||_b.test(a)?d(a,e):dc(a+"["+("object"==typeof e&&null!=e?b:"")+"]",e,c,d)});else if(c||"object"!==n.type(b))d(a,b);else for(e in b)dc(a+"["+e+"]",b[e],c,d)}n.param=function(a,b){var c,d=[],e=function(a,b){b=n.isFunction(b)?b():null==b?"":b,d[d.length]=encodeURIComponent(a)+"="+encodeURIComponent(b)};if(void 0===b&&(b=n.ajaxSettings&&n.ajaxSettings.traditional),n.isArray(a)||a.jquery&&!n.isPlainObject(a))n.each(a,function(){e(this.name,this.value)});else for(c in a)dc(c,a[c],b,e);return d.join("&").replace($b,"+")},n.fn.extend({serialize:function(){return n.param(this.serializeArray())},serializeArray:function(){return this.map(function(){var a=n.prop(this,"elements");return a?n.makeArray(a):this}).filter(function(){var a=this.type;return this.name&&!n(this).is(":disabled")&&cc.test(this.nodeName)&&!bc.test(a)&&(this.checked||!Z.test(a))}).map(function(a,b){var c=n(this).val();return null==c?null:n.isArray(c)?n.map(c,function(a){return{name:b.name,value:a.replace(ac,"\r\n")}}):{name:b.name,value:c.replace(ac,"\r\n")}}).get()}}),n.ajaxSettings.xhr=void 0!==a.ActiveXObject?function(){return this.isLocal?ic():d.documentMode>8?hc():/^(get|post|head|put|delete|options)$/i.test(this.type)&&hc()||ic()}:hc;var ec=0,fc={},gc=n.ajaxSettings.xhr();a.attachEvent&&a.attachEvent("onunload",function(){for(var a in fc)fc[a](void 0,!0)}),l.cors=!!gc&&"withCredentials"in gc,gc=l.ajax=!!gc,gc&&n.ajaxTransport(function(b){if(!b.crossDomain||l.cors){var c;return{send:function(d,e){var f,g=b.xhr(),h=++ec;if(g.open(b.type,b.url,b.async,b.username,b.password),b.xhrFields)for(f in b.xhrFields)g[f]=b.xhrFields[f];b.mimeType&&g.overrideMimeType&&g.overrideMimeType(b.mimeType),b.crossDomain||d["X-Requested-With"]||(d["X-Requested-With"]="XMLHttpRequest");for(f in d)void 0!==d[f]&&g.setRequestHeader(f,d[f]+"");g.send(b.hasContent&&b.data||null),c=function(a,d){var f,i,j;if(c&&(d||4===g.readyState))if(delete fc[h],c=void 0,g.onreadystatechange=n.noop,d)4!==g.readyState&&g.abort();else{j={},f=g.status,"string"==typeof g.responseText&&(j.text=g.responseText);try{i=g.statusText}catch(k){i=""}f||!b.isLocal||b.crossDomain?1223===f&&(f=204):f=j.text?200:404}j&&e(f,i,j,g.getAllResponseHeaders())},b.async?4===g.readyState?a.setTimeout(c):g.onreadystatechange=fc[h]=c:c()},abort:function(){c&&c(void 0,!0)}}}});function hc(){try{return new a.XMLHttpRequest}catch(b){}}function ic(){try{return new a.ActiveXObject("Microsoft.XMLHTTP")}catch(b){}}n.ajaxSetup({accepts:{script:"text/javascript, application/javascript, application/ecmascript, application/x-ecmascript"},contents:{script:/\b(?:java|ecma)script\b/},converters:{"text script":function(a){return n.globalEval(a),a}}}),n.ajaxPrefilter("script",function(a){void 0===a.cache&&(a.cache=!1),a.crossDomain&&(a.type="GET",a.global=!1)}),n.ajaxTransport("script",function(a){if(a.crossDomain){var b,c=d.head||n("head")[0]||d.documentElement;return{send:function(e,f){b=d.createElement("script"),b.async=!0,a.scriptCharset&&(b.charset=a.scriptCharset),b.src=a.url,b.onload=b.onreadystatechange=function(a,c){(c||!b.readyState||/loaded|complete/.test(b.readyState))&&(b.onload=b.onreadystatechange=null,b.parentNode&&b.parentNode.removeChild(b),b=null,c||f(200,"success"))},c.insertBefore(b,c.firstChild)},abort:function(){b&&b.onload(void 0,!0)}}}});var jc=[],kc=/(=)\?(?=&|$)|\?\?/;n.ajaxSetup({jsonp:"callback",jsonpCallback:function(){var a=jc.pop()||n.expando+"_"+Eb++;return this[a]=!0,a}}),n.ajaxPrefilter("json jsonp",function(b,c,d){var e,f,g,h=b.jsonp!==!1&&(kc.test(b.url)?"url":"string"==typeof b.data&&0===(b.contentType||"").indexOf("application/x-www-form-urlencoded")&&kc.test(b.data)&&"data");return h||"jsonp"===b.dataTypes[0]?(e=b.jsonpCallback=n.isFunction(b.jsonpCallback)?b.jsonpCallback():b.jsonpCallback,h?b[h]=b[h].replace(kc,"$1"+e):b.jsonp!==!1&&(b.url+=(Fb.test(b.url)?"&":"?")+b.jsonp+"="+e),b.converters["script json"]=function(){return g||n.error(e+" was not called"),g[0]},b.dataTypes[0]="json",f=a[e],a[e]=function(){g=arguments},d.always(function(){void 0===f?n(a).removeProp(e):a[e]=f,b[e]&&(b.jsonpCallback=c.jsonpCallback,jc.push(e)),g&&n.isFunction(f)&&f(g[0]),g=f=void 0}),"script"):void 0}),n.parseHTML=function(a,b,c){if(!a||"string"!=typeof a)return null;"boolean"==typeof b&&(c=b,b=!1),b=b||d;var e=x.exec(a),f=!c&&[];return e?[b.createElement(e[1])]:(e=ja([a],b,f),f&&f.length&&n(f).remove(),n.merge([],e.childNodes))};var lc=n.fn.load;n.fn.load=function(a,b,c){if("string"!=typeof a&&lc)return lc.apply(this,arguments);var d,e,f,g=this,h=a.indexOf(" ");return h>-1&&(d=n.trim(a.slice(h,a.length)),a=a.slice(0,h)),n.isFunction(b)?(c=b,b=void 0):b&&"object"==typeof b&&(e="POST"),g.length>0&&n.ajax({url:a,type:e||"GET",dataType:"html",data:b}).done(function(a){f=arguments,g.html(d?n("<div>").append(n.parseHTML(a)).find(d):a)}).always(c&&function(a,b){g.each(function(){c.apply(this,f||[a.responseText,b,a])})}),this},n.each(["ajaxStart","ajaxStop","ajaxComplete","ajaxError","ajaxSuccess","ajaxSend"],function(a,b){n.fn[b]=function(a){return this.on(b,a)}}),n.expr.filters.animated=function(a){return n.grep(n.timers,function(b){return a===b.elem}).length};function mc(a){return n.isWindow(a)?a:9===a.nodeType?a.defaultView||a.parentWindow:!1}n.offset={setOffset:function(a,b,c){var d,e,f,g,h,i,j,k=n.css(a,"position"),l=n(a),m={};"static"===k&&(a.style.position="relative"),h=l.offset(),f=n.css(a,"top"),i=n.css(a,"left"),j=("absolute"===k||"fixed"===k)&&n.inArray("auto",[f,i])>-1,j?(d=l.position(),g=d.top,e=d.left):(g=parseFloat(f)||0,e=parseFloat(i)||0),n.isFunction(b)&&(b=b.call(a,c,n.extend({},h))),null!=b.top&&(m.top=b.top-h.top+g),null!=b.left&&(m.left=b.left-h.left+e),"using"in b?b.using.call(a,m):l.css(m)}},n.fn.extend({offset:function(a){if(arguments.length)return void 0===a?this:this.each(function(b){n.offset.setOffset(this,a,b)});var b,c,d={top:0,left:0},e=this[0],f=e&&e.ownerDocument;if(f)return b=f.documentElement,n.contains(b,e)?("undefined"!=typeof e.getBoundingClientRect&&(d=e.getBoundingClientRect()),c=mc(f),{top:d.top+(c.pageYOffset||b.scrollTop)-(b.clientTop||0),left:d.left+(c.pageXOffset||b.scrollLeft)-(b.clientLeft||0)}):d},position:function(){if(this[0]){var a,b,c={top:0,left:0},d=this[0];return"fixed"===n.css(d,"position")?b=d.getBoundingClientRect():(a=this.offsetParent(),b=this.offset(),n.nodeName(a[0],"html")||(c=a.offset()),c.top+=n.css(a[0],"borderTopWidth",!0),c.left+=n.css(a[0],"borderLeftWidth",!0)),{top:b.top-c.top-n.css(d,"marginTop",!0),left:b.left-c.left-n.css(d,"marginLeft",!0)}}},offsetParent:function(){return this.map(function(){var a=this.offsetParent;while(a&&!n.nodeName(a,"html")&&"static"===n.css(a,"position"))a=a.offsetParent;return a||Qa})}}),n.each({scrollLeft:"pageXOffset",scrollTop:"pageYOffset"},function(a,b){var c=/Y/.test(b);n.fn[a]=function(d){return Y(this,function(a,d,e){var f=mc(a);return void 0===e?f?b in f?f[b]:f.document.documentElement[d]:a[d]:void(f?f.scrollTo(c?n(f).scrollLeft():e,c?e:n(f).scrollTop()):a[d]=e)},a,d,arguments.length,null)}}),n.each(["top","left"],function(a,b){n.cssHooks[b]=Ua(l.pixelPosition,function(a,c){return c?(c=Sa(a,b),Oa.test(c)?n(a).position()[b]+"px":c):void 0;
})}),n.each({Height:"height",Width:"width"},function(a,b){n.each({padding:"inner"+a,content:b,"":"outer"+a},function(c,d){n.fn[d]=function(d,e){var f=arguments.length&&(c||"boolean"!=typeof d),g=c||(d===!0||e===!0?"margin":"border");return Y(this,function(b,c,d){var e;return n.isWindow(b)?b.document.documentElement["client"+a]:9===b.nodeType?(e=b.documentElement,Math.max(b.body["scroll"+a],e["scroll"+a],b.body["offset"+a],e["offset"+a],e["client"+a])):void 0===d?n.css(b,c,g):n.style(b,c,d,g)},b,f?d:void 0,f,null)}})}),n.fn.extend({bind:function(a,b,c){return this.on(a,null,b,c)},unbind:function(a,b){return this.off(a,null,b)},delegate:function(a,b,c,d){return this.on(b,a,c,d)},undelegate:function(a,b,c){return 1===arguments.length?this.off(a,"**"):this.off(b,a||"**",c)}}),n.fn.size=function(){return this.length},n.fn.andSelf=n.fn.addBack,"function"==typeof define&&define.amd&&define("jquery",[],function(){return n});var nc=a.jQuery,oc=a.$;return n.noConflict=function(b){return a.$===n&&(a.$=oc),b&&a.jQuery===n&&(a.jQuery=nc),n},b||(a.jQuery=a.$=n),n});
/**
 * Owl Carousel v2.3.4
 * Copyright 2013-2018 David Deutsch
 * Licensed under: SEE LICENSE IN https://github.com/OwlCarousel2/OwlCarousel2/blob/master/LICENSE
 */
!function(a,b,c,d){function e(b,c){this.settings=null,this.options=a.extend({},e.Defaults,c),this.$element=a(b),this._handlers={},this._plugins={},this._supress={},this._current=null,this._speed=null,this._coordinates=[],this._breakpoint=null,this._width=null,this._items=[],this._clones=[],this._mergers=[],this._widths=[],this._invalidated={},this._pipe=[],this._drag={time:null,target:null,pointer:null,stage:{start:null,current:null},direction:null},this._states={current:{},tags:{initializing:["busy"],animating:["busy"],dragging:["interacting"]}},a.each(["onResize","onThrottledResize"],a.proxy(function(b,c){this._handlers[c]=a.proxy(this[c],this)},this)),a.each(e.Plugins,a.proxy(function(a,b){this._plugins[a.charAt(0).toLowerCase()+a.slice(1)]=new b(this)},this)),a.each(e.Workers,a.proxy(function(b,c){this._pipe.push({filter:c.filter,run:a.proxy(c.run,this)})},this)),this.setup(),this.initialize()}e.Defaults={items:3,loop:!1,center:!1,rewind:!1,checkVisibility:!0,mouseDrag:!0,touchDrag:!0,pullDrag:!0,freeDrag:!1,margin:0,stagePadding:0,merge:!1,mergeFit:!0,autoWidth:!1,startPosition:0,rtl:!1,smartSpeed:250,fluidSpeed:!1,dragEndSpeed:!1,responsive:{},responsiveRefreshRate:200,responsiveBaseElement:b,fallbackEasing:"swing",slideTransition:"",info:!1,nestedItemSelector:!1,itemElement:"div",stageElement:"div",refreshClass:"owl-refresh",loadedClass:"owl-loaded",loadingClass:"owl-loading",rtlClass:"owl-rtl",responsiveClass:"owl-responsive",dragClass:"owl-drag",itemClass:"owl-item",stageClass:"owl-stage",stageOuterClass:"owl-stage-outer",grabClass:"owl-grab"},e.Width={Default:"default",Inner:"inner",Outer:"outer"},e.Type={Event:"event",State:"state"},e.Plugins={},e.Workers=[{filter:["width","settings"],run:function(){this._width=this.$element.width()}},{filter:["width","items","settings"],run:function(a){a.current=this._items&&this._items[this.relative(this._current)]}},{filter:["items","settings"],run:function(){this.$stage.children(".cloned").remove()}},{filter:["width","items","settings"],run:function(a){var b=this.settings.margin||"",c=!this.settings.autoWidth,d=this.settings.rtl,e={width:"auto","margin-left":d?b:"","margin-right":d?"":b};!c&&this.$stage.children().css(e),a.css=e}},{filter:["width","items","settings"],run:function(a){var b=(this.width()/this.settings.items).toFixed(3)-this.settings.margin,c=null,d=this._items.length,e=!this.settings.autoWidth,f=[];for(a.items={merge:!1,width:b};d--;)c=this._mergers[d],c=this.settings.mergeFit&&Math.min(c,this.settings.items)||c,a.items.merge=c>1||a.items.merge,f[d]=e?b*c:this._items[d].width();this._widths=f}},{filter:["items","settings"],run:function(){var b=[],c=this._items,d=this.settings,e=Math.max(2*d.items,4),f=2*Math.ceil(c.length/2),g=d.loop&&c.length?d.rewind?e:Math.max(e,f):0,h="",i="";for(g/=2;g>0;)b.push(this.normalize(b.length/2,!0)),h+=c[b[b.length-1]][0].outerHTML,b.push(this.normalize(c.length-1-(b.length-1)/2,!0)),i=c[b[b.length-1]][0].outerHTML+i,g-=1;this._clones=b,a(h).addClass("cloned").appendTo(this.$stage),a(i).addClass("cloned").prependTo(this.$stage)}},{filter:["width","items","settings"],run:function(){for(var a=this.settings.rtl?1:-1,b=this._clones.length+this._items.length,c=-1,d=0,e=0,f=[];++c<b;)d=f[c-1]||0,e=this._widths[this.relative(c)]+this.settings.margin,f.push(d+e*a);this._coordinates=f}},{filter:["width","items","settings"],run:function(){var a=this.settings.stagePadding,b=this._coordinates,c={width:Math.ceil(Math.abs(b[b.length-1]))+2*a,"padding-left":a||"","padding-right":a||""};this.$stage.css(c)}},{filter:["width","items","settings"],run:function(a){var b=this._coordinates.length,c=!this.settings.autoWidth,d=this.$stage.children();if(c&&a.items.merge)for(;b--;)a.css.width=this._widths[this.relative(b)],d.eq(b).css(a.css);else c&&(a.css.width=a.items.width,d.css(a.css))}},{filter:["items"],run:function(){this._coordinates.length<1&&this.$stage.removeAttr("style")}},{filter:["width","items","settings"],run:function(a){a.current=a.current?this.$stage.children().index(a.current):0,a.current=Math.max(this.minimum(),Math.min(this.maximum(),a.current)),this.reset(a.current)}},{filter:["position"],run:function(){this.animate(this.coordinates(this._current))}},{filter:["width","position","items","settings"],run:function(){var a,b,c,d,e=this.settings.rtl?1:-1,f=2*this.settings.stagePadding,g=this.coordinates(this.current())+f,h=g+this.width()*e,i=[];for(c=0,d=this._coordinates.length;c<d;c++)a=this._coordinates[c-1]||0,b=Math.abs(this._coordinates[c])+f*e,(this.op(a,"<=",g)&&this.op(a,">",h)||this.op(b,"<",g)&&this.op(b,">",h))&&i.push(c);this.$stage.children(".active").removeClass("active"),this.$stage.children(":eq("+i.join("), :eq(")+")").addClass("active"),this.$stage.children(".center").removeClass("center"),this.settings.center&&this.$stage.children().eq(this.current()).addClass("center")}}],e.prototype.initializeStage=function(){this.$stage=this.$element.find("."+this.settings.stageClass),this.$stage.length||(this.$element.addClass(this.options.loadingClass),this.$stage=a("<"+this.settings.stageElement+">",{class:this.settings.stageClass}).wrap(a("<div/>",{class:this.settings.stageOuterClass})),this.$element.append(this.$stage.parent()))},e.prototype.initializeItems=function(){var b=this.$element.find(".owl-item");if(b.length)return this._items=b.get().map(function(b){return a(b)}),this._mergers=this._items.map(function(){return 1}),void this.refresh();this.replace(this.$element.children().not(this.$stage.parent())),this.isVisible()?this.refresh():this.invalidate("width"),this.$element.removeClass(this.options.loadingClass).addClass(this.options.loadedClass)},e.prototype.initialize=function(){if(this.enter("initializing"),this.trigger("initialize"),this.$element.toggleClass(this.settings.rtlClass,this.settings.rtl),this.settings.autoWidth&&!this.is("pre-loading")){var a,b,c;a=this.$element.find("img"),b=this.settings.nestedItemSelector?"."+this.settings.nestedItemSelector:d,c=this.$element.children(b).width(),a.length&&c<=0&&this.preloadAutoWidthImages(a)}this.initializeStage(),this.initializeItems(),this.registerEventHandlers(),this.leave("initializing"),this.trigger("initialized")},e.prototype.isVisible=function(){return!this.settings.checkVisibility||this.$element.is(":visible")},e.prototype.setup=function(){var b=this.viewport(),c=this.options.responsive,d=-1,e=null;c?(a.each(c,function(a){a<=b&&a>d&&(d=Number(a))}),e=a.extend({},this.options,c[d]),"function"==typeof e.stagePadding&&(e.stagePadding=e.stagePadding()),delete e.responsive,e.responsiveClass&&this.$element.attr("class",this.$element.attr("class").replace(new RegExp("("+this.options.responsiveClass+"-)\\S+\\s","g"),"$1"+d))):e=a.extend({},this.options),this.trigger("change",{property:{name:"settings",value:e}}),this._breakpoint=d,this.settings=e,this.invalidate("settings"),this.trigger("changed",{property:{name:"settings",value:this.settings}})},e.prototype.optionsLogic=function(){this.settings.autoWidth&&(this.settings.stagePadding=!1,this.settings.merge=!1)},e.prototype.prepare=function(b){var c=this.trigger("prepare",{content:b});return c.data||(c.data=a("<"+this.settings.itemElement+"/>").addClass(this.options.itemClass).append(b)),this.trigger("prepared",{content:c.data}),c.data},e.prototype.update=function(){for(var b=0,c=this._pipe.length,d=a.proxy(function(a){return this[a]},this._invalidated),e={};b<c;)(this._invalidated.all||a.grep(this._pipe[b].filter,d).length>0)&&this._pipe[b].run(e),b++;this._invalidated={},!this.is("valid")&&this.enter("valid")},e.prototype.width=function(a){switch(a=a||e.Width.Default){case e.Width.Inner:case e.Width.Outer:return this._width;default:return this._width-2*this.settings.stagePadding+this.settings.margin}},e.prototype.refresh=function(){this.enter("refreshing"),this.trigger("refresh"),this.setup(),this.optionsLogic(),this.$element.addClass(this.options.refreshClass),this.update(),this.$element.removeClass(this.options.refreshClass),this.leave("refreshing"),this.trigger("refreshed")},e.prototype.onThrottledResize=function(){b.clearTimeout(this.resizeTimer),this.resizeTimer=b.setTimeout(this._handlers.onResize,this.settings.responsiveRefreshRate)},e.prototype.onResize=function(){return!!this._items.length&&(this._width!==this.$element.width()&&(!!this.isVisible()&&(this.enter("resizing"),this.trigger("resize").isDefaultPrevented()?(this.leave("resizing"),!1):(this.invalidate("width"),this.refresh(),this.leave("resizing"),void this.trigger("resized")))))},e.prototype.registerEventHandlers=function(){a.support.transition&&this.$stage.on(a.support.transition.end+".owl.core",a.proxy(this.onTransitionEnd,this)),!1!==this.settings.responsive&&this.on(b,"resize",this._handlers.onThrottledResize),this.settings.mouseDrag&&(this.$element.addClass(this.options.dragClass),this.$stage.on("mousedown.owl.core",a.proxy(this.onDragStart,this)),this.$stage.on("dragstart.owl.core selectstart.owl.core",function(){return!1})),this.settings.touchDrag&&(this.$stage.on("touchstart.owl.core",a.proxy(this.onDragStart,this)),this.$stage.on("touchcancel.owl.core",a.proxy(this.onDragEnd,this)))},e.prototype.onDragStart=function(b){var d=null;3!==b.which&&(a.support.transform?(d=this.$stage.css("transform").replace(/.*\(|\)| /g,"").split(","),d={x:d[16===d.length?12:4],y:d[16===d.length?13:5]}):(d=this.$stage.position(),d={x:this.settings.rtl?d.left+this.$stage.width()-this.width()+this.settings.margin:d.left,y:d.top}),this.is("animating")&&(a.support.transform?this.animate(d.x):this.$stage.stop(),this.invalidate("position")),this.$element.toggleClass(this.options.grabClass,"mousedown"===b.type),this.speed(0),this._drag.time=(new Date).getTime(),this._drag.target=a(b.target),this._drag.stage.start=d,this._drag.stage.current=d,this._drag.pointer=this.pointer(b),a(c).on("mouseup.owl.core touchend.owl.core",a.proxy(this.onDragEnd,this)),a(c).one("mousemove.owl.core touchmove.owl.core",a.proxy(function(b){var d=this.difference(this._drag.pointer,this.pointer(b));a(c).on("mousemove.owl.core touchmove.owl.core",a.proxy(this.onDragMove,this)),Math.abs(d.x)<Math.abs(d.y)&&this.is("valid")||(b.preventDefault(),this.enter("dragging"),this.trigger("drag"))},this)))},e.prototype.onDragMove=function(a){var b=null,c=null,d=null,e=this.difference(this._drag.pointer,this.pointer(a)),f=this.difference(this._drag.stage.start,e);this.is("dragging")&&(a.preventDefault(),this.settings.loop?(b=this.coordinates(this.minimum()),c=this.coordinates(this.maximum()+1)-b,f.x=((f.x-b)%c+c)%c+b):(b=this.settings.rtl?this.coordinates(this.maximum()):this.coordinates(this.minimum()),c=this.settings.rtl?this.coordinates(this.minimum()):this.coordinates(this.maximum()),d=this.settings.pullDrag?-1*e.x/5:0,f.x=Math.max(Math.min(f.x,b+d),c+d)),this._drag.stage.current=f,this.animate(f.x))},e.prototype.onDragEnd=function(b){var d=this.difference(this._drag.pointer,this.pointer(b)),e=this._drag.stage.current,f=d.x>0^this.settings.rtl?"left":"right";a(c).off(".owl.core"),this.$element.removeClass(this.options.grabClass),(0!==d.x&&this.is("dragging")||!this.is("valid"))&&(this.speed(this.settings.dragEndSpeed||this.settings.smartSpeed),this.current(this.closest(e.x,0!==d.x?f:this._drag.direction)),this.invalidate("position"),this.update(),this._drag.direction=f,(Math.abs(d.x)>3||(new Date).getTime()-this._drag.time>300)&&this._drag.target.one("click.owl.core",function(){return!1})),this.is("dragging")&&(this.leave("dragging"),this.trigger("dragged"))},e.prototype.closest=function(b,c){var e=-1,f=30,g=this.width(),h=this.coordinates();return this.settings.freeDrag||a.each(h,a.proxy(function(a,i){return"left"===c&&b>i-f&&b<i+f?e=a:"right"===c&&b>i-g-f&&b<i-g+f?e=a+1:this.op(b,"<",i)&&this.op(b,">",h[a+1]!==d?h[a+1]:i-g)&&(e="left"===c?a+1:a),-1===e},this)),this.settings.loop||(this.op(b,">",h[this.minimum()])?e=b=this.minimum():this.op(b,"<",h[this.maximum()])&&(e=b=this.maximum())),e},e.prototype.animate=function(b){var c=this.speed()>0;this.is("animating")&&this.onTransitionEnd(),c&&(this.enter("animating"),this.trigger("translate")),a.support.transform3d&&a.support.transition?this.$stage.css({transform:"translate3d("+b+"px,0px,0px)",transition:this.speed()/1e3+"s"+(this.settings.slideTransition?" "+this.settings.slideTransition:"")}):c?this.$stage.animate({left:b+"px"},this.speed(),this.settings.fallbackEasing,a.proxy(this.onTransitionEnd,this)):this.$stage.css({left:b+"px"})},e.prototype.is=function(a){return this._states.current[a]&&this._states.current[a]>0},e.prototype.current=function(a){if(a===d)return this._current;if(0===this._items.length)return d;if(a=this.normalize(a),this._current!==a){var b=this.trigger("change",{property:{name:"position",value:a}});b.data!==d&&(a=this.normalize(b.data)),this._current=a,this.invalidate("position"),this.trigger("changed",{property:{name:"position",value:this._current}})}return this._current},e.prototype.invalidate=function(b){return"string"===a.type(b)&&(this._invalidated[b]=!0,this.is("valid")&&this.leave("valid")),a.map(this._invalidated,function(a,b){return b})},e.prototype.reset=function(a){(a=this.normalize(a))!==d&&(this._speed=0,this._current=a,this.suppress(["translate","translated"]),this.animate(this.coordinates(a)),this.release(["translate","translated"]))},e.prototype.normalize=function(a,b){var c=this._items.length,e=b?0:this._clones.length;return!this.isNumeric(a)||c<1?a=d:(a<0||a>=c+e)&&(a=((a-e/2)%c+c)%c+e/2),a},e.prototype.relative=function(a){return a-=this._clones.length/2,this.normalize(a,!0)},e.prototype.maximum=function(a){var b,c,d,e=this.settings,f=this._coordinates.length;if(e.loop)f=this._clones.length/2+this._items.length-1;else if(e.autoWidth||e.merge){if(b=this._items.length)for(c=this._items[--b].width(),d=this.$element.width();b--&&!((c+=this._items[b].width()+this.settings.margin)>d););f=b+1}else f=e.center?this._items.length-1:this._items.length-e.items;return a&&(f-=this._clones.length/2),Math.max(f,0)},e.prototype.minimum=function(a){return a?0:this._clones.length/2},e.prototype.items=function(a){return a===d?this._items.slice():(a=this.normalize(a,!0),this._items[a])},e.prototype.mergers=function(a){return a===d?this._mergers.slice():(a=this.normalize(a,!0),this._mergers[a])},e.prototype.clones=function(b){var c=this._clones.length/2,e=c+this._items.length,f=function(a){return a%2==0?e+a/2:c-(a+1)/2};return b===d?a.map(this._clones,function(a,b){return f(b)}):a.map(this._clones,function(a,c){return a===b?f(c):null})},e.prototype.speed=function(a){return a!==d&&(this._speed=a),this._speed},e.prototype.coordinates=function(b){var c,e=1,f=b-1;return b===d?a.map(this._coordinates,a.proxy(function(a,b){return this.coordinates(b)},this)):(this.settings.center?(this.settings.rtl&&(e=-1,f=b+1),c=this._coordinates[b],c+=(this.width()-c+(this._coordinates[f]||0))/2*e):c=this._coordinates[f]||0,c=Math.ceil(c))},e.prototype.duration=function(a,b,c){return 0===c?0:Math.min(Math.max(Math.abs(b-a),1),6)*Math.abs(c||this.settings.smartSpeed)},e.prototype.to=function(a,b){var c=this.current(),d=null,e=a-this.relative(c),f=(e>0)-(e<0),g=this._items.length,h=this.minimum(),i=this.maximum();this.settings.loop?(!this.settings.rewind&&Math.abs(e)>g/2&&(e+=-1*f*g),a=c+e,(d=((a-h)%g+g)%g+h)!==a&&d-e<=i&&d-e>0&&(c=d-e,a=d,this.reset(c))):this.settings.rewind?(i+=1,a=(a%i+i)%i):a=Math.max(h,Math.min(i,a)),this.speed(this.duration(c,a,b)),this.current(a),this.isVisible()&&this.update()},e.prototype.next=function(a){a=a||!1,this.to(this.relative(this.current())+1,a)},e.prototype.prev=function(a){a=a||!1,this.to(this.relative(this.current())-1,a)},e.prototype.onTransitionEnd=function(a){if(a!==d&&(a.stopPropagation(),(a.target||a.srcElement||a.originalTarget)!==this.$stage.get(0)))return!1;this.leave("animating"),this.trigger("translated")},e.prototype.viewport=function(){var d;return this.options.responsiveBaseElement!==b?d=a(this.options.responsiveBaseElement).width():b.innerWidth?d=b.innerWidth:c.documentElement&&c.documentElement.clientWidth?d=c.documentElement.clientWidth:console.warn("Can not detect viewport width."),d},e.prototype.replace=function(b){this.$stage.empty(),this._items=[],b&&(b=b instanceof jQuery?b:a(b)),this.settings.nestedItemSelector&&(b=b.find("."+this.settings.nestedItemSelector)),b.filter(function(){return 1===this.nodeType}).each(a.proxy(function(a,b){b=this.prepare(b),this.$stage.append(b),this._items.push(b),this._mergers.push(1*b.find("[data-merge]").addBack("[data-merge]").attr("data-merge")||1)},this)),this.reset(this.isNumeric(this.settings.startPosition)?this.settings.startPosition:0),this.invalidate("items")},e.prototype.add=function(b,c){var e=this.relative(this._current);c=c===d?this._items.length:this.normalize(c,!0),b=b instanceof jQuery?b:a(b),this.trigger("add",{content:b,position:c}),b=this.prepare(b),0===this._items.length||c===this._items.length?(0===this._items.length&&this.$stage.append(b),0!==this._items.length&&this._items[c-1].after(b),this._items.push(b),this._mergers.push(1*b.find("[data-merge]").addBack("[data-merge]").attr("data-merge")||1)):(this._items[c].before(b),this._items.splice(c,0,b),this._mergers.splice(c,0,1*b.find("[data-merge]").addBack("[data-merge]").attr("data-merge")||1)),this._items[e]&&this.reset(this._items[e].index()),this.invalidate("items"),this.trigger("added",{content:b,position:c})},e.prototype.remove=function(a){(a=this.normalize(a,!0))!==d&&(this.trigger("remove",{content:this._items[a],position:a}),this._items[a].remove(),this._items.splice(a,1),this._mergers.splice(a,1),this.invalidate("items"),this.trigger("removed",{content:null,position:a}))},e.prototype.preloadAutoWidthImages=function(b){b.each(a.proxy(function(b,c){this.enter("pre-loading"),c=a(c),a(new Image).one("load",a.proxy(function(a){c.attr("src",a.target.src),c.css("opacity",1),this.leave("pre-loading"),!this.is("pre-loading")&&!this.is("initializing")&&this.refresh()},this)).attr("src",c.attr("src")||c.attr("data-src")||c.attr("data-src-retina"))},this))},e.prototype.destroy=function(){this.$element.off(".owl.core"),this.$stage.off(".owl.core"),a(c).off(".owl.core"),!1!==this.settings.responsive&&(b.clearTimeout(this.resizeTimer),this.off(b,"resize",this._handlers.onThrottledResize));for(var d in this._plugins)this._plugins[d].destroy();this.$stage.children(".cloned").remove(),this.$stage.unwrap(),this.$stage.children().contents().unwrap(),this.$stage.children().unwrap(),this.$stage.remove(),this.$element.removeClass(this.options.refreshClass).removeClass(this.options.loadingClass).removeClass(this.options.loadedClass).removeClass(this.options.rtlClass).removeClass(this.options.dragClass).removeClass(this.options.grabClass).attr("class",this.$element.attr("class").replace(new RegExp(this.options.responsiveClass+"-\\S+\\s","g"),"")).removeData("owl.carousel")},e.prototype.op=function(a,b,c){var d=this.settings.rtl;switch(b){case"<":return d?a>c:a<c;case">":return d?a<c:a>c;case">=":return d?a<=c:a>=c;case"<=":return d?a>=c:a<=c}},e.prototype.on=function(a,b,c,d){a.addEventListener?a.addEventListener(b,c,d):a.attachEvent&&a.attachEvent("on"+b,c)},e.prototype.off=function(a,b,c,d){a.removeEventListener?a.removeEventListener(b,c,d):a.detachEvent&&a.detachEvent("on"+b,c)},e.prototype.trigger=function(b,c,d,f,g){var h={item:{count:this._items.length,index:this.current()}},i=a.camelCase(a.grep(["on",b,d],function(a){return a}).join("-").toLowerCase()),j=a.Event([b,"owl",d||"carousel"].join(".").toLowerCase(),a.extend({relatedTarget:this},h,c));return this._supress[b]||(a.each(this._plugins,function(a,b){b.onTrigger&&b.onTrigger(j)}),this.register({type:e.Type.Event,name:b}),this.$element.trigger(j),this.settings&&"function"==typeof this.settings[i]&&this.settings[i].call(this,j)),j},e.prototype.enter=function(b){a.each([b].concat(this._states.tags[b]||[]),a.proxy(function(a,b){this._states.current[b]===d&&(this._states.current[b]=0),this._states.current[b]++},this))},e.prototype.leave=function(b){a.each([b].concat(this._states.tags[b]||[]),a.proxy(function(a,b){this._states.current[b]--},this))},e.prototype.register=function(b){if(b.type===e.Type.Event){if(a.event.special[b.name]||(a.event.special[b.name]={}),!a.event.special[b.name].owl){var c=a.event.special[b.name]._default;a.event.special[b.name]._default=function(a){return!c||!c.apply||a.namespace&&-1!==a.namespace.indexOf("owl")?a.namespace&&a.namespace.indexOf("owl")>-1:c.apply(this,arguments)},a.event.special[b.name].owl=!0}}else b.type===e.Type.State&&(this._states.tags[b.name]?this._states.tags[b.name]=this._states.tags[b.name].concat(b.tags):this._states.tags[b.name]=b.tags,this._states.tags[b.name]=a.grep(this._states.tags[b.name],a.proxy(function(c,d){return a.inArray(c,this._states.tags[b.name])===d},this)))},e.prototype.suppress=function(b){a.each(b,a.proxy(function(a,b){this._supress[b]=!0},this))},e.prototype.release=function(b){a.each(b,a.proxy(function(a,b){delete this._supress[b]},this))},e.prototype.pointer=function(a){var c={x:null,y:null};return a=a.originalEvent||a||b.event,a=a.touches&&a.touches.length?a.touches[0]:a.changedTouches&&a.changedTouches.length?a.changedTouches[0]:a,a.pageX?(c.x=a.pageX,c.y=a.pageY):(c.x=a.clientX,c.y=a.clientY),c},e.prototype.isNumeric=function(a){return!isNaN(parseFloat(a))},e.prototype.difference=function(a,b){return{x:a.x-b.x,y:a.y-b.y}},a.fn.owlCarousel=function(b){var c=Array.prototype.slice.call(arguments,1);return this.each(function(){var d=a(this),f=d.data("owl.carousel");f||(f=new e(this,"object"==typeof b&&b),d.data("owl.carousel",f),a.each(["next","prev","to","destroy","refresh","replace","add","remove"],function(b,c){f.register({type:e.Type.Event,name:c}),f.$element.on(c+".owl.carousel.core",a.proxy(function(a){a.namespace&&a.relatedTarget!==this&&(this.suppress([c]),f[c].apply(this,[].slice.call(arguments,1)),this.release([c]))},f))})),"string"==typeof b&&"_"!==b.charAt(0)&&f[b].apply(f,c)})},a.fn.owlCarousel.Constructor=e}(window.Zepto||window.jQuery,window,document),function(a,b,c,d){var e=function(b){this._core=b,this._interval=null,this._visible=null,this._handlers={"initialized.owl.carousel":a.proxy(function(a){a.namespace&&this._core.settings.autoRefresh&&this.watch()},this)},this._core.options=a.extend({},e.Defaults,this._core.options),this._core.$element.on(this._handlers)};e.Defaults={autoRefresh:!0,autoRefreshInterval:500},e.prototype.watch=function(){this._interval||(this._visible=this._core.isVisible(),this._interval=b.setInterval(a.proxy(this.refresh,this),this._core.settings.autoRefreshInterval))},e.prototype.refresh=function(){this._core.isVisible()!==this._visible&&(this._visible=!this._visible,this._core.$element.toggleClass("owl-hidden",!this._visible),this._visible&&this._core.invalidate("width")&&this._core.refresh())},e.prototype.destroy=function(){var a,c;b.clearInterval(this._interval);for(a in this._handlers)this._core.$element.off(a,this._handlers[a]);for(c in Object.getOwnPropertyNames(this))"function"!=typeof this[c]&&(this[c]=null)},a.fn.owlCarousel.Constructor.Plugins.AutoRefresh=e}(window.Zepto||window.jQuery,window,document),function(a,b,c,d){var e=function(b){this._core=b,this._loaded=[],this._handlers={"initialized.owl.carousel change.owl.carousel resized.owl.carousel":a.proxy(function(b){if(b.namespace&&this._core.settings&&this._core.settings.lazyLoad&&(b.property&&"position"==b.property.name||"initialized"==b.type)){var c=this._core.settings,e=c.center&&Math.ceil(c.items/2)||c.items,f=c.center&&-1*e||0,g=(b.property&&b.property.value!==d?b.property.value:this._core.current())+f,h=this._core.clones().length,i=a.proxy(function(a,b){this.load(b)},this);for(c.lazyLoadEager>0&&(e+=c.lazyLoadEager,c.loop&&(g-=c.lazyLoadEager,e++));f++<e;)this.load(h/2+this._core.relative(g)),h&&a.each(this._core.clones(this._core.relative(g)),i),g++}},this)},this._core.options=a.extend({},e.Defaults,this._core.options),this._core.$element.on(this._handlers)};e.Defaults={lazyLoad:!1,lazyLoadEager:0},e.prototype.load=function(c){var d=this._core.$stage.children().eq(c),e=d&&d.find(".owl-lazy");!e||a.inArray(d.get(0),this._loaded)>-1||(e.each(a.proxy(function(c,d){var e,f=a(d),g=b.devicePixelRatio>1&&f.attr("data-src-retina")||f.attr("data-src")||f.attr("data-srcset");this._core.trigger("load",{element:f,url:g},"lazy"),f.is("img")?f.one("load.owl.lazy",a.proxy(function(){f.css("opacity",1),this._core.trigger("loaded",{element:f,url:g},"lazy")},this)).attr("src",g):f.is("source")?f.one("load.owl.lazy",a.proxy(function(){this._core.trigger("loaded",{element:f,url:g},"lazy")},this)).attr("srcset",g):(e=new Image,e.onload=a.proxy(function(){f.css({"background-image":'url("'+g+'")',opacity:"1"}),this._core.trigger("loaded",{element:f,url:g},"lazy")},this),e.src=g)},this)),this._loaded.push(d.get(0)))},e.prototype.destroy=function(){var a,b;for(a in this.handlers)this._core.$element.off(a,this.handlers[a]);for(b in Object.getOwnPropertyNames(this))"function"!=typeof this[b]&&(this[b]=null)},a.fn.owlCarousel.Constructor.Plugins.Lazy=e}(window.Zepto||window.jQuery,window,document),function(a,b,c,d){var e=function(c){this._core=c,this._previousHeight=null,this._handlers={"initialized.owl.carousel refreshed.owl.carousel":a.proxy(function(a){a.namespace&&this._core.settings.autoHeight&&this.update()},this),"changed.owl.carousel":a.proxy(function(a){a.namespace&&this._core.settings.autoHeight&&"position"===a.property.name&&this.update()},this),"loaded.owl.lazy":a.proxy(function(a){a.namespace&&this._core.settings.autoHeight&&a.element.closest("."+this._core.settings.itemClass).index()===this._core.current()&&this.update()},this)},this._core.options=a.extend({},e.Defaults,this._core.options),this._core.$element.on(this._handlers),this._intervalId=null;var d=this;a(b).on("load",function(){d._core.settings.autoHeight&&d.update()}),a(b).resize(function(){d._core.settings.autoHeight&&(null!=d._intervalId&&clearTimeout(d._intervalId),d._intervalId=setTimeout(function(){d.update()},250))})};e.Defaults={autoHeight:!1,autoHeightClass:"owl-height"},e.prototype.update=function(){var b=this._core._current,c=b+this._core.settings.items,d=this._core.settings.lazyLoad,e=this._core.$stage.children().toArray().slice(b,c),f=[],g=0;a.each(e,function(b,c){f.push(a(c).height())}),g=Math.max.apply(null,f),g<=1&&d&&this._previousHeight&&(g=this._previousHeight),this._previousHeight=g,this._core.$stage.parent().height(g).addClass(this._core.settings.autoHeightClass)},e.prototype.destroy=function(){var a,b;for(a in this._handlers)this._core.$element.off(a,this._handlers[a]);for(b in Object.getOwnPropertyNames(this))"function"!=typeof this[b]&&(this[b]=null)},a.fn.owlCarousel.Constructor.Plugins.AutoHeight=e}(window.Zepto||window.jQuery,window,document),function(a,b,c,d){var e=function(b){this._core=b,this._videos={},this._playing=null,this._handlers={"initialized.owl.carousel":a.proxy(function(a){a.namespace&&this._core.register({type:"state",name:"playing",tags:["interacting"]})},this),"resize.owl.carousel":a.proxy(function(a){a.namespace&&this._core.settings.video&&this.isInFullScreen()&&a.preventDefault()},this),"refreshed.owl.carousel":a.proxy(function(a){a.namespace&&this._core.is("resizing")&&this._core.$stage.find(".cloned .owl-video-frame").remove()},this),"changed.owl.carousel":a.proxy(function(a){a.namespace&&"position"===a.property.name&&this._playing&&this.stop()},this),"prepared.owl.carousel":a.proxy(function(b){if(b.namespace){var c=a(b.content).find(".owl-video");c.length&&(c.css("display","none"),this.fetch(c,a(b.content)))}},this)},this._core.options=a.extend({},e.Defaults,this._core.options),this._core.$element.on(this._handlers),this._core.$element.on("click.owl.video",".owl-video-play-icon",a.proxy(function(a){this.play(a)},this))};e.Defaults={video:!1,videoHeight:!1,videoWidth:!1},e.prototype.fetch=function(a,b){var c=function(){return a.attr("data-vimeo-id")?"vimeo":a.attr("data-vzaar-id")?"vzaar":"youtube"}(),d=a.attr("data-vimeo-id")||a.attr("data-youtube-id")||a.attr("data-vzaar-id"),e=a.attr("data-width")||this._core.settings.videoWidth,f=a.attr("data-height")||this._core.settings.videoHeight,g=a.attr("href");if(!g)throw new Error("Missing video URL.");if(d=g.match(/(http:|https:|)\/\/(player.|www.|app.)?(vimeo\.com|youtu(be\.com|\.be|be\.googleapis\.com|be\-nocookie\.com)|vzaar\.com)\/(video\/|videos\/|embed\/|channels\/.+\/|groups\/.+\/|watch\?v=|v\/)?([A-Za-z0-9._%-]*)(\&\S+)?/),d[3].indexOf("youtu")>-1)c="youtube";else if(d[3].indexOf("vimeo")>-1)c="vimeo";else{if(!(d[3].indexOf("vzaar")>-1))throw new Error("Video URL not supported.");c="vzaar"}d=d[6],this._videos[g]={type:c,id:d,width:e,height:f},b.attr("data-video",g),this.thumbnail(a,this._videos[g])},e.prototype.thumbnail=function(b,c){var d,e,f,g=c.width&&c.height?"width:"+c.width+"px;height:"+c.height+"px;":"",h=b.find("img"),i="src",j="",k=this._core.settings,l=function(c){e='<div class="owl-video-play-icon"></div>',d=k.lazyLoad?a("<div/>",{class:"owl-video-tn "+j,srcType:c}):a("<div/>",{class:"owl-video-tn",style:"opacity:1;background-image:url("+c+")"}),b.after(d),b.after(e)};if(b.wrap(a("<div/>",{class:"owl-video-wrapper",style:g})),this._core.settings.lazyLoad&&(i="data-src",j="owl-lazy"),h.length)return l(h.attr(i)),h.remove(),!1;"youtube"===c.type?(f="//img.youtube.com/vi/"+c.id+"/hqdefault.jpg",l(f)):"vimeo"===c.type?a.ajax({type:"GET",url:"//vimeo.com/api/v2/video/"+c.id+".json",jsonp:"callback",dataType:"jsonp",success:function(a){f=a[0].thumbnail_large,l(f)}}):"vzaar"===c.type&&a.ajax({type:"GET",url:"//vzaar.com/api/videos/"+c.id+".json",jsonp:"callback",dataType:"jsonp",success:function(a){f=a.framegrab_url,l(f)}})},e.prototype.stop=function(){this._core.trigger("stop",null,"video"),this._playing.find(".owl-video-frame").remove(),this._playing.removeClass("owl-video-playing"),this._playing=null,this._core.leave("playing"),this._core.trigger("stopped",null,"video")},e.prototype.play=function(b){var c,d=a(b.target),e=d.closest("."+this._core.settings.itemClass),f=this._videos[e.attr("data-video")],g=f.width||"100%",h=f.height||this._core.$stage.height();this._playing||(this._core.enter("playing"),this._core.trigger("play",null,"video"),e=this._core.items(this._core.relative(e.index())),this._core.reset(e.index()),c=a('<iframe frameborder="0" allowfullscreen mozallowfullscreen webkitAllowFullScreen ></iframe>'),c.attr("height",h),c.attr("width",g),"youtube"===f.type?c.attr("src","//www.youtube.com/embed/"+f.id+"?autoplay=1&rel=0&v="+f.id):"vimeo"===f.type?c.attr("src","//player.vimeo.com/video/"+f.id+"?autoplay=1"):"vzaar"===f.type&&c.attr("src","//view.vzaar.com/"+f.id+"/player?autoplay=true"),a(c).wrap('<div class="owl-video-frame" />').insertAfter(e.find(".owl-video")),this._playing=e.addClass("owl-video-playing"))},e.prototype.isInFullScreen=function(){var b=c.fullscreenElement||c.mozFullScreenElement||c.webkitFullscreenElement;return b&&a(b).parent().hasClass("owl-video-frame")},e.prototype.destroy=function(){var a,b;this._core.$element.off("click.owl.video");for(a in this._handlers)this._core.$element.off(a,this._handlers[a]);for(b in Object.getOwnPropertyNames(this))"function"!=typeof this[b]&&(this[b]=null)},a.fn.owlCarousel.Constructor.Plugins.Video=e}(window.Zepto||window.jQuery,window,document),function(a,b,c,d){var e=function(b){this.core=b,this.core.options=a.extend({},e.Defaults,this.core.options),this.swapping=!0,this.previous=d,this.next=d,this.handlers={"change.owl.carousel":a.proxy(function(a){a.namespace&&"position"==a.property.name&&(this.previous=this.core.current(),this.next=a.property.value)},this),"drag.owl.carousel dragged.owl.carousel translated.owl.carousel":a.proxy(function(a){a.namespace&&(this.swapping="translated"==a.type)},this),"translate.owl.carousel":a.proxy(function(a){a.namespace&&this.swapping&&(this.core.options.animateOut||this.core.options.animateIn)&&this.swap()},this)},this.core.$element.on(this.handlers)};e.Defaults={animateOut:!1,
animateIn:!1},e.prototype.swap=function(){if(1===this.core.settings.items&&a.support.animation&&a.support.transition){this.core.speed(0);var b,c=a.proxy(this.clear,this),d=this.core.$stage.children().eq(this.previous),e=this.core.$stage.children().eq(this.next),f=this.core.settings.animateIn,g=this.core.settings.animateOut;this.core.current()!==this.previous&&(g&&(b=this.core.coordinates(this.previous)-this.core.coordinates(this.next),d.one(a.support.animation.end,c).css({left:b+"px"}).addClass("animated owl-animated-out").addClass(g)),f&&e.one(a.support.animation.end,c).addClass("animated owl-animated-in").addClass(f))}},e.prototype.clear=function(b){a(b.target).css({left:""}).removeClass("animated owl-animated-out owl-animated-in").removeClass(this.core.settings.animateIn).removeClass(this.core.settings.animateOut),this.core.onTransitionEnd()},e.prototype.destroy=function(){var a,b;for(a in this.handlers)this.core.$element.off(a,this.handlers[a]);for(b in Object.getOwnPropertyNames(this))"function"!=typeof this[b]&&(this[b]=null)},a.fn.owlCarousel.Constructor.Plugins.Animate=e}(window.Zepto||window.jQuery,window,document),function(a,b,c,d){var e=function(b){this._core=b,this._call=null,this._time=0,this._timeout=0,this._paused=!0,this._handlers={"changed.owl.carousel":a.proxy(function(a){a.namespace&&"settings"===a.property.name?this._core.settings.autoplay?this.play():this.stop():a.namespace&&"position"===a.property.name&&this._paused&&(this._time=0)},this),"initialized.owl.carousel":a.proxy(function(a){a.namespace&&this._core.settings.autoplay&&this.play()},this),"play.owl.autoplay":a.proxy(function(a,b,c){a.namespace&&this.play(b,c)},this),"stop.owl.autoplay":a.proxy(function(a){a.namespace&&this.stop()},this),"mouseover.owl.autoplay":a.proxy(function(){this._core.settings.autoplayHoverPause&&this._core.is("rotating")&&this.pause()},this),"mouseleave.owl.autoplay":a.proxy(function(){this._core.settings.autoplayHoverPause&&this._core.is("rotating")&&this.play()},this),"touchstart.owl.core":a.proxy(function(){this._core.settings.autoplayHoverPause&&this._core.is("rotating")&&this.pause()},this),"touchend.owl.core":a.proxy(function(){this._core.settings.autoplayHoverPause&&this.play()},this)},this._core.$element.on(this._handlers),this._core.options=a.extend({},e.Defaults,this._core.options)};e.Defaults={autoplay:!1,autoplayTimeout:5e3,autoplayHoverPause:!1,autoplaySpeed:!1},e.prototype._next=function(d){this._call=b.setTimeout(a.proxy(this._next,this,d),this._timeout*(Math.round(this.read()/this._timeout)+1)-this.read()),this._core.is("interacting")||c.hidden||this._core.next(d||this._core.settings.autoplaySpeed)},e.prototype.read=function(){return(new Date).getTime()-this._time},e.prototype.play=function(c,d){var e;this._core.is("rotating")||this._core.enter("rotating"),c=c||this._core.settings.autoplayTimeout,e=Math.min(this._time%(this._timeout||c),c),this._paused?(this._time=this.read(),this._paused=!1):b.clearTimeout(this._call),this._time+=this.read()%c-e,this._timeout=c,this._call=b.setTimeout(a.proxy(this._next,this,d),c-e)},e.prototype.stop=function(){this._core.is("rotating")&&(this._time=0,this._paused=!0,b.clearTimeout(this._call),this._core.leave("rotating"))},e.prototype.pause=function(){this._core.is("rotating")&&!this._paused&&(this._time=this.read(),this._paused=!0,b.clearTimeout(this._call))},e.prototype.destroy=function(){var a,b;this.stop();for(a in this._handlers)this._core.$element.off(a,this._handlers[a]);for(b in Object.getOwnPropertyNames(this))"function"!=typeof this[b]&&(this[b]=null)},a.fn.owlCarousel.Constructor.Plugins.autoplay=e}(window.Zepto||window.jQuery,window,document),function(a,b,c,d){"use strict";var e=function(b){this._core=b,this._initialized=!1,this._pages=[],this._controls={},this._templates=[],this.$element=this._core.$element,this._overrides={next:this._core.next,prev:this._core.prev,to:this._core.to},this._handlers={"prepared.owl.carousel":a.proxy(function(b){b.namespace&&this._core.settings.dotsData&&this._templates.push('<div class="'+this._core.settings.dotClass+'">'+a(b.content).find("[data-dot]").addBack("[data-dot]").attr("data-dot")+"</div>")},this),"added.owl.carousel":a.proxy(function(a){a.namespace&&this._core.settings.dotsData&&this._templates.splice(a.position,0,this._templates.pop())},this),"remove.owl.carousel":a.proxy(function(a){a.namespace&&this._core.settings.dotsData&&this._templates.splice(a.position,1)},this),"changed.owl.carousel":a.proxy(function(a){a.namespace&&"position"==a.property.name&&this.draw()},this),"initialized.owl.carousel":a.proxy(function(a){a.namespace&&!this._initialized&&(this._core.trigger("initialize",null,"navigation"),this.initialize(),this.update(),this.draw(),this._initialized=!0,this._core.trigger("initialized",null,"navigation"))},this),"refreshed.owl.carousel":a.proxy(function(a){a.namespace&&this._initialized&&(this._core.trigger("refresh",null,"navigation"),this.update(),this.draw(),this._core.trigger("refreshed",null,"navigation"))},this)},this._core.options=a.extend({},e.Defaults,this._core.options),this.$element.on(this._handlers)};e.Defaults={nav:!1,navText:['<span aria-label="Previous">&#x2039;</span>','<span aria-label="Next">&#x203a;</span>'],navSpeed:!1,navElement:'button type="button" role="presentation"',navContainer:!1,navContainerClass:"owl-nav",navClass:["owl-prev","owl-next"],slideBy:1,dotClass:"owl-dot",dotsClass:"owl-dots",dots:!0,dotsEach:!1,dotsData:!1,dotsSpeed:!1,dotsContainer:!1},e.prototype.initialize=function(){var b,c=this._core.settings;this._controls.$relative=(c.navContainer?a(c.navContainer):a("<div>").addClass(c.navContainerClass).appendTo(this.$element)).addClass("disabled"),this._controls.$previous=a("<"+c.navElement+">").addClass(c.navClass[0]).html(c.navText[0]).prependTo(this._controls.$relative).on("click",a.proxy(function(a){this.prev(c.navSpeed)},this)),this._controls.$next=a("<"+c.navElement+">").addClass(c.navClass[1]).html(c.navText[1]).appendTo(this._controls.$relative).on("click",a.proxy(function(a){this.next(c.navSpeed)},this)),c.dotsData||(this._templates=[a('<button role="button">').addClass(c.dotClass).append(a("<span>")).prop("outerHTML")]),this._controls.$absolute=(c.dotsContainer?a(c.dotsContainer):a("<div>").addClass(c.dotsClass).appendTo(this.$element)).addClass("disabled"),this._controls.$absolute.on("click","button",a.proxy(function(b){var d=a(b.target).parent().is(this._controls.$absolute)?a(b.target).index():a(b.target).parent().index();b.preventDefault(),this.to(d,c.dotsSpeed)},this));for(b in this._overrides)this._core[b]=a.proxy(this[b],this)},e.prototype.destroy=function(){var a,b,c,d,e;e=this._core.settings;for(a in this._handlers)this.$element.off(a,this._handlers[a]);for(b in this._controls)"$relative"===b&&e.navContainer?this._controls[b].html(""):this._controls[b].remove();for(d in this.overides)this._core[d]=this._overrides[d];for(c in Object.getOwnPropertyNames(this))"function"!=typeof this[c]&&(this[c]=null)},e.prototype.update=function(){var a,b,c,d=this._core.clones().length/2,e=d+this._core.items().length,f=this._core.maximum(!0),g=this._core.settings,h=g.center||g.autoWidth||g.dotsData?1:g.dotsEach||g.items;if("page"!==g.slideBy&&(g.slideBy=Math.min(g.slideBy,g.items)),g.dots||"page"==g.slideBy)for(this._pages=[],a=d,b=0,c=0;a<e;a++){if(b>=h||0===b){if(this._pages.push({start:Math.min(f,a-d),end:a-d+h-1}),Math.min(f,a-d)===f)break;b=0,++c}b+=this._core.mergers(this._core.relative(a))}},e.prototype.draw=function(){var b,c=this._core.settings,d=this._core.items().length<=c.items,e=this._core.relative(this._core.current()),f=c.loop||c.rewind;this._controls.$relative.toggleClass("disabled",!c.nav||d),c.nav&&(this._controls.$previous.toggleClass("disabled",!f&&e<=this._core.minimum(!0)),this._controls.$next.toggleClass("disabled",!f&&e>=this._core.maximum(!0))),this._controls.$absolute.toggleClass("disabled",!c.dots||d),c.dots&&(b=this._pages.length-this._controls.$absolute.children().length,c.dotsData&&0!==b?this._controls.$absolute.html(this._templates.join("")):b>0?this._controls.$absolute.append(new Array(b+1).join(this._templates[0])):b<0&&this._controls.$absolute.children().slice(b).remove(),this._controls.$absolute.find(".active").removeClass("active"),this._controls.$absolute.children().eq(a.inArray(this.current(),this._pages)).addClass("active"))},e.prototype.onTrigger=function(b){var c=this._core.settings;b.page={index:a.inArray(this.current(),this._pages),count:this._pages.length,size:c&&(c.center||c.autoWidth||c.dotsData?1:c.dotsEach||c.items)}},e.prototype.current=function(){var b=this._core.relative(this._core.current());return a.grep(this._pages,a.proxy(function(a,c){return a.start<=b&&a.end>=b},this)).pop()},e.prototype.getPosition=function(b){var c,d,e=this._core.settings;return"page"==e.slideBy?(c=a.inArray(this.current(),this._pages),d=this._pages.length,b?++c:--c,c=this._pages[(c%d+d)%d].start):(c=this._core.relative(this._core.current()),d=this._core.items().length,b?c+=e.slideBy:c-=e.slideBy),c},e.prototype.next=function(b){a.proxy(this._overrides.to,this._core)(this.getPosition(!0),b)},e.prototype.prev=function(b){a.proxy(this._overrides.to,this._core)(this.getPosition(!1),b)},e.prototype.to=function(b,c,d){var e;!d&&this._pages.length?(e=this._pages.length,a.proxy(this._overrides.to,this._core)(this._pages[(b%e+e)%e].start,c)):a.proxy(this._overrides.to,this._core)(b,c)},a.fn.owlCarousel.Constructor.Plugins.Navigation=e}(window.Zepto||window.jQuery,window,document),function(a,b,c,d){"use strict";var e=function(c){this._core=c,this._hashes={},this.$element=this._core.$element,this._handlers={"initialized.owl.carousel":a.proxy(function(c){c.namespace&&"URLHash"===this._core.settings.startPosition&&a(b).trigger("hashchange.owl.navigation")},this),"prepared.owl.carousel":a.proxy(function(b){if(b.namespace){var c=a(b.content).find("[data-hash]").addBack("[data-hash]").attr("data-hash");if(!c)return;this._hashes[c]=b.content}},this),"changed.owl.carousel":a.proxy(function(c){if(c.namespace&&"position"===c.property.name){var d=this._core.items(this._core.relative(this._core.current())),e=a.map(this._hashes,function(a,b){return a===d?b:null}).join();if(!e||b.location.hash.slice(1)===e)return;b.location.hash=e}},this)},this._core.options=a.extend({},e.Defaults,this._core.options),this.$element.on(this._handlers),a(b).on("hashchange.owl.navigation",a.proxy(function(a){var c=b.location.hash.substring(1),e=this._core.$stage.children(),f=this._hashes[c]&&e.index(this._hashes[c]);f!==d&&f!==this._core.current()&&this._core.to(this._core.relative(f),!1,!0)},this))};e.Defaults={URLhashListener:!1},e.prototype.destroy=function(){var c,d;a(b).off("hashchange.owl.navigation");for(c in this._handlers)this._core.$element.off(c,this._handlers[c]);for(d in Object.getOwnPropertyNames(this))"function"!=typeof this[d]&&(this[d]=null)},a.fn.owlCarousel.Constructor.Plugins.Hash=e}(window.Zepto||window.jQuery,window,document),function(a,b,c,d){function e(b,c){var e=!1,f=b.charAt(0).toUpperCase()+b.slice(1);return a.each((b+" "+h.join(f+" ")+f).split(" "),function(a,b){if(g[b]!==d)return e=!c||b,!1}),e}function f(a){return e(a,!0)}var g=a("<support>").get(0).style,h="Webkit Moz O ms".split(" "),i={transition:{end:{WebkitTransition:"webkitTransitionEnd",MozTransition:"transitionend",OTransition:"oTransitionEnd",transition:"transitionend"}},animation:{end:{WebkitAnimation:"webkitAnimationEnd",MozAnimation:"animationend",OAnimation:"oAnimationEnd",animation:"animationend"}}},j={csstransforms:function(){return!!e("transform")},csstransforms3d:function(){return!!e("perspective")},csstransitions:function(){return!!e("transition")},cssanimations:function(){return!!e("animation")}};j.csstransitions()&&(a.support.transition=new String(f("transition")),a.support.transition.end=i.transition.end[a.support.transition]),j.cssanimations()&&(a.support.animation=new String(f("animation")),a.support.animation.end=i.animation.end[a.support.animation]),j.csstransforms()&&(a.support.transform=new String(f("transform")),a.support.transform3d=j.csstransforms3d())}(window.Zepto||window.jQuery,window,document);(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    $(function() {
      var $this ,$scroll;
      var $articleContent = $('.js-article-content');
      var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');
      var scroll = hasSidebar ? '.js-page-main' : 'html, body';
      $scroll = $(scroll);

      $articleContent.find('.highlight').each(function() {
        $this = $(this);
        $this.attr('data-lang', $this.find('code').attr('data-lang'));
      });
      $articleContent.find('h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]').each(function() {
        $this = $(this);
        $this.append($('<a class="anchor d-print-none" aria-hidden="true"></a>').html('<i class="fas fa-anchor"></i>'));
      });
      $articleContent.on('click', '.anchor', function() {
        $scroll.scrollToAnchor('#' + $(this).parent().attr('id'), 400);
      });
    });
  });
})();

$(document).ready(function () {

  try{
        /* Versions Pagination*/
      $('.pagination_big').owlCarousel({
        margin:10,
        nav:true,
        dots:false,
        responsive:{
            0:{
                items:3
            },
            400:{
                items:4
            },
            500:{
                items:6
            },
            1600:{
                items:11
            }
        }
    });
  } catch(e){}

});

</script></div><section class="page__comments d-print-none"></section></article><!-- start custom main bottom snippet -->

<!-- end custom main bottom snippet --></div>
            </div></div></div><div class="page__footer d-print-none">
<footer class="footer py-4 js-page-footer">
  <div class="main"><div itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content=""><meta itemprop="url" content="/"></div><div class="site-info mt-2">
      <div>© <span id="year"></span> John Snow Labs Inc.
        <a href="http://www.johnsnowlabs.com/terms-of-service">Terms of Service</a> | <a href="http://www.johnsnowlabs.com/privacy-policy/">Privacy Policy</a>
      </div>
    </div>
  </div>
</footer>

<script>

/* Responsive menu
	 ========================================================*/
jQuery(document).ready(function($) {
	jQuery('#responsive_menu').click(function(e) {
      e.preventDefault();
      jQuery(this).toggleClass('close');
      jQuery('.top_navigation').toggleClass('open');
  });
  jQuery('#aside_menu').click(function(e) {
      e.preventDefault();
      jQuery(this).toggleClass('close');
      jQuery('.js-col-aside').toggleClass('open');
      if (jQuery(window).width() <= 1023)
      {
        jQuery('.page__sidebar').toggleClass('open'); 
      jQuery('.demomenu').toggleClass('open');
      }
  });
  jQuery('.toc--ellipsis a').click(function(e) {
    if (jQuery(window).width() <= 767)
      {
        jQuery('.js-col-aside').removeClass('open');
        jQuery('.page__sidebar').removeClass('open');    
        jQuery('#aside_menu').removeClass('close');  
      }       
  });
});

/*OPen by URL*/
jQuery(document).ready(function () {  
  const tabName = (window.location.hash || '').replace('#', '');
  const tab = document.getElementById(tabName || 'opensource');
  if (tab) {
    tab.click();
  }
});

try{
  //Accordion demos categories
  let acc = document.getElementsByClassName("acc-top"),
    isResizeble = false;

  if(!isResizeble && document.querySelector(".acc-top")) {
      let accBody = document.querySelector('.acc-body li.active');
      accBody.parentElement.style.maxHeight = accBody.parentElement.scrollHeight + 20 + "px";
      accBody.parentElement.classList.add('open');
      accBody.parentElement.previousElementSibling.classList.add('active');
      isResizeble = true;
  }

for (let i = 0; i < acc.length; i++) {
  acc[i].addEventListener("click", function() {
    this.classList.toggle("active");
    var panel = this.nextElementSibling;
    if (panel.style.maxHeight) {
      panel.style.maxHeight = null;
      panel.classList.remove('open');
    } else {
      panel.style.maxHeight = panel.scrollHeight + 20 + "px";
      panel.classList.add('open');
    }
  });
}
} catch(e){}


try {
  //Show more in demos description
  let tabDescription = document.querySelectorAll('.tab-description');

  tabDescription.forEach(element => {
    let tabDescriptionInner = element.querySelector('.tab-description-inner');
    if(element.offsetHeight < tabDescriptionInner.offsetHeight) {
      element.classList.add('big-descr');
    }
  });

  let showMore = document.querySelectorAll('.show_more');

  showMore.forEach(element => {
    element.addEventListener("click", function(e) {
      e.preventDefault();
      this.parentElement.parentElement.classList.remove('big-descr');
      this.parentElement.parentElement.classList.add('big-descr-close');
    });
  });
} catch(e){}


try{
  //disable Colab link
  let btnDisable = document.querySelectorAll('.btn.disable');

  btnDisable.forEach(element => {
    element.addEventListener("click", function(e) {
      e.preventDefault();
    });
  });
} catch(e){}


try {
  // Ancor click
const anchors = [].slice.call(document.querySelectorAll('.btn-box-install a')),
animationTime = 300,
framesCount = 20;

anchors.forEach(function(item) {
item.addEventListener('click', function(e) {
  e.preventDefault();
  let coordY = document.querySelector(item.getAttribute('href')).getBoundingClientRect().top + window.pageYOffset -100;

  let scroller = setInterval(function() {
      let scrollBy = coordY / framesCount;

if(scrollBy > window.pageYOffset - coordY && window.innerHeight + window.pageYOffset < document.body.offsetHeight) {
    window.scrollBy(0, scrollBy);
} else {
          window.scrollTo(0, coordY);
  clearInterval(scroller);
}
  }, animationTime / framesCount);
});
}); 
} catch(e){}


try {
  //Pagination active
  let paginationItems = document.querySelectorAll('.pagination_big li'),
      nextVersionContainer = document.querySelector('#nextver'),
      previosVersionContainer = document.querySelector('#previosver'),
      currentVersionContainer = document.querySelector('#currversion'),
      currentPageTitle = document.querySelector('#section').innerText;

  // Set active page and update version containers
  for (let i = 0; i < paginationItems.length; i++) {
    const item = paginationItems[i];
    const itemTitle = item.firstElementChild.innerHTML;
    if (itemTitle === currentPageTitle) {
      item.classList.add('active');
      currentVersionContainer.textContent = itemTitle;       
      if(item.previousElementSibling) {
        previosVersionContainer.textContent = item.previousElementSibling.innerText; 
        previosVersionContainer.parentElement.href += item.previousElementSibling.innerText.replaceAll('.', '_');
      } else {
        previosVersionContainer.parentElement.parentElement.classList.add('hide');
      }
      if(item.nextElementSibling) {
        nextVersionContainer.textContent = item.nextElementSibling.innerText;
        nextVersionContainer.parentElement.href += item.nextElementSibling.innerText.replaceAll('.', '_');
      } else {
        nextVersionContainer.parentElement.parentElement.classList.add('hide');
      }         
      break;
    }
  }
} catch(e){}


try{
  // copy to clipboard
  let btnCopy = document.querySelectorAll('.button-copy-s3');

  btnCopy.forEach((element) => {
    //add span Copied!
    element.insertAdjacentHTML('beforeend', '<span>Copied!</span>');

    element.addEventListener('click', function (e) {
      e.preventDefault();
      element.classList.add('copied');
      setTimeout(function () {
        element.classList.remove('copied');
      }, 3000);
      navigator.clipboard.writeText(element.href);
    });
  });
} catch(e){}

document.getElementById("year").innerHTML = new Date().getFullYear();

</script></div></div>
    </div></div></div><script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $body = $('body'), $window = $(window);
    var $pageRoot = $('.js-page-root'), $pageMain = $('.js-page-main');
    var activeCount = 0;
    function modal(options) {
      var $root = this, visible, onChange, hideWhenWindowScroll = false;
      var scrollTop;
      function setOptions(options) {
        var _options = options || {};
        visible = _options.initialVisible === undefined ? false : show;
        onChange = _options.onChange;
        hideWhenWindowScroll = _options.hideWhenWindowScroll;
      }
      function init() {
        setState(visible);
      }
      function setState(isShow) {
        if (isShow === visible) {
          return;
        }
        visible = isShow;
        if (visible) {
          activeCount++;
          scrollTop = $(window).scrollTop() || $pageMain.scrollTop();
          $root.addClass('modal--show');
          $pageMain.scrollTop(scrollTop);
          activeCount === 1 && ($pageRoot.addClass('show-modal'), $body.addClass('of-hidden'));
          hideWhenWindowScroll && window.hasEvent('touchstart') && $window.on('scroll', hide);
          $window.on('keyup', handleKeyup);
        } else {
          activeCount > 0 && activeCount--;
          $root.removeClass('modal--show');
          $window.scrollTop(scrollTop);
          activeCount === 0 && ($pageRoot.removeClass('show-modal'), $body.removeClass('of-hidden'));
          hideWhenWindowScroll && window.hasEvent('touchstart') && $window.off('scroll', hide);
          $window.off('keyup', handleKeyup);
        }
        onChange && onChange(visible);
      }
      function show() {
        setState(true);
      }
      function hide() {
        setState(false);
      }
      function handleKeyup(e) {
        // Char Code: 27  ESC
        if (e.which ===  27) {
          hide();
        }
      }
      setOptions(options);
      init();
      return {
        show: show,
        hide: hide,
        $el: $root
      };
    }
    $.fn.modal = modal;
  });
})();
</script><div class="modal modal--overflow page__search-modal d-print-none js-page-search-modal"></div></div>


<script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function scrollToAnchor(anchor, duration, callback) {
      var $root = this;
      $root.animate({ scrollTop: $(anchor).position().top }, duration, function() {
        window.history.replaceState(null, '', window.location.href.split('#')[0] + anchor);
        callback && callback();
      });
    }
    $.fn.scrollToAnchor = scrollToAnchor;
  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function affix(options) {
      var $root = this, $window = $(window), $scrollTarget, $scroll,
        offsetBottom = 0, scrollTarget = window, scroll = window.document, disabled = false, isOverallScroller = true,
        rootTop, rootLeft, rootHeight, scrollBottom, rootBottomTop,
        hasInit = false, curState;

      function setOptions(options) {
        var _options = options || {};
        _options.offsetBottom && (offsetBottom = _options.offsetBottom);
        _options.scrollTarget && (scrollTarget = _options.scrollTarget);
        _options.scroll && (scroll = _options.scroll);
        _options.disabled !== undefined && (disabled = _options.disabled);
        $scrollTarget = $(scrollTarget);
        isOverallScroller = window.isOverallScroller($scrollTarget[0]);
        $scroll = $(scroll);
      }
      function preCalc() {
        top();
        rootHeight = $root.outerHeight();
        rootTop = $root.offset().top + (isOverallScroller ? 0 :  $scrollTarget.scrollTop());
        rootLeft = $root.offset().left;
      }
      function calc(needPreCalc) {
        needPreCalc && preCalc();
        scrollBottom = $scroll.outerHeight() - offsetBottom - rootHeight;
        rootBottomTop = scrollBottom - rootTop;
      }
      function top() {
        if (curState !== 'top') {
          $root.removeClass('fixed').css({
            left: 0,
            top: 0
          });
          curState = 'top';
        }
      }
      function fixed() {
        if (curState !== 'fixed') {
          $root.addClass('fixed').css({
            left: rootLeft + 'px',
            top: 0
          });
          curState = 'fixed';
        }
      }
      function bottom() {
        if (curState !== 'bottom') {
          $root.removeClass('fixed').css({
            left: 0,
            top: rootBottomTop + 'px'
          });
          curState = 'bottom';
        }
      }
      function setState() {
        var scrollTop = $scrollTarget.scrollTop();
        if (scrollTop >= rootTop && scrollTop <= scrollBottom) {
          fixed();
        } else if (scrollTop < rootTop) {
          top();
        } else {
          bottom();
        }
      }
      function init() {
        if(!hasInit) {
          var interval, timeout;
          calc(true); setState();
          // run calc every 100 millisecond
          interval = setInterval(function() {
            calc();
          }, 100);
          timeout = setTimeout(function() {
            clearInterval(interval);
          }, 45000);
          window.pageLoad.then(function() {
            setTimeout(function() {
              clearInterval(interval);
              clearTimeout(timeout);
            }, 3000);
          });
          $scrollTarget.on('scroll', function() {
            disabled || setState();
          });
          $window.on('resize', function() {
            disabled || (calc(true), setState());
          });
          hasInit = true;
        }
      }

      setOptions(options);
      if (!disabled) {
        init();
      }
      $window.on('resize', window.throttle(function() {
        init();
      }, 200));
      return {
        setOptions: setOptions,
        refresh: function() {
          calc(true, { animation: false }); setState();
        }
      };
    }
    $.fn.affix = affix;
  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function toc(options) {
      var $root = this, $window = $(window), $scrollTarget, $scroller, $tocUl = $('<ul class="toc toc--ellipsis"></ul>'), $tocLi, $headings, $activeLast, $activeCur,
        selectors = 'h1,h2,h3',  container = 'body', scrollTarget = window, scroller = 'html, body', disabled = false,
        headingsPos, scrolling = false, hasRendered = false, hasInit = false;

        

      function setOptions(options) {
        var _options = options || {};
        _options.selectors && (selectors = _options.selectors);
        _options.container && (container = _options.container);
        _options.scrollTarget && (scrollTarget = _options.scrollTarget);
        _options.scroller && (scroller = _options.scroller);
        _options.disabled !== undefined && (disabled = _options.disabled);
        $headings = $(container).find(selectors).filter('[id]');
        $scrollTarget = $(scrollTarget);
        $scroller = $(scroller);
      }
      function calc() {
        headingsPos = [];
        $headings.each(function() {
          headingsPos.push(Math.floor($(this).position().top));
        });
      }
      function setState(element, disabled) {
        var scrollTop = $scrollTarget.scrollTop(), i;
        if (disabled || !headingsPos || headingsPos.length < 1) { return; }
        if (element) {
          $activeCur = element;
        } else {
          for (i = 0; i < headingsPos.length; i++) {
            if (scrollTop >= headingsPos[i]) {
              $activeCur = $tocLi.eq(i);
            } else {
              $activeCur || ($activeCur = $tocLi.eq(i));
              break;
            }
          }
        }
        $activeLast && $activeLast.removeClass('active');
        ($activeLast = $activeCur).addClass('active');
      }
      function render() {
        if(!hasRendered) {
          $root.append($tocUl);
          $headings.each(function() {
            var $this = $(this);
            $tocUl.append($('<li></li>').addClass('toc-' + $this.prop('tagName')
              .toLowerCase() + ' ' + $this.prop('className'))
              .append($('<a></a>').text($this.text()).attr('href', '#' + $this.prop('id'))));
          });
          $tocLi = $tocUl.children('li');
          $tocUl.on('click', 'a', function(e) {
            e.preventDefault();
            var $this = $(this);
            scrolling = true;
            setState($this.parent());
            $scroller.scrollToAnchor($this.attr('href'), 400, function() {
              scrolling = false;
            });
          });
        }
        hasRendered = true;
      }
      function init() {
        var interval, timeout;
        if(!hasInit) {
          render(); calc(); setState(null, scrolling);
          // run calc every 100 millisecond
          interval = setInterval(function() {
            calc();
          }, 100);
          timeout = setTimeout(function() {
            clearInterval(interval);
          }, 45000);
          window.pageLoad.then(function() {
            setTimeout(function() {
              clearInterval(interval);
              clearTimeout(timeout);
            }, 3000);
          });
          $scrollTarget.on('scroll', function() {
            disabled || setState(null, scrolling);
          });
          $window.on('resize', window.throttle(function() {
            if (!disabled) {
              render(); calc(); setState(null, scrolling);
            }
          }, 100));
        }
        hasInit = true;
      }

      setOptions(options);
      if (!disabled) {
        init();
      }
      $window.on('resize', window.throttle(function() {
        init();
      }, 200));
      return {
        setOptions: setOptions
      };
    }
    $.fn.toc = toc;
  });
})();
/*(function () {

})();*/
</script>
<!-- Place this tag in your head or just before your close body tag. -->
<script async defer src="https://buttons.github.io/buttons.js"></script><script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;

  window.Lazyload.js(SOURCES.jquery, function() {
    var $pageMask = $('.js-page-mask');
    var $pageRoot = $('.js-page-root');
    var $sidebarShow = $('.js-sidebar-show');
    var $sidebarHide = $('.js-sidebar-hide');

    function freeze(e) {
      if (e.target === $pageMask[0]) {
        e.preventDefault();
      }
    }
    function stopBodyScrolling(bool) {
      if (bool === true) {
        window.addEventListener('touchmove', freeze, { passive: false });
      } else {
        window.removeEventListener('touchmove', freeze, { passive: false });
      }
    }

    $sidebarShow.on('click', function() {
      stopBodyScrolling(true); $pageRoot.addClass('show-sidebar');
    });
    $sidebarHide.on('click', function() {
      stopBodyScrolling(false); $pageRoot.removeClass('show-sidebar');
    });
  });
})();
</script><script>
  /* toc must before affix, since affix need to konw toc' height. */(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  var TOC_SELECTOR = window.TEXT_VARIABLES.site.toc.selectors;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $window = $(window);
    var $articleContent = $('.js-article-content');
    var $tocRoot = $('.js-toc-root'), $col2 = $('.js-col-aside');
    var toc;
    var tocDisabled = false;
    var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');
    var hasToc = $articleContent.find(TOC_SELECTOR).length > 0;

    function disabled() {
      return $col2.css('display') === 'none' || !hasToc;
    }

    tocDisabled = disabled();

    toc = $tocRoot.toc({
      selectors: TOC_SELECTOR,
      container: $articleContent,
      scrollTarget: hasSidebar ? '.js-page-main' : null,
      scroller: hasSidebar ? '.js-page-main' : null,
      disabled: tocDisabled
    });

    $window.on('resize', window.throttle(function() {
      tocDisabled = disabled();
      toc && toc.setOptions({
        disabled: tocDisabled
      });
    }, 100));

  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $window = $(window), $pageFooter = $('.js-page-footer');
    var $pageAside = $('.js-page-aside');
    var affix;
    var tocDisabled = false;
    var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');

    affix = $pageAside.affix({
      offsetBottom: $pageFooter.outerHeight(),
      scrollTarget: hasSidebar ? '.js-page-main' : null,
      scroller: hasSidebar ? '.js-page-main' : null,
      scroll: hasSidebar ? $('.js-page-main').children() : null,
      disabled: tocDisabled
    });

    $window.on('resize', window.throttle(function() {
      affix && affix.setOptions({
        disabled: tocDisabled
      });
    }, 100));

    window.pageAsideAffix = affix;
  });
})();
</script>
    </div>
    <script>(function () {
  var $root = document.getElementsByClassName('root')[0];
  if (window.hasEvent('touchstart')) {
    $root.dataset.isTouch = true;
    document.addEventListener('touchstart', function(){}, false);
  }
})();
</script>
  </body>
</html>