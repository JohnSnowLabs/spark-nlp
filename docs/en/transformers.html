<!DOCTYPE html><html lang="en">
  <head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-59JLR64');</script>
<!-- End Google Tag Manager --><title>Spark NLP</title><meta name="description" content="High Performance NLP with Apache Spark
">
<link rel="canonical" href="/docs/en/transformers"><link rel="alternate" type="application/rss+xml" title="Spark NLP" href="/feed.xml"><!-- start favicons snippet, use https://realfavicongenerator.net/ -->
<!---->
<!-- <link rel="apple-touch-icon" sizes="180x180" href="/fav.ico"> -->

<!---->
<!-- <link rel="icon" type="image/png" sizes="32x32" href="/fav.ico"> -->

<!---->
<!-- <link rel="icon" type="image/png" sizes="16x16" href="/fav.ico"> -->

<!---->
<!-- <link rel="manifest" href="/fav.ico"> --><link rel="mask-icon" href="/fav.ico" color="#fc4d50"><link rel="shortcut icon" href="/fav.ico">

<meta name="msapplication-TileColor" content="#ffc40d"><meta name="msapplication-config" content="/assets/browserconfig.xml">

<meta name="theme-color" content="#ffffff">
<!-- end favicons snippet --><link rel="stylesheet" href="/assets/css/main.css"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" ><!-- start custom head snippets -->
 <link href="https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700;800&display=swap" rel="stylesheet"> 
 <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
<!-- end custom head snippets -->
<script>(function() {
  window.isArray = function(val) {
    return Object.prototype.toString.call(val) === '[object Array]';
  };
  window.isString = function(val) {
    return typeof val === 'string';
  };

  window.decodeUrl = function(str) {
    return str ? decodeURIComponent(str.replace(/\+/g, '%20')) : '';
  };

  window.hasEvent = function(event) {
    return 'on'.concat(event) in window.document;
  };

  window.isOverallScroller = function(node) {
    return node === document.documentElement || node === document.body || node === window;
  };

  window.isFormElement = function(node) {
    var tagName = node.tagName;
    return tagName === 'INPUT' || tagName === 'SELECT' || tagName === 'TEXTAREA';
  };

  window.pageLoad = (function () {
    var loaded = false, cbs = [];
    window.addEventListener('load', function () {
      var i;
      loaded = true;
      if (cbs.length > 0) {
        for (i = 0; i < cbs.length; i++) {
          cbs[i]();
        }
      }
    });
    return {
      then: function(cb) {
        cb && (loaded ? cb() : (cbs.push(cb)));
      }
    };
  })();
})();
(function() {
  window.throttle = function(func, wait) {
    var args, result, thisArg, timeoutId, lastCalled = 0;

    function trailingCall() {
      lastCalled = new Date;
      timeoutId = null;
      result = func.apply(thisArg, args);
    }
    return function() {
      var now = new Date,
        remaining = wait - (now - lastCalled);

      args = arguments;
      thisArg = this;

      if (remaining <= 0) {
        clearTimeout(timeoutId);
        timeoutId = null;
        lastCalled = now;
        result = func.apply(thisArg, args);
      } else if (!timeoutId) {
        timeoutId = setTimeout(trailingCall, remaining);
      }
      return result;
    };
  };
})();
(function() {
  var Set = (function() {
    var add = function(item) {
      var i, data = this._data;
      for (i = 0; i < data.length; i++) {
        if (data[i] === item) {
          return;
        }
      }
      this.size ++;
      data.push(item);
      return data;
    };

    var Set = function(data) {
      this.size = 0;
      this._data = [];
      var i;
      if (data.length > 0) {
        for (i = 0; i < data.length; i++) {
          add.call(this, data[i]);
        }
      }
    };
    Set.prototype.add = add;
    Set.prototype.get = function(index) { return this._data[index]; };
    Set.prototype.has = function(item) {
      var i, data = this._data;
      for (i = 0; i < data.length; i++) {
        if (this.get(i) === item) {
          return true;
        }
      }
      return false;
    };
    Set.prototype.is = function(map) {
      if (map._data.length !== this._data.length) { return false; }
      var i, j, flag, tData = this._data, mData = map._data;
      for (i = 0; i < tData.length; i++) {
        for (flag = false, j = 0; j < mData.length; j++) {
          if (tData[i] === mData[j]) {
            flag = true;
            break;
          }
        }
        if (!flag) { return false; }
      }
      return true;
    };
    Set.prototype.values = function() {
      return this._data;
    };
    return Set;
  })();

  window.Lazyload = (function(doc) {
    var queue = {js: [], css: []}, sources = {js: {}, css: {}}, context = this;
    var createNode = function(name, attrs) {
      var node = doc.createElement(name), attr;
      for (attr in attrs) {
        if (attrs.hasOwnProperty(attr)) {
          node.setAttribute(attr, attrs[attr]);
        }
      }
      return node;
    };
    var end = function(type, url) {
      var s, q, qi, cbs, i, j, cur, val, flag;
      if (type === 'js' || type ==='css') {
        s = sources[type], q = queue[type];
        s[url] = true;
        for (i = 0; i < q.length; i++) {
          cur = q[i];
          if (cur.urls.has(url)) {
            qi = cur, val = qi.urls.values();
            qi && (cbs = qi.callbacks);
            for (flag = true, j = 0; j < val.length; j++) {
              cur = val[j];
              if (!s[cur]) {
                flag = false;
              }
            }
            if (flag && cbs && cbs.length > 0) {
              for (j = 0; j < cbs.length; j++) {
                cbs[j].call(context);
              }
              qi.load = true;
            }
          }
        }
      }
    };
    var load = function(type, urls, callback) {
      var s, q, qi, node, i, cur,
        _urls = typeof urls === 'string' ? new Set([urls]) : new Set(urls), val, url;
      if (type === 'js' || type ==='css') {
        s = sources[type], q = queue[type];
        for (i = 0; i < q.length; i++) {
          cur = q[i];
          if (_urls.is(cur.urls)) {
            qi = cur;
            break;
          }
        }
        val = _urls.values();
        if (qi) {
          callback && (qi.load || qi.callbacks.push(callback));
          callback && (qi.load && callback());
        } else {
          q.push({
            urls: _urls,
            callbacks: callback ? [callback] : [],
            load: false
          });
          for (i = 0; i < val.length; i++) {
            node = null, url = val[i];
            if (s[url] === undefined) {
              (type === 'js' ) && (node = createNode('script', { src: url }));
              (type === 'css') && (node = createNode('link', { rel: 'stylesheet', href: url }));
              if (node) {
                node.onload = (function(type, url) {
                  return function() {
                    end(type, url);
                  };
                })(type, url);
                (doc.head || doc.body).appendChild(node);
                s[url] = false;
              }
            }
          }
        }
      }
    };
    return {
      js: function(url, callback) {
        load('js', url, callback);
      },
      css: function(url, callback) {
        load('css', url, callback);
      }
    };
  })(this.document);
})();
</script><script>
  (function() {
    var TEXT_VARIABLES = {
      version: '2.2.4',
      sources: {
        font_awesome: 'https://use.fontawesome.com/releases/v5.0.13/css/all.css',
        jquery: 'https://cdn.bootcss.com/jquery/3.1.1/jquery.min.js',
        leancloud_js_sdk: '//cdn1.lncld.net/static/js/3.4.1/av-min.js',
        chart: 'https://cdn.bootcss.com/Chart.js/2.7.2/Chart.bundle.min.js',
        gitalk: {
          js: 'https://cdn.bootcss.com/gitalk/1.2.2/gitalk.min.js',
          css: 'https://cdn.bootcss.com/gitalk/1.2.2/gitalk.min.css'
        },
        valine: 'https://unpkg.com/valine/dist/Valine.min.js',
        mathjax: 'https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML',
        mermaid: 'https://cdn.bootcss.com/mermaid/8.0.0-rc.8/mermaid.min.js'
      },
      site: {
        toc: {
          selectors: 'h1,h2,h3'
        }
      },
      paths: {
        search_js: '/assets/search.js'
      }
    };
    window.TEXT_VARIABLES = TEXT_VARIABLES;
  })();
</script></head>
  <body>
    <div class="root" data-is-touch="false">
      <div class="layout--page layout--page--sidebar clearfix js-page-root&nbsp; layout--page--aside">
  <div class="page__mask d-print-none js-page-mask js-sidebar-hide"></div>
  <div class="page__viewport">
    <div class="page__actions d-print-none">
      <div class="js-sidebar-show">
        <i class="fas fa-bars icon--show"></i>
      </div>
    </div>

    <div class="grid page__grid">

      <div class="page__sidebar d-print-none"><a title="High Performance NLP with Apache Spark
" href="/">
    <!--<svg width="187" height="50" viewBox="0 0 187 50" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M38.6212 18.6877H42.3588V29.0697C42.3588 33.7209 40.1163 35.382 36.5448 35.382C35.7143 35.382 34.5515 35.2159 33.804 34.9668L34.2192 31.9767C34.7176 32.1428 35.382 32.3089 36.1295 32.3089C37.7076 32.3089 38.6212 31.6445 38.6212 29.0697V18.6877Z" fill="#3E4095"/>
<path d="M55.2325 28.9867C55.2325 33.3056 52.1594 35.299 48.9202 35.299C45.4319 35.299 42.774 32.9734 42.774 29.1528C42.774 25.3322 45.2657 22.8405 49.0863 22.8405C52.7408 22.8405 55.2325 25.4153 55.2325 28.9867ZM46.5946 29.0698C46.5946 31.1462 47.4252 32.6412 49.0033 32.6412C50.4152 32.6412 51.3289 31.2292 51.3289 29.0698C51.3289 27.3256 50.6644 25.4983 49.0033 25.4983C47.2591 25.4983 46.5946 27.3256 46.5946 29.0698Z" fill="#3E4095"/>
<path d="M55.6478 17.774H59.3854V24.5847H59.4684C59.8837 24.0863 60.382 23.6711 60.9634 23.3388C61.4618 23.0066 62.2093 22.8405 62.8737 22.8405C65.1993 22.8405 67.0266 24.5016 67.0266 28.0731V35.0498H63.289V28.4883C63.289 26.9103 62.7907 25.8305 61.3787 25.8305C60.382 25.8305 59.8006 26.495 59.5515 27.1594C59.4684 27.4086 59.4684 27.7408 59.4684 27.99V35.0498H55.6478V17.774Z" fill="#3E4095"/>
<path d="M68.1064 26.9103C68.1064 25.4153 68.0233 24.1694 68.0233 23.0897H71.2625L71.4286 24.7508C71.927 24.0033 73.0898 22.8405 75.0831 22.8405C77.4917 22.8405 79.319 24.4186 79.319 27.907V34.9668H75.5814V28.4053C75.5814 26.9103 75.0831 25.8305 73.6711 25.8305C72.6745 25.8305 72.01 26.495 71.7609 27.2425C71.6778 27.4917 71.5947 27.8239 71.5947 28.1561V35.0498H68.1064V26.9103Z" fill="#3E4095"/>
<path d="M83.887 31.2292C84.8836 31.7275 86.3787 32.2259 87.9567 32.2259C89.6179 32.2259 90.5315 31.5614 90.5315 30.4817C90.5315 29.485 89.784 28.9036 87.7906 28.1561C85.0497 27.2425 83.3056 25.6644 83.3056 23.3388C83.3056 20.5149 85.6311 18.4385 89.5348 18.4385C91.362 18.4385 92.774 18.8538 93.6876 19.269L92.8571 22.2591C92.1926 21.9268 91.0298 21.5116 89.4517 21.5116C87.8737 21.5116 87.0431 22.2591 87.0431 23.0896C87.0431 24.1694 87.9567 24.5847 90.1162 25.4152C93.0232 26.495 94.3521 27.99 94.3521 30.3156C94.3521 33.0564 92.2757 35.382 87.7076 35.382C85.7973 35.382 83.97 34.8837 83.0564 34.3853L83.887 31.2292Z" fill="#3E4095"/>
<path d="M94.9336 26.9103C94.9336 25.4153 94.8505 24.1694 94.8505 23.0897H98.0897L98.2558 24.7508H98.3389C98.8372 24.0033 100 22.8405 101.993 22.8405C104.402 22.8405 106.229 24.4186 106.229 27.907V34.9668H102.492V28.4053C102.492 26.9103 101.993 25.8305 100.581 25.8305C99.5847 25.8305 98.9203 26.495 98.6711 27.2425C98.5881 27.4917 98.505 27.8239 98.505 28.1561V35.0498H94.7675V26.9103H94.9336Z" fill="#3E4095"/>
<path d="M119.103 28.9867C119.103 33.3056 116.03 35.299 112.791 35.299C109.302 35.299 106.645 32.9734 106.645 29.1528C106.645 25.3322 109.136 22.8405 112.957 22.8405C116.694 22.8405 119.103 25.4153 119.103 28.9867ZM110.465 29.0698C110.465 31.1462 111.296 32.6412 112.874 32.6412C114.286 32.6412 115.199 31.2292 115.199 29.0698C115.199 27.3256 114.535 25.4983 112.874 25.4983C111.13 25.4983 110.465 27.3256 110.465 29.0698Z" fill="#3E4095"/>
<path d="M121.927 23.1727L122.841 28.0731C123.09 29.3189 123.339 30.6478 123.505 31.9767H123.588C123.837 30.6478 124.17 29.2359 124.502 28.0731L125.748 23.1727H128.655L129.817 27.9069C130.15 29.2359 130.482 30.5648 130.731 31.9767H130.814C130.98 30.6478 131.229 29.2359 131.478 27.9069L132.475 23.1727H136.13L132.475 35.0498H128.987L127.907 30.897C127.575 29.7342 127.409 28.6545 127.16 27.1594H127.076C126.827 28.6545 126.578 29.7342 126.329 30.897L125.166 35.0498H121.678L118.189 23.1727H121.927Z" fill="#3E4095"/>
<path d="M143.023 18.9369H145.1V32.8073H152.575V34.5515H143.023V18.9369Z" fill="#0098DA"/>
<path d="M155.399 29.5681L153.571 34.5515H151.329L157.226 18.9369H159.801L165.781 34.5515H163.455L161.545 29.5681H155.399ZM161.213 27.99L159.468 23.3389C159.136 22.3422 158.804 21.5116 158.555 20.6811H158.472C158.223 21.5116 157.973 22.3422 157.641 23.2558L155.897 27.99H161.213Z" fill="#0098DA"/>
<path d="M165.864 19.186C166.777 19.0199 168.355 18.8538 169.933 18.8538C172.176 18.8538 173.505 19.186 174.502 20.0166C175.332 20.6811 175.914 21.5947 175.914 22.8405C175.914 24.3355 174.834 25.6644 173.173 26.2458V26.3289C174.502 26.6611 176.495 27.8239 176.495 30.2326C176.495 31.5615 175.914 32.6412 175.083 33.3887C173.92 34.3854 172.093 34.8837 169.269 34.8837C167.774 34.8837 166.611 34.8007 165.864 34.7176V19.186ZM168.023 25.5814H170.183C172.508 25.5814 173.754 24.5017 173.754 23.0066C173.754 21.0963 172.176 20.4319 170.1 20.4319C169.02 20.4319 168.355 20.5149 168.023 20.598V25.5814ZM168.023 32.9734C168.521 33.0565 169.103 33.0565 169.933 33.0565C172.093 33.0565 174.252 32.392 174.252 29.9834C174.252 27.8239 172.342 26.9934 169.933 26.9934H167.94V32.9734H168.023Z" fill="#0098DA"/>
<path d="M176.91 31.9768C177.907 32.6412 179.402 33.1396 180.98 33.1396C183.223 33.1396 184.468 32.0598 184.468 30.4818C184.468 28.9867 183.638 28.1562 181.229 27.4087C178.239 26.495 176.661 25.1661 176.661 22.9236C176.661 20.4319 178.821 18.6047 182.06 18.6047C183.887 18.6047 185.133 19.02 185.963 19.4352L185.382 21.0964C184.884 20.7641 183.638 20.2658 182.06 20.2658C179.734 20.2658 178.821 21.5947 178.821 22.5914C178.821 24.0033 179.817 24.7509 182.226 25.4984C185.133 26.412 186.628 27.6578 186.628 30.1495C186.628 32.4751 184.884 34.7176 180.814 34.7176C179.153 34.7176 177.325 34.2193 176.412 33.6379L176.91 31.9768Z" fill="#0098DA"/>
<path d="M22.5083 35.6312C22.5083 40.1163 18.8538 43.7708 14.3688 43.7708C9.88372 43.7708 6.22924 40.1163 6.22924 35.6312V12.2093L0 11.4618V35.6312C0 43.6047 6.4784 50 14.3688 50C22.2591 50 28.7375 43.5216 28.7375 35.6312V11.4618L22.5083 12.2093V35.6312Z" fill="#0098DA"/>
<path d="M16.1129 17.7741H8.63786C8.13952 17.7741 7.72424 17.3588 7.72424 16.8604V9.38536C7.72424 8.88702 8.13952 8.47174 8.63786 8.47174H16.1129C16.6113 8.47174 17.0266 8.88702 17.0266 9.38536V16.8604C17.0266 17.3588 16.6113 17.7741 16.1129 17.7741Z" fill="#3E4095"/>
<path d="M20.515 22.7575H15.2824C14.7841 22.7575 14.3688 22.3422 14.3688 21.8439V16.6113C14.3688 16.113 14.7841 15.6977 15.2824 15.6977H20.515C21.0133 15.6977 21.4286 16.113 21.4286 16.6113V21.8439C21.4286 22.4253 21.0133 22.7575 20.515 22.7575Z" fill="#3E4095"/>
<path d="M19.8505 9.71762H16.113C15.6146 9.71762 15.1993 9.30233 15.1993 8.80399V5.06645C15.1993 4.56811 15.6146 4.15283 16.113 4.15283H19.8505C20.3488 4.15283 20.7641 4.56811 20.7641 5.06645V8.80399C20.6811 9.30233 20.3488 9.71762 19.8505 9.71762Z" fill="#3E4095"/>
<path d="M13.6213 3.48837H11.8771C11.3788 3.48837 10.9635 3.07309 10.9635 2.57475V0.913621C10.9635 0.415282 11.3788 0 11.8771 0H13.6213C14.1196 0 14.5349 0.415282 14.5349 0.913621V2.65781C14.5349 3.15615 14.1196 3.48837 13.6213 3.48837Z" fill="#3E4095"/>
<path d="M20.2658 41.196H8.38867V41.3622H20.2658V41.196Z" fill="#ECF9FF"/>
<path d="M20.2658 40.9469H8.38867V41.113H20.2658V40.9469Z" fill="#EBF9FF"/>
<path d="M20.2658 40.7808H8.38867V40.9469H20.2658V40.7808Z" fill="#EAF8FF"/>
<path d="M20.2658 40.6146H8.38867V40.7807H20.2658V40.6146Z" fill="#E9F8FF"/>
<path d="M20.2658 40.3655H8.38867V40.5316H20.2658V40.3655Z" fill="#E8F8FF"/>
<path d="M20.2658 40.1993H8.38867V40.3655H20.2658V40.1993Z" fill="#E7F7FF"/>
<path d="M20.2658 40.0333H8.38867V40.1994H20.2658V40.0333Z" fill="#E6F7FF"/>
<path d="M20.2658 39.8671H8.38867V40.0332H20.2658V39.8671Z" fill="#E5F7FF"/>
<path d="M20.2658 39.618H8.38867V39.7841H20.2658V39.618Z" fill="#E4F6FE"/>
<path d="M20.2658 39.4518H8.38867V39.618H20.2658V39.4518Z" fill="#E3F6FE"/>
<path d="M20.2658 39.2858H8.38867V39.4519H20.2658V39.2858Z" fill="#E2F5FE"/>
<path d="M20.2658 39.0366H8.38867V39.2027H20.2658V39.0366Z" fill="#E1F5FE"/>
<path d="M20.2658 38.8705H8.38867V39.0366H20.2658V38.8705Z" fill="#E0F5FE"/>
<path d="M20.2658 38.7043H8.38867V38.8705H20.2658V38.7043Z" fill="#DFF4FE"/>
<path d="M20.2658 38.4552H8.38867V38.6213H20.2658V38.4552Z" fill="#DEF4FE"/>
<path d="M20.2658 38.2891H8.38867V38.4552H20.2658V38.2891Z" fill="#DDF4FE"/>
<path d="M20.2658 38.1229H8.38867V38.289H20.2658V38.1229Z" fill="#DCF3FE"/>
<path d="M20.2658 37.8738H8.38867V38.0399H20.2658V37.8738Z" fill="#DBF3FE"/>
<path d="M20.2658 37.7077H8.38867V37.8738H20.2658V37.7077Z" fill="#DAF3FE"/>
<path d="M20.2658 37.5416H8.38867V37.7077H20.2658V37.5416Z" fill="#D9F2FE"/>
<path d="M20.2658 37.3754H8.38867V37.5415H20.2658V37.3754Z" fill="#D8F2FE"/>
<path d="M20.2658 37.1263H8.38867V37.2924H20.2658V37.1263Z" fill="#D7F2FE"/>
<path d="M20.2658 36.9601H8.38867V37.1263H20.2658V36.9601Z" fill="#D6F1FE"/>
<path d="M20.2658 36.7941H8.38867V36.9602H20.2658V36.7941Z" fill="#D5F1FE"/>
<path d="M20.2658 36.5449H8.38867V36.711H20.2658V36.5449Z" fill="#D4F1FD"/>
<path d="M20.2658 36.3788H8.38867V36.5449H20.2658V36.3788Z" fill="#D3F0FD"/>
<path d="M20.2658 36.2126H8.38867V36.3788H20.2658V36.2126Z" fill="#D2F0FD"/>
<path d="M20.2658 35.9635H8.38867V36.1296H20.2658V35.9635Z" fill="#D1F0FD"/>
<path d="M20.2658 35.7974H8.38867V35.9635H20.2658V35.7974Z" fill="#D0EFFD"/>
<path d="M20.2658 35.6313H8.38867V35.7974H20.2658V35.6313Z" fill="#CFEFFD"/>
<path d="M20.2658 35.3821H8.38867V35.5482H20.2658V35.3821Z" fill="#CEEEFD"/>
<path d="M20.2658 35.216H8.38867V35.3821H20.2658V35.216Z" fill="#CDEEFD"/>
<path d="M20.2658 35.0499H8.38867V35.216H20.2658V35.0499Z" fill="#CCEEFD"/>
<path d="M20.2658 34.8837H8.38867V35.0498H20.2658V34.8837Z" fill="#CBEDFD"/>
<path d="M20.2658 34.6346H8.38867V34.8007H20.2658V34.6346Z" fill="#CAEDFD"/>
<path d="M20.2658 34.4684H8.38867V34.6346H20.2658V34.4684Z" fill="#C9EDFD"/>
<path d="M20.2658 34.3024H8.38867V34.4685H20.2658V34.3024Z" fill="#C8ECFD"/>
<path d="M20.2658 34.0532H8.38867V34.2193H20.2658V34.0532Z" fill="#C7ECFD"/>
<path d="M20.2658 33.8871H8.38867V34.0532H20.2658V33.8871Z" fill="#C6ECFD"/>
<path d="M20.2658 33.7209H8.38867V33.8871H20.2658V33.7209Z" fill="#C4EBFC"/>
<path d="M20.2658 33.4718H8.38867V33.6379H20.2658V33.4718Z" fill="#C3EBFC"/>
<path d="M20.2658 33.3057H8.38867V33.4718H20.2658V33.3057Z" fill="#C2EBFC"/>
<path d="M20.2658 33.1396H8.38867V33.3057H20.2658V33.1396Z" fill="#C1EAFC"/>
<path d="M20.2658 32.8904H8.38867V33.0565H20.2658V32.8904Z" fill="#C0EAFC"/>
<path d="M20.2658 32.7242H8.38867V32.8904H20.2658V32.7242Z" fill="#BFEAFC"/>
<path d="M20.2658 32.5582H8.38867V32.7243H20.2658V32.5582Z" fill="#BEE9FC"/>
<path d="M20.2658 32.392H8.38867V32.5581H20.2658V32.392Z" fill="#BDE9FC"/>
<path d="M20.2658 32.1429H8.38867V32.309H20.2658V32.1429Z" fill="#BCE9FC"/>
<path d="M20.2658 31.9768H8.38867V32.1429H20.2658V31.9768Z" fill="#BBE8FC"/>
<path d="M20.2658 31.8107H8.38867V31.9768H20.2658V31.8107Z" fill="#BAE8FC"/>
<path d="M20.2658 31.5615H8.38867V31.7276H20.2658V31.5615Z" fill="#B9E7FC"/>
<path d="M20.2658 31.3954H8.38867V31.5615H20.2658V31.3954Z" fill="#B8E7FC"/>
<path d="M20.2658 31.2292H8.38867V31.3954H20.2658V31.2292Z" fill="#B7E7FC"/>
<path d="M20.2658 30.9801H8.38867V31.1462H20.2658V30.9801Z" fill="#B6E6FC"/>
<path d="M20.2658 30.814H8.38867V30.9801H20.2658V30.814Z" fill="#B5E6FB"/>
<path d="M20.2658 30.6479H8.38867V30.814H20.2658V30.6479Z" fill="#B4E6FB"/>
<path d="M20.2658 30.3987H8.38867V30.5648H20.2658V30.3987Z" fill="#B3E5FB"/>
<path d="M20.2658 30.2326H8.38867V30.3987H20.2658V30.2326Z" fill="#B2E5FB"/>
<path d="M20.2658 30.0665H8.38867V30.2326H20.2658V30.0665Z" fill="#B1E5FB"/>
<path d="M20.2658 29.9004H8.38867V30.0665H20.2658V29.9004Z" fill="#B0E4FB"/>
<path d="M20.2658 29.6512H8.38867V29.8173H20.2658V29.6512Z" fill="#AFE4FB"/>
<path d="M20.2658 29.4851H8.38867V29.6512H20.2658V29.4851Z" fill="#AEE4FB"/>
<path d="M20.2658 29.319H8.38867V29.4851H20.2658V29.319Z" fill="#ADE3FB"/>
<path d="M20.2658 29.0698H8.38867V29.2359H20.2658V29.0698Z" fill="#ACE3FB"/>
<path d="M20.2658 28.9037H8.38867V29.0698H20.2658V28.9037Z" fill="#ABE3FB"/>
<path d="M20.2658 28.7375H8.38867V28.9037H20.2658V28.7375Z" fill="#AAE2FB"/>
<path d="M20.2658 28.4884H8.38867V28.6545H20.2658V28.4884Z" fill="#A9E2FB"/>
<path d="M20.2658 28.3223H8.38867V28.4884H20.2658V28.3223Z" fill="#A8E2FB"/>
<path d="M20.2658 28.1562H8.38867V28.3223H20.2658V28.1562Z" fill="#A7E1FB"/>
<path d="M20.2658 27.907H8.38867V28.0731H20.2658V27.907Z" fill="#A6E1FB"/>
<path d="M20.2658 27.7409H8.38867V27.907H20.2658V27.7409Z" fill="#A5E0FA"/>
<path d="M20.2658 27.5748H8.38867V27.7409H20.2658V27.5748Z" fill="#A4E0FA"/>
<path d="M20.2658 27.4087H8.38867V27.5748H20.2658V27.4087Z" fill="#A3E0FA"/>
<path d="M20.2658 27.1595H8.38867V27.3256H20.2658V27.1595Z" fill="#A2DFFA"/>
<path d="M20.2658 26.9934H8.38867V27.1595H20.2658V26.9934Z" fill="#A1DFFA"/>
<path d="M20.2658 26.8273H8.38867V26.9934H20.2658V26.8273Z" fill="#A0DFFA"/>
<path d="M20.2658 26.5781H8.38867V26.7442H20.2658V26.5781Z" fill="#9FDEFA"/>
<path d="M20.2658 26.412H8.38867V26.5781H20.2658V26.412Z" fill="#9EDEFA"/>
</svg>
-->
</a><div class="sidebar-toc"><ul class="toc toc--navigator"><li class="toc-h1">Spark NLP</li><li class="toc-h2"><a href="/docs/en/quickstart">Getting Started</a></li><li class="toc-h2"><a href="/docs/en/install">Install Spark NLP</a></li><li class="toc-h2"><a href="/docs/en/concepts">General Concepts</a></li><li class="toc-h2"><a href="/docs/en/annotators">Annotators</a></li><li class="toc-h2 active"><a href="/docs/en/transformers">Transformers</a></li><li class="toc-h2"><a href="/docs/en/training">Training</a></li><li class="toc-h2"><a href="/docs/en/third-party-projects">Third Party Projects</a></li><li class="toc-h2"><a href="/models">Models</a></li><li class="toc-h2"><a href="/docs/en/auxiliary">Helpers</a></li><li class="toc-h2"><a href="/api/">Scala API (Scaladoc)</a></li><li class="toc-h2"><a href="/api/python/">Python API (Sphinx)</a></li><li class="toc-h2"><a href="/docs/en/display">Spark NLP Display</a></li><li class="toc-h2"><a href="/docs/en/developers">Developers</a></li><li class="toc-h2"><a href="/docs/en/mlflow">Experiment Tracking</a></li><li class="toc-h2"><a href="/docs/en/production-readiness">Productionizing Spark NLP</a></li><li class="toc-h2"><a href="/docs/en/CPUvsGPUbenchmarkClassifierDL">GPU vs CPU benchmark on ClassifierDL</a></li><li class="toc-h2"><a href="/docs/en/release_notes">Release Notes</a></li></ul></div></div><div class="page__main js-page-main has-aside cell cell--auto">

      <div class="page__main-inner"><div class="page__header d-print-none"><!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-59JLR64"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) --><header class="header"><div class="main">
      <div class="header__title">
        <a class="responsive_btn" href="#" id="responsive_menu">          
        <i class="fas fa-bars"></i>
        <i class="fas fa-times"></i>
        </a>
        <div class="header__brand">
          <a title="High Performance NLP with Apache Spark
" href="https://www.johnsnowlabs.com" target="_blank"><svg width="187" height="50" viewBox="0 0 187 50" fill="none" xmlns="http://www.w3.org/2000/svg">
<path d="M38.6212 18.6877H42.3588V29.0697C42.3588 33.7209 40.1163 35.382 36.5448 35.382C35.7143 35.382 34.5515 35.2159 33.804 34.9668L34.2192 31.9767C34.7176 32.1428 35.382 32.3089 36.1295 32.3089C37.7076 32.3089 38.6212 31.6445 38.6212 29.0697V18.6877Z" fill="#3E4095"/>
<path d="M55.2325 28.9867C55.2325 33.3056 52.1594 35.299 48.9202 35.299C45.4319 35.299 42.774 32.9734 42.774 29.1528C42.774 25.3322 45.2657 22.8405 49.0863 22.8405C52.7408 22.8405 55.2325 25.4153 55.2325 28.9867ZM46.5946 29.0698C46.5946 31.1462 47.4252 32.6412 49.0033 32.6412C50.4152 32.6412 51.3289 31.2292 51.3289 29.0698C51.3289 27.3256 50.6644 25.4983 49.0033 25.4983C47.2591 25.4983 46.5946 27.3256 46.5946 29.0698Z" fill="#3E4095"/>
<path d="M55.6478 17.774H59.3854V24.5847H59.4684C59.8837 24.0863 60.382 23.6711 60.9634 23.3388C61.4618 23.0066 62.2093 22.8405 62.8737 22.8405C65.1993 22.8405 67.0266 24.5016 67.0266 28.0731V35.0498H63.289V28.4883C63.289 26.9103 62.7907 25.8305 61.3787 25.8305C60.382 25.8305 59.8006 26.495 59.5515 27.1594C59.4684 27.4086 59.4684 27.7408 59.4684 27.99V35.0498H55.6478V17.774Z" fill="#3E4095"/>
<path d="M68.1064 26.9103C68.1064 25.4153 68.0233 24.1694 68.0233 23.0897H71.2625L71.4286 24.7508C71.927 24.0033 73.0898 22.8405 75.0831 22.8405C77.4917 22.8405 79.319 24.4186 79.319 27.907V34.9668H75.5814V28.4053C75.5814 26.9103 75.0831 25.8305 73.6711 25.8305C72.6745 25.8305 72.01 26.495 71.7609 27.2425C71.6778 27.4917 71.5947 27.8239 71.5947 28.1561V35.0498H68.1064V26.9103Z" fill="#3E4095"/>
<path d="M83.887 31.2292C84.8836 31.7275 86.3787 32.2259 87.9567 32.2259C89.6179 32.2259 90.5315 31.5614 90.5315 30.4817C90.5315 29.485 89.784 28.9036 87.7906 28.1561C85.0497 27.2425 83.3056 25.6644 83.3056 23.3388C83.3056 20.5149 85.6311 18.4385 89.5348 18.4385C91.362 18.4385 92.774 18.8538 93.6876 19.269L92.8571 22.2591C92.1926 21.9268 91.0298 21.5116 89.4517 21.5116C87.8737 21.5116 87.0431 22.2591 87.0431 23.0896C87.0431 24.1694 87.9567 24.5847 90.1162 25.4152C93.0232 26.495 94.3521 27.99 94.3521 30.3156C94.3521 33.0564 92.2757 35.382 87.7076 35.382C85.7973 35.382 83.97 34.8837 83.0564 34.3853L83.887 31.2292Z" fill="#3E4095"/>
<path d="M94.9336 26.9103C94.9336 25.4153 94.8505 24.1694 94.8505 23.0897H98.0897L98.2558 24.7508H98.3389C98.8372 24.0033 100 22.8405 101.993 22.8405C104.402 22.8405 106.229 24.4186 106.229 27.907V34.9668H102.492V28.4053C102.492 26.9103 101.993 25.8305 100.581 25.8305C99.5847 25.8305 98.9203 26.495 98.6711 27.2425C98.5881 27.4917 98.505 27.8239 98.505 28.1561V35.0498H94.7675V26.9103H94.9336Z" fill="#3E4095"/>
<path d="M119.103 28.9867C119.103 33.3056 116.03 35.299 112.791 35.299C109.302 35.299 106.645 32.9734 106.645 29.1528C106.645 25.3322 109.136 22.8405 112.957 22.8405C116.694 22.8405 119.103 25.4153 119.103 28.9867ZM110.465 29.0698C110.465 31.1462 111.296 32.6412 112.874 32.6412C114.286 32.6412 115.199 31.2292 115.199 29.0698C115.199 27.3256 114.535 25.4983 112.874 25.4983C111.13 25.4983 110.465 27.3256 110.465 29.0698Z" fill="#3E4095"/>
<path d="M121.927 23.1727L122.841 28.0731C123.09 29.3189 123.339 30.6478 123.505 31.9767H123.588C123.837 30.6478 124.17 29.2359 124.502 28.0731L125.748 23.1727H128.655L129.817 27.9069C130.15 29.2359 130.482 30.5648 130.731 31.9767H130.814C130.98 30.6478 131.229 29.2359 131.478 27.9069L132.475 23.1727H136.13L132.475 35.0498H128.987L127.907 30.897C127.575 29.7342 127.409 28.6545 127.16 27.1594H127.076C126.827 28.6545 126.578 29.7342 126.329 30.897L125.166 35.0498H121.678L118.189 23.1727H121.927Z" fill="#3E4095"/>
<path d="M143.023 18.9369H145.1V32.8073H152.575V34.5515H143.023V18.9369Z" fill="#0098DA"/>
<path d="M155.399 29.5681L153.571 34.5515H151.329L157.226 18.9369H159.801L165.781 34.5515H163.455L161.545 29.5681H155.399ZM161.213 27.99L159.468 23.3389C159.136 22.3422 158.804 21.5116 158.555 20.6811H158.472C158.223 21.5116 157.973 22.3422 157.641 23.2558L155.897 27.99H161.213Z" fill="#0098DA"/>
<path d="M165.864 19.186C166.777 19.0199 168.355 18.8538 169.933 18.8538C172.176 18.8538 173.505 19.186 174.502 20.0166C175.332 20.6811 175.914 21.5947 175.914 22.8405C175.914 24.3355 174.834 25.6644 173.173 26.2458V26.3289C174.502 26.6611 176.495 27.8239 176.495 30.2326C176.495 31.5615 175.914 32.6412 175.083 33.3887C173.92 34.3854 172.093 34.8837 169.269 34.8837C167.774 34.8837 166.611 34.8007 165.864 34.7176V19.186ZM168.023 25.5814H170.183C172.508 25.5814 173.754 24.5017 173.754 23.0066C173.754 21.0963 172.176 20.4319 170.1 20.4319C169.02 20.4319 168.355 20.5149 168.023 20.598V25.5814ZM168.023 32.9734C168.521 33.0565 169.103 33.0565 169.933 33.0565C172.093 33.0565 174.252 32.392 174.252 29.9834C174.252 27.8239 172.342 26.9934 169.933 26.9934H167.94V32.9734H168.023Z" fill="#0098DA"/>
<path d="M176.91 31.9768C177.907 32.6412 179.402 33.1396 180.98 33.1396C183.223 33.1396 184.468 32.0598 184.468 30.4818C184.468 28.9867 183.638 28.1562 181.229 27.4087C178.239 26.495 176.661 25.1661 176.661 22.9236C176.661 20.4319 178.821 18.6047 182.06 18.6047C183.887 18.6047 185.133 19.02 185.963 19.4352L185.382 21.0964C184.884 20.7641 183.638 20.2658 182.06 20.2658C179.734 20.2658 178.821 21.5947 178.821 22.5914C178.821 24.0033 179.817 24.7509 182.226 25.4984C185.133 26.412 186.628 27.6578 186.628 30.1495C186.628 32.4751 184.884 34.7176 180.814 34.7176C179.153 34.7176 177.325 34.2193 176.412 33.6379L176.91 31.9768Z" fill="#0098DA"/>
<path d="M22.5083 35.6312C22.5083 40.1163 18.8538 43.7708 14.3688 43.7708C9.88372 43.7708 6.22924 40.1163 6.22924 35.6312V12.2093L0 11.4618V35.6312C0 43.6047 6.4784 50 14.3688 50C22.2591 50 28.7375 43.5216 28.7375 35.6312V11.4618L22.5083 12.2093V35.6312Z" fill="#0098DA"/>
<path d="M16.1129 17.7741H8.63786C8.13952 17.7741 7.72424 17.3588 7.72424 16.8604V9.38536C7.72424 8.88702 8.13952 8.47174 8.63786 8.47174H16.1129C16.6113 8.47174 17.0266 8.88702 17.0266 9.38536V16.8604C17.0266 17.3588 16.6113 17.7741 16.1129 17.7741Z" fill="#3E4095"/>
<path d="M20.515 22.7575H15.2824C14.7841 22.7575 14.3688 22.3422 14.3688 21.8439V16.6113C14.3688 16.113 14.7841 15.6977 15.2824 15.6977H20.515C21.0133 15.6977 21.4286 16.113 21.4286 16.6113V21.8439C21.4286 22.4253 21.0133 22.7575 20.515 22.7575Z" fill="#3E4095"/>
<path d="M19.8505 9.71762H16.113C15.6146 9.71762 15.1993 9.30233 15.1993 8.80399V5.06645C15.1993 4.56811 15.6146 4.15283 16.113 4.15283H19.8505C20.3488 4.15283 20.7641 4.56811 20.7641 5.06645V8.80399C20.6811 9.30233 20.3488 9.71762 19.8505 9.71762Z" fill="#3E4095"/>
<path d="M13.6213 3.48837H11.8771C11.3788 3.48837 10.9635 3.07309 10.9635 2.57475V0.913621C10.9635 0.415282 11.3788 0 11.8771 0H13.6213C14.1196 0 14.5349 0.415282 14.5349 0.913621V2.65781C14.5349 3.15615 14.1196 3.48837 13.6213 3.48837Z" fill="#3E4095"/>
<path d="M20.2658 41.196H8.38867V41.3622H20.2658V41.196Z" fill="#ECF9FF"/>
<path d="M20.2658 40.9469H8.38867V41.113H20.2658V40.9469Z" fill="#EBF9FF"/>
<path d="M20.2658 40.7808H8.38867V40.9469H20.2658V40.7808Z" fill="#EAF8FF"/>
<path d="M20.2658 40.6146H8.38867V40.7807H20.2658V40.6146Z" fill="#E9F8FF"/>
<path d="M20.2658 40.3655H8.38867V40.5316H20.2658V40.3655Z" fill="#E8F8FF"/>
<path d="M20.2658 40.1993H8.38867V40.3655H20.2658V40.1993Z" fill="#E7F7FF"/>
<path d="M20.2658 40.0333H8.38867V40.1994H20.2658V40.0333Z" fill="#E6F7FF"/>
<path d="M20.2658 39.8671H8.38867V40.0332H20.2658V39.8671Z" fill="#E5F7FF"/>
<path d="M20.2658 39.618H8.38867V39.7841H20.2658V39.618Z" fill="#E4F6FE"/>
<path d="M20.2658 39.4518H8.38867V39.618H20.2658V39.4518Z" fill="#E3F6FE"/>
<path d="M20.2658 39.2858H8.38867V39.4519H20.2658V39.2858Z" fill="#E2F5FE"/>
<path d="M20.2658 39.0366H8.38867V39.2027H20.2658V39.0366Z" fill="#E1F5FE"/>
<path d="M20.2658 38.8705H8.38867V39.0366H20.2658V38.8705Z" fill="#E0F5FE"/>
<path d="M20.2658 38.7043H8.38867V38.8705H20.2658V38.7043Z" fill="#DFF4FE"/>
<path d="M20.2658 38.4552H8.38867V38.6213H20.2658V38.4552Z" fill="#DEF4FE"/>
<path d="M20.2658 38.2891H8.38867V38.4552H20.2658V38.2891Z" fill="#DDF4FE"/>
<path d="M20.2658 38.1229H8.38867V38.289H20.2658V38.1229Z" fill="#DCF3FE"/>
<path d="M20.2658 37.8738H8.38867V38.0399H20.2658V37.8738Z" fill="#DBF3FE"/>
<path d="M20.2658 37.7077H8.38867V37.8738H20.2658V37.7077Z" fill="#DAF3FE"/>
<path d="M20.2658 37.5416H8.38867V37.7077H20.2658V37.5416Z" fill="#D9F2FE"/>
<path d="M20.2658 37.3754H8.38867V37.5415H20.2658V37.3754Z" fill="#D8F2FE"/>
<path d="M20.2658 37.1263H8.38867V37.2924H20.2658V37.1263Z" fill="#D7F2FE"/>
<path d="M20.2658 36.9601H8.38867V37.1263H20.2658V36.9601Z" fill="#D6F1FE"/>
<path d="M20.2658 36.7941H8.38867V36.9602H20.2658V36.7941Z" fill="#D5F1FE"/>
<path d="M20.2658 36.5449H8.38867V36.711H20.2658V36.5449Z" fill="#D4F1FD"/>
<path d="M20.2658 36.3788H8.38867V36.5449H20.2658V36.3788Z" fill="#D3F0FD"/>
<path d="M20.2658 36.2126H8.38867V36.3788H20.2658V36.2126Z" fill="#D2F0FD"/>
<path d="M20.2658 35.9635H8.38867V36.1296H20.2658V35.9635Z" fill="#D1F0FD"/>
<path d="M20.2658 35.7974H8.38867V35.9635H20.2658V35.7974Z" fill="#D0EFFD"/>
<path d="M20.2658 35.6313H8.38867V35.7974H20.2658V35.6313Z" fill="#CFEFFD"/>
<path d="M20.2658 35.3821H8.38867V35.5482H20.2658V35.3821Z" fill="#CEEEFD"/>
<path d="M20.2658 35.216H8.38867V35.3821H20.2658V35.216Z" fill="#CDEEFD"/>
<path d="M20.2658 35.0499H8.38867V35.216H20.2658V35.0499Z" fill="#CCEEFD"/>
<path d="M20.2658 34.8837H8.38867V35.0498H20.2658V34.8837Z" fill="#CBEDFD"/>
<path d="M20.2658 34.6346H8.38867V34.8007H20.2658V34.6346Z" fill="#CAEDFD"/>
<path d="M20.2658 34.4684H8.38867V34.6346H20.2658V34.4684Z" fill="#C9EDFD"/>
<path d="M20.2658 34.3024H8.38867V34.4685H20.2658V34.3024Z" fill="#C8ECFD"/>
<path d="M20.2658 34.0532H8.38867V34.2193H20.2658V34.0532Z" fill="#C7ECFD"/>
<path d="M20.2658 33.8871H8.38867V34.0532H20.2658V33.8871Z" fill="#C6ECFD"/>
<path d="M20.2658 33.7209H8.38867V33.8871H20.2658V33.7209Z" fill="#C4EBFC"/>
<path d="M20.2658 33.4718H8.38867V33.6379H20.2658V33.4718Z" fill="#C3EBFC"/>
<path d="M20.2658 33.3057H8.38867V33.4718H20.2658V33.3057Z" fill="#C2EBFC"/>
<path d="M20.2658 33.1396H8.38867V33.3057H20.2658V33.1396Z" fill="#C1EAFC"/>
<path d="M20.2658 32.8904H8.38867V33.0565H20.2658V32.8904Z" fill="#C0EAFC"/>
<path d="M20.2658 32.7242H8.38867V32.8904H20.2658V32.7242Z" fill="#BFEAFC"/>
<path d="M20.2658 32.5582H8.38867V32.7243H20.2658V32.5582Z" fill="#BEE9FC"/>
<path d="M20.2658 32.392H8.38867V32.5581H20.2658V32.392Z" fill="#BDE9FC"/>
<path d="M20.2658 32.1429H8.38867V32.309H20.2658V32.1429Z" fill="#BCE9FC"/>
<path d="M20.2658 31.9768H8.38867V32.1429H20.2658V31.9768Z" fill="#BBE8FC"/>
<path d="M20.2658 31.8107H8.38867V31.9768H20.2658V31.8107Z" fill="#BAE8FC"/>
<path d="M20.2658 31.5615H8.38867V31.7276H20.2658V31.5615Z" fill="#B9E7FC"/>
<path d="M20.2658 31.3954H8.38867V31.5615H20.2658V31.3954Z" fill="#B8E7FC"/>
<path d="M20.2658 31.2292H8.38867V31.3954H20.2658V31.2292Z" fill="#B7E7FC"/>
<path d="M20.2658 30.9801H8.38867V31.1462H20.2658V30.9801Z" fill="#B6E6FC"/>
<path d="M20.2658 30.814H8.38867V30.9801H20.2658V30.814Z" fill="#B5E6FB"/>
<path d="M20.2658 30.6479H8.38867V30.814H20.2658V30.6479Z" fill="#B4E6FB"/>
<path d="M20.2658 30.3987H8.38867V30.5648H20.2658V30.3987Z" fill="#B3E5FB"/>
<path d="M20.2658 30.2326H8.38867V30.3987H20.2658V30.2326Z" fill="#B2E5FB"/>
<path d="M20.2658 30.0665H8.38867V30.2326H20.2658V30.0665Z" fill="#B1E5FB"/>
<path d="M20.2658 29.9004H8.38867V30.0665H20.2658V29.9004Z" fill="#B0E4FB"/>
<path d="M20.2658 29.6512H8.38867V29.8173H20.2658V29.6512Z" fill="#AFE4FB"/>
<path d="M20.2658 29.4851H8.38867V29.6512H20.2658V29.4851Z" fill="#AEE4FB"/>
<path d="M20.2658 29.319H8.38867V29.4851H20.2658V29.319Z" fill="#ADE3FB"/>
<path d="M20.2658 29.0698H8.38867V29.2359H20.2658V29.0698Z" fill="#ACE3FB"/>
<path d="M20.2658 28.9037H8.38867V29.0698H20.2658V28.9037Z" fill="#ABE3FB"/>
<path d="M20.2658 28.7375H8.38867V28.9037H20.2658V28.7375Z" fill="#AAE2FB"/>
<path d="M20.2658 28.4884H8.38867V28.6545H20.2658V28.4884Z" fill="#A9E2FB"/>
<path d="M20.2658 28.3223H8.38867V28.4884H20.2658V28.3223Z" fill="#A8E2FB"/>
<path d="M20.2658 28.1562H8.38867V28.3223H20.2658V28.1562Z" fill="#A7E1FB"/>
<path d="M20.2658 27.907H8.38867V28.0731H20.2658V27.907Z" fill="#A6E1FB"/>
<path d="M20.2658 27.7409H8.38867V27.907H20.2658V27.7409Z" fill="#A5E0FA"/>
<path d="M20.2658 27.5748H8.38867V27.7409H20.2658V27.5748Z" fill="#A4E0FA"/>
<path d="M20.2658 27.4087H8.38867V27.5748H20.2658V27.4087Z" fill="#A3E0FA"/>
<path d="M20.2658 27.1595H8.38867V27.3256H20.2658V27.1595Z" fill="#A2DFFA"/>
<path d="M20.2658 26.9934H8.38867V27.1595H20.2658V26.9934Z" fill="#A1DFFA"/>
<path d="M20.2658 26.8273H8.38867V26.9934H20.2658V26.8273Z" fill="#A0DFFA"/>
<path d="M20.2658 26.5781H8.38867V26.7442H20.2658V26.5781Z" fill="#9FDEFA"/>
<path d="M20.2658 26.412H8.38867V26.5781H20.2658V26.412Z" fill="#9EDEFA"/>
</svg>
</a><!---->
            <!-- <a title="High Performance NLP with Apache Spark
" href="/">Spark NLP</a> -->
          <!---->
        </div></div><nav class="navigation top_navigation">
        <ul class="top-menu"><li class="navigation__item "><a href="/">Home</a></li><li class="navigation__item navigation__item--active"><a href="/docs">Docs</a></li><li class="navigation__item "><a href="/learn">Learn</a></li><li class="navigation__item "><a href="/models">Models</a></li><li class="navigation__item "><a href="/demos">Demo</a></li><li class="navigation__item "><a href="https://github.com/JohnSnowLabs/spark-nlp"><span style="color: #FF8A00;"><i class="fab fa-github fa-2x"></i></span></a></li><li class="navigation__item "><a href="https://www.johnsnowlabs.com/slack-redirect/"><span style="color: #FF8A00;"><i class="fab fa-slack-hash fa-2x"></i></span></a></li></ul>
      </nav><a class="responsive_btn" href="#" id="aside_menu">          
        <i class="fas fa-bars"></i>
        <i class="fas fa-times"></i>
        </a>
    </div>
  </header>
</div><div class="page__content "><div class ="main"><div class="grid grid--reverse">

              <div class="col-aside d-print-none js-col-aside"><aside class="page__aside js-page-aside"><div class="toc-aside js-toc-root"></div></aside></div>

              <div class="col-main cell cell--auto"><!-- start custom main top snippet -->

<!-- end custom main top snippet --><article itemscope itemtype="http://schema.org/Article"><script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script><div class="article__header"><div class="header-nav"><div class="main-docs">
  <ul class="breadcrambs">
    <li><a href="/docs">Documentation</a></li>
    <li>Transformers</li>
  </ul>
</div></div><header><h1>Transformers</h1></header><span class="split-space">&nbsp;</span>
          <a class="edit-on-github"
            title="Edit on Github"
            href="https://github.com/johnsnowlabs/spark-nlp/tree/master/docs/en/transformers.md">
            <i class="far fa-edit"></i></a></div><meta itemprop="headline" content="Transformers"><meta itemprop="author" content=""/><div class="js-article-content"><div class="docs-wrapper">
<div class="layout--article"><!-- start custom article top snippet -->

<!-- end custom article top snippet --><div class="article__content" itemprop="articleBody"><script> jQuery(document).ready(function () {
    $(".prediction-button").click(function () {
        $(this).closest(".tabs-box").find(".prediction-button").removeClass('code-selector-un-active').addClass("code-selector-active");

        // remove active class from all other buttons
        $(this).closest(".tabs-box").find(".training-button").removeClass('code-selector-active').addClass('code-selector-un-active');
        $(this).closest(".tabs-box").find(".embeddings-button").removeClass('code-selector-active').addClass('code-selector-un-active');

        //toggle content
        $(this.parentNode).siblings(".tabs-box.training-content").hide()
        $(this.parentNode).siblings(".tabs-box.embeddings-content").hide()
        $(this.parentNode).siblings(".tabs-box.prediction-content").show()
    });

    $(".training-button").click(function () {
        $(this).closest(".tabs-box").find(".training-button").removeClass('code-selector-un-active').addClass("code-selector-active");

        // remove active class from all other buttons
        $(this).closest(".tabs-box").find(".prediction-button").removeClass('code-selector-active').addClass('code-selector-un-active');
        $(this).closest(".tabs-box").find(".embeddings-button").removeClass('code-selector-active').addClass('code-selector-un-active');

        //toggle content
        $(this.parentNode).siblings(".tabs-box.prediction-content").hide()
        $(this.parentNode).siblings(".tabs-box.embeddings-content").hide()
        $(this.parentNode).siblings(".tabs-box.training-content").show()
    });

    $(".embeddings-button").click(function () {
        $(this).closest(".tabs-box").find(".embeddings-button").removeClass('code-selector-un-active').addClass("code-selector-active");

        // remove active class from all other buttons
        $(this).closest(".tabs-box").find(".training-button").removeClass('code-selector-active').addClass('code-selector-un-active');
        $(this).closest(".tabs-box").find(".prediction-button").removeClass('code-selector-active').addClass('code-selector-un-active');

        //toggle content
        $(this.parentNode).siblings(".tabs-box.training-content").hide()
        $(this.parentNode).siblings(".tabs-box.prediction-content").hide()
        $(this.parentNode).siblings(".tabs-box.embeddings-content").show()
    });
});
 </script>

<div class="h3-box model-content">

  <h2 id="albertembeddings">AlbertEmbeddings</h2>

  <p>ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS - Google Research, Toyota Technological Institute at Chicago</p>

  <p>These word embeddings represent the outputs generated by the Albert model.
All official Albert releases by google in TF-HUB are supported with this Albert Wrapper:</p>

  <p><strong>Ported TF-Hub Models:</strong></p>

  <table>
    <thead>
      <tr>
        <th>Spark NLP Model</th>
        <th>TF-Hub Model</th>
        <th>Model Properties</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code class="language-plaintext highlighter-rouge">"albert_base_uncased"</code></td>
        <td><a href="https://tfhub.dev/google/albert_base/3">albert_base</a></td>
        <td>768-embed-dim,   12-layer,  12-heads, 12M parameters</td>
      </tr>
      <tr>
        <td><code class="language-plaintext highlighter-rouge">"albert_large_uncased"</code></td>
        <td><a href="https://tfhub.dev/google/albert_large/3">albert_large</a></td>
        <td>1024-embed-dim,  24-layer,  16-heads, 18M parameters</td>
      </tr>
      <tr>
        <td><code class="language-plaintext highlighter-rouge">"albert_xlarge_uncased"</code></td>
        <td><a href="https://tfhub.dev/google/albert_xlarge/3">albert_xlarge</a></td>
        <td>2048-embed-dim,  24-layer,  32-heads, 60M parameters</td>
      </tr>
      <tr>
        <td><code class="language-plaintext highlighter-rouge">"albert_xxlarge_uncased"</code></td>
        <td><a href="https://tfhub.dev/google/albert_xxlarge/3">albert_xxlarge</a></td>
        <td>4096-embed-dim,  12-layer,  64-heads, 235M parameters</td>
      </tr>
    </tbody>
  </table>

  <p>This model requires input tokenization with SentencePiece model, which is provided by Spark-NLP (See tokenizers package).</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val embeddings = AlbertEmbeddings.pretrained()
 .setInputCols("sentence", "token")
 .setOutputCol("embeddings")

# Offline - Download the pretrained model manually and extract it
albert = AlbertEmbeddings.load("/albert_base_uncased_en_2.5.0_2.4_1588073363475") \
        .setInputCols("sentence", "token") \
        .setOutputCol("albert")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"albert_base_uncased"</code>, if no name is provided.</p>

  <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_albert.ipynb">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/AlbertEmbeddingsTestSpec.scala">AlbertEmbeddingsTestSpec</a>.</p>

  <p><strong>Sources:</strong></p>

  <p><a href="https://arxiv.org/pdf/1909.11942.pdf">ALBERT: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS</a></p>

  <p>https://github.com/google-research/ALBERT</p>

  <p>https://tfhub.dev/s?q=albert</p>

  <p><strong>Paper abstract:</strong></p>

  <p><em>Increasing model size when pretraining natural language representations often results in improved performance on
downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and
longer training times. To address these problems, we present two parameter reduction techniques to lower memory
consumption and increase the training speed of BERT (Devlin et al., 2019). Comprehensive empirical evidence shows
that our proposed methods lead to models that scale much better compared to
the original BERT. We also use a self-supervised loss that focuses on modeling
inter-sentence coherence, and show it consistently helps downstream tasks with
multi-sentence inputs. As a result, our best model establishes new state-of-the-art
results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.</em></p>

  <p><strong>Tips:</strong>
ALBERT uses repeating layers which results in a small memory footprint,
however the computational cost remains similar to a BERT-like architecture with
the same number of hidden layers as it has to iterate through the same number of (repeating) layers.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">WORD_EMBEDDINGS</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.AlbertEmbeddings.html">AlbertEmbeddings</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/AlbertEmbeddings">AlbertEmbeddings</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/AlbertEmbeddings.scala">AlbertEmbeddings</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLModel
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">AlbertEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"albert_base_uncased"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">'document'</span><span class="p">,</span> <span class="s">'token'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">'embeddings'</span><span class="p">)</span>

<span class="c1"># This pretrained model requires those specific transformer embeddings
</span><span class="n">ner_model</span> <span class="o">=</span> <span class="n">NerDLModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"ner_conll_albert_base_uncased"</span><span class="p">,</span> <span class="s">"en"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">ner_model</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"ner.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">I</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLModel</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="c1">// Use the transformer embeddings</span>
<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">AlbertEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"albert_base_uncased"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// This pretrained model requires those specific transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerModel</span> <span class="k">=</span> <span class="nv">NerDLModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"ner_conll_albert_base_uncased"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerModel</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|[</span><span class="kt">I-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">I-LOC</span>, <span class="kt">O</span><span class="o">]|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLApproach
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">AlbertEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="c1"># Then the training can start
</span><span class="n">nerTagger</span> <span class="o">=</span> <span class="n">NerDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setRandomSeed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setVerbose</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">nerTagger</span>
<span class="p">])</span>

<span class="c1"># We use the text and labels from the CoNLL dataset
</span><span class="n">conll</span> <span class="o">=</span> <span class="n">CoNLL</span><span class="p">()</span>
<span class="n">trainingData</span> <span class="o">=</span> <span class="n">conll</span><span class="p">.</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s">"eng.train"</span><span class="p">)</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.CoNLL</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLApproach</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">AlbertEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// Then the training can start</span>
<span class="k">val</span> <span class="nv">nerTagger</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NerDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setVerbose</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerTagger</span>
<span class="o">))</span>

<span class="c1">// We use the text and labels from the CoNLL dataset</span>
<span class="k">val</span> <span class="nv">conll</span> <span class="k">=</span> <span class="nc">CoNLL</span><span class="o">()</span>
<span class="k">val</span> <span class="nv">trainingData</span> <span class="k">=</span> <span class="nv">conll</span><span class="o">.</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">AlbertEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="n">embeddingsFinisher</span> <span class="o">=</span> <span class="n">EmbeddingsFinisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"finished_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputAsVector</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCleanAnnotations</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">embeddingsFinisher</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"This is a sentence."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="mf">1.1342473030090332</span><span class="p">,</span><span class="o">-</span><span class="mf">1.3855540752410889</span><span class="p">,</span><span class="mf">0.9818322062492371</span><span class="p">,</span><span class="o">-</span><span class="mf">0.784737348556518</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.847029983997345</span><span class="p">,</span><span class="o">-</span><span class="mf">1.047153353691101</span><span class="p">,</span><span class="o">-</span><span class="mf">0.1520637571811676</span><span class="p">,</span><span class="o">-</span><span class="mf">0.6245765686035156</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.009860038757324219</span><span class="p">,</span><span class="o">-</span><span class="mf">0.13450059294700623</span><span class="p">,</span><span class="mf">2.707749128341675</span><span class="p">,</span><span class="mf">1.2916892766952</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.04192575812339783</span><span class="p">,</span><span class="o">-</span><span class="mf">0.5764210224151611</span><span class="p">,</span><span class="o">-</span><span class="mf">0.3196685314178467</span><span class="p">,</span><span class="o">-</span><span class="mf">0.527840495109</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.15583214163780212</span><span class="p">,</span><span class="o">-</span><span class="mf">0.1614152491092682</span><span class="p">,</span><span class="o">-</span><span class="mf">0.28423872590065</span><span class="p">,</span><span class="o">-</span><span class="mf">0.135491415858268</span><span class="p">...</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.AlbertEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.EmbeddingsFinisher</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">AlbertEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddingsFinisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">EmbeddingsFinisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"finished_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputAsVector</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCleanAnnotations</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">embeddingsFinisher</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"This is a sentence."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">80</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="err">1</span><span class="kt">.</span><span class="err">1342473030090332</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">3855540752410889</span>,<span class="err">0</span><span class="kt">.</span><span class="err">9818322062492371</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">784737348556518</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">847029983997345</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">047153353691101</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">1520637571811676</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">6245765686035156</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">009860038757324219</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">13450059294700623</span>,<span class="err">2</span><span class="kt">.</span><span class="err">707749128341675</span>,<span class="err">1</span><span class="kt">.</span><span class="err">2916892766952</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">04192575812339783</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">5764210224151611</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">3196685314178467</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">527840495109</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">15583214163780212</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">1614152491092682</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">28423872590065</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">135491415858268</span><span class="kt">...|</span>
<span class="kt">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="albertfortokenclassification">AlbertForTokenClassification</h2>

  <p>AlbertForTokenClassification can load ALBERT Models with a token classification head on top (a linear layer on top of the hidden-states output)
e.g. for Named-Entity-Recognition (NER) tasks.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val tokenClassifier = AlbertForTokenClassification.pretrained()
  .setInputCols("token", "document")
  .setOutputCol("label")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"albert_base_token_classifier_conll03"</code>, if no name is provided.</p>

  <p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition">Models Hub</a>.</p>

  <p>and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/AlbertForTokenClassificationTestSpec.scala">AlbertForTokenClassificationTestSpec</a>.
Models from the HuggingFace  Transformers library are also compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.AlbertForTokenClassification.html">AlbertForTokenClassification</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/classifier/dl/AlbertForTokenClassification">AlbertForTokenClassification</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/AlbertForTokenClassification.scala">AlbertForTokenClassification</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">AlbertForTokenClassification</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenClassifier</span> <span class="k">=</span> <span class="nv">AlbertForTokenClassification</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"label.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">B-PER</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span><span class="o">]|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator needs to be trained externally. Please see the training page
# for instructions.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator needs to be trained externally. Please see the training page</span>
<span class="c1">// for instructions.</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator has a fully connected layer attached for classification. For
# embeddings see the base transformer annotator.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator has a fully connected layer attached for classification. For</span>
<span class="c1">// embeddings see the base transformer annotator.</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="bertembeddings">BertEmbeddings</h2>

  <p>Token-level embeddings using BERT. BERT (Bidirectional Encoder Representations from Transformers) provides dense
vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val embeddings = BertEmbeddings.pretrained()
  .setInputCols("token", "document")
  .setOutputCol("bert_embeddings")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"small_bert_L2_768"</code>, if no name is provided.</p>

  <p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings">Models Hub</a>.</p>

  <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/blogposts/3.NER_with_BERT.ipynb">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/BertEmbeddingsTestSpec.scala">BertEmbeddingsTestSpec</a>.</p>

  <p><strong>Sources</strong> :</p>

  <p><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

  <p>https://github.com/google-research/bert</p>

  <p><strong>Paper abstract</strong></p>

  <p><em>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations
from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional
representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a
result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering and language inference, without
substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It
obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score
to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1
question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point
absolute improvement).</em></p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">WORD_EMBEDDINGS</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.BertEmbeddings.html">BertEmbeddings</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/BertEmbeddings">BertEmbeddings</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/BertEmbeddings.scala">BertEmbeddings</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLModel
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">BertEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'bert_base_cased'</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s">'en'</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">'document'</span><span class="p">,</span> <span class="s">'token'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">'embeddings'</span><span class="p">)</span>

<span class="c1"># This pretrained model requires those specific transformer embeddings
</span><span class="n">ner_model</span> <span class="o">=</span> <span class="n">NerDLModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"ner_dl_bert"</span><span class="p">,</span> <span class="s">"en"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">ner_model</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"ner.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">I</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.BertEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLModel</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="c1">// Use the transformer embeddings</span>
<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">BertEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="n">name</span> <span class="k">=</span> <span class="s">"bert_base_cased"</span><span class="o">,</span> <span class="n">lang</span> <span class="k">=</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// This pretrained model requires those specific transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerModel</span> <span class="k">=</span> <span class="nv">NerDLModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"ner_dl_bert"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerModel</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|[</span><span class="kt">I-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">I-LOC</span>, <span class="kt">O</span><span class="o">]|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLApproach
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">BertEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"bert_base_cased"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="c1"># Then the training can start with the transformer embeddings
</span><span class="n">nerTagger</span> <span class="o">=</span> <span class="n">NerDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setVerbose</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">nerTagger</span>
<span class="p">])</span>

<span class="c1"># We use the text and labels from the CoNLL dataset
</span><span class="n">conll</span> <span class="o">=</span> <span class="n">CoNLL</span><span class="p">()</span>
<span class="n">trainingData</span> <span class="o">=</span> <span class="n">conll</span><span class="p">.</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s">"eng.train"</span><span class="p">)</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.BertEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.CoNLL</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLApproach</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">BertEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// Then the training can start with the transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerTagger</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NerDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setVerbose</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerTagger</span>
<span class="o">))</span>

<span class="c1">// We use the text and labels from the CoNLL dataset</span>
<span class="k">val</span> <span class="nv">conll</span> <span class="k">=</span> <span class="nc">CoNLL</span><span class="o">()</span>
<span class="k">val</span> <span class="nv">trainingData</span> <span class="k">=</span> <span class="nv">conll</span><span class="o">.</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">BertEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"small_bert_L2_128"</span><span class="p">,</span> <span class="s">"en"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"bert_embeddings"</span><span class="p">)</span>

<span class="n">embeddingsFinisher</span> <span class="o">=</span> <span class="n">EmbeddingsFinisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"bert_embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"finished_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputAsVector</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">embeddingsFinisher</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"This is a sentence."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">2.3497989177703857</span><span class="p">,</span><span class="mf">0.480538547039032</span><span class="p">,</span><span class="o">-</span><span class="mf">0.3238905668258667</span><span class="p">,</span><span class="o">-</span><span class="mf">1.612930893898010</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">2.1357314586639404</span><span class="p">,</span><span class="mf">0.32984697818756104</span><span class="p">,</span><span class="o">-</span><span class="mf">0.6032363176345825</span><span class="p">,</span><span class="o">-</span><span class="mf">1.6791689395904</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">1.8244884014129639</span><span class="p">,</span><span class="o">-</span><span class="mf">0.27088963985443115</span><span class="p">,</span><span class="o">-</span><span class="mf">1.059438943862915</span><span class="p">,</span><span class="o">-</span><span class="mf">0.9817547798156</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">1.1648050546646118</span><span class="p">,</span><span class="o">-</span><span class="mf">0.4725411534309387</span><span class="p">,</span><span class="o">-</span><span class="mf">0.5938255786895752</span><span class="p">,</span><span class="o">-</span><span class="mf">1.5780693292617</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.9125322699546814</span><span class="p">,</span><span class="mf">0.4563939869403839</span><span class="p">,</span><span class="o">-</span><span class="mf">0.3975459933280945</span><span class="p">,</span><span class="o">-</span><span class="mf">1.81611204147338</span><span class="p">...</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.BertEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.EmbeddingsFinisher</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">BertEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"small_bert_L2_128"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"bert_embeddings"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddingsFinisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">EmbeddingsFinisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"bert_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"finished_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputAsVector</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">embeddingsFinisher</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"This is a sentence."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">80</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">-</span><span class="err">2</span><span class="kt">.</span><span class="err">3497989177703857</span>,<span class="err">0</span><span class="kt">.</span><span class="err">480538547039032</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">3238905668258667</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">612930893898010</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">2</span><span class="kt">.</span><span class="err">1357314586639404</span>,<span class="err">0</span><span class="kt">.</span><span class="err">32984697818756104</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">6032363176345825</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">6791689395904</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">8244884014129639</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">27088963985443115</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">059438943862915</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">9817547798156</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">1648050546646118</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">4725411534309387</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">5938255786895752</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">5780693292617</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">9125322699546814</span>,<span class="err">0</span><span class="kt">.</span><span class="err">4563939869403839</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">3975459933280945</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">81611204147338</span><span class="kt">...|</span>
<span class="kt">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="bertforsequenceclassification">BertForSequenceClassification</h2>

  <p>BertForSequenceClassification can load Bert Models with sequence classification/regression head on top (a linear layer on top of the pooled output)
e.g. for multi-class document classification tasks.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val sequenceClassifier = BertForSequenceClassification.pretrained()
  .setInputCols("token", "document")
  .setOutputCol("label")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"bert_base_sequence_classifier_imdb"</code>, if no name is provided.</p>

  <p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Text+Classification">Models Hub</a>.</p>

  <p>Models from the HuggingFace  Transformers library are also compatible with Spark NLP . The Spark NLP Workshop
example shows how to import them https://github.com/JohnSnowLabs/spark-nlp/discussions/5669.
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/BertForSequenceClassificationTestSpec.scala">BertForSequenceClassificationTestSpec</a>.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">CATEGORY</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.BertForSequenceClassification.html">BertForSequenceClassification</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/classifier/dl/BertForSequenceClassification">BertForSequenceClassification</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/BertForSequenceClassification.scala">BertForSequenceClassification</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

      <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">sequenceClassifier</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">sequenceClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"""John Lenon was born in London and lived
    in Paris. My name is Sarah and I live in London"""</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+--------------------+</span>
<span class="o">|</span><span class="n">result</span>              <span class="o">|</span>
<span class="o">+--------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">neg</span><span class="p">,</span> <span class="n">neg</span><span class="p">]</span>          <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">pos</span><span class="p">]</span><span class="o">|</span>
<span class="o">+--------------------+</span>
</code></pre></div>      </div>

      <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sequenceClassifier</span> <span class="k">=</span> <span class="nv">BertForSequenceClassification</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">sequenceClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"label.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+--------------------+</span>
<span class="o">|</span><span class="n">result</span>              <span class="o">|</span>
<span class="o">+--------------------+</span>
<span class="o">|[</span><span class="kt">neg</span>, <span class="kt">neg</span><span class="o">]</span>          <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="kt">pos</span>, <span class="kt">pos</span>, <span class="kt">pos</span><span class="o">]|</span>
<span class="o">+--------------------+</span>
</code></pre></div>      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="bertfortokenclassification">BertForTokenClassification</h2>

  <p>BertForTokenClassification can load Bert Models with a token classification head on top (a linear layer on top of the hidden-states output)
e.g. for Named-Entity-Recognition (NER) tasks.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val labels = BertForTokenClassification.pretrained()
  .setInputCols("token", "document")
  .setOutputCol("label")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"bert_base_token_classifier_conll03"</code>, if no name is provided.</p>

  <p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Text+Classification">Models Hub</a>.</p>

  <p>and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/BertForTokenClassificationTestSpec.scala">BertForTokenClassificationTestSpec</a>.
Models from the HuggingFace  Transformers library are also compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.BertForTokenClassification.html">BertForTokenClassification</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/classifier/dl/BertForTokenClassification">BertForTokenClassification</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/BertForTokenClassification.scala">BertForTokenClassification</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">BertForTokenClassification</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenClassifier</span> <span class="k">=</span> <span class="nv">BertForTokenClassification</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"label.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">B-PER</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span><span class="o">]|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator needs to be trained externally. Please see the training page
# for instructions.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator needs to be trained externally. Please see the training page</span>
<span class="c1">// for instructions.</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator has a fully connected layer attached for classification. For
# embeddings see the base transformer annotator.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator has a fully connected layer attached for classification. For</span>
<span class="c1">// embeddings see the base transformer annotator.</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="bertsentenceembeddings">BertSentenceEmbeddings</h2>

  <p>Sentence-level embeddings using BERT. BERT (Bidirectional Encoder Representations from Transformers) provides dense
vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val embeddings = BertSentenceEmbeddings.pretrained()
  .setInputCols("sentence")
  .setOutputCol("sentence_bert_embeddings")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"sent_small_bert_L2_768"</code>, if no name is provided.</p>

  <p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings">Models Hub</a>.</p>

  <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BERT%20Sentence.ipynb">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/BertSentenceEmbeddingsTestSpec.scala">BertSentenceEmbeddingsTestSpec</a>.</p>

  <p><strong>Sources</strong> :</p>

  <p><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

  <p>https://github.com/google-research/bert</p>

  <p><strong>Paper abstract</strong></p>

  <p><em>We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations
from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional
representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a
result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering and language inference, without
substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It
obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score
to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1
question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point
absolute improvement).</em></p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">SENTENCE_EMBEDDINGS</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.BertSentenceEmbeddings.html">BertSentenceEmbeddings</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/BertSentenceEmbeddings">BertSentenceEmbeddings</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/BertSentenceEmbeddings.scala">BertSentenceEmbeddings</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the ClassifierDLModel
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">BertSentenceEmbeddings</span>\
  <span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'sent_bert_multi_cased'</span><span class="p">,</span> <span class="s">'xx'</span><span class="p">)</span> \
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span>

<span class="c1"># This pretrained model requires those specific transformer embeddings
</span><span class="n">document_classifier</span> <span class="o">=</span> <span class="n">ClassifierDLModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"classifierdl_bert_news"</span><span class="p">,</span> <span class="s">"de"</span><span class="p">)</span> \
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"sentence_embeddings"</span><span class="p">])</span> \
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"class"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">document_classifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"Dressurreiterin Jessica von Bredow-Werndl hat ihr zweites Olympia-Gold gewonnen"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"class.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+-------+</span>
<span class="o">|</span><span class="n">result</span> <span class="o">|</span>
<span class="o">+-------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">Sport</span><span class="p">]</span><span class="o">|</span>
<span class="o">+-------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator.ClassifierDLModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the ClassifierDLModel</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="c1">// Use the transformer embeddings</span>
<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">BertSentenceEmbeddings</span>
<span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"sent_bert_multi_cased"</span><span class="o">,</span> <span class="s">"xx"</span><span class="o">)</span>
<span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
<span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>

<span class="c1">// This pretrained model requires those specific transformer embeddings</span>
<span class="k">val</span> <span class="nv">document_classifier</span> <span class="k">=</span> <span class="nv">ClassifierDLModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"classifierdl_bert_news"</span><span class="o">,</span> <span class="s">"de"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"sentence_embeddings"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"class"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">document_classifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"Dressurreiterin Jessica von Bredow-Werndl hat ihr zweites Olympia-Gold gewonnen"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+-------+</span>
<span class="o">|</span><span class="n">result</span> <span class="o">|</span>
<span class="o">+-------+</span>
<span class="o">|[</span><span class="kt">Sport</span><span class="o">]|</span>
<span class="o">+-------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">smallCorpus</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span><span class="s">"True"</span><span class="p">).</span><span class="n">csv</span><span class="p">(</span><span class="s">"sentiment.csv"</span><span class="p">)</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">BertSentenceEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span>

<span class="c1"># Then the training can start with the transformer embeddings
</span><span class="n">docClassifier</span> <span class="o">=</span> <span class="n">ClassifierDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"category"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setBatchSize</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLr</span><span class="p">(</span><span class="mf">5e-3</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setDropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">docClassifier</span>
<span class="p">])</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">smallCorpus</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">smallCorpus</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span> <span class="s">"true"</span><span class="o">).</span><span class="py">csv</span><span class="o">(</span><span class="s">"sentiment.csv"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">BertSentenceEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>

<span class="c1">// Then the training can start with the transformer embeddings</span>
<span class="k">val</span> <span class="nv">docClassifier</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ClassifierDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"category"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setBatchSize</span><span class="o">(</span><span class="mi">64</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">20</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLr</span><span class="o">(</span><span class="mi">5</span><span class="n">e</span><span class="o">-</span><span class="mf">3f</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setDropout</span><span class="o">(</span><span class="mf">0.5f</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">docClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">smallCorpus</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">BertSentenceEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"sent_small_bert_L2_128"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence_bert_embeddings"</span><span class="p">)</span>

<span class="n">embeddingsFinisher</span> <span class="o">=</span> <span class="n">EmbeddingsFinisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence_bert_embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"finished_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputAsVector</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">embeddingsFinisher</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"John loves apples. Mary loves oranges. John loves Mary."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.8951074481010437</span><span class="p">,</span><span class="mf">0.13753940165042877</span><span class="p">,</span><span class="mf">0.3108254075050354</span><span class="p">,</span><span class="o">-</span><span class="mf">1.65693199634552</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.6180210709571838</span><span class="p">,</span><span class="o">-</span><span class="mf">0.12179657071828842</span><span class="p">,</span><span class="o">-</span><span class="mf">0.191165953874588</span><span class="p">,</span><span class="o">-</span><span class="mf">1.4497021436691</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.822715163230896</span><span class="p">,</span><span class="mf">0.7568016648292542</span><span class="p">,</span><span class="o">-</span><span class="mf">0.1165061742067337</span><span class="p">,</span><span class="o">-</span><span class="mf">1.59048593044281</span><span class="p">,...</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.EmbeddingsFinisher</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">BertSentenceEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"sent_small_bert_L2_128"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence_bert_embeddings"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddingsFinisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">EmbeddingsFinisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence_bert_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"finished_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputAsVector</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">embeddingsFinisher</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John loves apples. Mary loves oranges. John loves Mary."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">80</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">8951074481010437</span>,<span class="err">0</span><span class="kt">.</span><span class="err">13753940165042877</span>,<span class="err">0</span><span class="kt">.</span><span class="err">3108254075050354</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">65693199634552</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">6180210709571838</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">12179657071828842</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">191165953874588</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">4497021436691</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">822715163230896</span>,<span class="err">0</span><span class="kt">.</span><span class="err">7568016648292542</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">1165061742067337</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">59048593044281</span>,<span class="kt">...|</span>
<span class="kt">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="distilbertembeddings">DistilBertEmbeddings</h2>

  <p>DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than
<code class="language-plaintext highlighter-rouge">bert-base-uncased</code>, runs 60% faster while preserving over 95% of BERTs performances as measured on the GLUE language understanding benchmark.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val embeddings = DistilBertEmbeddings.pretrained()
  .setInputCols("document", "token")
  .setOutputCol("embeddings")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"distilbert_base_cased"</code>, if no name is provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings">Models Hub</a>.</p>

  <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20DistilBERT.ipynb">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/DistilBertEmbeddingsTestSpec.scala">DistilBertEmbeddingsTestSpec</a>.
Models from the HuggingFace  Transformers library are also compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p>The DistilBERT model was proposed in the paper
<a href="https://arxiv.org/abs/1910.01108">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a>.</p>

  <p><strong>Paper Abstract:</strong></p>

  <p><em>As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP),
operating these large models in on-the-edge and/or under constrained computational training or inference budgets
remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation
model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger
counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage
knowledge distillation during the pretraining phase and show that it is possible to reduce the size of a BERT model by
40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive
biases learned by larger models during pretraining, we introduce a triple loss combining language modeling,
distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we
demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device
study.</em></p>

  <p>Tips:</p>
  <ul>
    <li>DistilBERT doesnt have <code class="language-plaintext highlighter-rouge">:obj:token_type_ids</code>, you dont need to indicate which token belongs to which segment. Just
separate your segments with the separation token <code class="language-plaintext highlighter-rouge">:obj:tokenizer.sep_token</code> (or <code class="language-plaintext highlighter-rouge">:obj:[SEP]</code>).</li>
    <li>DistilBERT doesnt have options to select the input positions (<code class="language-plaintext highlighter-rouge">:obj:position_ids</code> input). This could be added if
necessary though, just let us know if you need this option.</li>
  </ul>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">WORD_EMBEDDINGS</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.DistilBertEmbeddings.html">DistilBertEmbeddings</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/DistilBertEmbeddings">DistilBertEmbeddings</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/DistilBertEmbeddings.scala">DistilBertEmbeddings</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLModel
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">DistilBertEmbeddings</span>\
      <span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'distilbert_base_cased'</span><span class="p">,</span> <span class="s">'en'</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
      <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="c1"># This pretrained model requires those specific transformer embeddings
</span><span class="n">ner_model</span> <span class="o">=</span> <span class="n">NerDLModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'ner_mit_movie_complex_distilbert_base_cased'</span><span class="p">,</span> <span class="s">'en'</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">'document'</span><span class="p">,</span> <span class="s">'token'</span><span class="p">,</span> <span class="s">'embeddings'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">'ner'</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">ner_model</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"The Grand Budapest Hotel is a 2014 comedy-drama film written and directed by Wes Anderson"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"ner.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+----------------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                                        <span class="o">|</span>
<span class="o">+----------------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">I</span><span class="o">-</span><span class="n">Plot</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">Plot</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">Plot</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">Plot</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">Year</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">Genre</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">Director</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">Director</span><span class="p">]</span><span class="o">|</span>
<span class="o">+----------------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLModel</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="c1">// Use the transformer embeddings</span>
<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">DistilBertEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"distilbert_base_cased"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// This pretrained model requires those specific transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerModel</span> <span class="k">=</span> <span class="nv">NerDLModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"ner_mit_movie_complex_distilbert_base_cased"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerModel</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"The Grand Budapest Hotel is a 2014 comedy-drama film written and directed by Wes Anderson"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+----------------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                                        <span class="o">|</span>
<span class="o">+----------------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">I-Plot</span>, <span class="kt">I-Plot</span>, <span class="kt">I-Plot</span>, <span class="kt">I-Plot</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-Year</span>, <span class="kt">B-Genre</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-Director</span>, <span class="kt">I-Director</span><span class="o">]|</span>
<span class="o">+----------------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLApproach
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">DistilBertEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="c1"># Then the training can start with the transformer embeddings
</span><span class="n">nerTagger</span> <span class="o">=</span> <span class="n">NerDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setVerbose</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">nerTagger</span>
<span class="p">])</span>

<span class="c1"># We use the text and labels from the CoNLL dataset
</span><span class="n">conll</span> <span class="o">=</span> <span class="n">CoNLL</span><span class="p">()</span>
<span class="n">trainingData</span> <span class="o">=</span> <span class="n">conll</span><span class="p">.</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s">"eng.train"</span><span class="p">)</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.CoNLL</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLApproach</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">DistilBertEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// Then the training can start with the transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerTagger</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NerDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setVerbose</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerTagger</span>
<span class="o">))</span>

<span class="c1">// We use the text and labels from the CoNLL dataset</span>
<span class="k">val</span> <span class="nv">conll</span> <span class="k">=</span> <span class="nc">CoNLL</span><span class="o">()</span>
<span class="k">val</span> <span class="nv">trainingData</span> <span class="k">=</span> <span class="nv">conll</span><span class="o">.</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">DistilBertEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">embeddingsFinisher</span> <span class="o">=</span> <span class="n">EmbeddingsFinisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"finished_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputAsVector</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCleanAnnotations</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">([</span>
      <span class="n">documentAssembler</span><span class="p">,</span>
      <span class="n">tokenizer</span><span class="p">,</span>
      <span class="n">embeddings</span><span class="p">,</span>
      <span class="n">embeddingsFinisher</span>
    <span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"This is a sentence."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.1127224713563919</span><span class="p">,</span><span class="o">-</span><span class="mf">0.1982710212469101</span><span class="p">,</span><span class="mf">0.5360898375511169</span><span class="p">,</span><span class="o">-</span><span class="mf">0.272536993026733</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.35534414649009705</span><span class="p">,</span><span class="mf">0.13215228915214539</span><span class="p">,</span><span class="mf">0.40981462597846985</span><span class="p">,</span><span class="mf">0.14036104083061</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.328085333108902</span><span class="p">,</span><span class="o">-</span><span class="mf">0.06269335001707077</span><span class="p">,</span><span class="o">-</span><span class="mf">0.017595693469047546</span><span class="p">,</span><span class="o">-</span><span class="mf">0.024373905733</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.15617232024669647</span><span class="p">,</span><span class="mf">0.2967822253704071</span><span class="p">,</span><span class="mf">0.22324979305267334</span><span class="p">,</span><span class="o">-</span><span class="mf">0.04568954557180</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.45411425828933716</span><span class="p">,</span><span class="mf">0.01173491682857275</span><span class="p">,</span><span class="mf">0.190129816532135</span><span class="p">,</span><span class="mf">0.1178255230188369</span><span class="p">...</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.EmbeddingsFinisher</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">DistilBertEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddingsFinisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">EmbeddingsFinisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"finished_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputAsVector</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCleanAnnotations</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
    <span class="n">documentAssembler</span><span class="o">,</span>
    <span class="n">tokenizer</span><span class="o">,</span>
    <span class="n">embeddings</span><span class="o">,</span>
    <span class="n">embeddingsFinisher</span>
  <span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"This is a sentence."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">80</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="err">0</span><span class="kt">.</span><span class="err">1127224713563919</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">1982710212469101</span>,<span class="err">0</span><span class="kt">.</span><span class="err">5360898375511169</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">272536993026733</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">35534414649009705</span>,<span class="err">0</span><span class="kt">.</span><span class="err">13215228915214539</span>,<span class="err">0</span><span class="kt">.</span><span class="err">40981462597846985</span>,<span class="err">0</span><span class="kt">.</span><span class="err">14036104083061</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">328085333108902</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">06269335001707077</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">017595693469047546</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">024373905733</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">15617232024669647</span>,<span class="err">0</span><span class="kt">.</span><span class="err">2967822253704071</span>,<span class="err">0</span><span class="kt">.</span><span class="err">22324979305267334</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">04568954557180</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">45411425828933716</span>,<span class="err">0</span><span class="kt">.</span><span class="err">01173491682857275</span>,<span class="err">0</span><span class="kt">.</span><span class="err">190129816532135</span>,<span class="err">0</span><span class="kt">.</span><span class="err">1178255230188369</span><span class="kt">...|</span>
<span class="kt">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="distilbertforsequenceclassification">DistilBertForSequenceClassification</h2>

  <p>DistilBertForSequenceClassification can load DistilBERT Models with sequence classification/regression head on top
(a linear layer on top of the pooled output) e.g. for multi-class document classification tasks.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val sequenceClassifier = DistilBertForSequenceClassification.pretrained()
  .setInputCols("token", "document")
  .setOutputCol("label")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"distilbert_base_sequence_classifier_imdb"</code>, if no name is provided.</p>

  <p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Text+Classification">Models Hub</a>.</p>

  <p>Models from the HuggingFace  Transformers library are also compatible with Spark NLP . The Spark NLP Workshop
example shows how to import them https://github.com/JohnSnowLabs/spark-nlp/discussions/5669.
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/DistilBertForSequenceClassificationTestSpec.scala">DistilBertForSequenceClassificationTestSpec</a>.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">CATEGORY</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.DistilBertForSequenceClassification.html">DistilBertForSequenceClassification</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/classifier/dl/DistilBertForSequenceClassification">DistilBertForSequenceClassification</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/DistilBertForSequenceClassification.scala">DistilBertForSequenceClassification</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

      <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">sequenceClassifier</span> <span class="o">=</span> <span class="n">DistilBertForSequenceClassification</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">sequenceClassifier</span>
<span class="p">])</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"""John Lenon was born in London and lived
in Paris. My name is Sarah and I live in London"""</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+--------------------+</span>
<span class="o">|</span><span class="n">result</span>              <span class="o">|</span>
<span class="o">+--------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">neg</span><span class="p">,</span> <span class="n">neg</span><span class="p">]</span>          <span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">pos</span><span class="p">,</span> <span class="n">pos</span><span class="p">]</span><span class="o">|</span>
<span class="o">+--------------------+</span>
</code></pre></div>      </div>

      <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sequenceClassifier</span> <span class="k">=</span> <span class="nv">DistilBertForSequenceClassification</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">sequenceClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"label.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+--------------------+</span>
<span class="o">|</span><span class="n">result</span>              <span class="o">|</span>
<span class="o">+--------------------+</span>
<span class="o">|[</span><span class="kt">neg</span>, <span class="kt">neg</span><span class="o">]</span>          <span class="o">|</span>
<span class="o">|[</span><span class="kt">pos</span>, <span class="kt">pos</span>, <span class="kt">pos</span>, <span class="kt">pos</span><span class="o">]|</span>
<span class="o">+--------------------+</span>
</code></pre></div>      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="distilbertfortokenclassification">DistilBertForTokenClassification</h2>

  <p>DistilBertForTokenClassification can load Bert Models with a token classification head on top (a linear layer on top of the hidden-states output)
e.g. for Named-Entity-Recognition (NER) tasks.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val labels = DistilBertForTokenClassification.pretrained()
  .setInputCols("token", "document")
  .setOutputCol("label")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"distilbert_base_token_classifier_conll03"</code>, if no name is provided.</p>

  <p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Text+Classification">Models Hub</a>.</p>

  <p>and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/DistilBertForTokenClassificationTestSpec.scala">DistilBertForTokenClassificationTestSpec</a>.
Models from the HuggingFace  Transformers library are also compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.DistilBertForTokenClassification.html">DistilBertForTokenClassification</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/classifier/dl/DistilBertForTokenClassification">DistilBertForTokenClassification</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/DistilBertForTokenClassification.scala">DistilBertForTokenClassification</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLModel
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">DistilBertEmbeddings</span>\
      <span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'distilbert_base_cased'</span><span class="p">,</span> <span class="s">'en'</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
      <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="c1"># This pretrained model requires those specific transformer embeddings
</span><span class="n">ner_model</span> <span class="o">=</span> <span class="n">NerDLModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'ner_mit_movie_complex_distilbert_base_cased'</span><span class="p">,</span> <span class="s">'en'</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">'document'</span><span class="p">,</span> <span class="s">'token'</span><span class="p">,</span> <span class="s">'embeddings'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">'ner'</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">ner_model</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"The Grand Budapest Hotel is a 2014 comedy-drama film written and directed by Wes Anderson"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"ner.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+----------------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                                        <span class="o">|</span>
<span class="o">+----------------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">I</span><span class="o">-</span><span class="n">Plot</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">Plot</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">Plot</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">Plot</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">Year</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">Genre</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">Director</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">Director</span><span class="p">]</span><span class="o">|</span>
<span class="o">+----------------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.DistilBertEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLModel</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="c1">// Use the transformer embeddings</span>
<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">DistilBertEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"distilbert_base_cased"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// This pretrained model requires those specific transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerModel</span> <span class="k">=</span> <span class="nv">NerDLModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"ner_mit_movie_complex_distilbert_base_cased"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerModel</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"The Grand Budapest Hotel is a 2014 comedy-drama film written and directed by Wes Anderson"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+----------------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                                        <span class="o">|</span>
<span class="o">+----------------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">I-Plot</span>, <span class="kt">I-Plot</span>, <span class="kt">I-Plot</span>, <span class="kt">I-Plot</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-Year</span>, <span class="kt">B-Genre</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-Director</span>, <span class="kt">I-Director</span><span class="o">]|</span>
<span class="o">+----------------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator needs to be trained externally. Please see the training page
# for instructions.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator needs to be trained externally. Please see the training page</span>
<span class="c1">// for instructions.</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator has a fully connected layer attached for classification. For
# embeddings see the base transformer annotator.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator has a fully connected layer attached for classification. For</span>
<span class="c1">// embeddings see the base transformer annotator.</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="elmoembeddings">ElmoEmbeddings</h2>

  <p>Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark.</p>

  <p>Note that this is a very computationally expensive module compared to word embedding modules that only perform
embedding lookups. The use of an accelerator is recommended.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val embeddings = ElmoEmbeddings.pretrained()
  .setInputCols("sentence", "token")
  .setOutputCol("elmo_embeddings")

# Offline - Download the pretrained model manually and extract it
elmo = ElmoEmbeddings.load("/elmo_en_2.4.0_2.4_1580488815299") \
        .setInputCols("sentence", "token") \
        .setOutputCol("elmo")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"elmo"</code>, if no name is provided.</p>

  <p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings">Models Hub</a>.</p>

  <p>The pooling layer can be set with <code class="language-plaintext highlighter-rouge">setPoolingLayer</code> to the following values:</p>
  <ul>
    <li><code class="language-plaintext highlighter-rouge">"word_emb"</code>: the character-based word representations with shape <code class="language-plaintext highlighter-rouge">[batch_size, max_length, 512]</code>.</li>
    <li><code class="language-plaintext highlighter-rouge">"lstm_outputs1"</code>: the first LSTM hidden state with shape <code class="language-plaintext highlighter-rouge">[batch_size, max_length, 1024]</code>.</li>
    <li><code class="language-plaintext highlighter-rouge">"lstm_outputs2"</code>: the second LSTM hidden state with shape <code class="language-plaintext highlighter-rouge">[batch_size, max_length, 1024]</code>.</li>
    <li><code class="language-plaintext highlighter-rouge">"elmo"</code>: the weighted sum of the 3 layers, where the weights are trainable. This tensor has shape <code class="language-plaintext highlighter-rouge">[batch_size, max_length, 1024]</code>.</li>
  </ul>

  <p>For extended examples of usage, see the
<a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_elmo.ipynb">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/ElmoEmbeddingsTestSpec.scala">ElmoEmbeddingsTestSpec</a>.</p>

  <p><strong>Sources:</strong></p>

  <p>https://tfhub.dev/google/elmo/3</p>

  <p><a href="https://arxiv.org/abs/1802.05365">Deep contextualized word representations</a></p>

  <p><strong>Paper abstract:</strong></p>

  <p><em>We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of
word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model
polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model
(biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to
existing models and significantly improve the state of the art across six challenging NLP problems, including
question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the
deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of
semi-supervision signals.</em></p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">WORD_EMBEDDINGS</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.ElmoEmbeddings.html">ElmoEmbeddings</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/ElmoEmbeddings">ElmoEmbeddings</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/ElmoEmbeddings.scala">ElmoEmbeddings</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLModel
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">ElmoEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">'document'</span><span class="p">,</span> <span class="s">'token'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">'embeddings'</span><span class="p">)</span>

<span class="c1"># This pretrained model requires those specific transformer embeddings
</span><span class="n">ner_model</span> <span class="o">=</span> <span class="n">NerDLModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"ner_conll_elmo"</span><span class="p">,</span> <span class="s">"en"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">ner_model</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"ner.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">I</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLModel</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="c1">// Use the transformer embeddings</span>
<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">ElmoEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// This pretrained model requires those specific transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerModel</span> <span class="k">=</span> <span class="nv">NerDLModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"ner_conll_elmo"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerModel</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|[</span><span class="kt">I-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">I-LOC</span>, <span class="kt">O</span><span class="o">]|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLApproach
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">ElmoEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setPoolingLayer</span><span class="p">(</span><span class="s">"word_emb"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="c1"># Then the training can start with the transformer embeddings
</span><span class="n">nerTagger</span> <span class="o">=</span> <span class="n">NerDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setVerbose</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">nerTagger</span>
<span class="p">])</span>

<span class="c1"># We use the text and labels from the CoNLL dataset
</span><span class="n">conll</span> <span class="o">=</span> <span class="n">CoNLL</span><span class="p">()</span>
<span class="n">trainingData</span> <span class="o">=</span> <span class="n">conll</span><span class="p">.</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s">"eng.train"</span><span class="p">)</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.CoNLL</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLApproach</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">ElmoEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setPoolingLayer</span><span class="o">(</span><span class="s">"word_emb"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// Then the training can start with the transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerTagger</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NerDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setVerbose</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerTagger</span>
<span class="o">))</span>

<span class="c1">// We use the text and labels from the CoNLL dataset</span>
<span class="k">val</span> <span class="nv">conll</span> <span class="k">=</span> <span class="nc">CoNLL</span><span class="o">()</span>
<span class="k">val</span> <span class="nv">trainingData</span> <span class="k">=</span> <span class="nv">conll</span><span class="o">.</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">ElmoEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setPoolingLayer</span><span class="p">(</span><span class="s">"word_emb"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="n">embeddingsFinisher</span> <span class="o">=</span> <span class="n">EmbeddingsFinisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"finished_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputAsVector</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCleanAnnotations</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">embeddingsFinisher</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"This is a sentence."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="mf">6.662458181381226E-4</span><span class="p">,</span><span class="o">-</span><span class="mf">0.2541114091873169</span><span class="p">,</span><span class="o">-</span><span class="mf">0.6275503039360046</span><span class="p">,</span><span class="mf">0.5787073969841</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.19154725968837738</span><span class="p">,</span><span class="mf">0.22998669743537903</span><span class="p">,</span><span class="o">-</span><span class="mf">0.2894386649131775</span><span class="p">,</span><span class="mf">0.21524395048618</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.10400570929050446</span><span class="p">,</span><span class="mf">0.12288510054349899</span><span class="p">,</span><span class="o">-</span><span class="mf">0.07056470215320587</span><span class="p">,</span><span class="o">-</span><span class="mf">0.246389418840</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.49932169914245605</span><span class="p">,</span><span class="o">-</span><span class="mf">0.12706467509269714</span><span class="p">,</span><span class="mf">0.30969417095184326</span><span class="p">,</span><span class="mf">0.2643227577209</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.8871506452560425</span><span class="p">,</span><span class="o">-</span><span class="mf">0.20039963722229004</span><span class="p">,</span><span class="o">-</span><span class="mf">1.0601330995559692</span><span class="p">,</span><span class="mf">0.0348707810044</span><span class="p">...</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.ElmoEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.EmbeddingsFinisher</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">ElmoEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setPoolingLayer</span><span class="o">(</span><span class="s">"word_emb"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddingsFinisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">EmbeddingsFinisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"finished_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputAsVector</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCleanAnnotations</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">embeddingsFinisher</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"This is a sentence."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">80</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="err">6</span><span class="kt">.</span><span class="err">662458181381226</span><span class="kt">E-</span><span class="err">4</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">2541114091873169</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">6275503039360046</span>,<span class="err">0</span><span class="kt">.</span><span class="err">5787073969841</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">19154725968837738</span>,<span class="err">0</span><span class="kt">.</span><span class="err">22998669743537903</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">2894386649131775</span>,<span class="err">0</span><span class="kt">.</span><span class="err">21524395048618</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">10400570929050446</span>,<span class="err">0</span><span class="kt">.</span><span class="err">12288510054349899</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">07056470215320587</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">246389418840</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">49932169914245605</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">12706467509269714</span>,<span class="err">0</span><span class="kt">.</span><span class="err">30969417095184326</span>,<span class="err">0</span><span class="kt">.</span><span class="err">2643227577209</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">8871506452560425</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">20039963722229004</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">0601330995559692</span>,<span class="err">0</span><span class="kt">.</span><span class="err">0348707810044</span><span class="kt">...|</span>
<span class="kt">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="longformerembeddings">LongformerEmbeddings</h2>

  <p>Longformer is a transformer model for long documents. The Longformer model was presented in <a href="https://arxiv.org/pdf/2004.05150.pdf">Longformer: The Long-Document Transformer</a> by Iz Beltagy, Matthew E. Peters, Arman Cohan.
longformer-base-4096 is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents.
It supports sequences of length up to 4,096.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val embeddings = LongformerEmbeddings.pretrained()
  .setInputCols("document", "token")
  .setOutputCol("embeddings")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"longformer_base_4096"</code>, if no name is provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings">Models Hub</a>.</p>

  <p>For some examples of usage, see <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/LongformerEmbeddingsTestSpec.scala">LongformerEmbeddingsTestSpec</a>.
Models from the HuggingFace  Transformers library are compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p><strong>Paper Abstract:</strong></p>

  <p><em>Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length.
To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer.
Longformers attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention.
Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8.
In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks.
Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA.
We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.</em></p>

  <p>The original code can be found <code class="language-plaintext highlighter-rouge">here</code> https://github.com/allenai/longformer.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">WORD_EMBEDDINGS</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.LongformerEmbeddings.html">LongformerEmbeddings</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/LongformerEmbeddings">LongformerEmbeddings</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/LongformerEmbeddings.scala">LongformerEmbeddings</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLModel
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">LongformerEmbeddings</span> \
      <span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"longformer_large_4096"</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">'document'</span><span class="p">,</span> <span class="s">'token'</span><span class="p">])</span> \
      <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">setMaxSentenceLength</span><span class="p">(</span><span class="mi">4096</span><span class="p">)</span>

<span class="c1"># This pretrained model requires those specific transformer embeddings
</span><span class="n">ner_model</span> <span class="o">=</span> <span class="n">NerDLModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'ner_conll_longformer_large_4096'</span><span class="p">,</span> <span class="s">'en'</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">'document'</span><span class="p">,</span> <span class="s">'token'</span><span class="p">,</span> <span class="s">'embeddings'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">'ner'</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">ner_model</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"ner.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">ORG</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLModel</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="c1">// Use the transformer embeddings</span>
<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">LongformerEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"longformer_large_4096"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxSentenceLength</span><span class="o">(</span><span class="mi">4096</span><span class="o">)</span>

<span class="c1">// This pretrained model requires those specific transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerModel</span> <span class="k">=</span> <span class="nv">NerDLModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"ner_conll_longformer_large_4096"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerModel</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|[</span><span class="kt">B-ORG</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span><span class="o">]|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLApproach
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">LongformerEmbeddings</span> \
      <span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"longformer_base_4096"</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">'document'</span><span class="p">,</span> <span class="s">'token'</span><span class="p">])</span> \
      <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">setMaxSentenceLength</span><span class="p">(</span><span class="mi">4096</span><span class="p">)</span>

<span class="c1"># Then the training can start with the transformer embeddings
</span><span class="n">nerTagger</span> <span class="o">=</span> <span class="n">NerDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setVerbose</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">nerTagger</span>
<span class="p">])</span>

<span class="c1"># We use the text and labels from the CoNLL dataset
</span><span class="n">conll</span> <span class="o">=</span> <span class="n">CoNLL</span><span class="p">()</span>
<span class="n">trainingData</span> <span class="o">=</span> <span class="n">conll</span><span class="p">.</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s">"eng.train"</span><span class="p">)</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.LongformerEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.CoNLL</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLApproach</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">LongformerEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="c1">// Then the training can start with the transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerTagger</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NerDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setRandomSeed</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setVerbose</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerTagger</span>
<span class="o">))</span>

<span class="c1">// We use the text and labels from the CoNLL dataset</span>
<span class="k">val</span> <span class="nv">conll</span> <span class="k">=</span> <span class="nc">CoNLL</span><span class="o">()</span>
<span class="k">val</span> <span class="nv">trainingData</span> <span class="k">=</span> <span class="nv">conll</span><span class="o">.</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">LongformerEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">embeddingsFinisher</span> <span class="o">=</span> <span class="n">EmbeddingsFinisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"finished_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputAsVector</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCleanAnnotations</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">([</span>
      <span class="n">documentAssembler</span><span class="p">,</span>
      <span class="n">tokenizer</span><span class="p">,</span>
      <span class="n">embeddings</span><span class="p">,</span>
      <span class="n">embeddingsFinisher</span>
    <span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"This is a sentence."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.18792399764060974</span><span class="p">,</span><span class="o">-</span><span class="mf">0.14591649174690247</span><span class="p">,</span><span class="mf">0.20547787845134735</span><span class="p">,</span><span class="mf">0.1468472778797</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.22845706343650818</span><span class="p">,</span><span class="mf">0.18073144555091858</span><span class="p">,</span><span class="mf">0.09725798666477203</span><span class="p">,</span><span class="o">-</span><span class="mf">0.0417917296290</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.07037967443466187</span><span class="p">,</span><span class="o">-</span><span class="mf">0.14801117777824402</span><span class="p">,</span><span class="o">-</span><span class="mf">0.03603338822722435</span><span class="p">,</span><span class="o">-</span><span class="mf">0.17893412709</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.08734266459941864</span><span class="p">,</span><span class="mf">0.2486150562763214</span><span class="p">,</span><span class="o">-</span><span class="mf">0.009067727252840996</span><span class="p">,</span><span class="o">-</span><span class="mf">0.24408400058</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.22409197688102722</span><span class="p">,</span><span class="o">-</span><span class="mf">0.4312366545200348</span><span class="p">,</span><span class="mf">0.1401449590921402</span><span class="p">,</span><span class="mf">0.356410235166549</span><span class="p">...</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">LongformerEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddingsFinisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">EmbeddingsFinisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"finished_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputAsVector</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCleanAnnotations</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
    <span class="n">documentAssembler</span><span class="o">,</span>
    <span class="n">tokenizer</span><span class="o">,</span>
    <span class="n">embeddings</span><span class="o">,</span>
    <span class="n">embeddingsFinisher</span>
  <span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"This is a sentence."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">80</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="err">0</span><span class="kt">.</span><span class="err">18792399764060974</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">14591649174690247</span>,<span class="err">0</span><span class="kt">.</span><span class="err">20547787845134735</span>,<span class="err">0</span><span class="kt">.</span><span class="err">1468472778797</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">22845706343650818</span>,<span class="err">0</span><span class="kt">.</span><span class="err">18073144555091858</span>,<span class="err">0</span><span class="kt">.</span><span class="err">09725798666477203</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">0417917296290</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">07037967443466187</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">14801117777824402</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">03603338822722435</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">17893412709</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">08734266459941864</span>,<span class="err">0</span><span class="kt">.</span><span class="err">2486150562763214</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">009067727252840996</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">24408400058</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">22409197688102722</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">4312366545200348</span>,<span class="err">0</span><span class="kt">.</span><span class="err">1401449590921402</span>,<span class="err">0</span><span class="kt">.</span><span class="err">356410235166549</span><span class="kt">...|</span>
<span class="kt">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="longformerfortokenclassification">LongformerForTokenClassification</h2>

  <p>LongformerForTokenClassification can load Longformer Models with a token classification head on top (a linear layer on top of the hidden-states output)
e.g. for Named-Entity-Recognition (NER) tasks.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val tokenClassifier = LongformerForTokenClassification.pretrained()
  .setInputCols("token", "document")
  .setOutputCol("label")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"longformer_base_token_classifier_conll03"</code>, if no name is provided.</p>

  <p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition">Models Hub</a>.</p>

  <p>and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/LongformerForTokenClassificationTestSpec.scala">LongformerForTokenClassificationTestSpec</a>.
Models from the HuggingFace  Transformers library are also compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.LongformerForTokenClassification.html">LongformerForTokenClassification</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/classifier/dl/LongformerForTokenClassification">LongformerForTokenClassification</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/LongformerForTokenClassification.scala">LongformerForTokenClassification</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">LongformerForTokenClassification</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenClassifier</span> <span class="k">=</span> <span class="nv">LongformerForTokenClassification</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"label.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">B-PER</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span><span class="o">]|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator needs to be trained externally. Please see the training page
# for instructions.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator needs to be trained externally. Please see the training page</span>
<span class="c1">// for instructions.</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator has a fully connected layer attached for classification. For
# embeddings see the base transformer annotator.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator has a fully connected layer attached for classification. For</span>
<span class="c1">// embeddings see the base transformer annotator.</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="mariantransformer">MarianTransformer</h2>

  <p>MarianTransformer: Fast Neural Machine Translation</p>

  <p>Marian is an efficient, free Neural Machine Translation framework written in pure C++ with minimal dependencies.
It is mainly being developed by the Microsoft Translator team. Many academic (most notably the University of
Edinburgh and in the past the Adam Mickiewicz University in Pozna) and commercial contributors help with its
development. MarianTransformer uses the models trained by MarianNMT.</p>

  <p>It is currently the engine behind the Microsoft Translator Neural Machine Translation services and being deployed by
many companies, organizations and research projects.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val marian = MarianTransformer.pretrained()
  .setInputCols("sentence")
  .setOutputCol("translation")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"opus_mt_en_fr"</code>, default language is <code class="language-plaintext highlighter-rouge">"xx"</code> (meaning multi-lingual), if no values are provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Translation">Models Hub</a>.</p>

  <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/TRANSLATION_MARIAN.ipynb">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/seq2seq/MarianTransformerTestSpec.scala">MarianTransformerTestSpec</a>.</p>

  <p><strong>Sources</strong> :</p>

  <p><a href="https://marian-nmt.github.io/">MarianNMT at GitHub</a></p>

  <p><a href="https://www.aclweb.org/anthology/P18-4020/">Marian: Fast Neural Machine Translation in C++ </a></p>

  <p><strong>Paper Abstract:</strong></p>

  <p><em>We present Marian, an efficient and self-contained Neural Machine Translation framework with an integrated
automatic differentiation engine based on dynamic computation graphs. Marian is written entirely in C++. We describe
the design of the encoder-decoder framework and demonstrate that a research-friendly toolkit can achieve high
training and translation speed.</em></p>

  <p><strong>Note:</strong></p>

  <p>This is a very computationally expensive module especially on larger sequence.
The use of an accelerator such as GPU is recommended.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.MarianTransformer.html">MarianTransformer</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/seq2seq/MarianTransformer">MarianTransformer</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/seq2seq/MarianTransformer.scala">MarianTransformer</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

      <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetectorDLModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"sentence_detector_dl"</span><span class="p">,</span> <span class="s">"xx"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">marian</span> <span class="o">=</span> <span class="n">MarianTransformer</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"translation"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxInputLength</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">([</span>
      <span class="n">documentAssembler</span><span class="p">,</span>
      <span class="n">sentence</span><span class="p">,</span>
      <span class="n">marian</span>
    <span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"What is the capital of France? We should know this in french."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(translation.result) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+-------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                               <span class="o">|</span>
<span class="o">+-------------------------------------+</span>
<span class="o">|</span><span class="n">Quelle</span> <span class="n">est</span> <span class="n">la</span> <span class="n">capitale</span> <span class="n">de</span> <span class="n">la</span> <span class="n">France</span> <span class="err">?</span><span class="o">|</span>
<span class="o">|</span><span class="n">On</span> <span class="n">devrait</span> <span class="n">le</span> <span class="n">savoir</span> <span class="n">en</span> <span class="n">franais</span><span class="p">.</span>    <span class="o">|</span>
<span class="o">+-------------------------------------+</span>
</code></pre></div>      </div>

      <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator.SentenceDetectorDLModel</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.seq2seq.MarianTransformer</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="nv">SentenceDetectorDLModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"sentence_detector_dl"</span><span class="o">,</span> <span class="s">"xx"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">marian</span> <span class="k">=</span> <span class="nv">MarianTransformer</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"translation"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxInputLength</span><span class="o">(</span><span class="mi">30</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
    <span class="n">documentAssembler</span><span class="o">,</span>
    <span class="n">sentence</span><span class="o">,</span>
    <span class="n">marian</span>
  <span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"What is the capital of France? We should know this in french."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(translation.result) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+-------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                               <span class="o">|</span>
<span class="o">+-------------------------------------+</span>
<span class="o">|</span><span class="nc">Quelle</span> <span class="n">est</span> <span class="n">la</span> <span class="n">capitale</span> <span class="n">de</span> <span class="n">la</span> <span class="nc">France</span> <span class="o">?|</span>
<span class="o">|</span><span class="nc">On</span> <span class="n">devrait</span> <span class="n">le</span> <span class="n">savoir</span> <span class="n">en</span> <span class="n">franais</span><span class="o">.</span>    <span class="o">|</span>
<span class="o">+-------------------------------------+</span>
</code></pre></div>      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="robertaembeddings">RoBertaEmbeddings</h2>

  <p>The RoBERTa model was proposed in <a href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.
It is based on Googles BERT model released in 2018.</p>

  <p>It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val embeddings = RoBertaEmbeddings.pretrained()
  .setInputCols("document", "token")
  .setOutputCol("embeddings")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"roberta_base"</code>, if no name is provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings">Models Hub</a>.</p>

  <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20RoBERTa.ipynb">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/RoBertaEmbeddingsTestSpec.scala">RoBertaEmbeddingsTestSpec</a>.
Models from the HuggingFace  Transformers library are also compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p><strong>Paper Abstract:</strong></p>

  <p><em>Language model pretraining has led to significant performance gains but careful comparison between different
approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication
study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every
model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results
highlight the importance of previously overlooked design choices, and raise questions about the source of recently
reported improvements. We release our models and code.</em></p>

  <p>Tips:</p>
  <ul>
    <li>RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme.</li>
    <li>RoBERTa doesnt have :obj:<code class="language-plaintext highlighter-rouge">token_type_ids</code>, you dont need to indicate which token belongs to which segment. Just separate your segments with the separation token :obj:<code class="language-plaintext highlighter-rouge">tokenizer.sep_token</code> (or :obj:<code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code>)</li>
  </ul>

  <p>The original code can be found <code class="language-plaintext highlighter-rouge">here</code> https://github.com/pytorch/fairseq/tree/master/examples/roberta.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">WORD_EMBEDDINGS</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.RoBertaEmbeddings.html">RoBertaEmbeddings</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/RoBertaEmbeddings">RoBertaEmbeddings</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/RoBertaEmbeddings.scala">RoBertaEmbeddings</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLModel
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">RoBertaEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'roberta_base'</span><span class="p">,</span> <span class="s">'en'</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
      <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="c1"># This pretrained model requires those specific transformer embeddings
</span><span class="n">ner_model</span> <span class="o">=</span> <span class="n">NerDLModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'ner_conll_roberta_base'</span><span class="p">,</span> <span class="s">'en'</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">'document'</span><span class="p">,</span> <span class="s">'token'</span><span class="p">,</span> <span class="s">'embeddings'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">'ner'</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">ner_model</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"ner.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">ORG</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLModel</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="c1">// Use the transformer embeddings</span>
<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">RoBertaEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"roberta_base"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// This pretrained model requires those specific transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerModel</span> <span class="k">=</span> <span class="nv">NerDLModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"ner_conll_roberta_base"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerModel</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|[</span><span class="kt">B-ORG</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span><span class="o">]|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLApproach
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">RoBertaEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'roberta_base'</span><span class="p">,</span> <span class="s">'en'</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"sentence"</span><span class="p">])</span> \
      <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="c1"># Then the training can start with the transformer embeddings
</span><span class="n">nerTagger</span> <span class="o">=</span> <span class="n">NerDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setVerbose</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">nerTagger</span>
<span class="p">])</span>

<span class="c1"># We use the text and labels from the CoNLL dataset
</span><span class="n">conll</span> <span class="o">=</span> <span class="n">CoNLL</span><span class="p">()</span>
<span class="n">trainingData</span> <span class="o">=</span> <span class="n">conll</span><span class="p">.</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s">"eng.train"</span><span class="p">)</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.CoNLL</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLApproach</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">RoBertaEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// Then the training can start with the transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerTagger</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NerDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setRandomSeed</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setVerbose</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerTagger</span>
<span class="o">))</span>

<span class="c1">// We use the text and labels from the CoNLL dataset</span>
<span class="k">val</span> <span class="nv">conll</span> <span class="k">=</span> <span class="nc">CoNLL</span><span class="o">()</span>
<span class="k">val</span> <span class="nv">trainingData</span> <span class="k">=</span> <span class="nv">conll</span><span class="o">.</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">RoBertaEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">embeddingsFinisher</span> <span class="o">=</span> <span class="n">EmbeddingsFinisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"finished_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputAsVector</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCleanAnnotations</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">([</span>
      <span class="n">documentAssembler</span><span class="p">,</span>
      <span class="n">tokenizer</span><span class="p">,</span>
      <span class="n">embeddings</span><span class="p">,</span>
      <span class="n">embeddingsFinisher</span>
    <span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"This is a sentence."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.18792399764060974</span><span class="p">,</span><span class="o">-</span><span class="mf">0.14591649174690247</span><span class="p">,</span><span class="mf">0.20547787845134735</span><span class="p">,</span><span class="mf">0.1468472778797</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.22845706343650818</span><span class="p">,</span><span class="mf">0.18073144555091858</span><span class="p">,</span><span class="mf">0.09725798666477203</span><span class="p">,</span><span class="o">-</span><span class="mf">0.0417917296290</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.07037967443466187</span><span class="p">,</span><span class="o">-</span><span class="mf">0.14801117777824402</span><span class="p">,</span><span class="o">-</span><span class="mf">0.03603338822722435</span><span class="p">,</span><span class="o">-</span><span class="mf">0.17893412709</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.08734266459941864</span><span class="p">,</span><span class="mf">0.2486150562763214</span><span class="p">,</span><span class="o">-</span><span class="mf">0.009067727252840996</span><span class="p">,</span><span class="o">-</span><span class="mf">0.24408400058</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.22409197688102722</span><span class="p">,</span><span class="o">-</span><span class="mf">0.4312366545200348</span><span class="p">,</span><span class="mf">0.1401449590921402</span><span class="p">,</span><span class="mf">0.356410235166549</span><span class="p">...</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.RoBertaEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.EmbeddingsFinisher</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">RoBertaEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddingsFinisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">EmbeddingsFinisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"finished_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputAsVector</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCleanAnnotations</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
    <span class="n">documentAssembler</span><span class="o">,</span>
    <span class="n">tokenizer</span><span class="o">,</span>
    <span class="n">embeddings</span><span class="o">,</span>
    <span class="n">embeddingsFinisher</span>
  <span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"This is a sentence."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">80</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="err">0</span><span class="kt">.</span><span class="err">18792399764060974</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">14591649174690247</span>,<span class="err">0</span><span class="kt">.</span><span class="err">20547787845134735</span>,<span class="err">0</span><span class="kt">.</span><span class="err">1468472778797</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">22845706343650818</span>,<span class="err">0</span><span class="kt">.</span><span class="err">18073144555091858</span>,<span class="err">0</span><span class="kt">.</span><span class="err">09725798666477203</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">0417917296290</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">07037967443466187</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">14801117777824402</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">03603338822722435</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">17893412709</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">08734266459941864</span>,<span class="err">0</span><span class="kt">.</span><span class="err">2486150562763214</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">009067727252840996</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">24408400058</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">22409197688102722</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">4312366545200348</span>,<span class="err">0</span><span class="kt">.</span><span class="err">1401449590921402</span>,<span class="err">0</span><span class="kt">.</span><span class="err">356410235166549</span><span class="kt">...|</span>
<span class="kt">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="robertafortokenclassification">RoBertaForTokenClassification</h2>

  <p>RoBertaForTokenClassification can load RoBERTa Models with a token classification head on top (a linear layer on top of the hidden-states output)
e.g. for Named-Entity-Recognition (NER) tasks.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val tokenClassifier = RoBertaForTokenClassification.pretrained()
  .setInputCols("token", "document")
  .setOutputCol("label")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"roberta_base_token_classifier_conll03"</code>, if no name is provided.</p>

  <p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition">Models Hub</a>.</p>

  <p>and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/RoBertaForTokenClassificationTestSpec.scala">RoBertaForTokenClassificationTestSpec</a>.
Models from the HuggingFace  Transformers library are also compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.RoBertaForTokenClassification.html">RoBertaForTokenClassification</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/classifier/dl/RoBertaForTokenClassification">RoBertaForTokenClassification</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/RoBertaForTokenClassification.scala">RoBertaForTokenClassification</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">RoBertaForTokenClassification</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenClassifier</span> <span class="k">=</span> <span class="nv">RoBertaForTokenClassification</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"label.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">B-PER</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span><span class="o">]|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator needs to be trained externally. Please see the training page
# for instructions.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator needs to be trained externally. Please see the training page</span>
<span class="c1">// for instructions.</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator has a fully connected layer attached for classification. For
# embeddings see the base transformer annotator.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator has a fully connected layer attached for classification. For</span>
<span class="c1">// embeddings see the base transformer annotator.</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="robertasentenceembeddings">RoBertaSentenceEmbeddings</h2>

  <p>Sentence-level embeddings using RoBERTa. The RoBERTa model was proposed in <a href="https://arxiv.org/abs/1907.11692">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a>
by Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov.
It is based on Googles BERT model released in 2018.</p>

  <p>It builds on BERT and modifies key hyperparameters, removing the next-sentence pretraining objective and training with much larger mini-batches and learning rates.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val embeddings = RoBertaSentenceEmbeddings.pretrained()
  .setInputCols("sentence")
  .setOutputCol("sentence_embeddings")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"sent_roberta_base"</code>, if no name is provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings">Models Hub</a>.</p>

  <p>Models from the HuggingFace  Transformers library are also compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p><strong>Paper Abstract:</strong></p>

  <p><em>Language model pretraining has led to significant performance gains but careful comparison between different
approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication
study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every
model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results
highlight the importance of previously overlooked design choices, and raise questions about the source of recently
reported improvements. We release our models and code.</em></p>

  <p>Tips:</p>
  <ul>
    <li>RoBERTa has the same architecture as BERT, but uses a byte-level BPE as a tokenizer (same as GPT-2) and uses a different pretraining scheme.</li>
    <li>RoBERTa doesnt have :obj:<code class="language-plaintext highlighter-rouge">token_type_ids</code>, you dont need to indicate which token belongs to which segment. Just separate your segments with the separation token :obj:<code class="language-plaintext highlighter-rouge">tokenizer.sep_token</code> (or :obj:<code class="language-plaintext highlighter-rouge">&lt;/s&gt;</code>)</li>
  </ul>

  <p>The original code can be found <code class="language-plaintext highlighter-rouge">here</code> https://github.com/pytorch/fairseq/tree/master/examples/roberta.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">SENTENCE_EMBEDDINGS</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.RoBertaSentenceEmbeddings.html">RoBertaSentenceEmbeddings</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/RoBertaSentenceEmbeddings">RoBertaSentenceEmbeddings</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/RoBertaSentenceEmbeddings.scala">RoBertaSentenceEmbeddings</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Coming Soon!
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Coming Soon!</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">smallCorpus</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span><span class="s">"True"</span><span class="p">).</span><span class="n">csv</span><span class="p">(</span><span class="s">"sentiment.csv"</span><span class="p">)</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">RoBertaSentenceEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span>\
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span>

<span class="c1"># Then the training can start with the transformer embeddings
</span><span class="n">docClassifier</span> <span class="o">=</span> <span class="n">ClassifierDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"category"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setBatchSize</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLr</span><span class="p">(</span><span class="mf">5e-3</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setDropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">docClassifier</span>
<span class="p">])</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">smallCorpus</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">smallCorpus</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span> <span class="s">"true"</span><span class="o">).</span><span class="py">csv</span><span class="o">(</span><span class="s">"src/test/resources/classifier/sentiment.csv"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">RoBertaSentenceEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>

<span class="c1">// Then the training can start with the transformer embeddings</span>
<span class="k">val</span> <span class="nv">docClassifier</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ClassifierDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"category"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setBatchSize</span><span class="o">(</span><span class="mi">64</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">20</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLr</span><span class="o">(</span><span class="mi">5</span><span class="n">e</span><span class="o">-</span><span class="mf">3f</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setDropout</span><span class="o">(</span><span class="mf">0.5f</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">docClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">smallCorpus</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">sentenceEmbeddings</span> <span class="o">=</span> <span class="n">RoBertaSentenceEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># you can either use the output to train ClassifierDL, SentimentDL, or MultiClassifierDL
# or you can use EmbeddingsFinisher to prepare the results for Spark ML functions
</span>
<span class="n">embeddingsFinisher</span> <span class="o">=</span> <span class="n">EmbeddingsFinisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence_embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"finished_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputAsVector</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCleanAnnotations</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">([</span>
      <span class="n">documentAssembler</span><span class="p">,</span>
      <span class="n">tokenizer</span><span class="p">,</span>
      <span class="n">sentenceEmbeddings</span><span class="p">,</span>
      <span class="n">embeddingsFinisher</span>
    <span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"This is a sentence."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.18792399764060974</span><span class="p">,</span><span class="o">-</span><span class="mf">0.14591649174690247</span><span class="p">,</span><span class="mf">0.20547787845134735</span><span class="p">,</span><span class="mf">0.1468472778797</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.22845706343650818</span><span class="p">,</span><span class="mf">0.18073144555091858</span><span class="p">,</span><span class="mf">0.09725798666477203</span><span class="p">,</span><span class="o">-</span><span class="mf">0.0417917296290</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.07037967443466187</span><span class="p">,</span><span class="o">-</span><span class="mf">0.14801117777824402</span><span class="p">,</span><span class="o">-</span><span class="mf">0.03603338822722435</span><span class="p">,</span><span class="o">-</span><span class="mf">0.17893412709</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.08734266459941864</span><span class="p">,</span><span class="mf">0.2486150562763214</span><span class="p">,</span><span class="o">-</span><span class="mf">0.009067727252840996</span><span class="p">,</span><span class="o">-</span><span class="mf">0.24408400058</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.22409197688102722</span><span class="p">,</span><span class="o">-</span><span class="mf">0.4312366545200348</span><span class="p">,</span><span class="mf">0.1401449590921402</span><span class="p">,</span><span class="mf">0.356410235166549</span><span class="p">...</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.EmbeddingsFinisher</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentenceEmbeddings</span> <span class="k">=</span> <span class="nv">RoBertaSentenceEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="c1">// you can either use the output to train ClassifierDL, SentimentDL, or MultiClassifierDL</span>
<span class="c1">// or you can use EmbeddingsFinisher to prepare the results for Spark ML functions</span>

<span class="k">val</span> <span class="nv">embeddingsFinisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">EmbeddingsFinisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"finished_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputAsVector</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCleanAnnotations</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
    <span class="n">documentAssembler</span><span class="o">,</span>
    <span class="n">tokenizer</span><span class="o">,</span>
    <span class="n">sentenceEmbeddings</span><span class="o">,</span>
    <span class="n">embeddingsFinisher</span>
  <span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"This is a sentence."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">80</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="err">0</span><span class="kt">.</span><span class="err">18792399764060974</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">14591649174690247</span>,<span class="err">0</span><span class="kt">.</span><span class="err">20547787845134735</span>,<span class="err">0</span><span class="kt">.</span><span class="err">1468472778797</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">22845706343650818</span>,<span class="err">0</span><span class="kt">.</span><span class="err">18073144555091858</span>,<span class="err">0</span><span class="kt">.</span><span class="err">09725798666477203</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">0417917296290</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">07037967443466187</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">14801117777824402</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">03603338822722435</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">17893412709</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">08734266459941864</span>,<span class="err">0</span><span class="kt">.</span><span class="err">2486150562763214</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">009067727252840996</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">24408400058</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">22409197688102722</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">4312366545200348</span>,<span class="err">0</span><span class="kt">.</span><span class="err">1401449590921402</span>,<span class="err">0</span><span class="kt">.</span><span class="err">356410235166549</span><span class="kt">...|</span>
<span class="kt">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="t5transformer">T5Transformer</h2>

  <p>T5: the Text-To-Text Transfer Transformer</p>

  <p>T5 reconsiders all NLP tasks into a unified text-to-text-format where the input and output are always
text strings, in contrast to BERT-style models that can only output either a class label or a span of the input.
The text-to-text framework is able to use the same model, loss function, and hyper-parameters on any NLP task,
including machine translation, document summarization, question answering, and classification tasks
(e.g., sentiment analysis). T5 can even apply to regression tasks by training it to predict the string
representation of a number instead of the number itself.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val t5 = T5Transformer.pretrained()
  .setTask("summarize:")
  .setInputCols("document")
  .setOutputCol("summaries")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"t5_small"</code>, if no name is provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?q=t5">Models Hub</a>.</p>

  <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/10.Question_Answering_and_Summarization_with_T5.ipynb">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/seq2seq/T5TestSpec.scala">T5TestSpec</a>.</p>

  <p><strong>Sources:</strong></p>
  <ul>
    <li><a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">Exploring Transfer Learning with T5: the Text-To-Text Transfer Transformer</a></li>
    <li><a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a></li>
    <li>https://github.com/google-research/text-to-text-transfer-transformer</li>
  </ul>

  <p><strong>Paper Abstract:</strong></p>

  <p><em>Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream
task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer
learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the
landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based
language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures,
unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining
the insights from our exploration with scale and our new Colossal Clean Crawled Corpus, we achieve state-of-the-art
results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate
future work on transfer learning for NLP, we release our data set, pre-trained models, and code.</em></p>

  <p><strong>Note:</strong></p>

  <p>This is a very computationally expensive module especially on larger sequence.
The use of an accelerator such as GPU is recommended.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.T5Transformer.html">T5Transformer</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/seq2seq/T5Transformer">T5Transformer</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/seq2seq/T5Transformer.scala">T5Transformer</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Example</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

      <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"documents"</span><span class="p">)</span>

<span class="n">t5</span> <span class="o">=</span> <span class="n">T5Transformer</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"t5_small"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setTask</span><span class="p">(</span><span class="s">"summarize:"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"documents"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setMaxOutputLength</span><span class="p">(</span><span class="mi">200</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"summaries"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span><span class="n">documentAssembler</span><span class="p">,</span> <span class="n">t5</span><span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span>
    <span class="s">"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a "</span> <span class="o">+</span>
      <span class="s">"downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness"</span> <span class="o">+</span>
      <span class="s">" of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this "</span> <span class="o">+</span>
      <span class="s">"paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework "</span> <span class="o">+</span>
      <span class="s">"that converts all text-based language problems into a text-to-text format. Our systematic study compares "</span> <span class="o">+</span>
      <span class="s">"pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens "</span> <span class="o">+</span>
      <span class="s">"of language understanding tasks. By combining the insights from our exploration with scale and our new "</span> <span class="o">+</span>
      <span class="s">"Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering "</span> <span class="o">+</span>
      <span class="s">"summarization, question answering, text classification, and more. To facilitate future work on transfer "</span> <span class="o">+</span>
      <span class="s">"learning for NLP, we release our data set, pre-trained models, and code."</span>
<span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"summaries.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                                                                                                                                                        <span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">transfer</span> <span class="n">learning</span> <span class="n">has</span> <span class="n">emerged</span> <span class="k">as</span> <span class="n">a</span> <span class="n">powerful</span> <span class="n">technique</span> <span class="ow">in</span> <span class="n">natural</span> <span class="n">language</span> <span class="n">processing</span> <span class="p">(</span><span class="n">NLP</span><span class="p">)</span> <span class="n">the</span> <span class="n">effectiveness</span> <span class="n">of</span> <span class="n">transfer</span> <span class="n">learning</span> <span class="n">has</span> <span class="n">given</span> <span class="n">rise</span> <span class="n">to</span> <span class="n">a</span> <span class="n">diversity</span> <span class="n">of</span> <span class="n">approaches</span><span class="p">,</span> <span class="n">methodologies</span><span class="p">,</span> <span class="ow">and</span> <span class="n">practice</span> <span class="p">.]</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
</code></pre></div>      </div>

      <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.seq2seq.T5Transformer</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"documents"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">t5</span> <span class="k">=</span> <span class="nv">T5Transformer</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"t5_small"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setTask</span><span class="o">(</span><span class="s">"summarize:"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"documents"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setMaxOutputLength</span><span class="o">(</span><span class="mi">200</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"summaries"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="n">documentAssembler</span><span class="o">,</span> <span class="n">t5</span><span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span>
  <span class="s">"Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a "</span> <span class="o">+</span>
    <span class="s">"downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness"</span> <span class="o">+</span>
    <span class="s">" of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this "</span> <span class="o">+</span>
    <span class="s">"paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework "</span> <span class="o">+</span>
    <span class="s">"that converts all text-based language problems into a text-to-text format. Our systematic study compares "</span> <span class="o">+</span>
    <span class="s">"pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens "</span> <span class="o">+</span>
    <span class="s">"of language understanding tasks. By combining the insights from our exploration with scale and our new "</span> <span class="o">+</span>
    <span class="s">"Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering "</span> <span class="o">+</span>
    <span class="s">"summarization, question answering, text classification, and more. To facilitate future work on transfer "</span> <span class="o">+</span>
    <span class="s">"learning for NLP, we release our data set, pre-trained models, and code."</span>
<span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"summaries.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                                                                                                                                                        <span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">transfer</span> <span class="kt">learning</span> <span class="kt">has</span> <span class="kt">emerged</span> <span class="kt">as</span> <span class="kt">a</span> <span class="kt">powerful</span> <span class="kt">technique</span> <span class="kt">in</span> <span class="kt">natural</span> <span class="kt">language</span> <span class="kt">processing</span> <span class="o">(</span><span class="kt">NLP</span><span class="o">)</span> <span class="kt">the</span> <span class="kt">effectiveness</span> <span class="kt">of</span> <span class="kt">transfer</span> <span class="kt">learning</span> <span class="kt">has</span> <span class="kt">given</span> <span class="kt">rise</span> <span class="kt">to</span> <span class="kt">a</span> <span class="kt">diversity</span> <span class="kt">of</span> <span class="kt">approaches</span>, <span class="kt">methodologies</span>, <span class="kt">and</span> <span class="kt">practice</span> <span class="kt">.</span><span class="o">]|</span>
<span class="o">+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+</span>
</code></pre></div>      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="universalsentenceencoder">UniversalSentenceEncoder</h2>

  <p>The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val useEmbeddings = UniversalSentenceEncoder.pretrained()
  .setInputCols("sentence")
  .setOutputCol("sentence_embeddings")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"tfhub_use"</code>, if no name is provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings">Models Hub</a>.</p>

  <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/UniversalSentenceEncoderTestSpec.scala">UniversalSentenceEncoderTestSpec</a>.</p>

  <p><strong>Sources:</strong></p>

  <p><a href="https://arxiv.org/abs/1803.11175">Universal Sentence Encoder</a></p>

  <p>https://tfhub.dev/google/universal-sentence-encoder/2</p>

  <p><strong>Paper abstract:</strong></p>

  <p><em>We present models for encoding sentences into embedding vectors that specifically target transfer learning to other
NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the
encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and
report the relationship between model complexity, resource consumption, the availability of transfer task training
data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained
word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence
embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe
surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain
encouraging results on Word Embedding Association Tests (WEAT) targeted at detecting model bias. Our pre-trained
sentence encoding models are made freely available for download and on TF Hub.</em></p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">SENTENCE_EMBEDDINGS</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.UniversalSentenceEncoder.html">UniversalSentenceEncoder</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/UniversalSentenceEncoder">UniversalSentenceEncoder</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/UniversalSentenceEncoder.scala">UniversalSentenceEncoder</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">UniversalSentenceEncoder</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'tfhub_use'</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s">"en"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span>

<span class="c1"># This pretrained model requires those specific transformer embeddings
</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">SentimentDLModel</span><span class="p">().</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'sentimentdl_use_imdb'</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence_embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentiment"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">classifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"That was a fantastic movie!"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"sentiment.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------+</span>
<span class="o">|</span><span class="n">result</span><span class="o">|</span>
<span class="o">+------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">|</span>
<span class="o">+------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.classifier.dl.SentimentDLModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">UniversalSentenceEncoder</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"tfhub_use"</span><span class="o">,</span> <span class="n">lang</span> <span class="k">=</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>

<span class="c1">// This pretrained model requires those specific transformer embeddings</span>
<span class="k">val</span> <span class="nv">classifier</span> <span class="k">=</span> <span class="nv">SentimentDLModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"sentimentdl_use_imdb"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentiment"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">classifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"That was a fantastic movie!"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"sentiment.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------+</span>
<span class="o">|</span><span class="n">result</span><span class="o">|</span>
<span class="o">+------+</span>
<span class="o">|[</span><span class="kt">pos</span><span class="o">]</span> <span class="o">|</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">UniversalSentenceEncoder</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span>

<span class="c1"># Then the training can start with the transformer embeddings
</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">SentimentDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence_embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentiment"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setBatchSize</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLr</span><span class="p">(</span><span class="mf">5e-3</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setDropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">classifier</span>
<span class="p">])</span>

<span class="n">smallCorpus</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span> <span class="s">"True"</span><span class="p">).</span><span class="n">csv</span><span class="p">(</span><span class="s">"sentiment.csv"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">smallCorpus</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator.UniversalSentenceEncoder</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.classifier.dl.</span><span class="o">{</span><span class="nc">SentimentDLApproach</span><span class="o">,</span> <span class="nc">SentimentDLModel</span><span class="o">}</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="c1">// Use the transformer embeddings</span>
<span class="k">val</span> <span class="nv">useEmbeddings</span> <span class="k">=</span> <span class="nv">UniversalSentenceEncoder</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>

<span class="c1">// Then the training can start with the transformer embeddings</span>
<span class="k">val</span> <span class="nv">docClassifier</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentimentDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentiment"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setBatchSize</span><span class="o">(</span><span class="mi">32</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLr</span><span class="o">(</span><span class="mi">5</span><span class="n">e</span><span class="o">-</span><span class="mf">3f</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setDropout</span><span class="o">(</span><span class="mf">0.5f</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">useEmbeddings</span><span class="o">,</span>
  <span class="n">docClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">smallCorpus</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span> <span class="s">"true"</span><span class="o">).</span><span class="py">csv</span><span class="o">(</span><span class="s">"sentiment.csv"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">smallCorpus</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">UniversalSentenceEncoder</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span>

<span class="n">embeddingsFinisher</span> <span class="o">=</span> <span class="n">EmbeddingsFinisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence_embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"finished_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputAsVector</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCleanAnnotations</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">([</span>
      <span class="n">documentAssembler</span><span class="p">,</span>
      <span class="n">sentence</span><span class="p">,</span>
      <span class="n">embeddings</span><span class="p">,</span>
      <span class="n">embeddingsFinisher</span>
    <span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"This is a sentence."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.04616805538535118</span><span class="p">,</span><span class="mf">0.022307956591248512</span><span class="p">,</span><span class="o">-</span><span class="mf">0.044395286589860916</span><span class="p">,</span><span class="o">-</span><span class="mf">0.0016493503</span><span class="p">...</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.EmbeddingsFinisher</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">UniversalSentenceEncoder</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddingsFinisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">EmbeddingsFinisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"finished_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputAsVector</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCleanAnnotations</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
    <span class="n">documentAssembler</span><span class="o">,</span>
    <span class="n">sentence</span><span class="o">,</span>
    <span class="n">embeddings</span><span class="o">,</span>
    <span class="n">embeddingsFinisher</span>
  <span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"This is a sentence."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">80</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="err">0</span><span class="kt">.</span><span class="err">04616805538535118</span>,<span class="err">0</span><span class="kt">.</span><span class="err">022307956591248512</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">044395286589860916</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">0016493503</span><span class="kt">...|</span>
<span class="kt">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="xlmrobertaembeddings">XlmRoBertaEmbeddings</h2>

  <p>The XLM-RoBERTa model was proposed in <a href="https://arxiv.org/abs/1911.02116">Unsupervised Cross-lingual Representation Learning at Scale</a>
by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco Guzmn, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebooks
RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl
data.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val embeddings = XlmRoBertaEmbeddings.pretrained()
  .setInputCols("document", "token")
  .setOutputCol("embeddings")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"xlm_roberta_base"</code>, default language is <code class="language-plaintext highlighter-rouge">"xx"</code> (meaning multi-lingual), if no values are provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings">Models Hub</a>.</p>

  <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20XLM-RoBERTa.ipynb">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/XlmRoBertaEmbeddingsTestSpec.scala">XlmRoBertaEmbeddingsTestSpec</a>.
Models from the HuggingFace  Transformers library are also compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p><strong>Paper Abstract:</strong></p>

  <p><em>This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing
per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We
will make XLM-R code, data, and models publicly available.</em></p>

  <p><strong>Tips:</strong></p>
  <ul>
    <li>XLM-RoBERTa is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does
not require <strong>lang</strong> parameter to understand which language is used, and should be able to determine the correct
language from the input ids.</li>
    <li>This implementation is the same as RoBERTa. Refer to the RoBertaEmbeddings for usage examples
as well as the information relative to the inputs and outputs.</li>
  </ul>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">WORD_EMBEDDINGS</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.XlmRoBertaEmbeddings.html">XlmRoBertaEmbeddings</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/XlmRoBertaEmbeddings">XlmRoBertaEmbeddings</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/XlmRoBertaEmbeddings.scala">XlmRoBertaEmbeddings</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLModel
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">XlmRoBertaEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'xlm_roberta_base'</span><span class="p">,</span> <span class="s">'xx'</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="c1"># This pretrained model requires those specific transformer embeddings
</span><span class="n">ner_model</span> <span class="o">=</span> <span class="n">NerDLModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">'ner_conll_xlm_roberta_base'</span><span class="p">,</span> <span class="s">'en'</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">'document'</span><span class="p">,</span> <span class="s">'token'</span><span class="p">,</span> <span class="s">'embeddings'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">'ner'</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">ner_model</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"ner.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">ORG</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLModel</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="c1">// Use the transformer embeddings</span>
<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">XlmRoBertaEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// This pretrained model requires those specific transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerModel</span> <span class="k">=</span> <span class="nv">NerDLModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"ner_conll_roberta_base"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerModel</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|[</span><span class="kt">B-ORG</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span><span class="o">]|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLApproach
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">XlmRoBertaEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Then the training can start with the transformer embeddings
</span><span class="n">nerTagger</span> <span class="o">=</span> <span class="n">NerDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setVerbose</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">nerTagger</span>
<span class="p">])</span>

<span class="c1"># We use the text and labels from the CoNLL dataset
</span><span class="n">conll</span> <span class="o">=</span> <span class="n">CoNLL</span><span class="p">()</span>
<span class="n">trainingData</span> <span class="o">=</span> <span class="n">conll</span><span class="p">.</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s">"eng.train"</span><span class="p">)</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.CoNLL</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLApproach</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">XlmRoBertaEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// Then the training can start with the transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerTagger</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NerDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setRandomSeed</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setVerbose</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerTagger</span>
<span class="o">))</span>

<span class="c1">// We use the text and labels from the CoNLL dataset</span>
<span class="k">val</span> <span class="nv">conll</span> <span class="k">=</span> <span class="nc">CoNLL</span><span class="o">()</span>
<span class="k">val</span> <span class="nv">trainingData</span> <span class="k">=</span> <span class="nv">conll</span><span class="o">.</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">XlmRoBertaEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">embeddingsFinisher</span> <span class="o">=</span> <span class="n">EmbeddingsFinisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"finished_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputAsVector</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCleanAnnotations</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">([</span>
      <span class="n">documentAssembler</span><span class="p">,</span>
      <span class="n">tokenizer</span><span class="p">,</span>
      <span class="n">embeddings</span><span class="p">,</span>
      <span class="n">embeddingsFinisher</span>
    <span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"This is a sentence."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.05969233065843582</span><span class="p">,</span><span class="o">-</span><span class="mf">0.030789051204919815</span><span class="p">,</span><span class="mf">0.04443822056055069</span><span class="p">,</span><span class="mf">0.09564960747</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.038839809596538544</span><span class="p">,</span><span class="mf">0.011712731793522835</span><span class="p">,</span><span class="mf">0.019954433664679527</span><span class="p">,</span><span class="mf">0.0667808502</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.03952755779027939</span><span class="p">,</span><span class="o">-</span><span class="mf">0.03455188870429993</span><span class="p">,</span><span class="mf">0.019103847444057465</span><span class="p">,</span><span class="mf">0.04311436787</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.09579929709434509</span><span class="p">,</span><span class="mf">0.02494969218969345</span><span class="p">,</span><span class="o">-</span><span class="mf">0.014753809198737144</span><span class="p">,</span><span class="mf">0.10259044915</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.004710011184215546</span><span class="p">,</span><span class="o">-</span><span class="mf">0.022148698568344116</span><span class="p">,</span><span class="mf">0.011723337695002556</span><span class="p">,</span><span class="o">-</span><span class="mf">0.013356896</span><span class="p">...</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.XlmRoBertaEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.EmbeddingsFinisher</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">XlmRoBertaEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddingsFinisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">EmbeddingsFinisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"finished_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputAsVector</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCleanAnnotations</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
    <span class="n">documentAssembler</span><span class="o">,</span>
    <span class="n">tokenizer</span><span class="o">,</span>
    <span class="n">embeddings</span><span class="o">,</span>
    <span class="n">embeddingsFinisher</span>
  <span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"This is a sentence."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">80</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">05969233065843582</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">030789051204919815</span>,<span class="err">0</span><span class="kt">.</span><span class="err">04443822056055069</span>,<span class="err">0</span><span class="kt">.</span><span class="err">09564960747</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">038839809596538544</span>,<span class="err">0</span><span class="kt">.</span><span class="err">011712731793522835</span>,<span class="err">0</span><span class="kt">.</span><span class="err">019954433664679527</span>,<span class="err">0</span><span class="kt">.</span><span class="err">0667808502</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">03952755779027939</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">03455188870429993</span>,<span class="err">0</span><span class="kt">.</span><span class="err">019103847444057465</span>,<span class="err">0</span><span class="kt">.</span><span class="err">04311436787</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">09579929709434509</span>,<span class="err">0</span><span class="kt">.</span><span class="err">02494969218969345</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">014753809198737144</span>,<span class="err">0</span><span class="kt">.</span><span class="err">10259044915</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">004710011184215546</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">022148698568344116</span>,<span class="err">0</span><span class="kt">.</span><span class="err">011723337695002556</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">013356896</span><span class="kt">...|</span>
<span class="kt">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="xlmrobertafortokenclassification">XlmRoBertaForTokenClassification</h2>

  <p>XlmRoBertaForTokenClassification can load XLM-RoBERTa Models with a token classification head on top (a linear layer on top of the hidden-states output)
e.g. for Named-Entity-Recognition (NER) tasks.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val tokenClassifier = XlmRoBertaForTokenClassification.pretrained()
  .setInputCols("token", "document")
  .setOutputCol("label")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"xlm_roberta_base_token_classifier_conll03"</code>, if no name is provided.</p>

  <p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition">Models Hub</a>.</p>

  <p>and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/XlmRoBertaForTokenClassificationTestSpec.scala">XlmRoBertaForTokenClassificationTestSpec</a>.
Models from the HuggingFace  Transformers library are also compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.XlmRoBertaForTokenClassification.html">XlmRoBertaForTokenClassification</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/classifier/dl/XlmRoBertaForTokenClassification">XlmRoBertaForTokenClassification</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/XlmRoBertaForTokenClassification.scala">XlmRoBertaForTokenClassification</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">XlmRoBertaForTokenClassification</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenClassifier</span> <span class="k">=</span> <span class="nv">XlmRoBertaForTokenClassification</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"label.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">B-PER</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span><span class="o">]|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator needs to be trained externally. Please see the training page
# for instructions.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator needs to be trained externally. Please see the training page</span>
<span class="c1">// for instructions.</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator has a fully connected layer attached for classification. For
# embeddings see the base transformer annotator.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator has a fully connected layer attached for classification. For</span>
<span class="c1">// embeddings see the base transformer annotator.</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="xlmrobertasentenceembeddings">XlmRoBertaSentenceEmbeddings</h2>

  <p>Sentence-level embeddings using XLM-RoBERTa. The XLM-RoBERTa model was proposed in <a href="https://arxiv.org/abs/1911.02116">Unsupervised Cross-lingual Representation Learning at Scale</a>
by Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco Guzmn, Edouard Grave, Myle Ott, Luke Zettlemoyer and Veselin Stoyanov. It is based on Facebooks
RoBERTa model released in 2019. It is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl
data.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val embeddings = XlmRoBertaSentenceEmbeddings.pretrained()
  .setInputCols("document")
  .setOutputCol("sentence_embeddings")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"sent_xlm_roberta_base"</code>, default language is <code class="language-plaintext highlighter-rouge">"xx"</code> (meaning multi-lingual), if no values are provided.
For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Embeddings">Models Hub</a>.</p>

  <p>Models from the HuggingFace  Transformers library are also compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p><strong>Paper Abstract:</strong></p>

  <p><em>This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing
per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We
will make XLM-R code, data, and models publicly available.</em></p>

  <p><strong>Tips:</strong></p>
  <ul>
    <li>XLM-RoBERTa is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does
not require <strong>lang</strong> parameter to understand which language is used, and should be able to determine the correct
language from the input ids.</li>
    <li>This implementation is the same as RoBERTa. Refer to the RoBertaEmbeddings for usage examples
as well as the information relative to the inputs and outputs.</li>
  </ul>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">SENTENCE_EMBEDDINGS</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.XlmRoBertaSentenceEmbeddings.html">XlmRoBertaSentenceEmbeddings</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/XlmRoBertaSentenceEmbeddings">XlmRoBertaSentenceEmbeddings</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/XlmRoBertaSentenceEmbeddings.scala">XlmRoBertaSentenceEmbeddings</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Coming Soon!
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Coming Soon!</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">smallCorpus</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span><span class="s">"True"</span><span class="p">).</span><span class="n">csv</span><span class="p">(</span><span class="s">"sentiment.csv"</span><span class="p">)</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">XlmRoBertaSentenceEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
  <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span>\
  <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span>

<span class="c1"># Then the training can start with the transformer embeddings
</span><span class="n">docClassifier</span> <span class="o">=</span> <span class="n">ClassifierDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"category"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setBatchSize</span><span class="p">(</span><span class="mi">64</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setLr</span><span class="p">(</span><span class="mf">5e-3</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setDropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">docClassifier</span>
<span class="p">])</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">smallCorpus</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.RoBertaSentenceEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">smallCorpus</span> <span class="k">=</span> <span class="nv">spark</span><span class="o">.</span><span class="py">read</span><span class="o">.</span><span class="py">option</span><span class="o">(</span><span class="s">"header"</span><span class="o">,</span> <span class="s">"true"</span><span class="o">).</span><span class="py">csv</span><span class="o">(</span><span class="s">"src/test/resources/classifier/sentiment.csv"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">XlmRoBertaSentenceEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>

<span class="c1">// Then the training can start with the transformer embeddings</span>
<span class="k">val</span> <span class="nv">docClassifier</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">ClassifierDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"category"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setBatchSize</span><span class="o">(</span><span class="mi">64</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">20</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLr</span><span class="o">(</span><span class="mi">5</span><span class="n">e</span><span class="o">-</span><span class="mf">3f</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setDropout</span><span class="o">(</span><span class="mf">0.5f</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">docClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">smallCorpus</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">sentenceEmbeddings</span> <span class="o">=</span> <span class="n">XlmRoBertaSentenceEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># you can either use the output to train ClassifierDL, SentimentDL, or MultiClassifierDL
# or you can use EmbeddingsFinisher to prepare the results for Spark ML functions
</span>
<span class="n">embeddingsFinisher</span> <span class="o">=</span> <span class="n">EmbeddingsFinisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence_embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"finished_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputAsVector</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCleanAnnotations</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setStages</span><span class="p">([</span>
      <span class="n">documentAssembler</span><span class="p">,</span>
      <span class="n">tokenizer</span><span class="p">,</span>
      <span class="n">sentenceEmbeddings</span><span class="p">,</span>
      <span class="n">embeddingsFinisher</span>
    <span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"This is a sentence."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.05969233065843582</span><span class="p">,</span><span class="o">-</span><span class="mf">0.030789051204919815</span><span class="p">,</span><span class="mf">0.04443822056055069</span><span class="p">,</span><span class="mf">0.09564960747</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.038839809596538544</span><span class="p">,</span><span class="mf">0.011712731793522835</span><span class="p">,</span><span class="mf">0.019954433664679527</span><span class="p">,</span><span class="mf">0.0667808502</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.03952755779027939</span><span class="p">,</span><span class="o">-</span><span class="mf">0.03455188870429993</span><span class="p">,</span><span class="mf">0.019103847444057465</span><span class="p">,</span><span class="mf">0.04311436787</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.09579929709434509</span><span class="p">,</span><span class="mf">0.02494969218969345</span><span class="p">,</span><span class="o">-</span><span class="mf">0.014753809198737144</span><span class="p">,</span><span class="mf">0.10259044915</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="mf">0.004710011184215546</span><span class="p">,</span><span class="o">-</span><span class="mf">0.022148698568344116</span><span class="p">,</span><span class="mf">0.011723337695002556</span><span class="p">,</span><span class="o">-</span><span class="mf">0.013356896</span><span class="p">...</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.EmbeddingsFinisher</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span><span class="s">"document"</span><span class="o">))</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentenceEmbeddings</span> <span class="k">=</span> <span class="nv">XlmRoBertaSentenceEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="c1">// you can either use the output to train ClassifierDL, SentimentDL, or MultiClassifierDL</span>
<span class="c1">// or you can use EmbeddingsFinisher to prepare the results for Spark ML functions</span>

<span class="k">val</span> <span class="nv">embeddingsFinisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">EmbeddingsFinisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"finished_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputAsVector</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCleanAnnotations</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
    <span class="n">documentAssembler</span><span class="o">,</span>
    <span class="n">tokenizer</span><span class="o">,</span>
    <span class="n">sentenceEmbeddings</span><span class="o">,</span>
    <span class="n">embeddingsFinisher</span>
  <span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"This is a sentence."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">80</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">05969233065843582</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">030789051204919815</span>,<span class="err">0</span><span class="kt">.</span><span class="err">04443822056055069</span>,<span class="err">0</span><span class="kt">.</span><span class="err">09564960747</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">038839809596538544</span>,<span class="err">0</span><span class="kt">.</span><span class="err">011712731793522835</span>,<span class="err">0</span><span class="kt">.</span><span class="err">019954433664679527</span>,<span class="err">0</span><span class="kt">.</span><span class="err">0667808502</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">03952755779027939</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">03455188870429993</span>,<span class="err">0</span><span class="kt">.</span><span class="err">019103847444057465</span>,<span class="err">0</span><span class="kt">.</span><span class="err">04311436787</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">09579929709434509</span>,<span class="err">0</span><span class="kt">.</span><span class="err">02494969218969345</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">014753809198737144</span>,<span class="err">0</span><span class="kt">.</span><span class="err">10259044915</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="err">0</span><span class="kt">.</span><span class="err">004710011184215546</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">022148698568344116</span>,<span class="err">0</span><span class="kt">.</span><span class="err">011723337695002556</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">013356896</span><span class="kt">...|</span>
<span class="kt">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="xlnetembeddings">XlnetEmbeddings</h2>

  <p>XlnetEmbeddings (XLNet): Generalized Autoregressive Pretraining for Language Understanding</p>

  <p>XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language
modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance
for language tasks involving long context. Overall, XLNet achieves state-of-the-art (SOTA) results on various
downstream language tasks including question answering, natural language inference, sentiment analysis, and document
ranking.</p>

  <p>These word embeddings represent the outputs generated by the XLNet models.</p>

  <p>Note that this is a very computationally expensive module compared to word embedding modules that only perform embedding lookups.
The use of an accelerator is recommended.</p>

  <table>
    <thead>
      <tr>
        <th>Spark NLP Model</th>
        <th>Google Model</th>
        <th>Model Properties</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><code class="language-plaintext highlighter-rouge">"xlnet_large_cased"</code></td>
        <td><a href="https://storage.googleapis.com/xlnet/released_models/cased_L-24_H-1024_A-16.zip">XLNet-Large</a></td>
        <td>24-layer, 1024-hidden, 16-heads</td>
      </tr>
      <tr>
        <td><code class="language-plaintext highlighter-rouge">"xlnet_base_cased"</code></td>
        <td><a href="https://storage.googleapis.com/xlnet/released_models/cased_L-12_H-768_A-12.zip">XLNet-Base</a></td>
        <td>12-layer, 768-hidden, 12-heads. This model is trained on full data (different from the one in the paper).</td>
      </tr>
    </tbody>
  </table>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val embeddings = XlnetEmbeddings.pretrained()
  .setInputCols("sentence", "token")
  .setOutputCol("embeddings")

# Offline - Download the pretrained model manually and extract it
xlnet = XlnetEmbeddings.load("/xlnet_large_cased_en_2.5.0_2.4_1588074397954") \
        .setInputCols("sentence", "token") \
        .setOutputCol("xlnet")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"xlnet_base_cased"</code>, if no name is provided.</p>

  <p>For extended examples of usage, see the <a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/training/english/dl-ner/ner_xlnet.ipynb">Spark NLP Workshop</a>
and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/embeddings/XlnetEmbeddingsTestSpec.scala">XlnetEmbeddingsTestSpec</a>.</p>

  <p><strong>Sources :</strong></p>

  <p><a href="https://arxiv.org/abs/1906.08237">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></p>

  <p>https://github.com/zihangdai/xlnet</p>

  <p><strong>Paper abstract:</strong></p>

  <p><em>With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves
better performance than pretraining approaches based on autoregressive language modeling. However, relying on
corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune
discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that
(1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the
factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore,
XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically,
under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question
answering, natural language inference, sentiment analysis, and document ranking.</em></p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">WORD_EMBEDDINGS</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.XlnetEmbeddings.html">XlnetEmbeddings</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/XlnetEmbeddings">XlnetEmbeddings</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/embeddings/XlnetEmbeddings.scala">XlnetEmbeddings</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLModel
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">XlnetEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"xlnet_base_cased"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">'document'</span><span class="p">,</span> <span class="s">'token'</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">'embeddings'</span><span class="p">)</span>

<span class="c1"># This pretrained model requires those specific transformer embeddings
</span><span class="n">ner_model</span> <span class="o">=</span> <span class="n">NerDLModel</span><span class="p">.</span><span class="n">pretrained</span><span class="p">(</span><span class="s">"ner_conll_xlnet_base_cased"</span><span class="p">,</span> <span class="s">"en"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">ner_model</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"ner.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">I</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLModel</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="c1">// Use the transformer embeddings</span>
<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">XlnetEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"xlnet_base_cased"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
<span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
<span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// This pretrained model requires those specific transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerModel</span> <span class="k">=</span> <span class="nv">NerDLModel</span><span class="o">.</span><span class="py">pretrained</span><span class="o">(</span><span class="s">"ner_conll_xlnet_base_cased"</span><span class="o">,</span> <span class="s">"en"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerModel</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"U.N. official Ekeus heads for Baghdad."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"ner.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                              <span class="o">|</span>
<span class="o">+------------------------------------+</span>
<span class="o">|[</span><span class="kt">I-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">I-LOC</span>, <span class="kt">O</span><span class="o">]|</span>
<span class="o">+------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="c1"># First extract the prerequisites for the NerDLApproach
</span><span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">sentence</span> <span class="o">=</span> <span class="n">SentenceDetector</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"sentence"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="c1"># Use the transformer embeddings
</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">XlnetEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="c1"># Then the training can start with the transformer embeddings
</span><span class="n">nerTagger</span> <span class="o">=</span> <span class="n">NerDLApproach</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"sentence"</span><span class="p">,</span> <span class="s">"token"</span><span class="p">,</span> <span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setLabelColumn</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"ner"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setMaxEpochs</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setVerbose</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">sentence</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">nerTagger</span>
<span class="p">])</span>

<span class="c1"># We use the text and labels from the CoNLL dataset
</span><span class="n">conll</span> <span class="o">=</span> <span class="n">CoNLL</span><span class="p">()</span>
<span class="n">trainingData</span> <span class="o">=</span> <span class="n">conll</span><span class="p">.</span><span class="n">readDataset</span><span class="p">(</span><span class="n">spark</span><span class="p">,</span> <span class="s">"eng.train"</span><span class="p">)</span>

<span class="n">pipelineModel</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trainingData</span><span class="p">)</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.training.CoNLL</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="c1">// First extract the prerequisites for the NerDLApproach</span>
<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">sentence</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">SentenceDetector</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">XlnetEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="c1">// Then the training can start with the transformer embeddings</span>
<span class="k">val</span> <span class="nv">nerTagger</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">NerDLApproach</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"sentence"</span><span class="o">,</span> <span class="s">"token"</span><span class="o">,</span> <span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setLabelColumn</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"ner"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setMaxEpochs</span><span class="o">(</span><span class="mi">1</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setRandomSeed</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setVerbose</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">sentence</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">nerTagger</span>
<span class="o">))</span>

<span class="c1">// We use the text and labels from the CoNLL dataset</span>
<span class="k">val</span> <span class="nv">conll</span> <span class="k">=</span> <span class="nc">CoNLL</span><span class="o">()</span>
<span class="k">val</span> <span class="nv">trainingData</span> <span class="k">=</span> <span class="nv">conll</span><span class="o">.</span><span class="py">readDataset</span><span class="o">(</span><span class="n">spark</span><span class="o">,</span> <span class="s">"src/test/resources/conll2003/eng.train"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipelineModel</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">trainingData</span><span class="o">)</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.training</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">embeddings</span> <span class="o">=</span> <span class="n">XlnetEmbeddings</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"embeddings"</span><span class="p">)</span>

<span class="n">embeddingsFinisher</span> <span class="o">=</span> <span class="n">EmbeddingsFinisher</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"embeddings"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCols</span><span class="p">(</span><span class="s">"finished_embeddings"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputAsVector</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCleanAnnotations</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">,</span>
    <span class="n">embeddingsFinisher</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"This is a sentence."</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">80</span><span class="p">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.6287205219268799</span><span class="p">,</span><span class="o">-</span><span class="mf">0.4865287244319916</span><span class="p">,</span><span class="o">-</span><span class="mf">0.186111718416214</span><span class="p">,</span><span class="mf">0.234187275171279</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">1.1967450380325317</span><span class="p">,</span><span class="mf">0.2746637463569641</span><span class="p">,</span><span class="mf">0.9481253027915955</span><span class="p">,</span><span class="mf">0.3431355059146881</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">1.0777631998062134</span><span class="p">,</span><span class="o">-</span><span class="mf">2.092679977416992</span><span class="p">,</span><span class="o">-</span><span class="mf">1.5331977605819702</span><span class="p">,</span><span class="o">-</span><span class="mf">1.11190271377563</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.8349916934967041</span><span class="p">,</span><span class="o">-</span><span class="mf">0.45627787709236145</span><span class="p">,</span><span class="o">-</span><span class="mf">0.7890847325325012</span><span class="p">,</span><span class="o">-</span><span class="mf">1.028069257736</span><span class="p">...</span><span class="o">|</span>
<span class="o">|</span><span class="p">[</span><span class="o">-</span><span class="mf">0.134845569729805</span><span class="p">,</span><span class="o">-</span><span class="mf">0.11672890186309814</span><span class="p">,</span><span class="mf">0.4945235550403595</span><span class="p">,</span><span class="o">-</span><span class="mf">0.66587203741073</span><span class="p">...</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base.DocumentAssembler</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotators.Tokenizer</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.embeddings.XlnetEmbeddings</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.EmbeddingsFinisher</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddings</span> <span class="k">=</span> <span class="nv">XlnetEmbeddings</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">embeddingsFinisher</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">EmbeddingsFinisher</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCols</span><span class="o">(</span><span class="s">"finished_embeddings"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputAsVector</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCleanAnnotations</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">embeddings</span><span class="o">,</span>
  <span class="n">embeddingsFinisher</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"This is a sentence."</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">selectExpr</span><span class="o">(</span><span class="s">"explode(finished_embeddings) as result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="mi">80</span><span class="o">)</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|</span>                                                                          <span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">6287205219268799</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">4865287244319916</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">186111718416214</span>,<span class="err">0</span><span class="kt">.</span><span class="err">234187275171279</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">1967450380325317</span>,<span class="err">0</span><span class="kt">.</span><span class="err">2746637463569641</span>,<span class="err">0</span><span class="kt">.</span><span class="err">9481253027915955</span>,<span class="err">0</span><span class="kt">.</span><span class="err">3431355059146881</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">0777631998062134</span>,<span class="kt">-</span><span class="err">2</span><span class="kt">.</span><span class="err">092679977416992</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">5331977605819702</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">11190271377563</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">8349916934967041</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">45627787709236145</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">7890847325325012</span>,<span class="kt">-</span><span class="err">1</span><span class="kt">.</span><span class="err">028069257736</span><span class="kt">...|</span>
<span class="kt">|</span><span class="o">[</span><span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">134845569729805</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">11672890186309814</span>,<span class="err">0</span><span class="kt">.</span><span class="err">4945235550403595</span>,<span class="kt">-</span><span class="err">0</span><span class="kt">.</span><span class="err">66587203741073</span><span class="kt">...|</span>
<span class="kt">+--------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<div class="h3-box model-content">

  <h2 id="xlnetfortokenclassification">XlnetForTokenClassification</h2>

  <p>XlnetForTokenClassification can load XLNet Models with a token classification head on top (a linear layer on top of the hidden-states output)
e.g. for Named-Entity-Recognition (NER) tasks.</p>

  <p>Pretrained models can be loaded with <code class="language-plaintext highlighter-rouge">pretrained</code> of the companion object:</p>
  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>val tokenClassifier = XlnetForTokenClassification.pretrained()
  .setInputCols("token", "document")
  .setOutputCol("label")
</code></pre></div>  </div>
  <p>The default model is <code class="language-plaintext highlighter-rouge">"xlnet_base_token_classifier_conll03"</code>, if no name is provided.</p>

  <p>For available pretrained models please see the <a href="https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition">Models Hub</a>.</p>

  <p>and the <a href="https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/test/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/XlnetForTokenClassificationTestSpec.scala">XlnetForTokenClassificationTestSpec</a>.
Models from the HuggingFace  Transformers library are also compatible with Spark NLP . To see which models are compatible and how to import them see <a href="https://github.com/JohnSnowLabs/spark-nlp/discussions/5669">Import Transformers into Spark NLP </a>.</p>

  <p><strong>Input Annotator Types:</strong> <code class="language-plaintext highlighter-rouge">DOCUMENT, TOKEN</code></p>

  <p><strong>Output Annotator Type:</strong> <code class="language-plaintext highlighter-rouge">NAMED_ENTITY</code></p>

  <table>
    <tbody>
      <tr>
        <td><strong>Python API:</strong> <a href="https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp.annotator.XlnetForTokenClassification.html">XlnetForTokenClassification</a></td>
        <td><strong>Scala API:</strong> <a href="https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/classifier/dl/XlnetForTokenClassification">XlnetForTokenClassification</a></td>
        <td><strong>Source:</strong> <a href="https://github.com/JohnSnowLabs/spark-nlp/tree/master/src/main/scala/com/johnsnowlabs/nlp/annotators/classifier/dl/XlnetForTokenClassification.scala">XlnetForTokenClassification</a></td>
      </tr>
    </tbody>
  </table>

  <details>

<summary class="button"><b>Show Examples</b></summary>

<div class="tabs-box">

      <div class="top_tab_li">
    <button class="tab-li code-selector-active prediction-button">Prediction</button>
    <button class="tab-li code-selector-un-active training-button">Training</button>
    <button class="tab-li code-selector-un-active embeddings-button">Embeddings</button>
</div>

      <div class="tabs-box prediction-content">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to predict classes by using the embeddings generated by
the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sparknlp</span>
<span class="kn">from</span> <span class="nn">sparknlp.base</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">sparknlp.annotator</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">pyspark.ml</span> <span class="kn">import</span> <span class="n">Pipeline</span>

<span class="n">documentAssembler</span> <span class="o">=</span> <span class="n">DocumentAssembler</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCol</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"document"</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"token"</span><span class="p">)</span>

<span class="n">tokenClassifier</span> <span class="o">=</span> <span class="n">XlnetForTokenClassification</span><span class="p">.</span><span class="n">pretrained</span><span class="p">()</span> \
    <span class="p">.</span><span class="n">setInputCols</span><span class="p">([</span><span class="s">"token"</span><span class="p">,</span> <span class="s">"document"</span><span class="p">])</span> \
    <span class="p">.</span><span class="n">setOutputCol</span><span class="p">(</span><span class="s">"label"</span><span class="p">)</span> \
    <span class="p">.</span><span class="n">setCaseSensitive</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>

<span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">().</span><span class="n">setStages</span><span class="p">([</span>
    <span class="n">documentAssembler</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">,</span>
    <span class="n">tokenClassifier</span>
<span class="p">])</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">([[</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="p">]]).</span><span class="n">toDF</span><span class="p">(</span><span class="s">"text"</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">data</span><span class="p">).</span><span class="n">transform</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">result</span><span class="p">.</span><span class="n">select</span><span class="p">(</span><span class="s">"label.result"</span><span class="p">).</span><span class="n">show</span><span class="p">(</span><span class="n">truncate</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="p">[</span><span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">I</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">PER</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">O</span><span class="p">,</span> <span class="n">B</span><span class="o">-</span><span class="n">LOC</span><span class="p">]</span><span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="nn">spark.implicits._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.base._</span>
<span class="k">import</span> <span class="nn">com.johnsnowlabs.nlp.annotator._</span>
<span class="k">import</span> <span class="nn">org.apache.spark.ml.Pipeline</span>

<span class="k">val</span> <span class="nv">documentAssembler</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DocumentAssembler</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCol</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenizer</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Tokenizer</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"token"</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">tokenClassifier</span> <span class="k">=</span> <span class="nv">XlnetForTokenClassification</span><span class="o">.</span><span class="py">pretrained</span><span class="o">()</span>
  <span class="o">.</span><span class="py">setInputCols</span><span class="o">(</span><span class="s">"token"</span><span class="o">,</span> <span class="s">"document"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setOutputCol</span><span class="o">(</span><span class="s">"label"</span><span class="o">)</span>
  <span class="o">.</span><span class="py">setCaseSensitive</span><span class="o">(</span><span class="kc">true</span><span class="o">)</span>

<span class="k">val</span> <span class="nv">pipeline</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Pipeline</span><span class="o">().</span><span class="py">setStages</span><span class="o">(</span><span class="nc">Array</span><span class="o">(</span>
  <span class="n">documentAssembler</span><span class="o">,</span>
  <span class="n">tokenizer</span><span class="o">,</span>
  <span class="n">tokenClassifier</span>
<span class="o">))</span>

<span class="k">val</span> <span class="nv">data</span> <span class="k">=</span> <span class="nc">Seq</span><span class="o">(</span><span class="s">"John Lenon was born in London and lived in Paris. My name is Sarah and I live in London"</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"text"</span><span class="o">)</span>
<span class="k">val</span> <span class="nv">result</span> <span class="k">=</span> <span class="nv">pipeline</span><span class="o">.</span><span class="py">fit</span><span class="o">(</span><span class="n">data</span><span class="o">).</span><span class="py">transform</span><span class="o">(</span><span class="n">data</span><span class="o">)</span>

<span class="nv">result</span><span class="o">.</span><span class="py">select</span><span class="o">(</span><span class="s">"label.result"</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|</span><span class="n">result</span>                                                                              <span class="o">|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
<span class="o">|[</span><span class="kt">B-PER</span>, <span class="kt">I-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-PER</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">O</span>, <span class="kt">B-LOC</span><span class="o">]|</span>
<span class="o">+------------------------------------------------------------------------------------+</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box training-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to train an Approach Annotator by using the embeddings
generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator needs to be trained externally. Please see the training page
# for instructions.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator needs to be trained externally. Please see the training page</span>
<span class="c1">// for instructions.</span>
</code></pre></div>        </div>

      </div>

      <div class="tabs-box embeddings-content" style="display: none;">

        <div class="top_tab_li">   
    <button class="tab-li code-selector-active python-button">Python</button><button class="tab-li code-selector-un-active scala-button">Scala</button>
</div>

        <p>This example shows how to extract the embeddings generated by the Transformer.</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># This annotator has a fully connected layer attached for classification. For
# embeddings see the base transformer annotator.
</span></code></pre></div>        </div>

        <div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// This annotator has a fully connected layer attached for classification. For</span>
<span class="c1">// embeddings see the base transformer annotator.</span>
</code></pre></div>        </div>

      </div>

    </div>

</details>

</div>

<h2 id="import-transformers-into-spark-nlp">Import Transformers into Spark NLP</h2>

<h3 id="overview">Overview</h3>

<p>We have extended support for <code class="language-plaintext highlighter-rouge">HuggingFace</code>    and <code class="language-plaintext highlighter-rouge">TF Hub</code> exported models since <code class="language-plaintext highlighter-rouge">3.1.0</code> to equivalent Spark NLP  annotators. Starting this release, you can easily use the <code class="language-plaintext highlighter-rouge">saved_model</code> feature in HuggingFace within a few lines of codes and import any <code class="language-plaintext highlighter-rouge">BERT</code>, <code class="language-plaintext highlighter-rouge">DistilBERT</code>, <code class="language-plaintext highlighter-rouge">RoBERTa</code>, <code class="language-plaintext highlighter-rouge">XLM-RoBERTa</code>, <code class="language-plaintext highlighter-rouge">Longformer</code>, <code class="language-plaintext highlighter-rouge">BertForTokenClassification</code>, <code class="language-plaintext highlighter-rouge">DistilBertForTokenClassification</code>, <code class="language-plaintext highlighter-rouge">AlbertForTokenClassification</code>, <code class="language-plaintext highlighter-rouge">RoBertaForTokenClassification</code>, <code class="language-plaintext highlighter-rouge">XlmRoBertaForTokenClassification</code>, <code class="language-plaintext highlighter-rouge">XlnetForTokenClassification</code>,  <code class="language-plaintext highlighter-rouge">LongformerForTokenClassification</code>, <code class="language-plaintext highlighter-rouge">BertForSequenceClassification</code>, and <code class="language-plaintext highlighter-rouge">DistilBertForSequenceClassification</code>  models to Spark NLP. We will work on the remaining annotators and extend this support to the rest with each release </p>

<h3 id="compatibility">Compatibility</h3>

<p><strong>Spark NLP</strong>: The equivalent annotator in Spark NLP
<strong>TF Hub</strong>: Models from <a href="https://tfhub.dev/">TF Hub</a>
<strong>HuggingFace</strong>: Models from <a href="https://huggingface.co/models">HuggingFace</a>
<strong>Model Architecture</strong>: Which architecture is compatible with that annotator
<strong>Flags</strong>:</p>

<ul>
  <li>Fully supported </li>
  <li>Partially supported (requires workarounds) </li>
  <li>Under development </li>
  <li>Not supported </li>
</ul>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Spark NLP</th>
      <th style="text-align: left">TF Hub</th>
      <th style="text-align: left">HuggingFace</th>
      <th style="text-align: left">Model Architecture</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">BertEmbeddings</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">BERT - Small BERT - ELECTRA</td>
    </tr>
    <tr>
      <td style="text-align: left">BertSentenceEmbeddings</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">BERT - Small BERT - ELECTRA</td>
    </tr>
    <tr>
      <td style="text-align: left">DistilBertEmbeddings</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">DistilBERT</td>
    </tr>
    <tr>
      <td style="text-align: left">RoBertaEmbeddings</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">RoBERTa - DistilRoBERTa</td>
    </tr>
    <tr>
      <td style="text-align: left">XlmRoBertaEmbeddings</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">XLM-RoBERTa</td>
    </tr>
    <tr>
      <td style="text-align: left">AlbertEmbeddings</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">ALBERT</td>
    </tr>
    <tr>
      <td style="text-align: left">XlnetEmbeddings</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">XLNet</td>
    </tr>
    <tr>
      <td style="text-align: left">LongformerEmbeddings</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">Longformer</td>
    </tr>
    <tr>
      <td style="text-align: left">ElmoEmbeddings</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
    </tr>
    <tr>
      <td style="text-align: left">UniversalSentenceEncoder</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
    </tr>
    <tr>
      <td style="text-align: left">BertForTokenClassification</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">TFBertForTokenClassification</td>
    </tr>
    <tr>
      <td style="text-align: left">DistilBertForTokenClassification</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">TFDistilBertForTokenClassification</td>
    </tr>
    <tr>
      <td style="text-align: left">AlbertForTokenClassification</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">TFAlbertForTokenClassification</td>
    </tr>
    <tr>
      <td style="text-align: left">RoBertaForTokenClassification</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">TFRobertaForTokenClassification</td>
    </tr>
    <tr>
      <td style="text-align: left">XlmRoBertaForTokenClassification</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">TFXLMRobertaForTokenClassification</td>
    </tr>
    <tr>
      <td style="text-align: left">XlnetForTokenClassification</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">TFXLNetForTokenClassificationet</td>
    </tr>
    <tr>
      <td style="text-align: left">LongformerForTokenClassification</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">TFLongformerForTokenClassification</td>
    </tr>
    <tr>
      <td style="text-align: left">BertForSequenceClassification</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">TFBertForSequenceClassification</td>
    </tr>
    <tr>
      <td style="text-align: left">DistilBertForSequenceClassification</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left">TFDistilBertForSequenceClassification</td>
    </tr>
    <tr>
      <td style="text-align: left">T5Transformer</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
    </tr>
    <tr>
      <td style="text-align: left">MarianTransformer</td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
      <td style="text-align: left"></td>
    </tr>
  </tbody>
</table>

<h3 id="example-notebooks">Example Notebooks</h3>

<h4 id="huggingface-to-spark-nlp">HuggingFace to Spark NLP</h4>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Spark NLP</th>
      <th style="text-align: left">HuggingFace Notebooks</th>
      <th style="text-align: left">Colab</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">BertEmbeddings</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BERT.ipynb">HuggingFace in Spark NLP - BERT</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BERT.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">BertSentenceEmbeddings</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BERT%20Sentence.ipynb">HuggingFace in Spark NLP - BERT Sentence</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BERT%20Sentence.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">DistilBertEmbeddings</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20DistilBERT.ipynb">HuggingFace in Spark NLP - DistilBERT</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20DistilBERT.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">RoBertaEmbeddings</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20RoBERTa.ipynb">HuggingFace in Spark NLP - RoBERTa</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20RoBERTa.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">XlmRoBertaEmbeddings</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20XLM-RoBERTa.ipynb">HuggingFace in Spark NLP - XLM-RoBERTa</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20XLM-RoBERTa.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">AlbertEmbeddings</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20ALBERT.ipynb">HuggingFace in Spark NLP - ALBERT</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20ALBERT.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">XlnetEmbeddings</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20XLNet.ipynb">HuggingFace in Spark NLP - XLNet</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20XLNet.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">LongformerEmbeddings</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20Longformer.ipynb">HuggingFace in Spark NLP - Longformer</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20Longformer.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">BertForTokenClassification</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BertForTokenClassification.ipynb">HuggingFace in Spark NLP - BertForTokenClassification</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BertForTokenClassification.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">DistilBertForTokenClassification</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20DistilBertForTokenClassification.ipynb">HuggingFace in Spark NLP - DistilBertForTokenClassification</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20DistilBertForTokenClassification.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">AlbertForTokenClassification</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20AlbertForTokenClassification.ipynb">HuggingFace in Spark NLP - AlbertForTokenClassification</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20AlbertForTokenClassification.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">RoBertaForTokenClassification</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20RoBertaForTokenClassification.ipynb">HuggingFace in Spark NLP - RoBertaForTokenClassification</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20RoBertaForTokenClassification.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">XlmRoBertaForTokenClassification</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20XlmRoBertaForTokenClassification.ipynb">HuggingFace in Spark NLP - XlmRoBertaForTokenClassification</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20XlmRoBertaForTokenClassification.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">BertForSequenceClassification</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BertForSequenceClassification.ipynb">HuggingFace in Spark NLP - BertForSequenceClassification</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BertForSequenceClassification.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">DistilBertForSequenceClassification</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20DistilBertForSequenceClassification.ipynb">HuggingFace in Spark NLP - DistilBertForSequenceClassification</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20DistilBertForSequenceClassification.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
  </tbody>
</table>

<h4 id="tf-hub-to-spark-nlp">TF Hub to Spark NLP</h4>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Spark NLP</th>
      <th style="text-align: left">TF Hub Notebooks</th>
      <th style="text-align: left">Colab</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">BertEmbeddings</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/TF%20Hub%20in%20Spark%20NLP%20-%20BERT.ipynb">TF Hub in Spark NLP - BERT</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/TF%20Hub%20in%20Spark%20NLP%20-%20BERT.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">BertSentenceEmbeddings</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/TF%20Hub%20in%20Spark%20NLP%20-%20BERT%20Sentence.ipynb">TF Hub in Spark NLP - BERT Sentence</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/TF%20Hub%20in%20Spark%20NLP%20-%20BERT%20Sentence.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
    <tr>
      <td style="text-align: left">AlbertEmbeddings</td>
      <td style="text-align: left"><a href="https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/TF%20Hub%20in%20Spark%20NLP%20-%20ALBERT.ipynb">TF Hub in Spark NLP - ALBERT</a></td>
      <td style="text-align: left"><a href="https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/TF%20Hub%20in%20Spark%20NLP%20-%20ALBERT.ipynb"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" /></a></td>
    </tr>
  </tbody>
</table>
</div><div class="d-print-none"><footer class="article__footer"><span class="footer_date">Last updated
      <time itemprop="dateModified" datetime="2021-08-05T00:00:00+00:00">Aug 05, 2021</time>
    </span><!-- start custom article footer snippet -->

<!-- end custom article footer snippet --></footer>

<script>


jQuery(document).ready(function(){  
    $( ".scala-button" ).click(function() {
        $(this).closest( ".tabs-box" ).find(".scala-button").removeClass('code-selector-un-active').addClass( "code-selector-active" );        

        //remove  active class from all other buttons
        $(this).closest( ".tabs-box" ).find(".nlu-button").removeClass('code-selector-active').addClass('code-selector-un-active');
        $(this).closest( ".tabs-box" ).find(".python-button").removeClass('code-selector-active').addClass('code-selector-un-active');

        //toggle language snippets
        $(this).closest( ".tabs-box" ).find( ".language-scala" ).show();
        $(this).closest( ".tabs-box" ).find( ".language-python, .nlu-block" ).hide();
    });

    $( ".python-button" ).click(function() {
        //set current button to active class and remove unactive class
        $(this).closest( ".tabs-box" ).find(".python-button").removeClass('code-selector-un-active').addClass( "code-selector-active" ); 

        //remove  active class from all other buttons
        $(this).closest( ".tabs-box" ).find(".nlu-button").removeClass('code-selector-active').addClass('code-selector-un-active');
        $(this).closest( ".tabs-box" ).find(".scala-button").removeClass('code-selector-active').addClass('code-selector-un-active');


        //toggle language snippets
        $(this).closest( ".tabs-box" ).find( ".language-python" ).show();
        $(this).closest( ".tabs-box" ).find( ".nlu-block, .language-scala" ).hide();
    });

    $( ".nlu-button" ).click(function() {
        //set current button to active class and remove unactive class
        $(this).closest( ".tabs-box" ).find(".nlu-button").removeClass('code-selector-un-active').addClass( "code-selector-active" );        

        //remove  active class from all other buttons
        $(this).closest( ".tabs-box" ).find(".scala-button").removeClass('code-selector-active').addClass('code-selector-un-active');
        $(this).closest( ".tabs-box" ).find(".python-button").removeClass('code-selector-active').addClass('code-selector-un-active');

        //toggle language snippets        
        $(this).closest( ".tabs-box" ).find( ".language-python, .language-scala" ).hide();
        $(this).closest( ".tabs-box" ).find( ".nlu-block" ).show();
    });
});

function togglePython1() {

    //set current button to active class and remove unactive class
    $( ".python-button" ).addClass( "code-selector-active" );


    //toggle language snippets
    $( ".tabs-box .language-python" ).show() 
    $( ".tabs-box .nlu-block" ).hide()
    $( ".tabs-box .language-scala" ).hide()
}

function defer(method) { //wait until jquery ready
    if (window.jQuery) {
        method();
    } else {
        setTimeout(function() { defer(method) }, 15);
    }
}

defer(function () { // load inital language
    togglePython1()
});




</script>


<style>
  /* Remove Scrollbar from Code Segments */
.article__content .highlighter-rouge > .highlight > pre > code, .article__content figure.highlight > pre > code  {
    overflow: auto;
}



button.code-selector-active {
 background-color: white;
 color: #08c;
 font-weight: bold;
 border-width: 1px;
 padding-left: 12px;
 padding-right: 12px;
 width: 90px;
 padding-top: 6px;
 margin-right: 2px;

 border-bottom: none;

 position: relative;
 z-index: 2;
}

button.code-selector-un-active {
    background-color: white;
    padding-left: 12px;
    padding-right: 12px;
    width: 90px;
    margin-right: 2px;
    padding-top: 8px;
    position: relative;
    border-bottom: none;

   }

hr.code-selector-underlie {
    border-top: 1px solid;
    background-color: black;
    width: fill;
    height: 1px;
    margin-top: -3px;
    position: relative;

}

</style><div class="article__section-navigator clearfix"><div class="previous nav_link"><span>PREVIOUS</span><a href="/docs/en/annotators">Annotators</a></div><div class="next nav_link"><span>NEXT</span><a href="/docs/en/training">Training</a></div></div></div>

</div>
</div>

<script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    $(function() {
      var $this ,$scroll;
      var $articleContent = $('.js-article-content');
      var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');
      var scroll = hasSidebar ? '.js-page-main' : 'html, body';
      $scroll = $(scroll);

      $articleContent.find('.highlight').each(function() {
        $this = $(this);
        $this.attr('data-lang', $this.find('code').attr('data-lang'));
      });
      $articleContent.find('h1[id], h2[id], h3[id], h4[id], h5[id], h6[id]').each(function() {
        $this = $(this);
        $this.append($('<a class="anchor d-print-none" aria-hidden="true"></a>').html('<i class="fas fa-anchor"></i>'));
      });
      $articleContent.on('click', '.anchor', function() {
        $scroll.scrollToAnchor('#' + $(this).parent().attr('id'), 400);
      });
    });
  });
})();
</script></div><section class="page__comments d-print-none"></section></article><!-- start custom main bottom snippet -->

<!-- end custom main bottom snippet --></div>
            </div></div></div><div class="page__footer d-print-none">
<footer class="footer py-4 js-page-footer">
  <div class="main"><div itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content=""><meta itemprop="url" content="/"></div><div class="site-info mt-2">
      <div> 2021 John Snow Labs Inc.
        <a href="http://www.johnsnowlabs.com/terms-of-service">Terms of Service</a> | <a href="http://www.johnsnowlabs.com/privacy-policy/">Privacy Policy</a>
      </div>
    </div>
  </div>
</footer>

<script>

/* Responsive menu
	 ========================================================*/
jQuery(document).ready(function($) {
	jQuery('#responsive_menu').click(function(e) {
      e.preventDefault();
      jQuery(this).toggleClass('close');
      jQuery('.top_navigation').toggleClass('open');
  });
  jQuery('#aside_menu').click(function(e) {
      e.preventDefault();
      jQuery(this).toggleClass('close');
      jQuery('.js-col-aside').toggleClass('open');
      if (jQuery(window).width() <= 1023)
      {
        jQuery('.page__sidebar').toggleClass('open'); 
      jQuery('.demopage-sidemenu').toggleClass('open');
      }
  });
  jQuery('.toc--ellipsis a').click(function(e) {
    if (jQuery(window).width() <= 767)
      {
        jQuery('.js-col-aside').removeClass('open');
        jQuery('.page__sidebar').removeClass('open');    
        jQuery('#aside_menu').removeClass('close');  
      }       
  });
});

/*TABS*/
function openTabCall(cityName){
  // Declare all variables
  var i, tabcontent, tablinks;

  // Get all elements with class="tabcontent" and hide them
  tabcontent = document.getElementsByClassName("tabcontent");
  for (i = 0; i < tabcontent.length; i++) {
    tabcontent[i].style.display = "none";
  }

  // Get all elements with class="tablinks" and remove the class "active"
  tablinks = document.getElementsByClassName("tablinks");
  for (i = 0; i < tablinks.length; i++) {
    tablinks[i].className = tablinks[i].className.replace(" active", "");
  }

  // Show the current tab, and add an "active" class to the button that opened the tab
  document.getElementById(cityName).style.display = "block";
}

function openTab(evt, cityName) {
  openTabCall(cityName);
  evt.currentTarget.className += " active";
}

/*OPen by URL*/
$(document).ready(function () {  
  const tabName = (window.location.hash || '').replace('#', '');
  const tab = document.getElementById(tabName || 'opensource');
  if (tab) {
    tab.click();
  }
});

jQuery(document).ready(function(){
	jQuery('.tab-item').click(function(event) {		
		if (($(window).width() > 400) && ($(window).width() < 1199))
	    {
	    	jQuery('.tab-item').removeClass('open');
	        jQuery(this).toggleClass('open');
	    }
  });
  

});


 

</script></div></div>
    </div></div></div><script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $body = $('body'), $window = $(window);
    var $pageRoot = $('.js-page-root'), $pageMain = $('.js-page-main');
    var activeCount = 0;
    function modal(options) {
      var $root = this, visible, onChange, hideWhenWindowScroll = false;
      var scrollTop;
      function setOptions(options) {
        var _options = options || {};
        visible = _options.initialVisible === undefined ? false : show;
        onChange = _options.onChange;
        hideWhenWindowScroll = _options.hideWhenWindowScroll;
      }
      function init() {
        setState(visible);
      }
      function setState(isShow) {
        if (isShow === visible) {
          return;
        }
        visible = isShow;
        if (visible) {
          activeCount++;
          scrollTop = $(window).scrollTop() || $pageMain.scrollTop();
          $root.addClass('modal--show');
          $pageMain.scrollTop(scrollTop);
          activeCount === 1 && ($pageRoot.addClass('show-modal'), $body.addClass('of-hidden'));
          hideWhenWindowScroll && window.hasEvent('touchstart') && $window.on('scroll', hide);
          $window.on('keyup', handleKeyup);
        } else {
          activeCount > 0 && activeCount--;
          $root.removeClass('modal--show');
          $window.scrollTop(scrollTop);
          activeCount === 0 && ($pageRoot.removeClass('show-modal'), $body.removeClass('of-hidden'));
          hideWhenWindowScroll && window.hasEvent('touchstart') && $window.off('scroll', hide);
          $window.off('keyup', handleKeyup);
        }
        onChange && onChange(visible);
      }
      function show() {
        setState(true);
      }
      function hide() {
        setState(false);
      }
      function handleKeyup(e) {
        // Char Code: 27  ESC
        if (e.which ===  27) {
          hide();
        }
      }
      setOptions(options);
      init();
      return {
        show: show,
        hide: hide,
        $el: $root
      };
    }
    $.fn.modal = modal;
  });
})();
</script><div class="modal modal--overflow page__search-modal d-print-none js-page-search-modal"></div></div>


<script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function scrollToAnchor(anchor, duration, callback) {
      var $root = this;
      $root.animate({ scrollTop: $(anchor).position().top }, duration, function() {
        window.history.replaceState(null, '', window.location.href.split('#')[0] + anchor);
        callback && callback();
      });
    }
    $.fn.scrollToAnchor = scrollToAnchor;
  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function affix(options) {
      var $root = this, $window = $(window), $scrollTarget, $scroll,
        offsetBottom = 0, scrollTarget = window, scroll = window.document, disabled = false, isOverallScroller = true,
        rootTop, rootLeft, rootHeight, scrollBottom, rootBottomTop,
        hasInit = false, curState;

      function setOptions(options) {
        var _options = options || {};
        _options.offsetBottom && (offsetBottom = _options.offsetBottom);
        _options.scrollTarget && (scrollTarget = _options.scrollTarget);
        _options.scroll && (scroll = _options.scroll);
        _options.disabled !== undefined && (disabled = _options.disabled);
        $scrollTarget = $(scrollTarget);
        isOverallScroller = window.isOverallScroller($scrollTarget[0]);
        $scroll = $(scroll);
      }
      function preCalc() {
        top();
        rootHeight = $root.outerHeight();
        rootTop = $root.offset().top + (isOverallScroller ? 0 :  $scrollTarget.scrollTop());
        rootLeft = $root.offset().left;
      }
      function calc(needPreCalc) {
        needPreCalc && preCalc();
        scrollBottom = $scroll.outerHeight() - offsetBottom - rootHeight;
        rootBottomTop = scrollBottom - rootTop;
      }
      function top() {
        if (curState !== 'top') {
          $root.removeClass('fixed').css({
            left: 0,
            top: 0
          });
          curState = 'top';
        }
      }
      function fixed() {
        if (curState !== 'fixed') {
          $root.addClass('fixed').css({
            left: rootLeft + 'px',
            top: 0
          });
          curState = 'fixed';
        }
      }
      function bottom() {
        if (curState !== 'bottom') {
          $root.removeClass('fixed').css({
            left: 0,
            top: rootBottomTop + 'px'
          });
          curState = 'bottom';
        }
      }
      function setState() {
        var scrollTop = $scrollTarget.scrollTop();
        if (scrollTop >= rootTop && scrollTop <= scrollBottom) {
          fixed();
        } else if (scrollTop < rootTop) {
          top();
        } else {
          bottom();
        }
      }
      function init() {
        if(!hasInit) {
          var interval, timeout;
          calc(true); setState();
          // run calc every 100 millisecond
          interval = setInterval(function() {
            calc();
          }, 100);
          timeout = setTimeout(function() {
            clearInterval(interval);
          }, 45000);
          window.pageLoad.then(function() {
            setTimeout(function() {
              clearInterval(interval);
              clearTimeout(timeout);
            }, 3000);
          });
          $scrollTarget.on('scroll', function() {
            disabled || setState();
          });
          $window.on('resize', function() {
            disabled || (calc(true), setState());
          });
          hasInit = true;
        }
      }

      setOptions(options);
      if (!disabled) {
        init();
      }
      $window.on('resize', window.throttle(function() {
        init();
      }, 200));
      return {
        setOptions: setOptions,
        refresh: function() {
          calc(true, { animation: false }); setState();
        }
      };
    }
    $.fn.affix = affix;
  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    function toc(options) {
      var $root = this, $window = $(window), $scrollTarget, $scroller, $tocUl = $('<ul class="toc toc--ellipsis"></ul>'), $tocLi, $headings, $activeLast, $activeCur,
        selectors = 'h1,h2,h3', container = 'body', scrollTarget = window, scroller = 'html, body', disabled = false,
        headingsPos, scrolling = false, hasRendered = false, hasInit = false;

      function setOptions(options) {
        var _options = options || {};
        _options.selectors && (selectors = _options.selectors);
        _options.container && (container = _options.container);
        _options.scrollTarget && (scrollTarget = _options.scrollTarget);
        _options.scroller && (scroller = _options.scroller);
        _options.disabled !== undefined && (disabled = _options.disabled);
        $headings = $(container).find(selectors).filter('[id]');
        $scrollTarget = $(scrollTarget);
        $scroller = $(scroller);
      }
      function calc() {
        headingsPos = [];
        $headings.each(function() {
          headingsPos.push(Math.floor($(this).position().top));
        });
      }
      function setState(element, disabled) {
        var scrollTop = $scrollTarget.scrollTop(), i;
        if (disabled || !headingsPos || headingsPos.length < 1) { return; }
        if (element) {
          $activeCur = element;
        } else {
          for (i = 0; i < headingsPos.length; i++) {
            if (scrollTop >= headingsPos[i]) {
              $activeCur = $tocLi.eq(i);
            } else {
              $activeCur || ($activeCur = $tocLi.eq(i));
              break;
            }
          }
        }
        $activeLast && $activeLast.removeClass('active');
        ($activeLast = $activeCur).addClass('active');
      }
      function render() {
        if(!hasRendered) {
          $root.append($tocUl);
          $headings.each(function() {
            var $this = $(this);
            $tocUl.append($('<li></li>').addClass('toc-' + $this.prop('tagName').toLowerCase())
              .append($('<a></a>').text($this.text()).attr('href', '#' + $this.prop('id'))));
          });
          $tocLi = $tocUl.children('li');
          $tocUl.on('click', 'a', function(e) {
            e.preventDefault();
            var $this = $(this);
            scrolling = true;
            setState($this.parent());
            $scroller.scrollToAnchor($this.attr('href'), 400, function() {
              scrolling = false;
            });
          });
        }
        hasRendered = true;
      }
      function init() {
        var interval, timeout;
        if(!hasInit) {
          render(); calc(); setState(null, scrolling);
          // run calc every 100 millisecond
          interval = setInterval(function() {
            calc();
          }, 100);
          timeout = setTimeout(function() {
            clearInterval(interval);
          }, 45000);
          window.pageLoad.then(function() {
            setTimeout(function() {
              clearInterval(interval);
              clearTimeout(timeout);
            }, 3000);
          });
          $scrollTarget.on('scroll', function() {
            disabled || setState(null, scrolling);
          });
          $window.on('resize', window.throttle(function() {
            if (!disabled) {
              render(); calc(); setState(null, scrolling);
            }
          }, 100));
        }
        hasInit = true;
      }

      setOptions(options);
      if (!disabled) {
        init();
      }
      $window.on('resize', window.throttle(function() {
        init();
      }, 200));
      return {
        setOptions: setOptions
      };
    }
    $.fn.toc = toc;
  });
})();
/*(function () {

})();*/
</script><script>(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;

  window.Lazyload.js(SOURCES.jquery, function() {
    var $pageMask = $('.js-page-mask');
    var $pageRoot = $('.js-page-root');
    var $sidebarShow = $('.js-sidebar-show');
    var $sidebarHide = $('.js-sidebar-hide');

    function freeze(e) {
      if (e.target === $pageMask[0]) {
        e.preventDefault();
      }
    }
    function stopBodyScrolling(bool) {
      if (bool === true) {
        window.addEventListener('touchmove', freeze, { passive: false });
      } else {
        window.removeEventListener('touchmove', freeze, { passive: false });
      }
    }

    $sidebarShow.on('click', function() {
      stopBodyScrolling(true); $pageRoot.addClass('show-sidebar');
    });
    $sidebarHide.on('click', function() {
      stopBodyScrolling(false); $pageRoot.removeClass('show-sidebar');
    });
  });
})();
</script><script>
  /* toc must before affix, since affix need to konw toc' height. */(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  var TOC_SELECTOR = window.TEXT_VARIABLES.site.toc.selectors;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $window = $(window);
    var $articleContent = $('.js-article-content');
    var $tocRoot = $('.js-toc-root'), $col2 = $('.js-col-aside');
    var toc;
    var tocDisabled = false;
    var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');
    var hasToc = $articleContent.find(TOC_SELECTOR).length > 0;

    function disabled() {
      return $col2.css('display') === 'none' || !hasToc;
    }

    tocDisabled = disabled();

    toc = $tocRoot.toc({
      selectors: TOC_SELECTOR,
      container: $articleContent,
      scrollTarget: hasSidebar ? '.js-page-main' : null,
      scroller: hasSidebar ? '.js-page-main' : null,
      disabled: tocDisabled
    });

    $window.on('resize', window.throttle(function() {
      tocDisabled = disabled();
      toc && toc.setOptions({
        disabled: tocDisabled
      });
    }, 100));

  });
})();
(function() {
  var SOURCES = window.TEXT_VARIABLES.sources;
  window.Lazyload.js(SOURCES.jquery, function() {
    var $window = $(window), $pageFooter = $('.js-page-footer');
    var $pageAside = $('.js-page-aside');
    var affix;
    var tocDisabled = false;
    var hasSidebar = $('.js-page-root').hasClass('layout--page--sidebar');

    affix = $pageAside.affix({
      offsetBottom: $pageFooter.outerHeight(),
      scrollTarget: hasSidebar ? '.js-page-main' : null,
      scroller: hasSidebar ? '.js-page-main' : null,
      scroll: hasSidebar ? $('.js-page-main').children() : null,
      disabled: tocDisabled
    });

    $window.on('resize', window.throttle(function() {
      affix && affix.setOptions({
        disabled: tocDisabled
      });
    }, 100));

    window.pageAsideAffix = affix;
  });
})();
</script><script>
  window.Lazyload.js(['https://cdn.bootcss.com/jquery/3.1.1/jquery.min.js', 'https://cdn.bootcss.com/Chart.js/2.7.2/Chart.bundle.min.js'], function() {
    var $canvas = null, $this = null, _ctx = null, _text = '';
    $('.language-chart').each(function(){
      $this = $(this);
      $canvas = $('<canvas></canvas>');
      _text = $this.text();
      $this.text('').append($canvas);
      _ctx = $canvas.get(0).getContext('2d');
      (_ctx && _text) && (new Chart(_ctx, JSON.parse(_text)) && $this.attr('data-processed', true));
    });
  });
</script><script type="text/x-mathjax-config">
	var _config = { tex2jax: {
		inlineMath: [['$','$'], ['\\(','\\)']]
	}};_config.TeX = { equationNumbers: { autoNumber: "all" } };MathJax.Hub.Config(_config);
</script>
<script type="text/javascript" src="https://cdn.bootcss.com/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script>
  window.Lazyload.js('https://cdn.bootcss.com/mermaid/8.0.0-rc.8/mermaid.min.js', function() {
    mermaid.initialize({
      startOnLoad: true
    });
    mermaid.init(undefined, '.language-mermaid');
  });
</script>
    </div>
    <script>(function () {
  var $root = document.getElementsByClassName('root')[0];
  if (window.hasEvent('touchstart')) {
    $root.dataset.isTouch = true;
    document.addEventListener('touchstart', function(){}, false);
  }
})();
</script>
  </body>
</html>