
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell &#8212; Spark NLP 4.0.0 documentation</title>
  <script>
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../../static/styles/theme.css?digest=9b1a4fa89bdd0e95b23b" rel="stylesheet">
<link href="../../../../../../static/styles/pydata-sphinx-theme.css?digest=9b1a4fa89bdd0e95b23b" rel="stylesheet">

  
  <link rel="stylesheet"
    href="../../../../../../static/vendor/fontawesome/6.1.2/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../../../static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../../../static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../../../../static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../../../../../static/css/custom.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../../static/scripts/pydata-sphinx-theme.js?digest=9b1a4fa89bdd0e95b23b">

    <script data-url_root="../../../../../../" id="documentation_options" src="../../../../../../static/documentation_options.js"></script>
    <script src="../../../../../../static/jquery.js"></script>
    <script src="../../../../../../static/underscore.js"></script>
    <script src="../../../../../../static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../../../../static/doctools.js"></script>
    <script src="../../../../../../static/sphinx_highlight.js"></script>
    <script src="../../../../../../static/toggleprompt.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'reference/autosummary/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell/index';</script>
    <link rel="shortcut icon" href="../../../../../../static/fav.ico"/>
    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="../../../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fas fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="../../../../../../index.html">

  
  
  
  
  
  
  

  
    <img src="../../../../../../static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="../../../../../../static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                <li class="nav-item">
                    <a class="nav-link" href="../../../../../../getting_started/index.html">
                        Getting Started
                    </a>
                </li>
                

                <li class="nav-item">
                    <a class="nav-link" href="../../../../../index.html">
                        API Reference
                    </a>
                </li>
                
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      <div class="navbar-end-item navbar-end__search-button-container">
        
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search">
  <i class="fas fa-search"></i>
</button>
      </div>
      
      <div class="navbar-end-item">
        <span class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle">
    <a class="theme-switch" data-mode="light"><i class="fas fa-sun"></i></a>
    <a class="theme-switch" data-mode="dark"><i class="far fa-moon"></i></a>
    <a class="theme-switch" data-mode="auto"><i class="fas fa-adjust"></i></a>
</span>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  <div class="search-button-container--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search">
  <i class="fas fa-search"></i>
</button>
  </div>

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fas fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                <li class="nav-item">
                    <a class="nav-link" href="../../../../../../getting_started/index.html">
                        Getting Started
                    </a>
                </li>
                

                <li class="nav-item">
                    <a class="nav-link" href="../../../../../index.html">
                        API Reference
                    </a>
                </li>
                
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <span class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle">
    <a class="theme-switch" data-mode="light"><i class="fas fa-sun"></i></a>
    <a class="theme-switch" data-mode="dark"><i class="far fa-moon"></i></a>
    <a class="theme-switch" data-mode="auto"><i class="fas fa-adjust"></i></a>
</span>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Section navigation">
  <p class="bd-links__title" role="heading" aria-level="1">
    Section Navigation
  </p>
  <div class="bd-toc-item navbar-nav">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../core_rnn_cell/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     sparknlp_jsl._tf_graph_builders.tf2contrib.core_rnn_cell
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../fused_rnn_cell/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     sparknlp_jsl._tf_graph_builders.tf2contrib.fused_rnn_cell
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../gru_ops/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     sparknlp_jsl._tf_graph_builders.tf2contrib.gru_ops
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../lstm_ops/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     sparknlp_jsl._tf_graph_builders.tf2contrib.lstm_ops
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../rnn/index.html">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     sparknlp_jsl._tf_graph_builders.tf2contrib.rnn
    </span>
   </code>
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   <code class="xref py py-mod docutils literal notranslate">
    <span class="pre">
     sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell
    </span>
   </code>
  </a>
 </li>
</ul>

  </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

      </div>
      <main class="bd-main">
        
        
        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                
            </div>
            
            
            <article class="bd-article" role="main">
              
  <section id="module-sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell">
<span id="sparknlp-jsl-tf-graph-builders-tf2contrib-rnn-cell"></span><h1><a class="reference internal" href="#module-sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell</span></code></a><a class="headerlink" href="#module-sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell" title="Permalink to this heading">#</a></h1>
<p>Module for constructing RNN Cells.</p>
<section id="module-contents">
<h2>Module Contents<a class="headerlink" href="#module-contents" title="Permalink to this heading">#</a></h2>
<section id="classes">
<h3>Classes<a class="headerlink" href="#classes" title="Permalink to this heading">#</a></h3>
<table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CoupledInputForgetGateLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CoupledInputForgetGateLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CoupledInputForgetGateLSTMCell</span></code></a></p></td>
<td><p>Long short-term memory unit (LSTM) recurrent network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.TimeFreqLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.TimeFreqLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TimeFreqLSTMCell</span></code></a></p></td>
<td><p>Time-Frequency Long short-term memory unit (LSTM) recurrent network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GridLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GridLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GridLSTMCell</span></code></a></p></td>
<td><p>Grid Long short-term memory unit (LSTM) recurrent network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.BidirectionalGridLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.BidirectionalGridLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BidirectionalGridLSTMCell</span></code></a></p></td>
<td><p>Bidirectional GridLstm cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.AttentionCellWrapper" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.AttentionCellWrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AttentionCellWrapper</span></code></a></p></td>
<td><p>Basic attention cell wrapper.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.HighwayWrapper" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.HighwayWrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">HighwayWrapper</span></code></a></p></td>
<td><p>RNNCell wrapper that adds highway connection on cell input and output.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormBasicLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormBasicLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LayerNormBasicLSTMCell</span></code></a></p></td>
<td><p>LSTM unit with layer normalization and recurrent dropout.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NASCell</span></code></a></p></td>
<td><p>Neural Architecture Search (NAS) recurrent network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.UGRNNCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.UGRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">UGRNNCell</span></code></a></p></td>
<td><p>Update Gate Recurrent Neural Network (UGRNN) cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IntersectionRNNCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IntersectionRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">IntersectionRNNCell</span></code></a></p></td>
<td><p>Intersection Recurrent Neural Network (+RNN) cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CompiledWrapper" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CompiledWrapper"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CompiledWrapper</span></code></a></p></td>
<td><p>Wraps step execution in an XLA JIT scope.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.PhasedLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.PhasedLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PhasedLSTMCell</span></code></a></p></td>
<td><p>Phased LSTM recurrent network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.ConvLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.ConvLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConvLSTMCell</span></code></a></p></td>
<td><p>Convolutional LSTM recurrent network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv1DLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv1DLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv1DLSTMCell</span></code></a></p></td>
<td><p>1D Convolutional LSTM recurrent network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv2DLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv2DLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv2DLSTMCell</span></code></a></p></td>
<td><p>2D Convolutional LSTM recurrent network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv3DLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv3DLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Conv3DLSTMCell</span></code></a></p></td>
<td><p>3D Convolutional LSTM recurrent network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GLSTMCell</span></code></a></p></td>
<td><p>Group LSTM cell (G-LSTM).</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LayerNormLSTMCell</span></code></a></p></td>
<td><p>Long short-term memory unit (LSTM) recurrent network cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SRUCell</span></code></a></p></td>
<td><p>SRU, Simple Recurrent Unit.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.WeightNormLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.WeightNormLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">WeightNormLSTMCell</span></code></a></p></td>
<td><p>Weight normalized LSTM Cell. Adapted from <cite>rnn_cell_impl.LSTMCell</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">IndRNNCell</span></code></a></p></td>
<td><p>Independently Recurrent Neural Network (IndRNN) cell</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">IndyGRUCell</span></code></a></p></td>
<td><p>Independently Gated Recurrent Unit cell.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">IndyLSTMCell</span></code></a></p></td>
<td><p>Basic IndyLSTM recurrent network cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NTMCell</span></code></a></p></td>
<td><p>Neural Turing Machine Cell with RNN controller.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MinimalRNNCell</span></code></a></p></td>
<td><p>MinimalRNN cell.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CFNCell</span></code></a></p></td>
<td><p>Chaos Free Network cell.</p></td>
</tr>
</tbody>
</table>
</section>
<section id="attributes">
<h3>Attributes<a class="headerlink" href="#attributes" title="Permalink to this heading">#</a></h3>
<table class="autosummary longtable table autosummary">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMControllerState" title="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMControllerState"><code class="xref py py-obj docutils literal notranslate"><span class="pre">NTMControllerState</span></code></a></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CoupledInputForgetGateLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">CoupledInputForgetGateLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_peepholes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_proj</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">proj_clip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_unit_shards</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_proj_shards</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_is_tuple</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">math_ops.tanh</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_gain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_shift</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#CoupledInputForgetGateLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CoupledInputForgetGateLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>Long short-term memory unit (LSTM) recurrent network cell.</p>
<p>The default non-peephole implementation is based on:</p>
<blockquote>
<div><p><a class="reference external" href="https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf">https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf</a></p>
</div></blockquote>
<p>Felix Gers, Jurgen Schmidhuber, and Fred Cummins.
“Learning to forget: Continual prediction with LSTM.” IET, 850-855, 1999.</p>
<p>The peephole implementation is based on:</p>
<blockquote>
<div><p><a class="reference external" href="https://research.google.com/pubs/archive/43905.pdf">https://research.google.com/pubs/archive/43905.pdf</a></p>
</div></blockquote>
<p>Hasim Sak, Andrew Senior, and Francoise Beaufays.
“Long short-term memory recurrent neural network architectures for</p>
<blockquote>
<div><p>large scale acoustic modeling.” INTERSPEECH, 2014.</p>
</div></blockquote>
<p>The coupling of input and forget gate is based on:</p>
<blockquote>
<div><p><a class="reference external" href="http://arxiv.org/pdf/1503.04069.pdf">http://arxiv.org/pdf/1503.04069.pdf</a></p>
</div></blockquote>
<p>Greff et al. “LSTM: A Search Space Odyssey”</p>
<p>The class uses optional peep-hole connections, and an optional projection
layer.
Layer normalization implementation is based on:</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a>.</p>
</div></blockquote>
<p>“Layer Normalization”
Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</p>
<p>and is applied before the internal nonlinearities.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CoupledInputForgetGateLSTMCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#CoupledInputForgetGateLSTMCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CoupledInputForgetGateLSTMCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Run one step of LSTM.</p>
<dl>
<dt>Args:</dt><dd><p>inputs: input Tensor, 2D, batch x num_units.
state: if <cite>state_is_tuple</cite> is False, this must be a state Tensor,</p>
<blockquote>
<div><p><cite>2-D, batch x state_size</cite>.  If <cite>state_is_tuple</cite> is True, this must be a
tuple of state Tensors, both <cite>2-D</cite>, with column sizes <cite>c_state</cite> and
<cite>m_state</cite>.</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>A tuple containing:
- A <cite>2-D, [batch x output_dim]</cite>, Tensor representing the output of the</p>
<blockquote>
<div><p>LSTM after reading <cite>inputs</cite> when previous state was <cite>state</cite>.
Here output_dim is:</p>
<blockquote>
<div><p>num_proj if num_proj was set,
num_units otherwise.</p>
</div></blockquote>
</div></blockquote>
<ul class="simple">
<li><p>Tensor(s) representing the new state of LSTM after reading <cite>inputs</cite> when
the previous state was <cite>state</cite>.  Same type and shape(s) as <cite>state</cite>.</p></li>
</ul>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: If input size cannot be inferred from inputs via</dt><dd><p>static shape inference.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.TimeFreqLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">TimeFreqLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_peepholes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cell_clip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_unit_shards</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frequency_skip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#TimeFreqLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.TimeFreqLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>Time-Frequency Long short-term memory unit (LSTM) recurrent network cell.</p>
<p>This implementation is based on:</p>
<blockquote>
<div><p>Tara N. Sainath and Bo Li
“Modeling Time-Frequency Patterns with LSTM vs. Convolutional Architectures
for LVCSR Tasks.” submitted to INTERSPEECH, 2016.</p>
</div></blockquote>
<p>It uses peep-hole connections and optional cell clipping.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.TimeFreqLSTMCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#TimeFreqLSTMCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.TimeFreqLSTMCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Run one step of LSTM.</p>
<dl>
<dt>Args:</dt><dd><p>inputs: input Tensor, 2D, batch x num_units.
state: state Tensor, 2D, batch x state_size.</p>
</dd>
<dt>Returns:</dt><dd><p>A tuple containing:
- A 2D, batch x output_dim, Tensor representing the output of the LSTM</p>
<blockquote>
<div><p>after reading “inputs” when previous state was “state”.
Here output_dim is num_units.</p>
</div></blockquote>
<ul class="simple">
<li><p>A 2D, batch x state_size, Tensor representing the new state of LSTM
after reading “inputs” when previous state was “state”.</p></li>
</ul>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: if an input_size was specified and the provided inputs have</dt><dd><p>a different dimension.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GridLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">GridLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_peepholes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">share_time_frequency_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cell_clip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_unit_shards</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frequency_skip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_frequency_blocks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_freqindex_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_freqindex_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">couple_input_forget_gates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_is_tuple</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#GridLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GridLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>Grid Long short-term memory unit (LSTM) recurrent network cell.</p>
<dl class="simple">
<dt>The default is based on:</dt><dd><p>Nal Kalchbrenner, Ivo Danihelka and Alex Graves
“Grid Long Short-Term Memory,” Proc. ICLR 2016.
<a class="reference external" href="http://arxiv.org/abs/1507.01526">http://arxiv.org/abs/1507.01526</a></p>
</dd>
<dt>When peephole connections are used, the implementation is based on:</dt><dd><p>Tara N. Sainath and Bo Li
“Modeling Time-Frequency Patterns with LSTM vs. Convolutional Architectures
for LVCSR Tasks.” submitted to INTERSPEECH, 2016.</p>
</dd>
</dl>
<p>The code uses optional peephole connections, shared_weights and cell clipping.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GridLSTMCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#GridLSTMCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GridLSTMCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Run one step of LSTM.</p>
<dl>
<dt>Args:</dt><dd><p>inputs: input Tensor, 2D, [batch, feature_size].
state: Tensor or tuple of Tensors, 2D, [batch, state_size], depends on the</p>
<blockquote>
<div><p>flag self._state_is_tuple.</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>A tuple containing:
- A 2D, [batch, output_dim], Tensor representing the output of the LSTM</p>
<blockquote>
<div><p>after reading “inputs” when previous state was “state”.
Here output_dim is num_units.</p>
</div></blockquote>
<ul class="simple">
<li><p>A 2D, [batch, state_size], Tensor representing the new state of LSTM
after reading “inputs” when previous state was “state”.</p></li>
</ul>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: if an input_size was specified and the provided inputs have</dt><dd><p>a different dimension.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.BidirectionalGridLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">BidirectionalGridLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_peepholes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">share_time_frequency_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cell_clip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_unit_shards</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frequency_skip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_frequency_blocks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">start_freqindex_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">end_freqindex_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">couple_input_forget_gates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">backward_slice_offset</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#BidirectionalGridLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.BidirectionalGridLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>Bidirectional GridLstm cell.</p>
<p>The bidirection connection is only used in the frequency direction, which
hence doesn’t affect the time direction’s real-time processing that is
required for online recognition systems.
The current implementation uses different weights for the two directions.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.BidirectionalGridLSTMCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#BidirectionalGridLSTMCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.BidirectionalGridLSTMCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Run one step of LSTM.</p>
<dl>
<dt>Args:</dt><dd><p>inputs: input Tensor, 2D, [batch, num_units].
state: tuple of Tensors, 2D, [batch, state_size].</p>
</dd>
<dt>Returns:</dt><dd><p>A tuple containing:
- A 2D, [batch, output_dim], Tensor representing the output of the LSTM</p>
<blockquote>
<div><p>after reading “inputs” when previous state was “state”.
Here output_dim is num_units.</p>
</div></blockquote>
<ul class="simple">
<li><p>A 2D, [batch, state_size], Tensor representing the new state of LSTM
after reading “inputs” when previous state was “state”.</p></li>
</ul>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: if an input_size was specified and the provided inputs have</dt><dd><p>a different dimension.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.AttentionCellWrapper">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">AttentionCellWrapper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cell</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_length</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">attn_vec_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_is_tuple</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#AttentionCellWrapper"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.AttentionCellWrapper" title="Permalink to this definition">#</a></dt>
<dd><p>Basic attention cell wrapper.</p>
<p>Implementation based on <a class="reference external" href="https://arxiv.org/abs/1601.06733">https://arxiv.org/abs/1601.06733</a>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.AttentionCellWrapper.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#AttentionCellWrapper.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.AttentionCellWrapper.call" title="Permalink to this definition">#</a></dt>
<dd><p>Long short-term memory cell with attention (LSTMA).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.HighwayWrapper">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">HighwayWrapper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cell</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">couple_carry_transform_gates</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">carry_bias_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#HighwayWrapper"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.HighwayWrapper" title="Permalink to this definition">#</a></dt>
<dd><p>RNNCell wrapper that adds highway connection on cell input and output.</p>
<dl class="simple">
<dt>Based on:</dt><dd><p>R. K. Srivastava, K. Greff, and J. Schmidhuber, “Highway networks”,
arXiv preprint arXiv:1505.00387, 2015.
https://arxiv.org/abs/1505.00387</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.HighwayWrapper.zero_state">
<span class="sig-name descname"><span class="pre">zero_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#HighwayWrapper.zero_state"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.HighwayWrapper.zero_state" title="Permalink to this definition">#</a></dt>
<dd><p>Return zero-filled state tensor(s).</p>
<dl>
<dt>Args:</dt><dd><p>batch_size: int, float, or unit Tensor representing the batch size.
dtype: the data type to use for the state.</p>
</dd>
<dt>Returns:</dt><dd><p>If <cite>state_size</cite> is an int or TensorShape, then the return value is a
<cite>N-D</cite> tensor of shape <cite>[batch_size, state_size]</cite> filled with zeros.</p>
<p>If <cite>state_size</cite> is a nested list or tuple, then the return value is
a nested list or tuple (of the same structure) of <cite>2-D</cite> tensors with
the shapes <cite>[batch_size, s]</cite> for each s in <cite>state_size</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormBasicLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">LayerNormBasicLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">math_ops.tanh</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_gain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_shift</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_keep_prob</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dropout_prob_seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#LayerNormBasicLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormBasicLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>LSTM unit with layer normalization and recurrent dropout.</p>
<p>This class adds layer normalization and recurrent dropout to a
basic LSTM unit. Layer normalization implementation is based on:</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a>.</p>
</div></blockquote>
<p>“Layer Normalization”
Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</p>
<p>and is applied before the internal nonlinearities.
Recurrent dropout is base on:</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/1603.05118">https://arxiv.org/abs/1603.05118</a></p>
</div></blockquote>
<p>“Recurrent Dropout without Memory Loss”
Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormBasicLSTMCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#LayerNormBasicLSTMCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormBasicLSTMCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>LSTM cell with layer normalization and recurrent dropout.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">NASCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_proj</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#NASCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell" title="Permalink to this definition">#</a></dt>
<dd><p>Neural Architecture Search (NAS) recurrent network cell.</p>
<p>This implements the recurrent cell from the paper:</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/1611.01578">https://arxiv.org/abs/1611.01578</a></p>
</div></blockquote>
<p>Barret Zoph and Quoc V. Le.
“Neural Architecture Search with Reinforcement Learning” Proc. ICLR 2017.</p>
<p>The class uses an optional projection layer.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#NASCell.build"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell.build" title="Permalink to this definition">#</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#NASCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Run one step of NAS Cell.</p>
<dl>
<dt>Args:</dt><dd><p>inputs: input Tensor, 2D, batch x num_units.
state: This must be a tuple of state Tensors, both <cite>2-D</cite>, with column</p>
<blockquote>
<div><p>sizes <cite>c_state</cite> and <cite>m_state</cite>.</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>A tuple containing:
- A <cite>2-D, [batch x output_dim]</cite>, Tensor representing the output of the</p>
<blockquote>
<div><p>NAS Cell after reading <cite>inputs</cite> when previous state was <cite>state</cite>.
Here output_dim is:</p>
<blockquote>
<div><p>num_proj if num_proj was set,
num_units otherwise.</p>
</div></blockquote>
</div></blockquote>
<ul class="simple">
<li><p>Tensor(s) representing the new state of NAS Cell after reading <cite>inputs</cite>
when the previous state was <cite>state</cite>.  Same type and shape(s) as <cite>state</cite>.</p></li>
</ul>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: If input size cannot be inferred from inputs via</dt><dd><p>static shape inference.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.UGRNNCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">UGRNNCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">math_ops.tanh</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#UGRNNCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.UGRNNCell" title="Permalink to this definition">#</a></dt>
<dd><p>Update Gate Recurrent Neural Network (UGRNN) cell.</p>
<p>Compromise between a LSTM/GRU and a vanilla RNN.  There is only one
gate, and that is to determine whether the unit should be
integrating or computing instantaneously.  This is the recurrent
idea of the feedforward Highway Network.</p>
<p>This implements the recurrent cell from the paper:</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/1611.09913">https://arxiv.org/abs/1611.09913</a></p>
</div></blockquote>
<p>Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo.
“Capacity and Trainability in Recurrent Neural Networks” Proc. ICLR 2017.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.UGRNNCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#UGRNNCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.UGRNNCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Run one step of UGRNN.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>inputs: input Tensor, 2D, batch x input size.
state: state Tensor, 2D, batch x num units.</p>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt>new_output: batch x num units, Tensor representing the output of the UGRNN</dt><dd><p>after reading <cite>inputs</cite> when previous state was <cite>state</cite>. Identical to
<cite>new_state</cite>.</p>
</dd>
<dt>new_state: batch x num units, Tensor representing the state of the UGRNN</dt><dd><p>after reading <cite>inputs</cite> when previous state was <cite>state</cite>.</p>
</dd>
</dl>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: If input size cannot be inferred from inputs via</dt><dd><p>static shape inference.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IntersectionRNNCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">IntersectionRNNCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_in_proj</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">nn_ops.relu</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#IntersectionRNNCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IntersectionRNNCell" title="Permalink to this definition">#</a></dt>
<dd><p>Intersection Recurrent Neural Network (+RNN) cell.</p>
<p>Architecture with coupled recurrent gate as well as coupled depth
gate, designed to improve information flow through stacked RNNs. As the
architecture uses depth gating, the dimensionality of the depth
output (y) also should not change through depth (input size == output size).
To achieve this, the first layer of a stacked Intersection RNN projects
the inputs to N (num units) dimensions. Therefore when initializing an
IntersectionRNNCell, one should set <cite>num_in_proj = N</cite> for the first layer
and use default settings for subsequent layers.</p>
<p>This implements the recurrent cell from the paper:</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/1611.09913">https://arxiv.org/abs/1611.09913</a></p>
</div></blockquote>
<p>Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo.
“Capacity and Trainability in Recurrent Neural Networks” Proc. ICLR 2017.</p>
<p>The Intersection RNN is built for use in deeply stacked
RNNs so it may not achieve best performance with depth 1.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IntersectionRNNCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#IntersectionRNNCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IntersectionRNNCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Run one step of the Intersection RNN.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>inputs: input Tensor, 2D, batch x input size.
state: state Tensor, 2D, batch x num units.</p>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt>new_y: batch x num units, Tensor representing the output of the +RNN</dt><dd><p>after reading <cite>inputs</cite> when previous state was <cite>state</cite>.</p>
</dd>
<dt>new_state: batch x num units, Tensor representing the state of the +RNN</dt><dd><p>after reading <cite>inputs</cite> when previous state was <cite>state</cite>.</p>
</dd>
</dl>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: If input size cannot be inferred from <cite>inputs</cite> via</dt><dd><p>static shape inference.</p>
</dd>
<dt>ValueError: If input size != output size (these must be equal when</dt><dd><p>using the Intersection RNN).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CompiledWrapper">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">CompiledWrapper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cell</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compile_stateful</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#CompiledWrapper"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CompiledWrapper" title="Permalink to this definition">#</a></dt>
<dd><p>Wraps step execution in an XLA JIT scope.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CompiledWrapper.zero_state">
<span class="sig-name descname"><span class="pre">zero_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#CompiledWrapper.zero_state"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CompiledWrapper.zero_state" title="Permalink to this definition">#</a></dt>
<dd><p>Return zero-filled state tensor(s).</p>
<dl>
<dt>Args:</dt><dd><p>batch_size: int, float, or unit Tensor representing the batch size.
dtype: the data type to use for the state.</p>
</dd>
<dt>Returns:</dt><dd><p>If <cite>state_size</cite> is an int or TensorShape, then the return value is a
<cite>N-D</cite> tensor of shape <cite>[batch_size, state_size]</cite> filled with zeros.</p>
<p>If <cite>state_size</cite> is a nested list or tuple, then the return value is
a nested list or tuple (of the same structure) of <cite>2-D</cite> tensors with
the shapes <cite>[batch_size, s]</cite> for each s in <cite>state_size</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.PhasedLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">PhasedLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_peepholes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leak</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ratio_on</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainable_ratio_on</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">period_init_min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">period_init_max</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#PhasedLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.PhasedLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>Phased LSTM recurrent network cell.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1610.09513v1.pdf">https://arxiv.org/pdf/1610.09513v1.pdf</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.PhasedLSTMCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#PhasedLSTMCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.PhasedLSTMCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Phased LSTM Cell.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>inputs: A tuple of 2 Tensor.</dt><dd><p>The first Tensor has shape [batch, 1], and type float32 or float64.
It stores the time.
The second Tensor has shape [batch, features_size], and type float32.
It stores the features.</p>
</dd>
</dl>
<p>state: rnn_cell_impl.LSTMStateTuple, state from previous timestep.</p>
</dd>
<dt>Returns:</dt><dd><p>A tuple containing:
- A Tensor of float32, and shape [batch_size, num_units], representing the</p>
<blockquote>
<div><p>output of the cell.</p>
</div></blockquote>
<ul class="simple">
<li><p>A rnn_cell_impl.LSTMStateTuple, containing 2 Tensors of float32, shape
[batch_size, num_units], representing the new state and the output.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.ConvLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ConvLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">conv_ndims</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_channels</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_shape</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skip_connection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'conv_lstm_cell'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#ConvLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.ConvLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>Convolutional LSTM recurrent network cell.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1506.04214v1.pdf">https://arxiv.org/pdf/1506.04214v1.pdf</a></p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.ConvLSTMCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scope</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#ConvLSTMCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.ConvLSTMCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id1"><span class="problematic" id="id2">**</span></a>kwargs: Additional keyword arguments.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv1DLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Conv1DLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'conv_1d_lstm_cell'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#Conv1DLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv1DLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>1D Convolutional LSTM recurrent network cell.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1506.04214v1.pdf">https://arxiv.org/pdf/1506.04214v1.pdf</a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv2DLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Conv2DLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'conv_2d_lstm_cell'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#Conv2DLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv2DLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>2D Convolutional LSTM recurrent network cell.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1506.04214v1.pdf">https://arxiv.org/pdf/1506.04214v1.pdf</a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv3DLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Conv3DLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'conv_3d_lstm_cell'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#Conv3DLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv3DLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>3D Convolutional LSTM recurrent network cell.</p>
<p><a class="reference external" href="https://arxiv.org/pdf/1506.04214v1.pdf">https://arxiv.org/pdf/1506.04214v1.pdf</a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">GLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_proj</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">number_of_groups</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">math_ops.tanh</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#GLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>Group LSTM cell (G-LSTM).</p>
<p>The implementation is based on:</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/1703.10722">https://arxiv.org/abs/1703.10722</a></p>
</div></blockquote>
<p>O. Kuchaiev and B. Ginsburg
“Factorization Tricks for LSTM Networks”, ICLR 2017 workshop.</p>
<p>In brief, a G-LSTM cell consists of one LSTM sub-cell per group, where each
sub-cell operates on an evenly-sized sub-vector of the input and produces an
evenly-sized sub-vector of the output.  For example, a G-LSTM cell with 128
units and 4 groups consists of 4 LSTMs sub-cells with 32 units each.  If that
G-LSTM cell is fed a 200-dim input, then each sub-cell receives a 50-dim part
of the input and produces a 32-dim part of the output.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GLSTMCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#GLSTMCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GLSTMCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Run one step of G-LSTM.</p>
<dl>
<dt>Args:</dt><dd><dl class="simple">
<dt>inputs: input Tensor, 2D, [batch x num_inputs].  num_inputs must be</dt><dd><p>statically-known and evenly divisible into groups.  The innermost
vectors of the inputs are split into evenly-sized sub-vectors and fed
into the per-group LSTM sub-cells.</p>
</dd>
<dt>state: this must be a tuple of state Tensors, both <cite>2-D</cite>, with column</dt><dd><p>sizes <cite>c_state</cite> and <cite>m_state</cite>.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A tuple containing:</p>
<ul>
<li><p>A <cite>2-D, [batch x output_dim]</cite>, Tensor representing the output of the
G-LSTM after reading <cite>inputs</cite> when previous state was <cite>state</cite>.
Here output_dim is:</p>
<blockquote>
<div><p>num_proj if num_proj was set,
num_units otherwise.</p>
</div></blockquote>
</li>
<li><p>LSTMStateTuple representing the new state of G-LSTM cell
after reading <cite>inputs</cite> when the previous state was <cite>state</cite>.</p></li>
</ul>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: If input size cannot be inferred from inputs via</dt><dd><p>static shape inference, or if the input shape is incompatible
with the number of groups.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">LayerNormLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_peepholes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cell_clip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_proj</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">proj_clip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_gain</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm_shift</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#LayerNormLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>Long short-term memory unit (LSTM) recurrent network cell.</p>
<p>The default non-peephole implementation is based on:</p>
<blockquote>
<div><p><a class="reference external" href="https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf">https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf</a></p>
</div></blockquote>
<p>Felix Gers, Jurgen Schmidhuber, and Fred Cummins.
“Learning to forget: Continual prediction with LSTM.” IET, 850-855, 1999.</p>
<p>The peephole implementation is based on:</p>
<blockquote>
<div><p><a class="reference external" href="https://research.google.com/pubs/archive/43905.pdf">https://research.google.com/pubs/archive/43905.pdf</a></p>
</div></blockquote>
<p>Hasim Sak, Andrew Senior, and Francoise Beaufays.
“Long short-term memory recurrent neural network architectures for</p>
<blockquote>
<div><p>large scale acoustic modeling.” INTERSPEECH, 2014.</p>
</div></blockquote>
<p>The class uses optional peep-hole connections, optional cell clipping, and
an optional projection layer.</p>
<p>Layer normalization implementation is based on:</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/abs/1607.06450">https://arxiv.org/abs/1607.06450</a>.</p>
</div></blockquote>
<p>“Layer Normalization”
Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton</p>
<p>and is applied before the internal nonlinearities.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormLSTMCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#LayerNormLSTMCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormLSTMCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Run one step of LSTM.</p>
<dl>
<dt>Args:</dt><dd><p>inputs: input Tensor, 2D, batch x num_units.
state: this must be a tuple of state Tensors,</p>
<blockquote>
<div><dl class="simple">
<dt>both <cite>2-D</cite>, with column sizes <cite>c_state</cite> and</dt><dd><p><cite>m_state</cite>.</p>
</dd>
</dl>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>A tuple containing:</p>
<ul>
<li><p>A <cite>2-D, [batch x output_dim]</cite>, Tensor representing the output of the
LSTM after reading <cite>inputs</cite> when previous state was <cite>state</cite>.
Here output_dim is:</p>
<blockquote>
<div><p>num_proj if num_proj was set,
num_units otherwise.</p>
</div></blockquote>
</li>
<li><p>Tensor(s) representing the new state of LSTM after reading <cite>inputs</cite> when
the previous state was <cite>state</cite>.  Same type and shape(s) as <cite>state</cite>.</p></li>
</ul>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: If input size cannot be inferred from inputs via</dt><dd><p>static shape inference.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">SRUCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#SRUCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell" title="Permalink to this definition">#</a></dt>
<dd><p>SRU, Simple Recurrent Unit.</p>
<blockquote>
<div><p>Implementation based on
Training RNNs as Fast as CNNs (cf. <a class="reference external" href="https://arxiv.org/abs/1709.02755">https://arxiv.org/abs/1709.02755</a>).</p>
<p>This variation of RNN cell is characterized by the simplified data
dependence
between hidden states of two consecutive time steps. Traditionally, hidden
states from a cell at time step t-1 needs to be multiplied with a matrix
W_hh before being fed into the ensuing cell at time step t.
This flavor of RNN replaces the matrix multiplication between h_{t-1}
and W_hh with a pointwise multiplication, resulting in performance
gain.</p>
</div></blockquote>
<dl>
<dt>Args:</dt><dd><p>num_units: int, The number of units in the SRU cell.
activation: Nonlinearity to use.  Default: <cite>tanh</cite>.
reuse: (optional) Python boolean describing whether to reuse variables</p>
<blockquote>
<div><p>in an existing scope.  If not <cite>True</cite>, and the existing scope already has
the given variables, an error is raised.</p>
</div></blockquote>
<dl class="simple">
<dt>name: (optional) String, the name of the layer. Layers with the same name</dt><dd><p>will share weights, but to avoid mistakes we require reuse=True in such
cases.</p>
</dd>
</dl>
<p><a href="#id3"><span class="problematic" id="id4">**</span></a>kwargs: Additional keyword arguments.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#SRUCell.build"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell.build" title="Permalink to this definition">#</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#SRUCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Simple recurrent unit (SRU) with num_units cells.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.WeightNormLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">WeightNormLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_peepholes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cell_clip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_proj</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">proj_clip</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#WeightNormLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.WeightNormLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>Weight normalized LSTM Cell. Adapted from <cite>rnn_cell_impl.LSTMCell</cite>.</p>
<p>The weight-norm implementation is based on:
<a class="reference external" href="https://arxiv.org/abs/1602.07868">https://arxiv.org/abs/1602.07868</a>
Tim Salimans, Diederik P. Kingma.
Weight Normalization: A Simple Reparameterization to Accelerate
Training of Deep Neural Networks</p>
<p>The default LSTM implementation based on:</p>
<blockquote>
<div><p><a class="reference external" href="https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf">https://pdfs.semanticscholar.org/1154/0131eae85b2e11d53df7f1360eeb6476e7f4.pdf</a></p>
</div></blockquote>
<p>Felix Gers, Jurgen Schmidhuber, and Fred Cummins.
“Learning to forget: Continual prediction with LSTM.” IET, 850-855, 1999.</p>
<p>The class uses optional peephole connections, optional cell clipping
and an optional projection layer.</p>
<p>The optional peephole implementation is based on:
<a class="reference external" href="https://research.google.com/pubs/archive/43905.pdf">https://research.google.com/pubs/archive/43905.pdf</a>
Hasim Sak, Andrew Senior, and Francoise Beaufays.
“Long short-term memory recurrent neural network architectures for
large scale acoustic modeling.” INTERSPEECH, 2014.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.WeightNormLSTMCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#WeightNormLSTMCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.WeightNormLSTMCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Run one step of LSTM.</p>
<dl>
<dt>Args:</dt><dd><p>inputs: input Tensor, 2D, batch x num_units.
state: A tuple of state Tensors, both <cite>2-D</cite>, with column sizes</p>
<blockquote>
<div><p><cite>c_state</cite> and <cite>m_state</cite>.</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><p>A tuple containing:</p>
<ul>
<li><p>A <cite>2-D, [batch x output_dim]</cite>, Tensor representing the output of the
LSTM after reading <cite>inputs</cite> when previous state was <cite>state</cite>.
Here output_dim is:</p>
<blockquote>
<div><p>num_proj if num_proj was set,
num_units otherwise.</p>
</div></blockquote>
</li>
<li><p>Tensor(s) representing the new state of LSTM after reading <cite>inputs</cite> when
the previous state was <cite>state</cite>.  Same type and shape(s) as <cite>state</cite>.</p></li>
</ul>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: If input size cannot be inferred from inputs via</dt><dd><p>static shape inference.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">IndRNNCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#IndRNNCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell" title="Permalink to this definition">#</a></dt>
<dd><dl>
<dt>Independently Recurrent Neural Network (IndRNN) cell</dt><dd><p>(cf. <a class="reference external" href="https://arxiv.org/abs/1803.04831">https://arxiv.org/abs/1803.04831</a>).</p>
</dd>
<dt>Args:</dt><dd><p>num_units: int, The number of units in the RNN cell.
activation: Nonlinearity to use.  Default: <cite>tanh</cite>.
reuse: (optional) Python boolean describing whether to reuse variables</p>
<blockquote>
<div><p>in an existing scope.  If not <cite>True</cite>, and the existing scope already has
the given variables, an error is raised.</p>
</div></blockquote>
<dl class="simple">
<dt>name: String, the name of the layer. Layers with the same name will</dt><dd><p>share weights, but to avoid mistakes we require reuse=True in such
cases.</p>
</dd>
<dt>dtype: Default dtype of the layer (default of <cite>None</cite> means use the type</dt><dd><p>of the first input). Required when <cite>build</cite> is called before <cite>call</cite>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#IndRNNCell.build"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell.build" title="Permalink to this definition">#</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#IndRNNCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>IndRNN: output = new_state = act(W * input + u * state + B).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">IndyGRUCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#IndyGRUCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell" title="Permalink to this definition">#</a></dt>
<dd><p>Independently Gated Recurrent Unit cell.</p>
<p>Based on IndRNNs (<a class="reference external" href="https://arxiv.org/abs/1803.04831">https://arxiv.org/abs/1803.04831</a>) and similar to GRUCell,
yet with the \(U_r\), \(U_z\), and \(U\) matrices in equations 5, 6, and
8 of <a class="reference external" href="http://arxiv.org/abs/1406.1078">http://arxiv.org/abs/1406.1078</a> respectively replaced by diagonal
matrices, i.e. a Hadamard product with a single vector:</p>
<blockquote>
<div><dl class="simple">
<dt>$$r_j = sigmaleft([mathbf W_rmathbf x]_j +</dt><dd><p>[mathbf u_rcirc mathbf h_{(t-1)}]_jright)$$</p>
</dd>
<dt>$$z_j = sigmaleft([mathbf W_zmathbf x]_j +</dt><dd><p>[mathbf u_zcirc mathbf h_{(t-1)}]_jright)$$</p>
</dd>
<dt>$$tilde{h}^{(t)}_j = phileft([mathbf W mathbf x]_j +</dt><dd><p>[mathbf u circ mathbf r circ mathbf h_{(t-1)}]_jright)$$</p>
</dd>
</dl>
</div></blockquote>
<p>where \(circ\) denotes the Hadamard operator. This means that each IndyGRU
node sees only its own state, as opposed to seeing all states in the same
layer.</p>
<dl>
<dt>Args:</dt><dd><p>num_units: int, The number of units in the GRU cell.
activation: Nonlinearity to use.  Default: <cite>tanh</cite>.
reuse: (optional) Python boolean describing whether to reuse variables</p>
<blockquote>
<div><p>in an existing scope.  If not <cite>True</cite>, and the existing scope already has
the given variables, an error is raised.</p>
</div></blockquote>
<dl class="simple">
<dt>kernel_initializer: (optional) The initializer to use for the weight</dt><dd><p>matrices applied to the input.</p>
</dd>
</dl>
<p>bias_initializer: (optional) The initializer to use for the bias.
name: String, the name of the layer. Layers with the same name will</p>
<blockquote>
<div><p>share weights, but to avoid mistakes we require reuse=True in such
cases.</p>
</div></blockquote>
<dl class="simple">
<dt>dtype: Default dtype of the layer (default of <cite>None</cite> means use the type</dt><dd><p>of the first input). Required when <cite>build</cite> is called before <cite>call</cite>.</p>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#IndyGRUCell.build"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell.build" title="Permalink to this definition">#</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#IndyGRUCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Recurrently independent Gated Recurrent Unit (GRU) with nunits cells.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">IndyLSTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forget_bias</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reuse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#IndyLSTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>Basic IndyLSTM recurrent network cell.</p>
<p>Based on IndRNNs (<a class="reference external" href="https://arxiv.org/abs/1803.04831">https://arxiv.org/abs/1803.04831</a>) and similar to
BasicLSTMCell, yet with the \(U_f\), \(U_i\), \(U_o\) and \(U_c\)
matrices in the regular LSTM equations replaced by diagonal matrices, i.e. a
Hadamard product with a single vector:</p>
<blockquote>
<div><p>$$f_t = sigma_gleft(W_f x_t + u_f circ h_{t-1} + b_fright)$$
$$i_t = sigma_gleft(W_i x_t + u_i circ h_{t-1} + b_iright)$$
$$o_t = sigma_gleft(W_o x_t + u_o circ h_{t-1} + b_oright)$$
$$c_t = f_t circ c_{t-1} +</p>
<blockquote>
<div><p>i_t circ sigma_cleft(W_c x_t + u_c circ h_{t-1} + b_cright)$$</p>
</div></blockquote>
</div></blockquote>
<p>where \(circ\) denotes the Hadamard operator. This means that each IndyLSTM
node sees only its own state \(h\) and \(c\), as opposed to seeing all
states in the same layer.</p>
<p>We add forget_bias (default: 1) to the biases of the forget gate in order to
reduce the scale of forgetting in the beginning of the training.</p>
<p>It does not allow cell clipping, a projection layer, and does not
use peep-hole connections: it is the basic baseline.</p>
<p>For a detailed analysis of IndyLSTMs, see <a class="reference external" href="https://arxiv.org/abs/1903.08023">https://arxiv.org/abs/1903.08023</a>.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#IndyLSTMCell.build"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell.build" title="Permalink to this definition">#</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#IndyLSTMCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Independent Long short-term memory cell (IndyLSTM).</p>
<dl>
<dt>Args:</dt><dd><p>inputs: <cite>2-D</cite> tensor with shape <cite>[batch_size, input_size]</cite>.
state: An <cite>LSTMStateTuple</cite> of state tensors, each shaped</p>
<blockquote>
<div><p><cite>[batch_size, num_units]</cite>.</p>
</div></blockquote>
</dd>
<dt>Returns:</dt><dd><dl class="simple">
<dt>A pair containing the new hidden state, and the new state (a</dt><dd><p><cite>LSTMStateTuple</cite>).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py data">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMControllerState">
<span class="sig-name descname"><span class="pre">NTMControllerState</span></span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#NTMControllerState"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMControllerState" title="Permalink to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">NTMCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">controller</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">memory_vector_dim</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">read_head_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">write_head_num</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shift_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">clip_value</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">dtypes.float32</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#NTMCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell" title="Permalink to this definition">#</a></dt>
<dd><p>Neural Turing Machine Cell with RNN controller.</p>
<p>Implementation based on:
<a class="reference external" href="https://arxiv.org/abs/1807.08518">https://arxiv.org/abs/1807.08518</a>
Mark Collier, Joeran Beel</p>
<p>which is in turn based on the source code of:
<a class="reference external" href="https://github.com/snowkylin/ntm">https://github.com/snowkylin/ntm</a></p>
<p>and of course the original NTM paper:
Neural Turing Machines
<a class="reference external" href="https://arxiv.org/abs/1410.5401">https://arxiv.org/abs/1410.5401</a>
A Graves, G Wayne, I Danihelka</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#NTMCell.build"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell.build" title="Permalink to this definition">#</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prev_state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#NTMCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id5"><span class="problematic" id="id6">**</span></a>kwargs: Additional keyword arguments.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell.zero_state">
<span class="sig-name descname"><span class="pre">zero_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#NTMCell.zero_state"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell.zero_state" title="Permalink to this definition">#</a></dt>
<dd><p>Return zero-filled state tensor(s).</p>
<dl>
<dt>Args:</dt><dd><p>batch_size: int, float, or unit Tensor representing the batch size.
dtype: the data type to use for the state.</p>
</dd>
<dt>Returns:</dt><dd><p>If <cite>state_size</cite> is an int or TensorShape, then the return value is a
<cite>N-D</cite> tensor of shape <cite>[batch_size, state_size]</cite> filled with zeros.</p>
<p>If <cite>state_size</cite> is a nested list or tuple, then the return value is
a nested list or tuple (of the same structure) of <cite>2-D</cite> tensors with
the shapes <cite>[batch_size, s]</cite> for each s in <cite>state_size</cite>.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">MinimalRNNCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tanh'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'glorot_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ones'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#MinimalRNNCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell" title="Permalink to this definition">#</a></dt>
<dd><p>MinimalRNN cell.</p>
<p>The implementation is based on:</p>
<blockquote>
<div><p><a class="reference external" href="https://arxiv.org/pdf/1806.05394v2.pdf">https://arxiv.org/pdf/1806.05394v2.pdf</a></p>
</div></blockquote>
<p>Minmin Chen, Jeffrey Pennington, Samuel S. Schoenholz.
“Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal</p>
<blockquote>
<div><p>Propagation in Recurrent Neural Networks.” ICML, 2018.</p>
</div></blockquote>
<p>A MinimalRNN cell first projects the input to the hidden space. The new
hidden state is then calculated as a weighted sum of the projected input and
the previous hidden state, using a single update gate.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#MinimalRNNCell.build"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell.build" title="Permalink to this definition">#</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#MinimalRNNCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Run one step of MinimalRNN.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>inputs: input Tensor, must be 2-D, <cite>[batch, input_size]</cite>.
state: state Tensor, must be 2-D, <cite>[batch, state_size]</cite>.</p>
</dd>
<dt>Returns:</dt><dd><p>A tuple containing:</p>
<ul class="simple">
<li><p>Output: A <cite>2-D</cite> tensor with shape <cite>[batch_size, state_size]</cite>.</p></li>
<li><p>New state: A <cite>2-D</cite> tensor with shape <cite>[batch_size, state_size]</cite>.</p></li>
</ul>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: If input size cannot be inferred from inputs via</dt><dd><p>static shape inference.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">CFNCell</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">units</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'tanh'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">kernel_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'glorot_uniform'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bias_initializer</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'ones'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#CFNCell"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell" title="Permalink to this definition">#</a></dt>
<dd><p>Chaos Free Network cell.</p>
<p>The implementation is based on:</p>
<blockquote>
<div><p><a class="reference external" href="https://openreview.net/pdf?id=S1dIzvclg">https://openreview.net/pdf?id=S1dIzvclg</a></p>
</div></blockquote>
<p>Thomas Laurent, James von Brecht.
“A recurrent neural network without chaos.” ICLR, 2017.</p>
<p>A CFN cell first projects the input to the hidden space. The hidden state
goes through a contractive mapping. The new hidden state is then calculated
as a linear combination of the projected input and the contracted previous
hidden state, using decoupled input and forget gates.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#CFNCell.build"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell.build" title="Permalink to this definition">#</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../../../../../../modules/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell.html#CFNCell.call"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell.call" title="Permalink to this definition">#</a></dt>
<dd><p>Run one step of CFN.</p>
<dl class="simple">
<dt>Args:</dt><dd><p>inputs: input Tensor, must be 2-D, <cite>[batch, input_size]</cite>.
state: state Tensor, must be 2-D, <cite>[batch, state_size]</cite>.</p>
</dd>
<dt>Returns:</dt><dd><p>A tuple containing:</p>
<ul class="simple">
<li><p>Output: A <cite>2-D</cite> tensor with shape <cite>[batch_size, state_size]</cite>.</p></li>
<li><p>New state: A <cite>2-D</cite> tensor with shape <cite>[batch_size, state_size]</cite>.</p></li>
</ul>
</dd>
<dt>Raises:</dt><dd><dl class="simple">
<dt>ValueError: If input size cannot be inferred from inputs via</dt><dd><p>static shape inference.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>
</section>


            </article>
            
            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fas fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#module-contents">
   Module Contents
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classes">
     Classes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#attributes">
     Attributes
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CoupledInputForgetGateLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         CoupledInputForgetGateLSTMCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CoupledInputForgetGateLSTMCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           CoupledInputForgetGateLSTMCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.TimeFreqLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         TimeFreqLSTMCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.TimeFreqLSTMCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           TimeFreqLSTMCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GridLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         GridLSTMCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GridLSTMCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           GridLSTMCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.BidirectionalGridLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         BidirectionalGridLSTMCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.BidirectionalGridLSTMCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           BidirectionalGridLSTMCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.AttentionCellWrapper">
       <code class="docutils literal notranslate">
        <span class="pre">
         AttentionCellWrapper
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.AttentionCellWrapper.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           AttentionCellWrapper.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.HighwayWrapper">
       <code class="docutils literal notranslate">
        <span class="pre">
         HighwayWrapper
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.HighwayWrapper.zero_state">
         <code class="docutils literal notranslate">
          <span class="pre">
           HighwayWrapper.zero_state()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormBasicLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         LayerNormBasicLSTMCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormBasicLSTMCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           LayerNormBasicLSTMCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         NASCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell.build">
         <code class="docutils literal notranslate">
          <span class="pre">
           NASCell.build()
          </span>
         </code>
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           NASCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.UGRNNCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         UGRNNCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.UGRNNCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           UGRNNCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IntersectionRNNCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         IntersectionRNNCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IntersectionRNNCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           IntersectionRNNCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CompiledWrapper">
       <code class="docutils literal notranslate">
        <span class="pre">
         CompiledWrapper
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CompiledWrapper.zero_state">
         <code class="docutils literal notranslate">
          <span class="pre">
           CompiledWrapper.zero_state()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.PhasedLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         PhasedLSTMCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.PhasedLSTMCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           PhasedLSTMCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.ConvLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         ConvLSTMCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.ConvLSTMCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           ConvLSTMCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv1DLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         Conv1DLSTMCell
        </span>
       </code>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv2DLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         Conv2DLSTMCell
        </span>
       </code>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv3DLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         Conv3DLSTMCell
        </span>
       </code>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         GLSTMCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GLSTMCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           GLSTMCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         LayerNormLSTMCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormLSTMCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           LayerNormLSTMCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         SRUCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell.build">
         <code class="docutils literal notranslate">
          <span class="pre">
           SRUCell.build()
          </span>
         </code>
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           SRUCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.WeightNormLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         WeightNormLSTMCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.WeightNormLSTMCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           WeightNormLSTMCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         IndRNNCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell.build">
         <code class="docutils literal notranslate">
          <span class="pre">
           IndRNNCell.build()
          </span>
         </code>
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           IndRNNCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         IndyGRUCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell.build">
         <code class="docutils literal notranslate">
          <span class="pre">
           IndyGRUCell.build()
          </span>
         </code>
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           IndyGRUCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         IndyLSTMCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell.build">
         <code class="docutils literal notranslate">
          <span class="pre">
           IndyLSTMCell.build()
          </span>
         </code>
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           IndyLSTMCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMControllerState">
       <code class="docutils literal notranslate">
        <span class="pre">
         NTMControllerState
        </span>
       </code>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         NTMCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell.build">
         <code class="docutils literal notranslate">
          <span class="pre">
           NTMCell.build()
          </span>
         </code>
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           NTMCell.call()
          </span>
         </code>
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell.zero_state">
         <code class="docutils literal notranslate">
          <span class="pre">
           NTMCell.zero_state()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         MinimalRNNCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell.build">
         <code class="docutils literal notranslate">
          <span class="pre">
           MinimalRNNCell.build()
          </span>
         </code>
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           MinimalRNNCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell">
       <code class="docutils literal notranslate">
        <span class="pre">
         CFNCell
        </span>
       </code>
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell.build">
         <code class="docutils literal notranslate">
          <span class="pre">
           CFNCell.build()
          </span>
         </code>
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell.call">
         <code class="docutils literal notranslate">
          <span class="pre">
           CFNCell.call()
          </span>
         </code>
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CoupledInputForgetGateLSTMCell">
       CoupledInputForgetGateLSTMCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CoupledInputForgetGateLSTMCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.TimeFreqLSTMCell">
       TimeFreqLSTMCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.TimeFreqLSTMCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GridLSTMCell">
       GridLSTMCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GridLSTMCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.BidirectionalGridLSTMCell">
       BidirectionalGridLSTMCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.BidirectionalGridLSTMCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.AttentionCellWrapper">
       AttentionCellWrapper
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.AttentionCellWrapper.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.HighwayWrapper">
       HighwayWrapper
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.HighwayWrapper.zero_state">
         zero_state
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormBasicLSTMCell">
       LayerNormBasicLSTMCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormBasicLSTMCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell">
       NASCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell.build">
         build
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NASCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.UGRNNCell">
       UGRNNCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.UGRNNCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IntersectionRNNCell">
       IntersectionRNNCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IntersectionRNNCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CompiledWrapper">
       CompiledWrapper
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CompiledWrapper.zero_state">
         zero_state
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.PhasedLSTMCell">
       PhasedLSTMCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.PhasedLSTMCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.ConvLSTMCell">
       ConvLSTMCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.ConvLSTMCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv1DLSTMCell">
       Conv1DLSTMCell
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv2DLSTMCell">
       Conv2DLSTMCell
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.Conv3DLSTMCell">
       Conv3DLSTMCell
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GLSTMCell">
       GLSTMCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.GLSTMCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormLSTMCell">
       LayerNormLSTMCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.LayerNormLSTMCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell">
       SRUCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell.build">
         build
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.SRUCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.WeightNormLSTMCell">
       WeightNormLSTMCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.WeightNormLSTMCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell">
       IndRNNCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell.build">
         build
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndRNNCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell">
       IndyGRUCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell.build">
         build
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyGRUCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell">
       IndyLSTMCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell.build">
         build
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.IndyLSTMCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMControllerState">
       NTMControllerState
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell">
       NTMCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell.build">
         build
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell.call">
         call
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.NTMCell.zero_state">
         zero_state
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell">
       MinimalRNNCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell.build">
         build
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.MinimalRNNCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell">
       CFNCell
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell.build">
         build
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sparknlp_jsl._tf_graph_builders.tf2contrib.rnn_cell.CFNCell.call">
         call
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

</nav>
</div>

<div class="toc-item">
  
</div>

<div class="toc-item">
  
<div class="tocsection sourcelink">
    <a href="../../../../../../_sources/reference/autosummary/sparknlp_jsl/_tf_graph_builders/tf2contrib/rnn_cell/index.rst.txt">
        <i class="fas fa-file-alt"></i> Show Source
    </a>
</div>

</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
          </div>
        </footer>
        
      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../../../../static/scripts/pydata-sphinx-theme.js?digest=9b1a4fa89bdd0e95b23b"></script>

  <footer class="bd-footer"><div class="bd-footer__inner container">
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2022, John Snow Labs.<br>

</p>

  </div>
  
  <div class="footer-item">
    
<p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 5.2.3.<br>
</p>

  </div>
  
</div>
  </footer>
  </body>
</html>