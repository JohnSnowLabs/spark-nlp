
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>sparknlp_jsl._tf_graph_builders.tf2contrib.fused_rnn_cell &#8212; Spark NLP 4.0.0 documentation</title>
    
  <link href="../../../../static/css/theme.css" rel="stylesheet" />
  <link href="../../../../static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../../../static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../../static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../static/basic.css" />
    <link rel="stylesheet" type="text/css" href="../../../../static/graphviz.css" />
    <link rel="stylesheet" type="text/css" href="../../../../static/css/custom.css" />
    
  <link rel="preload" as="script" href="../../../../static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../static/documentation_options.js"></script>
    <script src="../../../../static/jquery.js"></script>
    <script src="../../../../static/underscore.js"></script>
    <script src="../../../../static/doctools.js"></script>
    <script src="../../../../static/toggleprompt.js"></script>
    <link rel="shortcut icon" href="../../../../static/fav.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="../../../../index.html">
  <img src="../../../../static/logo.png" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../getting_started/index.html">
  Getting Started
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="../../../../reference/index.html">
  API Reference
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    
  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <h1>Source code for sparknlp_jsl._tf_graph_builders.tf2contrib.fused_rnn_cell</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright 2016 The TensorFlow Authors. All Rights Reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>
<span class="c1"># ==============================================================================</span>
<span class="sd">&quot;&quot;&quot;Module for constructing fused RNN cells.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">abc</span>

<span class="kn">import</span> <span class="nn">six</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">array_ops</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.ops</span> <span class="kn">import</span> <span class="n">rnn</span>


<span class="nd">@six</span><span class="o">.</span><span class="n">add_metaclass</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">ABCMeta</span><span class="p">)</span>
<div class="viewcode-block" id="FusedRNNCell"><a class="viewcode-back" href="../../../../reference/autosummary/sparknlp_jsl/_tf_graph_builders/tf2contrib/fused_rnn_cell/index.html#sparknlp_jsl._tf_graph_builders.tf2contrib.fused_rnn_cell.FusedRNNCell">[docs]</a><span class="k">class</span> <span class="nc">FusedRNNCell</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Abstract object representing a fused RNN cell.</span>

<span class="sd">    A fused RNN cell represents the entire RNN expanded over the time</span>
<span class="sd">    dimension. In effect, this represents an entire recurrent network.</span>

<span class="sd">    Unlike RNN cells which are subclasses of `rnn_cell.RNNCell`, a `FusedRNNCell`</span>
<span class="sd">    operates on the entire time sequence at once, by putting the loop over time</span>
<span class="sd">    inside the cell. This usually leads to much more efficient, but more complex</span>
<span class="sd">    and less flexible implementations.</span>

<span class="sd">    Every `FusedRNNCell` must implement `__call__` with the following signature.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">inputs</span><span class="p">,</span>
                 <span class="n">initial_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">sequence_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Run this fused RNN on inputs, starting from the given state.</span>

<span class="sd">        Args:</span>
<span class="sd">          inputs: `3-D` tensor with shape `[time_len x batch_size x input_size]`</span>
<span class="sd">            or a list of `time_len` tensors of shape `[batch_size x input_size]`.</span>
<span class="sd">          initial_state: either a tensor with shape `[batch_size x state_size]`</span>
<span class="sd">            or a tuple with shapes `[batch_size x s] for s in state_size`, if the</span>
<span class="sd">            cell takes tuples. If this is not provided, the cell is expected to</span>
<span class="sd">            create a zero initial state of type `dtype`.</span>
<span class="sd">          dtype: The data type for the initial state and expected output. Required</span>
<span class="sd">            if `initial_state` is not provided or RNN state has a heterogeneous</span>
<span class="sd">              dtype.</span>
<span class="sd">          sequence_length: Specifies the length of each sequence in inputs. An</span>
<span class="sd">            `int32` or `int64` vector (tensor) size `[batch_size]`, values in `[0,</span>
<span class="sd">            time_len)`.</span>
<span class="sd">            Defaults to `time_len` for each element.</span>
<span class="sd">          scope: `VariableScope` or `string` for the created subgraph; defaults to</span>
<span class="sd">            class name.</span>

<span class="sd">        Returns:</span>
<span class="sd">          A pair containing:</span>

<span class="sd">          - Output: A `3-D` tensor of shape `[time_len x batch_size x output_size]`</span>
<span class="sd">            or a list of `time_len` tensors of shape `[batch_size x output_size]`,</span>
<span class="sd">            to match the type of the `inputs`.</span>
<span class="sd">          - Final state: Either a single `2-D` tensor, or a tuple of tensors</span>
<span class="sd">            matching the arity and shapes of `initial_state`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">pass</span></div>


<div class="viewcode-block" id="FusedRNNCellAdaptor"><a class="viewcode-back" href="../../../../reference/autosummary/sparknlp_jsl/_tf_graph_builders/tf2contrib/fused_rnn_cell/index.html#sparknlp_jsl._tf_graph_builders.tf2contrib.fused_rnn_cell.FusedRNNCellAdaptor">[docs]</a><span class="k">class</span> <span class="nc">FusedRNNCellAdaptor</span><span class="p">(</span><span class="n">FusedRNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This is an adaptor for RNNCell classes to be used with `FusedRNNCell`.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell</span><span class="p">,</span> <span class="n">use_dynamic_rnn</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Initialize the adaptor.</span>

<span class="sd">        Args:</span>
<span class="sd">          cell: an instance of a subclass of a `rnn_cell.RNNCell`.</span>
<span class="sd">          use_dynamic_rnn: whether to use dynamic (or static) RNN.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span> <span class="o">=</span> <span class="n">cell</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_use_dynamic_rnn</span> <span class="o">=</span> <span class="n">use_dynamic_rnn</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">inputs</span><span class="p">,</span>
                 <span class="n">initial_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">sequence_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">is_list</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_use_dynamic_rnn</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">is_list</span><span class="p">:</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">outputs</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="p">,</span>
                <span class="n">inputs</span><span class="p">,</span>
                <span class="n">sequence_length</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">,</span>
                <span class="n">initial_state</span><span class="o">=</span><span class="n">initial_state</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">time_major</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_list</span><span class="p">:</span>
                <span class="c1"># Convert outputs back to list</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1"># non-dynamic rnn</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_list</span><span class="p">:</span>
                <span class="n">inputs</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">unstack</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">outputs</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">rnn</span><span class="o">.</span><span class="n">static_rnn</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="p">,</span>
                <span class="n">inputs</span><span class="p">,</span>
                <span class="n">initial_state</span><span class="o">=</span><span class="n">initial_state</span><span class="p">,</span>
                <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
                <span class="n">sequence_length</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">,</span>
                <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_list</span><span class="p">:</span>
                <span class="c1"># Convert outputs back to tensor</span>
                <span class="n">outputs</span> <span class="o">=</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">state</span></div>


<div class="viewcode-block" id="TimeReversedFusedRNN"><a class="viewcode-back" href="../../../../reference/autosummary/sparknlp_jsl/_tf_graph_builders/tf2contrib/fused_rnn_cell/index.html#sparknlp_jsl._tf_graph_builders.tf2contrib.fused_rnn_cell.TimeReversedFusedRNN">[docs]</a><span class="k">class</span> <span class="nc">TimeReversedFusedRNN</span><span class="p">(</span><span class="n">FusedRNNCell</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;This is an adaptor to time-reverse a FusedRNNCell.</span>

<span class="sd">    For example,</span>

<span class="sd">    ```python</span>
<span class="sd">    cell = tf.compat.v1.nn.rnn_cell.BasicRNNCell(10)</span>
<span class="sd">    fw_lstm = tf.contrib.rnn.FusedRNNCellAdaptor(cell, use_dynamic_rnn=True)</span>
<span class="sd">    bw_lstm = tf.contrib.rnn.TimeReversedFusedRNN(fw_lstm)</span>
<span class="sd">    fw_out, fw_state = fw_lstm(inputs)</span>
<span class="sd">    bw_out, bw_state = bw_lstm(inputs)</span>
<span class="sd">    ```</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cell</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span> <span class="o">=</span> <span class="n">cell</span>

    <span class="k">def</span> <span class="nf">_reverse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Time reverse the provided tensor or list of tensors.</span>

<span class="sd">        Assumes the top dimension is the time dimension.</span>

<span class="sd">        Args:</span>
<span class="sd">          t: 3D tensor or list of 2D tensors to be reversed</span>
<span class="sd">          lengths: 1D tensor of lengths, or `None`</span>

<span class="sd">        Returns:</span>
<span class="sd">          A reversed tensor or list of tensors</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">lengths</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reverse_v2</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">array_ops</span><span class="o">.</span><span class="n">reverse_sequence</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">inputs</span><span class="p">,</span>
                 <span class="n">initial_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">sequence_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">scope</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reverse</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cell</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span>
            <span class="n">initial_state</span><span class="o">=</span><span class="n">initial_state</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
            <span class="n">sequence_length</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">,</span>
            <span class="n">scope</span><span class="o">=</span><span class="n">scope</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_reverse</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">state</span></div>
</pre></div>

              </div>
              
              
              <div class='prev-next-bottom'>
                

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="../../../../static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2022, John Snow Labs.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.2.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>