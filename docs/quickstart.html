<!DOCTYPE html>
<!--[if IE 8]>
<html lang="en" class="ie8"> <![endif]-->
<!--[if IE 9]>
<html lang="en" class="ie9"> <![endif]-->
<!--[if !IE]><!-->
<html lang="en"> <!--<![endif]-->
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-70312582-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() {
            dataLayer.push(arguments);
        }
        gtag('js', new Date());

        gtag('config', 'UA-70312582-2');
    </script>
    <title>Spark NLP - Quick Start</title>
    <!-- Meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="fav.ico">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800'
          rel='stylesheet' type='text/css'>
    <!-- Global CSS -->
    <link rel="stylesheet" href="assets/plugins/bootstrap/css/bootstrap.min.css">
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="assets/plugins/font-awesome/css/font-awesome.css">
    <link rel="stylesheet" href="assets/plugins/prism/prism.css">
    <link rel="stylesheet" href="assets/plugins/elegant_font/css/style.css">

    <!-- Theme CSS -->
    <link id="theme-style" rel="stylesheet" href="assets/css/styles.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <script type="text/javascript" src="assets/plugins/jquery-1.12.3.min.js"></script>
    <script>
        function getTasks() {
            $.get("https://api.github.com/repos/JohnSnowLabs/spark-nlp/commits?path=docs/articles.html",
                    function (data) {
                        var dateObject = new Date(data[0].commit.author.date);
                        $(".wrapper").html(dateObject.toDateString());
                    });
        }
        getTasks();
    </script>
</head>

<body class="body-green">
<script>
    $(function () {
        $("#includedHeader").load("header.html");
        $("#includedFooter").load("footer.html");
    });
</script>
<div class="page-wrapper">
    <!-- ******Header****** -->
    <header id="header" class="header">
        <div class="container">
            <div id="includedHeader"></div>
            <ol class="breadcrumb">
                <li><a href="index.html">Home</a></li>
                <li class="active">Quick Start</li>
            </ol>
        </div>
    </header>
    <div style="border:1px solid #e7e7e7;"></div>
    <div class="doc-wrapper">
        <div class="container">
            <div id="doc-header" class="doc-header text-center">
                <h1 class="doc-title"><i class="icon fa fa-paper-plane"></i> SparkNLP - Quick Start</h1>
                <div class="meta"><i class="fa fa-clock-o"></i> Last updated: <span class="wrapper"></span></div>
            </div>
            <div class="doc-body">
                <div class="doc-content">
                    <div class="content-inner">
                        <section id="concepts-section" class="doc-section">
                            <h2 class="section-title">Requirements & Setup</h2>
                            <div class="section-block">
                                <p>SparkNLP is built on top of Apache Spark 2.3.0 and works with any user provided Spark 2.x.x
                                    it is advised to have basic knowledge of the framework and a working environment
                                    before using Spark-NLP. Refer to its
                                    <a href="http://spark.apache.org/docs/2.3.0/index.html">documentation</a> to
                                    get started with Spark.
                                </p>
                                <p>
                                To start using the library, execute any of the following lines
                                depending on your desired use case:
                                </p>
                                <pre><code class="language-javascript">spark-shell --packages JohnSnowLabs:spark-nlp:1.8.2
pyspark --packages JohnSnowLabs:spark-nlp:1.8.2
spark-submit --packages JohnSnowLabs:spark-nlp:1.8.2
</code></pre>
                                <p/>
                                <h3><b>Databricks cloud cluster</b> & <b>Apache Zeppelin</b></h3>
                                <pre><code class="language-javascript">com.johnsnowlabs.nlp:spark-nlp_2.11:1.8.2</code></pre>
                                <p>
                                    For Python in <b>Apache Zeppelin</b> you may need to setup <i><b>SPARK_SUBMIT_OPTIONS</b></i> utilizing --packages instruction shown above like this
                                </p>
                                <pre><code class="language-javascript">export SPARK_SUBMIT_OPTIONS="--packages JohnSnowLabs:spark-nlp:1.8.2"</code></pre>
                                <h3><b>Python Jupyter Notebook with PySpark</b></h3>
                                <pre><code class="language-javascript">export SPARK_HOME=/path/to/your/spark/folder
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS=notebook

pyspark --packages JohnSnowLabs:spark-nlp:1.8.2</code></pre>
                                <h3><b>Python without explicit Spark Installation</b></h3>
                                <p>Use pip to install (after you pip installed pyspark)</p>
                                <pre><code class="language-javascript">pip install spark-nlp==1.8.2</code></pre>
                                <p>In this way, you will have to start SparkSession in your python program manually, this is an example</p>
                                <pre><code class="python">spark = SparkSession.builder \
    .appName("ner")\
    .master("local[*]")\
    .config("spark.driver.memory","4G")\
    .config("spark.driver.maxResultSize", "2G") \
    .config("spark.driver.extraClassPath", "lib/spark-nlp-assembly-1.8.2.jar")\
    .config("spark.kryoserializer.buffer.max", "500m")\
    .getOrCreate()</code></pre>
                                <h3>S3 based standalone cluster (No Hadoop)</h3>
                                <p>
                                    If your distributed storage is S3 and you don't have a standard hadoop configuration (i.e. fs.defaultFS)
                                    You need to specify where in the cluster distributed storage you want to store Spark-NLP's tmp files.
                                    First, decide where you want to put your <b>application.conf</b> file
                                </p>
                                <pre><code class="python">import com.johnsnowlabs.uti.ConfigLoader
ConfigLoader.setConfigPath("/somewhere/to/put/application.conf")</code></pre>
                                <p>
                                    And then we need to put in such application.conf the following content
                                </p>
                                <pre><code class="python">sparknlp {
  settings {
    cluster_tmp_dir = "somewhere in s3n:// path to some folder"
  }
}</code></pre>
                                <h3>Pre-Compiled Spark-NLP for download</h3>
                                <p>
                                    Pre-compiled Spark-NLP assembly fat-jar
                                    <a href="https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-1.8.2.jar">here</a>
                                    Pre-compiled Spark-NLP assembly fat-jar with Tensorflow GPU enabled
                                    <a href="https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp_2.11-1.8.2-gpu.jar">here</a>
                                    then, run spark-shell or spark-submit with appropriate <b>--jars
                                    /path/to/spark-nlp_2.11-1.8.2.jar</b> to use the library in spark.
                                </p>
                                <p>
                                    For further alternatives and documentation check out our README page in <a href="https://github.com/JohnSnowLabs/spark-nlp">GitHub</a>.
                                </p>
                            </div>
                        </section>
                        <section id="concepts-2-section" class="doc-section">
                            <h2 class="section-title">Concepts</h2>
                            <div class="section-block">
                                <p>Spark ML provides a set of Machine Learning applications, and
                                    it's logic consists of two main components: <b>Estimators</b> and
                                    <b>Transformers</b>. The first, have a method called fit() which secures and trains
                                    a piece of data to such application, and a <b>Transformer</b>, which is generally
                                    the result of a fitting process, applies changes to the the target dataset.
                                    These components have been embedded to be applicable to Spark NLP.
                                    <b>Pipelines</b> are a mechanism that allow multiple
                                    estimators and transformers within a single workflow, allowing multiple
                                    chained transformations along a Machine Learning task.
                                    Refer to <a href="https://spark.apache.org/docs/2.3.0/ml-guide.html">SparkML</a>
                                    library for more information.</p>
                            </div>
                        </section>
                        <section id="annotation-section" class="doc-section">
                            <h2 class="section-title">Annotation</h2>
                            <div class="section-block">
                                <p>An annotation is the basic form of the result of a Spark-NLP operation. It's
                                    structure is made of:</p>
                                <ul class="list">
                                    <li><b>annotatorType:</b> which annotator generated this annotation</li>
                                    <li><b>begin:</b> the begin of the matched content relative to raw-text</li>
                                    <li><b>end:</b> the end of the matched content relative to raw-text</li>
                                    <li><b>metadata:</b> content of matched result and additional information</li>
                                </ul>
                                <p>
                                    This object is <b>automatically generated</b> by annotators after a transform
                                    process. No manual work is
                                    required. But it must be understood in order to use it efficiently.
                                </p>
                            </div>
                        </section>
                        <section id="annotators-section" class="doc-section">
                            <h2 class="section-title">Annotators</h2>
                            <div class="section-block">
                                <p>Annotators are the spearhead of NLP functionalities in SparkNLP. There are two forms
                                    of annotators:</p>
                                <ul class="list">
                                    <li><b>Annotator Approaches:</b> Are those who represent a Spark ML Estimator and
                                        require a training stage. They have a function called fit(data) which trains a
                                        model based on some data. They produce the second type of annotator which is an
                                        annotator model or transformer.
                                    </li>
                                    <li><b>Annotator Model:</b> They are spark models or transformers, meaning they have
                                        a <b>transform(data)</b> function which take a dataset and add to it a column
                                        with the result of this annotation. All transformers are additive, meaning they
                                        append to current data, never replace or delete previous information.
                                    </li>
                                </ul>
                                <p>
                                    Both forms of annotators can be included in a Pipeline and will automatically go
                                    through
                                    all stages in the provided order and transform the data accordingly. A Pipeline is
                                    turned into a PipelineModel after the fit() stage. Either before or after can be
                                    saved
                                    and re-loaded to disk at any time.
                                </p>
                                <div id="anno-functions" class="section-block">
                                    <h3 class="block-title">Common Functions</h3>
                                    <ul class="list">
                                        <li><b>setInputCols</b>(column_names): Takes a list of column names of
                                            annotations
                                            required by this annotator
                                        </li>
                                        <li><b>setOutputCol(</b>column_name): Defines the name of the column containing
                                            the
                                            result of this annotator. Use this name as an input for other annotators
                                            requiring the annotations of this one.
                                        </li>
                                    </ul>
                                </div>
                            </div>
                        </section>
                        <section class="doc-section">
                            <h2 class="section-title">Quickly annotate some text</h2>
                            <div id="annotators-type-section-quick" class="section-block">
                                <h3 class="block-title">Basic Pipeline</h3>
                                <p>
                                A basic pipeline is a downloadable Spark ML pipeline ready to compute
                                some annotations for you right away. Keep in mind this is trained with
                                generic data and is not meant for production use, but should give you
                                an insight of how the library works at a glance.
                                <p>
                            </div>
                            <div class="section-block">
                                <div class="code-block">
                                    <h3 class="block-title">Downloading and using a pretrained pipeline</h3>
                                    <p>
                                        Basic pipelines allow to inject either common strings or
                                        Spark dataframes. Either way, SparkNLP will execute the
                                        right pipeline type to bring back the results. Let's retrieve it:
                                    </p>
                                    <pre><code class="language-javascript">import com.johnsnowlabs.nlp.pretrained.pipelines.en.BasicPipeline

val annotations = BasicPipeline().annotate(Array("We are very happy about SparkNLP", "And this is just another sentence"))

annotations.foreach(println(_, "\n"))</code></pre>
                                    <pre><code class="language-c">(Map(lemma -> List(We, be, very, happy, about, SparkNLP), document -> List(We are very happy about SparkNLP), normal -> List(We, are, very, happy, about, SparkNLP), pos -> ArrayBuffer(PRP, VBP, RB, JJ, IN, NNP), token -> List(We, are, very, happy, about, SparkNLP)))
(Map(lemma -> List(And, this, be, just, another, sentence), document -> List(And this is just another sentence), normal -> List(And, this, is, just, another, sentence), pos -> ArrayBuffer(CC, DT, VBZ, RB, DT, NN), token -> List(And, this, is, just, another, sentence)))</code></pre>
                                </div>
                            </div>
                            <div class="section-block">
                                <div class="code-block">
                                    <h3 class="block-title">Using a pretrained pipeline with spark dataframes</h3>
                                    <p>
                                        Since this is Apache Spark, we should make use of our DataFrame friends.
                                        Pretraned pipelines allow us to do so with the same function, just
                                        adding the target string column. The result here will be a dataframe
                                        with many annotation columns.
                                    </p>
                                    <pre><code class="language-javascript">import spark.implicits._

val data = Seq("hello, this is an example sentence").toDF("mainColumn")

BasicPipeline().annotate(data, "mainColumn").show()</code>
                                    </pre>
                                    <pre><code class="language-c">+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
|                text|            document|               token|              normal|               lemma|                 pos|
+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
|hello, this is an...|[[document, 0, 33...|[[token, 0, 4, he...|[[token, 0, 4, he...|[[token, 0, 4, he...|[[pos, 0, 4, UH, ...|
+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+
</code></pre>
                                </div>
                            </div>
                            <div class="section-block">
                                <div class="code-block">
                                    <h3 class="block-title">Manipulating pipelines</h3>
                                    <p>
                                        To add a bit of challenge, the output of the previous DataFrame
                                        was in terms of Annotation objects. What if we want to deal with
                                        just the resulting annotations? We can use the Finisher annotator,
                                        retrieve the BasicPipeline, and add them together in a Spark ML Pipeline.
                                        Note BasicPipeline expect the target column to be named "text".
                                    </p>
                                    <pre><code class="language-javascript">import com.johnsnowlabs.nlp.Finisher
import org.apache.spark.ml.Pipeline

val finisher = new Finisher().
    setInputCols("token", "normal", "lemma", "pos")

val basicPipeline = BasicPipeline().pretrained()

val pipeline = new Pipeline().
    setStages(Array(
        basicPipeline,
        finisher
    ))

pipeline.
    fit(spark.emptyDataFrame).
    transform(data.withColumnRenamed("mainColumn", "text")).
    show()</code></pre>
                                    <pre><code class="language-c">+--------------------+--------------------+--------------------+--------------------+--------------------+
|                text|      finished_token|     finished_normal|      finished_lemma|        finished_pos|
+--------------------+--------------------+--------------------+--------------------+--------------------+
|hello, this is an...|[hello, ,, this, ...|[hello, this, is,...|[hello, ,, this, ...|[UH, DT, VBZ, DT,...|
+--------------------+--------------------+--------------------+--------------------+--------------------+
</code></pre>
                                </div>
                            </div>
                        </section>
                        <section id="example-section" class="doc-section">
                            <h2 class="section-title">Train your own pipeline</h2>
                            <div id="annotators-type-section" class="section-block">
                                <h3 class="block-title">Annotator types</h3>
                                <p>
                                    Every annotator has a type. Those annotators that share a type, can be used
                                    interchangeably, meaning you could you use any of them when needed. For example,
                                    when a token type annotator is required by another annotator, such as a sentiment
                                    analysis annotator, you can either provide a normalized token or a lemma, as both
                                    are of type token.
                                <p>
                            </div>
                            <div id="anno-doc-example" class="section-block">
                                <div class="code-block">
                                    <h3 class="block-title">Necessary imports</h3>
                                    <p>
                                        Since version 1.5.0 we are making necessary imports easy to reach, <b>base._</b>
                                        will include general Spark NLP transformers and concepts, while <b>annotator._</b> will include
                                        all annotators that we currently provide. We also need SparkML pipelines.
                                    </p>
                                    <pre><code class="language-javascript">import com.johnsnowlabs.nlp.base._
import com.johnsnowlabs.nlp.annotator._
import org.apache.spark.ml.Pipeline</code></pre>
                                </div>
                                <div class="code-block">
                                    <h3 class="block-title">DocumentAssembler: Getting data in</h3>
                                    <p>
                                        In order to get through the NLP process, we need to get raw data annotated.
                                        There is a special <b>transformer</b> that does this for us: the <b>DocumentAssembler</b>,
                                        it creates the first annotation of type <b>Document</b> which may be used by
                                        annotators down the road
                                    </p>
                                    <pre><code class="language-javascript">val documentAssembler = new DocumentAssembler().
    setInputCol("text").
    setOutputCol("document")</code></pre>
                                </div>
                            </div>
                            <div id="anno-token-example" class="section-block">
                                <div class="code-block">
                                    <h3 class="block-title">Sentence detection and tokenization</h3>
                                    <p>
                                        In this quick example, we now proceed to identify the sentences in each of our
                                        document lines.
                                        SentenceDetector requires a Document annotation, which is provided by the
                                        DocumentAssembler
                                        output, and it's itself a Document type token.
                                        The Tokenizer requires a Document annotation type, meaning it works both
                                        with DocumentAssembler
                                        or SentenceDetector output, in here, we use the sentence output.
                                    </p>
                                    <pre><code class="language-javascript">val sentenceDetector = new SentenceDetector().
    setInputCols(Array("document")).
    setOutputCol("sentence")

val regexTokenizer = new Tokenizer().
    setInputCols(Array("sentence")).
    setOutputCol("token")</code></pre>
                                </div>
                            </div>
                            <div id="using-pipelines" class="section-block">
                                <div class="code-block">
                                    <h3 class="block-title">Using SparkML Pipeline</h3>
                                    <p>
                                        Now we want to put all this together and retrieve the results,
                                        we use a Pipeline for this. We also include another special transformer,
                                        called <b>Finisher</b> to show tokens in a human language.
                                        We use an emptyDataFrame in fit() since none of the pipeline stages
                                        have a training stage.
                                    </p>
                                    <pre><code class="language-javascript">val testData = Seq("Lorem ipsum dolor sit amet, " +
    "consectetur adipiscing elit, sed do eiusmod tempor " +
    "incididunt ut labore et dolore magna aliqua.").toDF("text")

val finisher = new Finisher().
    setInputCols("token").
    setCleanAnnotations(false)

val pipeline = new Pipeline().
    setStages(Array(
        documentAssembler,
        sentenceDetector,
        regexTokenizer,
        finisher
    ))

pipeline.
    fit(Seq.empty[String].toDF("text")).
    transform(Seq("hello, this is an example sentence").toDF("text")).
    show()</code></pre>
                                </div>
                                <div class="code-block">
                                    <h3 class="block-title">Using LightPipeline</h3>
                                    <p>
                                        LightPipeline is a Spark-NLP specific Pipeline class equivalent to SparkML's Pipeline. The difference is that
                                        it's execution does not hold to Spark principles, instead it computes everything locally (but in parallel) in order
                                        to achieve fast results when dealing with small amounts of data. This means, we do not input a Spark Dataframe, but a string
                                        or an Array of strings instead, to be annotated.
                                        To create Light Pipelines, you need to input an already trained (fit) Spark ML Pipeline. It's transform() stage is converted into annotate() instead.
                                    </p>
                                    <pre><code class="language-javascript">import com.johnsnowlabs.nlp.base._

val trainedModel = pipeline.fit(Seq.empty[String].toDF("text"))

val lightPipeline = new LightPipeline(trainedModel)

lightPipeline.annotate("Hello world, please annotate my text")
                                    </code></pre>
                                </div>
                            </div>
                        </section>
                        <section id="ocr-section" class="doc-section">
                            <h2 class="section-title">Utilizing Spark-NLP OCR PDF Converter</h2>
                            <div id="ocr-type-section" class="section-block">
                                <h3 class="block-title">Installing Spark-NLP OCRHelper</h3>
                                <p>
                                    First, either build from source or download the following standalone jar module (works both from Spark-NLP python and scala):
                                    <a href="https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-ocr-assembly-1.8.2.jar">Spark-NLP-OCR</a>
                                    And add it to your Spark environment (with --jars or spark.driver.extraClassPath and spark.executor.extraClassPath configuration)
                                    Second, if your PDFs don't have a text layer (this depends on how PDFs were created), the library will use Tesseract 4.0 on background.
                                    Tesseract will utilize native libraries, so you'll have to get them installed in your system.
                                    You can also use the following coordinates for Maven: com.johnsnowlabs.nlp:spark-nlp_2.11:1.8.2
                                </p>
                                <h3 class="block-title">Creating Spark datasets from PDF (For SparkML Pipeline)</h3>
                                <p>
                                    You can use OcrHelper to directly create spark dataframes from PDF. This will hold entire documents in single rows, meant to
                                    be later processed by a SentenceDetector. This way, you won't be breaking the content in rows as if you were reading a standard document.
                                    Metadata column will include page numbers and file name information per row.
                                </p>
                                    <pre><code class="language-python">import com.johnsnowlabs.nlp.util.io.OcrHelper

val data = OcrHelper.createDataset(spark, "/pdfs/", "text", "metadata")

val documentAssembler = new DocumentAssembler().setInputCol("text").setMetadataCol("metadata")

documentAssembler.transform(data).show()</code></pre>
                                <h3 class="block-title">Creating an Array of Strings from PDF (For LightPipeline)</h3>
                                <p>
                                    Another way, would be to simply create an array of strings. This is useful for example if you are parsing a small amount of pdf files
                                    and would like to use LightPipelines instead. See an example below.
                                </p>
                                <pre><code class="language-python">import com.johnsnowlabs.nlp.util.io.OcrHelper

val raw = OcrHelper.createMap("/pdfs/")

val documentAssembler = new DocumentAssembler().setInputCol("text").setOutputCol("document")

val sentenceDetector = new SentenceDetector().setInputCols("document").setOutputCol("sentence")

val lightPipeline = new LightPipeline(new Pipeline().setStages(Array(documentAssembler, sentenceDetector)).fit(Seq.empty[String].toDF("text")))

pipeline.annotate(raw.values.toArray)</code></pre>
                            </div>
                        </section>
                        <section id="training-section" class="doc-section">
                            <h2 class="section-title">Training annotators</h2>
                            <div id="training-type-section" class="section-block">
                                <h3 class="block-title">Training methodology</h3>
                                <p>
                                    Training your own annotators is the most key concept when dealing with real life scenarios.
                                    Any of the annotators provided above, such as pretrained pipelines and models, will rarely ever apply to
                                    a specific use case. Dealing with real life problems will require training your own models.

                                    In Spark-NLP, training annotators will vary depending on the annotators. Currently, we support three ways:
                                    <ol>
                                <li>Training from an <b>external source</b>: Most of our annotators train from an external file or folder passed to the annotator as a param. You will see such ones as
                                <b>setCorpus()</b> or <b>setDictionary()</b> param setter methods, allowing you to configure the input to use. You can set Spark-NLP to read them as Spark datasets or LINE_BY_LINE
                                which is usually faster for small enough corpora</li>
                                <p/>
                                <li>Some annotators are capable of training through the dataset passed to <b>fit()</b>. This is the standard behavior in Spark ML, but unfortunately in NLP, not all
                                annotators use structured data to train. Hence, we provide external resource capabilities. This form of training might be used when 'setCorpus' is not set
                                or any form for <b>setLabelCol()</b> is set instead. Check the reference to see which annotators allow training this one.</li>
                                <p/>
                                <li>Last but not least, some of our annotators are <b>Deep Learning</b> based. These models may be trained either on Tensorflow and read utilizing internal tools,
                                    or converted into Spark-NLP models (Using <b>tensorflow graphs</b>) and allowing them to be trainable from Spark-NLP as well.</li>
                                    </ol>
                                </p>
                                <h3 class="block-title">More examples in Scala and Python</h3>
                                <p>
                                    You can checkout the reference for pretrained models, and to check which params you can set in order to train your own.
                                </p>
                            </div>
                        </section>
                        <section id="next-section" class="doc-section">
                            <h2 class="section-title">Where to go next?</h2>
                            <div id="next-type-section" class="section-block">
                                <h3 class="block-title">Documentation and reference</h3>
                                <p>
                                    Detailed information about Spark-NLP concepts, annotators and more may be found
                                    <a href="http://nlp.johnsnowlabs.com/components.html">HERE</a>
                                </p>
                                <h3 class="block-title">More examples in Scala and Python</h3>
                                <p>
                                    We are working on examples to show you how the library may be used in
                                    different scenarios, take a look <a href="http://nlp.johnsnowlabs.com/notebooks.html">HERE</a>
                                </p>
                            </div>
                        </section>
                    </div>
                </div>
                <div class="doc-sidebar hidden-xs">
                    <nav id="doc-nav">
                        <ul id="doc-menu" class="nav doc-menu" data-spy="affix">
                            <li><a class="scrollto" href="#concepts-section">Requirements</a></li>
                            <li><a class="scrollto" href="#concepts-2-section">Concepts</a></li>
                            <li><a class="scrollto" href="#annotation-section">Annotation</a></li>
                            <li>
                                <a class="scrollto" href="#annotators-section">Annotators</a>
                                <ul class="nav doc-sub-menu">
                                    <li><a class="scrollto" href="#annotators-type-section-quick">Quick results test</a></li>
                                    <li><a class="scrollto" href="#example-section">Using Pipelines</a></li>
                                    <li><a class="scrollto" href="#ocr-section">Spark-NLP OCR</a></li>
                                    <li><a class="scrollto" href="#training-section">Training annotators</a></li>
                                </ul>
                            </li>
                            <li>
                                <a class="scrollto" href="#next-type-section">What next</a>
                            </li>
                        </ul>
                    </nav>
                </div>
            </div>
        </div>
    </div>
</div>
<footer id="footer" class="footer text-center">
    <div id="includedFooter"></div>
</footer>

<!-- Main Javascript -->
<script type="text/javascript" src="assets/plugins/bootstrap/js/bootstrap.min.js"></script>
<script type="text/javascript" src="assets/plugins/prism/prism.js"></script>
<script type="text/javascript" src="assets/plugins/jquery-scrollTo/jquery.scrollTo.min.js"></script>
<script type="text/javascript" src="assets/plugins/jquery-match-height/jquery.matchHeight-min.js"></script>
<script type="text/javascript" src="assets/js/main.js"></script>
</body>
</html>
