{
  "0": {
    "id": "0",
    "title": "404",
    "content": "404 Page not found :( &lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; Detect PHI for deidentification purposes- Spark NLP Model HomeDocsLearnModelsDemo John Snow Labs Nov 29, 2021 Detect PHI for deidentification purposesdeidnerphideidentificationlicensedi2b2en Description Named Entity Recognition model that finds Protected Health Information (PHI) for deidentification purposes. This NER model is trained with a reviewed version of the re-augmented 2014 i2b2 Deid dataset, and detects up to 23 entity types. Predicted Entities MEDICALRECORD, ORGANIZATION, DOCTOR, USERNAME, PROFESSION, HEALTHPLAN, URL, CITY, DATE, LOCATION-OTHER, STATE, PATIENT, DEVICE, COUNTRY, ZIP, PHONE, HOSPITAL, EMAIL, IDNUM, SREET, BIOID, FAX, AGE Live Demo Open in Colab Download How to use PythonScalaNLU ... word_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented_i2b2&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk_subentity&quot;) nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, word_embeddings, deid_ner, ner_converter]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame(pd.DataFrame({&quot;text&quot;: [&quot;&quot;&quot;A. Record date : 2093-01-13, David Hale, M.D., Name : Hendrickson, Ora MR. # 7194334 Date : 01/13/93 PCP : Oliveira, 25 years old, Record date : 1-11-2000. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227. Patient&#39;s complaints first surfaced when he started working for Brothers Coal-Mine.&quot;&quot;&quot;]}))) ... val word_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented_i2b2&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk_subentity&quot;) val nlpPipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, word_embeddings, deid_ner, ner_converter)) val model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) val result = pipeline.fit(Seq.empty[&quot;A. Record date : 2093-01-13, David Hale, M.D., Name : Hendrickson, Ora MR. # 7194334 Date : 01/13/93 PCP : Oliveira, 25 years old, Record date : 1-11-2000. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227. Patient&#39;s complaints first surfaced when he started working for Brothers Coal-Mine.&quot;].toDS.toDF(&quot;text&quot;)).transform(data) Results +--+-+ |chunk |ner_label | +--+-+ |2093-01-13 |DATE | |David Hale |DOCTOR | |Hendrickson, Ora |PATIENT | |7194334 |MEDICALRECORD| |01/13/93 |DATE | |Oliveira |DOCTOR | |25 |AGE | |1-11-2000 |DATE | |Cocke County Baptist Hospital|HOSPITAL | |0295 Keats Street |STREET | |(302) 786-5227 |PHONE | |Brothers Coal-Mine Corp |ORGANIZATION | +--+-+ Model Information Model Name: ner_deid_subentity_augmented_i2b2 Compatibility: Spark NLP for Healthcare 3.3.2+ License: Licensed Edition: Official Input Labels: [sentence, token, embeddings] Output Labels: [ner] Language: en Data Source In-house annotations based on 2014 i2b2 Deid dataset. Benchmarking (on official test set from 2014 i2b2 Deid) +-++-+--+++++ | entity| tp| fp| fn| total|precision|recall| f1| +-++-+--+++++ | PATIENT|1543.0|74.0| 99.0|1642.0| 0.9542|0.9397|0.9469| | HOSPITAL|1414.0|66.0|193.0|1607.0| 0.9554|0.8799|0.9161| | DATE|5869.0|79.0|134.0|6003.0| 0.9867|0.9777|0.9822| | ORGANIZATION| 89.0|15.0| 60.0| 149.0| 0.8558|0.5973|0.7036| | CITY| 301.0|62.0| 46.0| 347.0| 0.8292|0.8674|0.8479| | STREET| 411.0| 1.0| 5.0| 416.0| 0.9976| 0.988|0.9928| | USERNAME| 88.0| 0.0| 4.0| 92.0| 1.0|0.9565|0.9778| | DEVICE| 9.0| 1.0| 1.0| 10.0| 0.9| 0.9| 0.9| | IDNUM| 293.0|25.0| 22.0| 315.0| 0.9214|0.9302|0.9258| | STATE| 165.0| 8.0| 42.0| 207.0| 0.9538|0.7971|0.8684| | ZIP| 138.0| 3.0| 2.0| 140.0| 0.9787|0.9857|0.9822| |MEDICALRECORD| 423.0| 8.0| 26.0| 449.0| 0.9814|0.9421|0.9614| | OTHER| 10.0| 0.0| 10.0| 20.0| 1.0| 0.5|0.6667| | PROFESSION| 284.0|42.0| 56.0| 340.0| 0.8712|0.8353|0.8529| | PHONE| 340.0|18.0| 17.0| 357.0| 0.9497|0.9524| 0.951| | COUNTRY| 109.0|35.0| 21.0| 130.0| 0.7569|0.8385|0.7956| | DOCTOR|3370.0|64.0|248.0|3618.0| 0.9814|0.9315|0.9558| | AGE| 742.0|22.0| 29.0| 771.0| 0.9712|0.9624|0.9668| +-++-+--+++++ ++ | macro| ++ |0.8096816070245438| ++ ++ | micro| ++ |0.9521127666771618| ++ PREVIOUSSentence Entity Resolver for NDC (sbiobert_base_cased_mli embeddings) © 2021 John Snow Labs Inc. Terms of Service | Privacy Policy &lt;/html&gt;",
    "url": "/404.html",
    "relUrl": "/404.html"
  },
  "1": {
    "id": "1",
    "title": "GPU vs CPU benchmark on ClassifierDL",
    "content": "GPU vs CPU benchmark on ClassifierDL This experiment consisted of training a document level Deep Learning Binary Classifier (Question vs Statement classes) using a fully connected CNN and Bert Sentence Embeddings. Only 1 Spark node was used for the training. We used the Spark NLP class ClassifierDL and its method Approach() as described in the documentation. The pipeline looks as follows: Machine specs CPU An AWS m5.8xlarge machine was used for the CPU benchmarking. This machine consists of 32 vCPUs and 128 GB of RAM, as you can check in the official specification webpage available here GPU A Tesla V100 SXM2 GPU with 32GB of memory was used to calculate the GPU benchmarking. Versions The benchmarking was carried out with the following Spark NLP versions: Spark version: 3.0.2 Hadoop version: 3.2.0 SparkNLP version: 3.1.2 Spark nodes: 1 Dataset The size of the dataset was relatively small (200K), consisting of: Training (rows): 162250 Test (rows): 40301 Training params Different batch sizes were tested to demonstrate how GPU performance improves with bigger batches compared to CPU, for a constant number of epochs and learning rate. Epochs: 10 Learning rate: 0.003 Batch sizes: 32, 64, 256, 1024 Results Even for this average-sized dataset, we can observe that GPU is able to beat the CPU machine by an 19% in training time, and a 15% in inference time. Training times depending on batch (in minutes) Batch size CPU GPU 32 66 60 64 65 56 256 64 53 1024 64 52 We can see that CPU didn’t scale with bigger batches, although GPU got improvement specially up to a 256 batch size, not providing much more improvement afterwards. Inference times (in minutes) The average inference time remained more or less constant regardless the batch size: CPU: 8.7 min GPU: 7.4 min Performance metrics A weighted F1-score of 0.88 was achieved, with a 0.90 score for question detection and 0.83 for statements. Takeaways The main takeaways of this benchmark were: GPU provides with an improvement in performance even for relatively small datasets GPU performance scales with bigger batch sizes compared to almost constant CPU times We achieve bigger improvement in training stage than in inference, although both times are better than in CPU How to get the best of the GPU Not always GPU will beat CPU machines. Even in our use case, where we got almost a 20% of improvement, the GPU remains only at 40% of its utilization, what shows there is still room for improvement: The level of improvement depends on the specs on both machines and, evidently, on the model to be trained and the size of the dataset. Bigger improvements can be achived with bigger datasets and big batch sizes. Also, pipelines with heavy computational cost, as SentenceEntityResolver (see Training documentation here are more suitable to achieve improvements using a GPU. Where to look for more information about Training Please, take a look at the Spark NLP and Spark NLP for Healthcare Training sections, and feel free to reach us out in case you want to maximize the performance on your GPU.",
    "url": "/docs/en/CPUvsGPUbenchmarkClassifierDL",
    "relUrl": "/docs/en/CPUvsGPUbenchmarkClassifierDL"
  },
  "2": {
    "id": "2",
    "title": "Model Training",
    "content": "A Project Owner or a Manager can use the completed tasks (completions) from a project for training a new Spark NLP model. The training feature can be found on the Setup page. Named Entity Recognition Projects Named Entity Recognition (NER) projects usually include several labels. When the annotation team has generated a relevant sample of training data/examples for each one of the labels the Project Owner/Manager can use this data to train an DL model which can then be used to predict the labels on new tasks. The NER models can be easily trained as illustrated below. The “Train Now” button (See Arrow 6) can be used to trigger training of a new model when no other trainings or preannotations are in progress. Otherwise, the button is disabled. Information on the training progress is shown in the top right corner of Model Training tab. Here the user can get indications on the success or failure message depending on how the last training ended. When triggering the training, users are prompted to choose either to immediately deploy models or just do training. If immediate deployment is chosen, then the Labeling config is updated according to the name of the new model (item 1 on the above image). It is possible to download training logs by clicking on the download logs icon (see item 9 on the above image) of the recently trained NER model which includes information like training parameters and TF graph used along with precision, recall, f1 score, etc. Training parameters In Annotation Lab versions prior to 1.8.0, for mixed projects containing multiple types of annotations in a single project like classifications, NER, and assertion status, multiple trainings were triggered at the same time using the same system resources and Spark NLP resources. In this case, the training component could fail because of resource limitations. In order to improve the usability of the system, Annotation Lab 1.8.0 added a drop-down option to choose which type of training to run next. The project Owner or Manager of a project can go to the “Advanced Options” and choose the training type. The drop-down gives a list of possible training types for that particular project based on defined Labeling Config. Another drop-down also lists available embeddings which can be used for training the model. It is possible to tune the most common training parameters (Validation split ratio, Epoch, Learning rate, Decay, Dropout, and Batch) by editing their values on the popup window activated by the gear icon. It is also possible to train a model by using a sublist of tasks with predefined tags. This is done by specifing the targeted Tags on the Training Parameters popup window (last option). The Annotation Lab v1.8.0 includes additional filtering options for the training dataset based on the status of completions, either all submitted completions cab be used for training or only the reviewed ones (a drop-down is added in the “Advanced Options”). Transfer Learning Annotation Lab 2.0.0+ supports the Transfer Learning feature offerend by Spark NLP for Healthcare 3.1.2. This feature is available for project manages and project owners, but only if a valid Spark NLP for Healthcare license is loaded into the Annotationl Lab. In this case, the feature can be activated for any project by navigating to the Setup-&gt;Labeling Config -&gt; Model Training -&gt; Advanced Options. It requiers the presence of a base model trained with MedicalNERModel. If a MedicalNER model is available on the Models Hub section of the Annotation Lab, it can be choosen as a starting point of the training process. This means the base model will be Fine Tuned with the new training data. When Fine Tuning is enabled, the same embeddings used for training the base model will be used to train the new model. Those need to be available on the Models Hub section as well. If present, embeddings will be automatically selected, otherwise users must go to the Models Hub page and download or upload them. Custom Training Script If users want to change the default Training script present within the Annotation Lab, they can upload their own training pipeline. In the Training section of the Project Setup Page, admin users can upload the training scripts. At the moment we are supporting custom training script just for NER projects. Selection of Completions During the annotation project lifetime, normally not all tasks/completions are ready to be used as a training dataset. This is why the training process selects completions based on their status: Filter tasks by tags (if defined in Training Parameters window, otherwise all tasks are considered) For completed tasks, completions to be taken into account are also selected based on the following criteria: If a task has a completion accepted by a reviewer this is selected for training and all others are ignored Completions rejected by a Reviewer are not used for training If no reviewer is assigned to a task that has multiple submitted completions the most recent completion is selected for training purpose Assertion Status Projects NER configurations for the healthcare domain are often mixed with Assertion Status labels. In this case Annotation Lab offers support for training both types of models in one go. After the training is complete, the models will be listed in the Spark NLP Pipeline Config. On mouse over the model name in the Spark NLP pipeline config, the user can see more information about the model such as when it was trained and if the training was manually initiated or by the Active Learning process. Once the model(s) has been trained, the project configuration will be automatically updated to reference the new model for prediction. Notice below, for the Assertion Status **** tag the addition of model attribute to indicate which model will be used for task preannotation for this label. &lt;Label value=&quot;Absent&quot; assertion=&quot;true&quot; model=&quot;assertion_jsl_annotation_manual.model&quot;/&gt; &lt;Label value=&quot;Past&quot; assertion=&quot;true&quot; model=&quot;assertion_jsl_annotation_manual.model&quot;/&gt; It is not possible to mark a label as an Assertion Status label and use a NER model to predict it. A validation error is shown in the Interface Preview in case an invalid Assertion model is used. The Annotation Lab only allows the use of one single Assertion Status model in the same project. Classification Project Models Training Annotation Lab supports two types of classification training Single Choice Classification and Multi-Choice Classification. For doing so, it uses three important attributes of the **** tag to drive the Classification Models training and preannotation. Those are **name**, **choice** and **train**. Attribute name The attribute name allows the naming of the different choices present in the project configuration, and thus the training of separate models based on the same project annotations. For example, in the sample configuration illustrated below, the name=”age” attribute, tells the system to only consider age-related classification information when training an Age Classifier. The value specified by the name attribute is also used to name the resulting Classification model (classification_age_annotation_manual). Attribute choice The choice attribute specifies the type of model that will be trained: multiple or single. For example, in the Labeling Config below, Age and Gender are Single Choice Classification categories while the Smoking Status is Multi-Choice Classification. Depending upon the value of this attribute, the respective model will be trained as a Single Choice Classifier or Multi-Choice Classifier. &lt;View&gt; &lt;View style=&quot;overflow: auto;&quot;&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;/&gt; &lt;/View&gt; &lt;Header value=&quot;Smoking Status&quot;/&gt; &lt;Choices name=&quot;smokingstatus&quot; toName=&quot;text&quot; choice=&quot;multiple&quot; showInLine=&quot;true&quot;&gt; &lt;Choice value=&quot;Smoker&quot;/&gt; &lt;Choice value=&quot;Past Smoker&quot;/&gt; &lt;Choice value=&quot;Nonsmoker&quot;/&gt; &lt;/Choices&gt; &lt;Header value=&quot;Age&quot;/&gt; &lt;Choices name=&quot;age&quot; toName=&quot;text&quot; choice=&quot;single&quot; showInLine=&quot;true&quot;&gt; &lt;Choice value=&quot;Child (less than 18y)&quot; hotkey=&quot;c&quot;/&gt; &lt;Choice value=&quot;Adult (19-50y)&quot; hotkey=&quot;a&quot;/&gt; &lt;Choice value=&quot;Aged (50+y)&quot; hotkey=&quot;o&quot;/&gt; &lt;/Choices&gt; &lt;Header value=&quot;Gender&quot;/&gt; &lt;Choices name=&quot;gender&quot; toName=&quot;text&quot; choice=&quot;single&quot; showInLine=&quot;true&quot;&gt; &lt;Choice value=&quot;Female&quot; hotkey=&quot;f&quot;/&gt; &lt;Choice value=&quot;Male&quot; hotkey=&quot;m&quot;/&gt; &lt;/Choices&gt; &lt;/View&gt; Attribute train This version of Annotation Lab restricts the training of two or more Classification Models at the same time. If there are multiple Classification categories in a project (like the one above), only the category whose name comes first in alphabetical order will be trained by default. In the above example, based on the value of the name attribute, we conclude that the Age classifier model is trained. The model to be trained can also be specified by setting the train=”true” attribute for the targeted tag (like the one defined in Gender category below). &lt;View&gt; &lt;View style=&quot;overflow: auto;&quot;&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;/&gt; &lt;/View&gt; &lt;Header value=&quot;Smoking Status&quot;/&gt; &lt;Choices name=&quot;smokingstatus&quot; toName=&quot;text&quot; choice=&quot;multiple&quot; showInLine=&quot;true&quot;&gt; ... &lt;/Choices&gt; &lt;Header value=&quot;Age&quot;/&gt; &lt;Choices name=&quot;age&quot; toName=&quot;text&quot; choice=&quot;single&quot; showInLine=&quot;true&quot;&gt; ... &lt;/Choices&gt; &lt;Header value=&quot;Gender&quot;/&gt; &lt;Choices name=&quot;gender&quot; train=&quot;true&quot; toName=&quot;text&quot; choice=&quot;single&quot; showInLine=&quot;true&quot;&gt; ... &lt;/Choices&gt; &lt;/View&gt; The trained classification models are also available on the Spark NLP pipeline config list. Mixed Projects If a project is set up to include Classification, Named Entity Recognition and Assertion Status labels and the three kinds of annotations are present in the training data, it is possible to train three models: one for Named Entity Recognition, one for Assertion Status, and one for Classification at the same time. The training logs from all three trainings can be downloaded at once by clicking the download button present in the Training section of the Setup Page. The newly trained models will be added to the Spark NLP pipeline config. Active Learning Project Owners or Managers can enable the Active Learning feature by clicking on the corresponding Switch (item 7 on the above image) available on Model Training tab. If this feature is enabled, the NER training gets triggered automatically on every 50 new completions. It is also possible to change the completions frequency by dropdown (8) which is visible only when Active Learning is enabled. While enabling this feature, users are asked whether they want to deploy the newly trained model right after the training process or not. If the user chooses not to automatically deploy the newly trained model, this can be done on demand by navigating to the Spark NLP pipeline config and filtering the model by name of the project (3) and select that new model trained by Active Learning. This will update the Labeling Config (name of the model in tag is changed). Hovering on each trained model will show the training date and time. If the user opts for deploying the model after the training, the Project Configuration is automatically updated for each label that is not associated with a pretrained Spark NLP model, the model information is updated with the name of the new model. If there is any mistake in the name of models, the validation error is displayed in the Interface Preview Section present on the right side of the Labeling Config area.",
    "url": "/docs/en/alab/active_learning",
    "relUrl": "/docs/en/alab/active_learning"
  },
  "3": {
    "id": "3",
    "title": "Analytics",
    "content": "Inter-Annotator Agreement Charts (IAA) Inter Annotator Agreement charts can be used by annotators, reviewers, and managers for identifying contradictions or disagreements within the stared completions. When multiple annotators work on the same tasks, IAA charts are handy to measure how well the annotations created by different annotators align. IAA chart can also be used to identify outliers in the labeled data or to compare manual annotations with model predictions. To get access to the IAA charts, navigate on the third tab of the Analytics Dashboard of NER projects, called “Inter-Annotator Agreement”. Several charts should appear on the screen with a default selection of annotators to compare. The dropdown boxes below each chart allow you to change annotators for comparison purposes. It is also possible to download the data generated for some charts in CSV format by clicking the download button present at the bottom right corner of each of them. Note: Only the Submitted and starred (Ground Truth) completions are used to render these charts. Shows agreement with Label wise breakdown Shows whether two Annotators agree on every annotated Chunks Shows agreement between one Annotator and Preannotation result on every annotated Chunks Shows Labels of each Chunk by one Annotator and context in the tasks Shows the frequency of Chunk-Label by one Annotator Shows the frequency of Chunk-Annotatory by one Label",
    "url": "/docs/en/alab/analytics",
    "relUrl": "/docs/en/alab/analytics"
  },
  "4": {
    "id": "4",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/analyze_financial_models",
    "relUrl": "/analyze_financial_models"
  },
  "5": {
    "id": "5",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/analyze_medical_text_german",
    "relUrl": "/analyze_medical_text_german"
  },
  "6": {
    "id": "6",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/analyze_medical_text_spanish",
    "relUrl": "/analyze_medical_text_spanish"
  },
  "7": {
    "id": "7",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/analyze_non_english_medical_text",
    "relUrl": "/analyze_non_english_medical_text"
  },
  "8": {
    "id": "8",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/analyze_non_english_text_documents",
    "relUrl": "/analyze_non_english_text_documents"
  },
  "9": {
    "id": "9",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/analyze_spelling_grammar",
    "relUrl": "/analyze_spelling_grammar"
  },
  "10": {
    "id": "10",
    "title": "Text Annotation",
    "content": "Overview The Annotator Lab is designed to keep a human expert as productive as possible. It minimizes the number of mouse clicks, keystrokes, and eye movements in the main workflow, based on iterative feedback from daily users. Keyboard shortcuts are supported for all annotations – this enables having one hand on keyboard, one hand on mouse, and eyes on screen at all time. One-click completion and automated switching to the next task keeps experts in the loop. On the upper side of the Labeling screen, you can find the list of labels defined for the project. In the center of the screen the content of the task is displayed. On the right side there are several widgets: Completions Predictions Results Completions A completion is a list of annotations manually defined by a user for a given task. When the work on a task is done (e.g. all entities have been highlighted in the document or the task has been assigned one or more classes in the case of classification projects) the user clicks on the Save button. Starting Annotation Lab 1.2.0, we introduced the idea of completion submission. In the past, annotators could change or delete completions as many times as they wanted with no restriction. From now on, a submitted completion is no longer editable and cannot be deleted. Creating a new copy of the submitted completion is the only option to edit it. An annotator can modify or delete his/her completions only if the completions are not submitted yet. This is an important feature for ensuring a complete audit trail of all user actions. Now, it is possible to track the history and details of any deleted completions, which was not possible in previous releases. This means it is possible to see the name of the completion creator, date of creation, and deletion. Predictions A prediction is a list of annotations created automatically, via the use of Spark NLP pretrained models. Predictions are created using the “Preannotate” button form the Task view. Predictions are read only - users can see them but cannot modify them in any way. Results The results widget has two sections. The first section - Regions - gives a list overview of all annotated chunks. When you click on one annotation it gets automatically highlighted in the document. Annotations can be edited or removed if necessary. The second section - Relations - lists all the relations that have been labeled. When the user moves the mouse over one relation it is highlighted in the document. NER Labels To extract information using NER labels, first click on the label to select it or press the shortcut key assigned to it and then, with the mouse, select the relevant part of the text. Wrong extractions can be easily edited by clicking on them to select them and then by selecting the new label you want to assign to the text. For deleting a label, select it by clicking on it and press backspace. Assertion Labels To add an assertion label to an extracted entity, select the label and use it to do the same exact extraction as the NER label. After this, the extracted entity will have two labels: one for NER and one for assertion. In the example below, the chunks “heart disease”, “kidney disease”, “stroke” etc. ware extracted using first a blue yellow label and then a red assertion label (Absent). Relation Extraction Creating relations with the Annotation Lab is very simple. First select one labeled entity, then press r and select the second labeled entity. You can add a label to the relation, change its direction or delete it using the contextual menu displayed next to the relation arrow or using the relation box. Pre-annotations When you setup a project to use existing Spark NLP models for pre-annotation, you can run the designated models on all of your tasks by pressing the Preannotate button located on the upper side of the task view. As a result, all predicted labels for a given task will be available in the Prediction widget, on the main annotation screen. The predictions are not editable, you can only view them and navigate them or compare them with older predictions. However, you can create a new completion based on a given prediction. All labels and relations from such a new completion are now editable.",
    "url": "/docs/en/alab/annotation",
    "relUrl": "/docs/en/alab/annotation"
  },
  "11": {
    "id": "11",
    "title": "Annotators",
    "content": "How to read this section All annotators in Spark NLP share a common interface, this is: Annotation: Annotation(annotatorType, begin, end, result, meta-data, embeddings) AnnotatorType: some annotators share a type. This is not only figurative, but also tells about the structure of the metadata map in the Annotation. This is the one referred in the input and output of annotators. Inputs: Represents how many and which annotator types are expected in setInputCols(). These are column names of output of other annotators in the DataFrames. Output Represents the type of the output in the column setOutputCol(). There are two types of Annotators: Approach: AnnotatorApproach extend Estimators, which are meant to be trained through fit() Model: AnnotatorModel extend from Transformers, which are meant to transform DataFrames through transform() Model suffix is explicitly stated when the annotator is the result of a training process. Some annotators, such as Tokenizer are transformers, but do not contain the word Model since they are not trained annotators. Model annotators have a pretrained() on it’s static object, to retrieve the public pre-trained version of a model. pretrained(name, language, extra_location) -&gt; by default, pre-trained will bring a default model, sometimes we offer more than one model, in this case, you may have to use name, language or extra location to download them. Available Annotators Annotator Description Version BigTextMatcher Annotator to match exact phrases (by token) provided in a file against a Document. Opensource Chunk2Doc Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result. Opensource ChunkEmbeddings This annotator utilizes WordEmbeddings, BertEmbeddings etc. to generate chunk embeddings from either Chunker, NGramGenerator, or NerConverter outputs. Opensource ChunkTokenizer Tokenizes and flattens extracted NER chunks. Opensource Chunker This annotator matches a pattern of part-of-speech tags in order to return meaningful phrases from document. Opensource ClassifierDL ClassifierDL for generic Multi-class Text Classification. Opensource ContextSpellChecker Implements a deep-learning based Noisy Channel Model Spell Algorithm. Opensource DateMatcher Matches standard date formats into a provided format. Opensource DependencyParser Unlabeled parser that finds a grammatical relation between two words in a sentence. Opensource Doc2Chunk Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol. Opensource Doc2Vec Word2Vec model that creates vector representations of words in a text corpus. Opensource DocumentAssembler Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. Opensource DocumentNormalizer Annotator which normalizes raw text from tagged text, e.g. scraped web pages or xml documents, from document type columns into Sentence. Opensource EntityRuler Fits an Annotator to match exact strings or regex patterns provided in a file against a Document and assigns them an named entity. Opensource EmbeddingsFinisher Extracts embeddings from Annotations into a more easily usable form. Opensource Finisher Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. Opensource GraphExtraction Extracts a dependency graph between entities. Opensource GraphFinisher Helper class to convert the knowledge graph from GraphExtraction into a generic format, such as RDF. Opensource LanguageDetectorDL Language Identification and Detection by using CNN and RNN architectures in TensorFlow. Opensource Lemmatizer Finds lemmas out of words with the objective of returning a base dictionary word. Opensource MultiClassifierDL Multi-label Text Classification. Opensource MultiDateMatcher Matches standard date formats into a provided format. Opensource NGramGenerator A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). Opensource NerConverter Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. Opensource NerCrf Extracts Named Entities based on a CRF Model. Opensource NerDL This Named Entity recognition annotator is a generic NER model based on Neural Networks. Opensource NerOverwriter Overwrites entities of specified strings. Opensource Normalizer Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary. Opensource NorvigSweeting Spellchecker Retrieves tokens and makes corrections automatically if not found in an English dictionary. Opensource POSTagger (Part of speech tagger) Averaged Perceptron model to tag words part-of-speech. Opensource RecursiveTokenizer Tokenizes raw text recursively based on a handful of definable rules. Opensource RegexMatcher Uses a reference file to match a set of regular expressions and associate them with a provided identifier. Opensource RegexTokenizer A tokenizer that splits text by a regex pattern. Opensource SentenceDetector Detects sentence boundaries using any provided approach. Opensource SentenceDetectorDL Detects sentence boundaries using a deep learning approach. Opensource SentenceEmbeddings Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols). Opensource SentimentDL Annotator for multi-class sentiment analysis. Opensource SentimentDetector Rule based sentiment detector, which calculates a score based on predefined keywords. Opensource Stemmer Returns hard-stems out of words with the objective of retrieving the meaningful part of the word. Opensource StopWordsCleaner This annotator takes a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences. Opensource SymmetricDelete Spellchecker Symmetric Delete spelling correction algorithm. Opensource TextMatcher Matches exact phrases (by token) provided in a file against a Document. Opensource Token2Chunk Converts TOKEN type Annotations to CHUNK type. Opensource TokenAssembler This transformer reconstructs a DOCUMENT type annotation from tokens, usually after these have been normalized, lemmatized, normalized, spell checked, etc, in order to use this document annotation in further annotators. Opensource Tokenizer Tokenizes raw text into word pieces, tokens. Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. Opensource TypedDependencyParser Labeled parser that finds a grammatical relation between two words in a sentence. Opensource ViveknSentiment Sentiment analyser inspired by the algorithm by Vivek Narayanan. Opensource WordEmbeddings Word Embeddings lookup annotator that maps tokens to vectors. Opensource WordSegmenter Tokenizes non-english or non-whitespace separated texts. Opensource YakeKeywordExtraction Unsupervised, Corpus-Independent, Domain and Language-Independent and Single-Document keyword extraction. Opensource Available Transformers Additionally, these transformers are available to generate embeddings. Transformer Description Version AlbertEmbeddings ALBERT: A Lite BERT for Self-supervised Learning of Language Representations Opensource AlbertForTokenClassification AlbertForTokenClassification can load ALBERT Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource BertEmbeddings Token-level embeddings using BERT. BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture. Opensource BertForSequenceClassification Bert Models with sequence classification/regression head on top. Opensource BertForTokenClassification BertForTokenClassification can load Bert Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource BertSentenceEmbeddings Sentence-level embeddings using BERT. BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture. Opensource DistilBertEmbeddings DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. Opensource DistilBertForSequenceClassification DistilBertForSequenceClassification can load DistilBERT Models with sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for multi-class document classification tasks. Opensource DistilBertForTokenClassification DistilBertForTokenClassification can load DistilBERT Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource ElmoEmbeddings Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark. Opensource LongformerEmbeddings Longformer is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. Opensource LongformerForTokenClassification LongformerForTokenClassification can load Longformer Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource MarianTransformer Marian is an efficient, free Neural Machine Translation framework written in pure C++ with minimal dependencies. Opensource RoBertaEmbeddings RoBERTa: A Robustly Optimized BERT Pretraining Approach Opensource RoBertaForTokenClassification RoBertaForTokenClassification can load RoBERTa Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource RoBertaSentenceEmbeddings Sentence-level embeddings using RoBERTa. Opensource T5Transformer T5 reconsiders all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Opensource UniversalSentenceEncoder The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks. Opensource XlmRoBertaEmbeddings XlmRoBerta is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl Opensource XlmRoBertaForTokenClassification XlmRoBertaForTokenClassification can load XLM-RoBERTa Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource XlmRoBertaSentenceEmbeddings Sentence-level embeddings using XLM-RoBERTa. Opensource XlnetEmbeddings XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Opensource XlnetForTokenClassification XlnetForTokenClassification can load XLNet Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource BigTextMatcher Approach Model Annotator to match exact phrases (by token) provided in a file against a Document. A text file of predefined phrases must be provided with setStoragePath. In contrast to the normal TextMatcher, the BigTextMatcher is designed for large corpora. For extended examples of usage, see the BigTextMatcherTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: BigTextMatcher Scala API: BigTextMatcher Source: BigTextMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the entities file is of the form # # ... # dolore magna aliqua # lorem ipsum dolor. sit # laborum # ... # # where each line represents an entity phrase to be extracted. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) data = spark.createDataFrame([[&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;]]).toDF(&quot;text&quot;) entityExtractor = BigTextMatcher() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setStoragePath(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(False) pipeline = Pipeline().setStages([documentAssembler, tokenizer, entityExtractor]) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity)&quot;).show(truncate=False) +--+ |col | +--+ |[chunk, 6, 24, dolore magna aliqua, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 53, 59, laborum, [sentence -&gt; 0, chunk -&gt; 1], []] | +--+ // In this example, the entities file is of the form // // ... // dolore magna aliqua // lorem ipsum dolor. sit // laborum // ... // // where each line represents an entity phrase to be extracted. import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.BigTextMatcher import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val data = Seq(&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;).toDF(&quot;text&quot;) val entityExtractor = new BigTextMatcher() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setStoragePath(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(false) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer, entityExtractor)) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity)&quot;).show(false) +--+ |col | +--+ |[chunk, 6, 24, dolore magna aliqua, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 53, 59, laborum, [sentence -&gt; 0, chunk -&gt; 1], []] | +--+ Instantiated model of the BigTextMatcher. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: BigTextMatcherModel Scala API: BigTextMatcherModel Source: BigTextMatcherModel Chunk2Doc Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result. For more extended examples on document pre-processing see the Spark NLP Workshop. Input Annotator Types: CHUNK Output Annotator Type: DOCUMENT Python API: Chunk2Doc Scala API: Chunk2Doc Source: Chunk2Doc Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline # Location entities are extracted and converted back into `DOCUMENT` type for further processing data = spark.createDataFrame([[1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;]]).toDF(&quot;id&quot;, &quot;text&quot;) # Extracts Named Entities amongst other things pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) chunkToDoc = Chunk2Doc().setInputCols(&quot;entities&quot;).setOutputCol(&quot;chunkConverted&quot;) explainResult = pipeline.transform(data) result = chunkToDoc.transform(explainResult) result.selectExpr(&quot;explode(chunkConverted)&quot;).show(truncate=False) ++ |col | ++ |[document, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []] | |[document, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]| ++ // Location entities are extracted and converted back into `DOCUMENT` type for further processing import spark.implicits._ import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.Chunk2Doc val data = Seq((1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;)).toDF(&quot;id&quot;, &quot;text&quot;) // Extracts Named Entities amongst other things val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) val chunkToDoc = new Chunk2Doc().setInputCols(&quot;entities&quot;).setOutputCol(&quot;chunkConverted&quot;) val explainResult = pipeline.transform(data) val result = chunkToDoc.transform(explainResult) result.selectExpr(&quot;explode(chunkConverted)&quot;).show(false) ++ |col | ++ |[document, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []] | |[document, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]| ++ ChunkEmbeddings This annotator utilizes WordEmbeddings, BertEmbeddings etc. to generate chunk embeddings from either Chunker, NGramGenerator, or NerConverter outputs. For extended examples of usage, see the Spark NLP Workshop and the ChunkEmbeddingsTestSpec. Input Annotator Types: CHUNK, WORD_EMBEDDINGS Output Annotator Type: WORD_EMBEDDINGS Python API: ChunkEmbeddings Scala API: ChunkEmbeddings Source: ChunkEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Extract the Embeddings from the NGrams documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) nGrams = NGramGenerator() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;chunk&quot;) .setN(2) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) # Convert the NGram chunks into Word Embeddings chunkEmbeddings = ChunkEmbeddings() .setInputCols([&quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;chunk_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) pipeline = Pipeline() .setStages([ documentAssembler, sentence, tokenizer, nGrams, embeddings, chunkEmbeddings ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk_embeddings) as result&quot;) .select(&quot;result.annotatorType&quot;, &quot;result.result&quot;, &quot;result.embeddings&quot;) .show(5, 80) ++-+--+ | annotatorType| result| embeddings| ++-+--+ |word_embeddings| This is|[-0.55661, 0.42829502, 0.86661, -0.409785, 0.06316501, 0.120775, -0.0732005, ...| |word_embeddings| is a|[-0.40674996, 0.22938299, 0.50597, -0.288195, 0.555655, 0.465145, 0.140118, 0...| |word_embeddings|a sentence|[0.17417, 0.095253006, -0.0530925, -0.218465, 0.714395, 0.79860497, 0.0129999...| |word_embeddings|sentence .|[0.139705, 0.177955, 0.1887775, -0.45545, 0.20030999, 0.461557, -0.07891501, ...| ++-+--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.{NGramGenerator, Tokenizer} import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings import org.apache.spark.ml.Pipeline // Extract the Embeddings from the NGrams val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val nGrams = new NGramGenerator() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;chunk&quot;) .setN(2) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) // Convert the NGram chunks into Word Embeddings val chunkEmbeddings = new ChunkEmbeddings() .setInputCols(&quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;chunk_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentence, tokenizer, nGrams, embeddings, chunkEmbeddings )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk_embeddings) as result&quot;) .select(&quot;result.annotatorType&quot;, &quot;result.result&quot;, &quot;result.embeddings&quot;) .show(5, 80) ++-+--+ | annotatorType| result| embeddings| ++-+--+ |word_embeddings| This is|[-0.55661, 0.42829502, 0.86661, -0.409785, 0.06316501, 0.120775, -0.0732005, ...| |word_embeddings| is a|[-0.40674996, 0.22938299, 0.50597, -0.288195, 0.555655, 0.465145, 0.140118, 0...| |word_embeddings|a sentence|[0.17417, 0.095253006, -0.0530925, -0.218465, 0.714395, 0.79860497, 0.0129999...| |word_embeddings|sentence .|[0.139705, 0.177955, 0.1887775, -0.45545, 0.20030999, 0.461557, -0.07891501, ...| ++-+--+ ChunkTokenizer Approach Model Tokenizes and flattens extracted NER chunks. The ChunkTokenizer will split the extracted NER CHUNK type Annotations and will create TOKEN type Annotations. The result is then flattened, resulting in a single array. For extended examples of usage, see the ChunkTokenizerTestSpec. Input Annotator Types: CHUNK Output Annotator Type: TOKEN Python API: ChunkTokenizer Scala API: ChunkTokenizer Source: ChunkTokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) entityExtractor = TextMatcher() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setEntities(&quot;src/test/resources/entity-extractor/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) chunkTokenizer = ChunkTokenizer() .setInputCols([&quot;entity&quot;]) .setOutputCol(&quot;chunk_token&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, entityExtractor, chunkTokenizer ]) data = spark.createDataFrame([[ &quot;Hello world, my name is Michael, I am an artist and I work at Benezar&quot;, &quot;Robert, an engineer from Farendell, graduated last year. The other one, Lucas, graduated last week.&quot; ]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;entity.result as entity&quot; , &quot;chunk_token.result as chunk_token&quot;).show(truncate=False) +--++ |entity |chunk_token | +--++ |[world, Michael, work at Benezar] |[world, Michael, work, at, Benezar] | |[engineer from Farendell, last year, last week]|[engineer, from, Farendell, last, year, last, week]| +--++ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotators.{ChunkTokenizer, TextMatcher, Tokenizer} import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;src/test/resources/entity-extractor/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) val chunkTokenizer = new ChunkTokenizer() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;chunk_token&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, entityExtractor, chunkTokenizer )) val data = Seq( &quot;Hello world, my name is Michael, I am an artist and I work at Benezar&quot;, &quot;Robert, an engineer from Farendell, graduated last year. The other one, Lucas, graduated last week.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;entity.result as entity&quot; , &quot;chunk_token.result as chunk_token&quot;).show(false) +--++ |entity |chunk_token | +--++ |[world, Michael, work at Benezar] |[world, Michael, work, at, Benezar] | |[engineer from Farendell, last year, last week]|[engineer, from, Farendell, last, year, last, week]| +--++ Instantiated model of the ChunkTokenizer. For usage and examples see the documentation of the main class. Input Annotator Types: CHUNK Output Annotator Type: TOKEN Python API: ChunkTokenizerModel Scala API: ChunkTokenizerModel Source: ChunkTokenizerModel Chunker This annotator matches a pattern of part-of-speech tags in order to return meaningful phrases from document. Extracted part-of-speech tags are mapped onto the sentence, which can then be parsed by regular expressions. The part-of-speech tags are wrapped by angle brackets &lt;&gt; to be easily distinguishable in the text itself. This example sentence will result in the form: &quot;Peter Pipers employees are picking pecks of pickled peppers.&quot; &quot;&lt;NNP&gt;&lt;NNP&gt;&lt;NNS&gt;&lt;VBP&gt;&lt;VBG&gt;&lt;NNS&gt;&lt;IN&gt;&lt;JJ&gt;&lt;NNS&gt;&lt;.&gt;&quot; To then extract these tags, regexParsers need to be set with e.g.: val chunker = new Chunker() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;)) When defining the regular expressions, tags enclosed in angle brackets are treated as groups, so here specifically &quot;&lt;NNP&gt;+&quot; means 1 or more nouns in succession. Additional patterns can also be set with addRegexParsers. For more extended examples see the Spark NLP Workshop) and the ChunkerTestSpec. Input Annotator Types: DOCUMENT, POS Output Annotator Type: CHUNK Python API: Chunker Scala API: Chunker Source: Chunker Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) POSTag = PerceptronModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) chunker = Chunker() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;]) pipeline = Pipeline() .setStages([ documentAssembler, sentence, tokenizer, POSTag, chunker ]) data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(truncate=False) +-+ |result | +-+ |[chunk, 0, 11, Peter Pipers, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 13, 21, employees, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 35, 39, pecks, [sentence -&gt; 0, chunk -&gt; 2], []] | |[chunk, 52, 58, peppers, [sentence -&gt; 0, chunk -&gt; 3], []] | +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotators.{Chunker, Tokenizer} import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val POSTag = PerceptronModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val chunker = new Chunker() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;)) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentence, tokenizer, POSTag, chunker )) val data = Seq(&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(false) +-+ |result | +-+ |[chunk, 0, 11, Peter Pipers, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 13, 21, employees, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 35, 39, pecks, [sentence -&gt; 0, chunk -&gt; 2], []] | |[chunk, 52, 58, peppers, [sentence -&gt; 0, chunk -&gt; 3], []] | +-+ ClassifierDL Approach Model Trains a ClassifierDL for generic Multi-class Text Classification. ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 100 classes. For instantiated/pretrained models, see ClassifierDLModel. For extended examples of usage, see the Spark NLP Workshop [1] [2] and the ClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Note: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. UniversalSentenceEncoder, BertSentenceEmbeddings, or SentenceEmbeddings can be used for the inputCol Python API: ClassifierDLApproach Scala API: ClassifierDLApproach Source: ClassifierDLApproach Show Example PythonScala # In this example, the training data `&quot;sentiment.csv&quot;` has the form of # # text,label # This movie is the best movie I have wached ever! In my opinion this movie can win an award.,0 # This was a terrible movie! The acting was bad really bad!,1 # ... # # Then traning can be done like so: import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline smallCorpus = spark.read.option(&quot;header&quot;,&quot;True&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) docClassifier = ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(5e-3) .setDropout(0.5) pipeline = Pipeline() .setStages( [ documentAssembler, useEmbeddings, docClassifier ] ) pipelineModel = pipeline.fit(smallCorpus) // In this example, the training data `&quot;sentiment.csv&quot;` has the form of // // text,label // This movie is the best movie I have wached ever! In my opinion this movie can win an award.,0 // This was a terrible movie! The acting was bad really bad!,1 // ... // // Then traning can be done like so: import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach import org.apache.spark.ml.Pipeline val smallCorpus = spark.read.option(&quot;header&quot;,&quot;true&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val docClassifier = new ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(5e-3f) .setDropout(0.5f) val pipeline = new Pipeline() .setStages( Array( documentAssembler, useEmbeddings, docClassifier ) ) val pipelineModel = pipeline.fit(smallCorpus) ClassifierDL for generic Multi-class Text Classification. ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 100 classes. This is the instantiated model of the ClassifierDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val classifierDL = ClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;classification&quot;) The default model is &quot;classifierdl_use_trec6&quot;, if no name is provided. It uses embeddings from the UniversalSentenceEncoder and is trained on the TREC-6 dataset. For available pretrained models please see the Models Hub. For extended examples of usage, see the Spark NLP Workshop and the ClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: ClassifierDLModel Scala API: ClassifierDLModel Source: ClassifierDLModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) sarcasmDL = ClassifierDLModel.pretrained(&quot;classifierdl_use_sarcasm&quot;) .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sarcasm&quot;) pipeline = Pipeline() .setStages([ documentAssembler, sentence, useEmbeddings, sarcasmDL ]) data = spark.createDataFrame([ [&quot;I&#39;m ready!&quot;], [&quot;If I could put into words how much I love waking up at 6 am on Mondays I would.&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(arrays_zip(sentence, sarcasm)) as out&quot;) .selectExpr(&quot;out.sentence.result as sentence&quot;, &quot;out.sarcasm.result as sarcasm&quot;) .show(truncate=False) +-+-+ |sentence |sarcasm| +-+-+ |I&#39;m ready! |normal | |If I could put into words how much I love waking up at 6 am on Mondays I would.|sarcasm| +-+-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLModel import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val sarcasmDL = ClassifierDLModel.pretrained(&quot;classifierdl_use_sarcasm&quot;) .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sarcasm&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentence, useEmbeddings, sarcasmDL )) val data = Seq( &quot;I&#39;m ready!&quot;, &quot;If I could put into words how much I love waking up at 6 am on Mondays I would.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(arrays_zip(sentence, sarcasm)) as out&quot;) .selectExpr(&quot;out.sentence.result as sentence&quot;, &quot;out.sarcasm.result as sarcasm&quot;) .show(false) +-+-+ |sentence |sarcasm| +-+-+ |I&#39;m ready! |normal | |If I could put into words how much I love waking up at 6 am on Mondays I would.|sarcasm| +-+-+ ContextSpellChecker Approach Model Trains a deep-learning based Noisy Channel Model Spell Algorithm. Correction candidates are extracted combining context information and word information. For instantiated/pretrained models, see ContextSpellCheckerModel. Spell Checking is a sequence to sequence mapping problem. Given an input sequence, potentially containing a certain number of errors, ContextSpellChecker will rank correction sequences according to three things: Different correction candidates for each word — word level. The surrounding text of each word, i.e. it’s context — sentence level. The relative cost of different correction candidates according to the edit operations at the character level it requires — subword level. For an in-depth explanation of the module see the article Applying Context Aware Spell Checking in Spark NLP. For extended examples of usage, see the article Training a Contextual Spell Checker for Italian Language, the Spark NLP Workshop and the ContextSpellCheckerTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: ContextSpellCheckerApproach Scala API: ContextSpellCheckerApproach Source: ContextSpellCheckerApproach Show Example PythonScala # For this example, we use the first Sherlock Holmes book as the training dataset. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) spellChecker = ContextSpellCheckerApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;corrected&quot;) .setWordMaxDistance(3) .setBatchSize(24) .setEpochs(8) .setLanguageModelClasses(1650) # dependant on vocabulary size # .addVocabClass(&quot;_NAME_&quot;, names) # Extra classes for correction could be added like this pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker ]) path = &quot;sherlockholmes.txt&quot; dataset = spark.read.text(path) .toDF(&quot;text&quot;) pipelineModel = pipeline.fit(dataset) // For this example, we use the first Sherlock Holmes book as the training dataset. import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val spellChecker = new ContextSpellCheckerApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;corrected&quot;) .setWordMaxDistance(3) .setBatchSize(24) .setEpochs(8) .setLanguageModelClasses(1650) // dependant on vocabulary size // .addVocabClass(&quot;_NAME_&quot;, names) // Extra classes for correction could be added like this val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker )) val path = &quot;src/test/resources/spell/sherlockholmes.txt&quot; val dataset = spark.sparkContext.textFile(path) .toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(dataset) Implements a deep-learning based Noisy Channel Model Spell Algorithm. Correction candidates are extracted combining context information and word information. Spell Checking is a sequence to sequence mapping problem. Given an input sequence, potentially containing a certain number of errors, ContextSpellChecker will rank correction sequences according to three things: Different correction candidates for each word — word level. The surrounding text of each word, i.e. it’s context — sentence level. The relative cost of different correction candidates according to the edit operations at the character level it requires — subword level. For an in-depth explanation of the module see the article Applying Context Aware Spell Checking in Spark NLP. This is the instantiated model of the ContextSpellCheckerApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val spellChecker = ContextSpellCheckerModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;checked&quot;) The default model is &quot;spellcheck_dl&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Spark NLP Workshop and the ContextSpellCheckerTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: ContextSpellCheckerModel Scala API: ContextSpellCheckerModel Source: ContextSpellCheckerModel DateMatcher Matches standard date formats into a provided format. Reads from different forms of date and time expressions and converts them to a provided date format. Extracts only one date per document. Use with sentence detector to find matches in each sentence. To extract multiple dates from a document, please use the MultiDateMatcher. Reads the following kind of dates: &quot;1978-01-28&quot;, &quot;1984/04/02,1/02/1980&quot;, &quot;2/28/79&quot;, &quot;The 31st of April in the year 2008&quot;, &quot;Fri, 21 Nov 1997&quot;, &quot;Jan 21, ‘97&quot;, &quot;Sun&quot;, &quot;Nov 21&quot;, &quot;jan 1st&quot;, &quot;next thursday&quot;, &quot;last wednesday&quot;, &quot;today&quot;, &quot;tomorrow&quot;, &quot;yesterday&quot;, &quot;next week&quot;, &quot;next month&quot;, &quot;next year&quot;, &quot;day after&quot;, &quot;the day before&quot;, &quot;0600h&quot;, &quot;06:00 hours&quot;, &quot;6pm&quot;, &quot;5:30 a.m.&quot;, &quot;at 5&quot;, &quot;12:59&quot;, &quot;23:59&quot;, &quot;1988/11/23 6pm&quot;, &quot;next week at 7.30&quot;, &quot;5 am tomorrow&quot; For example &quot;The 31st of April in the year 2008&quot; will be converted into 2008/04/31. Pretrained pipelines are available for this module, see Pipelines. For extended examples of usage, see the Spark NLP Workshop and the DateMatcherTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DATE Python API: DateMatcher Scala API: DateMatcher Source: DateMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) date = DateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) pipeline = Pipeline().setStages([ documentAssembler, date ]) data = spark.createDataFrame([[&quot;Fri, 21 Nov 1997&quot;], [&quot;next week at 7.30&quot;], [&quot;see you a day after&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;date&quot;).show(truncate=False) +-+ |date | +-+ |[[date, 5, 15, 1997/11/21, [sentence -&gt; 0], []]] | |[[date, 0, 8, 2020/01/18, [sentence -&gt; 0], []]] | |[[date, 10, 18, 2020/01/12, [sentence -&gt; 0], []]]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.DateMatcher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val date = new DateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, date )) val data = Seq(&quot;Fri, 21 Nov 1997&quot;, &quot;next week at 7.30&quot;, &quot;see you a day after&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;date&quot;).show(false) +-+ |date | +-+ |[[date, 5, 15, 1997/11/21, [sentence -&gt; 0], []]] | |[[date, 0, 8, 2020/01/18, [sentence -&gt; 0], []]] | |[[date, 10, 18, 2020/01/12, [sentence -&gt; 0], []]]| +-+ DependencyParser Approach Model Trains an unlabeled parser that finds a grammatical relations between two words in a sentence. For instantiated/pretrained models, see DependencyParserModel. Dependency parser provides information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The required training data can be set in two different ways (only one can be chosen for a particular model): Dependency treebank in the Penn Treebank format set with setDependencyTreeBank Dataset in the CoNLL-U format set with setConllU Apart from that, no additional training data is needed. See DependencyParserApproachTestSpec for further reference on how to use this API. Input Annotator Types: DOCUMENT, POS, TOKEN Output Annotator Type: DEPENDENCY Python API: DependencyParserApproach Scala API: DependencyParserApproach Source: DependencyParserApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) dependencyParserApproach = DependencyParserApproach() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) .setDependencyTreeBank(&quot;src/test/resources/parser/unlabeled/dependency_treebank&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, posTagger, dependencyParserApproach ]) # Additional training data is not needed, the dependency parser relies on the dependency tree bank / CoNLL-U only. emptyDataSet = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(emptyDataSet) import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val dependencyParserApproach = new DependencyParserApproach() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) .setDependencyTreeBank(&quot;src/test/resources/parser/unlabeled/dependency_treebank&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, posTagger, dependencyParserApproach )) // Additional training data is not needed, the dependency parser relies on the dependency tree bank / CoNLL-U only. val emptyDataSet = Seq.empty[String].toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(emptyDataSet) Unlabeled parser that finds a grammatical relation between two words in a sentence. Dependency parser provides information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. This is the instantiated model of the DependencyParserApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val dependencyParserApproach = DependencyParserModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) The default model is &quot;dependency_conllu&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Spark NLP Workshop and the DependencyParserApproachTestSpec. Input Annotator Types: [String]DOCUMENT, POS, TOKEN Output Annotator Type: DEPENDENCY Python API: DependencyParserModel Scala API: DependencyParserModel Source: DependencyParserModel Doc2Chunk Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol. Chunk text must be contained within input DOCUMENT. May be either StringType or ArrayType[StringType] (using setIsArray). Useful for annotators that require a CHUNK type input. For more extended examples on document pre-processing see the Spark NLP Workshop. Input Annotator Types: DOCUMENT Output Annotator Type: CHUNK Python API: Doc2Chunk Scala API: Doc2Chunk Source: Doc2Chunk Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) chunkAssembler = Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) .setIsArray(True) data = spark.createDataFrame([[ &quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;, [&quot;Spark NLP&quot;, &quot;text processing library&quot;, &quot;natural language processing&quot;] ]]).toDF(&quot;text&quot;, &quot;target&quot;) pipeline = Pipeline().setStages([documentAssembler, chunkAssembler]).fit(data) result = pipeline.transform(data) result.selectExpr(&quot;chunk.result&quot;, &quot;chunk.annotatorType&quot;).show(truncate=False) +--++ |result |annotatorType | +--++ |[Spark NLP, text processing library, natural language processing]|[chunk, chunk, chunk]| +--++ import spark.implicits._ import com.johnsnowlabs.nlp.{Doc2Chunk, DocumentAssembler} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val chunkAssembler = new Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) .setIsArray(true) val data = Seq( (&quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;, Seq(&quot;Spark NLP&quot;, &quot;text processing library&quot;, &quot;natural language processing&quot;)) ).toDF(&quot;text&quot;, &quot;target&quot;) val pipeline = new Pipeline().setStages(Array(documentAssembler, chunkAssembler)).fit(data) val result = pipeline.transform(data) result.selectExpr(&quot;chunk.result&quot;, &quot;chunk.annotatorType&quot;).show(false) +--++ |result |annotatorType | +--++ |[Spark NLP, text processing library, natural language processing]|[chunk, chunk, chunk]| +--++ Doc2Vec Approach Model Trains a Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. For instantiated/pretrained models, see Doc2VecModel. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: SENTENCE_EMBEDDINGS Python API: Doc2VecApproach Scala API: Doc2VecApproach Source: Doc2VecApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Doc2VecApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings ]) path = &quot;sherlockholmes.txt&quot; dataset = spark.read.text(path).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(dataset) import spark.implicits._ import com.johnsnowlabs.nlp.annotator.{Tokenizer, Doc2VecApproach} import com.johnsnowlabs.nlp.base.DocumentAssembler import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = new Doc2VecApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings )) val path = &quot;src/test/resources/spell/sherlockholmes.txt&quot; val dataset = spark.sparkContext.textFile(path) .toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(dataset) Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. This is the instantiated model of the Doc2VecApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val embeddings = Doc2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;doc2vec_gigaword_300&quot;, if no name is provided. For available pretrained models please see the Models Hub. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: SENTENCE_EMBEDDINGS Python API: Doc2VecModel Scala API: Doc2VecModel Source: Doc2VecModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Doc2VecModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, embeddings, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Tokenizer, Doc2VecModel} import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = Doc2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsFinisher )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ DocumentAssembler Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. The DocumentAssembler can read either a String column or an Array[String]. Additionally, setCleanupMode can be used to pre-process the text (Default: disabled). For possible options please refer the parameters section. For more extended examples on document pre-processing see the Spark NLP Workshop. Input Annotator Types: NONE Output Annotator Type: DOCUMENT Python API: DocumentAssembler Scala API: DocumentAssembler Source: DocumentAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library.&quot;]]).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) result = documentAssembler.transform(data) result.select(&quot;document&quot;).show(truncate=False) +-+ |document | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document&quot;).printSchema root |-- document: array (nullable = True) | |-- element: struct (containsNull = True) | | |-- annotatorType: string (nullable = True) | | |-- begin: integer (nullable = False) | | |-- end: integer (nullable = False) | | |-- result: string (nullable = True) | | |-- metadata: map (nullable = True) | | | |-- key: string | | | |-- value: string (valueContainsNull = True) | | |-- embeddings: array (nullable = True) | | | |-- element: float (containsNull = False) import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler val data = Seq(&quot;Spark NLP is an open-source text processing library.&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val result = documentAssembler.transform(data) result.select(&quot;document&quot;).show(false) +-+ |document | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document&quot;).printSchema root |-- document: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- begin: integer (nullable = false) | | |-- end: integer (nullable = false) | | |-- result: string (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | | |-- embeddings: array (nullable = true) | | | |-- element: float (containsNull = false) DocumentNormalizer Annotator which normalizes raw text from tagged text, e.g. scraped web pages or xml documents, from document type columns into Sentence. Removes all dirty characters from text following one or more input regex patterns. Can apply not wanted character removal with a specific policy. Can apply lower case normalization. For extended examples of usage, see the Spark NLP Workshop. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: DocumentNormalizer Scala API: DocumentNormalizer Source: DocumentNormalizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) cleanUpPatterns = [&quot;&lt;[^&gt;]&gt;&quot;] documentNormalizer = DocumentNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;normalizedDocument&quot;) .setAction(&quot;clean&quot;) .setPatterns(cleanUpPatterns) .setReplacement(&quot; &quot;) .setPolicy(&quot;pretty_all&quot;) .setLowercase(True) pipeline = Pipeline().setStages([ documentAssembler, documentNormalizer ]) text = &quot;&quot;&quot; &lt;div id=&quot;theworldsgreatest&quot; class=&#39;my-right my-hide-small my-wide toptext&#39; style=&quot;font-family:&#39;Segoe UI&#39;,Arial,sans-serif&quot;&gt; THE WORLD&#39;S LARGEST WEB DEVELOPER SITE &lt;h1 style=&quot;font-size:300%;&quot;&gt;THE WORLD&#39;S LARGEST WEB DEVELOPER SITE&lt;/h1&gt; &lt;p style=&quot;font-size:160%;&quot;&gt;Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&quot;&quot;&quot; data = spark.createDataFrame([[text]]).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(data) result = pipelineModel.transform(data) result.selectExpr(&quot;normalizedDocument.result&quot;).show(truncate=False) +--+ |result | +--+ |[ the world&#39;s largest web developer site the world&#39;s largest web developer site lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum..]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.DocumentNormalizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val cleanUpPatterns = Array(&quot;&lt;[^&gt;]&gt;&quot;) val documentNormalizer = new DocumentNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;normalizedDocument&quot;) .setAction(&quot;clean&quot;) .setPatterns(cleanUpPatterns) .setReplacement(&quot; &quot;) .setPolicy(&quot;pretty_all&quot;) .setLowercase(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, documentNormalizer )) val text = &quot;&quot;&quot; &lt;div id=&quot;theworldsgreatest&quot; class=&#39;my-right my-hide-small my-wide toptext&#39; style=&quot;font-family:&#39;Segoe UI&#39;,Arial,sans-serif&quot;&gt; THE WORLD&#39;S LARGEST WEB DEVELOPER SITE &lt;h1 style=&quot;font-size:300%;&quot;&gt;THE WORLD&#39;S LARGEST WEB DEVELOPER SITE&lt;/h1&gt; &lt;p style=&quot;font-size:160%;&quot;&gt;Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&quot;&quot;&quot; val data = Seq(text).toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(data) val result = pipelineModel.transform(data) result.selectExpr(&quot;normalizedDocument.result&quot;).show(truncate=false) +--+ |result | +--+ |[ the world&#39;s largest web developer site the world&#39;s largest web developer site lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum..]| +--+ EmbeddingsFinisher Extracts embeddings from Annotations into a more easily usable form. This is useful for example: WordEmbeddings, BertEmbeddings, SentenceEmbeddings and ChunkEmbeddings. By using EmbeddingsFinisher you can easily transform your embeddings into array of floats or vectors which are compatible with Spark ML functions such as LDA, K-mean, Random Forest classifier or any other functions that require featureCol. For more extended examples see the Spark NLP Workshop. Input Annotator Types: EMBEDDINGS Output Annotator Type: NONE Python API: EmbeddingsFinisher Scala API: EmbeddingsFinisher Source: EmbeddingsFinisher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) stopwordsCleaner = StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) gloveEmbeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;cleanTokens&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) embeddingsFinisher = EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_sentence_embeddings&quot;) .setOutputAsVector(True) .setCleanAnnotations(False) data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library.&quot;]]) .toDF(&quot;text&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, normalizer, stopwordsCleaner, gloveEmbeddings, embeddingsFinisher ]).fit(data) result = pipeline.transform(data) resultWithSize = result.selectExpr(&quot;explode(finished_sentence_embeddings) as embeddings&quot;) resultWithSize.show(5, 80) +--+ | embeddings| +--+ |[0.1619900017976761,0.045552998781204224,-0.03229299932718277,-0.685609996318...| |[-0.42416998744010925,1.1378999948501587,-0.5717899799346924,-0.5078899860382...| |[0.08621499687433243,-0.15772999823093414,-0.06067200005054474,0.395359992980...| |[-0.4970499873161316,0.7164199948310852,0.40119001269340515,-0.05761000141501...| |[-0.08170200139284134,0.7159299850463867,-0.20677000284194946,0.0295659992843...| +--+ import spark.implicits._ import org.apache.spark.ml.Pipeline import com.johnsnowlabs.nlp.{DocumentAssembler, EmbeddingsFinisher} import com.johnsnowlabs.nlp.annotator.{Normalizer, StopWordsCleaner, Tokenizer, WordEmbeddingsModel} val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) val stopwordsCleaner = new StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val gloveEmbeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;cleanTokens&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_sentence_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) val data = Seq(&quot;Spark NLP is an open-source text processing library.&quot;) .toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, normalizer, stopwordsCleaner, gloveEmbeddings, embeddingsFinisher )).fit(data) val result = pipeline.transform(data) val resultWithSize = result.selectExpr(&quot;explode(finished_sentence_embeddings)&quot;) .map { row =&gt; val vector = row.getAs[org.apache.spark.ml.linalg.DenseVector](0) (vector.size, vector) }.toDF(&quot;size&quot;, &quot;vector&quot;) resultWithSize.show(5, 80) +-+--+ |size| vector| +-+--+ | 100|[0.1619900017976761,0.045552998781204224,-0.03229299932718277,-0.685609996318...| | 100|[-0.42416998744010925,1.1378999948501587,-0.5717899799346924,-0.5078899860382...| | 100|[0.08621499687433243,-0.15772999823093414,-0.06067200005054474,0.395359992980...| | 100|[-0.4970499873161316,0.7164199948310852,0.40119001269340515,-0.05761000141501...| | 100|[-0.08170200139284134,0.7159299850463867,-0.20677000284194946,0.0295659992843...| +-+--+ EntityRuler Approach Model Fits an Annotator to match exact strings or regex patterns provided in a file against a Document and assigns them an named entity. The definitions can contain any number of named entities. There are multiple ways and formats to set the extraction resource. It is possible to set it either as a “JSON”, “JSONL” or “CSV” file. A path to the file needs to be provided to setPatternsResource. The file format needs to be set as the “format” field in the option parameter map and depending on the file type, additional parameters might need to be set. To enable regex extraction, setEnablePatternRegex(true) needs to be called. If the file is in a JSON format, then the rule definitions need to be given in a list with the fields “id”, “label” and “patterns”: [ { &quot;id&quot;: &quot;person-regex&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot; w+ s w+&quot;, &quot; w+- w+&quot;] }, { &quot;id&quot;: &quot;locations-words&quot;, &quot;label&quot;: &quot;LOCATION&quot;, &quot;patterns&quot;: [&quot;Winterfell&quot;] } ] The same fields also apply to a file in the JSONL format: {&quot;id&quot;: &quot;names-with-j&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Jon&quot;, &quot;John&quot;, &quot;John Snow&quot;]} {&quot;id&quot;: &quot;names-with-s&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Stark&quot;, &quot;Snow&quot;]} {&quot;id&quot;: &quot;names-with-e&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Eddard&quot;, &quot;Eddard Stark&quot;]} In order to use a CSV file, an additional parameter “delimiter” needs to be set. In this case, the delimiter might be set by using .setPatternsResource(&quot;patterns.csv&quot;, ReadAs.TEXT, Map(&quot;format&quot;-&gt;&quot;csv&quot;, &quot;delimiter&quot; -&gt; &quot; |&quot;)) PERSON|Jon PERSON|John PERSON|John Snow LOCATION|Winterfell Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: EntityRulerApproach Scala API: EntityRulerApproach Source: EntityRulerApproach Show Example PythonScala # In this example, the entities file as the form of # # PERSON|Jon # PERSON|John # PERSON|John Snow # LOCATION|Winterfell # # where each line represents an entity and the associated string delimited by &quot;|&quot;. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.common import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) entityRuler = EntityRulerApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;entities&quot;) .setPatternsResource( &quot;patterns.csv&quot;, ReadAs.TEXT, {&quot;format&quot;: &quot;csv&quot;, &quot;delimiter&quot;: &quot; |&quot;} ) .setEnablePatternRegex(True) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, entityRuler ]) data = spark.createDataFrame([[&quot;Jon Snow wants to be lord of Winterfell.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(entities)&quot;).show(truncate=False) +--+ |col | +--+ |[chunk, 0, 2, Jon, [entity -&gt; PERSON, sentence -&gt; 0], []] | |[chunk, 29, 38, Winterfell, [entity -&gt; LOCATION, sentence -&gt; 0], []]| +--+ // In this example, the entities file as the form of // // PERSON|Jon // PERSON|John // PERSON|John Snow // LOCATION|Winterfell // // where each line represents an entity and the associated string delimited by &quot;|&quot;. import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.er.EntityRulerApproach import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val entityRuler = new EntityRulerApproach() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;entities&quot;) .setPatternsResource( &quot;src/test/resources/entity-ruler/patterns.csv&quot;, ReadAs.TEXT, {&quot;format&quot;: &quot;csv&quot;, &quot;delimiter&quot;: &quot;|&quot;)} ) .setEnablePatternRegex(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, entityRuler )) val data = Seq(&quot;Jon Snow wants to be lord of Winterfell.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(entities)&quot;).show(false) +--+ |col | +--+ |[chunk, 0, 2, Jon, [entity -&gt; PERSON, sentence -&gt; 0], []] | |[chunk, 29, 38, Winterfell, [entity -&gt; LOCATION, sentence -&gt; 0], []]| +--+ Instantiated model of the EntityRulerApproach. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: EntityRulerModel Scala API: EntityRulerModel Source: EntityRulerModel Finisher Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. The Finisher outputs annotation(s) values into String. For more extended examples on document pre-processing see the Spark NLP Workshop. Input Annotator Types: ANY Output Annotator Type: NONE Python API: Finisher Scala API: Finisher Source: Finisher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline data = spark.createDataFrame([[1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;]]).toDF(&quot;id&quot;, &quot;text&quot;) # Extracts Named Entities amongst other things pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) finisher = Finisher().setInputCols(&quot;entities&quot;).setOutputCols(&quot;output&quot;) explainResult = pipeline.transform(data) explainResult.selectExpr(&quot;explode(entities)&quot;).show(truncate=False) ++ |entities | ++ |[[chunk, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []], [chunk, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]]| ++ result = finisher.transform(explainResult) result.select(&quot;output&quot;).show(truncate=False) +-+ |output | +-+ |[New York, New Jersey]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.Finisher val data = Seq((1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;)).toDF(&quot;id&quot;, &quot;text&quot;) // Extracts Named Entities amongst other things val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) val finisher = new Finisher().setInputCols(&quot;entities&quot;).setOutputCols(&quot;output&quot;) val explainResult = pipeline.transform(data) explainResult.selectExpr(&quot;explode(entities)&quot;).show(false) ++ |entities | ++ |[[chunk, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []], [chunk, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]]| ++ val result = finisher.transform(explainResult) result.select(&quot;output&quot;).show(false) +-+ |output | +-+ |[New York, New Jersey]| +-+ GraphExtraction Extracts a dependency graph between entities. The GraphExtraction class takes e.g. extracted entities from a NerDLModel and creates a dependency tree which describes how the entities relate to each other. For that a triple store format is used. Nodes represent the entities and the edges represent the relations between those entities. The graph can then be used to find relevant relationships between words. Both the DependencyParserModel and TypedDependencyParserModel need to be present in the pipeline. There are two ways to set them: Both Annotators are present in the pipeline already. The dependencies are taken implicitly from these two Annotators. Setting setMergeEntities to true will download the default pretrained models for those two Annotators automatically. The specific models can also be set with setDependencyParserModel and setTypedDependencyParserModel: val graph_extraction = new GraphExtraction() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;graph&quot;) .setRelationshipTypes(Array(&quot;prefer-LOC&quot;)) .setMergeEntities(true) //.setDependencyParserModel(Array(&quot;dependency_conllu&quot;, &quot;en&quot;, &quot;public/models&quot;)) //.setTypedDependencyParserModel(Array(&quot;dependency_typed_conllu&quot;, &quot;en&quot;, &quot;public/models&quot;)) To transform the resulting graph into a more generic form such as RDF, see the GraphFinisher. Input Annotator Types: DOCUMENT, TOKEN, NAMED_ENTITY Output Annotator Type: NODE Python API: GraphExtraction Scala API: GraphExtraction Source: GraphExtraction Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) nerTagger = NerDLModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) posTagger = PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) dependencyParser = DependencyParserModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency&quot;) typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols([&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency_type&quot;) graph_extraction = GraphExtraction() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;graph&quot;) .setRelationshipTypes([&quot;prefer-LOC&quot;]) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, embeddings, nerTagger, posTagger, dependencyParser, typedDependencyParser, graph_extraction ]) data = spark.createDataFrame([[&quot;You and John prefer the morning flight through Denver&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;graph&quot;).show(truncate=False) +--+ |graph | +--+ |13, 18, prefer, [relationship -&gt; prefer,LOC, path1 -&gt; prefer,nsubj,morning,flat,flight,flat,Denver], []| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel import com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserModel import org.apache.spark.ml.Pipeline import com.johnsnowlabs.nlp.annotators.GraphExtraction val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val nerTagger = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val dependencyParser = DependencyParserModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) val typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols(&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency_type&quot;) val graph_extraction = new GraphExtraction() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;graph&quot;) .setRelationshipTypes(Array(&quot;prefer-LOC&quot;)) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger, posTagger, dependencyParser, typedDependencyParser, graph_extraction )) val data = Seq(&quot;You and John prefer the morning flight through Denver&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;graph&quot;).show(false) +--+ |graph | +--+ |[[node, 13, 18, prefer, [relationship -&gt; prefer,LOC, path1 -&gt; prefer,nsubj,morning,flat,flight,flat,Denver], []]]| +--+ GraphFinisher Helper class to convert the knowledge graph from GraphExtraction into a generic format, such as RDF. Input Annotator Types: NONE Output Annotator Type: NONE Python API: GraphFinisher Scala API: GraphFinisher Source: GraphFinisher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # This is a continuation of the example of # GraphExtraction. To see how the graph is extracted, see the # documentation of that class. graphFinisher = GraphFinisher() .setInputCol(&quot;graph&quot;) .setOutputCol(&quot;graph_finished&quot;) .setOutputAs[False] finishedResult = graphFinisher.transform(result) finishedResult.select(&quot;text&quot;, &quot;graph_finished&quot;).show(truncate=False) +--+--+ |text |graph_finished | +--+--+ |You and John prefer the morning flight through Denver|(morning,flat,flight), (flight,flat,Denver)| +--+--+ // This is a continuation of the example of // [[com.johnsnowlabs.nlp.annotators.GraphExtraction GraphExtraction]]. To see how the graph is extracted, see the // documentation of that class. import com.johnsnowlabs.nlp.GraphFinisher val graphFinisher = new GraphFinisher() .setInputCol(&quot;graph&quot;) .setOutputCol(&quot;graph_finished&quot;) .setOutputAsArray(false) val finishedResult = graphFinisher.transform(result) finishedResult.select(&quot;text&quot;, &quot;graph_finished&quot;).show(false) +--+--+ |text |graph_finished | +--+--+ |You and John prefer the morning flight through Denver|[[(prefer,nsubj,morning), (morning,flat,flight), (flight,flat,Denver)]]| +--+--+ LanguageDetectorDL Language Identification and Detection by using CNN and RNN architectures in TensorFlow. LanguageDetectorDL is an annotator that detects the language of documents or sentences depending on the inputCols. The models are trained on large datasets such as Wikipedia and Tatoeba. Depending on the language (how similar the characters are), the LanguageDetectorDL works best with text longer than 140 characters. The output is a language code in Wiki Code style. Pretrained models can be loaded with pretrained of the companion object: Val languageDetector = LanguageDetectorDL.pretrained() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;language&quot;) The default model is &quot;ld_wiki_tatoeba_cnn_21&quot;, default language is &quot;xx&quot; (meaning multi-lingual), if no values are provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Spark NLP Workshop And the LanguageDetectorDLTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: LANGUAGE Python API: LanguageDetectorDL Scala API: LanguageDetectorDL Source: LanguageDetectorDL Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) languageDetector = LanguageDetectorDL.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;language&quot;) pipeline = Pipeline() .setStages([ documentAssembler, languageDetector ]) data = spark.createDataFrame([ [&quot;Spark NLP is an open-source text processing library for advanced natural language processing for the Python, Java and Scala programming languages.&quot;], [&quot;Spark NLP est une bibliothèque de traitement de texte open source pour le traitement avancé du langage naturel pour les langages de programmation Python, Java et Scala.&quot;], [&quot;Spark NLP ist eine Open-Source-Textverarbeitungsbibliothek für fortgeschrittene natürliche Sprachverarbeitung für die Programmiersprachen Python, Java und Scala.&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;language.result&quot;).show(truncate=False) ++ |result| ++ |[en] | |[fr] | |[de] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.ld.dl.LanguageDetectorDL import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val languageDetector = LanguageDetectorDL.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;language&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, languageDetector )) val data = Seq( &quot;Spark NLP is an open-source text processing library for advanced natural language processing for the Python, Java and Scala programming languages.&quot;, &quot;Spark NLP est une bibliothèque de traitement de texte open source pour le traitement avancé du langage naturel pour les langages de programmation Python, Java et Scala.&quot;, &quot;Spark NLP ist eine Open-Source-Textverarbeitungsbibliothek für fortgeschrittene natürliche Sprachverarbeitung für die Programmiersprachen Python, Java und Scala.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;language.result&quot;).show(false) ++ |result| ++ |[en] | |[fr] | |[de] | ++ Lemmatizer Approach Model Class to find lemmas out of words with the objective of returning a base dictionary word. Retrieves the significant part of a word. A dictionary of predefined lemmas must be provided with setDictionary. The dictionary can be set as a delimited text file. Pretrained models can be loaded with LemmatizerModel.pretrained. For available pretrained models please see the Models Hub. For extended examples of usage, see the Spark NLP Workshop. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: Lemmatizer Scala API: Lemmatizer Source: Lemmatizer Show Example PythonScala # In this example, the lemma dictionary `lemmas_small.txt` has the form of # # ... # pick -&gt; pick picks picking picked # peck -&gt; peck pecking pecked pecks # pickle -&gt; pickle pickles pickled pickling # pepper -&gt; pepper peppers peppered peppering # ... # # where each key is delimited by `-&gt;` and values are delimited by ` t` import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = Lemmatizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) pipeline = Pipeline() .setStages([ documentAssembler, sentenceDetector, tokenizer, lemmatizer ]) data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;lemma.result&quot;).show(truncate=False) ++ |result | ++ |[Peter, Pipers, employees, are, pick, peck, of, pickle, pepper, .]| ++ // In this example, the lemma dictionary `lemmas_small.txt` has the form of // // ... // pick -&gt; pick picks picking picked // peck -&gt; peck pecking pecked pecks // pickle -&gt; pickle pickles pickled pickling // pepper -&gt; pepper peppers peppered peppering // ... // // where each key is delimited by `-&gt;` and values are delimited by ` t` import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.Lemmatizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val lemmatizer = new Lemmatizer() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentenceDetector, tokenizer, lemmatizer )) val data = Seq(&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;lemma.result&quot;).show(false) ++ |result | ++ |[Peter, Pipers, employees, are, pick, peck, of, pickle, pepper, .]| ++ Instantiated Model of the Lemmatizer. For usage and examples, please see the documentation of that class. For available pretrained models please see the Models Hub. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: LemmatizerModel Scala API: LemmatizerModel Source: LemmatizerModel MultiClassifierDL Approach Model Trains a MultiClassifierDL for Multi-label Text Classification. MultiClassifierDL uses a Bidirectional GRU with a convolutional model that we have built inside TensorFlow and supports up to 100 classes. For instantiated/pretrained models, see MultiClassifierDLModel. The input to MultiClassifierDL are Sentence Embeddings such as the state-of-the-art UniversalSentenceEncoder, BertSentenceEmbeddings or SentenceEmbeddings. In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to. Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y). For extended examples of usage, see the Spark NLP Workshop and the MultiClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Note: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. UniversalSentenceEncoder, BertSentenceEmbeddings, SentenceEmbeddings or other sentence based embeddings can be used for the inputCol Python API: MultiClassifierDLApproach Scala API: MultiClassifierDLApproach Source: MultiClassifierDLApproach Show Example PythonScala # In this example, the training data has the form # # +-+--+--+ # | id| text| labels| # +-+--+--+ # |ed58abb40640f983|PN NewsYou mean ... | [toxic]| # |a1237f726b5f5d89|Dude. Place the ...| [obscene, insult]| # |24b0d6c8733c2abe|Thanks - thanks ...| [insult]| # |8c4478fb239bcfc0|&quot; Gee, 5 minutes ...|[toxic, obscene, ...| # +-+--+--+ import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Process training data to create text with associated array of labels trainDataset.printSchema() # root # |-- id: string (nullable = true) # |-- text: string (nullable = true) # |-- labels: array (nullable = true) # | |-- element: string (containsNull = true) # Then create pipeline for training documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) .setCleanupMode(&quot;shrink&quot;) embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;embeddings&quot;) docClassifier = MultiClassifierDLApproach() .setInputCols(&quot;embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;labels&quot;) .setBatchSize(128) .setMaxEpochs(10) .setLr(1e-3) .setThreshold(0.5) .setValidationSplit(0.1) pipeline = Pipeline() .setStages( [ documentAssembler, embeddings, docClassifier ] ) pipelineModel = pipeline.fit(trainDataset) // In this example, the training data has the form (Note: labels can be arbitrary) // // mr,ref // &quot;name[Alimentum], area[city centre], familyFriendly[no], near[Burger King]&quot;,Alimentum is an adult establish found in the city centre area near Burger King. // &quot;name[Alimentum], area[city centre], familyFriendly[yes]&quot;,Alimentum is a family-friendly place in the city centre. // ... // // It needs some pre-processing first, so the labels are of type `Array[String]`. This can be done like so: import spark.implicits._ import com.johnsnowlabs.nlp.annotators.classifier.dl.MultiClassifierDLApproach import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import org.apache.spark.ml.Pipeline import org.apache.spark.sql.functions.{col, udf} // Process training data to create text with associated array of labels def splitAndTrim = udf { labels: String =&gt; labels.split(&quot;, &quot;).map(x=&gt;x.trim) } val smallCorpus = spark.read .option(&quot;header&quot;, true) .option(&quot;inferSchema&quot;, true) .option(&quot;mode&quot;, &quot;DROPMALFORMED&quot;) .csv(&quot;src/test/resources/classifier/e2e.csv&quot;) .withColumn(&quot;labels&quot;, splitAndTrim(col(&quot;mr&quot;))) .withColumn(&quot;text&quot;, col(&quot;ref&quot;)) .drop(&quot;mr&quot;) smallCorpus.printSchema() // root // |-- ref: string (nullable = true) // |-- labels: array (nullable = true) // | |-- element: string (containsNull = true) // Then create pipeline for training val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) .setCleanupMode(&quot;shrink&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;embeddings&quot;) val docClassifier = new MultiClassifierDLApproach() .setInputCols(&quot;embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;labels&quot;) .setBatchSize(128) .setMaxEpochs(10) .setLr(1e-3f) .setThreshold(0.5f) .setValidationSplit(0.1f) val pipeline = new Pipeline() .setStages( Array( documentAssembler, embeddings, docClassifier ) ) val pipelineModel = pipeline.fit(smallCorpus) MultiClassifierDL for Multi-label Text Classification. MultiClassifierDL Bidirectional GRU with Convolution model we have built inside TensorFlow and supports up to 100 classes. The input to MultiClassifierDL are Sentence Embeddings such as state-of-the-art UniversalSentenceEncoder, BertSentenceEmbeddings or SentenceEmbeddings. This is the instantiated model of the MultiClassifierDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val multiClassifier = MultiClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;categories&quot;) The default model is &quot;multiclassifierdl_use_toxic&quot;, if no name is provided. It uses embeddings from the UniversalSentenceEncoder and classifies toxic comments. The data is based on the Jigsaw Toxic Comment Classification Challenge. For available pretrained models please see the Models Hub. In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to. Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y). For extended examples of usage, see the Spark NLP Workshop and the MultiClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: MultiClassifierDLModel Scala API: MultiClassifierDLModel Source: MultiClassifierDLModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) multiClassifierDl = MultiClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;classifications&quot;) pipeline = Pipeline() .setStages([ documentAssembler, useEmbeddings, multiClassifierDl ]) data = spark.createDataFrame([ [&quot;This is pretty good stuff!&quot;], [&quot;Wtf kind of crap is this&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;text&quot;, &quot;classifications.result&quot;).show(truncate=False) +--+-+ |text |result | +--+-+ |This is pretty good stuff!|[] | |Wtf kind of crap is this |[toxic, obscene]| +--+-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.classifier.dl.MultiClassifierDLModel import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val multiClassifierDl = MultiClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;classifications&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, useEmbeddings, multiClassifierDl )) val data = Seq( &quot;This is pretty good stuff!&quot;, &quot;Wtf kind of crap is this&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;text&quot;, &quot;classifications.result&quot;).show(false) +--+-+ |text |result | +--+-+ |This is pretty good stuff!|[] | |Wtf kind of crap is this |[toxic, obscene]| +--+-+ MultiDateMatcher Matches standard date formats into a provided format. Reads the following kind of dates: &quot;1978-01-28&quot;, &quot;1984/04/02,1/02/1980&quot;, &quot;2/28/79&quot;, &quot;The 31st of April in the year 2008&quot;, &quot;Fri, 21 Nov 1997&quot;, &quot;Jan 21, ‘97&quot;, &quot;Sun&quot;, &quot;Nov 21&quot;, &quot;jan 1st&quot;, &quot;next thursday&quot;, &quot;last wednesday&quot;, &quot;today&quot;, &quot;tomorrow&quot;, &quot;yesterday&quot;, &quot;next week&quot;, &quot;next month&quot;, &quot;next year&quot;, &quot;day after&quot;, &quot;the day before&quot;, &quot;0600h&quot;, &quot;06:00 hours&quot;, &quot;6pm&quot;, &quot;5:30 a.m.&quot;, &quot;at 5&quot;, &quot;12:59&quot;, &quot;23:59&quot;, &quot;1988/11/23 6pm&quot;, &quot;next week at 7.30&quot;, &quot;5 am tomorrow&quot; For example &quot;The 31st of April in the year 2008&quot; will be converted into 2008/04/31. For extended examples of usage, see the Spark NLP Workshop and the MultiDateMatcherTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DATE Python API: MultiDateMatcher Scala API: MultiDateMatcher Source: MultiDateMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) date = MultiDateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) pipeline = Pipeline().setStages([ documentAssembler, date ]) data = spark.createDataFrame([[&quot;I saw him yesterday and he told me that he will visit us next week&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(date) as dates&quot;).show(truncate=False) +--+ |dates | +--+ |[date, 57, 65, 2020/01/18, [sentence -&gt; 0], []]| |[date, 10, 18, 2020/01/10, [sentence -&gt; 0], []]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.MultiDateMatcher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val date = new MultiDateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, date )) val data = Seq(&quot;I saw him yesterday and he told me that he will visit us next week&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(date) as dates&quot;).show(false) +--+ |dates | +--+ |[date, 57, 65, 2020/01/18, [sentence -&gt; 0], []]| |[date, 10, 18, 2020/01/10, [sentence -&gt; 0], []]| +--+ NGramGenerator A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words. When the input is empty, an empty array is returned. When the input array length is less than n (number of elements per n-gram), no n-grams are returned. For more extended examples see the Spark NLP Workshop and the NGramGeneratorTestSpec. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: NGramGenerator Scala API: NGramGenerator Source: NGramGenerator Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) nGrams = NGramGenerator() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;ngrams&quot;) .setN(2) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, nGrams ]) data = spark.createDataFrame([[&quot;This is my sentence.&quot;]]).toDF(&quot;text&quot;) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(ngrams) as result&quot;).show(truncate=False) ++ |result | ++ |[chunk, 0, 6, This is, [sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 5, 9, is my, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 8, 18, my sentence, [sentence -&gt; 0, chunk -&gt; 2], []]| |[chunk, 11, 19, sentence ., [sentence -&gt; 0, chunk -&gt; 3], []]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.NGramGenerator import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val nGrams = new NGramGenerator() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;ngrams&quot;) .setN(2) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, nGrams )) val data = Seq(&quot;This is my sentence.&quot;).toDF(&quot;text&quot;) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(ngrams) as result&quot;).show(false) ++ |result | ++ |[chunk, 0, 6, This is, [sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 5, 9, is my, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 8, 18, my sentence, [sentence -&gt; 0, chunk -&gt; 2], []]| |[chunk, 11, 19, sentence ., [sentence -&gt; 0, chunk -&gt; 3], []]| ++ NerConverter Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. Results in CHUNK Annotation type. NER chunks can then be filtered by setting a whitelist with setWhiteList. Chunks with no associated entity (tagged “O”) are filtered. See also Inside–outside–beginning (tagging) for more information. Input Annotator Types: DOCUMENT, TOKEN, NAMED_ENTITY Output Annotator Type: CHUNK Python API: NerConverter Scala API: NerConverter Source: NerConverter Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # This is a continuation of the example of the NerDLModel. See that class # on how to extract the entities. # The output of the NerDLModel follows the Annotator schema and can be converted like so: # # result.selectExpr(&quot;explode(ner)&quot;).show(truncate=False) # +-+ # |col | # +-+ # |[named_entity, 0, 2, B-ORG, [word -&gt; U.N], []] | # |[named_entity, 3, 3, O, [word -&gt; .], []] | # |[named_entity, 5, 12, O, [word -&gt; official], []] | # |[named_entity, 14, 18, B-PER, [word -&gt; Ekeus], []] | # |[named_entity, 20, 24, O, [word -&gt; heads], []] | # |[named_entity, 26, 28, O, [word -&gt; for], []] | # |[named_entity, 30, 36, B-LOC, [word -&gt; Baghdad], []]| # |[named_entity, 37, 37, O, [word -&gt; .], []] | # +-+ # # After the converter is used: converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;entities&quot;) converter.transform(result).selectExpr(&quot;explode(entities)&quot;).show(truncate=False) ++ |col | ++ |[chunk, 0, 2, U.N, [entity -&gt; ORG, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 14, 18, Ekeus, [entity -&gt; PER, sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 30, 36, Baghdad, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 2], []]| ++ // This is a continuation of the example of the [[com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel NerDLModel]]. See that class // on how to extract the entities. // The output of the NerDLModel follows the Annotator schema and can be converted like so: // // result.selectExpr(&quot;explode(ner)&quot;).show(false) // +-+ // |col | // +-+ // |[named_entity, 0, 2, B-ORG, [word -&gt; U.N], []] | // |[named_entity, 3, 3, O, [word -&gt; .], []] | // |[named_entity, 5, 12, O, [word -&gt; official], []] | // |[named_entity, 14, 18, B-PER, [word -&gt; Ekeus], []] | // |[named_entity, 20, 24, O, [word -&gt; heads], []] | // |[named_entity, 26, 28, O, [word -&gt; for], []] | // |[named_entity, 30, 36, B-LOC, [word -&gt; Baghdad], []]| // |[named_entity, 37, 37, O, [word -&gt; .], []] | // +-+ // // After the converter is used: val converter = new NerConverter() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) .setPreservePosition(false) converter.transform(result).selectExpr(&quot;explode(entities)&quot;).show(false) ++ |col | ++ |[chunk, 0, 2, U.N, [entity -&gt; ORG, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 14, 18, Ekeus, [entity -&gt; PER, sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 30, 36, Baghdad, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 2], []]| ++ NerCrf Approach Model Algorithm for training a Named Entity Recognition Model For instantiated/pretrained models, see NerCrfModel. This Named Entity recognition annotator allows for a generic model to be trained by utilizing a CRF machine learning algorithm. The training data should be a labeled Spark Dataset, e.g. CoNLL 2003 IOB with Annotation type columns. The data should have columns of type DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS and an additional label column of annotator type NAMED_ENTITY. Excluding the label, this can be done with for example a SentenceDetector, a Tokenizer and a PerceptronModel and a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). Optionally the user can provide an entity dictionary file with setExternalFeatures for better accuracy. For extended examples of usage, see the Spark NLP Workshop and the NerCrfApproachTestSpec. Input Annotator Types: DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerCrfApproach Scala API: NerCrfApproach Source: NerCrfApproach Show Example PythonScala # This CoNLL dataset already includes the sentence, token, pos and label column with their respective annotator types. # If a custom dataset is used, these need to be defined. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) nerTagger = NerCrfApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setMinEpochs(1) .setMaxEpochs(3) .setC0(34) .setL2(3.0) .setOutputCol(&quot;ner&quot;) pipeline = Pipeline().setStages([ documentAssembler, embeddings, nerTagger ]) conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) // This CoNLL dataset already includes the sentence, token, pos and label column with their respective annotator types. // If a custom dataset is used, these need to be defined. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotator.NerCrfApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val nerTagger = new NerCrfApproach() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;) .setLabelColumn(&quot;label&quot;) .setMinEpochs(1) .setMaxEpochs(3) .setC0(34) .setL2(3.0) .setOutputCol(&quot;ner&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, embeddings, nerTagger )) val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) Extracts Named Entities based on a CRF Model. This Named Entity recognition annotator allows for a generic model to be trained by utilizing a CRF machine learning algorithm. The data should have columns of type DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS. These can be extracted with for example a SentenceDetector, a Tokenizer and a PerceptronModel This is the instantiated model of the NerCrfApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val nerTagger = NerCrfModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;, &quot;pos&quot;) .setOutputCol(&quot;ner&quot; The default model is &quot;ner_crf&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Spark NLP Workshop. Input Annotator Types: DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerCrfModel Scala API: NerCrfModel Source: NerCrfModel NerDL Approach Model This Named Entity recognition annotator allows to train generic NER model based on Neural Networks. The architecture of the neural network is a Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. For instantiated/pretrained models, see NerDLModel. The training data should be a labeled Spark Dataset, in the format of CoNLL 2003 IOB with Annotation type columns. The data should have columns of type DOCUMENT, TOKEN, WORD_EMBEDDINGS and an additional label column of annotator type NAMED_ENTITY. Excluding the label, this can be done with for example a SentenceDetector, a Tokenizer and a PerceptronModel and a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). For extended examples of usage, see the Spark NLP Workshop and the NerDLSpec. Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerDLApproach Scala API: NerDLApproach Source: NerDLApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline # First extract the prerequisites for the NerDLApproach documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = BertEmbeddings.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Then the training can start nerTagger = NerDLApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(1) .setRandomSeed(0) .setVerbose(0) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, embeddings, nerTagger ]) # We use the text and labels from the CoNLL dataset conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.BertEmbeddings import com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline // First extract the prerequisites for the NerDLApproach val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger = new NerDLApproach() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(1) .setRandomSeed(0) .setVerbose(0) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) // We use the text and labels from the CoNLL dataset val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) This Named Entity recognition annotator is a generic NER model based on Neural Networks. Neural Network architecture is Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. This is the instantiated model of the NerDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val nerModel = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) The default model is &quot;ner_dl&quot;, if no name is provided. For available pretrained models please see the Models Hub. Additionally, pretrained pipelines are available for this module, see Pipelines. Note that some pretrained models require specific types of embeddings, depending on which they were trained on. For example, the default model &quot;ner_dl&quot; requires the WordEmbeddings &quot;glove_100d&quot;. For extended examples of usage, see the Spark NLP Workshop and the NerDLSpec. Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerDLModel Scala API: NerDLModel Source: NerDLModel NerOverwriter Overwrites entities of specified strings. The input for this Annotator have to be entities that are already extracted, Annotator type NAMED_ENTITY. The strings specified with setStopWords will have new entities assigned to, specified with setNewResult. Input Annotator Types: NAMED_ENTITY Output Annotator Type: NAMED_ENTITY Python API: NerOverwriter Scala API: NerOverwriter Source: NerOverwriter Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # First extract the prerequisite Entities documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;bert&quot;) nerTagger = NerDLModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;bert&quot;]) .setOutputCol(&quot;ner&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, embeddings, nerTagger ]) data = spark.createDataFrame([[&quot;Spark NLP Crosses Five Million Downloads, John Snow Labs Announces.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(ner)&quot;).show(truncate=False) # ++ # |col | # ++ # |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | # |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | # |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | # |[named_entity, 18, 21, O, [word -&gt; Five], []] | # |[named_entity, 23, 29, O, [word -&gt; Million], []] | # |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | # |[named_entity, 40, 40, O, [word -&gt; ,], []] | # |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | # |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | # |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | # |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []]| # |[named_entity, 66, 66, O, [word -&gt; .], []] | # ++ # The recognized entities can then be overwritten nerOverwriter = NerOverwriter() .setInputCols([&quot;ner&quot;]) .setOutputCol(&quot;ner_overwritten&quot;) .setStopWords([&quot;Million&quot;]) .setNewResult(&quot;B-CARDINAL&quot;) nerOverwriter.transform(result).selectExpr(&quot;explode(ner_overwritten)&quot;).show(truncate=False) ++ |col | ++ |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | |[named_entity, 18, 21, O, [word -&gt; Five], []] | |[named_entity, 23, 29, B-CARDINAL, [word -&gt; Million], []]| |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | |[named_entity, 40, 40, O, [word -&gt; ,], []] | |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []] | |[named_entity, 66, 66, O, [word -&gt; .], []] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel import com.johnsnowlabs.nlp.annotators.ner.NerOverwriter import org.apache.spark.ml.Pipeline // First extract the prerequisite Entities val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;bert&quot;) val nerTagger = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;bert&quot;) .setOutputCol(&quot;ner&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) val data = Seq(&quot;Spark NLP Crosses Five Million Downloads, John Snow Labs Announces.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(ner)&quot;).show(false) / ++ |col | ++ |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | |[named_entity, 18, 21, O, [word -&gt; Five], []] | |[named_entity, 23, 29, O, [word -&gt; Million], []] | |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | |[named_entity, 40, 40, O, [word -&gt; ,], []] | |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []]| |[named_entity, 66, 66, O, [word -&gt; .], []] | ++ / // The recognized entities can then be overwritten val nerOverwriter = new NerOverwriter() .setInputCols(&quot;ner&quot;) .setOutputCol(&quot;ner_overwritten&quot;) .setStopWords(Array(&quot;Million&quot;)) .setNewResult(&quot;B-CARDINAL&quot;) nerOverwriter.transform(result).selectExpr(&quot;explode(ner_overwritten)&quot;).show(false) ++ |col | ++ |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | |[named_entity, 18, 21, O, [word -&gt; Five], []] | |[named_entity, 23, 29, B-CARDINAL, [word -&gt; Million], []]| |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | |[named_entity, 40, 40, O, [word -&gt; ,], []] | |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []] | |[named_entity, 66, 66, O, [word -&gt; .], []] | ++ Normalizer Approach Model Annotator that cleans out tokens. Requires stems, hence tokens. Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary For extended examples of usage, see the Spark NLP Workshop. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: Normalizer Scala API: Normalizer Source: Normalizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) .setLowercase(True) .setCleanupPatterns([&quot;&quot;&quot;[^ w d s]&quot;&quot;&quot;]) # remove punctuations (keep alphanumeric chars) # if we don&#39;t set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z]) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, normalizer ]) data = spark.createDataFrame([[&quot;John and Peter are brothers. However they don&#39;t support each other that much.&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;normalized.result&quot;).show(truncate = False) +-+ |result | +-+ |[john, and, peter, are, brothers, however, they, dont, support, each, other, that, much]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Normalizer, Tokenizer} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) .setLowercase(true) .setCleanupPatterns(Array(&quot;&quot;&quot;[^ w d s]&quot;&quot;&quot;)) // remove punctuations (keep alphanumeric chars) // if we don&#39;t set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z]) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, normalizer )) val data = Seq(&quot;John and Peter are brothers. However they don&#39;t support each other that much.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;normalized.result&quot;).show(truncate = false) +-+ |result | +-+ |[john, and, peter, are, brothers, however, they, dont, support, each, other, that, much]| +-+ Instantiated Model of the Normalizer. For usage and examples, please see the documentation of that class. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: NormalizerModel Scala API: NormalizerModel Source: NormalizerModel NorvigSweeting Spellchecker Approach Model Trains annotator, that retrieves tokens and makes corrections automatically if not found in an English dictionary. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster (than the standard approach with deletes + transposes + replaces + inserts) and language independent. A dictionary of correct spellings must be provided with setDictionary as a text file, where each word is parsed by a regex pattern. Inspired by Norvig model and SymSpell. For instantiated/pretrained models, see NorvigSweetingModel. For extended examples of usage, see the Spark NLP Workshop and the NorvigSweetingTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: NorvigSweetingApproach Scala API: NorvigSweetingApproach Source: NorvigSweetingApproach Show Example PythonScala # In this example, the dictionary `&quot;words.txt&quot;` has the form of # # ... # gummy # gummic # gummier # gummiest # gummiferous # ... # # This dictionary is then set to be the basis of the spell checker. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) spellChecker = NorvigSweetingApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker ]) pipelineModel = pipeline.fit(trainingData) // In this example, the dictionary `&quot;words.txt&quot;` has the form of // // ... // gummy // gummic // gummier // gummiest // gummiferous // ... // // This dictionary is then set to be the basis of the spell checker. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.spell.norvig.NorvigSweetingApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val spellChecker = new NorvigSweetingApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker )) val pipelineModel = pipeline.fit(trainingData) This annotator retrieves tokens and makes corrections automatically if not found in an English dictionary. Inspired by Norvig model and SymSpell. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster (than the standard approach with deletes + transposes + replaces + inserts) and language independent. This is the instantiated model of the NorvigSweetingApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val spellChecker = NorvigSweetingModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) .setDoubleVariants(true) The default model is &quot;spellcheck_norvig&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Spark NLP Workshop and the NorvigSweetingTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: NorvigSweetingModel Scala API: NorvigSweetingModel Source: NorvigSweetingModel POSTagger (Part of speech tagger) Approach Model Trains an averaged Perceptron model to tag words part-of-speech. Sets a POS tag to each word within a sentence. For pretrained models please see the PerceptronModel. The training data needs to be in a Spark DataFrame, where the column needs to consist of Annotations of type POS. The Annotation needs to have member result set to the POS tag and have a &quot;word&quot; mapping to its word inside of member metadata. This DataFrame for training can easily created by the helper class POS. POS().readDataset(spark, datasetPath).selectExpr(&quot;explode(tags) as tags&quot;).show(false) ++ |tags | ++ |[pos, 0, 5, NNP, [word -&gt; Pierre], []] | |[pos, 7, 12, NNP, [word -&gt; Vinken], []] | |[pos, 14, 14, ,, [word -&gt; ,], []] | |[pos, 31, 34, MD, [word -&gt; will], []] | |[pos, 36, 39, VB, [word -&gt; join], []] | |[pos, 41, 43, DT, [word -&gt; the], []] | |[pos, 45, 49, NN, [word -&gt; board], []] | ... For extended examples of usage, see the Spark NLP Workshop and PerceptronApproach tests. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: POS Python API: PerceptronApproach Scala API: PerceptronApproach Source: PerceptronApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) datasetPath = &quot;src/test/resources/anc-pos-corpus-small/test-training.txt&quot; trainingPerceptronDF = POS().readDataset(spark, datasetPath) trainedPos = PerceptronApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) .setPosColumn(&quot;tags&quot;) .fit(trainingPerceptronDF) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, trainedPos ]) data = spark.createDataFrame([[&quot;To be or not to be, is this the question?&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;pos.result&quot;).show(truncate=False) +--+ |result | +--+ |[NNP, NNP, CD, JJ, NNP, NNP, ,, MD, VB, DT, CD, .]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.training.POS import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val datasetPath = &quot;src/test/resources/anc-pos-corpus-small/test-training.txt&quot; val trainingPerceptronDF = POS().readDataset(spark, datasetPath) val trainedPos = new PerceptronApproach() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) .setPosColumn(&quot;tags&quot;) .fit(trainingPerceptronDF) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, trainedPos )) val data = Seq(&quot;To be or not to be, is this the question?&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;pos.result&quot;).show(false) +--+ |result | +--+ |[NNP, NNP, CD, JJ, NNP, NNP, ,, MD, VB, DT, CD, .]| +--+ Averaged Perceptron model to tag words part-of-speech. Sets a POS tag to each word within a sentence. This is the instantiated model of the PerceptronApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) The default model is &quot;pos_anc&quot;, if no name is provided. For available pretrained models please see the Models Hub. Additionally, pretrained pipelines are available for this module, see Pipelines. For extended examples of usage, see the Spark NLP Workshop. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: POS Python API: PerceptronModel Scala API: PerceptronModel Source: PerceptronModel RecursiveTokenizer Approach Model Tokenizes raw text recursively based on a handful of definable rules. Unlike the Tokenizer, the RecursiveTokenizer operates based on these array string parameters only: prefixes: Strings that will be split when found at the beginning of token. suffixes: Strings that will be split when found at the end of token. infixes: Strings that will be split when found at the middle of token. whitelist: Whitelist of strings not to split For extended examples of usage, see the Spark NLP Workshop and the TokenizerTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: RecursiveTokenizer Scala API: RecursiveTokenizer Source: RecursiveTokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = RecursiveTokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer ]) data = spark.createDataFrame([[&quot;One, after the Other, (and) again. PO, QAM,&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;token.result&quot;).show(truncate=False) ++ |result | ++ |[One, ,, after, the, Other, ,, (, and, ), again, ., PO, ,, QAM, ,]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.RecursiveTokenizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new RecursiveTokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer )) val data = Seq(&quot;One, after the Other, (and) again. PO, QAM,&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;token.result&quot;).show(false) ++ |result | ++ |[One, ,, after, the, Other, ,, (, and, ), again, ., PO, ,, QAM, ,]| ++ Instantiated model of the RecursiveTokenizer. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: RecursiveTokenizerModel Scala API: RecursiveTokenizerModel Source: RecursiveTokenizerModel RegexMatcher Approach Model Uses a reference file to match a set of regular expressions and associate them with a provided identifier. A dictionary of predefined regular expressions must be provided with setExternalRules. The dictionary can be set as a delimited text file. Pretrained pipelines are available for this module, see Pipelines. For extended examples of usage, see the Spark NLP Workshop and the RegexMatcherTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: CHUNK Python API: RegexMatcher Scala API: RegexMatcher Source: RegexMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the `rules.txt` has the form of # # the s w+, followed by &#39;the&#39; # ceremonies, ceremony # # where each regex is separated by the identifier by `&quot;,&quot;` documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentence = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) regexMatcher = RegexMatcher() .setExternalRules(&quot;src/test/resources/regex-matcher/rules.txt&quot;, &quot;,&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;regex&quot;) .setStrategy(&quot;MATCH_ALL&quot;) pipeline = Pipeline().setStages([documentAssembler, sentence, regexMatcher]) data = spark.createDataFrame([[ &quot;My first sentence with the first rule. This is my second sentence with ceremonies rule.&quot; ]]).toDF(&quot;text&quot;) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(regex) as result&quot;).show(truncate=False) +--+ |result | +--+ |[chunk, 23, 31, the first, [identifier -&gt; followed by &#39;the&#39;, sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 71, 80, ceremonies, [identifier -&gt; ceremony, sentence -&gt; 1, chunk -&gt; 0], []] | +--+ // In this example, the `rules.txt` has the form of // // the s w+, followed by &#39;the&#39; // ceremonies, ceremony // // where each regex is separated by the identifier by `&quot;,&quot;` import ResourceHelper.spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.RegexMatcher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val regexMatcher = new RegexMatcher() .setExternalRules(&quot;src/test/resources/regex-matcher/rules.txt&quot;, &quot;,&quot;) .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;regex&quot;) .setStrategy(&quot;MATCH_ALL&quot;) val pipeline = new Pipeline().setStages(Array(documentAssembler, sentence, regexMatcher)) val data = Seq( &quot;My first sentence with the first rule. This is my second sentence with ceremonies rule.&quot; ).toDF(&quot;text&quot;) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(regex) as result&quot;).show(false) +--+ |result | +--+ |[chunk, 23, 31, the first, [identifier -&gt; followed by &#39;the&#39;, sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 71, 80, ceremonies, [identifier -&gt; ceremony, sentence -&gt; 1, chunk -&gt; 0], []] | +--+ Instantiated model of the RegexMatcher. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT Output Annotator Type: CHUNK Python API: RegexMatcherModel Scala API: RegexMatcherModel Source: RegexMatcherModel RegexTokenizer A tokenizer that splits text by a regex pattern. The pattern needs to be set with setPattern and this sets the delimiting pattern or how the tokens should be split. By default this pattern is s+ which means that tokens should be split by 1 or more whitespace characters. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: RegexTokenizer Scala API: RegexTokenizer Source: RegexTokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) regexTokenizer = RegexTokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;regexToken&quot;) .setToLowercase(True) .setPattern(&quot; s+&quot;) pipeline = Pipeline().setStages([ documentAssembler, regexTokenizer ]) data = spark.createDataFrame([[&quot;This is my first sentence. nThis is my second.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;regexToken.result&quot;).show(truncate=False) +-+ |result | +-+ |[this, is, my, first, sentence., this, is, my, second.]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.RegexTokenizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val regexTokenizer = new RegexTokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;regexToken&quot;) .setToLowercase(true) .setPattern(&quot; s+&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, regexTokenizer )) val data = Seq(&quot;This is my first sentence. nThis is my second.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;regexToken.result&quot;).show(false) +-+ |result | +-+ |[this, is, my, first, sentence., this, is, my, second.]| +-+ SentenceDetector Annotator that detects sentence boundaries using any provided approach. Each extracted sentence can be returned in an Array or exploded to separate rows, if explodeSentences is set to true. For extended examples of usage, see the Spark NLP Workshop. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: SentenceDetector Scala API: SentenceDetector Source: SentenceDetector Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentence ]) data = spark.createDataFrame([[&quot;This is my first sentence. This my second. How about a third?&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(sentence) as sentences&quot;).show(truncate=False) ++ |sentences | ++ |[document, 0, 25, This is my first sentence., [sentence -&gt; 0], []]| |[document, 27, 41, This my second., [sentence -&gt; 1], []] | |[document, 43, 60, How about a third?, [sentence -&gt; 2], []] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence )) val data = Seq(&quot;This is my first sentence. This my second. How about a third?&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(sentence) as sentences&quot;).show(false) ++ |sentences | ++ |[document, 0, 25, This is my first sentence., [sentence -&gt; 0], []]| |[document, 27, 41, This my second., [sentence -&gt; 1], []] | |[document, 43, 60, How about a third?, [sentence -&gt; 2], []] | ++ SentenceDetectorDL Approach Model Trains an annotator that detects sentence boundaries using a deep learning approach. For pretrained models see SentenceDetectorDLModel. Currently, only the CNN model is supported for training, but in the future the architecture of the model can be set with setModelArchitecture. The default model &quot;cnn&quot; is based on the paper Deep-EOS: General-Purpose Neural Networks for Sentence Boundary Detection (2020, Stefan Schweter, Sajawel Ahmed) using a CNN architecture. We also modified the original implementation a little bit to cover broken sentences and some impossible end of line chars. Each extracted sentence can be returned in an Array or exploded to separate rows, if explodeSentences is set to true. For extended examples of usage, see the Spark NLP Workshop and the SentenceDetectorDLSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: SentenceDetectorDLApproach Scala API: SentenceDetectorDLApproach Source: SentenceDetectorDLApproach Show Example PythonScala # The training process needs data, where each data point is a sentence. # In this example the `train.txt` file has the form of # # ... # Slightly more moderate language would make our present situation – namely the lack of progress – a little easier. # His political successors now have great responsibilities to history and to the heritage of values bequeathed to them by Nelson Mandela. # ... # # where each line is one sentence. # Training can then be started like so: import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline trainingData = spark.read.text(&quot;train.txt&quot;).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLApproach() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) .setEpochsNumber(100) pipeline = Pipeline().setStages([documentAssembler, sentenceDetector]) model = pipeline.fit(trainingData) // The training process needs data, where each data point is a sentence. // In this example the `train.txt` file has the form of // // ... // Slightly more moderate language would make our present situation – namely the lack of progress – a little easier. // His political successors now have great responsibilities to history and to the heritage of values bequeathed to them by Nelson Mandela. // ... // // where each line is one sentence. // Training can then be started like so: import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLApproach import org.apache.spark.ml.Pipeline val trainingData = spark.read.text(&quot;train.txt&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetectorDLApproach() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentences&quot;) .setEpochsNumber(100) val pipeline = new Pipeline().setStages(Array(documentAssembler, sentenceDetector)) val model = pipeline.fit(trainingData) Annotator that detects sentence boundaries using a deep learning approach. Instantiated Model of the SentenceDetectorDLApproach. Detects sentence boundaries using a deep learning approach. Pretrained models can be loaded with pretrained of the companion object: val sentenceDL = SentenceDetectorDLModel.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentencesDL&quot;) The default model is &quot;sentence_detector_dl&quot;, if no name is provided. For available pretrained models please see the Models Hub. Each extracted sentence can be returned in an Array or exploded to separate rows, if explodeSentences is set to true. For extended examples of usage, see the Spark NLP Workshop and the SentenceDetectorDLSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: SentenceDetectorDLModel Scala API: SentenceDetectorDLModel Source: SentenceDetectorDLModel SentenceEmbeddings Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols). This can be configured with setPoolingStrategy, which either be &quot;AVERAGE&quot; or &quot;SUM&quot;. For more extended examples see the Spark NLP Workshop. and the SentenceEmbeddingsTestSpec. TIP: Here is how you can explode and convert these embeddings into Vectors or what’s known as Feature column so it can be used in Spark ML regression or clustering functions: PythonScala from org.apache.spark.ml.linal import Vector, Vectors from pyspark.sql.functions import udf # Let&#39;s create a UDF to take array of embeddings and output Vectors @udf(Vector) def convertToVectorUDF(matrix): return Vectors.dense(matrix.toArray.map(_.toDouble)) # Now let&#39;s explode the sentence_embeddings column and have a new feature column for Spark ML pipelineDF.select(explode(&quot;sentence_embeddings.embeddings&quot;).as(&quot;sentence_embedding&quot;)) .withColumn(&quot;features&quot;, convertToVectorUDF(&quot;sentence_embedding&quot;)) import org.apache.spark.ml.linalg.{Vector, Vectors} // Let&#39;s create a UDF to take array of embeddings and output Vectors val convertToVectorUDF = udf((matrix : Seq[Float]) =&gt; { Vectors.dense(matrix.toArray.map(_.toDouble)) }) // Now let&#39;s explode the sentence_embeddings column and have a new feature column for Spark ML pipelineDF.select(explode($&quot;sentence_embeddings.embeddings&quot;).as(&quot;sentence_embedding&quot;)) .withColumn(&quot;features&quot;, convertToVectorUDF($&quot;sentence_embedding&quot;)) Input Annotator Types: DOCUMENT, WORD_EMBEDDINGS Output Annotator Type: SENTENCE_EMBEDDINGS Note: If you choose document as your input for Tokenizer, WordEmbeddings/BertEmbeddings, and SentenceEmbeddings then it averages/sums all the embeddings into one array of embeddings. However, if you choose sentence as inputCols then for each sentence SentenceEmbeddings generates one array of embeddings. Python API: SentenceEmbeddings Scala API: SentenceEmbeddings Source: SentenceEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsSentence = SentenceEmbeddings() .setInputCols([&quot;document&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) .setCleanAnnotations(False) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings, embeddingsSentence, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80) +--+ | result| +--+ |[-0.22093398869037628,0.25130119919776917,0.41810303926467896,-0.380883991718...| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsSentence = new SentenceEmbeddings() .setInputCols(Array(&quot;document&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;sentence_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsSentence, embeddingsFinisher )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80) +--+ | result| +--+ |[-0.22093398869037628,0.25130119919776917,0.41810303926467896,-0.380883991718...| +--+ SentimentDL Approach Model Trains a SentimentDL, an annotator for multi-class sentiment analysis. In natural language processing, sentiment analysis is the task of classifying the affective state or subjective view of a text. A common example is if either a product review or tweet can be interpreted positively or negatively. For the instantiated/pretrained models, see SentimentDLModel. Notes: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. So positive sentiment can be expressed as either &quot;positive&quot; or 0, negative sentiment as &quot;negative&quot; or 1. UniversalSentenceEncoder, BertSentenceEmbeddings, SentenceEmbeddings or other sentence based embeddings can be used For extended examples of usage, see the Spark NLP Workshop and the SentimentDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: SentimentDLApproach Scala API: SentimentDLApproach Source: SentimentDLApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, `sentiment.csv` is in the form # # text,label # This movie is the best movie I have watched ever! In my opinion this movie can win an award.,0 # This was a terrible movie! The acting was bad really bad!,1 # # The model can then be trained with smallCorpus = spark.read.option(&quot;header&quot;, &quot;True&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) docClassifier = SentimentDLApproach() .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCol(&quot;sentiment&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(32) .setMaxEpochs(1) .setLr(5e-3) .setDropout(0.5) pipeline = Pipeline() .setStages( [ documentAssembler, useEmbeddings, docClassifier ] ) pipelineModel = pipeline.fit(smallCorpus) // In this example, `sentiment.csv` is in the form // // text,label // This movie is the best movie I have watched ever! In my opinion this movie can win an award.,0 // This was a terrible movie! The acting was bad really bad!,1 // // The model can then be trained with import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.UniversalSentenceEncoder import com.johnsnowlabs.nlp.annotators.classifier.dl.{SentimentDLApproach, SentimentDLModel} import org.apache.spark.ml.Pipeline val smallCorpus = spark.read.option(&quot;header&quot;, &quot;true&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val docClassifier = new SentimentDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sentiment&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(32) .setMaxEpochs(1) .setLr(5e-3f) .setDropout(0.5f) val pipeline = new Pipeline() .setStages( Array( documentAssembler, useEmbeddings, docClassifier ) ) val pipelineModel = pipeline.fit(smallCorpus) SentimentDL, an annotator for multi-class sentiment analysis. In natural language processing, sentiment analysis is the task of classifying the affective state or subjective view of a text. A common example is if either a product review or tweet can be interpreted positively or negatively. This is the instantiated model of the SentimentDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val sentiment = SentimentDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sentiment&quot;) The default model is &quot;sentimentdl_use_imdb&quot;, if no name is provided. It is english sentiment analysis trained on the IMDB dataset. For available pretrained models please see the Models Hub. For extended examples of usage, see the Spark NLP Workshop and the SentimentDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: SentimentDLModel Scala API: SentimentDLModel Source: SentimentDLModel SentimentDetector Approach Model Trains a rule based sentiment detector, which calculates a score based on predefined keywords. A dictionary of predefined sentiment keywords must be provided with setDictionary, where each line is a word delimited to its class (either positive or negative). The dictionary can be set as a delimited text file. By default, the sentiment score will be assigned labels &quot;positive&quot; if the score is &gt;= 0, else &quot;negative&quot;. To retrieve the raw sentiment scores, enableScore needs to be set to true. For extended examples of usage, see the Spark NLP Workshop and the SentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: SentimentDetector Scala API: SentimentDetector Source: SentimentDetector Show Example PythonScala # In this example, the dictionary `default-sentiment-dict.txt` has the form of # # ... # cool,positive # superb,positive # bad,negative # uninspired,negative # ... # # where each sentiment keyword is delimited by `&quot;,&quot;`. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = Lemmatizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) sentimentDetector = SentimentDetector() .setInputCols([&quot;lemma&quot;, &quot;document&quot;]) .setOutputCol(&quot;sentimentScore&quot;) .setDictionary(&quot;default-sentiment-dict.txt&quot;, &quot;,&quot;, ReadAs.TEXT) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, lemmatizer, sentimentDetector, ]) data = spark.createDataFrame([ [&quot;The staff of the restaurant is nice&quot;], [&quot;I recommend others to avoid because it is too expensive&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;sentimentScore.result&quot;).show(truncate=False) +-+ # ++ for enableScore set to True |result | # |result| +-+ # ++ |[positive]| # |[1.0] | |[negative]| # |[-2.0]| +-+ # ++ // In this example, the dictionary `default-sentiment-dict.txt` has the form of // // ... // cool,positive // superb,positive // bad,negative // uninspired,negative // ... // // where each sentiment keyword is delimited by `&quot;,&quot;`. import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotators.Lemmatizer import com.johnsnowlabs.nlp.annotators.sda.pragmatic.SentimentDetector import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val lemmatizer = new Lemmatizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) val sentimentDetector = new SentimentDetector() .setInputCols(&quot;lemma&quot;, &quot;document&quot;) .setOutputCol(&quot;sentimentScore&quot;) .setDictionary(&quot;src/test/resources/sentiment-corpus/default-sentiment-dict.txt&quot;, &quot;,&quot;, ReadAs.TEXT) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, lemmatizer, sentimentDetector, )) val data = Seq( &quot;The staff of the restaurant is nice&quot;, &quot;I recommend others to avoid because it is too expensive&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;sentimentScore.result&quot;).show(false) +-+ // ++ for enableScore set to true |result | // |result| +-+ // ++ |[positive]| // |[1.0] | |[negative]| // |[-2.0]| +-+ // ++ Rule based sentiment detector, which calculates a score based on predefined keywords. This is the instantiated model of the SentimentDetector. For training your own model, please see the documentation of that class. A dictionary of predefined sentiment keywords must be provided with setDictionary, where each line is a word delimited to its class (either positive or negative). The dictionary can be set as a delimited text file. By default, the sentiment score will be assigned labels &quot;positive&quot; if the score is &gt;= 0, else &quot;negative&quot;. To retrieve the raw sentiment scores, enableScore needs to be set to true. For extended examples of usage, see the Spark NLP Workshop and the SentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: SentimentDetectorModel Scala API: SentimentDetectorModel Source: SentimentDetectorModel Stemmer Returns hard-stems out of words with the objective of retrieving the meaningful part of the word. For extended examples of usage, see the Spark NLP Workshop. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: Stemmer Scala API: Stemmer Source: Stemmer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) stemmer = Stemmer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;stem&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, stemmer ]) data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;stem.result&quot;).show(truncate = False) +-+ |result | +-+ |[peter, piper, employe, ar, pick, peck, of, pickl, pepper, .]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Stemmer, Tokenizer} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val stemmer = new Stemmer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;stem&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, stemmer )) val data = Seq(&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;stem.result&quot;).show(truncate = false) +-+ |result | +-+ |[peter, piper, employe, ar, pick, peck, of, pickl, pepper, .]| +-+ StopWordsCleaner This annotator takes a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences. By default, it uses stop words from MLlibs StopWordsRemover. Stop words can also be defined by explicitly setting them with setStopWords(value: Array[String]) or loaded from pretrained models using pretrained of its companion object. val stopWords = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) // will load the default pretrained model `&quot;stopwords_en&quot;`. For available pretrained models please see the Models Hub. For extended examples of usage, see the Spark NLP Workshop and StopWordsCleanerTestSpec. NOTE: If you need to setStopWords from a text file, you can first read and convert it into an array of string as follows. PythonScala # your stop words text file, each line is one stop word stopwords = sc.textFile(&quot;/tmp/stopwords/english.txt&quot;).collect() # simply use it in StopWordsCleaner stopWordsCleaner = StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setStopWords(stopwords) .setCaseSensitive(False) # or you can use pretrained models for StopWordsCleaner stopWordsCleaner = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) // your stop words text file, each line is one stop word val stopwords = sc.textFile(&quot;/tmp/stopwords/english.txt&quot;).collect() // simply use it in StopWordsCleaner val stopWordsCleaner = new StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setStopWords(stopwords) .setCaseSensitive(false) // or you can use pretrained models for StopWordsCleaner val stopWordsCleaner = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: StopWordsCleaner Scala API: StopWordsCleaner Source: StopWordsCleaner Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) stopWords = StopWordsCleaner() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, stopWords ]) data = spark.createDataFrame([ [&quot;This is my first sentence. This is my second.&quot;], [&quot;This is my third sentence. This is my forth.&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;cleanTokens.result&quot;).show(truncate=False) +-+ |result | +-+ |[first, sentence, ., second, .]| |[third, sentence, ., forth, .] | +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.StopWordsCleaner import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val stopWords = new StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, stopWords )) val data = Seq( &quot;This is my first sentence. This is my second.&quot;, &quot;This is my third sentence. This is my forth.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;cleanTokens.result&quot;).show(false) +-+ |result | +-+ |[first, sentence, ., second, .]| |[third, sentence, ., forth, .] | +-+ SymmetricDelete Spellchecker Approach Model Trains a Symmetric Delete spelling correction algorithm. Retrieves tokens and utilizes distance metrics to compute possible derived words. Inspired by SymSpell. For instantiated/pretrained models, see SymmetricDeleteModel. See SymmetricDeleteModelTestSpec for further reference. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: SymmetricDeleteApproach Scala API: SymmetricDeleteApproach Source: SymmetricDeleteApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the dictionary `&quot;words.txt&quot;` has the form of # # ... # gummy # gummic # gummier # gummiest # gummiferous # ... # # This dictionary is then set to be the basis of the spell checker. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) spellChecker = SymmetricDeleteApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker ]) pipelineModel = pipeline.fit(trainingData) // In this example, the dictionary `&quot;words.txt&quot;` has the form of // // ... // gummy // gummic // gummier // gummiest // gummiferous // ... // // This dictionary is then set to be the basis of the spell checker. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val spellChecker = new SymmetricDeleteApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker )) val pipelineModel = pipeline.fit(trainingData) Symmetric Delete spelling correction algorithm. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster (than the standard approach with deletes + transposes + replaces + inserts) and language independent. Inspired by SymSpell. Pretrained models can be loaded with pretrained of the companion object: val spell = SymmetricDeleteModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) The default model is &quot;spellcheck_sd&quot;, if no name is provided. For available pretrained models please see the Models Hub. See SymmetricDeleteModelTestSpec for further reference. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: SymmetricDeleteModel Scala API: SymmetricDeleteModel Source: SymmetricDeleteModel TextMatcher Approach Model Annotator to match exact phrases (by token) provided in a file against a Document. A text file of predefined phrases must be provided with setEntities. For extended examples of usage, see the Spark NLP Workshop and the TextMatcherTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: TextMatcher Scala API: TextMatcher Source: TextMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the entities file is of the form # # ... # dolore magna aliqua # lorem ipsum dolor. sit # laborum # ... # # where each line represents an entity phrase to be extracted. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) data = spark.createDataFrame([[&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;]]).toDF(&quot;text&quot;) entityExtractor = TextMatcher() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setEntities(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(False) pipeline = Pipeline().setStages([documentAssembler, tokenizer, entityExtractor]) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity) as result&quot;).show(truncate=False) ++ |result | ++ |[chunk, 6, 24, dolore magna aliqua, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 27, 48, Lorem ipsum dolor. sit, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 1], []]| |[chunk, 53, 59, laborum, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 2], []] | ++ // In this example, the entities file is of the form // // ... // dolore magna aliqua // lorem ipsum dolor. sit // laborum // ... // // where each line represents an entity phrase to be extracted. import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.TextMatcher import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val data = Seq(&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;).toDF(&quot;text&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setEntities(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(false) .setTokenizer(tokenizer.fit(data)) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer, entityExtractor)) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity) as result&quot;).show(false) ++ |result | ++ |[chunk, 6, 24, dolore magna aliqua, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 27, 48, Lorem ipsum dolor. sit, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 1], []]| |[chunk, 53, 59, laborum, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 2], []] | ++ Instantiated model of the TextMatcher. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: TextMatcherModel Scala API: TextMatcherModel Source: TextMatcherModel Token2Chunk Converts TOKEN type Annotations to CHUNK type. This can be useful if a entities have been already extracted as TOKEN and following annotators require CHUNK types. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: Token2Chunk Scala API: Token2Chunk Source: Token2Chunk Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) token2chunk = Token2Chunk() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;chunk&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, token2chunk ]) data = spark.createDataFrame([[&quot;One Two Three Four&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(truncate=False) ++ |result | ++ |[chunk, 0, 2, One, [sentence -&gt; 0], []] | |[chunk, 4, 6, Two, [sentence -&gt; 0], []] | |[chunk, 8, 12, Three, [sentence -&gt; 0], []]| |[chunk, 14, 17, Four, [sentence -&gt; 0], []]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.{Token2Chunk, Tokenizer} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val token2chunk = new Token2Chunk() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;chunk&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, token2chunk )) val data = Seq(&quot;One Two Three Four&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(false) ++ |result | ++ |[chunk, 0, 2, One, [sentence -&gt; 0], []] | |[chunk, 4, 6, Two, [sentence -&gt; 0], []] | |[chunk, 8, 12, Three, [sentence -&gt; 0], []]| |[chunk, 14, 17, Four, [sentence -&gt; 0], []]| ++ TokenAssembler This transformer reconstructs a DOCUMENT type annotation from tokens, usually after these have been normalized, lemmatized, normalized, spell checked, etc, in order to use this document annotation in further annotators. Requires DOCUMENT and TOKEN type annotations as input. For more extended examples on document pre-processing see the Spark NLP Workshop. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: DOCUMENT Python API: TokenAssembler Scala API: TokenAssembler Source: TokenAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # First, the text is tokenized and cleaned documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) .setLowercase(False) stopwordsCleaner = StopWordsCleaner() .setInputCols([&quot;normalized&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) # Then the TokenAssembler turns the cleaned tokens into a `DOCUMENT` type structure. tokenAssembler = TokenAssembler() .setInputCols([&quot;sentences&quot;, &quot;cleanTokens&quot;]) .setOutputCol(&quot;cleanText&quot;) data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;]]) .toDF(&quot;text&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, normalizer, stopwordsCleaner, tokenAssembler ]).fit(data) result = pipeline.transform(data) result.select(&quot;cleanText&quot;).show(truncate=False) ++ |cleanText | ++ |0, 80, Spark NLP opensource text processing library advanced natural language processing, [sentence -&gt; 0], []| ++ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.{Normalizer, StopWordsCleaner} import com.johnsnowlabs.nlp.TokenAssembler import org.apache.spark.ml.Pipeline // First, the text is tokenized and cleaned val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) .setLowercase(false) val stopwordsCleaner = new StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) // Then the TokenAssembler turns the cleaned tokens into a `DOCUMENT` type structure. val tokenAssembler = new TokenAssembler() .setInputCols(&quot;sentences&quot;, &quot;cleanTokens&quot;) .setOutputCol(&quot;cleanText&quot;) val data = Seq(&quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;) .toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, normalizer, stopwordsCleaner, tokenAssembler )).fit(data) val result = pipeline.transform(data) result.select(&quot;cleanText&quot;).show(false) ++ |cleanText | ++ |[[document, 0, 80, Spark NLP opensource text processing library advanced natural language processing, [sentence -&gt; 0], []]]| ++ Tokenizer Approach Model Tokenizes raw text in document type columns into TokenizedSentence . This class represents a non fitted tokenizer. Fitting it will cause the internal RuleFactory to construct the rules for tokenizing from the input configuration. Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. For extended examples of usage see the Spark NLP Workshop and Tokenizer test class Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Note: All these APIs receive regular expressions so please make sure that you escape special characters according to Java conventions. Python API: Tokenizer Scala API: Tokenizer Source: Tokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline data = spark.createDataFrame([[&quot;I&#39;d like to say we didn&#39;t expect that. Jane&#39;s boyfriend.&quot;]]).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) tokenizer = Tokenizer().setInputCols([&quot;document&quot;]).setOutputCol(&quot;token&quot;).fit(data) pipeline = Pipeline().setStages([documentAssembler, tokenizer]).fit(data) result = pipeline.transform(data) result.selectExpr(&quot;token.result&quot;).show(truncate=False) +--+ |output | +--+ |[I&#39;d, like, to, say, we, didn&#39;t, expect, that, ., Jane&#39;s, boyfriend, .]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import org.apache.spark.ml.Pipeline val data = Seq(&quot;I&#39;d like to say we didn&#39;t expect that. Jane&#39;s boyfriend.&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer().setInputCols(&quot;document&quot;).setOutputCol(&quot;token&quot;).fit(data) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer)).fit(data) val result = pipeline.transform(data) result.selectExpr(&quot;token.result&quot;).show(false) +--+ |output | +--+ |[I&#39;d, like, to, say, we, didn&#39;t, expect, that, ., Jane&#39;s, boyfriend, .]| +--+ Tokenizes raw text into word pieces, tokens. Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. This class represents an already fitted Tokenizer model. See the main class Tokenizer for more examples of usage. Input Annotator Types: DOCUMENT //A Tokenizer could require only for now a SentenceDetector annotator Output Annotator Type: TOKEN Python API: TokenizerModel Scala API: TokenizerModel Source: TokenizerModel TypedDependencyParser Approach Model Labeled parser that finds a grammatical relation between two words in a sentence. Its input is either a CoNLL2009 or ConllU dataset. For instantiated/pretrained models, see TypedDependencyParserModel. Dependency parsers provide information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The parser requires the dependant tokens beforehand with e.g. DependencyParser. The required training data can be set in two different ways (only one can be chosen for a particular model): Dataset in the CoNLL 2009 format set with setConll2009 Dataset in the CoNLL-U format set with setConllU Apart from that, no additional training data is needed. See TypedDependencyParserApproachTestSpec for further reference on this API. Input Annotator Types: TOKEN, POS, DEPENDENCY Output Annotator Type: LABELED_DEPENDENCY Python API: TypedDependencyParserApproach Scala API: TypedDependencyParserApproach Source: TypedDependencyParserApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) posTagger = PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) dependencyParser = DependencyParserModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency&quot;) typedDependencyParser = TypedDependencyParserApproach() .setInputCols([&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency_type&quot;) .setConllU(&quot;src/test/resources/parser/labeled/train_small.conllu.txt&quot;) .setNumberOfIterations(1) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, posTagger, dependencyParser, typedDependencyParser ]) # Additional training data is not needed, the dependency parser relies on CoNLL-U only. emptyDataSet = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(emptyDataSet) import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel import com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val dependencyParser = DependencyParserModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) val typedDependencyParser = new TypedDependencyParserApproach() .setInputCols(&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency_type&quot;) .setConllU(&quot;src/test/resources/parser/labeled/train_small.conllu.txt&quot;) .setNumberOfIterations(1) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, posTagger, dependencyParser, typedDependencyParser )) // Additional training data is not needed, the dependency parser relies on CoNLL-U only. val emptyDataSet = Seq.empty[String].toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(emptyDataSet) Labeled parser that finds a grammatical relation between two words in a sentence. Its input is either a CoNLL2009 or ConllU dataset. Dependency parsers provide information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The parser requires the dependant tokens beforehand with e.g. DependencyParser. Pretrained models can be loaded with pretrained of the companion object: val typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols(&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency_type&quot;) The default model is &quot;dependency_typed_conllu&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Spark NLP Workshop and the TypedDependencyModelTestSpec. Input Annotator Types: TOKEN, POS, DEPENDENCY Output Annotator Type: LABELED_DEPENDENCY Python API: TypedDependencyParserModel Scala API: TypedDependencyParserModel Source: TypedDependencyParserModel ViveknSentiment Approach Model Trains a sentiment analyser inspired by the algorithm by Vivek Narayanan https://github.com/vivekn/sentiment/. The algorithm is based on the paper “Fast and accurate sentiment classification using an enhanced Naive Bayes model”. The analyzer requires sentence boundaries to give a score in context. Tokenization is needed to make sure tokens are within bounds. Transitivity requirements are also required. The training data needs to consist of a column for normalized text and a label column (either &quot;positive&quot; or &quot;negative&quot;). For extended examples of usage, see the Spark NLP Workshop and the ViveknSentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: ViveknSentimentApproach Scala API: ViveknSentimentApproach Source: ViveknSentimentApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) token = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normal&quot;) vivekn = ViveknSentimentApproach() .setInputCols([&quot;document&quot;, &quot;normal&quot;]) .setSentimentCol(&quot;train_sentiment&quot;) .setOutputCol(&quot;result_sentiment&quot;) finisher = Finisher() .setInputCols([&quot;result_sentiment&quot;]) .setOutputCols(&quot;final_sentiment&quot;) pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher]) training = spark.createDataFrame([ (&quot;I really liked this movie!&quot;, &quot;positive&quot;), (&quot;The cast was horrible&quot;, &quot;negative&quot;), (&quot;Never going to watch this again or recommend it to anyone&quot;, &quot;negative&quot;), (&quot;It&#39;s a waste of time&quot;, &quot;negative&quot;), (&quot;I loved the protagonist&quot;, &quot;positive&quot;), (&quot;The music was really really good&quot;, &quot;positive&quot;) ]).toDF(&quot;text&quot;, &quot;train_sentiment&quot;) pipelineModel = pipeline.fit(training) data = spark.createDataFrame([ [&quot;I recommend this movie&quot;], [&quot;Dont waste your time!!!&quot;] ]).toDF(&quot;text&quot;) result = pipelineModel.transform(data) result.select(&quot;final_sentiment&quot;).show(truncate=False) ++ |final_sentiment| ++ |[positive] | |[negative] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.Normalizer import com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentApproach import com.johnsnowlabs.nlp.Finisher import org.apache.spark.ml.Pipeline val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val token = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normal&quot;) val vivekn = new ViveknSentimentApproach() .setInputCols(&quot;document&quot;, &quot;normal&quot;) .setSentimentCol(&quot;train_sentiment&quot;) .setOutputCol(&quot;result_sentiment&quot;) val finisher = new Finisher() .setInputCols(&quot;result_sentiment&quot;) .setOutputCols(&quot;final_sentiment&quot;) val pipeline = new Pipeline().setStages(Array(document, token, normalizer, vivekn, finisher)) val training = Seq( (&quot;I really liked this movie!&quot;, &quot;positive&quot;), (&quot;The cast was horrible&quot;, &quot;negative&quot;), (&quot;Never going to watch this again or recommend it to anyone&quot;, &quot;negative&quot;), (&quot;It&#39;s a waste of time&quot;, &quot;negative&quot;), (&quot;I loved the protagonist&quot;, &quot;positive&quot;), (&quot;The music was really really good&quot;, &quot;positive&quot;) ).toDF(&quot;text&quot;, &quot;train_sentiment&quot;) val pipelineModel = pipeline.fit(training) val data = Seq( &quot;I recommend this movie&quot;, &quot;Dont waste your time!!!&quot; ).toDF(&quot;text&quot;) val result = pipelineModel.transform(data) result.select(&quot;final_sentiment&quot;).show(false) ++ |final_sentiment| ++ |[positive] | |[negative] | ++ Sentiment analyser inspired by the algorithm by Vivek Narayanan https://github.com/vivekn/sentiment/. The algorithm is based on the paper “Fast and accurate sentiment classification using an enhanced Naive Bayes model”. This is the instantiated model of the ViveknSentimentApproach. For training your own model, please see the documentation of that class. The analyzer requires sentence boundaries to give a score in context. Tokenization is needed to make sure tokens are within bounds. Transitivity requirements are also required. For extended examples of usage, see the Spark NLP Workshop and the ViveknSentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: ViveknSentimentModel Scala API: ViveknSentimentModel Source: ViveknSentimentModel WordEmbeddings Approach Model Word Embeddings lookup annotator that maps tokens to vectors. For instantiated/pretrained models, see WordEmbeddingsModel. A custom token lookup dictionary for embeddings can be set with setStoragePath. Each line of the provided file needs to have a token, followed by their vector representation, delimited by a spaces. ... are 0.39658191506190343 0.630968081620067 0.5393722253731201 0.8428180123359783 were 0.7535235923631415 0.9699218875629833 0.10397182122983872 0.11833962569383116 stress 0.0492683418305907 0.9415954572751959 0.47624463167525755 0.16790967216778263 induced 0.1535748762292387 0.33498936903209897 0.9235178224122094 0.1158772920395934 ... If a token is not found in the dictionary, then the result will be a zero vector of the same dimension. Statistics about the rate of converted tokens, can be retrieved with[WordEmbeddingsModel.withCoverageColumn and WordEmbeddingsModel.overallCoverage. For extended examples of usage, see the Spark NLP Workshop and the WordEmbeddingsTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: WordEmbeddings Scala API: WordEmbeddings Source: WordEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the file `random_embeddings_dim4.txt` has the form of the content above. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddings() .setStoragePath(&quot;src/test/resources/random_embeddings_dim4.txt&quot;, ReadAs.TEXT) .setStorageRef(&quot;glove_4d&quot;) .setDimension(4) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) .setCleanAnnotations(False) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;The patient was diagnosed with diabetes.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(truncate=False) +-+ |result | +-+ |[0.9439099431037903,0.4707513153553009,0.806300163269043,0.16176554560661316] | |[0.7966810464859009,0.5551124811172485,0.8861005902290344,0.28284206986427307] | |[0.025029370561242104,0.35177749395370483,0.052506182342767715,0.1887107789516449]| |[0.08617766946554184,0.8399239182472229,0.5395117998123169,0.7864698767662048] | |[0.6599600911140442,0.16109347343444824,0.6041093468666077,0.8913561105728149] | |[0.5955275893211365,0.01899011991918087,0.4397728443145752,0.8911281824111938] | |[0.9840458631515503,0.7599489092826843,0.9417727589607239,0.8624503016471863] | +-+ // In this example, the file `random_embeddings_dim4.txt` has the form of the content above. import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.embeddings.WordEmbeddings import com.johnsnowlabs.nlp.util.io.ReadAs import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = new WordEmbeddings() .setStoragePath(&quot;src/test/resources/random_embeddings_dim4.txt&quot;, ReadAs.TEXT) .setStorageRef(&quot;glove_4d&quot;) .setDimension(4) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsFinisher )) val data = Seq(&quot;The patient was diagnosed with diabetes.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(false) +-+ |result | +-+ |[0.9439099431037903,0.4707513153553009,0.806300163269043,0.16176554560661316] | |[0.7966810464859009,0.5551124811172485,0.8861005902290344,0.28284206986427307] | |[0.025029370561242104,0.35177749395370483,0.052506182342767715,0.1887107789516449]| |[0.08617766946554184,0.8399239182472229,0.5395117998123169,0.7864698767662048] | |[0.6599600911140442,0.16109347343444824,0.6041093468666077,0.8913561105728149] | |[0.5955275893211365,0.01899011991918087,0.4397728443145752,0.8911281824111938] | |[0.9840458631515503,0.7599489092826843,0.9417727589607239,0.8624503016471863] | +-+ Word Embeddings lookup annotator that maps tokens to vectors This is the instantiated model of WordEmbeddings. Pretrained models can be loaded with pretrained of the companion object: val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;glove_100d&quot;, if no name is provided. For available pretrained models please see the Models Hub. There are also two convenient functions to retrieve the embeddings coverage with respect to the transformed dataset: withCoverageColumn(dataset, embeddingsCol, outputCol): Adds a custom column with word coverage stats for the embedded field: (coveredWords, totalWords, coveragePercentage). This creates a new column with statistics for each row. val wordsCoverage = WordEmbeddingsModel.withCoverageColumn(resultDF, &quot;embeddings&quot;, &quot;cov_embeddings&quot;) wordsCoverage.select(&quot;text&quot;,&quot;cov_embeddings&quot;).show(false) +-+--+ |text |cov_embeddings| +-+--+ |This is a sentence.|[5, 5, 1.0] | +-+--+ overallCoverage(dataset, embeddingsCol): Calculates overall word coverage for the whole data in the embedded field. This returns a single coverage object considering all rows in the field. val wordsOverallCoverage = WordEmbeddingsModel.overallCoverage(wordsCoverage,&quot;embeddings&quot;).percentage 1.0 For extended examples of usage, see the Spark NLP Workshop and the WordEmbeddingsTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: WordEmbeddingsModel Scala API: WordEmbeddingsModel Source: WordEmbeddingsModel WordSegmenter Approach Model Trains a WordSegmenter which tokenizes non-english or non-whitespace separated texts. Many languages are not whitespace separated and their sentences are a concatenation of many symbols, like Korean, Japanese or Chinese. Without understanding the language, splitting the words into their corresponding tokens is impossible. The WordSegmenter is trained to understand these languages and split them into semantically correct parts. For instantiated/pretrained models, see WordSegmenterModel. To train your own model, a training dataset consisting of Part-Of-Speech tags is required. The data has to be loaded into a dataframe, where the column is an Annotation of type &quot;POS&quot;. This can be set with setPosColumn. Tip: The helper class POS might be useful to read training data into data frames. For extended examples of usage, see the Spark NLP Workshop and the WordSegmenterTest. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: WordSegmenterApproach Scala API: WordSegmenterApproach Source: WordSegmenterApproach Show Example PythonScala # In this example, `&quot;chinese_train.utf8&quot;` is in the form of # # 十|LL 四|RR 不|LL 是|RR 四|LL 十|RR # # and is loaded with the `POS` class to create a dataframe of `&quot;POS&quot;` type Annotations. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) wordSegmenter = WordSegmenterApproach() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) .setPosColumn(&quot;tags&quot;) .setNIterations(5) pipeline = Pipeline().setStages([ documentAssembler, wordSegmenter ]) trainingDataSet = POS().readDataset( spark, &quot;src/test/resources/word-segmenter/chinese_train.utf8&quot; ) pipelineModel = pipeline.fit(trainingDataSet) // In this example, `&quot;chinese_train.utf8&quot;` is in the form of // // 十|LL 四|RR 不|LL 是|RR 四|LL 十|RR // // and is loaded with the `POS` class to create a dataframe of `&quot;POS&quot;` type Annotations. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.ws.WordSegmenterApproach import com.johnsnowlabs.nlp.training.POS import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val wordSegmenter = new WordSegmenterApproach() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) .setPosColumn(&quot;tags&quot;) .setNIterations(5) val pipeline = new Pipeline().setStages(Array( documentAssembler, wordSegmenter )) val trainingDataSet = POS().readDataset( ResourceHelper.spark, &quot;src/test/resources/word-segmenter/chinese_train.utf8&quot; ) val pipelineModel = pipeline.fit(trainingDataSet) WordSegmenter which tokenizes non-english or non-whitespace separated texts. Many languages are not whitespace separated and their sentences are a concatenation of many symbols, like Korean, Japanese or Chinese. Without understanding the language, splitting the words into their corresponding tokens is impossible. The WordSegmenter is trained to understand these languages and plit them into semantically correct parts. This is the instantiated model of the WordSegmenterApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val wordSegmenter = WordSegmenterModel.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;words_segmented&quot;) The default model is &quot;wordseg_pku&quot;, default language is &quot;zh&quot;, if no values are provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Spark NLP Workshop and the WordSegmenterTest. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: WordSegmenterModel Scala API: WordSegmenterModel Source: WordSegmenterModel YakeKeywordExtraction Yake is an Unsupervised, Corpus-Independent, Domain and Language-Independent and Single-Document keyword extraction algorithm. Extracting keywords from texts has become a challenge for individuals and organizations as the information grows in complexity and size. The need to automate this task so that text can be processed in a timely and adequate manner has led to the emergence of automatic keyword extraction tools. Yake is a novel feature-based system for multi-lingual keyword extraction, which supports texts of different sizes, domain or languages. Unlike other approaches, Yake does not rely on dictionaries nor thesauri, neither is trained against any corpora. Instead, it follows an unsupervised approach which builds upon features extracted from the text, making it thus applicable to documents written in different languages without the need for further knowledge. This can be beneficial for a large number of tasks and a plethora of situations where access to training corpora is either limited or restricted. The algorithm makes use of the position of a sentence and token. Therefore, to use the annotator, the text should be first sent through a Sentence Boundary Detector and then a tokenizer. Note that each keyword will be given a keyword score greater than 0 (The lower the score better the keyword). Therefore to filter the keywords, an upper bound for the score can be set with setThreshold. For extended examples of usage, see the Spark NLP Workshop and the YakeTestSpec. Sources : Campos, R., Mangaravite, V., Pasquali, A., Jatowt, A., Jorge, A., Nunes, C. and Jatowt, A. (2020). YAKE! Keyword Extraction from Single Documents using Multiple Local Features. In Information Sciences Journal. Elsevier, Vol 509, pp 257-289 Paper abstract: As the amount of generated information grows, reading and summarizing texts of large collections turns into a challenging task. Many documents do not come with descriptive terms, thus requiring humans to generate keywords on-the-fly. The need to automate this kind of task demands the development of keyword extraction systems with the ability to automatically identify keywords within the text. One approach is to resort to machine-learning algorithms. These, however, depend on large annotated text corpora, which are not always available. An alternative solution is to consider an unsupervised approach. In this article, we describe YAKE!, a light-weight unsupervised automatic keyword extraction method which rests on statistical text features extracted from single documents to select the most relevant keywords of a text. Our system does not need to be trained on a particular set of documents, nor does it depend on dictionaries, external corpora, text size, language, or domain. To demonstrate the merits and significance of YAKE!, we compare it against ten state-of-the-art unsupervised approaches and one supervised method. Experimental results carried out on top of twenty datasets show that YAKE! significantly outperforms other unsupervised methods on texts of different sizes, languages, and domains. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: YakeKeywordExtraction Scala API: YakeKeywordExtraction Source: YakeKeywordExtraction Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) token = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) .setContextChars([&quot;(&quot;, &quot;]&quot;, &quot;?&quot;, &quot;!&quot;, &quot;.&quot;, &quot;,&quot;]) keywords = YakeKeywordExtraction() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;keywords&quot;) .setThreshold(0.6) .setMinNGrams(2) .setNKeywords(10) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, token, keywords ]) data = spark.createDataFrame([[ &quot;Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud Next conference in San Francisco this week, the official announcement could come as early as tomorrow. Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. Google itself declined &#39;to comment on rumors&#39;. Kaggle, which has about half a million data scientists on its platform, was founded by Goldbloom and Ben Hamner in 2010. The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, it has managed to stay well ahead of them by focusing on its specific niche. The service is basically the de facto home for running data science and machine learning competitions. With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow and other projects). Kaggle has a bit of a history with Google, too, but that&#39;s pretty recent. Earlier this month, Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google will keep the service running - likely under its current name. While the acquisition is probably more about Kaggle&#39;s community than technology, Kaggle did build some interesting tools for hosting its competition and &#39;kernels&#39;, too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can share this code on the platform (the company previously called them &#39;scripts&#39;). Like similar competition-centric sites, Kaggle also runs a job board, too. It&#39;s unclear what Google will do with that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it&#39;s $12.75) since its launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, NaRavikant, Google chie economist Hal Varian, Khosla Ventures and Yuri Milner&quot; ]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) # combine the result and score (contained in keywords.metadata) scores = result .selectExpr(&quot;explode(arrays_zip(keywords.result, keywords.metadata)) as resultTuples&quot;) .selectExpr(&quot;resultTuples[&#39;0&#39;] as keyword&quot;, &quot;resultTuples[&#39;1&#39;].score as score&quot;) # Order ascending, as lower scores means higher importance scores.orderBy(&quot;score&quot;).show(5, truncate = False) ++-+ |keyword |score | ++-+ |google cloud |0.32051516486864573| |google cloud platform|0.37786450577630676| |ceo anthony goldbloom|0.39922830978423146| |san francisco |0.40224744669493756| |anthony goldbloom |0.41584827825302534| ++-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{SentenceDetector, Tokenizer} import com.johnsnowlabs.nlp.annotators.keyword.yake.YakeKeywordExtraction import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val token = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) .setContextChars(Array(&quot;(&quot;, &quot;)&quot;, &quot;?&quot;, &quot;!&quot;, &quot;.&quot;, &quot;,&quot;)) val keywords = new YakeKeywordExtraction() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;keywords&quot;) .setThreshold(0.6f) .setMinNGrams(2) .setNKeywords(10) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, token, keywords )) val data = Seq( &quot;Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud Next conference in San Francisco this week, the official announcement could come as early as tomorrow. Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. Google itself declined &#39;to comment on rumors&#39;. Kaggle, which has about half a million data scientists on its platform, was founded by Goldbloom and Ben Hamner in 2010. The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, it has managed to stay well ahead of them by focusing on its specific niche. The service is basically the de facto home for running data science and machine learning competitions. With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow and other projects). Kaggle has a bit of a history with Google, too, but that&#39;s pretty recent. Earlier this month, Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google will keep the service running - likely under its current name. While the acquisition is probably more about Kaggle&#39;s community than technology, Kaggle did build some interesting tools for hosting its competition and &#39;kernels&#39;, too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can share this code on the platform (the company previously called them &#39;scripts&#39;). Like similar competition-centric sites, Kaggle also runs a job board, too. It&#39;s unclear what Google will do with that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it&#39;s $12.75) since its launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, Naval Ravikant, Google chief economist Hal Varian, Khosla Ventures and Yuri Milner&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) // combine the result and score (contained in keywords.metadata) val scores = result .selectExpr(&quot;explode(arrays_zip(keywords.result, keywords.metadata)) as resultTuples&quot;) .select($&quot;resultTuples.0&quot; as &quot;keyword&quot;, $&quot;resultTuples.1.score&quot;) // Order ascending, as lower scores means higher importance scores.orderBy(&quot;score&quot;).show(5, truncate = false) ++-+ |keyword |score | ++-+ |google cloud |0.32051516486864573| |google cloud platform|0.37786450577630676| |ceo anthony goldbloom|0.39922830978423146| |san francisco |0.40224744669493756| |anthony goldbloom |0.41584827825302534| ++-+",
    "url": "/docs/en/annotators",
    "relUrl": "/docs/en/annotators"
  },
  "12": {
    "id": "12",
    "title": "API Integration",
    "content": "All features offered by the Annotation Lab via UI are also accessible via API. The complete API documentation is available on the SWAGGER page of the Annotation Lab. This can be accessed via UI by clicking on the documentation icon on the left lower side of the screen as shown in the image below: Concrete query examples are provided for each available endpoint. Example of creating a new project via API Get Client Secret Get CLIENT_ID and CLIENT_SECRET by following the steps illustrated in the video. Annotation Lab: Collect the Client Secret Call API endpoint For creating a new project via API you can use the following python script. import requests import json # URL to Annotation Lab API_URL = &quot;https://123.45.67.89&quot; # Add user credentials USERNAME = &quot;user&quot; PASSWORD = &quot;password&quot; # The above video shows how to get CLIENT_ID and CLIENT_SECRET CLIENT_ID = &quot;...&quot; CLIENT_SECRET = &quot;...&quot; PROJECT_NAME = &quot;sample_project&quot; IDENTITY_MANAGEMENT_URL = API_URL + &quot;/auth/&quot; IDENTITY_MANAGEMENT_REALM = &quot;master&quot; HEADERS = { &quot;Host&quot;: API_URL.replace(&quot;http://&quot;, &quot;&quot;).replace(&quot;https://&quot;, &quot;&quot;), &quot;Origin&quot;: API_URL, &quot;Content-Type&quot;: &quot;application/json&quot;, } def get_cookies(): url = f&quot;{IDENTITY_MANAGEMENT_URL}realms/{IDENTITY_MANAGEMENT_REALM}/protocol/openid-connect/token&quot; data = { &quot;grant_type&quot;: &quot;password&quot;, &quot;username&quot;: USERNAME, &quot;password&quot;: PASSWORD, &quot;client_id&quot;: CLIENT_ID, &quot;client_secret&quot;: CLIENT_SECRET, } auth_info = requests.post(url, data=data).json() cookies = { &quot;access_token&quot;: f&quot;Bearer {auth_info[&#39;access_token&#39;]}&quot;, &quot;refresh_token&quot;: auth_info[&quot;refresh_token&quot;], } return cookies def create_project(): # GET THIS FROM SWAGGER DOC url = f&quot;{API_URL}/api/projects/create&quot; data = { &quot;project_name&quot;: PROJECT_NAME, &quot;project_description&quot;: &quot;&quot;, &quot;project_sampling&quot;: &quot;uniform&quot;, &quot;project_instruction&quot;: &quot;&quot;, } r = requests.post( url, headers=HEADERS, data=json.dumps(data), cookies=get_cookies() ) return r create_project()",
    "url": "/docs/en/alab/api",
    "relUrl": "/docs/en/alab/api"
  },
  "13": {
    "id": "13",
    "title": "Helper functions",
    "content": "Spark NLP Annotation functions The functions presented here help users manipulate annotations, by providing both UDFs and dataframe utilities to deal with them more easily Python In python, the functions are straight forward and have both UDF and Dataframe applications map_annotations(f, output_type: DataType) UDF that applies f(). Requires output DataType from pyspark.sql.types map_annotations_strict(f) UDF that apples an f() method that returns a list of Annotations map_annotations_col(dataframe: DataFrame, f, column: str, output_column: str, annotatyon_type: str, output_type: DataType = Annotation.arrayType()) applies f() to column from dataframe map_annotations_cols(dataframe: DataFrame, f, columns: str, output_column: str, annotatyon_type: str, output_type: DataType = Annotation.arrayType()) applies f() to columns from dataframe filter_by_annotations_col(dataframe, f, column) applies a boolean filter f() to column from dataframe explode_annotations_col(dataframe: DataFrame, column, output_column) explodes annotation column from dataframe Scala In Scala, importing inner functions brings implicits that allow these functions to be applied directly on top of the dataframe mapAnnotations(function: Seq[Annotation] =&gt; T, outputType: DataType) mapAnnotationsStrict(function: Seq[Annotation] =&gt; Seq[Annotation]) mapAnnotationsCol[T: TypeTag](column: String, outputCol: String,annotatorType: String, function: Seq[Annotation] =&gt; T) mapAnnotationsCol[T: TypeTag](cols: Seq[String], outputCol: String,annotatorType: String, function: Seq[Annotation] =&gt; T) eachAnnotationsCol[T: TypeTag](column: String, function: Seq[Annotation] =&gt; Unit) def explodeAnnotationsCol[T: TypeTag](column: String, outputCol: String) Imports: PythonScala from sparknlp.functions import * from sparknlp.annotation import Annotation import com.johnsnowlabs.nlp.functions._ import com.johnsnowlabs.nlp.Annotation Examples: Complete usage examples can be seen here: https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/234-release-candidate/jupyter/annotation/english/spark-nlp-basics/spark-nlp-basics-functions.ipynb PythonScala val modified = data.mapAnnotationsCol(&quot;pos&quot;, &quot;mod_pos&quot;,&quot;pos&quot; ,(_: Seq[Annotation]) =&gt; { &quot;hello world&quot; }) def my_annoation_map_function(annotations): return list(map(lambda a: Annotation( &#39;my_own_type&#39;, a.begin, a.end, a.result, {&#39;my_key&#39;: &#39;custom_annotation_data&#39;}, []), annotations)) result.select( map_annotations(my_annoation_map_function, Annotation.arrayType())(&#39;token&#39;) ).toDF(&quot;my output&quot;).show(truncate=False)",
    "url": "/docs/en/auxiliary",
    "relUrl": "/docs/en/auxiliary"
  },
  "14": {
    "id": "14",
    "title": "Developers Guideline",
    "content": "Cluster Speed Benchmarks Explanation of Benchmark Experiment Dataset : 1000 Clinical Texts from MTSamples Oncology Dataset, approx. 500 tokens per text. Driver : Standard_D4s_v3 - 16 GB Memory - 4 Cores Enable Autoscaling : False Cluster Mode : Standart Worker : Standard_D4s_v3 - 16 GB Memory - 4 Cores Standard_D4s_v2 - 28 GB Memory - 8 Cores Versions : Databricks Runtime Version : 8.3(Scala 2.12, Spark 3.1.1) spark-nlp Version: v3.2.3 spark-nlp-jsl Version : v3.2.3 Spark Version : v3.1.1 Spark NLP Pipeline : nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings_clinical, clinical_ner, ner_converter ]) The first experiment with 5 different cluster configurations : ner_chunk as a column in Spark NLP Pipeline (ner_converter) output data frame, exploded (lazy evaluation) as ner_chunk and ner_label. Then results were written as parquet and delta formats. A second experiment with 2 different cluster configuration : Spark NLP Pipeline output data frame (except word_embeddings column) was written as parquet and delta formats. In the first experiment with the most basic driver node and worker (1 worker x 4 cores) configuration selection, it took 4.64 mins and 4.53 mins to write 4 partitioned data as parquet and delta formats respectively. With basic driver node and 8 workers (x8 cores) configuration selection, it took 40 seconds and 22 seconds to write 1000 partitioned data as parquet and delta formats respectively. In the second experiment with basic driver node and 4 workers (x 4 cores) configuration selection, it took 1.41 mins as parquet and 1.42 mins as delta format to write 16 partitioned (exploded results) data. Without explode it took 1.08 mins as parquet and 1.12 mins as delta format to write the data frame. Since given computation durations are highly dependent on different parameters including driver node and worker node configurations as well as partitions, results show that explode method increases duration %10-30 on chosen configurations. BENCHMARK TABLES driver_name driver_memory driver_cores worker_name worker_memory worker_cores input_data_rows output_data_rows action total_worker_number total_cores partition duration Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 1 4 4 4.64 min Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 1 4 4 4.53 min Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 1 4 10 4.42 min Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 1 4 10 4.55 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 1 4 100 4.19 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 1 4 100 4.18 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 1 4 1000 5.01 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 1 4 1000 4.99 mins driver_name driver_memory driver_cores worker_name worker_memory worker_cores input_data_rows output_data_rows action total_worker_number total_cores partition duration Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 2 8 10 2.82 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 2 8 10 2.82 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 2 8 100 2.27 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 2 8 100 2.25 min Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 2 8 1000 2.65 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 2 8 1000 2.7 mins driver_name driver_memory driver_cores worker_name worker_memory worker_cores input_data_rows output_data_rows action total_worker_number total_cores partition duration Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 4 16 10 1.4 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 4 16 10 1.76 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 4 16 16 1.41 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 4 16 16 1.42 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 4 16 32 1.36 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 4 16 32 1.35 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 4 16 100 1.21 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 4 16 100 1.24 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 4 16 1000 1.42 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 4 16 1000 1.46 mins driver_name driver_memory driver_cores worker_name worker_memory worker_cores input_data_rows output_data_rows action total_worker_number total_cores partition duration Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 1000 write_parquet 4 16 10 1.39 min Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 1000 write_deltalake 4 16 10 1.33 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 1000 write_parquet 4 16 16 1.08 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 1000 write_deltalake 4 16 16 1,12 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 1000 write_parquet 4 16 32 1.09 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 1000 write_deltalake 4 16 32 1.23 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 1000 write_parquet 4 16 100 1.05 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 1000 write_deltalake 4 16 100 1.11 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 1000 write_parquet 4 16 1000 1.37 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 1000 write_deltalake 4 16 1000 1.29 mins driver_name driver_memory driver_cores worker_name worker_memory worker_cores input_data_rows output_data_rows action total_worker_number total_cores partition duration Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 8 32 32 1.21 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 8 32 32 55.8 sec Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 8 32 100 41 sec Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 8 32 100 48 sec Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 8 32 1000 1.36 min Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 8 32 1000 48 sec driver_name driver_memory driver_cores worker_name worker_memory worker_cores input_data_rows output_data_rows action total_worker_number total_cores partition duration Standard_D4s_v3 16 GB 4 Standard_D4s_v2 28 GB 8 1000 78000 write_parquet 8 64 64 36 sec Standard_D4s_v3 16 GB 4 Standard_D4s_v2 28 GB 8 1000 78000 write_deltalake 8 64 64 19 sec Standard_D4s_v3 16 GB 4 Standard_D4s_v2 28 GB 8 1000 78000 write_parquet 8 64 100 21 sec Standard_D4s_v3 16 GB 4 Standard_D4s_v2 28 GB 8 1000 78000 write_deltalake 8 64 100 41 sec Standard_D4s_v3 16 GB 4 Standard_D4s_v2 28 GB 8 1000 78000 write_parquet 8 64 1000 40 sec Standard_D4s_v3 16 GB 4 Standard_D4s_v2 28 GB 8 1000 78000 write_deltalake 8 64 1000 22 sec",
    "url": "/docs/en/benchmark",
    "relUrl": "/docs/en/benchmark"
  },
  "15": {
    "id": "15",
    "title": "Bring Your Own License",
    "content": "By default the Annotation Lab allows access to community pretrained models and embeddings. Those are available on the Models Hub page. In order to gain access to licensed resources (e.g. pretrained models and embeddings) admin users can import a Spark NLP for Healthcare license which will activate additional features: access to licensed models for preannotation access to healthcare embeddings access to training custom models using licensed embeddings Admin users can upload a Spark NLP license json file by visiting the License page. The expected format for license the one generated by John Snow Labs license server. Once a valid license is uploaded, all the licensed/Healthcare models and embeddings become available for download. The License page shows the history of license uploads with detailed information like License Issued Date, Expiry Date, and Remaining Days of each upload. Admin users will see License expiry warnings in the Menu List if any license is approaching the expiry date or has expired.",
    "url": "/docs/en/alab/byol",
    "relUrl": "/docs/en/alab/byol"
  },
  "16": {
    "id": "16",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/classify_documents",
    "relUrl": "/classify_documents"
  },
  "17": {
    "id": "17",
    "title": "General Concepts",
    "content": "Concepts Spark ML provides a set of Machine Learning applications that can be build using two main components: Estimators and Transformers. The Estimators have a method called fit() which secures and trains a piece of data to such application. The Transformer is generally the result of a fitting process and applies changes to the the target dataset. These components have been embedded to be applicable to Spark NLP. Pipelines are a mechanism for combining multiple estimators and transformers in a single workflow. They allow multiple chained transformations along a Machine Learning task. For more information please refer to Spark ML library. Annotation The basic result of a Spark NLP operation is an annotation. It’s structure includes: annotatorType: the type of annotator that generated the current annotation begin: the begin of the matched content relative to raw-text end: the end of the matched content relative to raw-text result: the main output of the annotation metadata: content of matched result and additional information embeddings: (new in 2.0) contains vector mappings if required This object is automatically generated by annotators after a transform process. No manual work is required. However, it is important to clearly understand the structure of an annotation to be able too efficiently use it. Annotators Annotators are the spearhead of NLP functions in Spark NLP. There are two forms of annotators: Annotator Approaches: are those who represent a Spark ML Estimator and require a training stage. They have a function called fit(data) which trains a model based on some data. They produce the second type of annotator which is an annotator model or transformer. Annotator Models: are spark models or transformers, meaning they have a transform(data) function. This function takes as input a dataframe to which it adds a new column containing the result of the current annotation. All transformers are additive, meaning they append to current data, never replace or delete previous information. Both forms of annotators can be included in a Pipeline. All annotators included in a Pipeline will be automatically executed in the defined order and will transform the data accordingly. A Pipeline is turned into a PipelineModel after the fit() stage. The Pipeline can be saved to disk and re-loaded at any time. Common Functions setInputCols(column_names): Takes a list of column names of annotations required by this annotator. Those are generated by the annotators which precede the current annotator in the pipeline. setOutputCol(column_name): Defines the name of the column containing the result of the current annotator. Use this name as an input for other annotators down the pipeline requiring the outputs generated by the current annotator. Quickly annotate some text You can run these examples using Python or Scala. The easiest way to run the python examples is by starting a pyspark jupyter notebook including the spark-nlp package: $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.7 -y $ conda activate sparknlp # spark-nlp by default is based on pyspark 3.x $ pip install spark-nlp==3.3.4 pyspark==3.1.2 jupyter $ jupyter notebook Explain Document ML Spark NLP offers a variety of pretrained pipelines that will help you get started, and get a sense of how the library works. We are constantly working on improving the available content. You can checkout a demo application of the Explain Document ML pipeline here: View Demo Downloading and using a pretrained pipeline Explain Document ML (explain_document_ml) is a pretrained pipeline that does a little bit of everything NLP related. Let’s try it out in scala. Note that the first time you run the below code it might take longer since it downloads the pretrained pipeline from our servers! PythonScala import sparknlp sparknlp.start() from sparknlp.pretrained import PretrainedPipeline explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) annotations = explain_document_pipeline.annotate(&quot;We are very happy about SparkNLP&quot;) print(annotations) OUTPUT: { &#39;stem&#39;: [&#39;we&#39;, &#39;ar&#39;, &#39;veri&#39;, &#39;happi&#39;, &#39;about&#39;, &#39;sparknlp&#39;], &#39;checked&#39;: [&#39;We&#39;, &#39;are&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], &#39;lemma&#39;: [&#39;We&#39;, &#39;be&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], &#39;document&#39;: [&#39;We are very happy about SparkNLP&#39;], &#39;pos&#39;: [&#39;PRP&#39;, &#39;VBP&#39;, &#39;RB&#39;, &#39;JJ&#39;, &#39;IN&#39;, &#39;NNP&#39;], &#39;token&#39;: [&#39;We&#39;, &#39;are&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], &#39;sentence&#39;: [&#39;We are very happy about SparkNLP&#39;] } import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) OUTPUT: explain_document_ml download started this may take some time. Approximate size to download 9.4 MB Download done! Loading the resource. explain_document_pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_ml,en,public/models) val annotations = explainDocumentPipeline.annotate(&quot;We are very happy about SparkNLP&quot;) println(annotations) OUTPUT: Map( stem -&gt; List(we, ar, veri, happi, about, sparknlp), checked -&gt; List(We, are, very, happy, about, SparkNLP), lemma -&gt; List(We, be, very, happy, about, SparkNLP), document -&gt; List(We are very happy about SparkNLP), pos -&gt; ArrayBuffer(PRP, VBP, RB, JJ, IN, NNP), token -&gt; List(We, are, very, happy, about, SparkNLP), sentence -&gt; List(We are very happy about SparkNLP) ) As you can see the explain_document_ml is able to annotate any “document” providing as output a list of stems, check-spelling, lemmas, part of speech tags, tokens and sentence boundary detection and all this “out-of-the-box”!. Using a pretrained pipeline with spark dataframes You can also use the pipeline with a spark dataframe. You just need to create first a spark dataframe with a column named “text” that will work as the input for the pipeline and then use the .transform() method to run the pipeline over that dataframe and store the outputs of the different components in a spark dataframe. Remember than when starting jupyter notebook from pyspark or when running the spark-shell for scala, a Spark Session is started in the background by default within the namespace ‘scala’. PythonScala import sparknlp sparknlp.start() sentences = [ [&#39;Hello, this is an example sentence&#39;], [&#39;And this is a second sentence.&#39;] ] # spark is the Spark Session automatically started by pyspark. data = spark.createDataFrame(sentences).toDF(&quot;text&quot;) # Download the pretrained pipeline from Johnsnowlab&#39;s servers explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) OUTPUT: explain_document_ml download started this may take some time. Approx size to download 9.4 MB [OK!] # Transform &#39;data&#39; and store output in a new &#39;annotations_df&#39; dataframe annotations_df = explain_document_pipeline.transform(data) # Show the results annotations_df.show() OUTPUT: +--+--+--+--+--+--+--+--+ | text| document| sentence| token| checked| lemma| stem| pos| +--+--+--+--+--+--+--+--+ |Hello, this is an...|[[document, 0, 33...|[[document, 0, 33...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, he...|[[pos, 0, 4, UH, ...| |And this is a sec...|[[document, 0, 29...|[[document, 0, 29...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, an...|[[pos, 0, 2, CC, ...| +--+--+--+--+--+--+--+--+ val data = Seq( &quot;Hello, this is an example sentence&quot;, &quot;And this is a second sentence&quot;) .toDF(&quot;text&quot;) data.show(truncate=false) OUTPUT: ++ |text | ++ |Hello, this is an example set | |And this is a second sentence.| ++ val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) val annotations_df = explainDocumentPipeline.transform(data) annotations_df.show() OUTPUT: +--+--+--+--+--+--+--+--+ | text| document| sentence| token| checked| lemma| stem| pos| +--+--+--+--+--+--+--+--+ |Hello, this is an...|[[document, 0, 33...|[[document, 0, 33...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, he...|[[pos, 0, 4, UH, ...| |And this is a sec...|[[document, 0, 29...|[[document, 0, 29...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, an...|[[pos, 0, 2, CC, ...| +--+--+--+--+--+--+--+--+ Manipulating pipelines The output of the previous DataFrame was in terms of Annotation objects. This output is not really comfortable to deal with, as you can see by running the code: PythonScala annotations_df.select(&quot;token&quot;).show(truncate=False) OUTPUT: +--+ |token | +--+ |[[token, 0, 4, Hello, [sentence -&gt; 0], [], []], [token, 5, 5, ,, [sentence -&gt; 0], [], []], [token, 7, 10, this, [sentence -&gt; 0], [], []], [token, 12, 13, is, [sentence -&gt; 0], [], []], [token, 15, 16, an, [sentence -&gt; 0], [], []], [token, 18, 24, example, [sentence -&gt; 0], [], []], [token, 26, 33, sentence, [sentence -&gt; 0], [], []]]| |[[token, 0, 2, And, [sentence -&gt; 0], [], []], [token, 4, 7, this, [sentence -&gt; 0], [], []], [token, 9, 10, is, [sentence -&gt; 0], [], []], [token, 12, 12, a, [sentence -&gt; 0], [], []], [token, 14, 19, second, [sentence -&gt; 0], [], []], [token, 21, 28, sentence, [sentence -&gt; 0], [], []], [token, 29, 29, ., [sentence -&gt; 0], [], []]] | +--+ annotations_df.select(&quot;token&quot;).show(truncate=false) OUTPUT: +--+ |token | +--+ |[[token, 0, 4, Hello, [sentence -&gt; 0], [], []], [token, 5, 5, ,, [sentence -&gt; 0], [], []], [token, 7, 10, this, [sentence -&gt; 0], [], []], [token, 12, 13, is, [sentence -&gt; 0], [], []], [token, 15, 16, an, [sentence -&gt; 0], [], []], [token, 18, 24, example, [sentence -&gt; 0], [], []], [token, 26, 33, sentence, [sentence -&gt; 0], [], []]]| |[[token, 0, 2, And, [sentence -&gt; 0], [], []], [token, 4, 7, this, [sentence -&gt; 0], [], []], [token, 9, 10, is, [sentence -&gt; 0], [], []], [token, 12, 12, a, [sentence -&gt; 0], [], []], [token, 14, 19, second, [sentence -&gt; 0], [], []], [token, 21, 28, sentence, [sentence -&gt; 0], [], []], [token, 29, 29, ., [sentence -&gt; 0], [], []]] | +--+ What if we want to deal with just the resulting annotations? We can use the Finisher annotator, retrieve the Explain Document ML pipeline, and add them together in a Spark ML Pipeline. Remember that pretrained pipelines expect the input column to be named “text”. PythonScala from sparknlp import Finisher from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline finisher = Finisher().setInputCols([&quot;token&quot;, &quot;lemmas&quot;, &quot;pos&quot;]) explain_pipeline_model = PretrainedPipeline(&quot;explain_document_ml&quot;).model pipeline = Pipeline() .setStages([ explain_pipeline_model, finisher ]) sentences = [ [&#39;Hello, this is an example sentence&#39;], [&#39;And this is a second sentence.&#39;] ] data = spark.createDataFrame(sentences).toDF(&quot;text&quot;) model = pipeline.fit(data) annotations_finished_df = model.transform(data) annotations_finished_df.select(&#39;finished_token&#39;).show(truncate=False) OUTPUT: +-+ |finished_token | +-+ |[Hello, ,, this, is, an, example, sentence]| |[And, this, is, a, second, sentence, .] | +-+ scala&gt; import com.johnsnowlabs.nlp.Finisher scala&gt; import org.apache.spark.ml.Pipeline scala&gt; val finisher = new Finisher().setInputCols(&quot;token&quot;, &quot;lemma&quot;, &quot;pos&quot;) scala&gt; val explainPipelineModel = PretrainedPipeline(&quot;explain_document_ml&quot;).model scala&gt; val pipeline = new Pipeline(). setStages(Array( explainPipelineModel, finisher )) scala&gt; val data = Seq( &quot;Hello, this is an example sentence&quot;, &quot;And this is a second sentence&quot;) .toDF(&quot;text&quot;) scala&gt; val model = pipeline.fit(data) scala&gt; val annotations_df = model.transform(data) scala&gt; annotations_df.select(&quot;finished_token&quot;).show(truncate=false) OUTPUT: +-+ |finished_token | +-+ |[Hello, ,, this, is, an, example, sentence]| |[And, this, is, a, second, sentence, .] | +-+ Setup your own pipeline Annotator types Every annotator has a type. Those annotators that share a type, can be used interchangeably, meaning you could use any of them when needed. For example, when a token type annotator is required by another annotator, such as a sentiment analysis annotator, you can either provide a normalized token or a lemma, as both are of type token. Necessary imports Since version 1.5.0 we are making necessary imports easy to reach, base._ will include general Spark NLP transformers and concepts, while annotator._ will include all annotators that we currently provide. We also need Spark ML pipelines. PythonScala from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline DocumentAssembler: Getting data in In order to get through the NLP process, we need to get raw data annotated. There is a special transformer that does this for us: the DocumentAssembler, it creates the first annotation of type Document which may be used by annotators down the road. PythonScala documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val documentAssembler = new DocumentAssembler(). setInputCol(&quot;text&quot;). setOutputCol(&quot;document&quot;) Sentence detection and tokenization In this quick example, we now proceed to identify the sentences in the input document. SentenceDetector requires a Document annotation, which is provided by the DocumentAssembler output, and it’s itself a Document type token. The Tokenizer requires a Document annotation type. That means it works both with DocumentAssembler or SentenceDetector output. In the following example we use the sentence output. PythonScala sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;Sentence&quot;) regexTokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) val sentenceDetector = new SentenceDetector(). setInputCols(Array(&quot;document&quot;)). setOutputCol(&quot;sentence&quot;) val regexTokenizer = new Tokenizer(). setInputCols(Array(&quot;sentence&quot;)). setOutputCol(&quot;token&quot;) Spark NLP also includes another special transformer, called Finisher to show tokens in a human language. finisher = Finisher() .setInputCols([&quot;token&quot;]) .setCleanAnnotations(False) val finisher = new Finisher(). setInputCols(&quot;token&quot;). setCleanAnnotations(false) Finisher: Getting data out At the end of each pipeline or any stage that was done by Spark NLP, you may want to get results out whether onto another pipeline or simply write them on disk. The Finisher annotator helps you to clean the metadata (if it’s set to true) and output the results into an array: PythonScala finisher = Finisher() .setInputCols([&quot;token&quot;]) .setIncludeMetadata(True) val finisher = new Finisher() .setInputCols(&quot;token&quot;) .setIncludeMetadata(true) If you need to have a flattened DataFrame (each sub-array in a new column) from any annotations other than struct type columns, you can use explode function from Spark SQL. You can also use Apache Spark functions (SQL) to manipulate the output DataFrame in any way you need. Here we combine the tokens and NER results together: import pyspark.sql.functions as F df.withColumn(&quot;tmp&quot;, F.explode(&quot;chunk&quot;)).select(&quot;tmp.*&quot;) finisher.withColumn(&quot;newCol&quot;, explode(arrays_zip($&quot;finished_token&quot;, $&quot;finished_ner&quot;))) import org.apache.spark.sql.functions._ df.withColumn(&quot;tmp&quot;, explode(col(&quot;chunk&quot;))).select(&quot;tmp.*&quot;) Using Spark ML Pipeline Now we want to put all this together and retrieve the results, we use a Pipeline for this. We use the same data in fit() that we will use in transform since none of the pipeline stages have a training stage. PythonScala pipeline = Pipeline() .setStages([ documentAssembler, sentenceDetector, regexTokenizer, finisher ]) OUTPUT: +-+ |finished_token | +-+ |[hello, ,, this, is, an, example, sentence]| +-+ val pipeline = new Pipeline(). setStages(Array( documentAssembler, sentenceDetector, regexTokenizer, finisher )) val data = Seq(&quot;hello, this is an example sentence&quot;).toDF(&quot;text&quot;) val annotations = pipeline. fit(data). transform(data).toDF(&quot;text&quot;)) annotations.select(&quot;finished_token&quot;).show(truncate=false) OUTPUT: +-+ |finished_token | +-+ |[hello, ,, this, is, an, example, sentence]| +-+ Using Spark NLP’s LightPipeline LightPipeline is a Spark NLP specific Pipeline class equivalent to Spark ML Pipeline. The difference is that it’s execution does not hold to Spark principles, instead it computes everything locally (but in parallel) in order to achieve fast results when dealing with small amounts of data. This means, we do not input a Spark Dataframe, but a string or an Array of strings instead, to be annotated. To create Light Pipelines, you need to input an already trained (fit) Spark ML Pipeline. It’s transform() stage is converted into annotate() instead. PythonScala from sparknlp.base import LightPipeline explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) lightPipeline = LightPipeline(explain_document_pipeline.model) OUTPUT: explain_document_ml download started this may take some time. Approx size to download 9.4 MB [OK!] lightPipeline.annotate(&quot;Hello world, please annotate my text&quot;) OUTPUT: {&#39;stem&#39;: [&#39;hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;pleas&#39;, &#39;annot&#39;, &#39;my&#39;, &#39;text&#39;], &#39;checked&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;my&#39;, &#39;text&#39;], &#39;lemma&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;i&#39;, &#39;text&#39;], &#39;document&#39;: [&#39;Hello world, please annotate my text&#39;], &#39;pos&#39;: [&#39;UH&#39;, &#39;NN&#39;, &#39;,&#39;, &#39;VB&#39;, &#39;NN&#39;, &#39;PRP$&#39;, &#39;NN&#39;], &#39;token&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;my&#39;, &#39;text&#39;], &#39;sentence&#39;: [&#39;Hello world, please annotate my text&#39;]} import com.johnsnowlabs.nlp.base._ val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) val lightPipeline = new LightPipeline(explainDocumentPipeline.model) lightPipeline.annotate(&quot;Hello world, please annotate my text&quot;) OUTPUT: Map[String,Seq[String]] = Map( stem -&gt; List(hello, world, ,, pleas, annot, my, text), checked -&gt; List(Hello, world, ,, please, annotate, my, tex), lemma -&gt; List(Hello, world, ,, please, annotate, i, text), document -&gt; List(Hello world, please annotate my text), pos -&gt; ArrayBuffer(UH, NN, ,, VB, NN, PRP$, NN), token -&gt; List(Hello, world, ,, please, annotate, my, text), sentence -&gt; List(Hello world, please annotate my text) ) Training annotators Training methodology Training your own annotators is a key concept when dealing with real life scenarios. Any of the annotators provided above, such as pretrained pipelines and models, can be applied out-of-the-box to a specific use case, but better results are obtained when they are fine-tuned to your specific use-case. Dealing with real life problems ofter requires training your own models. In Spark NLP, we support three ways of training a custom annotator: Train from a dataset. Most annotators are capable of training from a dataset passed to fit() method just as Spark ML does. Annotators that use the suffix Approach are such trainable annotators. Training from fit() is the standard behavior in Spark ML. Annotators have different schema requirements for training. Check the reference to see what are the requirements of each annotators. Training from an external source: Some of our annotators train from an external file or folder passed to the annotator as a param. You will see such ones as setCorpus() or setDictionary() param setter methods, allowing you to configure the input to use. You can set Spark NLP to read them as Spark datasets or LINE_BY_LINE which is usually faster for small files. Last but not least, some of our annotators are Deep Learning based. These models may be trained with the standard AnnotatorApproach API just like any other annotator. For more advanced users, we also allow importing your own graphs or even training from Python and converting them into an AnnotatorModel. Spark NLP Imports base includes general Spark NLP transformers and concepts, annotator includes all annotators that we currently provide, embeddings includes word embedding annotators. Example: PythonScala from sparknlp.base import * from sparknlp.annotator import * from sparknlp.embeddings import * import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ Spark ML Pipelines SparkML Pipelines are a uniform structure that helps creating and tuning practical machine learning pipelines. Spark NLP integrates with them seamlessly so it is important to have this concept handy. Once a Pipeline is trained with fit(), it becomes a PipelineModel Example: PythonScala from pyspark.ml import Pipeline pipeline = Pipeline().setStages([...]) import org.apache.spark.ml.Pipeline new Pipeline().setStages(Array(...)) LightPipeline LightPipelines are Spark ML pipelines converted into a single machine but multithreaded task, becoming more than 10x times faster for smaller amounts of data (small is relative, but 50k sentences is roughly a good maximum). To use them, simply plug in a trained (fitted) pipeline. Example: PythonScala from sparknlp.base import LightPipeline LightPipeline(someTrainedPipeline).annotate(someStringOrArray) import com.johnsnowlabs.nlp.LightPipeline new LightPipeline(somePipelineModel).annotate(someStringOrArray)) Functions: annotate(string or string[]): returns dictionary list of annotation results fullAnnotate(string or string[]): returns dictionary list of entire annotations content For more details please refer to Using Spark NLP’s LightPipelines. RecursivePipeline Recursive pipelines are SparkNLP specific pipelines that allow a Spark ML Pipeline to know about itself on every Pipeline Stage task, allowing annotators to utilize this same pipeline against external resources to process them in the same way the user decides. Only some of our annotators take advantage of this. RecursivePipeline behaves exactly the same as normal Spark ML pipelines, so they can be used with the same intention. Example: PythonScala from sparknlp.annotator import * recursivePipeline = RecursivePipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, lemmatizer, finisher ]) import com.johnsnowlabs.nlp.RecursivePipeline val recursivePipeline = new RecursivePipeline() .setStages(Array( documentAssembler, sentenceDetector, tokenizer, lemmatizer, finisher )) Params and Features Annotator parameters SparkML uses ML Params to store pipeline parameter maps. In SparkNLP, we also use Features, which are a way to store parameter maps that are larger than just a string or a boolean. These features are serialized as either Parquet or RDD objects, allowing much faster and scalable annotator information. Features are also broadcasted among executors for better performance.",
    "url": "/docs/en/concepts",
    "relUrl": "/docs/en/concepts"
  },
  "18": {
    "id": "18",
    "title": "Contribute",
    "content": "Refer to our GitHub page to take a look at the GH Issues, as the project is yet small. You can create in there your own issues to either work on them yourself or simply propose them. Feel free to clone the repository locally and submit pull requests so we can review them and work together. feedback, ideas and bug reports testing and development training and testing nlp corpora documentation and research Help is always welcome, for any further questions, contact nlp@johnsnowlabs.com. Your own annotator model Creating your first annotator transformer should not be hard, here are a few guidelines to get you started. Lets assume we want a wrapper annotator, which puts a character surrounding tokens provided by a Tokenizer WordWrapper uid is utilized for transformer serialization, AnnotatorModel[MyAnnotator] will contain the common annotator logic We need to use standard constructor for java and python compatibility class WordWrapper(override val uid: String) extends AnnotatorModel[WordWrapper] { def this() = this(Identifiable.randomUID(&quot;WORD_WRAPPER&quot;)) } Annotator attributes This annotator is not flexible if we don’t provide parameters import com.johnsnowlabs.nlp.AnnotatorType._ override val annotatorType: AnnotatorType = TOKEN override val requiredAnnotatorTypes: Array[AnnotatorType] = Array[AnnotatorType](TOKEN) Annotator parameters This annotator is not flexible if we don’t provide parameters protected val character: Param[String] = new Param(this, &quot;character&quot;, &quot;this is the character used to wrap a token&quot;) def setCharacter(value: String): this.type = set(pattern, value) def getCharacter: String = $(pattern) setDefault(character, &quot;@&quot;) Annotator logic Here is how we act, annotations will automatically provide our required annotations We generally use annotatorType for metadata keys override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = { annotations.map(annotation =&gt; { Annotation( annotatorType, annotation.begin, annotation.end, Map(annotatorType -&gt; $(character) + annotation.result + $(character)) }) }",
    "url": "/contribute",
    "relUrl": "/contribute"
  },
  "19": {
    "id": "19",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/deidentification",
    "relUrl": "/deidentification"
  },
  "20": {
    "id": "20",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/demo",
    "relUrl": "/demo"
  },
  "21": {
    "id": "21",
    "title": "",
    "content": "",
    "url": "/demos",
    "relUrl": "/demos"
  },
  "22": {
    "id": "22",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/detect_sentiment_emotion",
    "relUrl": "/detect_sentiment_emotion"
  },
  "23": {
    "id": "23",
    "title": "Developers Guideline",
    "content": "Spark NLP is an open-source library and everyone’s contribution is welcome! In this section we provide a guide on how to setup your environment using IntelliJ IDEA for a smoother start. You can also check our video tutorials available on our YouTube channel: https://www.youtube.com/johnsnowlabs Setting up the Environment Import to IntelliJ IDEA Setup Spark NLP development environment. This section will cover library set up for IntelliJ IDEA. Before you begin, make sure what you have Java and Spark installed in your system. We suggest that you have installed jdk 8 and Apache Spark 2.4.x. To check installation run: java -version and spark-submit --version Next step is to open IntelliJ IDEA. On the Welcome to IntelliJ IDEA screen you will see ability to Check out from Version Controle. Log in into your github account in pop up. After select from a list Spark NLP repo url: https://github.com/JohnSnowLabs/spark-nlp and press clone button. If you don’t see url in the list, clone or fork repo first to your Github account and try again. When the repo cloned IDE will detect SBT file with dependencies. Click Yes to start import from sbt. In the Import from sbt pop up make sure you have JDK 8 detected. Click Ok to proceed and download required resources. If you already had dependences installed you may see the pop up Not empty folder, click Ok to ignore it and reload resources. IntelliJ IDEA will be open and it will start syncing SBT project. It make take some time, you will see the progress in the build output panel in the bottom of the screen. To see the project panel in the left press Alt+1. Next step is to install Python plugin to the IntelliJ IDEA. To do this, open File -&gt; Settings -&gt; Plugins, type Python in the search and select Python plugin by JetBrains. Install this plugin by clicking Install button. After this steps you can check project structure in the File -&gt; Project Structure -&gt; Modules. Make sure what you have spark-nlp and spark-nlp-build folders and no errors in the exported dependencies. In the Project settings check what project SDK is set to 1.8 and in Platform Settings -&gt; SDK&#39;s you have Java installation as well as Python installation. If you don’t see Python installed in the SDK&#39;s tab click + button, add Python SDK with new virtual environment in the project folder with Python 3.x. Compiling, assembly and unit testing Run tests in Scala Click Add configuration in the Top right corner. In the pop up click on the + and look for sbt task. In the Name field put Test. In the Tasks field write down test. After you can disable checkbox in Use sbt shell to have more custom configurations. In the VM parameters increase the memory by changing -Xmx1024M to -Xmx10G and click Ok. If everything was set up correctly you suhould see unabled green button Run ‘Test’ in the top right. Click on it to start running the tests. This algorithm will Run all tests under spark-nlp/src/test/scala/com.johnsnowlabs/ Copy tasks After you created task, click Edit configuration. Select target task and instead of + button you can click copy in the same menu. It will recreate all settings from parent task and create a new task. You can do it for Scala or for Python tasks. Run individual tests Open test file you want to run. For example, spark-nlp/src/test/scala/com.johnsnowlabs/nlp/FinisherTestSpec.scala. Right click on the class name and select Copy reference. It will copy to you buffer classpath - com.johnsnowlabs.nlp.FinisherTestSpec. Copy existing Scala task and Name it as FinisherTest. In the Tasks field write down &quot;testOnly *classpath*&quot; -&gt; &quot;testOnly com.johnsnowlabs.nlp.FinisherTestSpec&quot; and click Ok to save individual scala test run configuration. Press play button to run individual test. Debugging tests To run tests in debug mode click Debug button (next to play button). In this mode task will stop at the given break points. Run tests in Python To run Python test, first you need to configure project structure. Go to File -&gt; Project Settings -&gt; Modules, click on the + button and select New Module. In the pop up choose Python on left menu, select Python SDK from created virtual environment and click Next. Enter python in the Module name and click Finish. After you need to add Spark dependencies. Select created Python module and click on the + button in the Dependencies part. Choose Jars or directories… and find the find installation path of spark (usually the folder name is spark-2.4.5-bin-hadoop2.7). In the Spark folder go to the python/libs and select pyspark.zip to the project. Do the same for another file in the same folder - py4j-0.10.7-src.zip. All available tests are in spark-nlp/python/run-tests.py. Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and look for Python. In the Script path locate file spark-nlp/python/run-tests.py. Also you need to add SPARK_HOME environment variable to the project. Choose Environment variables and add new variable SPARK_HOME. Insert installation path of spark to the Value field. Click Ok to save and close pop up and click Ok to confirm new task creation. Before running the tests we need to install requered python dependencies in the new virtual environment. Select in the bottom menu Terminal and activate your environment with command source venv/bin/activate after install packages by running pip install pyspark==3.1.2 numpy Compiling jar Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and select sbt task. In the Name field put AssemblyCopy. In the Tasks field write down assemblyAndCopy. After you can disable checkbox in Use sbt shell to have more custom configurations. In the VM parameters increase the memory by changing -Xmx1024M to -Xmx6G and click Ok. You can find created jar in the folder spark-nlp/python/lib/sparknlp.jar Note: Assembly command creates a fat jars, that includes all dependencies within Compiling pypi, whl Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and select sbt task. In the Name field put AssemblyAndCopyForPyPi. In the Tasks field write down assemblyAndCopyForPyPi. Then you go to spark-nlp/python/ directory and run: python setup.py sdist bdist_wheel You can find created whl and tar.gz in the folder spark-nlp/python/dist/. Use this files to install spark-nlp locally: pip install spark_nlp-2.x.x-py3-none-any.whl",
    "url": "/docs/en/developers",
    "relUrl": "/docs/en/developers"
  },
  "24": {
    "id": "24",
    "title": "Spark NLP Display",
    "content": "Getting started Spark NLP Display is an open-source python library for visualizing the annotations generated with Spark NLP. It currently offers out-of-the-box suport for the following types of annotations: Dependency Parser Named Entity Recognition Entity Resolution Relation Extraction Assertion Status The ability to quickly visualize the entities/relations/assertion statuses, etc. generated using Spark NLP is a very useful feature for speeding up the development process as well as for understanding the obtained results. Getting all of this in a one liner is extremelly convenient especially when running Jupyter notebooks which offers full support for html visualizations. The visualisation classes work with the outputs returned by both Pipeline.transform() function and LightPipeline.fullAnnotate(). Install Spark NLP Display You can install the Spark NLP Display library via pip by using: pip install spark-nlp-display A complete guideline on how to use the Spark NLP Display library is available here. Visualize a dependency tree For visualizing a dependency trees generated with DependencyParserApproach you can use the following code. from sparknlp_display import DependencyParserVisualizer dependency_vis = DependencyParserVisualizer() dependency_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe. pos_col = &#39;pos&#39;, #specify the pos column dependency_col = &#39;dependency&#39;, #specify the dependency column dependency_type_col = &#39;dependency_type&#39; #specify the dependency type column ) The following image gives an example of html output that is obtained for a test sentence: Visualize extracted named entities The NerVisualizer highlights the named entities that are identified by Spark NLP and also displays their labels as decorations on top of the analyzed text. The colors assigned to the predicted labels can be configured to fit the particular needs of the application. from sparknlp_display import NerVisualizer ner_vis = NerVisualizer() ner_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe label_col=&#39;entities&#39;, #specify the entity column document_col=&#39;document&#39; #specify the document column (default: &#39;document&#39;) labels=[&#39;PER&#39;] #only allow these labels to be displayed. (default: [] - all labels will be displayed) ) ## To set custom label colors: ner_vis.set_label_colors({&#39;LOC&#39;:&#39;#800080&#39;, &#39;PER&#39;:&#39;#77b5fe&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences: Visualize relations The RelationExtractionVisualizer can be used to visualize the relations predicted by Spark NLP. The two entities involved in a relation will be highlighted and their label will be displayed. Also a directed and labeled arc(line) will be used to connect the two entities. from sparknlp_display import RelationExtractionVisualizer re_vis = RelationExtractionVisualizer() re_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe relation_col = &#39;relations&#39;, #specify relations column document_col = &#39;document&#39;, #specify document column show_relations=True #display relation names on arrows (default: True) ) The following image gives an example of html output that is obtained for a couple of test sentences: Visualize assertion status The AssertionVisualizer is a special type of NerVisualizer that also displays on top of the labeled entities the assertion status that was infered by a Spark NLP model. from sparknlp_display import AssertionVisualizer assertion_vis = AssertionVisualizer() assertion_vis.display(pipeline_result[0], label_col = &#39;entities&#39;, #specify the ner result column assertion_col = &#39;assertion&#39; #specify assertion column document_col = &#39;document&#39; #specify the document column (default: &#39;document&#39;) ) ## To set custom label colors: assertion_vis.set_label_colors({&#39;TREATMENT&#39;:&#39;#008080&#39;, &#39;problem&#39;:&#39;#800080&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences: Visualize entity resolution Entity resolution refers to the normalization of named entities predicted by Spark NLP with respect to standard terminologies such as ICD-10, SNOMED, RxNorm etc. You can read more about the available entity resolvers here. The EntityResolverVisualizer will automatically display on top of the NER label the standard code (ICD10 CM, PCS, ICDO; CPT) that corresponds to that entity as well as the short description of the code. If no resolution code could be identified a regular NER-type of visualization will be displayed. from sparknlp_display import EntityResolverVisualizer er_vis = EntityResolverVisualizer() er_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe label_col=&#39;entities&#39;, #specify the ner result column resolution_col = &#39;resolution&#39; document_col=&#39;document&#39; #specify the document column (default: &#39;document&#39;) ) ## To set custom label colors: er_vis.set_label_colors({&#39;TREATMENT&#39;:&#39;#800080&#39;, &#39;PROBLEM&#39;:&#39;#77b5fe&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences:",
    "url": "/docs/en/display",
    "relUrl": "/docs/en/display"
  },
  "25": {
    "id": "25",
    "title": "John Snow Labs NLP Documentation",
    "content": "",
    "url": "/docs",
    "relUrl": "/docs"
  },
  "26": {
    "id": "26",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/east_asian_languages",
    "relUrl": "/east_asian_languages"
  },
  "27": {
    "id": "27",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/enhance_low_quality_images",
    "relUrl": "/enhance_low_quality_images"
  },
  "28": {
    "id": "28",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/european_languages",
    "relUrl": "/european_languages"
  },
  "29": {
    "id": "29",
    "title": "Evaluation",
    "content": "Spark NLP Evaluation This module includes tools to evaluate the accuracy of annotators and visualize the parameters used on training. It includes specific metrics for each annotator and its training time. The results will display on the console or to an MLflow tracking UI. Just with a simple import you can start using eval module. Check how to setup MLflow UI See here on eval folder if you want to check specific running examples. Example: PythonScala from sparknlp.eval import * import com.johnsnowlabs.nlp.eval._ Evaluating Norvig Spell Checker You can evaluate this spell checker either by training an annotator or by using a pretrained model. spark: Spark session. trainFile: A corpus of documents with correctly spell words. testFile: A corpus of documents with misspells words. groundTruthFile: The same corpus used on testFile but with correctly spell words. Train File Example: Any document that you prefer with correctly spell words. Test File Example: My siter go to Munich. Ground Truth File Example: My sister goes to Munich. Example for annotator: PythonScala spell = NorvigSweetingApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;checked&quot;) .setDictionary(dictionary_file) norvigSpellEvaluation = NorvigSpellEvaluation(spark, test_file, ground_truth_file) norvigSpellEvaluation.computeAccuracyAnnotator(train_file, spell) val spell = new NorvigSweetingApproach() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;checked&quot;) .setDictionary(dictionary_file) val norvigSpellEvaluation = new NorvigSpellEvaluation(spark, testFile, groundTruthFile) norvigSpellEvaluation.computeAccuracyAnnotator(trainFile, spell) Example for pretrained model: PythonScala spell = NorvigSweetingModel.pretrained() norvigSpellEvaluation = NorvigSpellEvaluation(spark, test_file, ground_truth_file) norvigSpellEvaluation.computeAccuracyModel(spell) val spell = NorvigSweetingModel.pretrained() val norvigSpellEvaluation = new NorvigSpellEvaluation(spark, testFile, groundTruthFile) norvigSpellEvaluation.computeAccuracyModel(spell) Evaluating Symmetric Spell Checker You can evaluate this spell checker either by training an annotator or by using a pretrained model. spark: Spark session trainFile: A corpus of documents with correctly spell words. testFile: A corpus of documents with misspells words. groundTruthFile: The same corpus used on testFile but with correctly spell words. Train File Example: Any document that you prefer with correctly spell words. Test File Example: My siter go to Munich. Ground Truth File Example: My sister goes to Munich. Example for annotator: PythonScala spell = SymmetricDeleteApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;checked&quot;) .setDictionary(dictionary_file) symSpellEvaluation = SymSpellEvaluation(spark, test_file, ground_truth_file) symSpellEvaluation.computeAccuracyAnnotator(train_file, spell) val spell = new SymmetricDeleteApproach() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;checked&quot;) val symSpellEvaluation = new SymSpellEvaluation(spark, testFile, groundTruthFile) symSpellEvaluation.computeAccuracyAnnotator(trainFile, spell) Example for pretrained model: PythonScala spell = SymmetricDeleteModel.pretrained() symSpellEvaluation = NorvigSpellEvaluation(spark, test_file, ground_truth_file) symSpellEvaluation.computeAccuracyModel(spell) val spell = SymmetricDeleteModel.pretrained() val symSpellEvaluation = new SymSpellEvaluation(spark, testFile, groundTruthFile) symSpellEvaluation.computeAccuracyModel(spell) Evaluating NER DL You can evaluate NER DL when training an annotator. spark: Spark session. trainFile: Files with labeled NER entities for training. testFile: Files with labeled NER entities for testing. These files are used to evaluate the model. So, it’s used for prediction and the labels as ground truth. tagLevel: The granularity of tagging when measuring accuracy on entities. Set “IOB” to include inside and beginning, empty to ignore it. For example to display accuracy for entity I-PER and B-PER set “IOB” whereas just for entity PER set it as an empty string. Example: PythonScala embeddings = WordEmbeddings() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setEmbeddingsSource(&quot;glove.6B.100d.txt&quot;, 100, &quot;TEXT&quot;) ner_approach = NerDLApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) .setRandomSeed(0) nerDLEvaluation = NerDLEvaluation(spark, test_File, tag_level) nerDLEvaluation.computeAccuracyAnnotator(train_file, ner_approach, embeddings) val embeddings = new WordEmbeddings() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setEmbeddingsSource(&quot;glove.6B.100d.txt&quot;, 100, WordEmbeddingsFormat.TEXT) val nerApproach = new NerDLApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) .setRandomSeed(0) val nerDLEvaluation = new NerDLEvaluation(spark, testFile, tagLevel) nerDLEvaluation.computeAccuracyAnnotator(trainFile, nerApproach, embeddings) Example for pretrained model: PythonScala ner_dl = NerDLModel.pretrained() nerDlEvaluation = NerDLEvaluation(spark, test_File, tag_level) nerDlEvaluation.computeAccuracyModel(ner_dl) val nerDl = NerDLModel.pretrained() val nerDlEvaluation = NerDLEvaluation(spark, testFile, tagLevel) nerDlEvaluation.computeAccuracyModel(nerDl) Evaluating NER CRF You can evaluate NER CRF when training an annotator. spark: Spark session. trainFile: Files with labeled NER entities for training. testFile: Files with labeled NER entities for testing. These files are used to evaluate the model. So, it’s used for prediction and the labels as ground truth. format: The granularity of tagging when measuring accuracy on entities. Set “IOB” to include inside and beginning, empty to ignore it. For example to display accuracy for entity I-PER and B-PER set “IOB” whereas just for entity PER set it as an empty string. Example: PythonScala embeddings = WordEmbeddings() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setEmbeddingsSource(&quot;glove.6B.100d.txt&quot;, 100, &quot;TEXT&quot;) ner_approach = NerCrfApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) .setRandomSeed(0) nerCrfEvaluation = NerCrfEvaluation(spark, test_File, tag_level) nerCrfEvaluation.computeAccuracyAnnotator(train_file, ner_approach, embeddings) val embeddings = new WordEmbeddings() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setEmbeddingsSource(&quot;./glove.6B.100d.txt &quot;, 100, WordEmbeddingsFormat.TEXT) .setCaseSensitive(true) val nerTagger = new NerCrfApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;pos&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) val nerCrfEvaluation = new NerCrfEvaluation(testFile, format) nerCrfEvaluation.computeAccuracyAnnotator(trainFile, nerTagger, embeddings) Example for pretrained model: PythonScala ner_crf = NerCrfModel.pretrained() nerCrfEvaluation = NerCrfEvaluation(spark, test_File, tag_level) nerCrfEvaluation.computeAccuracyModel(ner_crf) nerCrf = NerCrfModel.pretrained() nerCrfEvaluation = NerCrfEvaluation(spark, testFile, tagLevel) nerCrfEvaluation.computeAccuracyModel(nerCrf) Evaluating POS Tagger You can evaluate POS either by training an annotator or by using a pretrained model. spark: Spark session. trainFile: A labeled POS file see and example here. testFile: A CoNLL-U format file. Example for annotator: PythonScala pos_tagger = PerceptronApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) .setNIterations(2) posEvaluation = POSEvaluation(spark, test_file) posEvaluation.computeAccuracyAnnotator(train_file, pos_tagger) val posTagger = new PerceptronApproach() .setInputCols(Array(&quot;document&quot;, &quot;token&quot;)) .setOutputCol(&quot;pos&quot;) .setNIterations(2) val posEvaluation = new POSEvaluation(spark, testFile) posEvaluation.computeAccuracyAnnotator(trainFile, posTagger)",
    "url": "/docs/en/evaluation",
    "relUrl": "/docs/en/evaluation"
  },
  "30": {
    "id": "30",
    "title": "Examples",
    "content": "Showcasing notebooks and codes of how to use Spark NLP in Python and Scala. Python Setup $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.7 -y $ conda activate sparknlp $ pip install spark-nlp==3.3.4 pyspark==3.1.2 Google Colab Notebook Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account. Run the following code in Google Colab notebook and start using spark-nlp right away. # This is only to setup PySpark and Spark NLP on Colab !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash This script comes with the two options to define pyspark and spark-nlp versions via options: # -p is for pyspark # -s is for spark-nlp # by default they are set to the latest !bash colab.sh -p 3.1.2 -s 3.3.4 Spark NLP quick start on Google Colab is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines. Kaggle Kernel Run the following code in Kaggle Kernel and start using spark-nlp right away. # Let&#39;s setup Kaggle for Spark NLP and PySpark !wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash Notebooks Tutorials and trainings Jupyter Notebooks Databricks Notebooks",
    "url": "/docs/en/examples",
    "relUrl": "/docs/en/examples"
  },
  "31": {
    "id": "31",
    "title": "Export Data",
    "content": "The completions and predictions are stored in a database for fast search and access. Completions and predictions can be exported into the formats described below. JSON You can convert and export the completions and predictions to json format by using the JSON option on the Export page. The obtained format is the following: [ { &quot;completions&quot;: [], &quot;predictions&quot;: [ { &quot;created_username&quot;: &quot;SparkNLP Pre-annotation&quot;, &quot;result&quot;: [ { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;7HGzTLkNUA&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 3554, &quot;labels&quot;: [ &quot;Symptom_Name&quot; ], &quot;start&quot;: 3548, &quot;text&quot;: &quot;snores&quot; } } ], &quot;created_ago&quot;: &quot;2020-11-09T14:44:57.713743Z&quot;, &quot;id&quot;: 2001 } ], &quot;created_at&quot;: &quot;2020-11-09 14:41:39&quot;, &quot;created_by&quot;: &quot;admin&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;Cardiovascular / Pulmonary nSample Name: Angina - Consult nDescription: Patient had a recurrent left arm pain after her stent, three days ago, and this persisted after two sublingual nitroglycerin. n(Medical Transcription Sample Report) nHISTORY OF PRESENT ILLNESS: The patient is a 68-year-old woman whom I have been following, who has had angina. In any case today, she called me because she had a recurrent left arm pain after her stent, three days ago, and this persisted after two sublingual nitroglycerin when I spoke to her.&quot;, &quot;title&quot;: &quot;sample_document3.txt&quot;, &quot;pre_annotation&quot;: true }, &quot;id&quot;: 2 }] CSV Results are stored in comma-separated tabular file with column names specified by “from_name” “to_name” values TSV Results are stored in tab-separated tabular file with column names specified by “from_name” “to_name” values CONLL2003 The CONLL export feature generates a single output file, containing all available completios for all the tasks in the project. The resulting file has the following format: -DOCSTART- -X- O Sample -X- _ O Type -X- _ O Medical -X- _ O Specialty: -X- _ O Endocrinology -X- _ O Sample -X- _ O Name: -X- _ O Diabetes -X- _ B-Diagnosis Mellitus -X- _ I-Diagnosis Followup -X- _ O Description: -X- _ O Return -X- _ O visit -X- _ O to -X- _ O the -X- _ O endocrine -X- _ O clinic -X- _ O for -X- _ O followup -X- _ O management -X- _ O of -X- _ O type -X- _ O 1 -X- _ O diabetes -X- _ O mellitus -X- _ O Plan -X- _ O today -X- _ O is -X- _ O to -X- _ O make -X- _ O adjustments -X- _ O to -X- _ O her -X- _ O pump -X- _ O based -X- _ O on -X- _ O a -X- _ O total -X- _ O daily -X- _ B-FREQUENCY dose -X- _ O of -X- _ O 90 -X- _ O units -X- _ O of -X- _ O insulin -X- _ O … User can specify if only starred completions should be included in the output file by checking “Only ground truth” option before generating the export.",
    "url": "/docs/en/alab/export",
    "relUrl": "/docs/en/alab/export"
  },
  "32": {
    "id": "32",
    "title": "Extract handwritten texts",
    "content": "",
    "url": "/extract_handwritten_texts",
    "relUrl": "/extract_handwritten_texts"
  },
  "33": {
    "id": "33",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/extract_relationships",
    "relUrl": "/extract_relationships"
  },
  "34": {
    "id": "34",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/extract_tables",
    "relUrl": "/extract_tables"
  },
  "35": {
    "id": "35",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/extract_text_from_documents",
    "relUrl": "/extract_text_from_documents"
  },
  "36": {
    "id": "36",
    "title": "Tensorflow Graph",
    "content": "NER DL uses Char CNNs - BiLSTM - CRF Neural Network architecture. Spark NLP defines this architecture through a Tensorflow graph, which requires the following parameters: Tags Embeddings Dimension Number of Chars Spark NLP infers these values from the training dataset used in NerDLApproach annotator and tries to load the graph embedded on spark-nlp package. Currently, Spark NLP has graphs for the most common combination of tags, embeddings, and number of chars values: Tags Embeddings Dimension 10 100 10 200 10 300 10 768 10 1024 25 300 All of these graphs use an LSTM of size 128 and number of chars 100 In case, your train dataset has a different number of tags, embeddings dimension, number of chars and LSTM size combinations shown in the table above, NerDLApproach will raise an IllegalArgumentException exception during runtime with the message below: Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Check https://nlp.johnsnowlabs.com/docs/en/graph for instructions to generate the required graph. To overcome this exception message we have to follow these steps: Clone spark-nlp github repo Run python file create_models with number of tags, embeddings dimension and number of char values mentioned on your exception message error. cd spark-nlp/python/tensorflow export PYTHONPATH=lib/ner python ner/create_models.py [number_of_tags] [embeddings_dimension] [number_of_chars] [output_path] This will generate a graph on the directory defined on `output_path argument. Retry training with NerDLApproach annotator but this time use the parameter setGraphFolder with the path of your graph. Note: Make sure that you have Python 3 and Tensorflow 1.15.0 installed on your system since create_models requires those versions to generate the graph successfully. Note: We also have a notebook in the same directory if you prefer Jupyter notebook to cerate your custom graph (create_models.ipynb).",
    "url": "/docs/en/graph",
    "relUrl": "/docs/en/graph"
  },
  "37": {
    "id": "37",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/identify_translate_languages",
    "relUrl": "/identify_translate_languages"
  },
  "38": {
    "id": "38",
    "title": "Import Documents",
    "content": "Once a new project is created and its configuration is saved, the user is redirected to the Import page. Here the user has multiple options for importing tasks. Plain text file When you upload a plain text file, only one task will be created which will contain the entire data in the input file. This is an update from versions of Annotation Lab when the input text file was split by the new line character and one task was created for each line. Json file For bulk importing a list of documents you can use the json import option. The expected format is illustrated in the image below. It consists of a list of dictionaries, each with 2 keys-values pairs (“text” and “title”). [{ &quot;text&quot;: &quot;Task text content.&quot;, &quot;title&quot;:&quot;Task title&quot;}] CSV, TSV file When CSV / TSV formatted text file is used, column names are interpreted as task data keys: Task text content, Task title this is a first task, Colon Cancer.txt this is a second task, Breast radiation therapy.txt Import annotated tasks When importing tasks that already contain annotations (e.g. exported from another project, with predictions generated by pre-trained models) the user has the option to overwrite completions/predictions or to skip the tasks that are already imported into the project.",
    "url": "/docs/en/alab/import",
    "relUrl": "/docs/en/alab/import"
  },
  "39": {
    "id": "39",
    "title": "Spark NLP: <span>State of the Art Natural Language Processing</span>",
    "content": "",
    "url": "/",
    "relUrl": "/"
  },
  "40": {
    "id": "40",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/infer_meaning_intent",
    "relUrl": "/infer_meaning_intent"
  },
  "41": {
    "id": "41",
    "title": "Installation",
    "content": "Deploy on a dedicated server Install Annotation Lab on a dedicated server to reduce the likelihood of conflicts or unexpected behavior. Fresh install To install Annotation Lab run the following command: wget https://setup.johnsnowlabs.com/annotationlab/install.sh -O - | sudo bash -s -- --version VERSION To upgrade your Annotation Lab installation to a newer version, run the following command on a terminal: Upgrade wget https://setup.johnsnowlabs.com/annotationlab/upgrade.sh -O - | sudo bash -s -- --version VERSION Replace VERSION within the above one liners with the version you want to install. After running the install/upgrade script the Annotation Lab is available at http://INSTANCE_IP or https://INSTANCE_IP The install/upgrade script displays the login credentials for the admin user on the terminal. Deploy on the AWS Marketplace Visit the product page on AWS Marketplace and follow the instructions on the video below to subscribe and deploy. Deploy Annotation Lab via AWS Marketplace Deploy on AirGap environment Get artifact wget https://s3.amazonaws.com/auxdata.johnsnowlabs.com/annotationlab/annotationlab-VERSION.tar.gz replace VERSION with the version you want to download and install. Fresh installation $ sudo su $ ./annotationlab-installer.sh Upgrade version $ sudo su $ ./annotationlab-updater.sh Work over proxy Custom CA certificate You can provide a custom CA certificate chain to be included into the deployment. To do it add --set-file custom_cacert=./cachain.pem options to helm install/upgrade command inside annotationlab-installer.sh and annotationlab-updater.sh files. cachain.pem must include a certificate in the following format: --BEGIN CERTIFICATE-- .... --END CERTIFICATE-- Proxy env variables You can provide a proxy to use for external communications. To do that add `--set proxy.http=[protocol://]&lt;host&gt;[:port]`, `--set proxy.https=[protocol://]&lt;host&gt;[:port]`, `--set proxy.no=&lt;comma-separated list of hosts/domains&gt;` commands inside annotationlab-installer.sh and annotationlab-updater.sh files. Backup and restore Backup You can enable daily backups by adding several variables with –set option to helm command in annotationlab-updater.sh: backup.enable=true backup.s3_access_key=&quot;&lt;ACCESS_KEY&gt;&quot; backup.s3_secret_key=&quot;&lt;SECRET_KEY&gt;&quot; backup.s3_bucket_fullpath=&quot;&lt;FULL_PATH&gt;&quot; &lt;ACCESS_KEY&gt; - your access key for aws s3 access &lt;SECRET_KEY&gt; - your secret key for aws s3 access &lt;FULL_PATH&gt; - full path to your backup in s3 bucket (f.e. s3://example.com/path/to/my/backup/dir) Restore To restore from backup you need new clear installation of Annotation Lab. Do it with annotationlab-install.sh. Next, you need to download latest backup from your s3 bucket and unpack an archive. There should be 3 sql backup files: annotationlab.sql keycloak.sql airflow.sql Run commands below to get PostgreSQL passwords. airflow-postgres password for user airflow: kubectl get secret -l app.kubernetes.io/name=airflow-postgresql -o jsonpath=&#39;{.items[0].data.postgresql-password}&#39; | base64 -d annotationlab-postgres password for user annotationlab: kubectl get secret -l app.kubernetes.io/name=postgresql -o jsonpath=&#39;{.items[0].data.postgresql-password}&#39; | base64 -d keycloak-postgress password for user keycloak: kubectl get secret -l app.kubernetes.io/name=keycloak-postgres -o jsonpath=&#39;{.items[0].data.postgresql-password}&#39; | base64 -d Now you can restore your databases with psql, pg_restore, etc. Recommended Configurations System requirements. You can install Annotation Lab on a Ubuntu 20+ machine. Port requirements. Annotation Lab expects ports 443 and 80 to be open by default. Server requirements. The minimal required configuration is 32GB RAM, 8 Core CPU, 512 SSD. The ideal configuration in case model training and preannotations are required on a large number of tasks is 64 GiB, 16 Core CPU, 2TB HDD, 512 SSD. Web browser support. Annotation Lab is tested with the latest version of Google Chrome and is expected to work in the latest versions of: • Google Chrome • Apple Safari • Mozilla Firefox",
    "url": "/docs/en/alab/install",
    "relUrl": "/docs/en/alab/install"
  },
  "42": {
    "id": "42",
    "title": "Installation",
    "content": "Spark NLP Cheat Sheet # Install Spark NLP from PyPI pip install spark-nlp==3.3.4 # Install Spark NLP from Anacodna/Conda conda install -c johnsnowlabs spark-nlp # Load Spark NLP with Spark Shell spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.4 # Load Spark NLP with PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.4 # Load Spark NLP with Spark Submit spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.4 # Load Spark NLP as external JAR after compiling and building Spark NLP by `sbt assembly` spark-shell --jar spark-nlp-assembly-3.3.4 Python Spark NLP supports Python 3.6.x and 3.7.x if you are using PySpark 2.3.x or 2.4.x and Python 3.8.x if you are using PySpark 3.x. Quick Install Let’s create a new Conda environment to manage all the dependencies there. You can use Python Virtual Environment if you prefer or not have any enviroment. $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.7 -y $ conda activate sparknlp $ pip install spark-nlp==3.3.4 pyspark==3.1.2 Of course you will need to have jupyter installed in your system: pip install jupyter Now you should be ready to create a jupyter notebook running from terminal: jupyter notebook Start Spark NLP Session from python If you need to manually start SparkSession because you have other configuraations and sparknlp.start() is not including them, you can manually start the SparkSession: spark = SparkSession.builder .appName(&quot;Spark NLP&quot;) .master(&quot;local[4]&quot;) .config(&quot;spark.driver.memory&quot;,&quot;16G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.4&quot;) .getOrCreate() Scala and Java Maven Spark NLP supports Scala 2.11.x if you are using Apache Spark 2.3.x or 2.4.x and Scala 2.12.x if you are using Apache Spark 3.0.x or 3.1.x. Our packages are deployed to Maven central. To add any of our packages as a dependency in your application you can follow these coordinates: spark-nlp on Apache Spark 3.x: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.3.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.3.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.3.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;3.3.4/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.3.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.3.4&lt;/version&gt; &lt;/dependency&gt; SBT spark-nlp on Apache Spark 3.x.x: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp&quot; % &quot;3.3.4&quot; spark-nlp-gpu: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-gpu&quot; % &quot;3.3.4&quot; spark-nlp on Apache Spark 2.4.x: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-spark24&quot; % &quot;3.3.4&quot; spark-nlp-gpu: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-gpu-spark24&quot; % &quot;3.3.4&quot; spark-nlp on Apache Spark 2.3.x: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23 libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-spark23&quot; % &quot;3.3.4&quot; spark-nlp-gpu: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23 libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-gpu-spark23&quot; % &quot;3.3.4&quot; Maven Central: https://mvnrepository.com/artifact/com.johnsnowlabs.nlp Google Colab Notebook Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account. Run the following code in Google Colab notebook and start using spark-nlp right away. # This is only to setup PySpark and Spark NLP on Colab !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash This script comes with the two options to define pyspark and spark-nlp versions via options: # -p is for pyspark # -s is for spark-nlp # by default they are set to the latest !bash colab.sh -p 3.1.2 -s 3.3.4 Spark NLP quick start on Google Colab is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines. Kaggle Kernel Run the following code in Kaggle Kernel and start using spark-nlp right away. # Let&#39;s setup Kaggle for Spark NLP and PySpark !wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash Spark NLP quick start on Kaggle Kernel is a live demo on Kaggle Kernel that performs named entity recognitions by using Spark NLP pretrained pipeline. Databricks Support Spark NLP 3.3.4 has been tested and is compatible with the following runtimes: CPU: 5.5 LTS 5.5 LTS ML 6.4 6.4 ML 7.3 7.3 ML 7.4 7.4 ML 7.5 7.5 ML 7.6 7.6 ML 8.0 8.0 ML 8.1 8.1 ML 8.2 8.2 ML 8.3 8.3 ML 8.4 8.4 ML GPU: 8.1 ML &amp; GPU 8.2 ML &amp; GPU 8.3 ML &amp; GPU 8.4 ML &amp; GPU NOTE: Spark NLP 3.1.x is based on TensorFlow 2.4.x which is compatible with CUDA11 and cuDNN 8.0.2. The only Databricks runtimes supporting CUDA 11. are 8.1 ML with GPU, 8.2 ML with GPU, and 8.3 ML with GPU. Install Spark NLP on Databricks Create a cluster if you don’t have one already On a new cluster or existing one you need to add the following to the Advanced Options -&gt; Spark tab: spark.kryoserializer.buffer.max 2000M spark.serializer org.apache.spark.serializer.KryoSerializer In Libraries tab inside your cluster you need to follow these steps: 3.1. Install New -&gt; PyPI -&gt; spark-nlp -&gt; Install 3.2. Install New -&gt; Maven -&gt; Coordinates -&gt; com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.4 -&gt; Install Now you can attach your notebook to the cluster and use Spark NLP! Databricks Notebooks You can view all the Databricks notebooks from this address: https://johnsnowlabs.github.io/spark-nlp-workshop/databricks/index.html Note: You can import these notebooks by using their URLs. EMR Support Spark NLP 3.3.4 has been tested and is compatible with the following EMR releases: emr-5.20.0 emr-5.21.0 emr-5.21.1 emr-5.22.0 emr-5.23.0 emr-5.24.0 emr-5.24.1 emr-5.25.0 emr-5.26.0 emr-5.27.0 emr-5.28.0 emr-5.29.0 emr-5.30.0 emr-5.30.1 emr-5.31.0 emr-5.32.0 emr-6.1.0 emr-6.2.0 emr-6.3.0 Full list of Amazon EMR 5.x releases Full list of Amazon EMR 6.x releases NOTE: The EMR 6.0.0 is not supported by Spark NLP 3.3.4 How to create EMR cluster via CLI To lanuch EMR cluster with Apache Spark/PySpark and Spark NLP correctly you need to have bootstrap and software configuration. A sample of your bootstrap script #!/bin/bash set -x -e echo -e &#39;export PYSPARK_PYTHON=/usr/bin/python3 export HADOOP_CONF_DIR=/etc/hadoop/conf export SPARK_JARS_DIR=/usr/lib/spark/jars export SPARK_HOME=/usr/lib/spark&#39; &gt;&gt; $HOME/.bashrc &amp;&amp; source $HOME/.bashrc sudo python3 -m pip install awscli boto spark-nlp set +x exit 0 A sample of your software configuration in JSON on S3 (must be public access): [{ &quot;Classification&quot;: &quot;spark-env&quot;, &quot;Configurations&quot;: [{ &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;PYSPARK_PYTHON&quot;: &quot;/usr/bin/python3&quot; } }] }, { &quot;Classification&quot;: &quot;spark-defaults&quot;, &quot;Properties&quot;: { &quot;spark.yarn.stagingDir&quot;: &quot;hdfs:///tmp&quot;, &quot;spark.yarn.preserve.staging.files&quot;: &quot;true&quot;, &quot;spark.kryoserializer.buffer.max&quot;: &quot;2000M&quot;, &quot;spark.serializer&quot;: &quot;org.apache.spark.serializer.KryoSerializer&quot;, &quot;spark.driver.maxResultSize&quot;: &quot;0&quot;, &quot;spark.jars.packages&quot;: &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.4&quot; } } ] A sample of AWS CLI to launch EMR cluster: .sh aws emr create-cluster --name &quot;Spark NLP 3.3.4&quot; --release-label emr-6.2.0 --applications Name=Hadoop Name=Spark Name=Hive --instance-type m4.4xlarge --instance-count 3 --use-default-roles --log-uri &quot;s3://&lt;S3_BUCKET&gt;/&quot; --bootstrap-actions Path=s3://&lt;S3_BUCKET&gt;/emr-bootstrap.sh,Name=custome --configurations &quot;https://&lt;public_access&gt;/sparknlp-config.json&quot; --ec2-attributes KeyName=&lt;your_ssh_key&gt;,EmrManagedMasterSecurityGroup=&lt;security_group_with_ssh&gt;,EmrManagedSlaveSecurityGroup=&lt;security_group_with_ssh&gt; --profile &lt;aws_profile_credentials&gt; GCP Dataproc Support Create a cluster if you don’t have one already as follows. At gcloud shell: gcloud services enable dataproc.googleapis.com compute.googleapis.com storage-component.googleapis.com bigquery.googleapis.com bigquerystorage.googleapis.com REGION=&lt;region&gt; BUCKET_NAME=&lt;bucket_name&gt; gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME} REGION=&lt;region&gt; ZONE=&lt;zone&gt; CLUSTER_NAME=&lt;cluster_name&gt; BUCKET_NAME=&lt;bucket_name&gt; You can set image-version, master-machine-type, worker-machine-type, master-boot-disk-size, worker-boot-disk-size, num-workers as your needs. If you use the previous image-version from 2.0, you should also add ANACONDA to optional-components. And, you should enable gateway. gcloud dataproc clusters create ${CLUSTER_NAME} --region=${REGION} --zone=${ZONE} --image-version=2.0 --master-machine-type=n1-standard-4 --worker-machine-type=n1-standard-2 --master-boot-disk-size=128GB --worker-boot-disk-size=128GB --num-workers=2 --bucket=${BUCKET_NAME} --optional-components=JUPYTER --enable-component-gateway --metadata &#39;PIP_PACKAGES=spark-nlp spark-nlp-display google-cloud-bigquery google-cloud-storage&#39; --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh On an existing one, you need to install spark-nlp and spark-nlp-display packages from PyPI. Now, you can attach your notebook to the cluster and use the Spark NLP! Amazon Linux 2 Support # Update Package List &amp; Install Required Packages sudo yum update sudo yum install -y amazon-linux-extras sudo yum -y install python3-pip # Create Python virtual environment and activate it: python3 -m venv .sparknlp-env source .sparknlp-env/bin/activate Check JAVA version: For Sparknlp versions above 3.x, please use JAVA-11 For Sparknlp versions below 3.x and SparkOCR, please use JAVA-8 Checking Java versions installed on your machine: sudo alternatives --config java You can pick the index number (I am using java-8 as default - index 2): &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; If you dont have java-11 or java-8 in you system, you can easily install via: sudo yum install java-1.8.0-openjdk Now, we can start installing the required libraries: pip install pyspark==3.1.2 pip install spark-nlp Docker Support For having Spark NLP, PySpark, Jupyter, and other ML/DL dependencies as a Docker image you can use the following template: #Download base image ubuntu 18.04 FROM ubuntu:18.04 ENV NB_USER jovyan ENV NB_UID 1000 ENV HOME /home/${NB_USER} ENV PYSPARK_PYTHON=python3 ENV PYSPARK_DRIVER_PYTHON=python3 RUN apt-get update &amp;&amp; apt-get install -y tar wget bash rsync gcc libfreetype6-dev libhdf5-serial-dev libpng-dev libzmq3-dev python3 python3-dev python3-pip unzip pkg-config software-properties-common graphviz RUN adduser --disabled-password --gecos &quot;Default user&quot; --uid ${NB_UID} ${NB_USER} # Install OpenJDK-8 RUN apt-get update &amp;&amp; apt-get install -y openjdk-8-jdk &amp;&amp; apt-get install -y ant &amp;&amp; apt-get clean; # Fix certificate issues RUN apt-get update &amp;&amp; apt-get install ca-certificates-java &amp;&amp; apt-get clean &amp;&amp; update-ca-certificates -f; # Setup JAVA_HOME -- useful for docker commandline ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/ RUN export JAVA_HOME RUN echo &quot;export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/&quot; &gt;&gt; ~/.bashrc RUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* RUN pip3 install --upgrade pip # You only need pyspark and spark-nlp paclages to use Spark NLP # The rest of the PyPI packages are here as examples RUN pip3 install --no-cache-dir pyspark spark-nlp==3.1.2 notebook==5.* numpy pandas mlflow Keras scikit-spark scikit-learn scipy matplotlib pydot tensorflow==2.4.1 graphviz # Make sure the contents of our repo are in ${HOME} RUN mkdir -p /home/jovyan/tutorials RUN mkdir -p /home/jovyan/jupyter COPY data ${HOME}/data COPY jupyter ${HOME}/jupyter COPY tutorials ${HOME}/tutorials RUN jupyter notebook --generate-config COPY jupyter_notebook_config.json /home/jovyan/.jupyter/jupyter_notebook_config.json USER root RUN chown -R ${NB_UID} ${HOME} USER ${NB_USER} WORKDIR ${HOME} # Specify the default command to run CMD [&quot;jupyter&quot;, &quot;notebook&quot;, &quot;--ip&quot;, &quot;0.0.0.0&quot;] Finally, use jupyter_notebook_config.json for the password: { &quot;NotebookApp&quot;: { &quot;password&quot;: &quot;sha1:65adaa6ffb9c:36df1c2086ef294276da703667d1b8ff38f92614&quot; } } &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Windows Support In order to fully take advantage of Spark NLP on Windows (8 or 10), you need to setup/install Apache Spark, Apache Hadoop, and Java correctly by following the following instructions: https://github.com/JohnSnowLabs/spark-nlp/discussions/1022 &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; How to correctly install Spark NLP on Windows 8 and 10 Follow the below steps: Download OpenJDK from here: https://adoptopenjdk.net/?variant=openjdk8&amp;jvmVariant=hotspot; Make sure it is 64-bit Make sure you install it in the root C: java Windows . During installation after changing the path, select setting Path Download winutils and put it in C: hadoop bin https://github.com/cdarlint/winutils/blob/master/hadoop-2.7.3/bin/winutils.exe; Download Anaconda 3.6 from Archive: https://repo.anaconda.com/archive/Anaconda3-2020.02-Windows-x86_64.exe; Download Apache Spark 3.1.2 and extract it in C: spark Set the env for HADOOP_HOME to C: hadoop and SPARK_HOME to C: spark Set Paths for %HADOOP_HOME% bin and %SPARK_HOME% bin Install C++ https://www.microsoft.com/en-us/download/confirmation.aspx?id=14632 Create C: temp and C: temp hive Fix permissions: C: Users maz&gt;%HADOOP_HOME% bin winutils.exe chmod 777 /tmp/hive C: Users maz&gt;%HADOOP_HOME% bin winutils.exe chmod 777 /tmp/ Either create a conda env for python 3.6, install pyspark==3.1.2 spark-nlp numpy and use Jupyter/python console, or in the same conda env you can go to spark bin for pyspark –packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.4. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Offline Spark NLP library and all the pre-trained models/pipelines can be used entirely offline with no access to the Internet. If you are behind a proxy or a firewall with no access to the Maven repository (to download packages) or/and no access to S3 (to automatically download models and pipelines), you can simply follow the instructions to have Spark NLP without any limitations offline: Instead of using the Maven package, you need to load our Fat JAR Instead of using PretrainedPipeline for pretrained pipelines or the .pretrained() function to download pretrained models, you will need to manually download your pipeline/model from Models Hub, extract it, and load it. Example of SparkSession with Fat JAR to have Spark NLP offline: spark = SparkSession.builder .appName(&quot;Spark NLP&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;,&quot;16G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars&quot;, &quot;/tmp/spark-nlp-assembly-3.3.4.jar&quot;) .getOrCreate() You can download provided Fat JARs from each release notes, please pay attention to pick the one that suits your environment depending on the device (CPU/GPU) and Apache Spark version (2.3.x, 2.4.x, and 3.x) If you are local, you can load the Fat JAR from your local FileSystem, however, if you are in a cluster setup you need to put the Fat JAR on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., hdfs:///tmp/spark-nlp-assembly-3.3.4.jar) Example of using pretrained Models and Pipelines in offline: # instead of using pretrained() for online: # french_pos = PerceptronModel.pretrained(&quot;pos_ud_gsd&quot;, lang=&quot;fr&quot;) # you download this model, extract it, and use .load french_pos = PerceptronModel.load(&quot;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) # example for pipelines # instead of using PretrainedPipeline # pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;) # you download this pipeline, extract it, and use PipelineModel PipelineModel.load(&quot;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&quot;) Since you are downloading and loading models/pipelines manually, this means Spark NLP is not downloading the most recent and compatible models/pipelines for you. Choosing the right model/pipeline is on you If you are local, you can load the model/pipeline from your local FileSystem, however, if you are in a cluster setup you need to put the model/pipeline on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., hdfs:///tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/)",
    "url": "/docs/en/install",
    "relUrl": "/docs/en/install"
  },
  "43": {
    "id": "43",
    "title": "Install NLP Libraries",
    "content": "Spark NLP For installing Spark NLP on your infrastructure please follow the instructions detailed here. Spark NLP for Healthcare For installing Spark NLP for Healthcare please follow the instructions detailed here. Spark OCR For installing Spark OCR please follow the instructions detailed here.",
    "url": "/docs/en/install_NLP_libraries",
    "relUrl": "/docs/en/install_NLP_libraries"
  },
  "44": {
    "id": "44",
    "title": "Installation",
    "content": "Docker Setup For deploying NLP Server on your instance run the following command. docker run --pull=always -p 5000:5000 johnsnowlabs/nlp-server:latest This will check if the latest docker image is available on your local machine and if not it will automatically download and run it. Deploy on AWS The steps you need to follow for deploying NLP Server on AWS are illustrated below. First make sure you have a valid AWS account and log in to AWS Marketplace using your credentials. NLP Server is available on AWS Marketplace at this URL. For deploying NLP Server on AWS, follow the video instructions or the seven steps described below. Deploy NLP Server via AWS Marketplace 1.Click on Continue to subscribe button for creating a subscription to the NLP Server product. The software is free of charge. 2.Read the subscription EULA and click on Accept terms button if you want to continue. 3.In a couple of seconds the subscription becomes active. Once it is active you see this screen. 4.Go to AWS Marketplace &gt; Manage subscriptions and click on the Launch new instance button corresponding to the NLP Server subscription. This will redirect you to the following screen. Click on Continue to launch through EC2 button. 5.From the available options select the instance type you want to use for the deployment. Then click on Review and Lauch button. 6.Select an existing key pair or create a new one. This ensures a secured connection to the instance. If you create a new key make sure that you download and safely store it for future usage. Click on the Launch button. 7.While the instance is starting you will see this screen. Then the instance will appear on your EC2 Instances list. The NLP Server can now be accessed via a web browser at http://PUBLIC_EC2_IP . API documentation is also available at http://PUBLIC_EC2_IP/docs",
    "url": "/docs/en/nlp_server/installation",
    "relUrl": "/docs/en/nlp_server/installation"
  },
  "45": {
    "id": "45",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/languages_africa",
    "relUrl": "/languages_africa"
  },
  "46": {
    "id": "46",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/languages_india",
    "relUrl": "/languages_india"
  },
  "47": {
    "id": "47",
    "title": "",
    "content": "",
    "url": "/latest.html",
    "relUrl": "/latest.html"
  },
  "48": {
    "id": "48",
    "title": "Learn",
    "content": "Introductions to Spark NLP Videos State of the Art Natural Language Processing at Scale. David Talby - April 13, 2020 Spark NLP: State of the art natural language processing at scale. David Talby - 4 Jun 2020 What is Spark NLP. John Snow Labs - 30 Jul 2019 Apache Spark NLP Extending Spark ML to Deliver Fast, Scalable, and Unified Natural Language Process. David Talby - 6 May 2019 Natural Language Understanding at Scale with Spark Native NLP, Spark ML &amp;TensorFlow with Alex Thomas. Alex Thomas - 26 Oct 2017 Articles Introducing the Natural Language Processing Library for Apache SparkDavid Talby - October 19, 2017 Improving Clinical Document Understanding on COVID-19 Research with Spark NLPVeysel Kocaman, David Talby - 7 December, 2020 Topic Modelling with PySpark and Spark NLPMaria Obedkova - May 29, 2020 Installing Spark NLP and Spark OCR in air-gapped networks (offline mode)Veysel Kocaman - May 04, 2020 Cleaning and extracting text from HTML/XML documents by using Spark NLPStefano Lori - Jan 13, 2020 A Google Colab Notebook Introducing Spark NLPVeysel Kocaman - September, 2020 State-of-the-art Natural Language Processing at ScaleDavid Talby - April 13, 2020 How to Wrap Your Head Around Spark NLPMustafa Aytuğ Kaya - August 25, 2020 5 Reasons Why Spark NLP Is The Most Widely Used Library In EnterprisesAmbika Choudhury - May 28, 2019 My Experience with SparkNLP Workshop &amp; CertificationAngelina Maria Leigh - August 17, 2020 Out of the box Spark NLP models in actionDia Trambitas - August 14, 2020 Get started with Machine Learning in Java using Spark NLPWill Price - August 27, 2020 SPARK NLP 3: MASSIVE SPEEDUPS &amp; THE LATEST COMPUTE PLATFORMSMaziyar Panahi - March 25, 2021 SPARK NLP 2.7: 720+ NEW MODELS &amp; PIPELINES FOR 192 LANGUAGES!David Talby - January 05, 2021 Python’s NLU Library Videos &quot;Python&#39;s NLU library: 1,000+ Models, 200+ Languages, 1 Line of Code&quot; by: Christian Kasim Loan - 18 June 2021 John Snow Labs NLU: Become a Data Science Superhero with One Line of Python code. Christian Kasim Loan - November, 2020 Making State of the Art Natural Language Processing Easier to Apply. David Talby - 3 Nov 2020 Articles 1 line to GLOVE Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to XLNET Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to ALBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to COVIDBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to ELECTRA Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to BioBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 Line of Code, 350 + NLP Models with John Snow Labs’ NLU in PythonChristian Kasim Loan - September 21, 2020 Easy sentence similarity with BERT Sentence Embeddings using John Snow Labs NLUChristian Kasim Loan - November 20, 2020 Training Deep Learning NLP Classifier TutorialChristian Kasim Loan - November 20, 2020 1 Python Line for ELMo Word Embeddings and t-SNE plots with John Snow Labs’ NLUChristian Kasim Loan - October 24, 2020 1 line of Python code for BERT, ALBERT, ELMO, ELECTRA, XLNET, GLOVE, Part of Speech with NLU and t-SNEChristian Kasim Loan - September 21, 2020 1 line to BERT Word Embeddings with NLU in PythonChristian Kasim Loan - September 21, 2020 Question answering, intent classification, aspect based ner, and new multilingual models in python’s NLU libraryChristian Kasim Loan - February 12, 2021 Intent and action classification, analyze chinese news and crypto market, 200+ languages &amp; answer questions with NLU 1.1.3Christian Kasim Loan - March 02, 2021 Hindi wordembeddings, bengali named entity recognition, 30+ new models, analyze crypto news with NLU 1.1.2Christian Kasim Loan - February 18, 2021 Named Entity Recognition Videos State-of-the-art Clinical Named Entity Recognition in Spark NLP Workshop - Veysel Kocaman Train your own NerDL. John Snow Labs - 7 Oct 2019 Articles State-of-the-art named entity recognition with BERTVeysel Kocaman - February 26th, 2020 State-of-the-art Named Entity Recognition in Spark NLPVeysel Kocaman Spark NLP in action: intelligent, high-accuracy fact extraction from long financial documentsSaif Addin Ellafi - May 5, 2020 Named Entity Recognition (NER) with BERT in Spark NLPVeysel Kocaman - Mar 4, 2020 Document Classification Videos Spark NLP in Action: Learning to read Life Science research - Saif Addin Ellafi. Saif Addin Ellafi - 1 Aug 2018 State of the art emotion and sentiment analysis with Spark NLP (Data science Salon). Dia Trambitas - December 1, 2020 Articles GloVe, ELMo &amp; BERT. A guide to state-of-the-art text classification using Spark NLP Ryan Burke - March 16, 2021 Distributed Topic Modelling using Spark NLP and Spark MLLib(LDA)Satish Silveri - June 11, 2020 Text Classification in Spark NLP with Bert and Universal Sentence EncodersVeysel Kocaman - April 12, 2020 Classification of Unstructured Documents into the Environmental, Social &amp; Governance (ESG) TaxonomyAlina Petukhova - May, 2020 Using Spark NLP to build a drug discovery knowledge graph for COVID-19Vishnu Vettrivel, Alexander Thomas - October 8, 2020 Build Text Categorization Model with Spark NLPSatish Silveri - Jul 8 2020 Topic Modelling with PySpark and Spark NLPMaria Obedkova - May 29 2020 Spark NLP Tasks &amp; Pipelines Videos Spark NLP Annotators, Annotations and Pipelines. John Snow Labs - 23 Oct 2019 Your first Spark NLP Pipeline. John Snow Labs - 23 Oct 2019 Natural Language Understanding at Scale with Spark NLP | DSS 2020. Veysel Kocaman - December 12, 2020 Articles Cleaning and extracting text from HTML/XML documents by using Spark NLPStefano Lori - January 13 2021 NER model with ELMo Embeddings in 5 minutes in Spark-NLPChristian Kasim Loan - Jule 2020 Applying Context Aware Spell Checking in Spark NLPAlberto Andreotti - May 2020 Spark nlp 2.5 delivers state-of-the-art accuracy for spell checking and sentiment analysisIda Lucente - May 12, 2020 Spark NLP 2.4: More Accurate NER, OCR, and Entity ResolutionIda Lucente - February 14, 2020 Introduction to Spark NLP: Foundations and Basic Components (Part-I)Veysel Kocaman - Sep 29, 2019 Introducing Spark NLP: Why would we need another NLP library (Part-I)Veysel Kocaman - October 22, 2019 Introducing Spark NLP: basic components and underlying technologies (Part-III)Veysel Kocaman - December 2, 2019 Explain document DL – Spark NLP pretrained pipelineVeysel Kocaman - January 15, 2020 Spark NLP Walkthrough, powered by TensorFlowSaif Addin Ellafi - Nov 19, 2018 Natural Language Processing with PySpark and Spark-NLPAllison Honold - Feb 5, 2020 Spark NLP for Healthcare Videos Advancing the State of the Art in Applied Natural Language Processing | Healthcare NLP Summit 2021. David Talby - 21 Apr 2021 Connecting the Dots in Clinical Document Understanding &amp; Information Extraction I Health NLP Summit. Veysel Kocaman - 15 Apr 2021 Advanced Natural Language Processing with Apache Spark NLP. David Talby - 20 Aug 2020 Applying State-of-the-art Natural Language Processing for Personalized Healthcare. David Talby - April 13, 2020 Apache Spark NLP for Healthcare: Lessons Learned Building Real-World Healthcare AI Systems. Veysel Kocaman - 9 Jul 2020 SNOMED entity resolver. John Snow Labs - 31 Jul 2020 NLP and its applications in Healthcare. Veysel Kocaman - 17 May 2020 Lessons Learned Building Real-World Healthcare AI Systems. Veysel Kocaman - April 13, 2020 Application of Spark NLP for Development of Multi-Modal Prediction Model from EHR | Healthcare NLP. Sutanay Choudhury - 14 Apr 2021 Best Practices in Improving NLP Accuracy for Clinical Use Cases I Healthcare NLP Summit 2021. Rajesh Chamarthi, Veysel Kocaman - 15 Apr 2021 Articles Named Entity Recognition for Healthcare with SparkNLP NerDL and NerCRFMaggie Yilmaz - Jul 20 2020 Roche automates knowledge extraction from pathology reports with Spark NLPCase Study Spark NLP in action: Improving patient flow forecastingCase Study Using Spark NLP to Enable Real-World Evidence (RWE) and Clinical Decision Support in OncologyVeysel Kocaman - April 13, 2020 Applying State-of-the-art Natural Language Processing for Personalized HealthcareDavid Talby - April 13, 2020 Automated Mapping of Clinical Entities from Natural Language Text to Medical TerminologiesAndrés Fernández - April 29 2020 Contextual Parser in Spark NLP: Extracting Medical Entities ContextuallyAlina Petukhova - May 28 2020 Deep6 accelerates clinical trial recruitment with Spark NLPCase Study SelectData uses AI to better understand home health patientsCase Study Explain Clinical Document Spark NLP Pretrained PipelineVeysel Kocaman - January 20, 2020 Introducing Spark NLP: State of the art NLP Package (Part-II)Veysel Kocaman - January 20, 2020 Automated Adverse Drug Event (ADE) Detection from Text in Spark NLP with BioBertVeysel Kocaman - Octover 4, 2020 Normalize drug names and dosage units with spark NLPDavid Cecchini - February 23, 2021 Spark NLP for healthcare 2.7.3 with biobert extraction models, higher accuracy, de-identification, new radiology ner model &amp; moreVeysel Kocaman - February 09, 2021 Spark OCR &amp; De-Identification Videos Maximizing Text Recognition Accuracy with Image Transformers in Spark OCR. Mykola Melnyk - June 24, 2020 Accurate de-identification, obfuscation, and editing of scanned medical documents and images. Alina Petukhova - August 19, 2020 Accurate De-Identification of Structured &amp; Unstructured Medical Data at Scale. Julio Bonis - March 18, 2020 Articles A Unified CV, OCR &amp; NLP Model Pipeline for Document Understanding at DocuSignPatrick Beukema, Michael Chertushkin - October 6, 2020 Scaling High-Accuracy Text Extraction from Images using Spark OCR on DatabricksMikola Melnyk - July 2, 2020 Spark NLP at Scale Videos Turbocharging State-of-the-art Natural Language Processing on Ray. David Talby - October 3, 2020 Articles Big Data Analysis of Meetup Events using Spark NLP, Kafka and Vegas VisualizationAndrei Deuşteanu - August 25, 2020 Setup Spark NLP on Databricks in 2 Minutes and get the taste of scalable NLPChristian Kasim Loan - May 25, 2020 Real-time trending topic detection using Spark NLP, Kafka and Vegas VisualizationValentina Crisan - Oct 15, 2020 Mueller Report for Nerds! Spark meets NLP with TensorFlow and BERTMaziyar Panahi - May 1, 2019 Spark in Docker in Kubernetes: A Practical Approach for Scalable NLPJürgen Schmidl - Jan 18 2020 Running Spark NLP in Docker Container for Named Entity Recognition and Other NLP FeaturesYuefeng Zhang - Jun 5 2020 Annotation Lab Videos Accelerating Clinical Data Abstraction and Real-World Data Curation with Active Learning, Dia Trambitas - Apr 15, 2021 MLOPS Veysel &amp; Dia. Dia Trambitas, Veysel Kocaman - July 16, 2020 Best Practices &amp; Tools for Accurate Document Annotation and Data Abstraction. Dia Trambitas - May 27, 2020 Articles John Snow Labs’ data annotator &amp; active learning for human-in-the-loop AI is now included with all subscriptionsIda Lucente - May 26, 2020 Auto NLP: Pretrain, Tune &amp; Deploy State-of-the-art Models Without CodingDia Trambitas - October 6, 2020 Lesson Learned annotating training data for healthcare NLP projectsRebecca Leung, Marianne Mak - October 8, 2020 Task review workflows in the annotation labDia Trambitas - March 08, 2021 The annotation lab 1.1 is here with improvements to speed, accuracy, and productivityIda Lucente - January 20, 2021 Tips and tricks on how to annotate assertion in clinical textsMauro Nievas Offidani - November 24, 2020 Spark NLP Benchmarks Articles Biomedical Named Entity Recognition at ScaleVeysel Kocaman, David Talby - November 12, 2020 NLP Industry Survey Analysis: the industry landscape of natural language use cases in 2020Paco Nathan - October 6, 2020 Comparing the Functionality of Open Source Natural Language Processing LibrariesMaziyar Panahi and David Talby - April 7, 2019 SpaCy or Spark NLP — A Benchmarking ComparisonMustafa Aytuğ Kaya - Aug 27, 2020 Comparing production-grade NLP libraries: Training Spark-NLP and spaCy pipelinesSaif Addin Ellafi - February 28, 2018 Comparing production-grade NLP libraries: Running Spark-NLP and spaCy pipelinesSaif Addin Ellafi - February 28, 2018 Comparing production-grade NLP libraries: Accuracy, performance, and scalabilitySaif Addin Ellafi - February 28, 2018 Spark NLP Awards Articles John Snow Labs is healthcare tech outlook’s 2020 healthcare analytics provider of the yearIda Lucente - July 14, 2020 John Snow Labs wins the 2020 artificial intelligence excellence awardIda Lucente - April 27, 2020 John Snow Labs is named ‘2019 ai platform of the yearIda Lucente - August 14, 2019 Spark NLP is the world’s most widely used nlp library by enterprise practitionersIda Lucente - May 6, 2019 John Snow Labs’ spark nlp wins “most significant open source project” at the strata data awardsIda Lucente April 1 - 2019 John Snow Labs named “artificial intelligence solution provider of the year” by cio reviewIda Lucente - February 7, 2019",
    "url": "/learn",
    "relUrl": "/learn"
  },
  "49": {
    "id": "49",
    "title": "Spark NLP for Healthcare",
    "content": "Getting started Spark NLP for Healthcare is a commercial extension of Spark NLP for clinical and biomedical text mining. If you don’t have a Spark NLP for Healthcare subscription yet, you can ask for a free trial by clicking on the Try Free button and following the instructions provides in the video below. Try Free A detailed step by step guide on how to obtain and use a trial license for John Snow Labs NLP Libraries is provided in the video below: Get a FREE license for John Snow Labs NLP Libraries Spark NLP for Healthcare provides healthcare-specific annotators, pipelines, models, and embeddings for: Clinical entity recognition Clinical Entity Linking Entity normalization Assertion Status Detection De-identification Relation Extraction Spell checking &amp; correction note: If you are going to use any pretrained licensed NER model, you don’t need to install licensed libray. As long as you have the AWS keys and license keys in your environment, you will be able to use licensed NER models with Spark NLP public library. For the other licensed pretrained models like AssertionDL, Deidentification, Entity Resolvers and Relation Extraction models, you will need to install Spark NLP Enterprise as well. The library offers access to several clinical and biomedical transformers: JSL-BERT-Clinical, BioBERT, ClinicalBERT, GloVe-Med, GloVe-ICD-O. It also includes over 50 pre-trained healthcare models, that can recognize the following entities (any many more): Clinical - support Signs, Symptoms, Treatments, Procedures, Tests, Labs, Sections Drugs - support Name, Dosage, Strength, Route, Duration, Frequency Risk Factors- support Smoking, Obesity, Diabetes, Hypertension, Substance Abuse Anatomy - support Organ, Subdivision, Cell, Structure Organism, Tissue, Gene, Chemical Demographics - support Age, Gender, Height, Weight, Race, Ethnicity, Marital Status, Vital Signs Sensitive Data- support Patient Name, Address, Phone, Email, Dates, Providers, Identifiers",
    "url": "/docs/en/license_getting_started",
    "relUrl": "/docs/en/license_getting_started"
  },
  "50": {
    "id": "50",
    "title": "Healthcare NLP Annotators",
    "content": "A Spark NLP for Healthcare subscription includes access to several pretrained annotators. At the Spark NLP Healthcare Workshop you can see different types of annotators in action. Check out the Spark NLP Annotators page for more information on how to read this page. Available Annotators Annotator Description AssertionDL AssertionDL is a deep Learning based approach used to extract Assertion Status from extracted entities and text. AssertionFilterer Filters entities coming from ASSERTION type annotations and returns the CHUNKS. AssertionLogReg Logarithmic Regression is used to extract Assertion Status from extracted entities and text. Chunk2Token A feature transformer that converts the input array of strings (annotatorType CHUNK) into an array of chunk-based tokens (annotatorType TOKEN). ChunkEntityResolver Returns a normalized entity for a particular trained ontology / curated dataset (e.g. ICD-10, RxNorm, SNOMED etc). ChunkFilterer Filters entities coming from CHUNK annotations. ChunkKeyPhraseExtraction Uses Bert Sentence Embeddings to determine the most relevant key phrases describing a text. ChunkMerge Merges entities coming from different CHUNK annotations. ContextualParser Extracts entity from a document based on user defined rules. DeIdentification Deidentifies Input Annotations of types DOCUMENT, TOKEN and CHUNK, by either masking or obfuscating the given CHUNKS. DocumentLogRegClassifier Classifies documents with a Logarithmic Regression algorithm. DrugNormalizer Annotator which normalizes raw text from clinical documents, e.g. scraped web pages or xml documents FeaturesAssembler Collects features from different columns. GenericClassifier Creates a generic single-label classifier which uses pre-generated Tensorflow graphs. IOBTagger Merges token tags and NER labels from chunks in the specified format. NerChunker Extracts phrases that fits into a known pattern using the NER tags. NerConverterInternal Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. NerDisambiguator Links words of interest, such as names of persons, locations and companies, from an input text document to a corresponding unique entity in a target Knowledge Base (KB). MedicalNer This Named Entity recognition annotator is a generic NER model based on Neural Networks.. RENerChunksFilter Filters and outputs combinations of relations between extracted entities, for further processing. ReIdentification Reidentifies obfuscated entities by DeIdentification. RelationExtraction Extracts and classifies instances of relations between named entities. RelationExtractionDL Extracts and classifies instances of relations between named entities. SentenceEntityResolver Returns the normalized entity for a particular trained ontology / curated dataset (e.g. ICD-10, RxNorm, SNOMED etc.) based on sentence embeddings. AssertionDL Approach Model Trains AssertionDL, a deep Learning based approach used to extract Assertion Status from extracted entities and text. Contains all the methods for training an AssertionDLModel. For pretrained models please use AssertionDLModel and see the Models Hub for available models. Input Annotator Types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output Annotator Type: ASSERTION Scala API: AssertionDLApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # First, pipeline stages for pre-processing the dataset (containing columns for text and label) are defined. document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) chunk = Doc2Chunk() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;chunk&quot;) token = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Define AssertionDLApproach with parameters and start training assertionStatus = AssertionDLApproach() .setLabelCol(&quot;label&quot;) .setInputCols([&quot;document&quot;, &quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) .setBatchSize(128) .setDropout(0.012) .setLearningRate(0.015) .setEpochs(1) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) .setMaxSentLen(250) trainingPipeline = Pipeline().setStages([ document, chunk, token, embeddings, assertionStatus ]) assertionModel = trainingPipeline.fit(data) assertionResults = assertionModel.transform(data).cache() // First, pipeline stages for pre-processing the dataset (containing columns for text and label) are defined. val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val chunk = new Doc2Chunk() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;chunk&quot;) val token = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Define AssertionDLApproach with parameters and start training val assertionStatus = new AssertionDLApproach() .setLabelCol(&quot;label&quot;) .setInputCols(&quot;document&quot;, &quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setBatchSize(128) .setDropout(0.012f) .setLearningRate(0.015f) .setEpochs(1) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) .setMaxSentLen(250) val trainingPipeline = new Pipeline().setStages(Array( document, chunk, token, embeddings, assertionStatus )) val assertionModel = trainingPipeline.fit(data) val assertionResults = assertionModel.transform(data).cache() AssertionDL is a deep Learning based approach used to extract Assertion Status from extracted entities and text. AssertionDLModel requires DOCUMENT, CHUNK and WORD_EMBEDDINGS type annotator inputs, which can be obtained by e.g a DocumentAssembler, NerConverter and WordEmbeddingsModel. The result is an assertion status annotation for each recognized entity. Possible values include “present”, “absent”, “hypothetical”, “conditional”, “associated_with_other_person” etc. For pretrained models please see the Models Hub for available models. Input Annotator Types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output Annotator Type: ASSERTION Scala API: AssertionDLModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Define pipeline stages to extract NER chunks first data = spark.createDataFrame([ [&quot;Patient with severe fever and sore throat&quot;], [&quot;Patient shows no stomach pain&quot;], [&quot;She was maintained on an epidural and PCA for pain control.&quot;]]).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setOutputCol(&quot;embeddings&quot;) nerModel = MedicalNerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]).setOutputCol(&quot;ner&quot;) nerConverter = NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]).setOutputCol(&quot;ner_chunk&quot;) # Then a pretrained AssertionDLModel is used to extract the assertion status clinicalAssertion = AssertionDLModel.pretrained(&quot;assertion_dl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion ]) assertionModel = assertionPipeline.fit(data) # Show results result = assertionModel.transform(data) result.selectExpr(&quot;ner_chunk.result&quot;, &quot;assertion.result&quot;).show(3, truncate=False) +--+--+ |result |result | +--+--+ |[severe fever, sore throat] |[present, present] | |[stomach pain] |[absent] | |[an epidural, PCA, pain control]|[present, present, hypothetical]| +--+--+ // Define pipeline stages to extract NER chunks first val data = Seq( &quot;Patient with severe fever and sore throat&quot;, &quot;Patient shows no stomach pain&quot;, &quot;She was maintained on an epidural and PCA for pain control.&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setOutputCol(&quot;embeddings&quot;) val nerModel = MedicalNerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;).setOutputCol(&quot;ner&quot;) val nerConverter = new NerConverter().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;).setOutputCol(&quot;ner_chunk&quot;) // Then a pretrained AssertionDLModel is used to extract the assertion status val clinicalAssertion = AssertionDLModel.pretrained(&quot;assertion_dl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion )) val assertionModel = assertionPipeline.fit(data) // Show results val result = assertionModel.transform(data) result.selectExpr(&quot;ner_chunk.result&quot;, &quot;assertion.result&quot;).show(3, truncate=false) +--+--+ |result |result | +--+--+ |[severe fever, sore throat] |[present, present] | |[stomach pain] |[absent] | |[an epidural, PCA, pain control]|[present, present, hypothetical]| +--+--+ AssertionFilterer Filters entities coming from ASSERTION type annotations and returns the CHUNKS. Filters can be set via a white list on the extracted chunk, the assertion or a regular expression. White list for assertion is enabled by default. To use chunk white list, criteria has to be set to &quot;isin&quot;. For regex, criteria has to be set to &quot;regex&quot;. Input Annotator Types: DOCUMENT, CHUNK, ASSERTION Output Annotator Type: CHUNK Scala API: AssertionFilterer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # To see how the assertions are extracted, see the example for AssertionDLModel. # Define an extra step where the assertions are filtered assertionFilterer = AssertionFilterer() .setInputCols([&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;]) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;assertion&quot;) .setWhiteList([&quot;present&quot;]) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion, assertionFilterer ]) assertionModel = assertionPipeline.fit(data) result = assertionModel.transform(data) # Show results: result.selectExpr(&quot;ner_chunk.result&quot;, &quot;assertion.result&quot;).show(3, truncate=False) +--+--+ |result |result | +--+--+ |[severe fever, sore throat] |[present, present] | |[stomach pain] |[absent] | |[an epidural, PCA, pain control]|[present, present, hypothetical]| +--+--+ result.select(&quot;filtered.result&quot;).show(3, truncate=False) ++ |result | ++ |[severe fever, sore throat]| |[] | |[an epidural, PCA] | ++ // To see how the assertions are extracted, see the example for // [[com.johnsnowlabs.nlp.annotators.assertion.dl.AssertionDLModel AssertionDLModel]]. // Define an extra step where the assertions are filtered val assertionFilterer = new AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;assertion&quot;) .setWhiteList(&quot;present&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion, assertionFilterer )) val assertionModel = assertionPipeline.fit(data) val result = assertionModel.transform(data) // Show results: // // result.selectExpr(&quot;ner_chunk.result&quot;, &quot;assertion.result&quot;).show(3, truncate=false) // +--+--+ // |result |result | // +--+--+ // |[severe fever, sore throat] |[present, present] | // |[stomach pain] |[absent] | // |[an epidural, PCA, pain control]|[present, present, hypothetical]| // +--+--+ // result.select(&quot;filtered.result&quot;).show(3, truncate=false) // ++ // |result | // ++ // |[severe fever, sore throat]| // |[] | // |[an epidural, PCA] | // ++ // AssertionLogReg Approach Model Trains a classification method, which uses the Logarithmic Regression Algorithm. It is used to extract Assertion Status from extracted entities and text. Contains all the methods for training a AssertionLogRegModel, together with trainWithChunk, trainWithStartEnd. Input Annotator Types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output Annotator Type: ASSERTION Scala API: AssertionLogRegApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Training with Glove Embeddings # First define pipeline stages to extract embeddings and text chunks documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) glove = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) .setCaseSensitive(False) chunkAssembler = Doc2Chunk() .setInputCols([&quot;document&quot;]) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) # Then the AssertionLogRegApproach model is defined. Label column is needed in the dataset for training. assertion = AssertionLogRegApproach() .setLabelCol(&quot;label&quot;) .setInputCols([&quot;document&quot;, &quot;chunk&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, assertion ]) assertionModel = assertionPipeline.fit(dataset) // Training with Glove Embeddings // First define pipeline stages to extract embeddings and text chunks val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val glove = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;word_embeddings&quot;) .setCaseSensitive(false) val chunkAssembler = new Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) // Then the AssertionLogRegApproach model is defined. Label column is needed in the dataset for training. val assertion = new AssertionLogRegApproach() .setLabelCol(&quot;label&quot;) .setInputCols(&quot;document&quot;, &quot;chunk&quot;, &quot;word_embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, assertion )) val assertionModel = assertionPipeline.fit(dataset) This is a main class in AssertionLogReg family. Logarithmic Regression is used to extract Assertion Status from extracted entities and text. AssertionLogRegModel requires DOCUMENT, CHUNK and WORD_EMBEDDINGS type annotator inputs, which can be obtained by e.g a DocumentAssembler, NerConverter and WordEmbeddingsModel. The result is an assertion status annotation for each recognized entity. Possible values are &quot;Negated&quot;, &quot;Affirmed&quot; and &quot;Historical&quot;. Unlike the DL Model, this class does not extend AnnotatorModel. Instead it extends the RawAnnotator, that’s why the main point of interest is method transform(). At the moment there are no pretrained models available for this class. Please refer to AssertionLogRegApproach to train your own model. Input Annotator Types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output Annotator Type: ASSERTION Scala API: AssertionLogRegModel Chunk2Token A feature transformer that converts the input array of strings (annotatorType CHUNK) into an array of chunk-based tokens (annotatorType TOKEN). When the input is empty, an empty array is returned. This Annotator is specially convenient when using NGramGenerator annotations as inputs to WordEmbeddingsModels Input Annotator Types: CHUNK Output Annotator Type: TOKEN Scala API: Chunk2Token Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Define a pipeline for generating n-grams data = spark.createDataFrame([[&quot;A 63-year-old man presents to the hospital ...&quot;]]).toDF(&quot;text&quot;) document = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) token = Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) ngrammer = NGramGenerator() .setN(2) .setEnableCumulative(False) .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;ngrams&quot;) .setDelimiter(&quot;_&quot;) # Stage to convert n-gram CHUNKS to TOKEN type chunk2Token = Chunk2Token().setInputCols([&quot;ngrams&quot;]).setOutputCol(&quot;ngram_tokens&quot;) trainingPipeline = Pipeline(stages=[document, sentenceDetector, token, ngrammer, chunk2Token]).fit(data) result = trainingPipeline.transform(data).cache() result.selectExpr(&quot;explode(ngram_tokens)&quot;).show(5, False) +-+ |col | +-+ |{token, 3, 15, A_63-year-old, {sentence -&gt; 0, chunk -&gt; 0}, []} | |{token, 5, 19, 63-year-old_man, {sentence -&gt; 0, chunk -&gt; 1}, []}| |{token, 17, 28, man_presents, {sentence -&gt; 0, chunk -&gt; 2}, []} | |{token, 21, 31, presents_to, {sentence -&gt; 0, chunk -&gt; 3}, []} | |{token, 30, 35, to_the, {sentence -&gt; 0, chunk -&gt; 4}, []} | +-+ // Define a pipeline for generating n-grams val data = Seq((&quot;A 63-year-old man presents to the hospital ...&quot;)).toDF(&quot;text&quot;) val document = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val token = new Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val ngrammer = new NGramGenerator() .setN(2) .setEnableCumulative(false) .setInputCols(&quot;token&quot;) .setOutputCol(&quot;ngrams&quot;) .setDelimiter(&quot;_&quot;) // Stage to convert n-gram CHUNKS to TOKEN type val chunk2Token = new Chunk2Token().setInputCols(&quot;ngrams&quot;).setOutputCol(&quot;ngram_tokens&quot;) val trainingPipeline = new Pipeline().setStages(Array(document, sentenceDetector, token, ngrammer, chunk2Token)).fit(data) val result = trainingPipeline.transform(data).cache() result.selectExpr(&quot;explode(ngram_tokens)&quot;).show(5, false) +-+ |col | +-+ |{token, 3, 15, A_63-year-old, {sentence -&gt; 0, chunk -&gt; 0}, []} | |{token, 5, 19, 63-year-old_man, {sentence -&gt; 0, chunk -&gt; 1}, []}| |{token, 17, 28, man_presents, {sentence -&gt; 0, chunk -&gt; 2}, []} | |{token, 21, 31, presents_to, {sentence -&gt; 0, chunk -&gt; 3}, []} | |{token, 30, 35, to_the, {sentence -&gt; 0, chunk -&gt; 4}, []} | +-+ ChunkEntityResolver Approach Model Contains all the parameters and methods to train a ChunkEntityResolverModel. It transform a dataset with two Input Annotations of types TOKEN and WORD_EMBEDDINGS, coming from e.g. ChunkTokenizer and ChunkEmbeddings Annotators and returns the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.) To use pretrained models please use ChunkEntityResolverModel and see the Models Hub for available models. Input Annotator Types: TOKEN, WORD_EMBEDDINGS Output Annotator Type: ENTITY Scala API: ChunkEntityResolverApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Training a SNOMED model # Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data # and their labels. document = DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) chunk = Doc2Chunk() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;chunk&quot;) token = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_healthcare_100d&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) chunkEmb = ChunkEmbeddings() .setInputCols([&quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;chunk_embeddings&quot;) snomedTrainingPipeline = Pipeline().setStages([ document, chunk, token, embeddings, chunkEmb ]) snomedTrainingModel = snomedTrainingPipeline.fit(data) snomedData = snomedTrainingModel.transform(data).cache() # Then the Resolver can be trained with snomedExtractor = ChunkEntityResolverApproach() .setInputCols([&quot;token&quot;, &quot;chunk_embeddings&quot;]) .setOutputCol(&quot;recognized&quot;) .setNeighbours(1000) .setAlternatives(25) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setEnableWmd(True).setEnableTfidf(True).setEnableJaccard(True) .setEnableSorensenDice(True).setEnableJaroWinkler(True).setEnableLevenshtein(True) .setDistanceWeights([1, 2, 2, 1, 1, 1]) .setAllDistancesMetadata(True) .setPoolingStrategy(&quot;MAX&quot;) .setThreshold(1e32) model = snomedExtractor.fit(snomedData) // Training a SNOMED model // Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data // and their labels. val document = new DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) val chunk = new Doc2Chunk() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;chunk&quot;) val token = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_healthcare_100d&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val chunkEmb = new ChunkEmbeddings() .setInputCols(&quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;chunk_embeddings&quot;) val snomedTrainingPipeline = new Pipeline().setStages(Array( document, chunk, token, embeddings, chunkEmb )) val snomedTrainingModel = snomedTrainingPipeline.fit(data) val snomedData = snomedTrainingModel.transform(data).cache() // Then the Resolver can be trained with val snomedExtractor = new ChunkEntityResolverApproach() .setInputCols(&quot;token&quot;, &quot;chunk_embeddings&quot;) .setOutputCol(&quot;recognized&quot;) .setNeighbours(1000) .setAlternatives(25) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setEnableWmd(true).setEnableTfidf(true).setEnableJaccard(true) .setEnableSorensenDice(true).setEnableJaroWinkler(true).setEnableLevenshtein(true) .setDistanceWeights(Array(1, 2, 2, 1, 1, 1)) .setAllDistancesMetadata(true) .setPoolingStrategy(&quot;MAX&quot;) .setThreshold(1e32) val model = snomedExtractor.fit(snomedData) Returns a normalized entity for a particular trained ontology / curated dataset (e.g. ICD-10, RxNorm, SNOMED etc). For available pretrained models please see the Models Hub. Input Annotator Types: TOKEN, WORD_EMBEDDINGS Output Annotator Type: ENTITY Scala API: ChunkEntityResolverModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Using pretrained models for SNOMED # First the prior steps of the pipeline are defined. # Output of types TOKEN and WORD_EMBEDDINGS are needed. data = spark.createDataFrame([[&quot;A 63-year-old man presents to the hospital ...&quot;]]).toDF(&quot;text&quot;) docAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) word_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) icdo_ner = MedicalNerModel.pretrained(&quot;ner_bionlp&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;icdo_ner&quot;) icdo_chunk = NerConverter().setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;icdo_ner&quot;]).setOutputCol(&quot;icdo_chunk&quot;).setWhiteList([&quot;Cancer&quot;]) icdo_chunk_embeddings = ChunkEmbeddings() .setInputCols([&quot;icdo_chunk&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;icdo_chunk_embeddings&quot;) icdo_chunk_resolver = ChunkEntityResolverModel.pretrained(&quot;chunkresolve_icdo_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;token&quot;,&quot;icdo_chunk_embeddings&quot;]) .setOutputCol(&quot;tm_icdo_code&quot;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) ner_chunk_tokenizer = ChunkTokenizer() .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;ner_token&quot;) ner_chunk_embeddings = ChunkEmbeddings() .setInputCols([&quot;ner_chunk&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;ner_chunk_embeddings&quot;) # Definition of the SNOMED Resolution ner_snomed_resolver = ChunkEntityResolverModel.pretrained(&quot;chunkresolve_snomed_findings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;ner_token&quot;,&quot;ner_chunk_embeddings&quot;]).setOutputCol(&quot;snomed_result&quot;) pipelineFull = Pipeline().setStages([ docAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, ner_converter, ner_chunk_embeddings, ner_chunk_tokenizer, ner_snomed_resolver, icdo_ner, icdo_chunk, icdo_chunk_embeddings, icdo_chunk_resolver ]) pipelineModelFull = pipelineFull.fit(data) result = pipelineModelFull.transform(data).cache() # Show results result.selectExpr(&quot;explode(snomed_result)&quot;) .selectExpr( &quot;col.metadata.target_text&quot;, &quot;col.metadata.resolved_text&quot;, &quot;col.metadata.confidence&quot;, &quot;col.metadata.all_k_results&quot;, &quot;col.metadata.all_k_resolutions&quot;) .filter($&quot;confidence&quot; &gt; 0.2).show(5) +--+--+-+--+--+ | target_text| resolved_text|confidence| all_k_results| all_k_resolutions| +--+--+-+--+--+ |hypercholesterolemia|Hypercholesterolemia| 0.2524|13644009:::267432...|Hypercholesterole...| | CBC| Neocyte| 0.4980|259680000:::11573...|Neocyte:::Blood g...| | CD38| Hypoviscosity| 0.2560|47872005:::370970...|Hypoviscosity:::E...| | platelets| Increased platelets| 0.5267|6631009:::2596800...|Increased platele...| | CD38| Hypoviscosity| 0.2560|47872005:::370970...|Hypoviscosity:::E...| +--+--+-+--+--+ // Using pretrained models for SNOMED // First the prior steps of the pipeline are defined. // Output of types TOKEN and WORD_EMBEDDINGS are needed. val data = Seq((&quot;A 63-year-old man presents to the hospital ...&quot;)).toDF(&quot;text&quot;) val docAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val word_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;word_embeddings&quot;) val icdo_ner = MedicalNerModel.pretrained(&quot;ner_bionlp&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;) .setOutputCol(&quot;icdo_ner&quot;) val icdo_chunk = new NerConverter().setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;icdo_ner&quot;).setOutputCol(&quot;icdo_chunk&quot;).setWhiteList(&quot;Cancer&quot;) val icdo_chunk_embeddings = new ChunkEmbeddings() .setInputCols(&quot;icdo_chunk&quot;, &quot;word_embeddings&quot;) .setOutputCol(&quot;icdo_chunk_embeddings&quot;) val icdo_chunk_resolver = ChunkEntityResolverModel.pretrained(&quot;chunkresolve_icdo_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;,&quot;icdo_chunk_embeddings&quot;) .setOutputCol(&quot;tm_icdo_code&quot;) val clinical_ner = MedicalNerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;) .setOutputCol(&quot;ner&quot;) val ner_converter = new NerConverter() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;ner_chunk&quot;) val ner_chunk_tokenizer = new ChunkTokenizer() .setInputCols(&quot;ner_chunk&quot;) .setOutputCol(&quot;ner_token&quot;) val ner_chunk_embeddings = new ChunkEmbeddings() .setInputCols(&quot;ner_chunk&quot;, &quot;word_embeddings&quot;) .setOutputCol(&quot;ner_chunk_embeddings&quot;) // Definition of the SNOMED Resolution val ner_snomed_resolver = ChunkEntityResolverModel.pretrained(&quot;chunkresolve_snomed_findings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;ner_token&quot;,&quot;ner_chunk_embeddings&quot;).setOutputCol(&quot;snomed_result&quot;) val pipelineFull = new Pipeline().setStages(Array( docAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, ner_converter, ner_chunk_embeddings, ner_chunk_tokenizer, ner_snomed_resolver, icdo_ner, icdo_chunk, icdo_chunk_embeddings, icdo_chunk_resolver )) val pipelineModelFull = pipelineFull.fit(data) val result = pipelineModelFull.transform(data).cache() // Show results // // result.selectExpr(&quot;explode(snomed_result)&quot;) // .selectExpr( // &quot;col.metadata.target_text&quot;, // &quot;col.metadata.resolved_text&quot;, // &quot;col.metadata.confidence&quot;, // &quot;col.metadata.all_k_results&quot;, // &quot;col.metadata.all_k_resolutions&quot;) // .filter($&quot;confidence&quot; &gt; 0.2).show(5) // +--+--+-+--+--+ // | target_text| resolved_text|confidence| all_k_results| all_k_resolutions| // +--+--+-+--+--+ // |hypercholesterolemia|Hypercholesterolemia| 0.2524|13644009:::267432...|Hypercholesterole...| // | CBC| Neocyte| 0.4980|259680000:::11573...|Neocyte:::Blood g...| // | CD38| Hypoviscosity| 0.2560|47872005:::370970...|Hypoviscosity:::E...| // | platelets| Increased platelets| 0.5267|6631009:::2596800...|Increased platele...| // | CD38| Hypoviscosity| 0.2560|47872005:::370970...|Hypoviscosity:::E...| // +--+--+-+--+--+ // ChunkFilterer Filters entities coming from CHUNK annotations. Filters can be set via a white list of terms or a regular expression. White list criteria is enabled by default. To use regex, criteria has to be set to regex. Input Annotator Types: DOCUMENT,CHUNK Output Annotator Type: CHUNK Scala API: ChunkFilterer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Filtering POS tags # First pipeline stages to extract the POS tags are defined data = spark.createDataFrame([[&quot;Has a past history of gastroenteritis and stomach pain, however patient ...&quot;]]).toDF(&quot;text&quot;) docAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) posTagger = PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) chunker = Chunker() .setInputCols([&quot;pos&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;(&lt;NN&gt;)+&quot;]) # Then the chunks can be filtered via a white list. Here only terms with &quot;gastroenteritis&quot; remain. chunkerFilter = ChunkFilterer() .setInputCols([&quot;sentence&quot;,&quot;chunk&quot;]) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList([&quot;gastroenteritis&quot;]) pipeline = Pipeline(stages=[ docAssembler, sentenceDetector, tokenizer, posTagger, chunker, chunkerFilter]) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk)&quot;).show(truncate=False) ++ |col | ++ |{chunk, 11, 17, history, {sentence -&gt; 0, chunk -&gt; 0}, []} | |{chunk, 22, 36, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 42, 53, stomach pain, {sentence -&gt; 0, chunk -&gt; 2}, []} | |{chunk, 64, 70, patient, {sentence -&gt; 0, chunk -&gt; 3}, []} | |{chunk, 81, 110, stomach pain now.We don&#39;t care, {sentence -&gt; 0, chunk -&gt; 4}, []}| |{chunk, 118, 132, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 5}, []} | ++ result.selectExpr(&quot;explode(filtered)&quot;).show(truncate=False) +-+ |col | +-+ |{chunk, 22, 36, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 118, 132, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 5}, []}| +-+ // Filtering POS tags // First pipeline stages to extract the POS tags are defined val data = Seq(&quot;Has a past history of gastroenteritis and stomach pain, however patient ...&quot;).toDF(&quot;text&quot;) val docAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val chunker = new Chunker() .setInputCols(&quot;pos&quot;, &quot;sentence&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;(&lt;NN&gt;)+&quot;)) // Then the chunks can be filtered via a white list. Here only terms with &quot;gastroenteritis&quot; remain. val chunkerFilter = new ChunkFilterer() .setInputCols(&quot;sentence&quot;,&quot;chunk&quot;) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList(&quot;gastroenteritis&quot;) val pipeline = new Pipeline().setStages(Array( docAssembler, sentenceDetector, tokenizer, posTagger, chunker, chunkerFilter)) result.selectExpr(&quot;explode(chunk)&quot;).show(truncate=false) ++ |col | ++ |{chunk, 11, 17, history, {sentence -&gt; 0, chunk -&gt; 0}, []} | |{chunk, 22, 36, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 42, 53, stomach pain, {sentence -&gt; 0, chunk -&gt; 2}, []} | |{chunk, 64, 70, patient, {sentence -&gt; 0, chunk -&gt; 3}, []} | |{chunk, 81, 110, stomach pain now.We don&#39;t care, {sentence -&gt; 0, chunk -&gt; 4}, []}| |{chunk, 118, 132, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 5}, []} | ++ result.selectExpr(&quot;explode(filtered)&quot;).show(truncate=false) +-+ |col | +-+ |{chunk, 22, 36, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 118, 132, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 5}, []}| +-+ ChunkKeyPhraseExtraction Chunk KeyPhrase Extraction uses Bert Sentence Embeddings to determine the most relevant key phrases describing a text. The input to the model consists of chunk annotations and sentence or document annotation. The model compares the chunks against the corresponding sentences/documents and selects the chunks which are most representative of the broader text context (i.e. the document or the sentence they belong to). The key phrases candidates (i.e. the input chunks) can be generated in various ways, e.g. by NGramGenerator, TextMatcher or NerConverter. The model operates either at sentence (selecting the most descriptive chunks from the sentence they belong to) or at document level. In the latter case, the key phrases are selected to represent all the input document annotations. This model is a subclass of [[BertSentenceEmbeddings]] and shares all parameters with it. It can load any pretrained BertSentenceEmbeddings model. Available models can be found at the Models Hub. Input Annotator Types: DOCUMENT, CHUNK Output Annotator Type: CHUNK Scala API: ChunkKeyPhraseExtraction Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline documenter = sparknlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencer = sparknlp.annotators.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = sparknlp.annotators.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;tokens&quot;) embeddings = sparknlp.annotators.WordEmbeddingsModel() .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_tagger = MedicalNerModel() .pretrained(&quot;ner_jsl_slim&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) ner_converter = NerConverter() .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;) .setOutputCol(&quot;ner_chunks&quot;) key_phrase_extractor = ChunkKeyPhraseExtraction .pretrained() .setTopN(1) .setDocumentLevelProcessing(False) .setDivergence(0.4) .setInputCols([&quot;sentences&quot;, &quot;ner_chunks&quot;]) .setOutputCol(&quot;ner_chunk_key_phrases&quot;) pipeline = sparknlp.base.Pipeline() .setStages([documenter, sentencer, tokenizer, embeddings, ner_tagger, ner_converter, key_phrase_extractor]) data = spark.createDataFrame([[&quot;Her Diabetes has become type 2 in the last year with her Diabetes.He complains of swelling in his right forearm.&quot;]]).toDF(&quot;text&quot;) results = pipeline.fit(data).transform(data) results .selectExpr(&quot;explode(ner_chunk_key_phrases) AS key_phrase&quot;) .selectExpr( &quot;key_phrase.result&quot;, &quot;key_phrase.metadata.entity&quot;, &quot;key_phrase.metadata.DocumentSimilarity&quot;, &quot;key_phrase.metadata.MMRScore&quot;) .show(truncate=False) +--++-+ |result |DocumentSimilarity|MMRScore | +--++-+ |gestational diabetes mellitus|0.7391447825527298|0.44348688715422274| |28-year-old |0.4366776288430703|0.13577881610104517| |type two diabetes mellitus |0.7323921930094919|0.085800103824974 | +--++-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.embeddings.BertSentenceEmbeddings import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;tokens&quot;) val stopWordsCleaner = StopWordsCleaner.pretrained() .setInputCols(&quot;tokens&quot;) .setOutputCol(&quot;clean_tokens&quot;) .setCaseSensitive(false) val nGrams = new NGramGenerator() .setInputCols(Array(&quot;clean_tokens&quot;)) .setOutputCol(&quot;ngrams&quot;) .setN(3) val chunkKeyPhraseExtractor = ChunkKeyPhraseExtraction .pretrained() .setTopN(2) .setDivergence(0.7f) .setInputCols(Array(&quot;document&quot;, &quot;ngrams&quot;)) .setOutputCol(&quot;key_phrases&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, stopWordsCleaner, nGrams, chunkKeyPhraseExtractor)) val sampleText = &quot;Her Diabetes has become type 2 in the last year with her Diabetes.&quot; + &quot; He complains of swelling in his right forearm.&quot; val testDataset = Seq(&quot;&quot;).toDS.toDF(&quot;text&quot;) val result = pipeline.fit(emptyDataset).transform(testDataset) result .selectExpr(&quot;explode(key_phrases) AS key_phrase&quot;) .selectExpr( &quot;key_phrase.result&quot;, &quot;key_phrase.metadata.DocumentSimilarity&quot;, &quot;key_phrase.metadata.MMRScore&quot;) .show(truncate=false) +--+-++ |result |DocumentSimilarity |MMRScore | +--+-++ |complains swelling forearm|0.6325718954229369 |0.1897715761677257| |type 2 year |0.40181028931546364|-0.189501077108947| +--+-++ ChunkMerge Approach Model Merges two chunk columns coming from two annotators(NER, ContextualParser or any other annotator producing chunks). The merger of the two chunk columns is made by selecting one chunk from one of the columns according to certain criteria. The decision on which chunk to select is made according to the chunk indices in the source document. (chunks with longer lengths and highest information will be kept from each source) Labels can be changed by setReplaceDictResource. Input Annotator Types: CHUNK, CHUNK Output Annotator Type: CHUNK Scala API: ChunkMergeApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Define a pipeline with 2 different NER models with a ChunkMergeApproach at the end data = spark.createDataFrame([[&quot;A 63-year-old man presents to the hospital ...&quot;]]).toDF(&quot;text&quot;) pipeline = Pipeline(stages=[ DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;), SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;), Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;), WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setOutputCol(&quot;embs&quot;), MedicalNerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;]).setOutputCol(&quot;jsl_ner&quot;), NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;jsl_ner&quot;]).setOutputCol(&quot;jsl_ner_chunk&quot;), MedicalNerModel.pretrained(&quot;ner_bionlp&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;]).setOutputCol(&quot;bionlp_ner&quot;), NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;bionlp_ner&quot;]) .setOutputCol(&quot;bionlp_ner_chunk&quot;), ChunkMergeApproach().setInputCols([&quot;jsl_ner_chunk&quot;, &quot;bionlp_ner_chunk&quot;]).setOutputCol(&quot;merged_chunk&quot;) ]) # Show results result = pipeline.fit(data).transform(data).cache() result.selectExpr(&quot;explode(merged_chunk) as a&quot;) .selectExpr(&quot;a.begin&quot;,&quot;a.end&quot;,&quot;a.result as chunk&quot;,&quot;a.metadata.entity as entity&quot;) .show(5, False) +--++--++ |begin|end|chunk |entity | +--++--++ |5 |15 |63-year-old|Age | |17 |19 |man |Gender | |64 |72 |recurrent |Modifier | |98 |107|cellulitis |Diagnosis| |110 |119|pneumonias |Diagnosis| +--++--++ // Define a pipeline with 2 different NER models with a ChunkMergeApproach at the end val data = Seq((&quot;A 63-year-old man presents to the hospital ...&quot;)).toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;), new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;), new Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;), WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setOutputCol(&quot;embs&quot;), MedicalNerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;).setOutputCol(&quot;jsl_ner&quot;), new NerConverter().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;jsl_ner&quot;).setOutputCol(&quot;jsl_ner_chunk&quot;), MedicalNerModel.pretrained(&quot;ner_bionlp&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;).setOutputCol(&quot;bionlp_ner&quot;), new NerConverter().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;bionlp_ner&quot;) .setOutputCol(&quot;bionlp_ner_chunk&quot;), new ChunkMergeApproach().setInputCols(&quot;jsl_ner_chunk&quot;, &quot;bionlp_ner_chunk&quot;).setOutputCol(&quot;merged_chunk&quot;) )) // Show results val result = pipeline.fit(data).transform(data).cache() result.selectExpr(&quot;explode(merged_chunk) as a&quot;) .selectExpr(&quot;a.begin&quot;,&quot;a.end&quot;,&quot;a.result as chunk&quot;,&quot;a.metadata.entity as entity&quot;) .show(5, false) +--++--++ |begin|end|chunk |entity | +--++--++ |5 |15 |63-year-old|Age | |17 |19 |man |Gender | |64 |72 |recurrent |Modifier | |98 |107|cellulitis |Diagnosis| |110 |119|pneumonias |Diagnosis| +--++--++ Merges entities coming from different CHUNK annotations Input Annotator Types: CHUNK, CHUNK Output Annotator Type: CHUNK Scala API: ChunkMergeModel ContextualParser Approach Model Creates a model, that extracts entity from a document based on user defined rules. Rule matching is based on a RegexMatcher defined in a JSON file. It is set through the parameter setJsonPath() In this JSON file, regex is defined that you want to match along with the information that will output on metadata field. Additionally, a dictionary can be provided with setDictionary to map extracted entities to a unified representation. The first column of the dictionary file should be the representation with following columns the possible matches. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Scala API: ContextualParserApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # An example JSON file `regex_token.json` can look like this: # # { # &quot;entity&quot;: &quot;Stage&quot;, # &quot;ruleScope&quot;: &quot;sentence&quot;, # &quot;regex&quot;: &quot;[cpyrau]?[T][0-9X?][a-z^cpyrau]&quot;, # &quot;matchScope&quot;: &quot;token&quot; # } # # Which means to extract the stage code on a sentence level. # An example pipeline could then be defined like this # Pipeline could then be defined like this documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) # Define the parser (json file needs to be provided) data = spark.createDataFrame([[&quot;A patient has liver metastases pT1bN0M0 and the T5 primary site may be colon or... &quot;]]).toDF(&quot;text&quot;) contextualParser = ContextualParserApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;entity&quot;) .setJsonPath(&quot;/path/to/regex_token.json&quot;) .setCaseSensitive(True) .setContextMatch(False) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, contextualParser ]) result = pipeline.fit(data).transform(data) # Show Results result.selectExpr(&quot;explode(entity)&quot;).show(5, truncate=False) +-+ |col | +-+ |{chunk, 32, 39, pT1bN0M0, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 0}, []} | |{chunk, 49, 50, T5, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 0}, []} | |{chunk, 148, 156, cT4bcN2M1, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 1}, []}| |{chunk, 189, 194, T?N3M1, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 2}, []} | |{chunk, 316, 323, pT1bN0M0, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 3}, []} | +-+ // An example JSON file `regex_token.json` can look like this: // // { // &quot;entity&quot;: &quot;Stage&quot;, // &quot;ruleScope&quot;: &quot;sentence&quot;, // &quot;regex&quot;: &quot;[cpyrau]?[T][0-9X?][a-z^cpyrau]&quot;, // &quot;matchScope&quot;: &quot;token&quot; // } // // Which means to extract the stage code on a sentence level. // An example pipeline could then be defined like this val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) // Define the parser (json file needs to be provided) val data = Seq(&quot;A patient has liver metastases pT1bN0M0 and the T5 primary site may be colon or... &quot;).toDF(&quot;text&quot;) val contextualParser = new ContextualParserApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;entity&quot;) .setJsonPath(&quot;/path/to/regex_token.json&quot;) .setCaseSensitive(true) .setContextMatch(false) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, contextualParser )) val result = pipeline.fit(data).transform(data) // Show Results // // result.selectExpr(&quot;explode(entity)&quot;).show(5, truncate=false) // +-+ // |col | // +-+ // |{chunk, 32, 39, pT1bN0M0, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 0}, []} | // |{chunk, 49, 50, T5, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 0}, []} | // |{chunk, 148, 156, cT4bcN2M1, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 1}, []}| // |{chunk, 189, 194, T?N3M1, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 2}, []} | // |{chunk, 316, 323, pT1bN0M0, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 3}, []} | // +-+ // Extracts entity from a document based on user defined rules. Rule matching is based on a RegexMatcher defined in a JSON file. In this file, regex is defined that you want to match along with the information that will output on metadata field. To instantiate a model, see ContextualParserApproach and its accompanied example. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Scala API: ContextualParserModel DeIdentification Approach Model Contains all the methods for training a DeIdentificationModel model. This module can obfuscate or mask the entities that contains personal information. These can be set with a file of regex patterns with setRegexPatternsDictionary, where each line is a mapping of entity to regex. DATE d{4} AID d{6,7} Additionally, obfuscation strings can be defined with setObfuscateRefFile, where each line is a mapping of string to entity. The format and seperator can be speficied with setRefFileFormat and setRefSep. Dr. Gregory House#DOCTOR 01010101#MEDICALRECORD Ideally this annotator works in conjunction with Demographic Named EntityRecognizers that can be trained either using TextMatchers, RegexMatchers, DateMatchers, NerCRFs or NerDLs Input Annotator Types: DOCUMENT, TOKEN, CHUNK Output Annotator Type: DOCUMENT Scala API: DeIdentification Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(True) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Ner entities clinical_sensitive_entities = MedicalNerModel .pretrained(&quot;ner_deid_enriched&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]).setOutputCol(&quot;ner&quot;) nerConverter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_con&quot;) # Deidentification deIdentification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) # file with custom regex pattern for custom entities .setRegexPatternsDictionary(&quot;path/to/dic_regex_patterns_main_categories.txt&quot;) # file with custom obfuscator names for the entities .setObfuscateRefFile(&quot;path/to/obfuscate_fixed_entities.txt&quot;) .setRefFileFormat(&quot;csv&quot;) .setRefSep(&quot;#&quot;) .setMode(&quot;obfuscate&quot;) .setDateFormats(Array(&quot;MM/dd/yy&quot;,&quot;yyyy-MM-dd&quot;)) .setObfuscateDate(True) .setDateTag(&quot;DATE&quot;) .setDays(5) .setObfuscateRefSource(&quot;file&quot;) # Pipeline data = spark.createDataFrame([ [&quot;# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09.&quot;] ]).toDF(&quot;text&quot;) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, clinical_sensitive_entities, nerConverter, deIdentification ]) result = pipeline.fit(data).transform(data) # Show Results result.select(&quot;dei.result&quot;).show(truncate = False) +--+ |result | +--+ |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , &lt;AGE&gt; years-old , Record date : 2079-11-14.]| +--+ val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(true) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) // Ner entities val clinical_sensitive_entities = MedicalNerModel.pretrained(&quot;ner_deid_enriched&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)).setOutputCol(&quot;ner&quot;) val nerConverter = new NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_con&quot;) // Deidentification val deIdentification = new DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) // file with custom regex patterns for custom entities .setRegexPatternsDictionary(&quot;path/to/dic_regex_patterns_main_categories.txt&quot;) // file with custom obfuscator names for the entities .setObfuscateRefFile(&quot;path/to/obfuscate_fixed_entities.txt&quot;) .setRefFileFormat(&quot;csv&quot;) .setRefSep(&quot;#&quot;) .setMode(&quot;obfuscate&quot;) .setDateFormats(Array(&quot;MM/dd/yy&quot;,&quot;yyyy-MM-dd&quot;)) .setObfuscateDate(true) .setDateTag(&quot;DATE&quot;) .setDays(5) .setObfuscateRefSource(&quot;file&quot;) // Pipeline val data = Seq( &quot;# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09.&quot; ).toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, clinical_sensitive_entities, nerConverter, deIdentification )) val result = pipeline.fit(data).transform(data) result.select(&quot;dei.result&quot;).show(truncate = false) // Show Results // // result.select(&quot;dei.result&quot;).show(truncate = false) // +--+ // |result | // +--+ // |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , &lt;AGE&gt; years-old , Record date : 2079-11-14.]| // +--+ // Deidentifies Input Annotations of types DOCUMENT, TOKEN and CHUNK, by either masking or obfuscating the given CHUNKS. To create a configured DeIdentificationModel, please see the example of DeIdentification. Input Annotator Types: DOCUMENT, TOKEN, CHUNK Output Annotator Type: DOCUMENT Scala API: DeIdentificationModel DocumentLogRegClassifier Approach Model Trains a model to classify documents with a Logarithmic Regression algorithm. Training data requires columns for text and their label. The result is a trained DocumentLogRegClassifierModel. Input Annotator Types: TOKEN Output Annotator Type: CATEGORY Scala API: DocumentLogRegClassifierApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Define pipeline stages to prepare the data document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) stopwords_cleaner = StopWordsCleaner() .setInputCols([&quot;normalized&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) stemmer = Stemmer() .setInputCols([&quot;cleanTokens&quot;]) .setOutputCol(&quot;stem&quot;) # Define the document classifier and fit training data to it logreg = DocumentLogRegClassifierApproach() .setInputCols([&quot;stem&quot;]) .setLabelCol(&quot;category&quot;) .setOutputCol(&quot;prediction&quot;) pipeline = Pipeline(stages=[ document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, logreg ]) model = pipeline.fit(trainingData) // Define pipeline stages to prepare the data val document_assembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) val stopwords_cleaner = new StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val stemmer = new Stemmer() .setInputCols(&quot;cleanTokens&quot;) .setOutputCol(&quot;stem&quot;) // Define the document classifier and fit training data to it val logreg = new DocumentLogRegClassifierApproach() .setInputCols(&quot;stem&quot;) .setLabelCol(&quot;category&quot;) .setOutputCol(&quot;prediction&quot;) val pipeline = new Pipeline().setStages(Array( document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, logreg )) val model = pipeline.fit(trainingData) Classifies documents with a Logarithmic Regression algorithm. Currently there are no pretrained models available. Please see DocumentLogRegClassifierApproach to train your own model. Please check out the Models Hub for available models in the future. Input Annotator Types: TOKEN Output Annotator Type: CATEGORY Scala API: DocumentLogRegClassifierModel DrugNormalizer Annotator which normalizes raw text from clinical documents, e.g. scraped web pages or xml documents, from document type columns into Sentence. Removes all dirty characters from text following one or more input regex patterns. Can apply non wanted character removal which a specific policy. Can apply lower case normalization. See Spark NLP Workshop for more examples of usage. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Scala API: DrugNormalizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline data = spark.createDataFrame([ [&quot;Sodium Chloride/Potassium Chloride 13bag&quot;], [&quot;interferon alfa-2b 10 million unit ( 1 ml ) injec&quot;], [&quot;aspirin 10 meq/ 5 ml oral sol&quot;] ]).toDF(&quot;text&quot;) document = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) drugNormalizer = DrugNormalizer().setInputCols([&quot;document&quot;]).setOutputCol(&quot;document_normalized&quot;) trainingPipeline = Pipeline(stages=[document, drugNormalizer]) result = trainingPipeline.fit(data).transform(data) result.selectExpr(&quot;explode(document_normalized.result) as normalized_text&quot;).show(truncate=False) +-+ |normalized_text | +-+ |Sodium Chloride / Potassium Chloride 13 bag | |interferon alfa - 2b 10000000 unt ( 1 ml ) injection| |aspirin 2 meq/ml oral solution | +-+ val data = Seq( (&quot;Sodium Chloride/Potassium Chloride 13bag&quot;), (&quot;interferon alfa-2b 10 million unit ( 1 ml ) injec&quot;), (&quot;aspirin 10 meq/ 5 ml oral sol&quot;) ).toDF(&quot;text&quot;) val document = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val drugNormalizer = new DrugNormalizer().setInputCols(&quot;document&quot;).setOutputCol(&quot;document_normalized&quot;) val trainingPipeline = new Pipeline().setStages(Array(document, drugNormalizer)) val result = trainingPipeline.fit(data).transform(data) result.selectExpr(&quot;explode(document_normalized.result) as normalized_text&quot;).show(false) +-+ |normalized_text | +-+ |Sodium Chloride / Potassium Chloride 13 bag | |interferon alfa - 2b 10000000 unt ( 1 ml ) injection| |aspirin 2 meq/ml oral solution | +-+ FeaturesAssembler The FeaturesAssembler is used to collect features from different columns. It can collect features from single value columns (anything which can be cast to a float, if casts fails then the value is set to 0), array columns or SparkNLP annotations (if the annotation is an embedding, it takes the embedding, otherwise tries to cast the result field). The output of the transformer is a FEATURE_VECTOR annotation (the numeric vector is in the embeddings field). Input Annotator Types: NONE Output Annotator Type: &quot;feature_vector&quot; Scala API: FeaturesAssembler Show Example PythonScala features_asm = FeaturesAssembler() .setInputCols([&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;]) .setOutputCol(&quot;features&quot;) gen_clf = GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setLearningRate(0.001) .setFixImbalance(True) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2) # keep 20% of the data for validation purposes pipeline = Pipeline(stages=[ features_asm, gen_clf ]) clf_model = pipeline.fit(data) val features_asm = new FeaturesAssembler() .setInputCols(Array(&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;)) .setOutputCol(&quot;features&quot;) val gen_clf = new GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols(&quot;features&quot;) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001f) .setFixImbalance(true) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2f) // keep 20% of the data for validation purposes val pipeline = new Pipeline().setStages(Array( features_asm, gen_clf )) val clf_model = pipeline.fit(data) GenericClassifier Approach Model Trains a TensorFlow model for generic classification of feature vectors. It takes FEATURE_VECTOR annotations from FeaturesAssembler as input, classifies them and outputs CATEGORY annotations. Please see the Parameters section for required training parameters. For a more extensive example please see the Spark NLP Workshop. Input Annotator Types: FEATURE_VECTOR Output Annotator Type: CATEGORY Scala API: GenericClassifierApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline features_asm = FeaturesAssembler() .setInputCols([&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;]) .setOutputCol(&quot;features&quot;) gen_clf = GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001) .setFixImbalance(True) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2) # keep 20% of the data for validation purposes pipeline = Pipeline().setStages([ features_asm, gen_clf ]) clf_model = pipeline.fit(data) val features_asm = new FeaturesAssembler() .setInputCols(Array(&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;)) .setOutputCol(&quot;features&quot;) val gen_clf = new GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols(&quot;features&quot;) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001f) .setFixImbalance(true) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2f) // keep 20% of the data for validation purposes val pipeline = new Pipeline().setStages(Array( features_asm, gen_clf )) val clf_model = pipeline.fit(data) Creates a generic single-label classifier which uses pre-generated Tensorflow graphs. The model operates on FEATURE_VECTOR annotations which can be produced using FeatureAssembler. Requires the FeaturesAssembler to create the input. Input Annotator Types: FEATURE_VECTOR Output Annotator Type: CATEGORY Scala API: GenericClassifierModel IOBTagger Merges token tags and NER labels from chunks in the specified format. For example output columns as inputs from NerConverter and Tokenizer can be used to merge. Input Annotator Types: TOKEN, CHUNK Output Annotator Type: NAMED_ENTITY Scala API: IOBTagger Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Pipeline stages are defined where NER is done. NER is converted to chunks. data = spark.createDataFrame([[&quot;A 63-year-old man presents to the hospital ...&quot;]]).toDF(&quot;text&quot;) docAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setOutputCol(&quot;embs&quot;) nerModel = MedicalNerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;]).setOutputCol(&quot;ner&quot;) nerConverter = NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]).setOutputCol(&quot;ner_chunk&quot;) # Define the IOB tagger, which needs tokens and chunks as input. Show results. iobTagger = IOBTagger().setInputCols([&quot;token&quot;, &quot;ner_chunk&quot;]).setOutputCol(&quot;ner_label&quot;) pipeline = Pipeline(stages=[docAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, iobTagger]) result.selectExpr(&quot;explode(ner_label) as a&quot;) .selectExpr(&quot;a.begin&quot;,&quot;a.end&quot;,&quot;a.result as chunk&quot;,&quot;a.metadata.word as word&quot;) .where(&quot;chunk!=&#39;O&#39;&quot;).show(5, False) +--++--+--+ |begin|end|chunk |word | +--++--+--+ |5 |15 |B-Age |63-year-old| |17 |19 |B-Gender |man | |64 |72 |B-Modifier |recurrent | |98 |107|B-Diagnosis|cellulitis | |110 |119|B-Diagnosis|pneumonias | +--++--+--+ // Pipeline stages are defined where NER is done. NER is converted to chunks. val data = Seq((&quot;A 63-year-old man presents to the hospital ...&quot;)).toDF(&quot;text&quot;) val docAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setOutputCol(&quot;embs&quot;) val nerModel = MedicalNerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;).setOutputCol(&quot;ner&quot;) val nerConverter = new NerConverter().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;).setOutputCol(&quot;ner_chunk&quot;) // Define the IOB tagger, which needs tokens and chunks as input. Show results. val iobTagger = new IOBTagger().setInputCols(&quot;token&quot;, &quot;ner_chunk&quot;).setOutputCol(&quot;ner_label&quot;) val pipeline = new Pipeline().setStages(Array(docAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, iobTagger)) result.selectExpr(&quot;explode(ner_label) as a&quot;) .selectExpr(&quot;a.begin&quot;,&quot;a.end&quot;,&quot;a.result as chunk&quot;,&quot;a.metadata.word as word&quot;) .where(&quot;chunk!=&#39;O&#39;&quot;).show(5, false) +--++--+--+ |begin|end|chunk |word | +--++--+--+ |5 |15 |B-Age |63-year-old| |17 |19 |B-Gender |man | |64 |72 |B-Modifier |recurrent | |98 |107|B-Diagnosis|cellulitis | |110 |119|B-Diagnosis|pneumonias | +--++--+--+ MedicalNer Approach Model This Named Entity recognition annotator allows to train generic NER model based on Neural Networks. The architecture of the neural network is a Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. For instantiated/pretrained models, see NerDLModel. The training data should be a labeled Spark Dataset, in the format of CoNLL 2003 IOB with Annotation type columns. The data should have columns of type DOCUMENT, TOKEN, WORD_EMBEDDINGS and an additional label column of annotator type NAMED_ENTITY. Excluding the label, this can be done with for example a SentenceDetector, a Tokenizer and a WordEmbeddingsModel with clinical embeddings (any clinical word embeddings can be chosen). For extended examples of usage, see the Spark NLP Workshop (sections starting with Training a Clinical NER) Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Scala API: MedicalNerApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp_jsl.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline # First extract the prerequisites for the NerDLApproach documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) clinical_embeddings = WordEmbeddingsModel.pretrained(&#39;embeddings_clinical&#39;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Then the training can start nerTagger = MedicalNerApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(2) .setBatchSize(64) .setRandomSeed(0) .setVerbose(1) .setValidationSplit(0.2) .setEvaluationLogExtended(True) .setEnableOutputLogs(True) .setIncludeConfidence(True) .setOutputLogsPath(&#39;ner_logs&#39;) .setGraphFolder(&#39;medical_ner_graphs&#39;) .setEnableMemoryOptimizer(True) #&gt;&gt; if you have a limited memory and a large conll file, you can set this True to train batch by batch pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, clinical_embeddings, nerTagger ]) # We use the text and labels from the CoNLL dataset conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.ner.MedicalNerApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline // First extract the prerequisites for the NerDLApproach val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger =new MedicalNerApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(5) .setLr(0.003f) .setBatchSize(8) .setRandomSeed(0) .setVerbose(1) .setEvaluationLogExtended(false) .setEnableOutputLogs(false) .setIncludeConfidence(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) // We use the text and labels from the CoNLL dataset val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) This Named Entity recognition annotator is a generic NER model based on Neural Networks. Pretrained models can be loaded with pretrained of the companion object: val nerModel = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) The default model is &quot;ner_clinical&quot;, if no name is provided. For available pretrained models please see the Models Hub. Additionally, pretrained pipelines are available for this module, see Pipelines. Note that some pretrained models require specific types of embeddings, depending on which they were trained on. For example, the default model &quot;ner_dl&quot; requires the WordEmbeddings &quot;ner_clinical&quot;. For extended examples of usage, see the Spark NLP Workshop (sections starting with Training a Clinical NER) Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Scala API: MedicalNerModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # First extract the prerequisites for the NerDLModel from sparknlp.annotator import * from sparknlp_jsl.annotator import * from sparknlp.base import * import sparknlp from pyspark.ml import Pipeline # First extract the prerequisites for the NerDLApproach documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) clinical_embeddings = WordEmbeddingsModel.pretrained(&#39;embeddings_clinical&#39;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Then the training can start nerTagger = MedicalNerApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(2) .setBatchSize(64) .setRandomSeed(0) .setVerbose(1) .setValidationSplit(0.2) .setEvaluationLogExtended(True) .setEnableOutputLogs(True) .setIncludeConfidence(True) .setOutputLogsPath(&#39;ner_logs&#39;) .setGraphFolder(&#39;medical_ner_graphs&#39;) .setEnableMemoryOptimizer(True) #&gt;&gt; if you have a limited memory and a large conll file, you can set this True to train batch by batch pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, clinical_embeddings, nerTagger ]) conll = CoNLL() conll_data = CoNLL().readDataset(spark, &#39;NER_NCBIconlltrain.txt&#39;) result = pipeline.fit(data).transform(data) import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.ner.dl.MedicalNerModel import org.apache.spark.ml.Pipeline // First extract the prerequisites for the NerDLModel val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;embeddings&quot;) // Then NER can be extracted val nerTagger = MedicalNerModel .pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) val data = Seq(&quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to&quot; + &quot; presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) conll = CoNLL() conll_data = CoNLL().readDataset(spark, &#39;NER_NCBIconlltrain.txt&#39;) ner_model = ner_pipeline.fit(conll_data) NerChunker Extracts phrases that fits into a known pattern using the NER tags. Useful for entity groups with neighboring tokens when there is no pretrained NER model to address certain issues. A Regex needs to be provided to extract the tokens between entities. Input Annotator Types: DOCUMENT, NAMED_ENTITY Output Annotator Type: CHUNK Scala API: NerChunker Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Defining pipeline stages for NER data= spark.createDataFrame([[&quot;She has cystic cyst on her kidney.&quot;]]).toDF(&quot;text&quot;) documentAssembler= DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector= SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(False) tokenizer= Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) ner = MedicalNerModel.pretrained(&quot;ner_radiology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) .setIncludeConfidence(True) # Define the NerChunker to combine to chunks chunker = NerChunker() .setInputCols([&quot;sentence&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers([&quot;&lt;ImagingFindings&gt;.*&lt;BodyPart&gt;&quot;]) pipeline= Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner, chunker ]) result = pipeline.fit(data).transform(data) # Show results: result.selectExpr(&quot;explode(arrays_zip(ner.metadata , ner.result))&quot;) .selectExpr(&quot;col[&#39;0&#39;].word as word&quot; , &quot;col[&#39;1&#39;] as ner&quot;).show(truncate=False) ++--+ |word |ner | ++--+ |She |O | |has |O | |cystic|B-ImagingFindings| |cyst |I-ImagingFindings| |on |O | |her |O | |kidney|B-BodyPart | |. |O | ++--+ result.select(&quot;ner_chunk.result&quot;).show(truncate=False) ++ |result | ++ |[cystic cyst on her kidney]| ++ // Defining pipeline stages for NER val data= Seq(&quot;She has cystic cyst on her kidney.&quot;).toDF(&quot;text&quot;) val documentAssembler=new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector=new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(false) val tokenizer=new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val ner = MedicalNerModel.pretrained(&quot;ner_radiology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) .setIncludeConfidence(true) // Define the NerChunker to combine to chunks val chunker = new NerChunker() .setInputCols(Array(&quot;sentence&quot;,&quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers(Array(&quot;&lt;ImagingFindings&gt;.&lt;BodyPart&gt;&quot;)) val pipeline=new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner, chunker )) val result = pipeline.fit(data).transform(data) // Show results: // // result.selectExpr(&quot;explode(arrays_zip(ner.metadata , ner.result))&quot;) // .selectExpr(&quot;col[&#39;0&#39;].word as word&quot; , &quot;col[&#39;1&#39;] as ner&quot;).show(truncate=false) // ++--+ // |word |ner | // ++--+ // |She |O | // |has |O | // |cystic|B-ImagingFindings| // |cyst |I-ImagingFindings| // |on |O | // |her |O | // |kidney|B-BodyPart | // |. |O | // ++--+ // result.select(&quot;ner_chunk.result&quot;).show(truncate=false) // ++ // |result | // ++ // |[cystic cyst on her kidney]| // ++ // NerConverterInternal Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. Chunks with no associated entity (tagged “O”) are filtered. See also Inside–outside–beginning (tagging) for more information. Input Annotator Types: DOCUMENT, TOKEN, NAMED_ENTITY Output Annotator Type: CHUNK Scala API: NerConverterInternal Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # The output of a MedicalNerModel follows the Annotator schema and looks like this after the transformation. result.selectExpr(&quot;explode(ner_result)&quot;).show(5, False) +--+ |col | +--+ |{named_entity, 3, 3, O, {word -&gt; A, confidence -&gt; 0.994}, []} | |{named_entity, 5, 15, B-Age, {word -&gt; 63-year-old, confidence -&gt; 1.0}, []}| |{named_entity, 17, 19, B-Gender, {word -&gt; man, confidence -&gt; 0.9858}, []} | |{named_entity, 21, 28, O, {word -&gt; presents, confidence -&gt; 0.9952}, []} | |{named_entity, 30, 31, O, {word -&gt; to, confidence -&gt; 0.7063}, []} | +--+ # After the converter is used: result.selectExpr(&quot;explode(ner_converter_result)&quot;).show(5, False) +--+ |col | +--+ |{chunk, 5, 15, 63-year-old, {entity -&gt; Age, sentence -&gt; 0, chunk -&gt; 0}, []} | |{chunk, 17, 19, man, {entity -&gt; Gender, sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 64, 72, recurrent, {entity -&gt; Modifier, sentence -&gt; 0, chunk -&gt; 2}, []} | |{chunk, 98, 107, cellulitis, {entity -&gt; Diagnosis, sentence -&gt; 0, chunk -&gt; 3}, []} | |{chunk, 110, 119, pneumonias, {entity -&gt; Diagnosis, sentence -&gt; 0, chunk -&gt; 4}, []}| +--+ // The output of a [[MedicalNerModel]] follows the Annotator schema and looks like this after the transformation. // // result.selectExpr(&quot;explode(ner_result)&quot;).show(5, false) // +--+ // |col | // +--+ // |{named_entity, 3, 3, O, {word -&gt; A, confidence -&gt; 0.994}, []} | // |{named_entity, 5, 15, B-Age, {word -&gt; 63-year-old, confidence -&gt; 1.0}, []}| // |{named_entity, 17, 19, B-Gender, {word -&gt; man, confidence -&gt; 0.9858}, []} | // |{named_entity, 21, 28, O, {word -&gt; presents, confidence -&gt; 0.9952}, []} | // |{named_entity, 30, 31, O, {word -&gt; to, confidence -&gt; 0.7063}, []} | // +--+ // // After the converter is used: // // result.selectExpr(&quot;explode(ner_converter_result)&quot;).show(5, false) // +--+ // |col | // +--+ // |{chunk, 5, 15, 63-year-old, {entity -&gt; Age, sentence -&gt; 0, chunk -&gt; 0}, []} | // |{chunk, 17, 19, man, {entity -&gt; Gender, sentence -&gt; 0, chunk -&gt; 1}, []} | // |{chunk, 64, 72, recurrent, {entity -&gt; Modifier, sentence -&gt; 0, chunk -&gt; 2}, []} | // |{chunk, 98, 107, cellulitis, {entity -&gt; Diagnosis, sentence -&gt; 0, chunk -&gt; 3}, []} | // |{chunk, 110, 119, pneumonias, {entity -&gt; Diagnosis, sentence -&gt; 0, chunk -&gt; 4}, []}| // +--+ // NerDisambiguator Approach Model Links words of interest, such as names of persons, locations and companies, from an input text document to a corresponding unique entity in a target Knowledge Base (KB). Words of interest are called Named Entities (NEs), mentions, or surface forms. The model needs extracted CHUNKS and SENTENCE_EMBEDDINGS type input from e.g. SentenceEmbeddings and NerConverter. Input Annotator Types: CHUNK, SENTENCE_EMBEDDINGS Output Annotator Type: DISAMBIGUATION Scala API: NerDisambiguator Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Extracting Person identities # First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. # Extracting Person identities # First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. data = spark.createDataFrame([[&quot;The show also had a contestant named Donald Trump who later defeated Christina Aguilera ...&quot;]]) .toDF(&quot;text&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) sentence_embeddings = SentenceEmbeddings() .setInputCols([&quot;sentence&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) ner_model = NerDLModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&quot;PER&quot;]) # Then the extracted entities can be disambiguated. disambiguator = NerDisambiguator() .setS3KnowledgeBaseName(&quot;i-per&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;disambiguation&quot;) .setNumFirstChars(5) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, word_embeddings, sentence_embeddings, ner_model, ner_converter, disambiguator]) model = nlpPipeline.fit(data) result = model.transform(data) # Show results result.selectExpr(&quot;explode(disambiguation)&quot;) .selectExpr(&quot;col.metadata.chunk as chunk&quot;, &quot;col.result as result&quot;).show(5, False) +++ |chunk |result | +++ |Donald Trump |http:#en.wikipedia.org/?curid=4848272, http:#en.wikipedia.org/?curid=31698421, http:#en.wikipedia.org/?curid=55907961 | |Christina Aguilera|http:#en.wikipedia.org/?curid=144171, http:#en.wikipedia.org/?curid=6636454 | +++ // Extracting Person identities // First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. val data = Seq(&quot;The show also had a contestant named Donald Trump who later defeated Christina Aguilera ...&quot;) .toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val word_embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val sentence_embeddings = new SentenceEmbeddings() .setInputCols(&quot;sentence&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val ner_model = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val ner_converter = new NerConverter() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList(&quot;PER&quot;) // Then the extracted entities can be disambiguated. val disambiguator = new NerDisambiguator() .setS3KnowledgeBaseName(&quot;i-per&quot;) .setInputCols(&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;) .setOutputCol(&quot;disambiguation&quot;) .setNumFirstChars(5) val nlpPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, word_embeddings, sentence_embeddings, ner_model, ner_converter, disambiguator)) val model = nlpPipeline.fit(data) val result = model.transform(data) // Show results // // result.selectExpr(&quot;explode(disambiguation)&quot;) // .selectExpr(&quot;col.metadata.chunk as chunk&quot;, &quot;col.result as result&quot;).show(5, false) // +++ // |chunk |result | // +++ // |Donald Trump |http://en.wikipedia.org/?curid=4848272, http://en.wikipedia.org/?curid=31698421, http://en.wikipedia.org/?curid=55907961| // |Christina Aguilera|http://en.wikipedia.org/?curid=144171, http://en.wikipedia.org/?curid=6636454 | // +++ // Links words of interest, such as names of persons, locations and companies, from an input text document to a corresponding unique entity in a target Knowledge Base (KB). Words of interest are called Named Entities (NEs), mentions, or surface forms. Instantiated / pretrained model of the NerDisambiguator. Links words of interest, such as names of persons, locations and companies, from an input text document to a corresponding unique entity in a target Knowledge Base (KB). Words of interest are called Named Entities (NEs), mentions, or surface forms. Input Annotator Types: CHUNK, SENTENCE_EMBEDDINGS Output Annotator Type: DISAMBIGUATION Scala API: NerDisambiguatorModel RENerChunksFilter Filters and outputs combinations of relations between extracted entities, for further processing. This annotator is especially useful to create inputs for the RelationExtractionDLModel. Input Annotator Types: CHUNK, DEPENDENCY Output Annotator Type: CHUNK Scala API: RENerChunksFilter Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Define pipeline stages to extract entities documenter = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencer = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;tokens&quot;) words_embedder = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) pos_tagger = PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;pos_tags&quot;) dependency_parser = DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) clinical_ner_tagger = MedicalNerModel.pretrained(&quot;jsl_ner_wip_greedy_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) ner_chunker = NerConverter() .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;]) .setOutputCol(&quot;ner_chunks&quot;) # Define the relation pairs and the filter relationPairs = [ &quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot; ] re_ner_chunk_filter = RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs([&quot;internal_organ_or_component-direction&quot;]) trained_pipeline = Pipeline(stages=[ documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_ner_chunk_filter ]) data = spark.createDataFrame([[&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;]]).toDF(&quot;text&quot;) result = trained_pipeline.fit(data).transform(data) # Show results result.selectExpr(&quot;explode(re_ner_chunks) as re_chunks&quot;) .selectExpr(&quot;re_chunks.begin&quot;, &quot;re_chunks.result&quot;, &quot;re_chunks.metadata.entity&quot;, &quot;re_chunks.metadata.paired_to&quot;) .show(6, truncate=False) +--+-+++ |begin|result |entity |paired_to| +--+-+++ |35 |upper |Direction |41 | |41 |brain stem |Internal_organ_or_component|35 | |35 |upper |Direction |59 | |59 |cerebellum |Internal_organ_or_component|35 | |35 |upper |Direction |81 | |81 |basil ganglia|Internal_organ_or_component|35 | +--+-+++ // Define pipeline stages to extract entities val documenter = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentencer = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;tokens&quot;) val words_embedder = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;) .setOutputCol(&quot;embeddings&quot;) val pos_tagger = PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;) .setOutputCol(&quot;pos_tags&quot;) val dependency_parser = DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;) .setOutputCol(&quot;dependencies&quot;) val clinical_ner_tagger = MedicalNerModel.pretrained(&quot;jsl_ner_wip_greedy_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner_tags&quot;) val ner_chunker = new NerConverter() .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;) .setOutputCol(&quot;ner_chunks&quot;) // Define the relation pairs and the filter val relationPairs = Array(&quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot;) val re_ner_chunk_filter = new RENerChunksFilter() .setInputCols(&quot;ner_chunks&quot;, &quot;dependencies&quot;) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs(Array(&quot;internal_organ_or_component-direction&quot;)) val trained_pipeline = new Pipeline().setStages(Array( documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_ner_chunk_filter )) val data = Seq(&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;).toDF(&quot;text&quot;) val result = trained_pipeline.fit(data).transform(data) // Show results // // result.selectExpr(&quot;explode(re_ner_chunks) as re_chunks&quot;) // .selectExpr(&quot;re_chunks.begin&quot;, &quot;re_chunks.result&quot;, &quot;re_chunks.metadata.entity&quot;, &quot;re_chunks.metadata.paired_to&quot;) // .show(6, truncate=false) // +--+-+++ // |begin|result |entity |paired_to| // +--+-+++ // |35 |upper |Direction |41 | // |41 |brain stem |Internal_organ_or_component|35 | // |35 |upper |Direction |59 | // |59 |cerebellum |Internal_organ_or_component|35 | // |35 |upper |Direction |81 | // |81 |basil ganglia|Internal_organ_or_component|35 | // +--+-+++ // ReIdentification Reidentifies obfuscated entities by DeIdentification. This annotator requires the outputs from the deidentification as input. Input columns need to be the deidentified document and the deidentification mappings set with DeIdentification.setMappingsColumn. To see how the entities are deidentified, please refer to the example of that class. Input Annotator Types: DOCUMENT,CHUNK Output Annotator Type: DOCUMENT Scala API: ReIdentification Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Define the reidentification stage and transform the deidentified documents reideintification = ReIdentification() .setInputCols([&quot;dei&quot;, &quot;protectedEntities&quot;]) .setOutputCol(&quot;reid&quot;) .transform(result) # Show results result.select(&quot;dei.result&quot;).show(truncate = False) +--+ |result | +--+ |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , &lt;AGE&gt; years-old , Record date : 2079-11-14.]| +--+ reideintification.selectExpr(&quot;explode(reid.result)&quot;).show(truncate=False) +--+ |col | +--+ |# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09.| +--+ // Define the reidentification stage and transform the deidentified documents val reideintification = new ReIdentification() .setInputCols(&quot;dei&quot;, &quot;protectedEntities&quot;) .setOutputCol(&quot;reid&quot;) .transform(result) // Show results // // result.select(&quot;dei.result&quot;).show(truncate = false) // +--+ // |result | // +--+ // |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , &lt;AGE&gt; years-old , Record date : 2079-11-14.]| // +--+ // reideintification.selectExpr(&quot;explode(reid.result)&quot;).show(false) // +--+ // |col | // +--+ // |# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09.| // +--+ // RelationExtraction Approach Model Trains a TensorFlow model for relation extraction. The Tensorflow graph in .pb format needs to be specified with setModelFile. The result is a RelationExtractionModel. To start training, see the parameters that need to be set in the Parameters section. Input Annotator Types: WORD_EMBEDDINGS, POS, CHUNK, DEPENDENCY Output Annotator Type: NONE Scala API: RelationExtractionApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Defining pipeline stages to extract entities first documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;tokens&quot;) embedder = WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) posTagger = PerceptronModel .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;posTags&quot;) nerTagger = MedicalNerModel .pretrained(&quot;ner_events_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) nerConverter = NerConverter() .setInputCols([&quot;document&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;]) .setOutputCol(&quot;nerChunks&quot;) depencyParser = DependencyParserModel .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;document&quot;, &quot;posTags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) # Then define `RelationExtractionApproach` and training parameters re = RelationExtractionApproach() .setInputCols([&quot;embeddings&quot;, &quot;posTags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations_t&quot;) .setLabelColumn(&quot;target_rel&quot;) .setEpochsNumber(300) .setBatchSize(200) .setLearningRate(0.001) .setModelFile(&quot;path/to/graph_file.pb&quot;) .setFixImbalance(True) .setValidationSplit(0.05) .setFromEntity(&quot;from_begin&quot;, &quot;from_end&quot;, &quot;from_label&quot;) .setToEntity(&quot;to_begin&quot;, &quot;to_end&quot;, &quot;to_label&quot;) finisher = Finisher() .setInputCols([&quot;relations_t&quot;]) .setOutputCols([&quot;relations&quot;]) .setCleanAnnotations(False) .setValueSplitSymbol(&quot;,&quot;) .setAnnotationSplitSymbol(&quot;,&quot;) .setOutputAsArray(False) # Define complete pipeline and start training pipeline = Pipeline(stages=[ documentAssembler, tokenizer, embedder, posTagger, nerTagger, nerConverter, depencyParser, re, finisher]) model = pipeline.fit(trainData) // Defining pipeline stages to extract entities first val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;tokens&quot;) val embedder = WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;embeddings&quot;) val posTagger = PerceptronModel .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;posTags&quot;) val nerTagger = MedicalNerModel .pretrained(&quot;ner_events_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner_tags&quot;) val nerConverter = new NerConverter() .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;)) .setOutputCol(&quot;nerChunks&quot;) val depencyParser = DependencyParserModel .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(Array(&quot;document&quot;, &quot;posTags&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;dependencies&quot;) // Then define `RelationExtractionApproach` and training parameters val re = new RelationExtractionApproach() .setInputCols(Array(&quot;embeddings&quot;, &quot;posTags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;)) .setOutputCol(&quot;relations_t&quot;) .setLabelColumn(&quot;target_rel&quot;) .setEpochsNumber(300) .setBatchSize(200) .setlearningRate(0.001f) .setModelFile(&quot;path/to/graph_file.pb&quot;) .setFixImbalance(true) .setValidationSplit(0.05f) .setFromEntity(&quot;from_begin&quot;, &quot;from_end&quot;, &quot;from_label&quot;) .setToEntity(&quot;to_begin&quot;, &quot;to_end&quot;, &quot;to_label&quot;) val finisher = new Finisher() .setInputCols(Array(&quot;relations_t&quot;)) .setOutputCols(Array(&quot;relations&quot;)) .setCleanAnnotations(false) .setValueSplitSymbol(&quot;,&quot;) .setAnnotationSplitSymbol(&quot;,&quot;) .setOutputAsArray(false) // Define complete pipeline and start training val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embedder, posTagger, nerTagger, nerConverter, depencyParser, re, finisher)) val model = pipeline.fit(trainData) Extracts and classifies instances of relations between named entities. For this, relation pairs need to be defined with setRelationPairs, to specify between which entities the extraction should be done. For pretrained models please see the Models Hub for available models. Input Annotator Types: WORD_EMBEDDINGS, POS, CHUNK, DEPENDENCY Output Annotator Type: CATEGORY Scala API: RelationExtractionModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Relation Extraction between body parts # Define pipeline stages to extract entities documenter = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencer = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;tokens&quot;) words_embedder = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) pos_tagger = PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;pos_tags&quot;) dependency_parser = DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) clinical_ner_tagger = MedicalNerModel.pretrained(&quot;jsl_ner_wip_greedy_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) ner_chunker = NerConverter() .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;]) .setOutputCol(&quot;ner_chunks&quot;) # Define the relations that are to be extracted relationPairs = [ &quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot; ] re_model = RelationExtractionModel.pretrained(&quot;re_bodypart_directions&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setRelationPairs(relationPairs) .setMaxSyntacticDistance(4) .setPredictionThreshold(0.9) pipeline = Pipeline().setStages([ documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_model ]) data = spark.createDataFrame([[&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) # Show results # result.selectExpr(&quot;explode(relations) as relations&quot;) .select( &quot;relations.metadata.chunk1&quot;, &quot;relations.metadata.entity1&quot;, &quot;relations.metadata.chunk2&quot;, &quot;relations.metadata.entity2&quot;, &quot;relations.result&quot; ) .where(&quot;result != 0&quot;) .show(truncate=False) # Show results result.selectExpr(&quot;explode(relations) as relations&quot;) .select( &quot;relations.metadata.chunk1&quot;, &quot;relations.metadata.entity1&quot;, &quot;relations.metadata.chunk2&quot;, &quot;relations.metadata.entity2&quot;, &quot;relations.result&quot; ).where(&quot;result != 0&quot;) .show(truncate=False) +++-+++ |chunk1|entity1 |chunk2 |entity2 |result| +++-+++ |upper |Direction|brain stem |Internal_organ_or_component|1 | |left |Direction|cerebellum |Internal_organ_or_component|1 | |right |Direction|basil ganglia|Internal_organ_or_component|1 | +++-+++ // Relation Extraction between body parts // Define pipeline stages to extract entities val documenter = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentencer = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;tokens&quot;) val words_embedder = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;) .setOutputCol(&quot;embeddings&quot;) val pos_tagger = PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;) .setOutputCol(&quot;pos_tags&quot;) val dependency_parser = DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;) .setOutputCol(&quot;dependencies&quot;) val clinical_ner_tagger = MedicalNerModel.pretrained(&quot;jsl_ner_wip_greedy_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner_tags&quot;) val ner_chunker = new NerConverter() .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;) .setOutputCol(&quot;ner_chunks&quot;) // Define the relations that are to be extracted val relationPairs = Array(&quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot;) val re_model = RelationExtractionModel.pretrained(&quot;re_bodypart_directions&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;) .setOutputCol(&quot;relations&quot;) .setRelationPairs(relationPairs) .setMaxSyntacticDistance(4) .setPredictionThreshold(0.9f) val pipeline = new Pipeline().setStages(Array( documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_model )) val data = Seq(&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) // Show results // // result.selectExpr(&quot;explode(relations) as relations&quot;) // .select( // &quot;relations.metadata.chunk1&quot;, // &quot;relations.metadata.entity1&quot;, // &quot;relations.metadata.chunk2&quot;, // &quot;relations.metadata.entity2&quot;, // &quot;relations.result&quot; // ) // .where(&quot;result != 0&quot;) // .show(truncate=false) // +++-+++ // |chunk1|entity1 |chunk2 |entity2 |result| // +++-+++ // |upper |Direction|brain stem |Internal_organ_or_component|1 | // |left |Direction|cerebellum |Internal_organ_or_component|1 | // |right |Direction|basil ganglia|Internal_organ_or_component|1 | // +++-+++ // RelationExtractionDL Extracts and classifies instances of relations between named entities. In contrast with RelationExtractionModel, RelationExtractionDLModel is based on BERT. For pretrained models please see the Models Hub for available models. Input Annotator Types: CHUNK, DOCUMENT Output Annotator Type: CATEGORY Scala API: RelationExtractionDLModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Relation Extraction between body parts # This is a continuation of the RENerChunksFilter example. See that class on how to extract the relation chunks. # Define the extraction model re_ner_chunk_filter = RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs([&quot;internal_organ_or_component-direction&quot;]) re_model = RelationExtractionDLModel.pretrained(&quot;redl_bodypart_direction_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setPredictionThreshold(0.5) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) trained_pipeline = Pipeline(stages=[ documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_ner_chunk_filter, re_model ]) data = spark.createDataFrame([[&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;]]).toDF(&quot;text&quot;) result = trained_pipeline.fit(data).transform(data) # Show results result.selectExpr(&quot;explode(relations) as relations&quot;) .select( &quot;relations.metadata.chunk1&quot;, &quot;relations.metadata.entity1&quot;, &quot;relations.metadata.chunk2&quot;, &quot;relations.metadata.entity2&quot;, &quot;relations.result&quot; ) .where(&quot;result != 0&quot;) .show(truncate=False) +++-+++ |chunk1|entity1 |chunk2 |entity2 |result| +++-+++ |upper |Direction|brain stem |Internal_organ_or_component|1 | |left |Direction|cerebellum |Internal_organ_or_component|1 | |right |Direction|basil ganglia|Internal_organ_or_component|1 | +++-+++ // Relation Extraction between body parts // This is a continuation of the [[RENerChunksFilter]] example. See that class on how to extract the relation chunks. // Define the extraction model val re_ner_chunk_filter = new RENerChunksFilter() .setInputCols(&quot;ner_chunks&quot;, &quot;dependencies&quot;) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs(Array(&quot;internal_organ_or_component-direction&quot;)) val re_model = RelationExtractionDLModel.pretrained(&quot;redl_bodypart_direction_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setPredictionThreshold(0.5f) .setInputCols(&quot;re_ner_chunks&quot;, &quot;sentences&quot;) .setOutputCol(&quot;relations&quot;) val trained_pipeline = new Pipeline().setStages(Array( documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_ner_chunk_filter, re_model )) val data = Seq(&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;).toDF(&quot;text&quot;) val result = trained_pipeline.fit(data).transform(data) // Show results // // result.selectExpr(&quot;explode(relations) as relations&quot;) // .select( // &quot;relations.metadata.chunk1&quot;, // &quot;relations.metadata.entity1&quot;, // &quot;relations.metadata.chunk2&quot;, // &quot;relations.metadata.entity2&quot;, // &quot;relations.result&quot; // ) // .where(&quot;result != 0&quot;) // .show(truncate=false) // +++-+++ // |chunk1|entity1 |chunk2 |entity2 |result| // +++-+++ // |upper |Direction|brain stem |Internal_organ_or_component|1 | // |left |Direction|cerebellum |Internal_organ_or_component|1 | // |right |Direction|basil ganglia|Internal_organ_or_component|1 | // +++-+++ // SentenceEntityResolver Approach Model Contains all the parameters and methods to train a SentenceEntityResolverModel. The model transforms a dataset with Input Annotation type SENTENCE_EMBEDDINGS, coming from e.g. BertSentenceEmbeddings and returns the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.) To use pretrained models please use SentenceEntityResolverModel and see the Models Hub for available models. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: ENTITY Scala API: SentenceEntityResolverApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Training a SNOMED resolution model using BERT sentence embeddings # Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. documentAssembler = DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) bertEmbeddings = BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;bert_embeddings&quot;) snomedTrainingPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, bertEmbeddings ]) snomedTrainingModel = snomedTrainingPipeline.fit(data) snomedData = snomedTrainingModel.transform(data).cache() # Then the Resolver can be trained with bertExtractor = SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols([&quot;bert_embeddings&quot;]) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(False) snomedModel = bertExtractor.fit(snomedData) // Training a SNOMED resolution model using BERT sentence embeddings // Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. val documentAssembler = new DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val bertEmbeddings = BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;bert_embeddings&quot;) val snomedTrainingPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, bertEmbeddings )) val snomedTrainingModel = snomedTrainingPipeline.fit(data) val snomedData = snomedTrainingModel.transform(data).cache() // Then the Resolver can be trained with val bertExtractor = new SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols(&quot;bert_embeddings&quot;) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(false) val snomedModel = bertExtractor.fit(snomedData) The model transforms a dataset with Input Annotation type SENTENCE_EMBEDDINGS, coming from e.g. BertSentenceEmbeddings and returns the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.) To use pretrained models please see the Models Hub for available models. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: ENTITY Scala API: SentenceEntityResolverModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Resolving CPT # First define pipeline stages to extract entities documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLModel.pretrained() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) clinical_ner = MedicalNerModel.pretrained(&quot;jsl_ner_wip_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&quot;Test&quot;,&quot;Procedure&quot;]) c2doc = Chunk2Doc() .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;ner_chunk_doc&quot;) sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) # Then the resolver is defined on the extracted entities and sentence embeddings cpt_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_cpt_procedures_augmented&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;cpt_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) sbert_pipeline_cpt = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, ner_converter, c2doc, sbert_embedder, cpt_resolver]) sbert_outputs = sbert_pipeline_cpt.fit(data_ner).transform(data) # Show results # # sbert_outputs # .select(&quot;explode(arrays_zip(ner_chunk.result ,ner_chunk.metadata, cpt_code.result, cpt_code.metadata, ner_chunk.begin, ner_chunk.end)) as cpt_code&quot;) # .selectExpr( # &quot;cpt_code[&#39;0&#39;] as chunk&quot;, # &quot;cpt_code[&#39;1&#39;].entity as entity&quot;, # &quot;cpt_code[&#39;2&#39;] as code&quot;, # &quot;cpt_code[&#39;3&#39;].confidence as confidence&quot;, # &quot;cpt_code[&#39;3&#39;].all_k_resolutions as all_k_resolutions&quot;, # &quot;cpt_code[&#39;3&#39;].all_k_results as all_k_results&quot; # ).show(5) # +--++--+-+--+--+ # | chunk| entity| code|confidence| all_k_resolutions| all_k_codes| # +--++--+-+--+--+ # | heart cath|Procedure|93566| 0.1180|CCA - Cardiac cat...|93566:::62319:::9...| # |selective coronar...| Test|93460| 0.1000|Coronary angiogra...|93460:::93458:::9...| # |common femoral an...| Test|35884| 0.1808|Femoral artery by...|35884:::35883:::3...| # | StarClose closure|Procedure|33305| 0.1197|Heart closure:::H...|33305:::33300:::3...| # | stress test| Test|93351| 0.2795|Cardiovascular st...|93351:::94621:::9...| # +--++--+-+--+--+ # // Resolving CPT // First define pipeline stages to extract entities val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = SentenceDetectorDLModel.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val word_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val clinical_ner = MedicalNerModel.pretrained(&quot;jsl_ner_wip_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val ner_converter = new NerConverter() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList(&quot;Test&quot;,&quot;Procedure&quot;) val c2doc = new Chunk2Doc() .setInputCols(&quot;ner_chunk&quot;) .setOutputCol(&quot;ner_chunk_doc&quot;) val sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;ner_chunk_doc&quot;) .setOutputCol(&quot;sbert_embeddings&quot;) // Then the resolver is defined on the extracted entities and sentence embeddings val cpt_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_cpt_procedures_augmented&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;) .setOutputCol(&quot;cpt_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) val sbert_pipeline_cpt = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, ner_converter, c2doc, sbert_embedder, cpt_resolver)) // Show results // // sbert_outputs // .select(&quot;explode(arrays_zip(ner_chunk.result ,ner_chunk.metadata, cpt_code.result, cpt_code.metadata, ner_chunk.begin, ner_chunk.end)) as cpt_code&quot;) // .selectExpr( // &quot;cpt_code[&#39;0&#39;] as chunk&quot;, // &quot;cpt_code[&#39;1&#39;].entity as entity&quot;, // &quot;cpt_code[&#39;2&#39;] as code&quot;, // &quot;cpt_code[&#39;3&#39;].confidence as confidence&quot;, // &quot;cpt_code[&#39;3&#39;].all_k_resolutions as all_k_resolutions&quot;, // &quot;cpt_code[&#39;3&#39;].all_k_results as all_k_results&quot; // ).show(5) // +--++--+-+--+--+ // | chunk| entity| code|confidence| all_k_resolutions| all_k_codes| // +--++--+-+--+--+ // | heart cath|Procedure|93566| 0.1180|CCA - Cardiac cat...|93566:::62319:::9...| // |selective coronar...| Test|93460| 0.1000|Coronary angiogra...|93460:::93458:::9...| // |common femoral an...| Test|35884| 0.1808|Femoral artery by...|35884:::35883:::3...| // | StarClose closure|Procedure|33305| 0.1197|Heart closure:::H...|33305:::33300:::3...| // | stress test| Test|93351| 0.2795|Cardiovascular st...|93351:::94621:::9...| // +--++--+-+--+--+ //",
    "url": "/docs/en/licensed_annotators",
    "relUrl": "/docs/en/licensed_annotators"
  },
  "51": {
    "id": "51",
    "title": "Spark NLP for Healthcare Installation",
    "content": "Install NLP libraries on Ubuntu For installing John Snow Labs NLP libraries on an Ubuntu machine/VM please run the following command: wget https://setup.johnsnowlabs.com/nlp/install.sh -O - | sudo bash -s -- -a PATH_TO_LICENSE_JSON_FILE -i -r This script will install Spark NLP, Spark NLP for Healthcare, Spark OCR, NLU and Spark NLP Display on the specified virtual environment. It will also create a special folder, ./JohnSnowLabs, dedicated to all resources necessary for using the libraries. Under ./JohnSnowLabs/example_notebooks you will find some ready to use example notebooks that you can use to test the libraries on your data. For a complete step by step guide on how to install NLP Libraries check the video below: Install John Snow Labs NLP Libraries on Ubuntu The install script offers several options: -h show brief help -i install mode: create a virtual environment and install the library -r run mode: start jupyter after installation of the library -v path to virtual environment (default: ./sparknlp_env) -j path to license json file for Spark NLP for Healthcare -o path to license json file for Spark OCR -a path to a single license json file for both Spark OCR and Spark NLP -s specify pyspark version -p specify port of jupyter notebook Use the -i flag for installing the libraries in a new virtual environment. You can provide the desired path for virtual env using -v flag, otherwise a default location of ./sparknlp_env will be selected. The PATH_TO_LICENSE_JSON_FILE parameter must be replaced with the path where the license file is available on the local machine. According to the libraries you want to use different flags are available: -j, -o or -a. The license files can be easily downloaded from My Subscription section in your my.JohnSnowLabs.com account. To start using Jupyter Notebook after the installation of the libraries use the -r flag. The install script downloads a couple of example notebooks that you can use to start experimenting with the libraries. Those will be availabe under ./JohnSnowLabs/example_notebooks folder. Install via Docker A docker image that contains all the required libraries for installing and running Spark NLP for Healthcare is also available. However, it does not contain the library itself, as it is licensed, and requires installation credentials. Make sure you have a valid license for Spark NLP for Healthcare (in case you do not have one, you can ask for a trial here ), and follow the instructions below: Instructions Run the following commands to download the docker-compose.yml and the sparknlp_keys.txt files on your local machine: curl -o docker-compose.yaml https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/513a4d682f11abc33b2e26ef8a9d72ad52a7b4f0/jupyter/docker_image_nlp_hc/docker-compose.yaml curl -o sparknlp_keys.txt https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jupyter/docker_image_nlp_hc/sparknlp_keys.txt Download your license key in json format from my.JohnSnowLabs.com Populate License keys in sparknlp_keys.txt file. Run the following command to run the container in detached mode: docker-compose up -d By default, the jupyter notebook runs on port 8888 - you can access it by typing localhost:8888 in your browser. Troubleshooting Make sure docker is installed on your system. If you face any error while importing the lib inside jupyter, make sure all the credentials are correct in the key files and restart the service again. If the default port 8888 is already occupied by another process, please change the mapping. You can change/adjust volume and port mapping in the docker-compose.yml file. You don’t have a license key? Ask for a trial license here. Install locally on Python You can install the Spark NLP for Healthcare package by using: pip install -q spark-nlp-jsl==${version} --extra-index-url https://pypi.johnsnowlabs.com/${secret.code} --upgrade {version} is the version part of the {secret.code} ({secret.code}.split(&#39;-&#39;)[0]) (i.e. 2.6.0) The {secret.code} is a secret code that is only available to users with valid/trial license. You can ask for a free trial for Spark NLP for Healthcare here. Then, you can obtain the secret code by visiting your account on my.JohnSnowLabs.com. Read more on how to get a license here. Setup AWS-CLI Credentials for licensed pretrained models Starting from Spark NLP for Healthcare version 2.4.2, you need to first setup your AWS credentials to be able to access the private repository for John Snow Labs Pretrained Models. You can do this setup via Amazon AWS Command Line Interface (AWSCLI). Instructions about how to install AWSCLI are available at: Installing the AWS CLI Make sure you configure your credentials with AWS configure following the instructions at: Configuring the AWS CLI Please substitute the ACCESS_KEY and SECRET_KEY with the credentials available on your license json file. This is available on your account from my.JohnSnowLabs.com. Read this for more information. Start Spark NLP for Healthcare Session from Python The following will initialize the spark session in case you have run the Jupyter Notebook directly. If you have started the notebook using pyspark this cell is just ignored. Initializing the spark session takes some seconds (usually less than 1 minute) as the jar from the server needs to be loaded. The {secret.code} is a secret code that is only available to users with valid/trial license. You can ask for a free trial for Spark NLP for Healthcare here. Then, you can obtain the secret code by visiting your account on my.JohnSnowLabs.com. Read more on how to get a license here. You can either use our convenience function to start your Spark Session that will use standard configuration arguments: import sparknlp_jsl spark = sparknlp_jsl.start(&quot;{secret.code}&quot;) Or use the SparkSession module for more flexibility: from pyspark.sql import SparkSession spark = SparkSession.builder .appName(&quot;Spark NLP Enterprise&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;,&quot;16&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;1000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.6&quot;) .config(&quot;spark.jars&quot;, &quot;https://pypi.johnsnowlabs.com/${secret.code}/spark-nlp-jsl-${version}.jar&quot;) .getOrCreate() If you want to download the source files (jar and whl files) locally, you can follow the instructions here. Spark NLP for Healthcare Cheat Sheet # Install Spark NLP from PyPI pip install spark-nlp==3.2.3 #install Spark NLP helathcare pip install spark-nlp-jsl==${version} --extra-index-url https://pypi.johnsnowlabs.com/${secret.code} --upgrade # Load Spark NLP with Spark Shell spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.3 --jars spark-nlp-jsl-${version}.jar # Load Spark NLP with PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.3 --jars spark-nlp-jsl-${version}.jar # Load Spark NLP with Spark Submit spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.3 --jars spark-nlp-jsl-${version}.jar Install locally for Scala Use Spark NLP for Healthcare in Spark shell 1.Download the fat jar for Spark NLP for Healthcare aws s3 cp --region us-east-2 s3://pypi.johnsnowlabs.com/$jsl_secret/spark-nlp-jsl-$jsl_version.jar spark-nlp-jsl-$jsl_version.jar 2.Set up the Environment Variables box: AWS_ACCESS_KEY_ID=xxx AWS_SECRET_ACCESS_KEY=yyy SPARK_NLP_LICENSE=zzz 3.The preferred way to use the library when running Spark programs is using the --packagesand --jar option as specified in the spark-packages section. spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:${public-version} --jars /spark-nlp-jsl-${version}.jar Use Spark NLP for Healthcare in Sbt project 1.Download the fat jar for Spark NLP for Healthcare. aws s3 cp --region us-east-2 s3://pypi.johnsnowlabs.com/$jsl_secret/spark-nlp-jsl-$jsl_version.jar spark-nlp-jsl-$jsl_version.jar 2.Set up the Environment Variables box: AWS_ACCESS_KEY_ID=xxx AWS_SECRET_ACCESS_KEY=yyy SPARK_NLP_LICENSE=zzz 3.Add the spark-nlp jar in your build.sbt project libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp&quot; % &quot;{public-version}&quot; 4.You need to create the /lib folder and paste the spark-nlp-jsl-${version}.jar file. 5.Add the fat spark-nlp-healthcare in your classpath. You can do it by adding this line in your build.sbt unmanagedJars in Compile += file(&quot;lib/sparknlp-jsl.jar&quot;) Install on Databricks Automatic deployment of John Snow Labs NLP libraries You can automatically deploy John Snow Labs libraries on Databricks by filling in the form available here. This will allow you to start a 30-day free trial with no limit on the amount of processed data. You just need to provide a Databricks Access Token that is used by our deployment script to connect to your Databricks instance and install John Snow Labs NLP libraries on a cluster of your choice. Manual deployment of Spark NLP for Healthcare Create a cluster if you don’t have one already On a new cluster or existing one you need to add the following to the Advanced Options -&gt; Spark tab, in Spark.Config box: spark.kryoserializer.buffer.max 1000M spark.serializer org.apache.spark.serializer.KryoSerializer spark.driver.extraJavaOptions -Dspark.jsl.settings.pretrained.credentials.secret_access_key=xxx -Dspark.jsl.settings.pretrained.credentials.access_key_id=yyy Please add the following to the Advanced Options -&gt; Spark tab, in Environment Variables box: SPARK_NLP_LICENSE=zzz (OPTIONAL) If the environment variables used to setup the AWS Access/Secret keys are conflicting with the credential provider chain in Databricks, you may not be able to access to other s3 buckets. To access both JSL repos with JSL AWS keys as well as your own s3 bucket with your own AWS keys), you need to use the following script, copy that to dbfs folder, then go to the Databricks console (init scripts menu) to add the init script for your cluster as follows: %scala val script = &quot;&quot;&quot; #!/bin/bash echo &quot;******** Inject Spark NLP AWS Profile Credentials ******** &quot; mkdir ~/.aws/ cat &lt;&lt; EOF &gt; ~/.aws/credentials [spark_nlp] aws_access_key_id=&lt;YOUR_AWS_ACCESS_KEY&gt; aws_secret_access_key=&lt;YOUR_AWS_SECRET_KEY&gt; EOF echo &quot;******** End Inject Spark NLP AWS Profile Credentials ******** &quot; &quot;&quot;&quot; In Libraries tab inside your cluster you need to follow these steps: Install New -&gt; PyPI -&gt; spark-nlp -&gt; Install Install New -&gt; Maven -&gt; Coordinates -&gt; com.johnsnowlabs.nlp:spark-nlp_2.12:${version} -&gt; Install Please add following jars: Install New -&gt; Python Whl -&gt; upload https://pypi.johnsnowlabs.com/${secret.code}/spark-nlp-jsl/spark_nlp_jsl-${version}-py3-none-any.whl Install New -&gt; Jar -&gt; upload https://pypi.johnsnowlabs.com/${secret.code}/spark-nlp-jsl-${version}.jar Now you can attach your notebook to the cluster and use Spark NLP! Use on Google Colab Run the following code in Google Colab notebook and start using Spark NLP right away. The first thing that you need is to create the json file with the credentials and the configuration in your local system. { &quot;PUBLIC_VERSION&quot;: &quot;3.2.3&quot;, &quot;JSL_VERSION&quot;: &quot;{version}&quot;, &quot;SECRET&quot;: &quot;{version}-{secret.code}&quot;, &quot;SPARK_NLP_LICENSE&quot;: &quot;xxxxx&quot;, &quot;AWS_ACCESS_KEY_ID&quot;: &quot;yyyy&quot;, &quot;AWS_SECRET_ACCESS_KEY&quot;: &quot;zzzz&quot; } If you have a valid floating license, the license json file can be downloaded from your account on my.JohnSnowLabs.com on My Subscriptions section. To get a trial license please visit Then you need to write that piece of code to load the credentials that you created before. import json from google.colab import files license_keys = files.upload() with open(list(license_keys.keys())[0]) as f: license_keys = json.load(f) # This is only to setup PySpark and Spark NLP on Colab !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jsl_colab_setup.sh This script comes with the two options to define pyspark,spark-nlp and spark-nlp-jsl versions via options: # -p is for pyspark # -s is for spark-nlp # by default they are set to the latest !bash jsl_colab_setup.sh Spark NLP quick start on Google Colab is a live demo on Google Colab that performs named entity recognitions for HealthCare. Install on GCP Dataproc Create a cluster if you don’t have one already as follows. At gcloud shell: gcloud services enable dataproc.googleapis.com compute.googleapis.com storage-component.googleapis.com bigquery.googleapis.com bigquerystorage.googleapis.com REGION=&lt;region&gt; BUCKET_NAME=&lt;bucket_name&gt; gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME} REGION=&lt;region&gt; ZONE=&lt;zone&gt; CLUSTER_NAME=&lt;cluster_name&gt; BUCKET_NAME=&lt;bucket_name&gt; You can set image-version, master-machine-type, worker-machine-type, master-boot-disk-size, worker-boot-disk-size, num-workers as your needs. If you use the previous image-version from 2.0, you should also add ANACONDA to optional-components. And, you should enable gateway. As noticed below, you should explicitly write JSL_SECRET and JSL_VERSION at metadata param inside the quotes. This will start the pip installation using the wheel file of Licensed SparkNLP! gcloud dataproc clusters create ${CLUSTER_NAME} --region=${REGION} --network=${NETWORK} --zone=${ZONE} --image-version=2.0 --master-machine-type=n1-standard-4 --worker-machine-type=n1-standard-2 --master-boot-disk-size=128GB --worker-boot-disk-size=128GB --num-workers=2 --bucket=${BUCKET_NAME} --optional-components=JUPYTER --enable-component-gateway --metadata &#39;PIP_PACKAGES=google-cloud-bigquery google-cloud-storage spark-nlp-display https://s3.eu-west-1.amazonaws.com/pypi.johnsnowlabs.com/JSL_SECRET/spark-nlp-jsl/spark_nlp_jsl-JSL_VERSION-py3-none-any.whl&#39; --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh On an existing one, you need to install spark-nlp and spark-nlp-display packages from PyPI. Now, you can attach your notebook to the cluster and use Spark NLP via following the instructions. The key part of this usage is how to start SparkNLP sessions using Apache Hadoop YARN cluster manager. 3.1. Read license file from the notebook using GCS. 3.2. Set the right path of the Java Home Path. 3.3. Use the start function to start the SparkNLP JSL version such as follows: def start(secret): builder = SparkSession.builder .appName(&quot;Spark NLP Licensed&quot;) .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:&quot;+version) .config(&quot;spark.jars&quot;, &quot;https://pypi.johnsnowlabs.com/&quot;+secret+&quot;/spark-nlp-jsl-&quot;+jsl_version+&quot;.jar&quot;) return builder.getOrCreate() spark = start(SECRET) As you see, we did not set .master(&#39;local[*]&#39;) explicitly to let YARN manage the cluster. Or you can set .master(&#39;yarn&#39;). Spark-NLP for Healthcare in AWS EMR In this page we explain how to setup Spark-NLP + Spark-NLP Healthcare in AWS EMR, using the AWS console. Steps You must go to the blue button “Create Cluster” on the UI. By doing that you will get directed to the “Create Cluster - Quick Options” page. Don’t use the quick options, click on “Go to advanced options” instead. Now in Advanced Options, on Step 1, “Software and Steps”, please pick the following selection in the checkboxes, Also in the “Edit Software Settings” page, enter the following, [{ &quot;Classification&quot;: &quot;spark-env&quot;, &quot;Configurations&quot;: [{ &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;PYSPARK_PYTHON&quot;: &quot;/usr/bin/python3&quot;, &quot;AWS_ACCESS_KEY_ID&quot;: &quot;XYXYXYXYXYXYXYXYXYXY&quot;, &quot;AWS_SECRET_ACCESS_KEY&quot;: &quot;XYXYXYXYXYXYXYXYXYXY&quot;, &quot;SPARK_NLP_LICENSE&quot;: &quot;XYXYXYXYXYXYXYXYXYXYXYXYXYXY&quot; } }] }, { &quot;Classification&quot;: &quot;spark-defaults&quot;, &quot;Properties&quot;: { &quot;spark.yarn.stagingDir&quot;: &quot;hdfs:///tmp&quot;, &quot;spark.yarn.preserve.staging.files&quot;: &quot;true&quot;, &quot;spark.kryoserializer.buffer.max&quot;: &quot;2000M&quot;, &quot;spark.serializer&quot;: &quot;org.apache.spark.serializer.KryoSerializer&quot;, &quot;spark.driver.maxResultSize&quot;: &quot;0&quot;, &quot;spark.driver.memory&quot;: &quot;32G&quot; } }] Make sure that you replace all the secret information(marked here as XYXYXYXYXY) by the appropriate values that you received with your license. In “Step 2” choose the hardware and networking configuration you prefer, or just pick the defaults. Move to next step by clocking the “Next” blue button. Now you are in “Step 3”, in which you assign a name to your cluster, and you can change the location of the cluster logs. If the location of the logs is OK for you, take note of the path so you can debug potential problems by using the logs. Still on “Step 3”, go to the bottom of the page, and expand the “Bootstrap Actions” tab. We’re gonna add an action to execute during bootstrap of the cluster. Select “Custom Action”, then press on “Configure and add”. You need to provide a path to a script on S3. The path needs to be public. Keep this in mind, no secret information can be contained there. The script we’ll used for this setup is emr_bootstrap.sh . This script will install Spark-NLP 3.1.0, and Spark-NLP Healthcare 3.1.1. You’ll have to edit the script if you need different versions. After you entered the route to S3 in which you place the emr_bootstrap.sh file, and before clicking “add” in the dialog box, you must pass an additional parameter containing the SECRET value you received with your license. Just paste the secret on the “Optional arguments” field in that dialog box. There’s not much additional setup you need to perform. So just start a notebook server, connect it to the cluster you just created(be patient, it takes a while), and test with the NLP_EMR_Setup.ipynb test notebook. Amazon Linux 2 Support # Update Package List &amp; Install Required Packages sudo yum update sudo yum install -y amazon-linux-extras sudo yum -y install python3-pip # Create Python virtual environment and activate it: python3 -m venv .sparknlp-env source .sparknlp-env/bin/activate Check JAVA version: For Sparknlp versions above 3.x, please use JAVA-11 For Sparknlp versions below 3.x and SparkOCR, please use JAVA-8 Checking Java versions installed on your machine: sudo alternatives --config java You can pick the index number (I am using java-8 as default - index 2): &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; If you dont have java-11 or java-8 in you system, you can easily install via: sudo yum install java-1.8.0-openjdk Now, we can start installing the required libraries: pip install jupyter We can start jupyter notebook via: jupyter notebook ### Now we are in the jupyter notebook cell: import json import os with open(&#39;sparknlp_for_healthcare.json) as f: license_keys = json.load(f) # Defining license key-value pairs as local variables locals().update(license_keys) # Adding license key-value pairs to environment variables os.environ.update(license_keys) # Installing pyspark and spark-nlp ! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION # Installing Spark NLP Healthcare ! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION --extra-index-url https://pypi.johnsnowlabs.com/$SECRET Get a Spark NLP for Healthcare license You can ask for a free trial for Spark NLP for Healthcare here. This will automatically create a new account for you on my.JohnSnowLabs.com. Login in to your new account and from My Subscriptions section, you can download your license key as a json file. The license json file contains: the secrets for installing the Spark NLP for Healthcare and Spark OCR libraries, the license key as well as AWS credentials that you need to access the s3 bucket where the healthcare models and pipelines are published. If you have asked for a trial license but you cannot access your account on my.JohnSnowLabs.com and you did not receive the license information via email, please contact us at support@johnsnowlabs.com.",
    "url": "/docs/en/licensed_install",
    "relUrl": "/docs/en/licensed_install"
  },
  "52": {
    "id": "52",
    "title": "Licensed Models",
    "content": "Pretrained Models We are currently in the process of moving the pretrained models and pipelines to a Model Hub that you can explore here: Models Hub",
    "url": "/docs/en/licensed_models",
    "relUrl": "/docs/en/licensed_models"
  },
  "53": {
    "id": "53",
    "title": "Spark NLP for Healthcare Release Notes",
    "content": "3.3.4 We are glad to announce that Spark NLP Healthcare 3.3.4 has been released! Highlights New Clinical NER Models New NER Model Finder Pretrained Pipeline New Relation Extraction Model New LOINC, MeSH, NDC and SNOMED Entity Resolver Models Updated RxNorm Sentence Entity Resolver Model New Shift Days Feature in StructuredDeid Deidentification Module New Multiple Chunks Merge Ability in ChunkMergeApproach New setBlackList Feature in ChunkMergeApproach New setBlackList Feature in NerConverterInternal New setLabelCasing Feature in MedicalNerModel New Update Models Functionality New and Updated Notebooks New Clinical NER Models We have three new clinical NER models. ner_deid_subentity_augmented_i2b2 : This model annotates text to find protected health information(PHI) that may need to be removed. It is trained with 2014 i2b2 dataset (no augmentation applied) and can detect MEDICALRECORD, ORGANIZATION, DOCTOR, USERNAME, PROFESSION, HEALTHPLAN, URL, CITY, DATE, LOCATION-OTHER, STATE, PATIENT, DEVICE, COUNTRY, ZIP, PHONE, HOSPITAL, EMAIL, IDNUM, SREET, BIOID, FAX, AGE entities. Example : ... deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented_i2b2&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = ner_model.transform(spark.createDataFrame([[&quot;A. Record date : 2093-01-13, David Hale, M.D., Name : Hendrickson, Ora MR. # 7194334 Date : 01/13/93 PCP : Oliveira, 25 years old, Record date : 1-11-2000. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227. Patient&#39;s complaints first surfaced when he started working for Brothers Coal-Mine.&quot;]], [&quot;text&quot;])) Results : +--+-+ |chunk |ner_label | +--+-+ |2093-01-13 |DATE | |David Hale |DOCTOR | |Hendrickson, Ora |PATIENT | |7194334 |MEDICALRECORD| |01/13/93 |DATE | |Oliveira |DOCTOR | |25 |AGE | |1-11-2000 |DATE | |Cocke County Baptist Hospital|HOSPITAL | |0295 Keats Street |STREET | |(302) 786-5227 |PHONE | |Brothers Coal-Mine Corp |ORGANIZATION | +--+-+ ner_biomarker : This model is trained to extract biomarkers, therapies, oncological, and other general concepts from text. Following are the entities it can detect: Oncogenes, Tumor_Finding, UnspecificTherapy, Ethnicity, Age, ResponseToTreatment, Biomarker, HormonalTherapy, Staging, Drug, CancerDx, Radiotherapy, CancerSurgery, TargetedTherapy, PerformanceStatus, CancerModifier, Radiological_Test_Result, Biomarker_Measurement, Metastasis, Radiological_Test, Chemotherapy, Test, Dosage, Test_Result, Immunotherapy, Date, Gender, Prognostic_Biomarkers, Duration, Predictive_Biomarkers Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_biomarker&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = ner_model.transform(spark.createDataFrame([[&quot;Here , we report the first case of an intraductal tubulopapillary neoplasm of the pancreas with clear cell morphology . Immunohistochemistry revealed positivity for Pan-CK , CK7 , CK8/18 , MUC1 , MUC6 , carbonic anhydrase IX , CD10 , EMA , β-catenin and e-cadherin .&quot;]], [&quot;text&quot;])) Results : | | ner_chunk | entity | confidence | |:|:-|:-|-:| | 0 | intraductal | CancerModifier | 0.9934 | | 1 | tubulopapillary | CancerModifier | 0.6403 | | 2 | neoplasm of the pancreas | CancerDx | 0.758825 | | 3 | clear cell | CancerModifier | 0.9633 | | 4 | Immunohistochemistry | Test | 0.9534 | | 5 | positivity | Biomarker_Measurement | 0.8795 | | 6 | Pan-CK | Biomarker | 0.9975 | | 7 | CK7 | Biomarker | 0.9975 | | 8 | CK8/18 | Biomarker | 0.9987 | | 9 | MUC1 | Biomarker | 0.9967 | | 10 | MUC6 | Biomarker | 0.9972 | | 11 | carbonic anhydrase IX | Biomarker | 0.937567 | | 12 | CD10 | Biomarker | 0.9974 | | 13 | EMA | Biomarker | 0.9899 | | 14 | β-catenin | Biomarker | 0.8059 | | 15 | e-cadherin | Biomarker | 0.9806 | ner_nihss : NER model that can identify entities according to NIHSS guidelines for clinical stroke assessment to evaluate neurological status in acute stroke patients. Here are the labels it can detect : 11_ExtinctionInattention, 6b_RightLeg, 1c_LOCCommands, 10_Dysarthria, NIHSS, 5_Motor, 8_Sensory, 4_FacialPalsy, 6_Motor, 2_BestGaze, Measurement, 6a_LeftLeg, 5b_RightArm, 5a_LeftArm, 1b_LOCQuestions, 3_Visual, 9_BestLanguage, 7_LimbAtaxia, 1a_LOC . Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_nihss&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = ner_model.transform(spark.createDataFrame([[&quot;Abdomen , soft , nontender . NIH stroke scale on presentation was 23 to 24 for , one for consciousness , two for month and year and two for eye / grip , one to two for gaze , two for face , eight for motor , one for limited ataxia , one to two for sensory , three for best language and two for attention . On the neurologic examination the patient was intermittently&quot;]], [&quot;text&quot;])) Results : | | chunk | entity | |:|:-|:-| | 0 | NIH stroke scale | NIHSS | | 1 | 23 to 24 | Measurement | | 2 | one | Measurement | | 3 | consciousness | 1a_LOC | | 4 | two | Measurement | | 5 | month and year and | 1b_LOCQuestions | | 6 | two | Measurement | | 7 | eye / grip | 1c_LOCCommands | | 8 | one to | Measurement | | 9 | two | Measurement | | 10 | gaze | 2_BestGaze | | 11 | two | Measurement | | 12 | face | 4_FacialPalsy | | 13 | eight | Measurement | | 14 | one | Measurement | | 15 | limited | 7_LimbAtaxia | | 16 | ataxia | 7_LimbAtaxia | | 17 | one to two | Measurement | | 18 | sensory | 8_Sensory | | 19 | three | Measurement | | 20 | best language | 9_BestLanguage | | 21 | two | Measurement | | 22 | attention | 11_ExtinctionInattention | New NER Model Finder Pretrained Pipeline We are releasing new ner_model_finder pretrained pipeline trained with bert embeddings that can be used to find the most appropriate NER model given the entity name. Example : from sparknlp.pretrained import PretrainedPipeline finder_pipeline = PretrainedPipeline(&quot;ner_model_finder&quot;, &quot;en&quot;, &quot;clinical/models&quot;) result = finder_pipeline.fullAnnotate(&quot;psychology&quot;) Results : entity top models all models resolutions psychology [‘ner_medmentions_coarse’, ‘jsl_rd_ner_wip_greedy_clinical’, ‘ner_jsl_enriched’, ‘ner_jsl’, ‘jsl_ner_wip_modifier_clinical’, ‘ner_jsl_greedy’] [‘ner_medmentions_coarse’, ‘jsl_rd_ner_wip_greedy_clinical’, ‘ner_jsl_enriched’, ‘ner_jsl’, ‘jsl_ner_wip_modifier_clinical’, ‘ner_jsl_greedy’]:::[‘jsl_rd_ner_wip_greedy_clinical’, ‘ner_jsl_enriched’, ‘ner_jsl_slim’, ‘ner_jsl’, ‘jsl_ner_wip_modifier_clinical,… psychological condition:::clinical department::: … New Relation Extraction Model We are releasing new redl_nihss_biobert relation extraction model that can relate scale items and their measurements according to NIHSS guidelines. Example : ... re_model = RelationExtractionDLModel() .pretrained(&#39;redl_nihss_biobert&#39;, &#39;en&#39;, &quot;clinical/models&quot;) .setPredictionThreshold(0.5) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) ... sample_text = &quot;There , her initial NIHSS score was 4 , as recorded by the ED physicians . This included 2 for weakness in her left leg and 2 for what they felt was subtle ataxia in her left arm and leg .&quot; result = re_model.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : | chunk1 | entity1 | entity1_begin | entity1_end | entity2 | chunk2 | entity2_begin | entity2_end | relation | |:--|:-|-:|--:|:|:|-:|--:|:--| | initial NIHSS score | NIHSS | 12 | 30 | Measurement | 4 | 36 | 36 | Has_Value | | left leg | 6a_LeftLeg | 111 | 118 | Measurement | 2 | 89 | 89 | Has_Value | | subtle ataxia in her left arm and leg | 7_LimbAtaxia | 149 | 185 | Measurement | 2 | 124 | 124 | Has_Value | | left leg | 6a_LeftLeg | 111 | 118 | Measurement | 4 | 36 | 36 | 0 | | initial NIHSS score | NIHSS | 12 | 30 | Measurement | 2 | 124 | 124 | 0 | | subtle ataxia in her left arm and leg | 7_LimbAtaxia | 149 | 185 | Measurement | 4 | 36 | 36 | 0 | | subtle ataxia in her left arm and leg | 7_LimbAtaxia | 149 | 185 | Measurement | 2 | 89 | 89 | 0 | New LOINC, MeSH, NDC and SNOMED Entity Resolver Models We have four new Sentence Entity Resolver Models. sbiobertresolve_mesh : This model maps clinical entities to Medical Subject Heading (MeSH) codes using sbiobert_base_cased_mli Sentence Bert Embeddings. Example : ... mesh_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_mesh&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;mesh_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) .setCaseSensitive(False) ... sample_text = &quot;&quot;&quot;She was admitted to the hospital with chest pain and found to have bilateral pleural effusion, the right greater than the left. We reviewed the pathology obtained from the pericardectomy in March 2006, which was diagnostic of mesothelioma. At this time, chest tube placement for drainage of the fluid occurred and thoracoscopy with fluid biopsies, which were performed, which revealed malignant mesothelioma.&quot;&quot;&quot; result = resolver_model.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : +--++-+-+-+-+ | ner_chunk| entity| mesh_code| all_codes| resolutions| distances| +--++-+-+-+-+ | chest pain| PROBLEM| D002637|D002637:::D059350:::D019547:::D020069:::D015746:::D000072716:::D005157:::D059265:::D001416:::D048...|Chest Pain:::Chronic Pain:::Neck Pain:::Shoulder Pain:::Abdominal Pain:::Cancer Pain:::Facial Pai...|0.0000:::0.0577:::0.0587:::0.0601:::0.0658:::0.0704:::0.0712:::0.0741:::0.0766:::0.0778:::0.0794:...| |bilateral pleural effusion| PROBLEM| D010996|D010996:::D010490:::D011654:::D016724:::D010995:::D016066:::D011001:::D007819:::D035422:::D004653...|Pleural Effusion:::Pericardial Effusion:::Pulmonary Edema:::Empyema, Pleural:::Pleural Diseases::...|0.0309:::0.1010:::0.1115:::0.1213:::0.1218:::0.1398:::0.1425:::0.1401:::0.1451:::0.1464:::0.1464:...| | the pathology| TEST| D010336|D010336:::D010335:::D001004:::D020969:::C001675:::C536472:::D004194:::D003951:::D013631:::C535329...|Pathology:::Pathologic Processes:::Anus Diseases:::Disease Attributes:::malformins:::Upington dis...|0.0788:::0.0977:::0.1364:::0.1396:::0.1419:::0.1459:::0.1418:::0.1393:::0.1514:::0.1541:::0.1491:...| | the pericardectomy|TREATMENT| D010492|D010492:::D011670:::D018700:::D020884:::D011672:::D005927:::D064727:::D002431:::C000678968:::D011...|Pericardiectomy:::Pulpectomy:::Pleurodesis:::Colpotomy:::Pulpotomy:::Glossectomy:::Posterior Caps...|0.1098:::0.1448:::0.1801:::0.1852:::0.1871:::0.1923:::0.1901:::0.2023:::0.2075:::0.2010:::0.1996:...| | mesothelioma| PROBLEM|D000086002|D000086002:::C535700:::D009208:::D032902:::D018301:::D018199:::C562740:::C000686536:::D018276:::D...|Mesothelioma, Malignant:::Malignant mesenchymal tumor:::Myoepithelioma:::Ganoderma:::Neoplasms, M...|0.0813:::0.1515:::0.1599:::0.1810:::0.1864:::0.1881:::0.1907:::0.1938:::0.1924:::0.1876:::0.2040:...| | chest tube placement|TREATMENT| D015505|D015505:::D019616:::D013896:::D012124:::D013906:::D013510:::D020708:::D035423:::D013903:::D000066...|Chest Tubes:::Thoracic Surgical Procedures:::Thoracic Diseases:::Respiratory Care Units:::Thoraco...|0.0557:::0.1473:::0.1598:::0.1604:::0.1725:::0.1651:::0.1795:::0.1760:::0.1804:::0.1846:::0.1883:...| | drainage of the fluid|TREATMENT| D004322|D004322:::D018495:::C045413:::D021061:::D045268:::D018508:::D005441:::D015633:::D014906:::D001834...|Drainage:::Fluid Shifts:::Bonain&#39;s liquid:::Liquid Ventilation:::Flowmeters:::Water Purification:...|0.1141:::0.1403:::0.1582:::0.1549:::0.1586:::0.1626:::0.1599:::0.1655:::0.1667:::0.1656:::0.1741:...| | thoracoscopy|TREATMENT| D013906|D013906:::D020708:::D035423:::D013905:::D035441:::D013897:::D001468:::D000069258:::D013909:::D013...|Thoracoscopy:::Thoracoscopes:::Thoracic Cavity:::Thoracoplasty:::Thoracic Wall:::Thoracic Duct:::...|0.0000:::0.0359:::0.0744:::0.1007:::0.1070:::0.1143:::0.1186:::0.1257:::0.1228:::0.1356:::0.1354:...| | fluid biopsies| TEST|D000073890|D000073890:::D010533:::D020420:::D011677:::D017817:::D001706:::D005441:::D005751:::D013582:::D000...|Liquid Biopsy:::Peritoneal Lavage:::Cyst Fluid:::Punctures:::Nasal Lavage Fluid:::Biopsy:::Fluids...|0.1408:::0.1612:::0.1763:::0.1744:::0.1744:::0.1810:::0.1744:::0.1828:::0.1896:::0.1909:::0.1950:...| | malignant mesothelioma| PROBLEM|D000086002|D000086002:::C535700:::C562740:::D009236:::D007890:::D012515:::D009208:::C009823:::C000683999:::C...|Mesothelioma, Malignant:::Malignant mesenchymal tumor:::Hemangiopericytoma, Malignant:::Myxosarco...|0.0737:::0.1106:::0.1658:::0.1627:::0.1660:::0.1639:::0.1728:::0.1676:::0.1791:::0.1843:::0.1849:...| +-+--++-+-+-+-+ sbiobertresolve_ndc : This model maps clinical entities and concepts (like drugs/ingredients) to National Drug Codes using sbiobert_base_cased_mli Sentence Bert Embeddings. Also, if a drug has more than one NDC code, it returns all available codes in the all_k_aux_label column separated by | symbol. Example : ... ndc_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_ndc&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;ndc_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) .setCaseSensitive(False) ... sample_text = &quot;&quot;&quot;The patient was transferred secondary to inability and continue of her diabetes, the sacral decubitus, left foot pressure wound, and associated complications of diabetes. She is given aspirin 81 mg, folic acid 1 g daily, insulin glargine 100 UNT/ML injection and metformin 500 mg p.o. p.r.n.&quot;&quot;&quot; result = resolver_model.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : +-++--++--+--+--+ | ner_chunk|entity| ndc_code| description| all_codes| all_resolutions| other ndc codes| +-++--++--+--+--+ | aspirin 81 mg| DRUG|73089008114| aspirin 81 mg/81mg, 81 mg in 1 carton , capsule|[73089008114, 71872708704, 71872715401, 68210101500, 69536028110, 63548086706, 71679001000, 68196090051, 00113400500, 69536018112, 73089008112, 63981056362, 63739043402, 63548086705, 00113046708, 7...|[aspirin 81 mg/81mg, 81 mg in 1 carton , capsule, aspirin 81 mg 81 mg/1, 4 blister pack in 1 bag , tablet, aspirin 81 mg/1, 1 blister pack in 1 bag , tablet, coated, aspirin 81 mg/1, 1 bag in 1 dru...| [-, -, -, -, -, -, -, -, -, -, -, 63940060962, -, -, -, -, -, -, -, -, 70000042002|00363021879|41250027408|36800046708|59779027408|49035027408|71476010131|81522046708|30142046708, -, -, -, -]| | folic acid 1 g| DRUG|43744015101| folic acid 1 g/g, 1 g in 1 package , powder|[43744015101, 63238340000, 66326050555, 51552041802, 51552041805, 63238340001, 81919000204, 51552041804, 66326050556, 51552106301, 51927003300, 71092997701, 51927296300, 51552146602, 61281900002, 6...|[folic acid 1 g/g, 1 g in 1 package , powder, folic acid 1 kg/kg, 1 kg in 1 bottle , powder, folic acid 1 kg/kg, 1 kg in 1 drum , powder, folic acid 1 g/g, 5 g in 1 container , powder, folic acid 1...| [-, -, -, -, -, -, -, -, -, -, -, 51552139201, -, -, -, 81919000203, -, 81919000201, -, -, -, -, -, -, -]| |insulin glargine 100 UNT/ML injection| DRUG|00088502101|insulin glargine 100 [iu]/ml, 1 vial, glass in 1 package , injection, solution|[00088502101, 00088222033, 49502019580, 00002771563, 00169320111, 00088250033, 70518139000, 00169266211, 50090127600, 50090407400, 00002771559, 00002772899, 70518225200, 70518138800, 00024592410, 0...|[insulin glargine 100 [iu]/ml, 1 vial, glass in 1 package , injection, solution, insulin glargine 100 [iu]/ml, 1 vial, glass in 1 carton , injection, solution, insulin glargine 100 [iu]/ml, 1 vial ...|[-, -, -, 00088221900, -, -, 50090139800|00088502005, -, 70518146200|00169368712, 00169368512|73070020011, 00088221905|49502019675|50090406800, -, 73070010011|00169750111|50090495500, 66733077301|0...| | metformin 500 mg| DRUG|70010006315| metformin hydrochloride 500 mg/500mg, 500 mg in 1 drum , tablet|[70010006315, 62207041613, 71052050750, 62207049147, 71052091050, 25000010197, 25000013498, 25000010198, 71052063005, 51662139201, 70010049118, 70882012456, 71052011005, 71052065905, 71052050850, 1...|[metformin hydrochloride 500 mg/500mg, 500 mg in 1 drum , tablet, metformin hcl 500 mg/kg, 50 kg in 1 drum , powder, 5-fluorouracil 500 g/500g, 500 g in 1 container , powder, metformin er 500 mg 50...| [-, -, -, 70010049105, -, -, -, -, -, -, -, -, -, -, -, 71800000801|42571036007, -, -, -, -, -, -, -, -, -]| +-++--++--+--+--+ sbiobertresolve_loinc_augmented : This model maps extracted clinical NER entities to LOINC codes using sbiobert_base_cased_mli Sentence Bert Embeddings. It is trained on the augmented version of the dataset which is used in previous LOINC resolver models. Example : ... loinc_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_loinc_augmented&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;loinc_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) .setCaseSensitive(False) ... sample_text=&quot;&quot;&quot;The patient is a 22-year-old female with a history of obesity. She has a Body mass index (BMI) of 33.5 kg/m2, aspartate aminotransferase 64, and alanine aminotransferase 126. Her hgba1c is 8.2%.&quot;&quot;&quot; result = resolver_model.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : +--+--+++-+--+--+ | chunk|begin|end|entity|Loinc_Code| all_codes| resolutions| +--+--+++-+--+--+ | Body mass index| 74| 88| Test| LP35925-4|LP35925-4:::BDYCRC:::LP172732-2:::39156-5:::LP7...|body mass index:::body circumference:::body mus...| |aspartate aminotransferase| 111|136| Test| LP15426-7|LP15426-7:::14409-7:::LP307348-5:::LP15333-5:::...|aspartate aminotransferase::: aspartate transam...| | alanine aminotransferase| 146|169| Test| LP15333-5|LP15333-5:::LP307326-1:::16324-6:::LP307348-5::...|alanine aminotransferase:::alanine aminotransfe...| | hgba1c| 180|185| Test| 17855-8|17855-8:::4547-6:::55139-0:::72518-4:::45190-6:...| hba1c::: hgb a1::: hb1::: hcds1::: hhc1::: htr...| +--+--+++-+--+--+ sbiobertresolve_clinical_snomed_procedures_measurements : This model maps medical entities to SNOMED codes using sent_biobert_clinical_base_cased Sentence Bert Embeddings. The corpus of this model includes Procedures and Measurement domains. Example : ... snomed_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_clinical_snomed_procedures_measurements&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) ... light_model = LightPipeline(resolver_model) result = light_model.fullAnnotate([&#39;coronary calcium score&#39;, &#39;heart surgery&#39;, &#39;ct scan&#39;, &#39;bp value&#39;]) Results : | | chunk | code | code_description | all_k_codes | all_k_resolutions | |:|:--|-:|:|:--|:-| | 0 | coronary calcium score | 450360000 | Coronary artery calcium score | [&#39;450360000&#39;, &#39;450734004&#39;, &#39;1086491000000104&#39;, &#39;1086481000000101&#39;, &#39;762241007&#39;] | [&#39;Coronary artery calcium score&#39;, &#39;Coronary artery calcium score&#39;, &#39;Dundee Coronary Risk Disk score&#39;, &#39;Dundee Coronary Risk rank&#39;, &#39;Dundee Coronary Risk Disk&#39;] | | 1 | heart surgery | 2598006 | Open heart surgery | [&#39;2598006&#39;, &#39;64915003&#39;, &#39;119766003&#39;, &#39;34068001&#39;, &#39;233004008&#39;] | [&#39;Open heart surgery&#39;, &#39;Operation on heart&#39;, &#39;Heart reconstruction&#39;, &#39;Heart valve replacement&#39;, &#39;Coronary sinus operation&#39;] | | 2 | ct scan | 303653007 | CT of head | [&#39;303653007&#39;, &#39;431864000&#39;, &#39;363023007&#39;, &#39;418272005&#39;, &#39;241577003&#39;] | [&#39;CT of head&#39;, &#39;CT guided injection&#39;, &#39;CT of site&#39;, &#39;CT angiography&#39;, &#39;CT of spine&#39;] | | 3 | bp value | 75367002 | Blood pressure | [&#39;75367002&#39;, &#39;6797001&#39;, &#39;723232008&#39;, &#39;46973005&#39;, &#39;427732000&#39;] | [&#39;Blood pressure&#39;, &#39;Mean blood pressure&#39;, &#39;Average blood pressure&#39;, &#39;Blood pressure taking&#39;, &#39;Speed of blood pressure response&#39;] | Updated RxNorm Sentence Entity Resolver Model We have updated sbiobertresolve_rxnorm_augmented model training on an augmented version of the dataset used in previous versions of the model. New Shift Days Feature in StructuredDeid Deidentification Module Now we can shift n days in the structured deidentification when the column is a Date. Example : df = spark.createDataFrame([ [&quot;Juan García&quot;, &quot;13/02/1977&quot;, &quot;711 Nulla St.&quot;, &quot;140&quot;, &quot;673 431234&quot;], [&quot;Will Smith&quot;, &quot;23/02/1977&quot;, &quot;1 Green Avenue.&quot;, &quot;140&quot;, &quot;+23 (673) 431234&quot;], [&quot;Pedro Ximénez&quot;, &quot;11/04/1900&quot;, &quot;Calle del Libertador, 7&quot;, &quot;100&quot;, &quot;912 345623&quot;] ]).toDF(&quot;NAME&quot;, &quot;DOB&quot;, &quot;ADDRESS&quot;, &quot;SBP&quot;, &quot;TEL&quot;) obfuscator = StructuredDeidentification(spark=spark, columns={&quot;NAME&quot;: &quot;ID&quot;, &quot;DOB&quot;: &quot;DATE&quot;}, columnsSeed={&quot;NAME&quot;: 23, &quot;DOB&quot;: 23}, obfuscateRefSource=&quot;faker&quot;, days=5 ) result = obfuscator.obfuscateColumns(self.df) result.show(truncate=False) Results : +-++--++-+ |NAME |DOB |ADDRESS |SBP|TEL | +-++--++-+ |[T1825511]|[18/02/1977]|711 Nulla St. |140|673 431234 | |[G6835267]|[28/02/1977]|1 Green Avenue. |140|+23 (673) 431234| |[S2371443]|[16/04/1900]|Calle del Libertador, 7|100|912 345623 | +-++--++-+ New Multiple Chunks Merge Ability in ChunkMergeApproach Updated ChunkMergeApproach to admit N input cols (.setInputCols(&quot;ner_chunk&quot;,&quot;ner_chunk_1&quot;,&quot;ner_chunk_2&quot;)). The input columns must be chunk columns. Example : ... deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_large&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&#39;DATE&#39;, &#39;AGE&#39;, &#39;NAME&#39;, &#39;PROFESSION&#39;, &#39;ID&#39;]) medical_ner = MedicalNerModel.pretrained(&quot;ner_events_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner2&quot;) ner_converter_2 = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner2&quot;]) .setOutputCol(&quot;ner_chunk_2&quot;) ssn_parser = ContextualParserApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;entity_ssn&quot;) .setJsonPath(&quot;../../src/test/resources/ssn.json&quot;) .setCaseSensitive(False) .setContextMatch(False) chunk_merge = ChunkMergeApproach() .setInputCols(&quot;entity_ssn&quot;,&quot;ner_chunk&quot;,&quot;ner_chunk_2&quot;) .setOutputCol(&quot;deid_merged_chunk&quot;) .setChunkPrecedence(&quot;field&quot;) ... New setBlackList Feature in ChunkMergeApproach Now we can filter out the entities in the ChunkMergeApproach using a black list .setBlackList([&quot;NAME&quot;,&quot;ID&quot;]). The entities specified in the blackList will be excluded from the final entity list. Example : chunk_merge = ChunkMergeApproach() .setInputCols(&quot;entity_ssn&quot;,&quot;ner_chunk&quot;) .setOutputCol(&quot;deid_merged_chunk&quot;) .setBlackList([&quot;NAME&quot;,&quot;ID&quot;]) New setBlackList Feature in NerConverterInternal Now we can filter out the entities in the NerConverterInternal using a black list .setBlackList([&quot;Drug&quot;,&quot;Treatment&quot;]). The entities specified in the blackList will be excluded from the final entity list. Example : ner = MedicalNerModel.pretrained(&quot;ner_jsl_slim&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) converter = NerConverterInternal() .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;) .setOutputCol(&quot;entities&quot;) .setBlackList([&quot;Drug&quot;,&quot;Treatment&quot;]) New setLabelCasing Feature in MedicalNerModel Now we can decide if we want to return the tags in upper or lower case with setLabelCasing(). That method convert the I-tags and B-tags in lower or upper case during the inference. The values will be ‘lower’ for lower case and ‘upper’ for upper case. Example : ... ner_tagger = MedicalNerModel() .pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) .setLabelCasing(&quot;lower&quot;) ... results = LightPipeline(pipelineModel).annotate(&quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus &quot;) results[&quot;ner_tags&quot;] Results : [&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-problem&#39;, &#39;I-problem&#39;, &#39;I-problem&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-problem&#39;, &#39;I-problem&#39;, &#39;I-problem&#39;, &#39;I-problem&#39;, &#39;I-problem&#39;] New Update Models Functionality We developed a new utility function called UpdateModels that allows you to refresh your cache_pretrained folder without running any annotator or manually checking. It has two methods; UpdateModels.updateCacheModels() : This method lets you update all the models existing in the cache_pretrained folder. It downloads the latest version of all the models existing in the cache_pretrained. Example : # Models in /cache_pretrained ls ~/cache_pretrained &gt;&gt; ner_clinical_large_en_3.0.0_2.3_1617206114650/ # Update models in /cache_pretrained from sparknlp_jsl.updateModels import UpdateModels UpdateModels.updateCacheModels() Results : # Updated models in /cache_pretrained ls ~/cache_pretrained &gt;&gt; ner_clinical_large_en_3.0.0_2.3_1617206114650/ ner_clinical_large_en_3.0.0_3.0_1617206114650/ UpdateModels.updateModels(&quot;11/24/2021&quot;) : This method lets you download all the new models uploaded to the Models Hub starting from a cut-off date (i.e. the last sync update). Example : # Models in /cache_pretrained ls ~/cache_pretrained &gt;&gt; ner_clinical_large_en_3.0.0_2.3_1617206114650/ ner_clinical_large_en_3.0.0_3.0_1617206114650/ # Update models in /cache_pretrained according to date from sparknlp_jsl.updateModels import UpdateModels UpdateModels.updateModels(&quot;11/24/2021&quot;) Results : # Updated models in /cache_pretrained ls ~/cache_pretrained &gt;&gt;ner_clinical_large_en_3.0.0_2.3_1617206114650/ ner_clinical_large_en_3.0.0_3.0_1617206114650/ ner_model_finder_en_3.3.2_2.4_1637761259895/ sbertresolve_ner_model_finder_en_3.3.2_2.4_1637764208798/ New and Updated Notebooks We have a new Connect to Annotation Lab via API Notebook you can find how to; upload pre-annotations to ALAB import a project form ALAB and convert to CoNLL file upload tasks without pre-annotations We have updated Clinical Relation Extraction Notebook by adding a Relation Extraction Model-NER Model-Relation Pairs table that can be used to get the most optimal results when using these models. To see more, please check : Spark NLP Healthcare Workshop Repo 3.3.2 We are glad to announce that Spark NLP Healthcare 3.3.2 has been released!. Highlights New Clinical NER Models and Spanish NER Model New BERT-Based Clinical NER Models Updated Clinical NER Model New NER Model Class Distribution Feature New RxNorm Sentence Entity Resolver Model New Spanish SNOMED Sentence Entity Resolver Model New Clinical Question vs Statement BertForSequenceClassification model New Sentence Entity Resolver Fine-Tune Features (Overwriting and Drop Code) Updated ICD10CM Entity Resolver Models Updated NER Profiling Pretrained Pipelines New ChunkSentenceSplitter Annotator Updated Spark NLP For Healthcare Notebooks and New Notebooks New Clinical NER Models (including a new Spanish one) We are releasing three new clinical NER models trained by MedicalNerApproach(). roberta_ner_diag_proc : This models leverages Spanish Roberta Biomedical Embeddings (roberta_base_biomedical) to extract two entities, Diagnosis and Procedures (DIAGNOSTICO, PROCEDIMIENTO). It’s a renewed version of ner_diag_proc_es, available here, that was trained with embeddings_scielowiki_300d embeddings instead. Example : ... embeddings = RoBertaEmbeddings.pretrained(&quot;roberta_base_biomedical&quot;, &quot;es&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner = MedicalNerModel.pretrained(&quot;roberta_ner_diag_proc&quot;, &quot;es&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&#39;sentence&#39;, &#39;token&#39;, &#39;ner&#39;]) .setOutputCol(&#39;ner_chunk&#39;) pipeline = Pipeline(stages = [ documentAssembler, sentenceDetector, tokenizer, embeddings, ner, ner_converter]) empty = spark.createDataFrame([[&#39;&#39;]]).toDF(&quot;text&quot;) p_model = pipeline.fit(empty) test_sentence = &#39;Mujer de 28 años con antecedentes de diabetes mellitus gestacional diagnosticada ocho años antes de la presentación y posterior diabetes mellitus tipo dos (DM2), un episodio previo de pancreatitis inducida por HTG tres años antes de la presentación, asociado con una hepatitis aguda, y obesidad con un índice de masa corporal (IMC) de 33,5 kg / m2, que se presentó con antecedentes de una semana de poliuria, polidipsia, falta de apetito y vómitos. Dos semanas antes de la presentación, fue tratada con un ciclo de cinco días de amoxicilina por una infección del tracto respiratorio. Estaba tomando metformina, glipizida y dapagliflozina para la DM2 y atorvastatina y gemfibrozil para la HTG. Había estado tomando dapagliflozina durante seis meses en el momento de la presentación. El examen físico al momento de la presentación fue significativo para la mucosa oral seca; significativamente, su examen abdominal fue benigno sin dolor a la palpación, protección o rigidez. Los hallazgos de laboratorio pertinentes al ingreso fueron: glucosa sérica 111 mg / dl, bicarbonato 18 mmol / l, anión gap 20, creatinina 0,4 mg / dl, triglicéridos 508 mg / dl, colesterol total 122 mg / dl, hemoglobina glucosilada (HbA1c) 10%. y pH venoso 7,27. La lipasa sérica fue normal a 43 U / L. Los niveles séricos de acetona no pudieron evaluarse ya que las muestras de sangre se mantuvieron hemolizadas debido a una lipemia significativa. La paciente ingresó inicialmente por cetosis por inanición, ya que refirió una ingesta oral deficiente durante los tres días previos a la admisión. Sin embargo, la química sérica obtenida seis horas después de la presentación reveló que su glucosa era de 186 mg / dL, la brecha aniónica todavía estaba elevada a 21, el bicarbonato sérico era de 16 mmol / L, el nivel de triglicéridos alcanzó un máximo de 2050 mg / dL y la lipasa fue de 52 U / L. Se obtuvo el nivel de β-hidroxibutirato y se encontró que estaba elevado a 5,29 mmol / L; la muestra original se centrifugó y la capa de quilomicrones se eliminó antes del análisis debido a la interferencia de la turbidez causada por la lipemia nuevamente. El paciente fue tratado con un goteo de insulina para euDKA y HTG con una reducción de la brecha aniónica a 13 y triglicéridos a 1400 mg / dL, dentro de las 24 horas. Se pensó que su euDKA fue precipitada por su infección del tracto respiratorio en el contexto del uso del inhibidor de SGLT2. La paciente fue atendida por el servicio de endocrinología y fue dada de alta con 40 unidades de insulina glargina por la noche, 12 unidades de insulina lispro con las comidas y metformina 1000 mg dos veces al día. Se determinó que todos los inhibidores de SGLT2 deben suspenderse indefinidamente. Tuvo un seguimiento estrecho con endocrinología post alta.&#39; res = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +++ | text|ner_label | +++ | diabetes mellitus gestacional|DIAGNOSTICO| | diabetes mellitus tipo dos|DIAGNOSTICO| | DM2|DIAGNOSTICO| | pancreatitis inducida por HTG|DIAGNOSTICO| | hepatitis aguda|DIAGNOSTICO| | obesidad|DIAGNOSTICO| | índice de masa corporal|DIAGNOSTICO| | IMC|DIAGNOSTICO| | poliuria|DIAGNOSTICO| | polidipsia|DIAGNOSTICO| | vómitos|DIAGNOSTICO| |infección del tracto respiratorio|DIAGNOSTICO| | DM2|DIAGNOSTICO| | HTG|DIAGNOSTICO| | dolor|DIAGNOSTICO| | rigidez|DIAGNOSTICO| | cetosis|DIAGNOSTICO| |infección del tracto respiratorio|DIAGNOSTICO| ++--+ ner_covid_trials : This model is trained to extract covid-specific medical entities in clinical trials. It supports the following entities ranging from virus type to trial design: Stage, Severity, Virus, Trial_Design, Trial_Phase, N_Patients, Institution, Statistical_Indicator, Section_Header, Cell_Type, Cellular_component, Viral_components, Physiological_reaction, Biological_molecules, Admission_Discharge, Age, BMI, Cerebrovascular_Disease, Date, Death_Entity, Diabetes, Disease_Syndrome_Disorder, Dosage, Drug_Ingredient, Employment, Frequency, Gender, Heart_Disease, Hypertension, Obesity, Pulse, Race_Ethnicity, Respiration, Route, Smoking, Time, Total_Cholesterol, Treatment, VS_Finding, Vaccine . Example : ... covid_ner = MedicalNerModel.pretrained(&#39;ner_covid_trials&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = covid_model.transform(spark.createDataFrame(pd.DataFrame({&quot;text&quot;: [&quot;&quot;&quot;In December 2019 , a group of patients with the acute respiratory disease was detected in Wuhan , Hubei Province of China . A month later , a new beta-coronavirus was identified as the cause of the 2019 coronavirus infection . SARS-CoV-2 is a coronavirus that belongs to the group of β-coronaviruses of the subgenus Coronaviridae . The SARS-CoV-2 is the third known zoonotic coronavirus disease after severe acute respiratory syndrome ( SARS ) and Middle Eastern respiratory syndrome ( MERS ). The diagnosis of SARS-CoV-2 recommended by the WHO , CDC is the collection of a sample from the upper respiratory tract ( nasal and oropharyngeal exudate ) or from the lower respiratory tract such as expectoration of endotracheal aspirate and bronchioloalveolar lavage and its analysis using the test of real-time polymerase chain reaction ( qRT-PCR ).&quot;&quot;&quot;]}))) Results : | | chunk | begin | end | entity | |:|:|--:|:|:--| | 0 | December 2019 | 3 | 15 | Date | | 1 | acute respiratory disease | 48 | 72 | Disease_Syndrome_Disorder | | 2 | beta-coronavirus | 146 | 161 | Virus | | 3 | 2019 coronavirus infection | 198 | 223 | Disease_Syndrome_Disorder | | 4 | SARS-CoV-2 | 227 | 236 | Virus | | 5 | coronavirus | 243 | 253 | Virus | | 6 | β-coronaviruses | 284 | 298 | Virus | | 7 | subgenus Coronaviridae | 307 | 328 | Virus | | 8 | SARS-CoV-2 | 336 | 345 | Virus | | 9 | zoonotic coronavirus disease | 366 | 393 | Disease_Syndrome_Disorder | | 10 | severe acute respiratory syndrome | 401 | 433 | Disease_Syndrome_Disorder | | 11 | SARS | 437 | 440 | Disease_Syndrome_Disorder | | 12 | Middle Eastern respiratory syndrome | 448 | 482 | Disease_Syndrome_Disorder | | 13 | MERS | 486 | 489 | Disease_Syndrome_Disorder | | 14 | SARS-CoV-2 | 511 | 520 | Virus | | 15 | WHO | 541 | 543 | Institution | | 16 | CDC | 547 | 549 | Institution | ner_chemd_clinical : This model extract the names of chemical compounds and drugs in medical texts. The entities that can be detected are as follows : SYSTEMATIC, IDENTIFIERS, FORMULA, TRIVIAL, ABBREVIATION, FAMILY, MULTIPLE . For reference click here . Example : ... chemd_ner = MedicalNerModel.pretrained(&#39;ner_chemd&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = chemd_model.transform(spark.createDataFrame(pd.DataFrame({&quot;text&quot;: [&quot;&quot;&quot;Isolation, Structure Elucidation, and Iron-Binding Properties of Lystabactins, Siderophores Isolated from a Marine Pseudoalteromonas sp. The marine bacterium Pseudoalteromonas sp. S2B, isolated from the Gulf of Mexico after the Deepwater Horizon oil spill, was found to produce lystabactins A, B, and C (1-3), three new siderophores. The structures were elucidated through mass spectrometry, amino acid analysis, and NMR. The lystabactins are composed of serine (Ser), asparagine (Asn), two formylated/hydroxylated ornithines (FOHOrn), dihydroxy benzoic acid (Dhb), and a very unusual nonproteinogenic amino acid, 4,8-diamino-3-hydroxyoctanoic acid (LySta). The iron-binding properties of the compounds were investigated through a spectrophotometric competition.&quot;&quot;&quot;]}))) Results : +-++ |chunk |ner_label | +-++ |Lystabactins |FAMILY | |lystabactins A, B, and C |MULTIPLE | |amino acid |FAMILY | |lystabactins |FAMILY | |serine |TRIVIAL | |Ser |FORMULA | |asparagine |TRIVIAL | |Asn |FORMULA | |formylated/hydroxylated ornithines|FAMILY | |FOHOrn |FORMULA | |dihydroxy benzoic acid |SYSTEMATIC | |amino acid |FAMILY | |4,8-diamino-3-hydroxyoctanoic acid|SYSTEMATIC | |LySta |ABBREVIATION| +-++ New BERT-Based Clinical NER Models We have two new BERT-Based token classifier NER models. bert_token_classifier_ner_bionlp : This model is BERT-based version of ner_bionlp model and can detect biological and genetics terms in cancer-related texts. (Amino_acid, Anatomical_system, Cancer, Cell, Cellular_component, Developing_anatomical_Structure, Gene_or_gene_product, Immaterial_anatomical_entity, Multi-tissue_structure, Organ, Organism, Organism_subdivision, Simple_chemical, Tissue) Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_bionlp&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ... test_sentence = &quot;&quot;&quot;Both the erbA IRES and the erbA/myb virus constructs transformed erythroid cells after infection of bone marrow or blastoderm cultures. The erbA/myb IRES virus exhibited a 5-10-fold higher transformed colony forming efficiency than the erbA IRES virus in the blastoderm assay.&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +-+-+ |chunk |ner_label | +-+-+ |erbA IRES |Organism | |erbA/myb virus |Organism | |erythroid cells |Cell | |bone marrow |Multi-tissue_structure| |blastoderm cultures|Cell | |erbA/myb IRES virus|Organism | |erbA IRES virus |Organism | |blastoderm |Cell | +-+-+ bert_token_classifier_ner_cellular : This model is BERT-based version of ner_cellular model and can detect molecular biology-related terms (DNA, Cell_type, Cell_line, RNA, Protein) in medical texts. Metrics : precision recall f1-score support B-DNA 0.87 0.77 0.82 1056 B-RNA 0.85 0.79 0.82 118 B-cell_line 0.66 0.70 0.68 500 B-cell_type 0.87 0.75 0.81 1921 B-protein 0.90 0.85 0.88 5067 I-DNA 0.93 0.86 0.90 1789 I-RNA 0.92 0.84 0.88 187 I-cell_line 0.67 0.76 0.71 989 I-cell_type 0.92 0.76 0.84 2991 I-protein 0.94 0.80 0.87 4774 accuracy 0.80 19392 macro avg 0.76 0.81 0.78 19392 weighted avg 0.89 0.80 0.85 19392 Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_cellular&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ... test_sentence = &quot;&quot;&quot;Detection of various other intracellular signaling proteins is also described. Genetic characterization of transactivation of the human T-cell leukemia virus type 1 promoter: Binding of Tax to Tax-responsive element 1 is mediated by the cyclic AMP-responsive members of the CREB/ATF family of transcription factors. To achieve a better understanding of the mechanism of transactivation by Tax of human T-cell leukemia virus type 1 Tax-responsive element 1 (TRE-1), we developed a genetic approach with Saccharomyces cerevisiae. We constructed a yeast reporter strain containing the lacZ gene under the control of the CYC1 promoter associated with three copies of TRE-1. Expression of either the cyclic AMP response element-binding protein (CREB) or CREB fused to the GAL4 activation domain (GAD) in this strain did not modify the expression of the reporter gene. Tax alone was also inactive.&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +-++ |chunk |ner_label| +-++ |intracellular signaling proteins |protein | |human T-cell leukemia virus type 1 promoter|DNA | |Tax |protein | |Tax-responsive element 1 |DNA | |cyclic AMP-responsive members |protein | |CREB/ATF family |protein | |transcription factors |protein | |Tax |protein | |human T-cell leukemia virus type 1 |DNA | |Tax-responsive element 1 |DNA | |TRE-1 |DNA | |lacZ gene |DNA | |CYC1 promoter |DNA | |TRE-1 |DNA | |cyclic AMP response element-binding protein|protein | |CREB |protein | |CREB |protein | |GAL4 activation domain |protein | |GAD |protein | |reporter gene |DNA | |Tax |protein | +-++ Updated Clinical NER Model We have updated ner_jsl_enriched model by enriching the training data using clinical trials data to make it more robust. This model is capable of predicting up to 87 different entities and is based on ner_jsl model. Here are the entities this model can detect; Social_History_Header, Oncology_Therapy, Blood_Pressure, Respiration, Performance_Status, Family_History_Header, Dosage, Clinical_Dept, Diet, Procedure, HDL, Weight, Admission_Discharge, LDL, Kidney_Disease, Oncological, Route, Imaging_Technique, Puerperium, Overweight, Temperature, Diabetes, Vaccine, Age, Test_Result, Employment, Time, Obesity, EKG_Findings, Pregnancy, Communicable_Disease, BMI, Strength, Tumor_Finding, Section_Header, RelativeDate, ImagingFindings, Death_Entity, Date, Cerebrovascular_Disease, Treatment, Labour_Delivery, Pregnancy_Delivery_Puerperium, Direction, Internal_organ_or_component, Psychological_Condition, Form, Medical_Device, Test, Symptom, Disease_Syndrome_Disorder, Staging, Birth_Entity, Hyperlipidemia, O2_Saturation, Frequency, External_body_part_or_region, Drug_Ingredient, Vital_Signs_Header, Substance_Quantity, Race_Ethnicity, VS_Finding, Injury_or_Poisoning, Medical_History_Header, Alcohol, Triglycerides, Total_Cholesterol, Sexually_Active_or_Sexual_Orientation, Female_Reproductive_Status, Relationship_Status, Drug_BrandName, RelativeTime, Duration, Hypertension, Metastasis, Gender, Oxygen_Therapy, Pulse, Heart_Disease, Modifier, Allergen, Smoking, Substance, Cancer_Modifier, Fetus_NewBorn, Height . Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_jsl_enriched&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = model.transform(spark.createDataFrame([[&quot;The patient is a 21-day-old Caucasian male here for 2 days of congestion - mom has been suctioning yellow discharge from the patient&#39;s nares, plus she has noticed some mild problems with his breathing while feeding (but negative for any perioral cyanosis or retractions). One day ago, mom also noticed a tactile temperature and gave the patient Tylenol. Baby also has had some decreased p.o. intake. His normal breast-feeding is down from 20 minutes q.2h. to 5 to 10 minutes secondary to his respiratory congestion. He sleeps well, but has been more tired and has been fussy over the past 2 days. The parents noticed no improvement with albuterol treatments given in the ER. His urine output has also decreased; normally he has 8 to 10 wet and 5 dirty diapers per 24 hours, now he has down to 4 wet diapers per 24 hours. Mom denies any diarrhea. His bowel movements are yellow colored and soft in nature.&quot;]], [&quot;text&quot;])) Results : | | chunk | begin | end | entity | |:|:|--:|:|:--| | 0 | 21-day-old | 17 | 26 | Age | | 1 | Caucasian | 28 | 36 | Race_Ethnicity | | 2 | male | 38 | 41 | Gender | | 3 | 2 days | 52 | 57 | Duration | | 4 | congestion | 62 | 71 | Symptom | | 5 | mom | 75 | 77 | Gender | | 6 | suctioning yellow discharge | 88 | 114 | Symptom | | 7 | nares | 135 | 139 | External_body_part_or_region | | 8 | she | 147 | 149 | Gender | | 9 | mild | 168 | 171 | Modifier | | 10 | problems with his breathing while feeding | 173 | 213 | Symptom | | 11 | perioral cyanosis | 237 | 253 | Symptom | | 12 | retractions | 258 | 268 | Symptom | | 13 | One day ago | 272 | 282 | RelativeDate | | 14 | mom | 285 | 287 | Gender | | 15 | tactile temperature | 304 | 322 | Symptom | | 16 | Tylenol | 345 | 351 | Drug_BrandName | | 17 | Baby | 354 | 357 | Age | | 18 | decreased p.o. intake | 377 | 397 | Symptom | | 19 | His | 400 | 402 | Gender | | 20 | q.2h | 450 | 453 | Frequency | | 21 | 5 to 10 minutes | 459 | 473 | Duration | | 22 | his | 488 | 490 | Gender | | 23 | respiratory congestion | 492 | 513 | Symptom | | 24 | He | 516 | 517 | Gender | | 25 | tired | 550 | 554 | Symptom | | 26 | fussy | 569 | 573 | Symptom | | 27 | over the past 2 days | 575 | 594 | RelativeDate | | 28 | albuterol | 637 | 645 | Drug_Ingredient | | 29 | ER | 671 | 672 | Clinical_Dept | | 30 | His | 675 | 677 | Gender | | 31 | urine output has also decreased | 679 | 709 | Symptom | | 32 | he | 721 | 722 | Gender | | 33 | per 24 hours | 760 | 771 | Frequency | | 34 | he | 778 | 779 | Gender | | 35 | per 24 hours | 807 | 818 | Frequency | | 36 | Mom | 821 | 823 | Gender | | 37 | diarrhea | 836 | 843 | Symptom | | 38 | His | 846 | 848 | Gender | | 39 | bowel | 850 | 854 | Internal_organ_or_component | New NER Model Class Distribution Feature getTrainingClassDistribution : This parameter returns the distribution of labels used when training the NER model. Example: ner_model.getTrainingClassDistribution() &gt;&gt; {&#39;B-Disease&#39;: 2536, &#39;O&#39;: 31659, &#39;I-Disease&#39;: 2960} New RxNorm Sentence Entity Resolver Model sbiobertresolve_rxnorm_augmented : This model maps clinical entities and concepts (like drugs/ingredients) to RxNorm codes using sbiobert_base_cased_mli Sentence Bert Embeddings. It trained on the augmented version of the dataset which is used in previous RxNorm resolver models. Additionally, this model returns concept classes of the drugs in all_k_aux_labels column. New Spanish SNOMED Sentence Entity Resolver Model robertaresolve_snomed : This models leverages Spanish Roberta Biomedical Embeddings (roberta_base_biomedical) at sentence-level to map ner chunks into Spanish SNOMED codes. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLModel.pretrained() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) word_embeddings = RoBertaEmbeddings.pretrained(&quot;roberta_base_biomedical&quot;, &quot;es&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;roberta_embeddings&quot;) ner = MedicalNerModel.pretrained(&quot;roberta_ner_diag_proc&quot;,&quot;es&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;roberta_embeddings&quot;) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) c2doc = Chunk2Doc() .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;ner_chunk_doc&quot;) chunk_embeddings = SentenceEmbeddings() .setInputCols([&quot;ner_chunk_doc&quot;, &quot;roberta_embeddings&quot;]) .setOutputCol(&quot;chunk_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) er = SentenceEntityResolverModel.pretrained(&quot;robertaresolve_snomed&quot;, &quot;es&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk_doc&quot;, &quot;chunk_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) snomed_training_pipeline = Pipeline(stages = [ documentAssembler, sentenceDetector, tokenizer, word_embeddings, ner, ner_converter, c2doc, chunk_embeddings, er]) empty = spark.createDataFrame([[&#39;&#39;]]).toDF(&quot;text&quot;) p_model = snomed_pipeline .fit(empty) test_sentence = &#39;Mujer de 28 años con antecedentes de diabetes mellitus gestacional diagnosticada ocho años antes de la presentación y posterior diabetes mellitus tipo dos (DM2), un episodio previo de pancreatitis inducida por HTG tres años antes de la presentación, asociado con una hepatitis aguda, y obesidad con un índice de masa corporal (IMC) de 33,5 kg / m2, que se presentó con antecedentes de una semana de poliuria, polidipsia, falta de apetito y vómitos. Dos semanas antes de la presentación, fue tratada con un ciclo de cinco días de amoxicilina por una infección del tracto respiratorio. Estaba tomando metformina, glipizida y dapagliflozina para la DM2 y atorvastatina y gemfibrozil para la HTG. Había estado tomando dapagliflozina durante seis meses en el momento de la presentación. El examen físico al momento de la presentación fue significativo para la mucosa oral seca; significativamente, su examen abdominal fue benigno sin dolor a la palpación, protección o rigidez. Los hallazgos de laboratorio pertinentes al ingreso fueron: glucosa sérica 111 mg / dl, bicarbonato 18 mmol / l, anión gap 20, creatinina 0,4 mg / dl, triglicéridos 508 mg / dl, colesterol total 122 mg / dl, hemoglobina glucosilada (HbA1c) 10%. y pH venoso 7,27. La lipasa sérica fue normal a 43 U / L. Los niveles séricos de acetona no pudieron evaluarse ya que las muestras de sangre se mantuvieron hemolizadas debido a una lipemia significativa. La paciente ingresó inicialmente por cetosis por inanición, ya que refirió una ingesta oral deficiente durante los tres días previos a la admisión. Sin embargo, la química sérica obtenida seis horas después de la presentación reveló que su glucosa era de 186 mg / dL, la brecha aniónica todavía estaba elevada a 21, el bicarbonato sérico era de 16 mmol / L, el nivel de triglicéridos alcanzó un máximo de 2050 mg / dL y la lipasa fue de 52 U / L. Se obtuvo el nivel de β-hidroxibutirato y se encontró que estaba elevado a 5,29 mmol / L; la muestra original se centrifugó y la capa de quilomicrones se eliminó antes del análisis debido a la interferencia de la turbidez causada por la lipemia nuevamente. El paciente fue tratado con un goteo de insulina para euDKA y HTG con una reducción de la brecha aniónica a 13 y triglicéridos a 1400 mg / dL, dentro de las 24 horas. Se pensó que su euDKA fue precipitada por su infección del tracto respiratorio en el contexto del uso del inhibidor de SGLT2. La paciente fue atendida por el servicio de endocrinología y fue dada de alta con 40 unidades de insulina glargina por la noche, 12 unidades de insulina lispro con las comidas y metformina 1000 mg dos veces al día. Se determinó que todos los inhibidores de SGLT2 deben suspenderse indefinidamente. Tuvo un seguimiento estrecho con endocrinología post alta.&#39; res = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +-+-+-+--+ | | ner_chunk | entity | snomed_code| |-+-+-+--| | 0 | diabetes mellitus gestacional | DIAGNOSTICO | 11687002 | | 1 | diabetes mellitus tipo dos ( | DIAGNOSTICO | 44054006 | | 2 | pancreatitis | DIAGNOSTICO | 75694006 | | 3 | HTG | DIAGNOSTICO | 266569009 | | 4 | hepatitis aguda | DIAGNOSTICO | 37871000 | | 5 | obesidad | DIAGNOSTICO | 5476005 | | 6 | índice de masa corporal | DIAGNOSTICO | 162859006 | | 7 | poliuria | DIAGNOSTICO | 56574000 | | 8 | polidipsia | DIAGNOSTICO | 17173007 | | 9 | falta de apetito | DIAGNOSTICO | 49233005 | | 10 | vómitos | DIAGNOSTICO | 422400008 | | 11 | infección | DIAGNOSTICO | 40733004 | | 12 | HTG | DIAGNOSTICO | 266569009 | | 13 | dolor | DIAGNOSTICO | 22253000 | | 14 | rigidez | DIAGNOSTICO | 271587009 | | 15 | cetosis | DIAGNOSTICO | 2538008 | | 16 | infección | DIAGNOSTICO | 40733004 | +-+-+-+--+ New Clinical Question vs Statement BertForSequenceClassification model bert_sequence_classifier_question_statement_clinical : This model classifies sentences into one of these two classes: question (interrogative sentence) or statement (declarative sentence) and trained with BertForSequenceClassification. This model is at first trained on SQuAD and SPAADIA dataset and then fine tuned on the clinical visit documents and MIMIC-III dataset annotated in-house. Using this model, you can find the question statements and exclude &amp; utilize in the downstream tasks such as NER and relation extraction models. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLModel.pretrained() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) seq = BertForSequenceClassification.pretrained(&#39;bert_sequence_classifier_question_statement_clinical&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;label&quot;) .setCaseSensitive(True) pipeline = Pipeline(stages = [ documentAssembler, sentenceDetector, tokenizer, seq]) test_sentences = [&quot;&quot;&quot;Hello I am going to be having a baby throughand have just received my medical results before I have my tubes tested. I had the tests on day 23 of my cycle. My progresterone level is 10. What does this mean? What does progesterone level of 10 indicate? Your progesterone report is perfectly normal. We expect this result on day 23rd of the cycle.So there&#39;s nothing to worry as it&#39;s perfectly alright&quot;&quot;&quot;] res = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: test_sentences}))) Results : +--++ |sentence |label | +--++ |Hello I am going to be having a baby throughand have just received my medical results before I have my tubes tested.|statement| |I had the tests on day 23 of my cycle. |statement| |My progresterone level is 10. |statement| |What does this mean? |question | |What does progesterone level of 10 indicate? |question | |Your progesterone report is perfectly normal. We expect this result on day 23rd of the cycle. |statement| |So there&#39;s nothing to worry as it&#39;s perfectly alright |statement| +--+ Metrics : precision recall f1-score support question 0.97 0.94 0.96 243 statement 0.98 0.99 0.99 729 accuracy 0.98 972 macro avg 0.98 0.97 0.97 972 weighted avg 0.98 0.98 0.98 972 New Sentence Entity Resolver Fine-Tune Features (Overwriting and Drop Code) .setOverwriteExistingCode() : This parameter provides overwriting codes over the existing codes if in pretrained Sentence Entity Resolver Model. For example, you want to add a new term to a pretrained resolver model, and if the code of term already exists in the pretrained model, when you .setOverwriteExistingCode(True), it removes all the same codes and their descriptions from the model, then you will have just the new term with its code in the fine-tuned model. .setDropCodesList() : This parameter drops list of codes from a pretrained Sentence Entity Resolver Model. For more examples, please check Fine-Tuning Sentence Entity Resolver Notebook Updated ICD10CM Entity Resolver Models We have updated sbiobertresolve_icd10cm_augmented model with ICD10CM 2022 Dataset and sbiobertresolve_icd10cm_augmented_billable_hcc model by dropping invalid codes. Updated NER Profiling Pretrained Pipelines We have updated ner_profiling_clinical and ner_profiling_biobert pretrained pipelines by adding new clinical NER models and NER model outputs to the previous versions. In this way, you can see all the NER labels of tokens. For examples, please check NER Profiling Pretrained Pipeline Notebook. New ChunkSentenceSplitter Annotator We are releasing ChunkSentenceSplitter annotator that splits documents or sentences by chunks provided. Splitted parts can be named with the splitting chunks. By using this annotator, you can do some some tasks like splitting clinical documents according into sections in accordance with CDA (Clinical Document Architecture). Example : ... ner_converter = NerConverter() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&quot;Header&quot;]) chunkSentenceSplitter = ChunkSentenceSplitter() .setInputCols(&quot;ner_chunk&quot;,&quot;document&quot;) .setOutputCol(&quot;paragraphs&quot;) .setGroupBySentences(True) .setDefaultEntity(&quot;Intro&quot;) .setInsertChunk(False) ... text = [&quot;&quot;&quot;INTRODUCTION: Right pleural effusion and suspected malignant mesothelioma. PREOPERATIVE DIAGNOSIS: Right pleural effusion and suspected malignant mesothelioma. POSTOPERATIVE DIAGNOSIS: Right pleural effusion, suspected malignant mesothelioma. PROCEDURE: Right VATS pleurodesis and pleural biopsy.&quot;&quot;&quot;] results = pipeline_model.transform(df) Results : +-++ | result|entity| +-++ |INTRODUCTION: Right pleural effusion and suspected malignant mesoth...|Header| |PREOPERATIVE DIAGNOSIS: Right pleural effusion and suspected malig...|Header| |POSTOPERATIVE DIAGNOSIS: Right pleural effusion, suspected malignan...|Header| | PROCEDURE: Right VATS pleurodesis and pleural biopsy|Header| +-++ By using .setInsertChunk() parameter you can remove the chunk from splitted parts. Example : chunkSentenceSplitter = ChunkSentenceSplitter() .setInputCols(&quot;ner_chunk&quot;,&quot;document&quot;) .setOutputCol(&quot;paragraphs&quot;) .setGroupBySentences(True) .setDefaultEntity(&quot;Intro&quot;) .setInsertChunk(False) paragraphs = chunkSentenceSplitter.transform(results) df = paragraphs.selectExpr(&quot;explode(paragraphs) as result&quot;) .selectExpr(&quot;result.result&quot;, &quot;result.metadata.entity&quot;, &quot;result.metadata.splitter_chunk&quot;) Results : +--+++ | result|entity| splitter_chunk| +--+++ | Right pleural effusion and suspected malignant...|Header| INTRODUCTION:| | Right pleural effusion and suspected malignan...|Header| PREOPERATIVE DIAGNOSIS:| | Right pleural effusion, suspected malignant me...|Header|POSTOPERATIVE DIAGNOSIS:| | Right VATS pleurodesis and pleural biopsy|Header| PROCEDURE:| +--+++ Updated Spark NLP For Healthcare Notebooks NER Profiling Pretrained Pipeline Notebook . Fine-Tuning Sentence Entity Resolver Notebook To see more, please check : Spark NLP Healthcare Workshop Repo 3.3.1 We are glad to announce that Spark NLP Healthcare 3.3.1 has been released!. Highlights New ChunkKeyPhraseExtraction Annotator New BERT-Based NER Models New UMLS Sentence Entity Resolver Models Updated RxNorm Entity Resolver Model (Dropping Invalid Codes) New showVersion() Method in Compatibility Class New Docker Images for Spark NLP for Healthcare and Spark OCR New and Updated Deidentification() Parameters New Python API Documentation Updated Spark NLP For Healthcare Notebooks and New Notebooks New ChunkKeyPhraseExtraction Annotator We are releasing ChunkKeyPhraseExtraction annotator that leverages Sentence BERT embeddings to select keywords and key phrases that are most similar to a document. This annotator can be fed by either the output of NER model, NGramGenerator or YAKE, and could be used to generate similarity scores for each NER chunk that is coming out of any (clinical) NER model. That is, you can now sort your clinical entities by the importance of them with respect to document or sentence that they live in. Additionally, you can also use this new annotator to grab new clinical chunks that are missed by a pretrained NER model as well as summarizing the whole document into a few important sentences or phrases. You can find more examples in ChunkKeyPhraseExtraction notebook Example : ... ngram_ner_key_phrase_extractor = ChunkKeyPhraseExtraction.pretrained(&quot;sbert_jsl_medium_uncased &quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setTopN(5) .setDivergence(0.4) .setInputCols([&quot;sentences&quot;, &quot;merged_chunks&quot;]) .setOutputCol(&quot;key_phrases&quot;) ... text = &quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting . Two weeks prior to presentation, she was treated with a five-day course of amoxicillin for a respiratory tract infection. She was on metformin , glipizide , and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG. She had been on dapagliflozin for six months at the time of presentation . Physical examination on presentation was significant for dry oral mucosa ; significantly, her abdominal examination was benign with no tenderness , guarding , or rigidity . Pertinent laboratory findings on admission were: serum glucose 111 mg/dl , bicarbonate 18 mmol/l , anion gap 20 , creatinine 0.4 mg/dL , triglycerides 508 mg/dL , total cholesterol 122 mg/dL , glycated hemoglobin ( HbA1c ) 10% , and venous pH 7.27. Serum lipase was normal at 43 U/L . Serum acetone levels could not be assessed as blood samples kept hemolyzing due to significant lipemia .&quot; textDF = spark.createDataFrame([[text]]).toDF(&quot;text&quot;) ngram_ner_results = ngram_ner_pipeline.transform(textDF) Results : +--++-+-+--+ |key_phrase |source|DocumentSimilarity |MMRScore |sentence| +--++-+-+--+ |type two diabetes mellitus|NER |0.7639750686118073 |0.4583850593816694 |0 | |HTG-induced pancreatitis |ngrams|0.66933222897749 |0.10416352343367463|0 | |vomiting |ngrams|0.5824238088130589 |0.14864183399720493|0 | |history polyuria |ngrams|0.46337313737310987|0.0959500325843913 |0 | |28-year-old female |ngrams|0.31692529374916967|0.10043002919664669|0 | +--++-+-+--+ New BERT-Based NER Models We have two new BERT-Based token classifier NER models. bert_token_classifier_ner_chemicals : This model is BERT-based version of ner_chemicals model and can detect chemical compounds (CHEM) in the medical texts. Metrics : precision recall f1-score support B-CHEM 0.94 0.92 0.93 30731 I-CHEM 0.95 0.93 0.94 31270 accuracy 0.99 62001 macro avg 0.96 0.95 0.96 62001 weighted avg 0.99 0.93 0.96 62001 Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_chemicals&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ... test_sentence = &quot;&quot;&quot;The results have shown that the product p - choloroaniline is not a significant factor in chlorhexidine - digluconate associated erosive cystitis. A high percentage of kanamycin - colistin and povidone - iodine irrigations were associated with erosive cystitis.&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame([[test_sentence]]).toDF(&quot;text&quot;)) Results : +++ |chunk |ner_label| +++ |p - choloroaniline |CHEM | |chlorhexidine - digluconate|CHEM | |kanamycin |CHEM | |colistin |CHEM | |povidone - iodine |CHEM | +++ bert_token_classifier_ner_chemprot : This model is BERT-based version of ner_chemprot_clinical model and can detect chemical compounds and genes (CHEMICAL, GENE-Y, GENE-N) in the medical texts. Metrics : precision recall f1-score support B-CHEMICAL 0.80 0.79 0.80 8649 B-GENE-N 0.53 0.56 0.54 2752 B-GENE-Y 0.71 0.73 0.72 5490 I-CHEMICAL 0.82 0.79 0.81 1313 I-GENE-N 0.62 0.62 0.62 1993 I-GENE-Y 0.75 0.72 0.74 2420 accuracy 0.96 22617 macro avg 0.75 0.74 0.75 22617 weighted avg 0.83 0.73 0.78 22617 Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_chemprot&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ... test_sentence = &quot;Keratinocyte growth factor and acidic fibroblast growth factor are mitogens for primary cultures of mammary epithelium.&quot; result = p_model.transform(spark.createDataFrame([[test_sentence]]).toDF(&quot;text&quot;)) Results : +-++ |chunk |ner_label| +-++ |Keratinocyte growth factor |GENE-Y | |acidic fibroblast growth factor|GENE-Y | +-++ New UMLS Sentence Entity Resolver Models We are releasing two new UMLS Sentence Entity Resolver models trained on 2021AB UMLS dataset and map clinical entities to UMLS CUI codes. sbiobertresolve_umls_disease_syndrome : This model is trained on the Disease or Syndrome category using sbiobert_base_cased_mli embeddings. Example : ... resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_umls_disease_syndrome&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) ... data = spark.createDataFrame([[&quot;&quot;&quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus (T2DM), one prior episode of HTG-induced pancreatitis three years prior to presentation, associated with an acute hepatitis, and obesity with a body mass index (BMI) of 33.5 kg/m2, presented with a one-week history of polyuria, polydipsia, poor appetite, and vomiting.&quot;&quot;&quot;]]).toDF(&quot;text&quot;) results = model.fit(data).transform(data) Results : | | chunk | code | code_description | all_k_codes | all_k_codes_desc | |:|:--|:|:--|:-|:| | 0 | gestational diabetes mellitus | C0085207 | gestational diabetes mellitus | [&#39;C0085207&#39;, &#39;C0032969&#39;, &#39;C2063017&#39;, &#39;C1283034&#39;, &#39;C0271663&#39;] | [&#39;gestational diabetes mellitus&#39;, &#39;pregnancy diabetes mellitus&#39;, &#39;pregnancy complicated by diabetes mellitus&#39;, &#39;maternal diabetes mellitus&#39;, &#39;gestational diabetes mellitus, a2&#39;] | | 1 | subsequent type two diabetes mellitus | C0348921 | pre-existing type 2 diabetes mellitus | [&#39;C0348921&#39;, &#39;C1719939&#39;, &#39;C0011860&#39;, &#39;C0877302&#39;, &#39;C0271640&#39;] | [&#39;pre-existing type 2 diabetes mellitus&#39;, &#39;disorder associated with type 2 diabetes mellitus&#39;, &#39;diabetes mellitus, type 2&#39;, &#39;insulin-requiring type 2 diabetes mellitus&#39;, &#39;secondary diabetes mellitus&#39;] | | 2 | HTG-induced pancreatitis | C0376670 | alcohol-induced pancreatitis | [&#39;C0376670&#39;, &#39;C1868971&#39;, &#39;C4302243&#39;, &#39;C0267940&#39;, &#39;C2350449&#39;] | [&#39;alcohol-induced pancreatitis&#39;, &#39;toxic pancreatitis&#39;, &#39;igg4-related pancreatitis&#39;, &#39;hemorrhage pancreatitis&#39;, &#39;graft pancreatitis&#39;] | | 3 | an acute hepatitis | C0019159 | acute hepatitis | [&#39;C0019159&#39;, &#39;C0276434&#39;, &#39;C0267797&#39;, &#39;C1386146&#39;, &#39;C2063407&#39;] | [&#39;acute hepatitis a&#39;, &#39;acute hepatitis a&#39;, &#39;acute hepatitis&#39;, &#39;acute infectious hepatitis&#39;, &#39;acute hepatitis e&#39;] | | 4 | obesity | C0028754 | obesity | [&#39;C0028754&#39;, &#39;C0342940&#39;, &#39;C0342942&#39;, &#39;C0857116&#39;, &#39;C1561826&#39;] | [&#39;obesity&#39;, &#39;abdominal obesity&#39;, &#39;generalized obesity&#39;, &#39;obesity gross&#39;, &#39;overweight and obesity&#39;] | | 5 | polyuria | C0018965 | hematuria | [&#39;C0018965&#39;, &#39;C0151582&#39;, &#39;C3888890&#39;, &#39;C0268556&#39;, &#39;C2936921&#39;] | [&#39;hematuria&#39;, &#39;uricosuria&#39;, &#39;polyuria-polydipsia syndrome&#39;, &#39;saccharopinuria&#39;, &#39;saccharopinuria&#39;] | | 6 | polydipsia | C0268813 | primary polydipsia | [&#39;C0268813&#39;, &#39;C0030508&#39;, &#39;C3888890&#39;, &#39;C0393777&#39;, &#39;C0206085&#39;] | [&#39;primary polydipsia&#39;, &#39;parasomnia&#39;, &#39;polyuria-polydipsia syndrome&#39;, &#39;hypnogenic paroxysmal dystonias&#39;, &#39;periodic hypersomnias&#39;] | | 7 | poor appetite | C0003123 | lack of appetite | [&#39;C0003123&#39;, &#39;C0011168&#39;, &#39;C0162429&#39;, &#39;C1282895&#39;, &#39;C0039338&#39;] | [&#39;lack of appetite&#39;, &#39;poor swallowing&#39;, &#39;poor nutrition&#39;, &#39;neurologic unpleasant taste&#39;, &#39;taste dis&#39;] | | 8 | vomiting | C0152164 | periodic vomiting | [&#39;C0152164&#39;, &#39;C0267172&#39;, &#39;C0152517&#39;, &#39;C0011119&#39;, &#39;C0152227&#39;] | [&#39;periodic vomiting&#39;, &#39;habit vomiting&#39;, &#39;viral vomiting&#39;, &#39;choking&#39;, &#39;tearing&#39;] | sbiobertresolve_umls_clinical_drugs : This model is trained on the Clinical Drug category using sbiobert_base_cased_mli embeddings. Example : ... resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_umls_clinical_drugs&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) ... data = spark.createDataFrame([[&quot;&quot;&quot;She was immediately given hydrogen peroxide 30 mg to treat the infection on her leg, and has been advised Neosporin Cream for 5 days. She has a history of taking magnesium hydroxide 100mg/1ml and metformin 1000 mg.&quot;&quot;&quot;]]).toDF(&quot;text&quot;) results = model.fit(data).transform(data) Results : | | chunk | code | code_description | all_k_codes | all_k_codes_desc | |:|:|:|:|:-|:-| | 0 | hydrogen peroxide 30 mg | C1126248 | hydrogen peroxide 30 mg/ml | [&#39;C1126248&#39;, &#39;C0304655&#39;, &#39;C1605252&#39;, &#39;C0304656&#39;, &#39;C1154260&#39;] | [&#39;hydrogen peroxide 30 mg/ml&#39;, &#39;hydrogen peroxide solution 30%&#39;, &#39;hydrogen peroxide 30 mg/ml [proxacol]&#39;, &#39;hydrogen peroxide 30 mg/ml cutaneous solution&#39;, &#39;benzoyl peroxide 30 mg/ml&#39;] | | 1 | Neosporin Cream | C0132149 | neosporin cream | [&#39;C0132149&#39;, &#39;C0358174&#39;, &#39;C0357999&#39;, &#39;C0307085&#39;, &#39;C0698810&#39;] | [&#39;neosporin cream&#39;, &#39;nystan cream&#39;, &#39;nystadermal cream&#39;, &#39;nupercainal cream&#39;, &#39;nystaform cream&#39;] | | 2 | magnesium hydroxide 100mg/1ml | C1134402 | magnesium hydroxide 100 mg | [&#39;C1134402&#39;, &#39;C1126785&#39;, &#39;C4317023&#39;, &#39;C4051486&#39;, &#39;C4047137&#39;] | [&#39;magnesium hydroxide 100 mg&#39;, &#39;magnesium hydroxide 100 mg/ml&#39;, &#39;magnesium sulphate 100mg/ml injection&#39;, &#39;magnesium sulfate 100 mg&#39;, &#39;magnesium sulfate 100 mg/ml&#39;] | | 3 | metformin 1000 mg | C0987664 | metformin 1000 mg | [&#39;C0987664&#39;, &#39;C2719784&#39;, &#39;C0978482&#39;, &#39;C2719786&#39;, &#39;C4282269&#39;] | [&#39;metformin 1000 mg&#39;, &#39;metformin hydrochloride 1000 mg&#39;, &#39;metformin hcl 1000mg tab&#39;, &#39;metformin hydrochloride 1000 mg [fortamet]&#39;, &#39;metformin hcl 1000mg sa tab&#39;] | Updated RxNorm Entity Resolver Model (Dropping Invalid Codes) sbiobertresolve_rxnorm model was updated by dropping invalid codes using 02 August 2021 RxNorm dataset. New showVersion() Method in Compatibility Class We added the .showVersion() method in our Compatibility class that shows the name of the models and the version in a pretty way. compatibility = Compatibility() compatibility.showVersion(&#39;sentence_detector_dl_healthcare&#39;) After the execution you will see the following table, ++++ | Pipeline/Model | lang | version | ++++ | sentence_detector_dl_healthcare | en | 2.6.0 | | sentence_detector_dl_healthcare | en | 2.7.0 | | sentence_detector_dl_healthcare | en | 3.2.0 | ++++ New Docker Images for Spark NLP for Healthcare and Spark OCR We are releasing new Docker Images for Spark NLP for Healthcare and Spark OCR containing a jupyter environment. Users having a valid license can run the image on their local system, and connect to pre-configured jupyter instance without installing the library on their local system. Spark NLP for Healthcare Docker Image For running Spark NLP for Healthcare inside a container: Instructions: Spark NLP for Healthcare Docker Image Video Instructions: Youtube Video Spark NLP for Healthcare &amp; OCR Docker Image For users who want to run Spark OCR and then feed the output of OCR pipeline to healthcare modules to process further: Instructions: Spark NLP for Healthcare &amp; OCR Docker Image New and Updated Deidentification() Parameters New Parameter : setBlackList() : List of entities ignored for masking or obfuscation.The default values are: SSN, PASSPORT, DLN, NPI, C_CARD, IBAN, DEA. Updated Parameter : .setObfuscateRefSource() : It was set faker as default. New Python API Documentation We have new Spark NLP for Healthcare Python API Documentation . This page contains information how to use the library with Python examples. Updated Spark NLP For Healthcare Notebooks and New Notebooks New BertForTokenClassification NER Model Training with Transformers Notebook for showing how to train a BertForTokenClassification NER model with transformers and then import into Spark NLP. New ChunkKeyPhraseExtraction notebook for showing how to get chunk key phrases using ChunkKeyPhraseExtraction. Updated all Spark NLP For Healthcare Notebooks with v3.3.0 by adding the new features. To see more, please check : Spark NLP Healthcare Workshop Repo 3.3.0 We are glad to announce that Spark NLP Healthcare 3.3.0 has been released!. Highlights NER Finder Pretrained Pipelines to Run Run 48 different Clinical NER and 21 Different Biobert Models At Once Over the Input Text 3 New Sentence Entity Resolver Models (3-char ICD10CM, RxNorm_NDC, HCPCS) Updated UMLS Entity Resolvers (Dropping Invalid Codes) 5 New Clinical NER Models (Trained By BertForTokenClassification Approach) Radiology NER Model Trained On cheXpert Dataset New Speed Benchmarks on Databricks NerConverterInternal Fixes Simplified Setup and Recommended Use of start() Function NER Evaluation Metrics Fix New Notebooks (Including How to Use SparkNLP with Neo4J) NER Finder Pretrained Pipelines to Run Run 48 different Clinical NER and 21 Different Biobert Models At Once Over the Input Text We are releasing two new NER Pretrained Pipelines that can be used to explore all the available pretrained NER models at once. You can check NER Profiling Notebook to see how to use these pretrained pipelines. ner_profiling_clinical : When you run this pipeline over your text, you will end up with the predictions coming out of each of the 48 pretrained clinical NER models trained with embeddings_clinical. Clinical NER Model List ner_ade_clinical ner_posology_greedy ner_risk_factors jsl_ner_wip_clinical ner_human_phenotype_gene_clinical jsl_ner_wip_greedy_clinical ner_cellular ner_cancer_genetics jsl_ner_wip_modifier_clinical ner_drugs_greedy ner_deid_sd_large ner_diseases nerdl_tumour_demo ner_deid_subentity_augmented ner_jsl_enriched ner_genetic_variants ner_bionlp ner_measurements_clinical ner_diseases_large ner_radiology ner_deid_augmented ner_anatomy ner_chemprot_clinical ner_posology_experimental ner_drugs ner_deid_sd ner_posology_large ner_deid_large ner_posology ner_deidentify_dl ner_deid_enriched ner_bacterial_species ner_drugs_large ner_clinical_large jsl_rd_ner_wip_greedy_clinical ner_medmentions_coarse ner_radiology_wip_clinical ner_clinical ner_chemicals ner_deid_synthetic ner_events_clinical ner_posology_small ner_anatomy_coarse ner_human_phenotype_go_clinical ner_jsl_slim ner_jsl ner_jsl_greedy ner_events_admission_clinical ner_profiling_biobert : When you run this pipeline over your text, you will end up with the predictions coming out of each of the 21 pretrained clinical NER models trained with biobert_pubmed_base_cased. BioBert NER Model List ner_cellular_biobert ner_diseases_biobert ner_events_biobert ner_bionlp_biobert ner_jsl_greedy_biobert ner_jsl_biobert ner_anatomy_biobert ner_jsl_enriched_biobert ner_human_phenotype_go_biobert ner_deid_biobert ner_deid_enriched_biobert ner_clinical_biobert ner_anatomy_coarse_biobert ner_human_phenotype_gene_biobert ner_posology_large_biobert jsl_rd_ner_wip_greedy_biobert ner_posology_biobert jsl_ner_wip_greedy_biobert ner_chemprot_biobert ner_ade_biobert ner_risk_factors_biobert You can also check Models Hub page for more information about all these NER models and more. Example : from sparknlp.pretrained import PretrainedPipeline ner_profiling_pipeline = PretrainedPipeline(&#39;ner_profiling_biobert&#39;, &#39;en&#39;, &#39;clinical/models&#39;) result = ner_profiling_pipeline.annotate(&quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting .&quot;) Results : sentence : [&#39;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting .&#39;] token : [&#39;A&#39;, &#39;28-year-old&#39;, &#39;female&#39;, &#39;with&#39;, &#39;a&#39;, &#39;history&#39;, &#39;of&#39;, &#39;gestational&#39;, &#39;diabetes&#39;, &#39;mellitus&#39;, &#39;diagnosed&#39;, &#39;eight&#39;, &#39;years&#39;, &#39;prior&#39;, &#39;to&#39;, &#39;presentation&#39;, &#39;and&#39;, &#39;subsequent&#39;, &#39;type&#39;, &#39;two&#39;, &#39;diabetes&#39;, &#39;mellitus&#39;, &#39;(&#39;, &#39;T2DM&#39;, &#39;),&#39;, &#39;one&#39;, &#39;prior&#39;, &#39;episode&#39;, &#39;of&#39;, &#39;HTG-induced&#39;, &#39;pancreatitis&#39;, &#39;three&#39;, &#39;years&#39;, &#39;prior&#39;, &#39;to&#39;, &#39;presentation&#39;, &#39;,&#39;, &#39;associated&#39;, &#39;with&#39;, &#39;an&#39;, &#39;acute&#39;, &#39;hepatitis&#39;, &#39;,&#39;, &#39;and&#39;, &#39;obesity&#39;, &#39;with&#39;, &#39;a&#39;, &#39;body&#39;, &#39;mass&#39;, &#39;index&#39;, &#39;(&#39;, &#39;BMI&#39;, &#39;)&#39;, &#39;of&#39;, &#39;33.5&#39;, &#39;kg/m2&#39;, &#39;,&#39;, &#39;presented&#39;, &#39;with&#39;, &#39;a&#39;, &#39;one-week&#39;, &#39;history&#39;, &#39;of&#39;, &#39;polyuria&#39;, &#39;,&#39;, &#39;polydipsia&#39;, &#39;,&#39;, &#39;poor&#39;, &#39;appetite&#39;, &#39;,&#39;, &#39;and&#39;, &#39;vomiting&#39;, &#39;.&#39;] ner_cellular_biobert_chunks : [] ner_diseases_biobert_chunks : [&#39;gestational diabetes mellitus&#39;, &#39;type two diabetes mellitus&#39;, &#39;T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;hepatitis&#39;, &#39;obesity&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_events_biobert_chunks : [&#39;gestational diabetes mellitus&#39;, &#39;eight years&#39;, &#39;presentation&#39;, &#39;type two diabetes mellitus ( T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;three years&#39;, &#39;presentation&#39;, &#39;an acute hepatitis&#39;, &#39;obesity&#39;, &#39;a body mass index&#39;, &#39;BMI&#39;, &#39;presented&#39;, &#39;a one-week&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_bionlp_biobert_chunks : [] ner_jsl_greedy_biobert_chunks : [&#39;28-year-old&#39;, &#39;female&#39;, &#39;gestational diabetes mellitus&#39;, &#39;eight years prior&#39;, &#39;type two diabetes mellitus&#39;, &#39;T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;three years prior&#39;, &#39;acute hepatitis&#39;, &#39;obesity&#39;, &#39;body mass index&#39;, &#39;BMI ) of 33.5 kg/m2&#39;, &#39;one-week&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_jsl_biobert_chunks : [&#39;28-year-old&#39;, &#39;female&#39;, &#39;gestational diabetes mellitus&#39;, &#39;eight years prior&#39;, &#39;type two diabetes mellitus&#39;, &#39;T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;three years prior&#39;, &#39;acute&#39;, &#39;hepatitis&#39;, &#39;obesity&#39;, &#39;body mass index&#39;, &#39;BMI ) of 33.5 kg/m2&#39;, &#39;one-week&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_anatomy_biobert_chunks : [&#39;body&#39;] ner_jsl_enriched_biobert_chunks : [&#39;28-year-old&#39;, &#39;female&#39;, &#39;gestational diabetes mellitus&#39;, &#39;type two diabetes mellitus&#39;, &#39;T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;acute&#39;, &#39;hepatitis&#39;, &#39;obesity&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_human_phenotype_go_biobert_chunks : [&#39;obesity&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;] ner_deid_biobert_chunks : [&#39;eight years&#39;, &#39;three years&#39;] ner_deid_enriched_biobert_chunks : [] ner_clinical_biobert_chunks : [&#39;gestational diabetes mellitus&#39;, &#39;subsequent type two diabetes mellitus ( T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;an acute hepatitis&#39;, &#39;obesity&#39;, &#39;a body mass index ( BMI )&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_anatomy_coarse_biobert_chunks : [&#39;body&#39;] ner_human_phenotype_gene_biobert_chunks : [&#39;obesity&#39;, &#39;mass&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;vomiting&#39;] ner_posology_large_biobert_chunks : [] jsl_rd_ner_wip_greedy_biobert_chunks : [&#39;gestational diabetes mellitus&#39;, &#39;type two diabetes mellitus&#39;, &#39;T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;acute hepatitis&#39;, &#39;obesity&#39;, &#39;body mass index&#39;, &#39;33.5&#39;, &#39;kg/m2&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_posology_biobert_chunks : [] jsl_ner_wip_greedy_biobert_chunks : [&#39;28-year-old&#39;, &#39;female&#39;, &#39;gestational diabetes mellitus&#39;, &#39;eight years prior&#39;, &#39;type two diabetes mellitus&#39;, &#39;T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;three years prior&#39;, &#39;acute hepatitis&#39;, &#39;obesity&#39;, &#39;body mass index&#39;, &#39;BMI ) of 33.5 kg/m2&#39;, &#39;one-week&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_chemprot_biobert_chunks : [] ner_ade_biobert_chunks : [&#39;pancreatitis&#39;, &#39;acute hepatitis&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_risk_factors_biobert_chunks : [&#39;diabetes mellitus&#39;, &#39;subsequent type two diabetes mellitus&#39;, &#39;obesity&#39;] 3 New Sentence Entity Resolver Models (3-char ICD10CM, RxNorm_NDC, HCPCS) sbiobertresolve_hcpcs : This model maps extracted medical entities to Healthcare Common Procedure Coding System (HCPCS) codes using sbiobert_base_cased_mli sentence embeddings. It also returns the domain information of the codes in the all_k_aux_labels parameter in the metadata of the result. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbiobert_base_cased_mli&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) hcpcs_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_hcpcs&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;hcpcs_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) hcpcs_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, hcpcs_resolver]) res = hcpcs_pipelineModel.transform(spark.createDataFrame([[&quot;Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type&quot;]]).toDF(&quot;text&quot;)) Results : ner_chunk hcpcs_code all_codes all_resolutions domain Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type L8001 [L8001, L8002, L8000, L8033, L8032, …] ‘Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type’, ‘Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, bilateral, any size, any type’, ‘Breast prosthesis, mastectomy bra, without integrated breast prosthesis form, any size, any type’, ‘Nipple prosthesis, custom fabricated, reusable, any material, any type, each’, … Device, Device, Device, Device, Device, … sbiobertresolve_icd10cm_generalised : This model maps medical entities to 3 digit ICD10CM codes (according to ICD10 code structure the first three characters represent general type of the injury or disease). Difference in results (compared with sbiobertresolve_icd10cm) can be observed in the example below. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbiobert_base_cased_mli&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) icd_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_icd10cm_generalised&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;icd_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) icd_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, icd_resolver]) res = icd_pipelineModel.transform(spark.createDataFrame([[&quot;82 - year-old male with a history of hypertension , chronic renal insufficiency , COPD , and gastritis&quot;]]).toDF(&quot;text&quot;)) Results : | | chunk | entity | code_3char | code_desc_3char | code_full | code_full_description | distance | all_k_resolutions_3char | all_k_codes_3char | |:|:-|:--|:--|:-|:-|:|-:|:|:--| | 0 | hypertension | SYMPTOM | I10 | hypertension | I150 | Renovascular hypertension | 0 | [hypertension, hypertension (high blood pressure), h/o: hypertension, ...] | [I10, I15, Z86, Z82, I11, R03, Z87, E27] | | 1 | chronic renal insufficiency | SYMPTOM | N18 | chronic renal impairment | N186 | End stage renal disease | 0.014 | [chronic renal impairment, renal insufficiency, renal failure, anaemi ...] | [N18, P96, N19, D63, N28, Z87, N17, N25, R94] | | 2 | COPD | SYMPTOM | J44 | chronic obstructive lung disease (disorder) | I2781 | Cor pulmonale (chronic) | 0.1197 | [chronic obstructive lung disease (disorder), chronic obstructive pul ...] | [J44, Z76, J81, J96, R06, I27, Z87] | | 3 | gastritis | SYMPTOM | K29 | gastritis | K5281 | Eosinophilic gastritis or gastroenteritis | 0 | gastritis:::bacterial gastritis:::parasitic gastritis | [K29, B96, K93] | sbiobertresolve_rxnorm_ndc : This model maps DRUG entities to rxnorm codes and their National Drug Codes (NDC) using sbiobert_base_cased_mli sentence embeddings. You can find all NDC codes of drugs seperated by | in the all_k_aux_labels parameter of the metadata. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbiobert_base_cased_mli&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) rxnorm_ndc_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_rxnorm_ndc&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;rxnorm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) rxnorm_ndc_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, rxnorm_ndc_resolver]) res = rxnorm_ndc_pipelineModel.transform(spark.createDataFrame([[&quot;activated charcoal 30000 mg powder for oral suspension&quot;]]).toDF(&quot;text&quot;)) Results : chunk rxnorm_code all_codes resolutions all_k_aux_labels all_distances activated charcoal 30000 mg powder for oral suspension 1440919 1440919, 808917, 1088194, 1191772, 808921,… activated charcoal 30000 MG Powder for Oral Suspension, Activated Charcoal 30000 MG Powder for Oral Suspension, wheat dextrin 3000 MG Powder for Oral Solution [Benefiber], cellulose 3000 MG Oral Powder [Unifiber], fosfomycin 3000 MG Powder for Oral Solution [Monurol] … 69784030828, 00395052791, 08679001362|86790016280|00067004490, 46017004408|68220004416, 00456430001,… 0.0000, 0.0000, 0.1128, 0.1148, 0.1201,… Updated UMLS Entity Resolvers (Dropping Invalid Codes) UMLS model sbiobertresolve_umls_findings and sbiobertresolve_umls_major_concepts were updated by dropping the invalid codes using the latest UMLS release done May 2021. 5 New Clinical NER Models (Trained By BertForTokenClassification Approach) We are releasing four new BERT-based NER models. bert_token_classifier_ner_ade : This model is BERT-Based version of ner_ade_clinical model and performs 5% better. It can detect drugs and adverse reactions of drugs in reviews, tweets, and medical texts using DRUG and ADE labels. Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_ade&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;document&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[documentAssembler, tokenizer, tokenClassifier, ner_converter]) p_model = pipeline.fit(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [&#39;&#39;]}))) test_sentence = &quot;&quot;&quot;Been taking Lipitor for 15 years , have experienced severe fatigue a lot!!! . Doctor moved me to voltaren 2 months ago , so far , have only experienced cramps&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +--++ |chunk |ner_label| +--++ |Lipitor |DRUG | |severe fatigue|ADE | |voltaren |DRUG | |cramps |ADE | +--++ bert_token_classifier_ner_jsl_slim : This model is BERT-Based version of ner_jsl_slim model and 2% better than the legacy NER model (MedicalNerModel) that is based on BiLSTM-CNN-Char architecture. It can detect Death_Entity, Medical_Device, Vital_Sign, Alergen, Drug, Clinical_Dept, Lifestyle, Symptom, Body_Part, Physical_Measurement, Admission_Discharge, Date_Time, Age, Birth_Entity, Header, Oncological, Substance_Quantity, Test_Result, Test, Procedure, Treatment, Disease_Syndrome_Disorder, Pregnancy_Newborn, Demographics entities. Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_jsl_slim&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[documentAssembler, sentence_detector, tokenizer, tokenClassifier, ner_converter]) p_model = pipeline.fit(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [&#39;&#39;]}))) test_sentence = &quot;&quot;&quot;HISTORY: 30-year-old female presents for digital bilateral mammography secondary to a soft tissue lump palpated by the patient in the upper right shoulder. The patient has a family history of breast cancer within her mother at age 58. Patient denies personal history of breast cancer.&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +-++ |chunk |ner_label | +-++ |HISTORY: |Header | |30-year-old |Age | |female |Demographics| |mammography |Test | |soft tissue lump|Symptom | |shoulder |Body_Part | |breast cancer |Oncological | |her mother |Demographics| |age 58 |Age | |breast cancer |Oncological | +-++ bert_token_classifier_ner_drugs : This model is BERT-based version of ner_drugs model and detects drug chemicals. This new model is 3% better than the legacy NER model (MedicalNerModel) that is based on BiLSTM-CNN-Char architecture. Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_drugs&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;sentence&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[documentAssembler, sentenceDetector, tokenizer, tokenClassifier, ner_converter]) model = pipeline.fit(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [&#39;&#39;]}))) test_sentence = &quot;&quot;&quot;The human KCNJ9 (Kir 3.3, GIRK3) is a member of the G-protein-activated inwardly rectifying potassium (GIRK) channel family. Here we describe the genomicorganization of the KCNJ9 locus on chromosome 1q21-23 as a candidate gene forType II diabetes mellitus in the Pima Indian population. The gene spansapproximately 7.6 kb and contains one noncoding and two coding exons separated byapproximately 2.2 and approximately 2.6 kb introns, respectively. We identified14 single nucleotide polymorphisms (SNPs), including one that predicts aVal366Ala substitution, and an 8 base-pair (bp) insertion/deletion. Ourexpression studies revealed the presence of the transcript in various humantissues including pancreas, and two major insulin-responsive tissues: fat andskeletal muscle. The characterization of the KCNJ9 gene should facilitate furtherstudies on the function of the KCNJ9 protein and allow evaluation of thepotential role of the locus in Type II diabetes.BACKGROUND: At present, it is one of the most important issues for the treatment of breast cancer to develop the standard therapy for patients previously treated with anthracyclines and taxanes. With the objective of determining the usefulnessof vinorelbine monotherapy in patients with advanced or recurrent breast cancerafter standard therapy, we evaluated the efficacy and safety of vinorelbine inpatients previously treated with anthracyclines and taxanes.&quot;&quot;&quot; result = model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +--++ |chunk |ner_label| +--++ |potassium |DrugChem | |nucleotide |DrugChem | |anthracyclines|DrugChem | |taxanes |DrugChem | |vinorelbine |DrugChem | |vinorelbine |DrugChem | |anthracyclines|DrugChem | |taxanes |DrugChem | +--++ bert_token_classifier_ner_anatomy : This model is BERT-Based version of ner_anatomy model and 3% better. It can detect Anatomical_system, Cell, Cellular_component, Developing_anatomical_structure, Immaterial_anatomical_entity, Multi-tissue_structure, Organ, Organism_subdivision, Organism_substance, Pathological_formation, Tissue entities. Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_anatomy&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;sentence&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[documentAssembler, sentenceDetector, tokenizer, tokenClassifier, ner_converter]) pp_model = pipeline.fit(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [&#39;&#39;]}))) test_sentence = &quot;&quot;&quot;This is an 11-year-old female who comes in for two different things. 1. She was seen by the allergist. No allergies present, so she stopped her Allegra, but she is still real congested and does a lot of snorting. They do not notice a lot of snoring at night though, but she seems to be always like that. 2. On her right great toe, she has got some redness and erythema. Her skin is kind of peeling a little bit, but it has been like that for about a week and a half now. nGeneral: Well-developed female, in no acute distress, afebrile. nHEENT: Sclerae and conjunctivae clear. Extraocular muscles intact. TMs clear. Nares patent. A little bit of swelling of the turbinates on the left. Oropharynx is essentially clear. Mucous membranes are moist. nNeck: No lymphadenopathy. nChest: Clear. nAbdomen: Positive bowel sounds and soft. nDermatologic: She has got redness along her right great toe, but no bleeding or oozing. Some dryness of her skin. Her toenails themselves are very short and even on her left foot and her left great toe the toenails are very short.&quot;&quot;&quot; result = pp_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +-+-+ |chunk |ner_label | +-+-+ |great toe |Multi-tissue_structure| |skin |Organ | |conjunctivae |Multi-tissue_structure| |Extraocular muscles|Multi-tissue_structure| |Nares |Multi-tissue_structure| |turbinates |Multi-tissue_structure| |Oropharynx |Multi-tissue_structure| |Mucous membranes |Tissue | |Neck |Organism_subdivision | |bowel |Organ | |great toe |Multi-tissue_structure| |skin |Organ | |toenails |Organism_subdivision | |foot |Organism_subdivision | |great toe |Multi-tissue_structure| |toenails |Organism_subdivision | +-+-+ bert_token_classifier_ner_bacteria : This model is BERT-Based version of ner_bacterial_species model and detects different types of species of bacteria in clinical texts using SPECIES label. Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_bacteria&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;document&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[documentAssembler, tokenizer, tokenClassifier, ner_converter]) p_model = pipeline.fit(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [&#39;&#39;]}))) test_sentence = &quot;&quot;&quot;Based on these genetic and phenotypic properties, we propose that strain SMSP (T) represents a novel species of the genus Methanoregula, for which we propose the name Methanoregula formicica sp. nov., with the type strain SMSP (T) (= NBRC 105244 (T) = DSM 22288 (T)).&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +--++ |chunk |ner_label| +--++ |SMSP (T) |SPECIES | |Methanoregula formicica|SPECIES | |SMSP (T) |SPECIES | +--++ Radiology NER Model Trained On cheXpert Dataset Ner NER model ner_chexpert trained on Radiology Chest reports to extract anatomical sites and observation entities. The model achieves 92.8% and 77.4% micro and macro f1 scores on the cheXpert dataset. Example : ... embeddings_clinical = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_chexpert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings_clinical, clinical_ner, ner_converter]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) EXAMPLE_TEXT = &quot;&quot;&quot;FINAL REPORT HISTORY : Chest tube leak , to assess for pneumothorax . FINDINGS : In comparison with study of ___ , the endotracheal tube and Swan - Ganz catheter have been removed . The left chest tube remains in place and there is no evidence of pneumothorax. Mild atelectatic changes are seen at the left base.&quot;&quot;&quot; results = model.transform(spark.createDataFrame([[EXAMPLE_TEXT]]).toDF(&quot;text&quot;)) Results : | | chunk | label | |:|:-|:--| | 0 | endotracheal tube | OBS | | 1 | Swan - Ganz catheter | OBS | | 2 | left chest | ANAT | | 3 | tube | OBS | | 4 | in place | OBS | | 5 | pneumothorax | OBS | | 6 | Mild atelectatic changes | OBS | | 7 | left base | ANAT | New Speed Benchmarks on Databricks We prepared a speed benchmark table by running a NER pipeline on various number of cluster configurations (worker number, driver node, specs etc) and also writing the results to parquet or delta formats. You can find all the details of these tries in here : Speed Benchmark Table NerConverterInternal Fixes Now NerConverterInternal can deal with tags that have some dash (-) charachter like B-GENE-N and B-GENE-Y. Simplified Setup and Recommended Use of start() Function Starting with this release, we are shipping AWS credentials inside Spark NLP Healthcare’s license. This removes the requirement of setting the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables. To use this feature, you just need to make sure that you always call the start() function at the beginning of your program, from sparknlp_jsl import start spark = start() import com.johnsnowlabs.util.start val spark = start() If for some reason you don’t want to use this mechanism, the keys will continue to be shipped separately, and the environment variables will continue to work as they did in the past. Ner Evaluation Metrics Fix Bug fixed in the NerDLMetrics package. Previously, the full_chunk option was using greedy approach to merge chunks for a strict evaluation, which has been fixed to merge chunks using IOB scheme to get accurate entities boundaries and metrics. Also, the tag option has been fixed to get metrics that align with the default NER logs. New Notebooks Clinical Relation Extraction Knowledge Graph with Neo4j Notebook NER Profiling Pretrained Pipelines Notebook New Databricks Detecting Adverse Drug Events From Conversational Texts case study notebook. To see more, please check : Spark NLP Healthcare Workshop Repo 3.2.3 We are glad to announce that Spark NLP Healthcare 3.2.3 has been released!. Highlights New BERT-Based Deidentification NER Model New Sentence Entity Resolver Models For German Language New Spell Checker Model For Drugs Allow To Use Disambiguator Pretrained Model Allow To Use Seeds in StructuredDeidentification Added Compatibility with Tensorflow 1.15 For Graph Generation. New Setup Videos New BERT-Based Deidentification NER Model We have a new bert_token_classifier_ner_deid model that is BERT-based version of ner_deid_subentity_augmented and annotates text to find protected health information that may need to be de-identified. It can detect 23 different entities (MEDICALRECORD, ORGANIZATION, DOCTOR, USERNAME, PROFESSION, HEALTHPLAN, URL, CITY, DATE, LOCATION-OTHER, STATE, PATIENT, DEVICE, COUNTRY, ZIP, PHONE, HOSPITAL, EMAIL, IDNUM, SREET, BIOID, FAX, AGE). Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_deid&quot;, &quot;en&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;document&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[documentAssembler, tokenizer, tokenClassifier, ner_converter]) p_model = pipeline.fit(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [&#39;&#39;]}))) text = &quot;&quot;&quot;A. Record date : 2093-01-13, David Hale, M.D. Name : Hendrickson, Ora MR. # 7194334. PCP : Oliveira, non-smoking. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227. Patient&#39;s complaints first surfaced when he started working for Brothers Coal-Mine.&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [text]}))) Results: +--+-+ |chunk |ner_label | +--+-+ |2093-01-13 |DATE | |David Hale |DOCTOR | |Hendrickson, Ora |PATIENT | |7194334 |MEDICALRECORD| |Oliveira |PATIENT | |Cocke County Baptist Hospital|HOSPITAL | |0295 Keats Street |STREET | |302) 786-5227 |PHONE | |Brothers Coal-Mine |ORGANIZATION | +--+-+ New Sentence Entity Resolver Models For German Language We are releasing two new Sentence Entity Resolver Models for German language that use sent_bert_base_cased (de) embeddings. sbertresolve_icd10gm : This model maps extracted medical entities to ICD10-GM codes for the German language. Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&quot;sent_bert_base_cased&quot;, &quot;de&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) icd10gm_resolver = SentenceEntityResolverModel.pretrained(&quot;sbertresolve_icd10gm&quot;, &quot;de&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;icd10gm_code&quot;) icd10gm_pipelineModel = PipelineModel( stages = [documentAssembler, sbert_embedder, icd10gm_resolver]) icd_lp = LightPipeline(icd10gm_pipelineModel) icd_lp.fullAnnotate(&quot;Dyspnoe&quot;) Results : chunk code resolutions all_codes all_distances Dyspnoe C671 Dyspnoe, Schlafapnoe, Dysphonie, Frühsyphilis, Hyperzementose, Hypertrichose, … [R06.0, G47.3, R49.0, A51, K03.4, L68, …] [0.0000, 2.5602, 3.0529, 3.3310, 3.4645, 3.7148, …] sbertresolve_snomed : This model maps extracted medical entities to SNOMED codes for the German language. Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&quot;sent_bert_base_cased&quot;, &quot;de&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) snomed_resolver = SentenceEntityResolverModel.pretrained(&quot;sbertresolve_snomed&quot;, &quot;de&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) snomed_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, snomed_resolver]) snomed_lp = LightPipeline(snomed_pipelineModel) snomed_lp.fullAnnotate(&quot;Bronchialkarzinom &quot;) Results : chunk code resolutions all_codes all_distances Bronchialkarzinom 22628 Bronchialkarzinom, Bronchuskarzinom, Rektumkarzinom, Klavikulakarzinom, Lippenkarzinom, Urothelkarzinom, … [22628, 111139, 18116, 107569, 18830, 22909, …] [0.0000, 0.0073, 0.0090, 0.0098, 0.0098, 0.0102, …] New Spell Checker Model For Drugs We are releasing new spellcheck_drug_norvig model that detects and corrects spelling errors of drugs in a text based on the Norvig’s approach. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) spell = NorvigSweetingModel.pretrained(&quot;spellcheck_drug_norvig&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) pipeline = Pipeline( stages = [documentAssembler, tokenizer, spell]) model = pipeline.fit(spark.createDataFrame([[&#39;&#39;]]).toDF(&#39;text&#39;)) lp = LightPipeline(model) lp.annotate(&quot;You have to take Neutrcare and colfosrinum and a bit of Fluorometholne &amp; Ribotril&quot;) Results : Original text : You have to take Neutrcare and colfosrinum and a bit of fluorometholne &amp; Ribotril Corrected text : You have to take Neutracare and colforsinum and a bit of fluorometholone &amp; Rivotril Allow to use Disambiguator pretrained model. Now we can use the NerDisambiguatorModel as a pretrained model to disambiguate person entities. text = &quot;The show also had a contestant named Brad Pitt&quot; + &quot;who later defeated Christina Aguilera on the way to become Female Vocalist Champion in the 1989 edition of Star Search in the United States. &quot; data = SparkContextForTest.spark.createDataFrame([ [text]]) .toDF(&quot;text&quot;).cache() da = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sd = SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) tk = Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) emb = WordEmbeddingsModel.pretrained().setOutputCol(&quot;embs&quot;) semb = SentenceEmbeddings().setInputCols(&quot;sentence&quot;, &quot;embs&quot;).setOutputCol(&quot;sentence_embeddings&quot;) ner = NerDLModel.pretrained().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;).setOutputCol(&quot;ner&quot;) nc = NerConverter().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;).setOutputCol(&quot;ner_chunk&quot;).setWhiteList([&quot;PER&quot;]) NerDisambiguatorModel.pretrained().setInputCols(&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;).setOutputCol(&quot;disambiguation&quot;) pl = Pipeline().setStages([da, sd, tk, emb, semb, ner, nc, disambiguator]) data = pl.fit(data).transform(data) data.select(&quot;disambiguation&quot;).show(10, False) +-+ |disambiguation | +-+ |[[disambiguation, 65, 82, http://en.wikipedia.org/?curid=144171, http://en.wikipedia.org/?curid=6636454, [chunk -&gt; Christina Aguilera, titles -&gt; christina aguilera ::::: christina aguilar, links -&gt; http://en.wikipedia.org/?curid=144171 ::::: http://en.wikipedia.org/?curid=6636454, beginInText -&gt; 65, scores -&gt; 0.9764155197864447, 0.9727793647472524, categories -&gt; Musicians, Singers, Actors, Businesspeople, Musicians, Singers, ids -&gt; 144171, 6636454, endInText -&gt; 82], []]]| +-+ - Allow to use seeds in StructuredDeidentification Now, we can use a seed for a specific column. The seed is used to randomly select the entities used during obfuscation mode. By providing the same seed, you can replicate the same mapping multiple times. df = spark.createDataFrame([ [&quot;12&quot;, &quot;12&quot;, &quot;Juan García&quot;], [&quot;24&quot;, &quot;56&quot;, &quot;Will Smith&quot;], [&quot;56&quot;, &quot;32&quot;, &quot;Pedro Ximénez&quot;] ]).toDF(&quot;ID1&quot;, &quot;ID2&quot;, &quot;NAME&quot;) obfuscator = StructuredDeidentification(spark=spark, columns={&quot;ID1&quot;: &quot;ID&quot;, &quot;ID2&quot;: &quot;ID&quot;, &quot;NAME&quot;: &quot;PATIENT&quot;}, columnsSeed={&quot;ID1&quot;: 23, &quot;ID2&quot;: 23}, obfuscateRefSource=&quot;faker&quot;) result = obfuscator.obfuscateColumns(df) result.show(truncate=False) +-+-+-+ |ID1 |ID2 |NAME | +-+-+-+ |[D3379888]|[D3379888]|[Raina Cleaves] | |[R8448971]|[M8851891]|[Jennell Barre] | |[M8851891]|[L5448098]|[Norene Salines]| +-+-+-+ Here, you can see that as we have provided the same seed `23` for columns `ID1`, and `ID2`, the number `12` which is appears twice in the first row is mapped to the same randomly generated id `D3379888` each time. Added compatibility with Tensorflow 1.15 for graph generation Some users reported problems while using graphs generated by Tensorflow 2.x. We provide compatibility with Tensorflow 1.15 in the tf_graph_1x module, that can be used like this, from sparknlp_jsl.training import tf_graph_1x In next releases, we will provide full support for graph generation using Tensorflow 2.x. New Setup Videos Now we have videos showing how to setup Spark NLP, Spark NLP for Healthcare and Spark OCR on UBUNTU. How to Setup Spark NLP on UBUNTU How to Setup Spark NLP for HEALTHCARE on UBUNTU How to Setup Spark OCR on UBUNTU To see more, please check: Spark NLP Healthcare Workshop Repo 3.2.2 We are glad to announce that Spark NLP Healthcare 3.2.2 has been released!. Highlights New NER Model For Detecting Drugs, Posology, and Administration Cycles New Sentence Entity Resolver Models New Router Annotator To Use Multiple Resolvers Optimally In the Same Pipeline Re-Augmented Deidentification NER Model New NER Model For Detecting Drugs, Posology, and Administration Cycles We are releasing a new NER posology model ner_posology_experimental. This model is based on the original ner_posology_large model, but trained with additional clinical trials data to detect experimental drugs, experiment cycles, cycle counts, and cycles numbers. Supported Entities: Administration, Cyclenumber, Strength, Cycleday, Duration, Cyclecount, Route, Form, Frequency, Cyclelength, Drug, Dosage Example: ... word_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_posology_experimental&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, word_embeddings, clinical_ner, ner_converter]) model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame([[&quot;Y-90 Humanized Anti-Tac: 10 mCi (if a bone marrow transplant was part of the patient&#39;s previous therapy) or 15 mCi of yttrium labeled anti-TAC; followed by calcium trisodium Inj (Ca DTPA).. n nCalcium-DTPA: Ca-DTPA will be administered intravenously on Days 1-3 to clear the radioactive agent from the body.&quot;]]).toDF(&quot;text&quot;)) Results: | | chunk | begin | end | entity | |:|:-|--:|:|:| | 0 | Y-90 Humanized Anti-Tac | 0 | 22 | Drug | | 1 | 10 mCi | 25 | 30 | Dosage | | 2 | 15 mCi | 108 | 113 | Dosage | | 3 | yttrium labeled anti-TAC | 118 | 141 | Drug | | 4 | calcium trisodium Inj | 156 | 176 | Drug | | 5 | Calcium-DTPA | 191 | 202 | Drug | | 6 | Ca-DTPA | 205 | 211 | Drug | | 7 | intravenously | 234 | 246 | Route | | 8 | Days 1-3 | 251 | 258 | Cycleday | New Sentence Entity Resolver Models We have two new sentence entity resolver models trained with using sbert_jsl_medium_uncased embeddings. sbertresolve_rxnorm_disposition : This model maps medication entities (like drugs/ingredients) to RxNorm codes and their dispositions using sbert_jsl_medium_uncased Sentence Bert Embeddings. If you look for a faster inference with just drug names (excluding dosage and strength), this version of RxNorm model would be a better alternative. In the result, look for the aux_label parameter in the metadata to get dispositions divided by |. Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbert_jsl_medium_uncased&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) rxnorm_resolver = SentenceEntityResolverModel.pretrained(&quot;sbertresolve_rxnorm_disposition&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;rxnorm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) rxnorm_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, rxnorm_resolver]) rxnorm_lp = LightPipeline(rxnorm_pipelineModel) rxnorm_lp = LightPipeline(pipelineModel) result = rxnorm_lp.fullAnnotate(&quot;alizapride 25 mg/ml&quot;) Result: | | chunks | code | resolutions | all_codes | all_k_aux_labels | all_distances | |:|:-|:-|:|:-|:|:--| | 0 |alizapride 25 mg/ml | 330948 | [alizapride 25 mg/ml, alizapride 50 mg, alizapride 25 mg/ml oral solution, adalimumab 50 mg/ml, adalimumab 100 mg/ml [humira], adalimumab 50 mg/ml [humira], alirocumab 150 mg/ml, ...]| [330948, 330949, 249531, 358817, 1726845, 576023, 1659153, ...] | [Dopamine receptor antagonist, Dopamine receptor antagonist, Dopamine receptor antagonist, -, -, -, -, ...] | [0.0000, 0.0936, 0.1166, 0.1525, 0.1584, 0.1567, 0.1631, ...] | sbertresolve_snomed_conditions : This model maps clinical entities (domain: Conditions) to Snomed codes using sbert_jsl_medium_uncased Sentence Bert Embeddings. Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbert_jsl_medium_uncased&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) snomed_resolver = SentenceEntityResolverModel.pretrained(&quot;sbertresolve_snomed_conditions&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) snomed_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, snomed_resolver ]) snomed_lp = LightPipeline(snomed_pipelineModel) result = snomed_lp.fullAnnotate(&quot;schizophrenia&quot;) Result: | | chunks | code | resolutions | all_codes | all_distances | |:|:--|:|:-|:|:--| | 0 | schizophrenia | 58214004 | [schizophrenia, chronic schizophrenia, borderline schizophrenia, schizophrenia, catatonic, subchronic schizophrenia, ...]| [58214004, 83746006, 274952002, 191542003, 191529003, 16990005, ...] | 0.0000, 0.0774, 0.0838, 0.0927, 0.0970, 0.0970, ...] | New Router Annotator To Use Multiple Resolvers Optimally In the Same Pipeline Normally, when we need to use more than one sentence entity resolver models in the same pipeline, we used to hit BertSentenceEmbeddings annotator more than once given the number of different resolver models in the same pipeline. Now we are introducing a solution with the help of Router annotator that could allow us to feed all the NER chunks to BertSentenceEmbeddings at once and then route the output of Sentence Embeddings to different resolver models needed. You can find an example of how to use this annotator in the updated 3.Clinical_Entity_Resolvers.ipynb Notebook Example: ... # to get PROBLEM entitis clinical_ner = MedicalNerModel().pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;clinical_ner&quot;) clinical_ner_chunk = NerConverter() .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;clinical_ner&quot;) .setOutputCol(&quot;clinical_ner_chunk&quot;) .setWhiteList([&quot;PROBLEM&quot;]) # to get DRUG entities posology_ner = MedicalNerModel().pretrained(&quot;ner_posology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;posology_ner&quot;) posology_ner_chunk = NerConverter() .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;posology_ner&quot;) .setOutputCol(&quot;posology_ner_chunk&quot;) .setWhiteList([&quot;DRUG&quot;]) # merge the chunks into a single ner_chunk chunk_merger = ChunkMergeApproach() .setInputCols(&quot;clinical_ner_chunk&quot;,&quot;posology_ner_chunk&quot;) .setOutputCol(&quot;final_ner_chunk&quot;) .setMergeOverlapping(False) # convert chunks to doc to get sentence embeddings of them chunk2doc = Chunk2Doc().setInputCols(&quot;final_ner_chunk&quot;).setOutputCol(&quot;final_chunk_doc&quot;) sbiobert_embeddings = BertSentenceEmbeddings.pretrained(&quot;sbiobert_base_cased_mli&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;final_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) # filter PROBLEM entity embeddings router_sentence_icd10 = Router() .setInputCols(&quot;sbert_embeddings&quot;) .setFilterFieldsElements([&quot;PROBLEM&quot;]) .setOutputCol(&quot;problem_embeddings&quot;) # filter DRUG entity embeddings router_sentence_rxnorm = Router() .setInputCols(&quot;sbert_embeddings&quot;) .setFilterFieldsElements([&quot;DRUG&quot;]) .setOutputCol(&quot;drug_embeddings&quot;) # use problem_embeddings only icd_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_icd10cm_slim_billable_hcc&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;clinical_ner_chunk&quot;, &quot;problem_embeddings&quot;]) .setOutputCol(&quot;icd10cm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) # use drug_embeddings only rxnorm_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_rxnorm&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;posology_ner_chunk&quot;, &quot;drug_embeddings&quot;]) .setOutputCol(&quot;rxnorm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, clinical_ner_chunk, posology_ner, posology_ner_chunk, chunk_merger, chunk2doc, sbiobert_embeddings, router_sentence_icd10, router_sentence_rxnorm, icd_resolver, rxnorm_resolver ]) Re-Augmented Deidentification NER Model We re-augmented ner_deid_subentity_augmented deidentification NER model improving the previous metrics by 2%. Example: ... deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, word_embeddings, deid_ner, ner_converter]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame(pd.DataFrame({&quot;text&quot;: [&quot;&quot;&quot;A. Record date : 2093-01-13, David Hale, M.D., Name : Hendrickson, Ora MR. # 7194334 Date : 01/13/93 PCP : Oliveira, 25 -year-old, Record date : 1-11-2000. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227.&quot;&quot;&quot;]}))) Results: +--+-+ |chunk |ner_label | +--+-+ |2093-01-13 |DATE | |David Hale |DOCTOR | |Hendrickson, Ora |PATIENT | |7194334 |MEDICALRECORD| |01/13/93 |DATE | |Oliveira |DOCTOR | |25-year-old |AGE | |1-11-2000 |DATE | |Cocke County Baptist Hospital|HOSPITAL | |0295 Keats Street. |STREET | |(302) 786-5227 |PHONE | |Brothers Coal-Mine |ORGANIZATION | +--+-+ To see more, please check: Spark NLP Healthcare Workshop Repo 3.2.1 We are glad to announce that Spark NLP Healthcare 3.2.1 has been released!. Highlights Deprecated ChunkEntityResolver. New BERT-Based NER Models HCC module added support for versions v22 and v23. Updated Notebooks for resolvers and graph builders. New TF Graph Builder. New BERT-Based NER Models We have two new BERT-based token classifier NER models. These models are the first clinical NER models that use the BertForTokenCLassification approach that was introduced in Spark NLP 3.2.0. bert_token_classifier_ner_clinical: This model is BERT-based version of ner_clinical model. This new model is 4% better than the legacy NER model (MedicalNerModel) that is based on BiLSTM-CNN-Char architecture. Metrics: precision recall f1-score support PROBLEM 0.88 0.92 0.90 30276 TEST 0.91 0.86 0.88 17237 TREATMENT 0.87 0.88 0.88 17298 O 0.97 0.97 0.97 202438 accuracy 0.95 267249 macro avg 0.91 0.91 0.91 267249 weighted avg 0.95 0.95 0.95 267249 Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl_healthcare&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;sentence&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, tokenClassifier, ner_converter ]) p_model = pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) text = &#39;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting . Two weeks prior to presentation , she was treated with a five-day course of amoxicillin for a respiratory tract infection . She was on metformin , glipizide , and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG . She had been on dapagliflozin for six months at the time of presentation . Physical examination on presentation was significant for dry oral mucosa ; significantly , her abdominal examination was benign with no tenderness , guarding , or rigidity . Pertinent laboratory findings on admission were : serum glucose 111 mg/dl , bicarbonate 18 mmol/l , anion gap 20 , creatinine 0.4 mg/dL , triglycerides 508 mg/dL , total cholesterol 122 mg/dL , glycated hemoglobin ( HbA1c ) 10% , and venous pH 7.27 . Serum lipase was normal at 43 U/L . Serum acetone levels could not be assessed as blood samples kept hemolyzing due to significant lipemia . The patient was initially admitted for starvation ketosis , as she reported poor oral intake for three days prior to admission . However , serum chemistry obtained six hours after presentation revealed her glucose was 186 mg/dL , the anion gap was still elevated at 21 , serum bicarbonate was 16 mmol/L , triglyceride level peaked at 2050 mg/dL , and lipase was 52 U/L . The β-hydroxybutyrate level was obtained and found to be elevated at 5.29 mmol/L - the original sample was centrifuged and the chylomicron layer removed prior to analysis due to interference from turbidity caused by lipemia again . The patient was treated with an insulin drip for euDKA and HTG with a reduction in the anion gap to 13 and triglycerides to 1400 mg/dL , within 24 hours . Her euDKA was thought to be precipitated by her respiratory tract infection in the setting of SGLT2 inhibitor use . The patient was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should be discontinued indefinitely . She had close follow-up with endocrinology post discharge .&#39; res = p_model.transform(spark.createDataFrame([[text]]).toDF(&quot;text&quot;)).collect() res[0][&#39;label&#39;] bert_token_classifier_ner_jsl: This model is BERT-based version of ner_jsl model. This new model is better than the legacy NER model (MedicalNerModel) that is based on BiLSTM-CNN-Char architecture. Metrics: precision recall f1-score support Admission_Discharge 0.84 0.97 0.90 415 Age 0.96 0.96 0.96 2434 Alcohol 0.75 0.83 0.79 145 Allergen 0.33 0.16 0.22 25 BMI 1.00 0.77 0.87 26 Birth_Entity 1.00 0.17 0.29 12 Blood_Pressure 0.86 0.88 0.87 597 Cerebrovascular_Disease 0.74 0.77 0.75 266 Clinical_Dept 0.90 0.92 0.91 2385 Communicable_Disease 0.70 0.59 0.64 85 Date 0.95 0.98 0.96 1438 Death_Entity 0.83 0.83 0.83 59 Diabetes 0.95 0.95 0.95 350 Diet 0.60 0.49 0.54 229 Direction 0.88 0.90 0.89 6187 Disease_Syndrome_Disorder 0.90 0.89 0.89 13236 Dosage 0.57 0.49 0.53 263 Drug 0.91 0.93 0.92 15926 Duration 0.82 0.85 0.83 1218 EKG_Findings 0.64 0.70 0.67 325 Employment 0.79 0.85 0.82 539 External_body_part_or_region 0.84 0.84 0.84 4805 Family_History_Header 1.00 1.00 1.00 889 Fetus_NewBorn 0.57 0.56 0.56 341 Form 0.53 0.43 0.48 81 Frequency 0.87 0.90 0.88 1718 Gender 0.98 0.98 0.98 5666 HDL 0.60 1.00 0.75 6 Heart_Disease 0.88 0.88 0.88 2295 Height 0.89 0.96 0.92 134 Hyperlipidemia 1.00 0.95 0.97 194 Hypertension 0.95 0.98 0.97 566 ImagingFindings 0.66 0.64 0.65 601 Imaging_Technique 0.62 0.67 0.64 108 Injury_or_Poisoning 0.85 0.83 0.84 1680 Internal_organ_or_component 0.90 0.91 0.90 21318 Kidney_Disease 0.89 0.89 0.89 446 LDL 0.88 0.97 0.92 37 Labour_Delivery 0.82 0.71 0.76 306 Medical_Device 0.89 0.93 0.91 12852 Medical_History_Header 0.96 0.97 0.96 1013 Modifier 0.68 0.60 0.64 1398 O2_Saturation 0.84 0.82 0.83 199 Obesity 0.96 0.98 0.97 130 Oncological 0.88 0.96 0.92 1635 Overweight 0.80 0.80 0.80 10 Oxygen_Therapy 0.91 0.92 0.92 231 Pregnancy 0.81 0.83 0.82 439 Procedure 0.91 0.91 0.91 14410 Psychological_Condition 0.81 0.81 0.81 354 Pulse 0.85 0.95 0.89 389 Race_Ethnicity 1.00 1.00 1.00 163 Relationship_Status 0.93 0.91 0.92 57 RelativeDate 0.83 0.86 0.84 1562 RelativeTime 0.74 0.79 0.77 431 Respiration 0.99 0.95 0.97 221 Route 0.68 0.69 0.69 597 Section_Header 0.97 0.98 0.98 28580 Sexually_Active_or_Sexual_Orientation 1.00 0.64 0.78 14 Smoking 0.83 0.90 0.86 225 Social_History_Header 0.95 0.99 0.97 825 Strength 0.71 0.55 0.62 227 Substance 0.85 0.81 0.83 193 Substance_Quantity 0.00 0.00 0.00 28 Symptom 0.84 0.86 0.85 23092 Temperature 0.94 0.97 0.96 410 Test 0.84 0.88 0.86 9050 Test_Result 0.84 0.84 0.84 2766 Time 0.90 0.81 0.86 140 Total_Cholesterol 0.69 0.95 0.80 73 Treatment 0.73 0.72 0.73 506 Triglycerides 0.83 0.80 0.81 30 VS_Finding 0.76 0.77 0.76 588 Vaccine 0.70 0.84 0.76 92 Vital_Signs_Header 0.95 0.98 0.97 2223 Weight 0.88 0.89 0.88 306 O 0.97 0.96 0.97 253164 accuracy 0.94 445974 macro avg 0.82 0.82 0.81 445974 weighted avg 0.94 0.94 0.94 445974 Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl_healthcare&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;sentence&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, tokenClassifier, ner_converter ]) p_model = pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) text = &#39;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting . Two weeks prior to presentation , she was treated with a five-day course of amoxicillin for a respiratory tract infection . She was on metformin , glipizide , and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG . She had been on dapagliflozin for six months at the time of presentation . Physical examination on presentation was significant for dry oral mucosa ; significantly , her abdominal examination was benign with no tenderness , guarding , or rigidity . Pertinent laboratory findings on admission were : serum glucose 111 mg/dl , bicarbonate 18 mmol/l , anion gap 20 , creatinine 0.4 mg/dL , triglycerides 508 mg/dL , total cholesterol 122 mg/dL , glycated hemoglobin ( HbA1c ) 10% , and venous pH 7.27 . Serum lipase was normal at 43 U/L . Serum acetone levels could not be assessed as blood samples kept hemolyzing due to significant lipemia . The patient was initially admitted for starvation ketosis , as she reported poor oral intake for three days prior to admission . However , serum chemistry obtained six hours after presentation revealed her glucose was 186 mg/dL , the anion gap was still elevated at 21 , serum bicarbonate was 16 mmol/L , triglyceride level peaked at 2050 mg/dL , and lipase was 52 U/L . The β-hydroxybutyrate level was obtained and found to be elevated at 5.29 mmol/L - the original sample was centrifuged and the chylomicron layer removed prior to analysis due to interference from turbidity caused by lipemia again . The patient was treated with an insulin drip for euDKA and HTG with a reduction in the anion gap to 13 and triglycerides to 1400 mg/dL , within 24 hours . Her euDKA was thought to be precipitated by her respiratory tract infection in the setting of SGLT2 inhibitor use . The patient was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should be discontinued indefinitely . She had close follow-up with endocrinology post discharge .&#39; res = p_model.transform(spark.createDataFrame([[text]]).toDF(&quot;text&quot;)).collect() res[0][&#39;label&#39;] HCC module added support for versions v22 and v23 Now we can use the version 22 and the version 23 for the new HCC module to calculate CMS-HCC Risk Adjustment score. Added the following parameters elig, orec and medicaid on the profiles functions. These parameters may not be stored in clinical notes, and may require to be imported from other sources. elig : The eligibility segment of the patient. Allowed values are as follows: - &quot;CFA&quot;: Community Full Benefit Dual Aged - &quot;CFD&quot;: Community Full Benefit Dual Disabled - &quot;CNA&quot;: Community NonDual Aged - &quot;CND&quot;: Community NonDual Disabled - &quot;CPA&quot;: Community Partial Benefit Dual Aged - &quot;CPD&quot;: Community Partial Benefit Dual Disabled - &quot;INS&quot;: Long Term Institutional - &quot;NE&quot;: New Enrollee - &quot;SNPNE&quot;: SNP NE orec: Original reason for entitlement code. - &quot;0&quot;: Old age and survivor&#39;s insurance - &quot;1&quot;: Disability insurance benefits - &quot;2&quot;: End-stage renal disease - &quot;3&quot;: Both DIB and ESRD medicaid: If the patient is in Medicaid or not. Required parameters should be stored in Spark dataframe. df.show(truncate=False) +++++--+-+--+ |hcc_profileV24 |icd10_code |age|gender|eligibility|orec|medicaid| +++++--+-+--+ |{&quot;hcc_lst&quot;:[...|[E1169, I5030, I509, E852] |64 |F |CFA |0 |true | |{&quot;hcc_lst&quot;:[...|[G629, D469, D6181] |77 |M |CND |1 |false | |{&quot;hcc_lst&quot;:[...|[D473, D473, D473, M069, C969]|16 |F |CPA |3 |true | +++++--+-+--+ The content of the hcc_profileV24 column is a JSON-parsable string, like in the following example, { &quot;hcc_lst&quot;: [ &quot;HCC18&quot;, &quot;HCC85_gDiabetesMellit&quot;, &quot;HCC85&quot;, &quot;HCC23&quot;, &quot;D3&quot; ], &quot;details&quot;: { &quot;CNA_HCC18&quot;: 0.302, &quot;CNA_HCC85&quot;: 0.331, &quot;CNA_HCC23&quot;: 0.194, &quot;CNA_D3&quot;: 0.0, &quot;CNA_HCC85_gDiabetesMellit&quot;: 0.0 }, &quot;hcc_map&quot;: { &quot;E1169&quot;: [ &quot;HCC18&quot; ], &quot;I5030&quot;: [ &quot;HCC85&quot; ], &quot;I509&quot;: [ &quot;HCC85&quot; ], &quot;E852&quot;: [ &quot;HCC23&quot; ] }, &quot;risk_score&quot;: 0.827, &quot;parameters&quot;: { &quot;elig&quot;: &quot;CNA&quot;, &quot;age&quot;: 56, &quot;sex&quot;: &quot;F&quot;, &quot;origds&quot;: false, &quot;disabled&quot;: false, &quot;medicaid&quot;: false } } We can import different CMS-HCC model versions as seperate functions and use them in the same program. from sparknlp_jsl.functions import profile,profileV22,profileV23 df = df.withColumn(&quot;hcc_profileV24&quot;, profile(df.icd10_code, df.age, df.gender, df.eligibility, df.orec, df.medicaid )) df.withColumn(&quot;hcc_profileV22&quot;, profileV22(df.codes, df.age, df.sex,df.elig,df.orec,df.medicaid)) df.withColumn(&quot;hcc_profileV23&quot;, profileV23(df.codes, df.age, df.sex,df.elig,df.orec,df.medicaid)) df.show(truncate=False) +-++++--+-+--+ |risk_score|icd10_code |age|gender|eligibility|orec|medicaid| +-++++--+-+--+ |0.922 |[E1169, I5030, I509, E852] |64 |F |CFA |0 |true | |3.566 |[G629, D469, D6181] |77 |M |CND |1 |false | |1.181 |[D473, D473, D473, M069, C969]|16 |F |CPA |3 |true | +-++++--+-+--+ Updated Notebooks for resolvers and graph builders We have updated the resolver notebooks on spark-nlp-workshop repo with new BertSentenceChunkEmbeddings annotator. This annotator lets users aggregate sentence embeddings and ner chunk embeddings to get more specific and accurate resolution codes. It works by averaging context and chunk embeddings to get contextual information. Input to this annotator is the context (sentence) and ner chunks, while the output is embedding for each chunk that can be fed to the resolver model. The setChunkWeight parameter can be used to control the influence of surrounding context. Example below shows the comparison of old vs new approach. text ner_chunk entity icd10_code all_codes resolutions icd10_code_SCE all_codes_SCE resolutions_SCE Two weeks prior to presentation, she was treated with a five-day course of amoxicillin for a respiratory tract infection. a respiratory tract infection PROBLEM J988 [J988, J069, A499, J22, J209,…] [respiratory tract infection, upper respiratory tract infection, bacterial respiratory infection, acute respiratory infection, bronchial infection,…] Z870 [Z870, Z8709, J470, J988, A499,… [history of acute lower respiratory tract infection (situation), history of acute lower respiratory tract infection, bronchiectasis with acute lower respiratory infection, rti - respiratory tract infection, bacterial respiratory infection,… Here are the updated resolver notebooks: 3.Clinical_Entity_Resolvers.ipynb 24.Improved_Entity_Resolvers_in_SparkNLP_with_sBert.ipynb You can also check for more examples of this annotator: 24.1.Improved_Entity_Resolution_with_SentenceChunkEmbeddings.ipynb We have updated TF Graph builder notebook to show how to create TF graphs with TF2.x. Here is the updated notebook: 17.Graph_builder_for_DL_models.ipynb To see more, please check: Spark NLP Healthcare Workshop Repo New TF Graph Builder TF graph builder to create graphs and train DL models for licensed annotators (MedicalNer, Relation Extraction, Assertion and Generic Classifier) is made compatible with TF2.x. To see how to create TF Graphs, you can check here: 17.Graph_builder_for_DL_models.ipynb 3.2.0 We are glad to announce that Spark NLP Healthcare 3.2.0 has been released!. Highlights New Sentence Boundary Detection Model for Healthcare text New Assertion Status Models New Sentence Entity Resolver Model Finetuning Sentence Entity Resolvers with Your Data New Clinical NER Models New CMS-HCC risk-adjustment score calculation module New Embedding generation module for entity resolution New Sentence Boundary Detection Model for Healthcare text We are releasing an updated Sentence Boundary detection model to identify complex sentences containing multiple measurements, and punctuations. This model is trained on an in-house dataset. Example: Python: ... documenter = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencerDL = SentenceDetectorDLModel .pretrained(&quot;sentence_detector_dl_healthcare&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) text = &quot;&quot;&quot;He was given boluses of MS04 with some effect.he has since been placed on a PCA . He takes 80 mg. of ativan at home ativan for anxiety, with 20 meq kcl po, 30 mmol K-phos iv and 2 gms mag so4 iv. Size: Prostate gland measures 10x1.1x 4.9 cm (LS x AP x TS). Estimated volume is 51.9 ml. and is mildly enlarged in size.Normal delineation pattern of the prostate gland is preserved. &quot;&quot;&quot; sd_model = LightPipeline(PipelineModel(stages=[documenter, sentencerDL])) result = sd_model.fullAnnotate(text) Results: | s.no | sentences | |--:|:| | 0 | He was given boluses of MS04 with some effect. | | 1 | he has since been placed on a PCA . | | 2 | He takes 80 mg. of ativan at home ativan for anxiety, | | | with 20 meq kcl po, 30 mmol K-phos iv and 2 gms mag so4 iv. | | 3 | Size: Prostate gland measures 10x1.1x 4.9 cm (LS x AP x TS). | | 4 | Estimated volume is | | | 51.9 ml. and is mildly enlarged in size. | | 5 | Normal delineation pattern of the prostate gland is preserved. | New Assertion Status Models We are releasing two new Assertion Status Models based on the BiLSTM architecture. Apart from what we released in other assertion models, an in-house annotations on a curated dataset (6K clinical notes) is used to augment the base assertion dataset (2010 i2b2/VA). assertion_jsl: This model can classify the assertions made on given medical concepts as being Present, Absent, Possible, Planned, Someoneelse, Past, Family, None, Hypotetical. assertion_jsl_large: This model can classify the assertions made on given medical concepts as being present, absent, possible, planned, someoneelse, past. assertion_dl vs assertion_jsl: chunks entities assertion_dl assertion_jsl Mesothelioma PROBLEM present Present CVA PROBLEM absent Absent cancer PROBLEM associated_with_someone_else Family her INR TEST present Planned Amiodarone TREATMENT hypothetical Hypothetical lymphadenopathy PROBLEM absent Absent stage III disease PROBLEM possible Possible IV piggyback TREATMENT conditional Past Example: Python: ... clinical_assertion = AssertionDLModel.pretrained(&quot;assertion_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) nlpPipeline = Pipeline(stages=[documentAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, ner_converter, clinical_assertion]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) result = model.transform(spark.createDataFrame([[&quot;The patient is a 41-year-old and has a nonproductive cough that started last week. She has had right-sided chest pain radiating to her back with fever starting today. She has no nausea. She has a history of pericarditis and pericardectomy in May 2006 and developed cough with right-sided chest pain, and went to an urgent care center and Chest x-ray revealed right-sided pleural effusion. In family history, her father has a colon cancer history.&quot;]], [&quot;text&quot;]) Results: +-+--++-+-++ |chunk |begin|end|ner_label |sent_id|assertion| +-+--++-+-++ |nonproductive cough|35 |53 |Symptom |0 |Present | |last week |68 |76 |RelativeDate |0 |Past | |chest pain |103 |112|Symptom |1 |Present | |fever |141 |145|VS_Finding |1 |Present | |today |156 |160|RelativeDate |1 |Present | |nausea |174 |179|Symptom |2 |Absent | |pericarditis |203 |214|Disease_Syndrome_Disorder|3 |Past | |pericardectomy |220 |233|Procedure |3 |Past | |May 2006 |238 |245|Date |3 |Past | |cough |261 |265|Symptom |3 |Past | |chest pain |284 |293|Symptom |3 |Past | |Chest x-ray |334 |344|Test |3 |Past | |pleural effusion |367 |382|Disease_Syndrome_Disorder|3 |Past | |colon cancer |421 |432|Oncological |4 |Family | +-+--++-+-++ New Sentence Entity Resolver Model We are releasing sbiobertresolve_rxnorm_disposition model that maps medication entities (like drugs/ingredients) to RxNorm codes and their dispositions using sbiobert_base_cased_mli Sentence Bert Embeddings. In the result, look for the aux_label parameter in the metadata to get dispositions that were divided by |. Example: Python: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbiobert_base_cased_mli&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) rxnorm_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_rxnorm_disposition&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;rxnorm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, rxnorm_resolver ]) rxnorm_lp = LightPipeline(pipelineModel) result = rxnorm_lp.fullAnnotate(&quot;belimumab 80 mg/ml injectable solution&quot;) Results: | | chunks | code | resolutions | all_codes | all_k_aux_labels | all_distances | |:|:--|:--|:--|:--|:--|:-| | 0 |belimumab 80 mg/ml injectable solution | 1092440 | [belimumab 80 mg/ml injectable solution, belimumab 80 mg/ml injectable solution [benlysta], ifosfamide 80 mg/ml injectable solution, belimumab 80 mg/ml [benlysta], belimumab 80 mg/ml, ...]| [1092440, 1092444, 107034, 1092442, 1092438, ...] | [Immunomodulator, Immunomodulator, Alkylating agent, Immunomodulator, Immunomodulator, ...] | [0.0000, 0.0145, 0.0479, 0.0619, 0.0636, ...] | Finetuning Sentence Entity Resolvers with Your Data Instead of starting from scratch when training a new Sentence Entity Resolver model, you can train a new model by adding your new data to the pretrained model. There’s a new method setPretrainedModelPath(path), which allows you to point the training process to an existing model, and allows you to initialize your model with the data from the pretrained model. When both the new data and the pretrained model contain the same code, you will see both of the results at the top. Here is a sample notebook : Finetuning Sentence Entity Resolver Model Notebook Example: In the example below, we changed the code of sepsis to X1234 and re-retrain the main ICD10-CM model with this new dataset. So we want to see the X1234 code as a result in the all_codes. Python: ... bertExtractor = SentenceEntityResolverApproach() .setNeighbours(50) .setThreshold(1000) .setInputCols(&quot;sentence_embeddings&quot;) .setNormalizedCol(&quot;description_normalized&quot;) # concept_name .setLabelCol(&quot;code&quot;) # concept_code .setOutputCol(&quot;recognized_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) .setCaseSensitive(False) .setUseAuxLabel(True) # if exist .setPretrainedModelPath(&quot;path_to_a_pretrained_model&quot;) new_model = bertExtractor.fit(&quot;new_dataset&quot;) new_model.save(&quot;models/new_resolver_model&quot;) # save and use later ... resolver_model = SentenceEntityResolverModel.load(&quot;models/new_resolver_model&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;output_code&quot;) pipelineModel = PipelineModel( stages = [ documentAssembler, sentence_embedder, resolver_model]) light_model = LightPipeline(pipelineModel) light_model.fullAnnotate(&quot;sepsis&quot;) Main Model Results: chunks begin end code all_codes resolutions all_k_aux_labels all_distances sepsis 0 5 A4189 [A4189, L419, A419, A267, E771, …] [sepsis [Other specified sepsis], parapsoriasis [Parapsoriasis, unspecified], postprocedural sepsis [Sepsis, unspecified organism], erysipelothrix sepsis [Erysipelothrix sepsis], fucosidosis [Defects in glycoprotein degradation], … ] [1|1|2, 1|1|2, 1|1|2, 1|1|2, 1|1|23, …] [0.0000, 0.2079, 0.2256, 0.2359, 0.2399,…] Re-Trained Model Results: chunks begin end code all_codes resolutions all_k_aux_labels all_distances sepsis 0 5 X1234 [X1234, A4189, A419, L419, A267, …] [sepsis [Sepsis, new resolution], sepsis [Other specified sepsis], SEPSIS [Sepsis, unspecified organism], parapsoriasis [Parapsoriasis, unspecified], erysipelothrix sepsis [Erysipelothrix sepsis], … ] [1|1|74, 1|1|2, 1|1|2, 1|1|2, 1|1|2, …] [0.0000, 0.0000, 0.0000, 0.2079, 0.2359, …] New Clinical NER Models ner_jsl_slim: This model is trained based on ner_jsl model with more generalized entities. (Death_Entity, Medical_Device, Vital_Sign, Alergen, Drug, Clinical_Dept, Lifestyle, Symptom, Body_Part, Physical_Measurement, Admission_Discharge, Date_Time, Age, Birth_Entity, Header, Oncological, Substance_Quantity, Test_Result, Test, Procedure, Treatment, Disease_Syndrome_Disorder, Pregnancy_Newborn, Demographics) ner_jsl vs ner_jsl_slim: chunks ner_jsl ner_jsl_slim Description: Section_Header Header atrial fibrillation Heart_Disease Disease_Syndrome_Disorder August 24, 2007 Date Date_Time transpleural fluoroscopy Procedure Test last week RelativeDate Date_Time She Gender Demographics fever VS_Finding Vital_Sign PAST MEDICAL HISTORY: Medical_History_Header Header Pericardial window Internal_organ_or_component Body_Part FAMILY HISTORY: Family_History_Header Header CVA Cerebrovascular_Disease Disease_Syndrome_Disorder diabetes Diabetes Disease_Syndrome_Disorder married Relationship_Status Demographics alcohol Alcohol Lifestyle illicit drug Substance Lifestyle Coumadin Drug_BrandName Drug Blood pressure 123/95 Blood_Pressure Vital_Sign heart rate 83 Pulse Vital_Sign anticoagulated Drug_Ingredient Drug Example: Python: ... embeddings_clinical = WordEmbeddingsModel().pretrained(&#39;embeddings_clinical&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&#39;sentence&#39;, &#39;token&#39;]) .setOutputCol(&#39;embeddings&#39;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_jsl_slim&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings_clinical, clinical_ner, ner_converter]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame([[&quot;HISTORY: 30-year-old female presents for digital bilateral mammography secondary to a soft tissue lump palpated by the patient in the upper right shoulder. The patient has a family history of breast cancer within her mother at age 58. Patient denies personal history of breast cancer.&quot;]], [&quot;text&quot;])) Results: | | chunk | entity | |:|:--|:-| | 0 | HISTORY: | Header | | 1 | 30-year-old | Age | | 2 | female | Demographics | | 3 | mammography | Test | | 4 | soft tissue lump | Symptom | | 5 | shoulder | Body_Part | | 6 | breast cancer | Oncological | | 7 | her mother | Demographics | | 8 | age 58 | Age | | 9 | breast cancer | Oncological | ner_jsl_biobert : This model is the BioBert version of ner_jsl model and trained with biobert_pubmed_base_cased embeddings. ner_jsl_greedy_biobert : This model is the BioBert version of ner_jsl_greedy models and trained with biobert_pubmed_base_cased embeddings. Example: Python: ... embeddings_clinical = BertEmbeddings.pretrained(&#39;biobert_pubmed_base_cased&#39;) .setInputCols([&#39;sentence&#39;, &#39;token&#39;]) .setOutputCol(&#39;embeddings&#39;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_jsl_greedy_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings_clinical, clinical_ner, ner_converter]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame([[&quot;The patient is a 21-day-old Caucasian male here for 2 days of congestion - mom has been suctioning yellow discharge from the patient&#39;s nares, plus she has noticed some mild problems with his breathing while feeding (but negative for any perioral cyanosis or retractions). One day ago, mom also noticed a tactile temperature and gave the patient Tylenol. Baby also has had some decreased p.o. intake. His normal breast-feeding is down from 20 minutes q.2h. to 5 to 10 minutes secondary to his respiratory congestion. He sleeps well, but has been more tired and has been fussy over the past 2 days. The parents noticed no improvement with albuterol treatments given in the ER. His urine output has also decreased; normally he has 8 to 10 wet and 5 dirty diapers per 24 hours, now he has down to 4 wet diapers per 24 hours. Mom denies any diarrhea. His bowel movements are yellow colored and soft in nature.&quot;]], [&quot;text&quot;])) Results: | | chunk | entity | |:|:--|:--| | 0 | 21-day-old | Age | | 1 | Caucasian | Race_Ethnicity | | 2 | male | Gender | | 3 | for 2 days | Duration | | 4 | congestion | Symptom | | 5 | mom | Gender | | 6 | suctioning yellow discharge | Symptom | | 7 | nares | External_body_part_or_region | | 8 | she | Gender | | 9 | mild problems with his breathing while feeding | Symptom | | 10 | perioral cyanosis | Symptom | | 11 | retractions | Symptom | | 12 | One day ago | RelativeDate | | 13 | mom | Gender | | 14 | tactile temperature | Symptom | | 15 | Tylenol | Drug | | 16 | Baby | Age | | 17 | decreased p.o. intake | Symptom | | 18 | His | Gender | | 19 | breast-feeding | External_body_part_or_region | | 20 | q.2h | Frequency | | 21 | to 5 to 10 minutes | Duration | | 22 | his | Gender | | 23 | respiratory congestion | Symptom | | 24 | He | Gender | | 25 | tired | Symptom | | 26 | fussy | Symptom | | 27 | over the past 2 days | RelativeDate | | 28 | albuterol | Drug | | 29 | ER | Clinical_Dept | | 30 | His | Gender | | 31 | urine output has also decreased | Symptom | | 32 | he | Gender | | 33 | per 24 hours | Frequency | | 34 | he | Gender | | 35 | per 24 hours | Frequency | | 36 | Mom | Gender | | 37 | diarrhea | Symptom | | 38 | His | Gender | | 39 | bowel | Internal_organ_or_component | New CMS-HCC risk-adjustment score calculation module We are releasing a new module to calculate medical risk adjusment score by using the Centers for Medicare &amp; Medicaid Service (CMS) risk adjustment model. The main input to this model are ICD codes of the diseases. After getting ICD codes of diseases by Spark NLP Healthcare ICD resolvers, risk score can be calculated by this module in spark environment. Current supported version for the model is CMS-HCC V24. The model needs following parameters in order to calculate the risk score: ICD Codes Age Gender The eligibility segment of the patient Original reason for entitlement If the patient is in Medicaid or not If the patient is disabled or not Example: Python: sample_patients.show() Results: +-++++ |Patient_ID|ICD_codes |Age|Gender| +-++++ |101 |[E1169, I5030, I509, E852] |64 |F | |102 |[G629, D469, D6181] |77 |M | |103 |[D473, D473, D473, M069, C969]|16 |F | +-++++ Python: from sparknlp_jsl.functions import profile df = df.withColumn(&quot;hcc_profile&quot;, profile(df.ICD_codes, df.Age, df.Gender)) df = df.withColumn(&quot;hcc_profile&quot;, F.from_json(F.col(&quot;hcc_profile&quot;), schema)) df= df.withColumn(&quot;risk_score&quot;, df.hcc_profile.getItem(&quot;risk_score&quot;)) .withColumn(&quot;hcc_lst&quot;, df.hcc_profile.getItem(&quot;hcc_map&quot;)) .withColumn(&quot;parameters&quot;, df.hcc_profile.getItem(&quot;parameters&quot;)) .withColumn(&quot;details&quot;, df.hcc_profile.getItem(&quot;details&quot;)) df.select(&#39;Patient_ID&#39;, &#39;risk_score&#39;,&#39;ICD_codes&#39;, &#39;Age&#39;, &#39;Gender&#39;).show(truncate=False ) df.show(truncate=100, vertical=True) Results: +-+-++++ |Patient_ID|risk_score|ICD_codes |Age|Gender| +-+-++++ |101 |0.827 |[E1169, I5030, I509, E852] |64 |F | |102 |1.845 |[G629, D469, D6181] |77 |M | |103 |1.288 |[D473, D473, D473, M069, C969]|16 |F | +-+-++++ RECORD 0- Patient_ID | 101 ICD_codes | [E1169, I5030, I509, E852] Age | 64 Gender | F Eligibility_Segment | CNA OREC | 0 Medicaid | false Disabled | false hcc_profile | {{&quot;CNA_HCC18&quot;:0.302,&quot;CNA_HCC85&quot;:0.331,&quot;CNA_HCC23&quot;:0.194,&quot;CNA_D3&quot;:0.0,&quot;CNA_HCC85_gDiabetesMellit&quot;:... risk_score | 0.827 hcc_lst | {&quot;E1169&quot;:[&quot;HCC18&quot;],&quot;I5030&quot;:[&quot;HCC85&quot;],&quot;I509&quot;:[&quot;HCC85&quot;],&quot;E852&quot;:[&quot;HCC23&quot;]} parameters | {&quot;elig&quot;:&quot;CNA&quot;,&quot;age&quot;:64,&quot;sex&quot;:&quot;F&quot;,&quot;origds&quot;:&#39;0&#39;,&quot;disabled&quot;:false,&quot;medicaid&quot;:false} details | {&quot;CNA_HCC18&quot;:0.302,&quot;CNA_HCC85&quot;:0.331,&quot;CNA_HCC23&quot;:0.194,&quot;CNA_D3&quot;:0.0,&quot;CNA_HCC85_gDiabetesMellit&quot;:0.0} -RECORD 1- Patient_ID | 102 ICD_codes | [G629, D469, D6181] Age | 77 Gender | M Eligibility_Segment | CNA OREC | 0 Medicaid | false Disabled | false hcc_profile | {{&quot;CNA_M75_79&quot;:0.473,&quot;CNA_D1&quot;:0.0,&quot;CNA_HCC46&quot;:1.372}, [&quot;D1&quot;,&quot;HCC46&quot;], {&quot;D469&quot;:[&quot;HCC46&quot;]}, {&quot;elig&quot;... risk_score | 1.845 hcc_lst | {&quot;D469&quot;:[&quot;HCC46&quot;]} parameters | {&quot;elig&quot;:&quot;CNA&quot;,&quot;age&quot;:77,&quot;sex&quot;:&quot;M&quot;,&quot;origds&quot;:&#39;0&#39;,&quot;disabled&quot;:false,&quot;medicaid&quot;:false} details | {&quot;CNA_M75_79&quot;:0.473,&quot;CNA_D1&quot;:0.0,&quot;CNA_HCC46&quot;:1.372} -RECORD 2- Patient_ID | 103 ICD_codes | [D473, D473, D473, M069, C969] Age | 16 Gender | F Eligibility_Segment | CNA OREC | 0 Medicaid | false Disabled | false hcc_profile | {{&quot;CNA_HCC10&quot;:0.675,&quot;CNA_HCC40&quot;:0.421,&quot;CNA_HCC48&quot;:0.192,&quot;CNA_D3&quot;:0.0}, [&quot;HCC10&quot;,&quot;HCC40&quot;,&quot;HCC48&quot;,&quot;... risk_score | 1.288 hcc_lst | {&quot;D473&quot;:[&quot;HCC48&quot;],&quot;M069&quot;:[&quot;HCC40&quot;],&quot;C969&quot;:[&quot;HCC10&quot;]} parameters | {&quot;elig&quot;:&quot;CNA&quot;,&quot;age&quot;:16,&quot;sex&quot;:&quot;F&quot;,&quot;origds&quot;:&#39;0&#39;,&quot;disabled&quot;:false,&quot;medicaid&quot;:false} details | {&quot;CNA_HCC10&quot;:0.675,&quot;CNA_HCC40&quot;:0.421,&quot;CNA_HCC48&quot;:0.192,&quot;CNA_D3&quot;:0.0} Here is a sample notebook : Calculating Medicare Risk Adjustment Score New Embedding generation module for entity resolution We are releasing a new annotator BertSentenceChunkEmbeddings to let users aggregate sentence embeddings and ner chunk embeddings to get more specific and accurate resolution codes. It works by averaging context and chunk embeddings to get contextual information. This is specially helpful when ner chunks do not have additional information (like body parts or severity) as explained in the example below. Input to this annotator is the context (sentence) and ner chunks, while the output is embedding for each chunk that can be fed to the resolver model. The setChunkWeight parameter can be used to control the influence of surrounding context. Example below shows the comparison of old vs new approach. Sample Notebook: Improved_Entity_Resolution_with_SentenceChunkEmbeddings Example: Python: ... sentence_chunk_embeddings = BertSentenceChunkEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;ner_chunk&quot;]) .setOutputCol(&quot;sentence_chunk_embeddings&quot;) .setChunkWeight(0.5) resolver = SentenceEntityResolverModel.pretrained(&#39;sbiobertresolve_icd10cm&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_chunk_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) text = &quot;&quot;&quot;A 20 year old female patient badly tripped while going down stairs. She complains of right leg pain. Her x-ray showed right hip fracture. Hair line fractures also seen on the left knee joint. She also suffered from trauma and slight injury on the head. OTHER CONDITIONS: She was also recently diagnosed with diabetes, which is of type 2. &quot;&quot;&quot; nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings_clinical, clinical_ner, ner_converter, sentence_chunk_embeddings, resolver]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame([[text]], [&quot;text&quot;])) Results: | | chunk | entity | code_with_old_approach | resolutions_with_old_approach | code_with_new_approach | resolutions_with_new_approach | |:|:--|:--|:--|:--|:--|:-| | 0 | leg pain | Symptom | R1033 | Periumbilical pain | M79661 | Pain in right lower leg | | 1 | hip fracture | Injury_or_Poisoning | M84459S | Pathological fracture, hip, unspecified, sequela | M84451S | Pathological fracture, right femur, sequela | | 2 | Hair line fractures | Injury_or_Poisoning | S070XXS | Crushing injury of face, sequela | S92592P | Other fracture of left lesser toe(s), subsequent encounter for fracture with malunion | | 3 | trauma | Injury_or_Poisoning | T794XXS | Traumatic shock, sequela | S0083XS | Contusion of other part of head, sequela | | 4 | slight injury | Injury_or_Poisoning | B03 | Smallpox | S0080XD | Unspecified superficial injury of other part of head, subsequent encounter | | 5 | diabetes | Diabetes | E118 | Type 2 diabetes mellitus with unspecified complications | E1169 | Type 2 diabetes mellitus with other specified complication | To see more, please check : Spark NLP Healthcare Workshop Repo 3.1.3 We are glad to announce that Spark NLP for Healthcare 3.1.3 has been released!. This release comes with new features, new models, bug fixes, and examples. Highlights New Relation Extraction model and a Pretrained pipeline for extracting and linking ADEs New Entity Resolver model for SNOMED codes ChunkConverter Annotator BugFix: getAnchorDateMonth method in DateNormalizer. BugFix: character map in MedicalNerModel fine-tuning. New Relation Extraction model and a Pretrained pipeline for extracting and linking ADEs We are releasing a new Relation Extraction Model for ADEs. This model is trained using Bert Word embeddings (biobert_pubmed_base_cased), and is capable of linking ADEs and Drugs. Example: re_model = RelationExtractionModel() .pretrained(&quot;re_ade_biobert&quot;, &quot;en&quot;, &#39;clinical/models&#39;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setMaxSyntacticDistance(3) #default: 0 .setPredictionThreshold(0.5) #default: 0.5 .setRelationPairs([&quot;ade-drug&quot;, &quot;drug-ade&quot;]) # Possible relation pairs. Default: All Relations. nlp_pipeline = Pipeline(stages=[documenter, sentencer, tokenizer, words_embedder, pos_tagger, ner_tagger, ner_chunker, dependency_parser, re_model]) light_pipeline = LightPipeline(nlp_pipeline.fit(spark.createDataFrame([[&#39;&#39;]]).toDF(&quot;text&quot;))) text =&quot;&quot;&quot;Been taking Lipitor for 15 years , have experienced sever fatigue a lot!!! . Doctor moved me to voltaren 2 months ago , so far , have only experienced cramps&quot;&quot;&quot; annotations = light_pipeline.fullAnnotate(text) We also have a new pipeline comprising of all models related to ADE(Adversal Drug Event) as part of this release. This pipeline includes classification, NER, assertion and relation extraction models. Users can now use this pipeline to get classification result, ADE and Drug entities, assertion status for ADE entities, and relations between ADE and Drug entities. Example: pretrained_ade_pipeline = PretrainedPipeline(&#39;explain_clinical_doc_ade&#39;, &#39;en&#39;, &#39;clinical/models&#39;) result = pretrained_ade_pipeline.fullAnnotate(&quot;&quot;&quot;Been taking Lipitor for 15 years , have experienced sever fatigue a lot!!! . Doctor moved me to voltaren 2 months ago , so far , have only experienced cramps&quot;&quot;&quot;)[0] Results: Class: True NER_Assertion: | | chunk | entitiy | assertion | |-|-||-| | 0 | Lipitor | DRUG | - | | 1 | sever fatigue | ADE | Conditional | | 2 | voltaren | DRUG | - | | 3 | cramps | ADE | Conditional | Relations: | | chunk1 | entitiy1 | chunk2 | entity2 | relation | |-|-||-||-| | 0 | sever fatigue | ADE | Lipitor | DRUG | 1 | | 1 | cramps | ADE | Lipitor | DRUG | 0 | | 2 | sever fatigue | ADE | voltaren | DRUG | 0 | | 3 | cramps | ADE | voltaren | DRUG | 1 | New Entity Resolver model for SNOMED codes We are releasing a new SentenceEntityResolver model for SNOMED codes. This model also includes AUX SNOMED concepts and can find codes for Morph Abnormality, Procedure, Substance, Physical Object, and Body Structure entities. In the metadata, the all_k_aux_labels can be divided to get further information: ground truth, concept, and aux . In the example shared below the ground truth is Atherosclerosis, concept is Observation, and aux is Morph Abnormality. Example: snomed_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_snomed_findings_aux_concepts&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) snomed_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, snomed_resolver]) snomed_lp = LightPipeline(snomed_pipelineModel) result = snomed_lp.fullAnnotate(&quot;atherosclerosis&quot;) Results: | | chunks | code | resolutions | all_codes | all_k_aux_labels | all_distances | |:|:-|:|:-|:|:|:| | 0 | atherosclerosis | 38716007 | [atherosclerosis, atherosclerosis, atherosclerosis, atherosclerosis, atherosclerosis, atherosclerosis, atherosclerosis artery, coronary atherosclerosis, coronary atherosclerosis, coronary atherosclerosis, coronary atherosclerosis, coronary atherosclerosis, arteriosclerosis, carotid atherosclerosis, cardiovascular arteriosclerosis, aortic atherosclerosis, aortic atherosclerosis, atherosclerotic ischemic disease] | [38716007, 155382007, 155414001, 195251000, 266318005, 194848007, 441574008, 443502000, 41702007, 266231003, 155316000, 194841001, 28960008, 300920004, 39468009, 155415000, 195252007, 129573006] | &#39;Atherosclerosis&#39;, &#39;Observation&#39;, &#39;Morph Abnormality&#39; | [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0280, 0.0451, 0.0451, 0.0451, 0.0451, 0.0451, 0.0462, 0.0477, 0.0466, 0.0490, 0.0490, 0.0485 | ChunkConverter Annotator Allows to use RegexMather chunks as NER chunks and feed the output to the downstream annotators like RE or Deidentification. document_assembler = DocumentAssembler().setInputCol(&#39;text&#39;).setOutputCol(&#39;document&#39;) sentence_detector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) regex_matcher = RegexMatcher() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;regex&quot;) .setExternalRules(path=&quot;../src/test/resources/regex-matcher/rules.txt&quot;,delimiter=&quot;,&quot;) chunkConverter = ChunkConverter().setInputCols(&quot;regex&quot;).setOutputCol(&quot;chunk&quot;) 3.1.2 We are glad to announce that Spark NLP for Healthcare 3.1.2 has been released!. This release comes with new features, new models, bug fixes, and examples. Highlights Support for Fine-tuning of Ner models. More builtin(pre-defined) graphs for MedicalNerApproach. Date Normalizer. New Relation Extraction Models for ADE. Bug Fixes. Support for user-defined Custom Transformer. Java Workshop Examples. Deprecated Compatibility class in Python. Support for Fine Tuning of Ner models Users can now resume training/fine-tune existing(already trained) Spark NLP MedicalNer models on new data. Users can simply provide the path to any existing MedicalNer model and train it further on the new dataset: ner_tagger = MedicalNerApproach().setPretrainedModelPath(&quot;/path/to/trained/medicalnermodel&quot;) If the new dataset contains new tags/labels/entities, users can choose to override existing tags with the new ones. The default behaviour is to reset the list of existing tags and generate a new list from the new dataset. It is also possible to preserve the existing tags by setting the ‘overrideExistingTags’ parameter: ner_tagger = MedicalNerApproach() .setPretrainedModelPath(&quot;/path/to/trained/medicalnermodel&quot;) .setOverrideExistingTags(False) Setting overrideExistingTags to false is intended to be used when resuming trainig on the same, or very similar dataset (i.e. with the same tags or with just a few different ones). If tags overriding is disabled, and new tags are found in the training set, then the approach will try to allocate them to unused output nodes, if any. It is also possible to override specific tags of the old model by mapping them to new tags: ner_tagger = MedicalNerApproach() .setPretrainedModelPath(&quot;/path/to/trained/medicalnermodel&quot;) .setOverrideExistingTags(False) .setTagsMapping(&quot;B-PER,B-VIP&quot;, &quot;I-PER,I-VIP&quot;) In this case, the new tags B-VIP and I-VIP will replace the already trained tags ‘B-PER’ and ‘I-PER’. Unmapped old tags will remain in use and unmapped new tags will be allocated to new outpout nodes, if any. Jupyter Notebook: [Finetuning Medical NER Model Notebook] (https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.5.Resume_MedicalNer_Model_Training.ipynb) More builtin graphs for MedicalNerApproach Seventy new TensorFlow graphs have been added to the library of available graphs which are used to train MedicalNer models. The graph with the optimal set of parameters is automatically chosen by MedicalNerApproach. DateNormalizer New annotator that normalize dates to the format YYYY/MM/DD. This annotator identifies dates in chunk annotations, and transform these dates to the format YYYY/MM/DD. Both the input and output formats for the annotator are chunk. Example: sentences = [ [&#39;08/02/2018&#39;], [&#39;11/2018&#39;], [&#39;11/01/2018&#39;], [&#39;12Mar2021&#39;], [&#39;Jan 30, 2018&#39;], [&#39;13.04.1999&#39;], [&#39;3April 2020&#39;], [&#39;next monday&#39;], [&#39;today&#39;], [&#39;next week&#39;], ] df = spark.createDataFrame(sentences).toDF(&quot;text&quot;) document_assembler = DocumentAssembler().setInputCol(&#39;text&#39;).setOutputCol(&#39;document&#39;) chunksDF = document_assembler.transform(df) aa = map_annotations_col(chunksDF.select(&quot;document&quot;), lambda x: [Annotation(&#39;chunk&#39;, a.begin, a.end, a.result, a.metadata, a.embeddings) for a in x], &quot;document&quot;, &quot;chunk_date&quot;, &quot;chunk&quot;) dateNormalizer = DateNormalizer().setInputCols(&#39;chunk_date&#39;).setOutputCol(&#39;date&#39;).setAnchorDateYear(2021).setAnchorDateMonth(2).setAnchorDateDay(27) dateDf = dateNormalizer.transform(aa) dateDf.select(&quot;date.result&quot;,&quot;text&quot;).show() +--+-+ |text | date | +--+-+ |08/02/2018 |2018/08/02| |11/2018 |2018/11/DD| |11/01/2018 |2018/11/01| |12Mar2021 |2021/03/12| |Jan 30, 2018|2018/01/30| |13.04.1999 |1999/04/13| |3April 2020 |2020/04/03| |next Monday |2021/06/19| |today |2021/06/12| |next week |2021/06/19| +--+-+ New Relation Extraction Models for ADE We are releasing new Relation Extraction models for ADE (Adverse Drug Event). This model is available in both RelationExtraction and Bert based RelationExtractionDL versions, and is capabale of linking drugs with ADE mentions. Example ade_re_model = new RelationExtractionModel().pretrained(&#39;ner_ade_clinical&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunk&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setPredictionThreshold(0.5) .setRelationPairs([&#39;ade-drug&#39;, &#39;drug-ade&#39;]) pipeline = Pipeline(stages=[documenter, sentencer, tokenizer, pos_tagger, words_embedder, ner_tagger, ner_converter, dependency_parser, re_ner_chunk_filter, re_model]) text =&quot;&quot;&quot;A 30 year old female presented with tense bullae due to excessive use of naproxin, and leg cramps relating to oxaprozin.&quot;&quot;&quot; p_model = pipeline.fit(spark.createDataFrame([[text]]).toDF(&quot;text&quot;)) result = p_model.transform(data) Results | | chunk1 | entity1 | chunk2 | entity2 | result | |:|:--|:--|:--|:-|--:| | 0 | tense bullae | ADE | naproxin | DRUG | 1 | | 1 | tense bullae | ADE | oxaprozin | DRUG | 0 | | 2 | naproxin | DRUG | leg cramps | ADE | 0 | | 3 | leg cramps | ADE | oxaprozin | DRUG | 1 | Benchmarking Model: re_ade_clinical precision recall f1-score support 0 0.85 0.89 0.87 1670 1 0.88 0.84 0.86 1673 micro avg 0.87 0.87 0.87 3343 macro avg 0.87 0.87 0.87 3343 weighted avg 0.87 0.87 0.87 3343 Model: redl_ade_biobert Relation Recall Precision F1 Support 0 0.894 0.946 0.919 1011 1 0.963 0.926 0.944 1389 Avg. 0.928 0.936 0.932 Weighted Avg. 0.934 0.934 0.933 Bug Fixes RelationExtractionDLModel had an issue(BufferOverflowException) on versions 3.1.0 and 3.1.1, which is fixed with this release. Some pretrained RelationExtractionDLModels got outdated after release 3.0.3, new updated models were created, tested and made available to be used with versions 3.0.3, and later. Some SentenceEntityResolverModels which did not work with Spark 2.4/2.3 were fixed. Support for user-defined Custom Transformer. Utility classes to define custom transformers in python are included in this release. This allows users to define functions in Python to manipulate Spark-NLP annotations. This new Transformers can be added to pipelines like any of the other models you’re already familiar with. Example how to use the custom transformer. def myFunction(annotations): # lower case the content of the annotations return [a.copy(a.result.lower()) for a in annotations] custom_transformer = CustomTransformer(f=myFunction).setInputCol(&quot;ner_chunk&quot;).setOutputCol(&quot;custom&quot;) outputDf = custom_transformer.transform(outdf).select(&quot;custom&quot;).toPandas() Java Workshop Examples Add Java examples in the workshop repository. https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/java/healthcare Deprecated Compatibility class in Python Due to active release cycle, we are adding &amp; training new pretrained models at each release and it might be tricky to maintain the backward compatibility or keep up with the latest models and versions, especially for the users using our models locally in air-gapped networks. We are releasing a new utility class to help you check your local &amp; existing models with the latest version of everything we have up to date. You will not need to specify your AWS credentials from now on. This new class is now replacing the previous Compatibility class written in Python and CompatibilityBeta class written in Scala. from sparknlp_jsl.compatibility import Compatibility compatibility = Compatibility(spark) print(compatibility.findVersion(&#39;sentence_detector_dl_healthcare&#39;)) Output [{&#39;name&#39;: &#39;sentence_detector_dl_healthcare&#39;, &#39;sparkVersion&#39;: &#39;2.4&#39;, &#39;version&#39;: &#39;2.6.0&#39;, &#39;language&#39;: &#39;en&#39;, &#39;date&#39;: &#39;2020-09-13T14:44:42.565&#39;, &#39;readyToUse&#39;: &#39;true&#39;}, {&#39;name&#39;: &#39;sentence_detector_dl_healthcare&#39;, &#39;sparkVersion&#39;: &#39;2.4&#39;, &#39;version&#39;: &#39;2.7.0&#39;, &#39;language&#39;: &#39;en&#39;, &#39;date&#39;: &#39;2021-03-16T08:42:34.391&#39;, &#39;readyToUse&#39;: &#39;true&#39;}] 3.1.1 We are glad to announce that Spark NLP for Healthcare 3.1.1 has been released! Highlights MedicalNerModel new parameter includeAllConfidenceScores. MedicalNerModel new parameter inferenceBatchSize. New Resolver Models Updated Resolver Models Getting Started with Spark NLP for Healthcare Notebook in Databricks MedicalNer new parameter includeAllConfidenceScores You can now customize whether you will require confidence score for every token(both entities and non-entities) at the output of the MedicalNerModel, or just for the tokens recognized as entities. MedicalNerModel new parameter inferenceBatchSize You can now control the batch size used during inference as a separate parameter from the one you used during training of the model. This can be useful in the situation in which the hardware on which you run inference has different capacity. For example, when you have lower available memory during inference, you can reduce the batch size. New Resolver Models We trained three new sentence entity resolver models. sbertresolve_snomed_bodyStructure_med and sbiobertresolve_snomed_bodyStructure models map extracted medical (anatomical structures) entities to Snomed codes (body structure version). sbertresolve_snomed_bodyStructure_med : Trained with using sbert_jsl_medium_uncased embeddings. sbiobertresolve_snomed_bodyStructure : Trained with using sbiobert_base_cased_mli embeddings. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) jsl_sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbert_jsl_medium_uncased&#39;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) snomed_resolver = SentenceEntityResolverModel.pretrained(&quot;sbertresolve_snomed_bodyStructure_med, &quot;en&quot;, &quot;clinical/models) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) snomed_pipelineModel = PipelineModel( stages = [ documentAssembler, jsl_sbert_embedder, snomed_resolver]) snomed_lp = LightPipeline(snomed_pipelineModel) result = snomed_lp.fullAnnotate(&quot;Amputation stump&quot;) Result: | | chunks | code | resolutions | all_codes | all_distances | |:|:--|:|:|:|:-| | 0 | amputation stump | 38033009 | [Amputation stump, Amputation stump of upper limb, Amputation stump of left upper limb, Amputation stump of lower limb, Amputation stump of left lower limb, Amputation stump of right upper limb, Amputation stump of right lower limb, ...]| [&#39;38033009&#39;, &#39;771359009&#39;, &#39;771364008&#39;, &#39;771358001&#39;, &#39;771367001&#39;, &#39;771365009&#39;, &#39;771368006&#39;, ...] | [&#39;0.0000&#39;, &#39;0.0773&#39;, &#39;0.0858&#39;, &#39;0.0863&#39;, &#39;0.0905&#39;, &#39;0.0911&#39;, &#39;0.0972&#39;, ...] | sbiobertresolve_icdo_augmented : This model maps extracted medical entities to ICD-O codes using sBioBert sentence embeddings. This model is augmented using the site information coming from ICD10 and synonyms coming from SNOMED vocabularies. It is trained with a dataset that is 20x larger than the previous version of ICDO resolver. Given the oncological entity found in the text (via NER models like ner_jsl), it returns top terms and resolutions along with the corresponding ICD-10 codes to present more granularity with respect to body parts mentioned. It also returns the original histological behavioral codes and descriptions in the aux metadata. Example: ... chunk2doc = Chunk2Doc().setInputCols(&quot;ner_chunk&quot;).setOutputCol(&quot;ner_chunk_doc&quot;) sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) icdo_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_icdo_augmented&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, word_embeddings, clinical_ner, ner_converter, chunk2doc, sbert_embedder, icdo_resolver]) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = nlpPipeline.fit(empty_data) results = model.transform(spark.createDataFrame([[&quot;The patient is a very pleasant 61-year-old female with a strong family history of colon polyps. The patient reports her first polyps noted at the age of 50. We reviewed the pathology obtained from the pericardectomy in March 2006, which was diagnostic of mesothelioma. She also has history of several malignancies in the family. Her father died of a brain tumor at the age of 81. Her sister died at the age of 65 breast cancer. She has two maternal aunts with history of lung cancer both of whom were smoker. Also a paternal grandmother who was diagnosed with leukemia at 86 and a paternal grandfather who had B-cell lymphoma.&quot;]]).toDF(&quot;text&quot;)) Result: +--+--++--+-+-+-+ | chunk|begin|end| entity| code| all_k_resolutions| all_k_codes| +--+--++--+-+-+-+ | mesothelioma| 255|266|Oncological|9971/3||C38.3|malignant mediastinal ...|9971/3||C38.3:::8854/3...| |several malignancies| 293|312|Oncological|8894/3||C39.8|overlapping malignant ...|8894/3||C39.8:::8070/2...| | brain tumor| 350|360|Oncological|9562/0||C71.9|cancer of the brain:::...|9562/0||C71.9:::9070/3...| | breast cancer| 413|425|Oncological|9691/3||C50.9|carcinoma of breast:::...|9691/3||C50.9:::8070/2...| | lung cancer| 471|481|Oncological|8814/3||C34.9|malignant tumour of lu...|8814/3||C34.9:::8550/3...| | leukemia| 560|567|Oncological|9670/3||C80.9|anemia in neoplastic d...|9670/3||C80.9:::9714/3...| | B-cell lymphoma| 610|624|Oncological|9818/3||C77.9|secondary malignant ne...|9818/3||C77.9:::9655/3...| +--+--++--+-+-+-+ Updated Resolver Models We updated sbiobertresolve_snomed_findings and sbiobertresolve_cpt_procedures_augmented resolver models to reflect the latest changes in the official terminologies. Getting Started with Spark NLP for Healthcare Notebook in Databricks We prepared a new notebook for those who want to get started with Spark NLP for Healthcare in Databricks : Getting Started with Spark NLP for Healthcare Notebook 3.1.0 We are glad to announce that Spark NLP for Healthcare 3.1.0 has been released! Highlights Improved load time &amp; memory consumption for SentenceResolver models. New JSL Bert Models. JSL SBert Model Speed Benchmark. New ICD10CM resolver models. New Deidentification NER models. New column returned in DeidentificationModel New Reidentification feature New Deidentification Pretrained Pipelines Chunk filtering based on confidence Extended regex dictionary fuctionallity in Deidentification Enhanced RelationExtractionDL Model to create and identify relations between entities across the entire document MedicalNerApproach can now accept a graph file directly. MedicalNerApproach can now accept a user-defined name for log file. More improvements in Scaladocs. Bug fixes in Deidentification module. New notebooks. Sentence Resolver Models load time improvement Sentence resolver models now have faster load times, with a speedup of about 6X when compared to previous versions. Also, the load process now is more memory friendly meaning that the maximum memory required during load time is smaller, reducing the chances of OOM exceptions, and thus relaxing hardware requirements. New JSL SBert Models We trained new sBert models in TF2 and fined tuned on MedNLI, NLI and UMLS datasets with various parameters to cover common NLP tasks in medical domain. You can find the details in the following table. sbiobert_jsl_cased sbiobert_jsl_umls_cased sbert_jsl_medium_uncased sbert_jsl_medium_umls_uncased sbert_jsl_mini_uncased sbert_jsl_mini_umls_uncased sbert_jsl_tiny_uncased sbert_jsl_tiny_umls_uncased JSL SBert Model Speed Benchmark JSL SBert Model Base Model Is Cased Train Datasets Inference speed (100 rows) sbiobert_jsl_cased biobert_v1.1_pubmed Cased medNLI, allNLI 274,53 sbiobert_jsl_umls_cased biobert_v1.1_pubmed Cased medNLI, allNLI, umls 274,52 sbert_jsl_medium_uncased uncased_L-8_H-512_A-8 Uncased medNLI, allNLI 80,40 sbert_jsl_medium_umls_uncased uncased_L-8_H-512_A-8 Uncased medNLI, allNLI, umls 78,35 sbert_jsl_mini_uncased uncased_L-4_H-256_A-4 Uncased medNLI, allNLI 10,68 sbert_jsl_mini_umls_uncased uncased_L-4_H-256_A-4 Uncased medNLI, allNLI, umls 10,29 sbert_jsl_tiny_uncased uncased_L-2_H-128_A-2 Uncased medNLI, allNLI 4,54 sbert_jsl_tiny_umls_uncased uncased_L-2_H-128_A-2 Uncased medNLI, allNL, umls 4,54 New ICD10CM resolver models: These models map clinical entities and concepts to ICD10 CM codes using sentence bert embeddings. They also return the official resolution text within the brackets inside the metadata. Both models are augmented with synonyms, and previous augmentations are flexed according to cosine distances to unnormalized terms (ground truths). sbiobertresolve_icd10cm_slim_billable_hcc: Trained with classic sbiobert mli. (sbiobert_base_cased_mli) Models Hub Page : https://nlp.johnsnowlabs.com/2021/05/25/sbiobertresolve_icd10cm_slim_billable_hcc_en.html sbertresolve_icd10cm_slim_billable_hcc_med: Trained with new jsl sbert(sbert_jsl_medium_uncased) Models Hub Page : https://nlp.johnsnowlabs.com/2021/05/25/sbertresolve_icd10cm_slim_billable_hcc_med_en.html Example: ‘bladder cancer’ sbiobertresolve_icd10cm_augmented_billable_hcc chunks code all_codes resolutions all_distances 100x Loop(sec) bladder cancer C679 [C679, Z126, D090, D494, C7911] [bladder cancer, suspected bladder cancer, cancer in situ of urinary bladder, tumor of bladder neck, malignant tumour of bladder neck] [0.0000, 0.0904, 0.0978, 0.1080, 0.1281] 26,9 ` sbiobertresolve_icd10cm_slim_billable_hcc` chunks code all_codes resolutions all_distances 100x Loop(sec) bladder cancer D090 [D090, D494, C7911, C680, C679] [cancer in situ of urinary bladder [Carcinoma in situ of bladder], tumor of bladder neck [Neoplasm of unspecified behavior of bladder], malignant tumour of bladder neck [Secondary malignant neoplasm of bladder], carcinoma of urethra [Malignant neoplasm of urethra], malignant tumor of urinary bladder [Malignant neoplasm of bladder, unspecified]] [0.0978, 0.1080, 0.1281, 0.1314, 0.1284] 20,9 sbertresolve_icd10cm_slim_billable_hcc_med chunks code all_codes resolutions all_distances 100x Loop(sec) bladder cancer C671 [C671, C679, C61, C672, C673] [bladder cancer, dome [Malignant neoplasm of dome of bladder], cancer of the urinary bladder [Malignant neoplasm of bladder, unspecified], prostate cancer [Malignant neoplasm of prostate], cancer of the urinary bladder] [0.0894, 0.1051, 0.1184, 0.1180, 0.1200] 12,8 New Deidentification NER Models We trained four new NER models to find PHI data (protected health information) that may need to be deidentified. ner_deid_generic_augmented and ner_deid_subentity_augmented models are trained with a combination of 2014 i2b2 Deid dataset and in-house annotations as well as some augmented version of them. Compared to the same test set coming from 2014 i2b2 Deid dataset, we achieved a better accuracy and generalisation on some entity labels as summarised in the following tables. We also trained the same models with glove_100d embeddings to get more memory friendly versions. ner_deid_generic_augmented : Detects PHI 7 entities (DATE, NAME, LOCATION, PROFESSION, CONTACT, AGE, ID). Models Hub Page : https://nlp.johnsnowlabs.com/2021/06/01/ner_deid_generic_augmented_en.html entity ner_deid_large (v3.0.3 and before) ner_deid_generic_augmented (v3.1.0) CONTACT 0.8695 0.9592 NAME 0.9452 0.9648 DATE 0.9778 0.9855 LOCATION 0.8755 0.923 ner_deid_subentity_augmented: Detects PHI 23 entities (MEDICALRECORD, ORGANIZATION, DOCTOR, USERNAME, PROFESSION, HEALTHPLAN, URL, CITY, DATE, LOCATION-OTHER, STATE, PATIENT, DEVICE, COUNTRY, ZIP, PHONE, HOSPITAL, EMAIL, IDNUM, SREET, BIOID, FAX, AGE) Models Hub Page : https://nlp.johnsnowlabs.com/2021/06/01/ner_deid_subentity_augmented_en.html entity ner_deid_enriched (v3.0.3 and before) ner_deid_subentity_augmented (v3.1.0) HOSPITAL 0.8519 0.8983 DATE 0.9766 0.9854 CITY 0.7493 0.8075 STREET 0.8902 0.9772 ZIP 0.8 0.9504 PHONE 0.8615 0.9502 DOCTOR 0.9191 0.9347 AGE 0.9416 0.9469 ner_deid_generic_glove: Small version of ner_deid_generic_augmented and detects 7 entities. ner_deid_subentity_glove: Small version of ner_deid_subentity_augmented and detects 23 entities. Example: Scala ... val deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) ... val nlpPipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, word_embeddings, deid_ner, ner_converter)) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) val result = pipeline.fit(Seq.empty[&quot;A. Record date : 2093-01-13, David Hale, M.D., Name : Hendrickson, Ora MR. # 7194334 Date : 01/13/93 PCP : Oliveira, 25 -year-old, Record date : 1-11-2000. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227.&quot;].toDS.toDF(&quot;text&quot;)).transform(data) Python ... deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, word_embeddings, deid_ner, ner_converter]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame(pd.DataFrame({&quot;text&quot;: [&quot;&quot;&quot;A. Record date : 2093-01-13, David Hale, M.D., Name : Hendrickson, Ora MR. # 7194334 Date : 01/13/93 PCP : Oliveira, 25 -year-old, Record date : 1-11-2000. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227.&quot;&quot;&quot;]}))) Results: +--+-+ |chunk |ner_label | +--+-+ |2093-01-13 |DATE | |David Hale |DOCTOR | |Hendrickson, Ora |PATIENT | |7194334 |MEDICALRECORD| |01/13/93 |DATE | |Oliveira |DOCTOR | |25-year-old |AGE | |1-11-2000 |DATE | |Cocke County Baptist Hospital|HOSPITAL | |0295 Keats Street. |STREET | |(302) 786-5227 |PHONE | |Brothers Coal-Mine |ORGANIZATION | +--+-+ New column returned in DeidentificationModel DeidentificationModel now can return a new column to save the mappings between the mask/obfuscated entities and original entities. This column is optional and you can set it up with the .setReturnEntityMappings(True) method. The default value is False. Also, the name for the column can be changed using the following method; .setMappingsColumn(&quot;newAlternativeName&quot;) The new column will produce annotations with the following structure, Annotation( type: chunk, begin: 17, end: 25, result: 47, metadata:{ originalChunk -&gt; 01/13/93 //Original text of the chunk chunk -&gt; 0 // The number of the chunk in the sentence beginOriginalChunk -&gt; 95 // Start index of the original chunk endOriginalChunk -&gt; 102 // End index of the original chunk entity -&gt; AGE // Entity of the chunk sentence -&gt; 2 // Number of the sentence } ) New Reidentification feature With the new ReidetificationModel, the user can go back to the original sentences using the mappings columns and the deidentification sentences. Example: Scala val redeidentification = new ReIdentification() .setInputCols(Array(&quot;mappings&quot;, &quot;deid_chunks&quot;)) .setOutputCol(&quot;original&quot;) Python reDeidentification = ReIdentification() .setInputCols([&quot;mappings&quot;,&quot;deid_chunks&quot;]) .setOutputCol(&quot;original&quot;) New Deidentification Pretrained Pipelines We developed a clinical_deidentification pretrained pipeline that can be used to deidentify PHI information from medical texts. The PHI information will be masked and obfuscated in the resulting text. The pipeline can mask and obfuscate AGE, CONTACT, DATE, ID, LOCATION, NAME, PROFESSION, CITY, COUNTRY, DOCTOR, HOSPITAL, IDNUM, MEDICALRECORD, ORGANIZATION, PATIENT, PHONE, PROFESSION, STREET, USERNAME, ZIP, ACCOUNT, LICENSE, VIN, SSN, DLN, PLATE, IPADDR entities. Models Hub Page : clinical_deidentification There is also a lightweight version of the same pipeline trained with memory efficient glove_100dembeddings. Here are the model names: clinical_deidentification clinical_deidentification_glove Example: Python: from sparknlp.pretrained import PretrainedPipeline deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;, &quot;en&quot;, &quot;clinical/models&quot;) deid_pipeline.annotate(&quot;Record date : 2093-01-13, David Hale, M.D. IP: 203.120.223.13. The driver&#39;s license no:A334455B. the SSN:324598674 and e-mail: hale@gmail.com. Name : Hendrickson, Ora MR. # 719435 Date : 01/13/93. PCP : Oliveira, 25 years-old. Record date : 2079-11-09, Patient&#39;s VIN : 1HGBH41JXMN109286.&quot;) Scala: import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;,&quot;en&quot;,&quot;clinical/models&quot;) val result = deid_pipeline.annotate(&quot;Record date : 2093-01-13, David Hale, M.D. IP: 203.120.223.13. The driver&#39;s license no:A334455B. the SSN:324598674 and e-mail: hale@gmail.com. Name : Hendrickson, Ora MR. # 719435 Date : 01/13/93. PCP : Oliveira, 25 years-old. Record date : 2079-11-09, Patient&#39;s VIN : 1HGBH41JXMN109286.&quot;) Result: {&#39;sentence&#39;: [&#39;Record date : 2093-01-13, David Hale, M.D.&#39;, &#39;IP: 203.120.223.13.&#39;, &#39;The driver&#39;s license no:A334455B.&#39;, &#39;the SSN:324598674 and e-mail: hale@gmail.com.&#39;, &#39;Name : Hendrickson, Ora MR. # 719435 Date : 01/13/93.&#39;, &#39;PCP : Oliveira, 25 years-old.&#39;, &#39;Record date : 2079-11-09, Patient&#39;s VIN : 1HGBH41JXMN109286.&#39;], &#39;masked&#39;: [&#39;Record date : &lt;DATE&gt;, &lt;DOCTOR&gt;, M.D.&#39;, &#39;IP: &lt;IPADDR&gt;.&#39;, &#39;The driver&#39;s license &lt;DLN&gt;.&#39;, &#39;the &lt;SSN&gt; and e-mail: &lt;EMAIL&gt;.&#39;, &#39;Name : &lt;PATIENT&gt; MR. # &lt;MEDICALRECORD&gt; Date : &lt;DATE&gt;.&#39;, &#39;PCP : &lt;DOCTOR&gt;, &lt;AGE&gt; years-old.&#39;, &#39;Record date : &lt;DATE&gt;, Patient&#39;s VIN : &lt;VIN&gt;.&#39;], &#39;obfuscated&#39;: [&#39;Record date : 2093-01-18, Dr Alveria Eden, M.D.&#39;, &#39;IP: 001.001.001.001.&#39;, &#39;The driver&#39;s license K783518004444.&#39;, &#39;the SSN-400-50-8849 and e-mail: Merilynn@hotmail.com.&#39;, &#39;Name : Charls Danger MR. # J3366417 Date : 01-18-1974.&#39;, &#39;PCP : Dr Sina Sewer, 55 years-old.&#39;, &#39;Record date : 2079-11-23, Patient&#39;s VIN : 6ffff55gggg666777.&#39;], &#39;ner_chunk&#39;: [&#39;2093-01-13&#39;, &#39;David Hale&#39;, &#39;no:A334455B&#39;, &#39;SSN:324598674&#39;, &#39;Hendrickson, Ora&#39;, &#39;719435&#39;, &#39;01/13/93&#39;, &#39;Oliveira&#39;, &#39;25&#39;, &#39;2079-11-09&#39;, &#39;1HGBH41JXMN109286&#39;]} Chunk filtering based on confidence We added a new annotator ChunkFiltererApproach that allows to load a csv with both entities and confidence thresholds. This annotator will produce a ChunkFilterer model. You can load the dictionary with the following property setEntitiesConfidenceResource(). An example dictionary is: TREATMENT,0.7 With that dictionary, the user can filter the chunks corresponding to treatment entities which have confidence lower than 0.7. Example: We have a ner_chunk column and sentence column with the following data: Ner_chunk |[{chunk, 141, 163, the genomicorganization, {entity -&gt; TREATMENT, sentence -&gt; 0, chunk -&gt; 0, confidence -&gt; 0.57785}, []}, {chunk, 209, 267, a candidate gene forType II diabetes mellitus, {entity -&gt; PROBLEM, sentence -&gt; 0, chunk -&gt; 1, confidence -&gt; 0.6614286}, []}, {chunk, 394, 408, byapproximately, {entity -&gt; TREATMENT, sentence -&gt; 1, chunk -&gt; 2, confidence -&gt; 0.7705}, []}, {chunk, 478, 508, single nucleotide polymorphisms, {entity -&gt; TREATMENT, sentence -&gt; 2, chunk -&gt; 3, confidence -&gt; 0.7204666}, []}, {chunk, 559, 581, aVal366Ala substitution, {entity -&gt; TREATMENT, sentence -&gt; 2, chunk -&gt; 4, confidence -&gt; 0.61505}, []}, {chunk, 588, 601, an 8 base-pair, {entity -&gt; TREATMENT, sentence -&gt; 2, chunk -&gt; 5, confidence -&gt; 0.29226667}, []}, {chunk, 608, 625, insertion/deletion, {entity -&gt; PROBLEM, sentence -&gt; 3, chunk -&gt; 6, confidence -&gt; 0.9841}, []}]| +- Sentence [{document, 0, 298, The human KCNJ9 (Kir 3.3, GIRK3) is a member of the G-protein-activated inwardly rectifying potassium (GIRK) channel family.Here we describe the genomicorganization of the KCNJ9 locus on chromosome 1q21-23 as a candidate gene forType II diabetes mellitus in the Pima Indian population., {sentence -&gt; 0}, []}, {document, 300, 460, The gene spansapproximately 7.6 kb and contains one noncoding and two coding exons ,separated byapproximately 2.2 and approximately 2.6 kb introns, respectively., {sentence -&gt; 1}, []}, {document, 462, 601, We identified14 single nucleotide polymorphisms (SNPs), including one that predicts aVal366Ala substitution, and an 8 base-pair, {sentence -&gt; 2}, []}, {document, 603, 626, (bp) insertion/deletion., {sentence -&gt; 3}, []}] We can filter the entities using the following annotator: chunker_filter = ChunkFiltererApproach().setInputCols(&quot;sentence&quot;, &quot;ner_chunk&quot;) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;regex&quot;) .setRegex([&quot;.*&quot;]) .setEntitiesConfidenceResource(&quot;entities_confidence.csv&quot;) Where entities-confidence.csv has the following data: TREATMENT,0.7 PROBLEM,0.9 We can use that chunk_filter: chunker_filter.fit(data).transform(data) Producing the following entities: |[{chunk, 394, 408, byapproximately, {entity -&gt; TREATMENT, sentence -&gt; 1, chunk -&gt; 2, confidence -&gt; 0.7705}, []}, {chunk, 478, 508, single nucleotide polymorphisms, {entity -&gt; TREATMENT, sentence -&gt; 2, chunk -&gt; 3, confidence -&gt; 0.7204666}, []}, {chunk, 608, 625, insertion/deletion, {entity -&gt; PROBLEM, sentence -&gt; 3, chunk -&gt; 6, confidence -&gt; 0.9841}, []}]| As you can see, only the treatment entities with confidence score of more than 0.7, and the problem entities with confidence score of more than 0.9 have been kept in the output. Extended regex dictionary fuctionallity in Deidentification The RegexPatternsDictionary can now use a regex that spawns the 2 previous token and the 2 next tokens. That feature is implemented using regex groups. Examples: Given the sentence The patient with ssn 123123123 we can use the following regex to capture the entitty ssn ( d{9}) Given the sentence The patient has 12 years we can use the following regex to capture the entitty ( d{2}) years Enhanced RelationExtractionDL Model to create and identify relations between entities across the entire document A new option has been added to RENerChunksFilter to support pairing entities from different sentences using .setDocLevelRelations(True), to pass to the Relation Extraction Model. The RelationExtractionDL Model has also been updated to process document-level relations. How to use: re_dl_chunks = RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setDocLevelRelations(True) .setMaxSyntacticDistance(7) .setOutputCol(&quot;redl_ner_chunks&quot;) Examples: Given a document containing multiple sentences: John somkes cigrettes. He also consumes alcohol., now we can generate relation pairs across sentences and relate alcohol with John . Set NER graph explicitely in MedicalNerApproach Now MedicalNerApproach can receives the path to the graph directly. When a graph location is provided through this method, previous graph search behavior is disabled. MedicalNerApproach.setGraphFile(graphFilePath) MedicalNerApproach can now accept a user-defined name for log file. Now MedicalNerApproach can accept a user-defined name for the log file. If not such a name is provided, the conventional naming will take place. MedicalNerApproach.setLogPrefix(&quot;oncology_ner&quot;) This will result in oncology_ner_20210605_141701.log filename being used, in which the 20210605_141701 is a timestamp. New Notebooks A new notebook to reproduce our peer-reviewed NER paper (https://arxiv.org/abs/2011.06315) New databricks case study notebooks. In these notebooks, we showed the examples of how to work with oncology notes dataset and OCR on databricks for both DBr and community edition versions. 3.0.3 We are glad to announce that Spark NLP for Healthcare 3.0.3 has been released! Highlights Five new entity resolution models to cover UMLS, HPO and LIONC terminologies. New feature for random displacement of dates on deidentification model. Five new pretrained pipelines to map terminologies across each other (from UMLS to ICD10, from RxNorm to MeSH etc.) AnnotationToolReader support for Spark 2.3. The tool that helps model training on Spark-NLP to leverage data annotated using JSL Annotation Tool now has support for Spark 2.3. Updated documentation (Scaladocs) covering more APIs, and examples. Five new resolver models: sbiobertresolve_umls_major_concepts: This model returns CUI (concept unique identifier) codes for Clinical Findings, Medical Devices, Anatomical Structures and Injuries &amp; Poisoning terms. sbiobertresolve_umls_findings: This model returns CUI (concept unique identifier) codes for 200K concepts from clinical findings. sbiobertresolve_loinc: Map clinical NER entities to LOINC codes using sbiobert. sbluebertresolve_loinc: Map clinical NER entities to LOINC codes using sbluebert. sbiobertresolve_HPO: This model returns Human Phenotype Ontology (HPO) codes for phenotypic abnormalities encountered in human diseases. It also returns associated codes from the following vocabularies for each HPO code: * MeSH (Medical Subject Headings) * SNOMED * UMLS (Unified Medical Language System ) * ORPHA (international reference resource for information on rare diseases and orphan drugs) * OMIM (Online Mendelian Inheritance in Man) Related Notebook: Resolver Models New feature on Deidentification Module isRandomDateDisplacement(True): Be able to apply a random displacement on obfuscation dates. The randomness is based on the seed. Fix random dates when the format is not correct. Now you can repeat an execution using a seed for dates. Random dates will be based on the seed. Five new healthcare code mapping pipelines: icd10cm_umls_mapping: This pretrained pipeline maps ICD10CM codes to UMLS codes without using any text data. You’ll just feed white space-delimited ICD10CM codes and it will return the corresponding UMLS codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;icd10cm&#39;: [&#39;M89.50&#39;, &#39;R82.2&#39;, &#39;R09.01&#39;], &#39;umls&#39;: [&#39;C4721411&#39;, &#39;C0159076&#39;, &#39;C0004044&#39;]} mesh_umls_mapping: This pretrained pipeline maps MeSH codes to UMLS codes without using any text data. You’ll just feed white space-delimited MeSH codes and it will return the corresponding UMLS codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;mesh&#39;: [&#39;C028491&#39;, &#39;D019326&#39;, &#39;C579867&#39;], &#39;umls&#39;: [&#39;C0970275&#39;, &#39;C0886627&#39;, &#39;C3696376&#39;]} rxnorm_umls_mapping: This pretrained pipeline maps RxNorm codes to UMLS codes without using any text data. You’ll just feed white space-delimited RxNorm codes and it will return the corresponding UMLS codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;rxnorm&#39;: [&#39;1161611&#39;, &#39;315677&#39;, &#39;343663&#39;], &#39;umls&#39;: [&#39;C3215948&#39;, &#39;C0984912&#39;, &#39;C1146501&#39;]} rxnorm_mesh_mapping: This pretrained pipeline maps RxNorm codes to MeSH codes without using any text data. You’ll just feed white space-delimited RxNorm codes and it will return the corresponding MeSH codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;rxnorm&#39;: [&#39;1191&#39;, &#39;6809&#39;, &#39;47613&#39;], &#39;mesh&#39;: [&#39;D001241&#39;, &#39;D008687&#39;, &#39;D019355&#39;]} snomed_umls_mapping: This pretrained pipeline maps SNOMED codes to UMLS codes without using any text data. You’ll just feed white space-delimited SNOMED codes and it will return the corresponding UMLS codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;snomed&#39;: [&#39;733187009&#39;, &#39;449433008&#39;, &#39;51264003&#39;], &#39;umls&#39;: [&#39;C4546029&#39;, &#39;C3164619&#39;, &#39;C0271267&#39;]} Related Notebook: Healthcare Code Mapping 3.0.2 We are very excited to announce that Spark NLP for Healthcare 3.0.2 has been released! This release includes bug fixes and some compatibility improvements. Highlights Dictionaries for Obfuscator were augmented with more than 10K names. Improved support for spark 2.3 and spark 2.4. Bug fixes in DrugNormalizer. New Features Provide confidence scores for all available tags in MedicalNerModel, MedicalNerModel before 3.0.2 [[named_entity, 0, 9, B-PROBLEM, [word -&gt; Pneumonia, confidence -&gt; 0.9998], []] Now in Spark NLP for Healthcare 3.0.2 [[named_entity, 0, 9, B-PROBLEM, [B-PROBLEM -&gt; 0.9998, I-TREATMENT -&gt; 0.0, I-PROBLEM -&gt; 0.0, I-TEST -&gt; 0.0, B-TREATMENT -&gt; 1.0E-4, word -&gt; Pneumonia, B-TEST -&gt; 0.0], []] 3.0.1 We are very excited to announce that Spark NLP for Healthcare 3.0.1 has been released! Highlights: Fixed problem in Assertion Status internal tokenization (reported in Spark-NLP #2470). Fixes in the internal implementation of DeIdentificationModel/Obfuscator. Being able to disable the use of regexes in the Deidentification process Other minor bug fixes &amp; general improvements. DeIdentificationModel Annotator New seed parameter. Now we have the possibility of using a seed to guide the process of obfuscating entities and returning the same result across different executions. To make that possible a new method setSeed(seed:Int) was introduced. Example: Return obfuscated documents in a repeatable manner based on the same seed. Scala deIdentification = DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(true) Python de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(True) This seed controls how the obfuscated values are picked from a set of obfuscation candidates. Fixing the seed allows the process to be replicated. Example: Given the following input to the deidentification: &quot;David Hale was in Cocke County Baptist Hospital. David Hale&quot; If the annotator is set up with a seed of 10: Scala val deIdentification = new DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(true) Python de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(True) The result will be the following for any execution, &quot;Brendan Kitten was in New Megan.Brendan Kitten&quot; Now if we set up a seed of 32, Scala val deIdentification = new DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(32) .setIgnoreRegex(true) Python de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(True) The result will be the following for any execution, &quot;Louise Pear was in Lake Edward.Louise Pear&quot; New ignoreRegex parameter. You can now choose to completely disable the use of regexes in the deidentification process by setting the setIgnoreRegex param to True. Example: Scala DeIdentificationModel.setIgnoreRegex(true) Python DeIdentificationModel().setIgnoreRegex(True) The default value for this param is False meaning that regexes will be used by default. New supported entities for Deidentification &amp; Obfuscation: We added new entities to the default supported regexes: SSN - Social security number. PASSPORT - Passport id. DLN - Department of Labor Number. NPI - National Provider Identifier. C_CARD - The id number for credits card. IBAN - International Bank Account Number. DEA - DEA Registration Number, which is an identifier assigned to a health care provider by the United States Drug Enforcement Administration. We also introduced new Obfuscator cases for these new entities. 3.0.0 We are very excited to announce that Spark NLP for Healthcare 3.0.0 has been released! This has been one of the biggest releases we have ever done and we are so proud to share this with our customers. Highlights: Spark NLP for Healthcare 3.0.0 extends the support for Apache Spark 3.0.x and 3.1.x major releases on Scala 2.12 with both Hadoop 2.7. and 3.2. We now support all 4 major Apache Spark and PySpark releases of 2.3.x, 2.4.x, 3.0.x, and 3.1.x helping the customers to migrate from earlier Apache Spark versions to newer releases without being worried about Spark NLP support. Highlights: Support for Apache Spark and PySpark 3.0.x on Scala 2.12 Support for Apache Spark and PySpark 3.1.x on Scala 2.12 Migrate to TensorFlow v2.3.1 with native support for Java to take advantage of many optimizations for CPU/GPU and new features/models introduced in TF v2.x A brand new MedicalNerModel annotator to train &amp; load the licensed clinical NER models. Two times faster NER and Entity Resolution due to new batch annotation technique. Welcoming 9x new Databricks runtimes to our Spark NLP family: Databricks 7.3 Databricks 7.3 ML GPU Databricks 7.4 Databricks 7.4 ML GPU Databricks 7.5 Databricks 7.5 ML GPU Databricks 7.6 Databricks 7.6 ML GPU Databricks 8.0 Databricks 8.0 ML (there is no GPU in 8.0) Databricks 8.1 Beta Welcoming 2x new EMR 6.x series to our Spark NLP family: EMR 6.1.0 (Apache Spark 3.0.0 / Hadoop 3.2.1) EMR 6.2.0 (Apache Spark 3.0.1 / Hadoop 3.2.1) Starting Spark NLP for Healthcare 3.0.0 the default packages for CPU and GPU will be based on Apache Spark 3.x and Scala 2.12. Deprecated Text2SQL annotator is deprecated and will not be maintained going forward. We are working on a better and faster version of Text2SQL at the moment and will announce soon. 1. MedicalNerModel Annotator Starting Spark NLP for Healthcare 3.0.0, the licensed clinical and biomedical pretrained NER models will only work with this brand new annotator called MedicalNerModel and will not work with NerDLModel in open source version. In order to make this happen, we retrained all the clinical NER models (more than 80) and uploaded to models hub. Example: clinical_ner = MedicalNerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) 2. Speed Improvements A new batch annotation technique implemented in Spark NLP 3.0.0 for NerDLModel,BertEmbeddings, and BertSentenceEmbeddings annotators will be reflected in MedicalNerModel and it improves prediction/inferencing performance radically. From now on the batchSize for these annotators means the number of rows that can be fed into the models for prediction instead of sentences per row. You can control the throughput when you are on accelerated hardware such as GPU to fully utilise it. Here are the overall speed comparison: Now, NER inference and Entity Resolution are two times faster on CPU and three times faster on GPU. 3. JSL Clinical NER Model We are releasing the richest clinical NER model ever, spanning over 80 entities. It has been under development for the last 6 months and we manually annotated more than 4000 clinical notes to cover such a high number of entities in a single model. It has 4 variants at the moment: jsl_ner_wip_clinical jsl_ner_wip_greedy_clinical jsl_ner_wip_modifier_clinical jsl_rd_ner_wip_greedy_clinical Entities: Kidney_Disease, HDL, Diet, Test, Imaging_Technique, Triglycerides, Obesity, Duration, Weight, Social_History_Header, ImagingTest, Labour_Delivery, Disease_Syndrome_Disorder, Communicable_Disease, Overweight, Units, Smoking, Score, Substance_Quantity, Form, Race_Ethnicity, Modifier, Hyperlipidemia, ImagingFindings, Psychological_Condition, OtherFindings, Cerebrovascular_Disease, Date, Test_Result, VS_Finding, Employment, Death_Entity, Gender, Oncological, Heart_Disease, Medical_Device, Total_Cholesterol, ManualFix, Time, Route, Pulse, Admission_Discharge, RelativeDate, O2_Saturation, Frequency, RelativeTime, Hypertension, Alcohol, Allergen, Fetus_NewBorn, Birth_Entity, Age, Respiration, Medical_History_Header, Oxygen_Therapy, Section_Header, LDL, Treatment, Vital_Signs_Header, Direction, BMI, Pregnancy, Sexually_Active_or_Sexual_Orientation, Symptom, Clinical_Dept, Measurements, Height, Family_History_Header, Substance, Strength, Injury_or_Poisoning, Relationship_Status, Blood_Pressure, Drug, Temperature, EKG_Findings, Diabetes, BodyPart, Vaccine, Procedure, Dosage 4. JSL Clinical Assertion Model We are releasing a brand new clinical assertion model, supporting 8 assertion statuses. jsl_assertion_wip Assertion Labels : Present, Absent, Possible, Planned, Someoneelse, Past, Family, Hypotetical 5. Library Version Compatibility Table : Spark NLP for Healthcare 3.0.0 is compatible with Spark NLP 3.0.1 6. Pretrained Models Version Control (Beta): Due to active release cycle, we are adding &amp; training new pretrained models at each release and it might be tricky to maintain the backward compatibility or keep up with the latest models, especially for the users using our models locally in air-gapped networks. We are releasing a new utility class to help you check your local &amp; existing models with the latest version of everything we have up to date. You will not need to specify your AWS credentials from now on. This is the second version of the model checker we released with 2.7.6 and will replace that soon. from sparknlp_jsl.compatibility_beta import CompatibilityBeta compatibility = CompatibilityBeta(spark) print(compatibility.findVersion(&quot;ner_deid&quot;)) 7. Updated Pretrained Models: (requires fresh .pretraned()) None 2.7.6 We are glad to announce that Spark NLP for Healthcare 2.7.6 has been released! Highlights: New pretrained Radiology Assertion Status model to assign Confirmed, Suspected, Negative assertion scopes to imaging findings or any clinical tests. Obfuscating the same sensitive information (patient or doctor name) with the same fake names across the same clinical note. Version compatibility checker for the pretrained clinical models and builds to keep up with the latest development efforts in production. Adding more English names to faker module in Deidentification. Updated &amp; improved clinical SentenceDetectorDL model. New upgrades on ner_deid_large and ner_deid_enriched NER models to cover more use cases with better resolutions. Adding more examples to workshop repo for Scala users to practice more on healthcare annotators. Bug fixes &amp; general improvements. 1. Radiology Assertion Status Model We trained a new assertion model to assign Confirmed, Suspected, Negative assertion scopes to imaging findings or any clinical tests. It will try to assign these statuses to any named entity you would feed to the assertion annotater in the same pipeline. radiology_assertion = AssertionDLModel.pretrained(&quot;assertion_dl_radiology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) text = Blunting of the left costophrenic angle on the lateral view posteriorly suggests a small left pleural effusion. No right-sided pleural effusion or pneumothorax is definitively seen. There are mildly displaced fractures of the left lateral 8th and likely 9th ribs. sentences chunk ner_label sent_id assertion Blunting of the left costophrenic angle on the lateral view posteriorly suggests a small left pleural effusion. Blunting ImagingFindings 0 Confirmed Blunting of the left costophrenic angle on the lateral view posteriorly suggests a small left pleural effusion. effusion ImagingFindings 0 Suspected No right-sided pleural effusion or pneumothorax is definitively seen. effusion ImagingFindings 1 Negative No right-sided pleural effusion or pneumothorax is definitively seen. pneumothorax ImagingFindings 1 Negative There are mildly displaced fractures of the left lateral 8th and likely 9th ribs. displaced fractures ImagingFindings 2 Confirmed You can also use this with AssertionFilterer to return clinical findings from a note only when it is i.e. confirmed or suspected. assertion_filterer = AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;assertion_filtered&quot;) .setWhiteList([&quot;confirmed&quot;,&quot;suspected&quot;]) &gt;&gt; [&quot;displaced fractures&quot;, &quot;effusion&quot;] 2. Obfuscating with the same fake name across the same note: obfuscation = DeIdentification() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_chunk&quot;]) .setOutputCol(&quot;deidentified&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateDate(True) .setSameEntityThreshold(0.8) .setObfuscateRefSource(&quot;faker&quot;) text =&#39;&#39;&#39; Provider: David Hale, M.D. Pt: Jessica Parker David told Jessica that she will need to visit the clinic next month.&#39;&#39;&#39;   sentence obfuscated 0 Provider: David Hale, M.D. Provider: Dennis Perez, M.D. 1 Pt: Jessica Parker Pt: Gerth Bayer 2 David told Jessica that she will need to visit the clinic next month. Dennis told Gerth that she will need to visit the clinic next month. 3. Library Version Compatibility Table : We are releasing the version compatibility table to help users get to see which Spark NLP licensed version is built against which core (open source) version. We are going to release a detailed one after running some tests across the jars from each library. Healthcare Public 2.7.6 2.7.4 2.7.5 2.7.4 2.7.4 2.7.3 2.7.3 2.7.3 2.7.2 2.6.5 2.7.1 2.6.4 2.7.0 2.6.3 2.6.2 2.6.2 2.6.0 2.6.0 2.5.5 2.5.5 2.5.3 2.5.3 2.5.2 2.5.2 2.5.0 2.5.0 2.4.7 2.4.5 2.4.6 2.4.5 2.4.5 2.4.5 2.4.2 2.4.2 2.4.1 2.4.1 2.4.0 2.4.0 2.3.6 2.3.6 2.3.5 2.3.5 2.3.4 2.3.4 4. Pretrained Models Version Control : Due to active release cycle, we are adding &amp; training new pretrained models at each release and it might be tricky to maintain the backward compatibility or keep up with the latest models, especially for the users using our models locally in air-gapped networks. We are releasing a new utility class to help you check your local &amp; existing models with the latest version of everything we have up to date. This is an highly experimental feature of which we plan to improve and add more capability later on. from sparknlp_jsl.check_compatibility import Compatibility checker = sparknlp_jsl.Compatibility() result = checker.find_version(aws_access_key_id=license_keys[&#39;AWS_ACCESS_KEY_ID&#39;], aws_secret_access_key=license_keys[&#39;AWS_SECRET_ACCESS_KEY&#39;], metadata_path=None, model = &#39;all&#39; , # or a specific model name target_version=&#39;all&#39;, cache_pretrained_path=&#39;/home/ubuntu/cache_pretrained&#39;) &gt;&gt; result[&#39;outdated_models&#39;] [{&#39;model_name&#39;: &#39;clinical_ner_assertion&#39;, &#39;current_version&#39;: &#39;2.4.0&#39;, &#39;latest_version&#39;: &#39;2.6.4&#39;}, {&#39;model_name&#39;: &#39;jsl_rd_ner_wip_greedy_clinical&#39;, &#39;current_version&#39;: &#39;2.6.1&#39;, &#39;latest_version&#39;: &#39;2.6.2&#39;}, {&#39;model_name&#39;: &#39;ner_anatomy&#39;, &#39;current_version&#39;: &#39;2.4.2&#39;, &#39;latest_version&#39;: &#39;2.6.4&#39;}, {&#39;model_name&#39;: &#39;ner_aspect_based_sentiment&#39;, &#39;current_version&#39;: &#39;2.6.2&#39;, &#39;latest_version&#39;: &#39;2.7.2&#39;}, {&#39;model_name&#39;: &#39;ner_bionlp&#39;, &#39;current_version&#39;: &#39;2.4.0&#39;, &#39;latest_version&#39;: &#39;2.7.0&#39;}, {&#39;model_name&#39;: &#39;ner_cellular&#39;, &#39;current_version&#39;: &#39;2.4.2&#39;, &#39;latest_version&#39;: &#39;2.5.0&#39;}] &gt;&gt; result[&#39;version_comparison_dict&#39;] [{&#39;clinical_ner_assertion&#39;: {&#39;current_version&#39;: &#39;2.4.0&#39;, &#39;latest_version&#39;: &#39;2.6.4&#39;}}, {&#39;jsl_ner_wip_clinical&#39;: {&#39;current_version&#39;: &#39;2.6.5&#39;, &#39;latest_version&#39;: &#39;2.6.1&#39;}}, {&#39;jsl_ner_wip_greedy_clinical&#39;: {&#39;current_version&#39;: &#39;2.6.5&#39;, &#39;latest_version&#39;: &#39;2.6.5&#39;}}, {&#39;jsl_ner_wip_modifier_clinical&#39;: {&#39;current_version&#39;: &#39;2.6.4&#39;, &#39;latest_version&#39;: &#39;2.6.4&#39;}}, {&#39;jsl_rd_ner_wip_greedy_clinical&#39;: {&#39;current_version&#39;: &#39;2.6.1&#39;,&#39;latest_version&#39;: &#39;2.6.2&#39;}}] 5. Updated Pretrained Models: (requires fresh .pretraned()) ner_deid_large ner_deid_enriched 2.7.5 We are glad to announce that Spark NLP for Healthcare 2.7.5 has been released! Highlights: New pretrained Relation Extraction model to link clinical tests to test results and dates to clinical entities: re_test_result_date Adding two new Admission and Discharge entities to ner_events_clinical and renaming it to ner_events_admission_clinical Improving ner_deid_enriched NER model to cover Doctor and Patient name entities in various context and notations. Bug fixes &amp; general improvements. 1. re_test_result_date : text = “Hospitalized with pneumonia in June, confirmed by a positive PCR of any specimen, evidenced by SPO2 &lt;/= 93% or PaO2/FiO2 &lt; 300 mmHg”   Chunk-1 Entity-1 Chunk-2 Entity-2 Relation 0 pneumonia Problem june Date is_date_of 1 PCR Test positive Test_Result is_result_of 2 SPO2 Test 93% Test_Result is_result_of 3 PaO2/FiO2 Test 300 mmHg Test_Result is_result_of 2. ner_events_admission_clinical : ner_events_clinical NER model is updated &amp; improved to include Admission and Discharge entities. text =”She is diagnosed as cancer in 1991. Then she was admitted to Mayo Clinic in May 2000 and discharged in October 2001”   chunk entity 0 diagnosed OCCURRENCE 1 cancer PROBLEM 2 1991 DATE 3 admitted ADMISSION 4 Mayo Clinic CLINICAL_DEPT 5 May 2000 DATE 6 discharged DISCHARGE 7 October 2001 DATE 3. Improved ner_deid_enriched : PHI NER model is retrained to cover Doctor and Patient name entities even there is a punctuation between tokens as well as all upper case or lowercased. text =”A . Record date : 2093-01-13 , DAVID HALE , M.D . , Name : Hendrickson , Ora MR . # 7194334 Date : 01/13/93 PCP : Oliveira , 25 month years-old , Record date : 2079-11-09 . Cocke County Baptist Hospital . 0295 Keats Street”   chunk entity 0 2093-01-13 MEDICALRECORD 1 DAVID HALE DOCTOR 2 Hendrickson , Ora PATIENT 3 7194334 MEDICALRECORD 4 01/13/93 DATE 5 Oliveira DOCTOR 6 25 AGE 7 2079-11-09 MEDICALRECORD 8 Cocke County Baptist Hospital HOSPITAL 9 0295 Keats Street STREET 2.7.4 We are glad to announce that Spark NLP for Healthcare 2.7.4 has been released! Highlights: Introducing a new annotator to extract chunks with NER tags using regex-like patterns: NerChunker. Introducing two new annotators to filter chunks: ChunkFilterer and AssertionFilterer. Ability to change the entity type in NerConverterInternal without using ChunkMerger (setReplaceDict). In DeIdentification model, ability to use faker and static look-up lists at the same time randomly in Obfuscation mode. New De-Identification NER model, augmented with synthetic datasets to detect uppercased name entities. Bug fixes &amp; general improvements. 1. NerChunker: Similar to what we used to do in POSChunker with POS tags, now we can also extract phrases that fits into a known pattern using the NER tags. NerChunker would be quite handy to extract entity groups with neighboring tokens when there is no pretrained NER model to address certain issues. Lets say we want to extract clinical findings and body parts together as a single chunk even if there are some unwanted tokens between. How to use: ner_model = NerDLModel.pretrained(&quot;ner_radiology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) ner_chunker = NerChunker(). .setInputCols([&quot;sentence&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers([&quot;&lt;IMAGINGFINDINGS&gt;*&lt;BODYPART&gt;&quot;]) text = &#39;She has cystic cyst on her kidney.&#39; &gt;&gt; ner tags: [(cystic, B-IMAGINGFINDINGS), (cyst,I-IMAGINGFINDINGS), (kidney, B-BODYPART) &gt;&gt; ner_chunk: [&#39;cystic cyst on her kidney&#39;] 2. ChunkFilterer: ChunkFilterer will allow you to filter out named entities by some conditions or predefined look-up lists, so that you can feed these entities to other annotators like Assertion Status or Entity Resolvers. It can be used with two criteria: isin and regex. How to use: ner_model = NerDLModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) chunk_filterer = ChunkFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;) .setOutputCol(&quot;chunk_filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList([&#39;severe fever&#39;,&#39;sore throat&#39;]) text = &#39;Patient with severe fever, sore throat, stomach pain, and a headache.&#39; &gt;&gt; ner_chunk: [&#39;severe fever&#39;,&#39;sore throat&#39;,&#39;stomach pain&#39;,&#39;headache&#39;] &gt;&gt; chunk_filtered: [&#39;severe fever&#39;,&#39;sore throat&#39;] 3. AssertionFilterer: AssertionFilterer will allow you to filter out the named entities by the list of acceptable assertion statuses. This annotator would be quite handy if you want to set a white list for the acceptable assertion statuses like present or conditional; and do not want absent conditions get out of your pipeline. How to use: clinical_assertion = AssertionDLModel.pretrained(&quot;assertion_dl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) assertion_filterer = AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;assertion_filtered&quot;) .setWhiteList([&quot;present&quot;]) text = &#39;Patient with severe fever and sore throat, but no stomach pain.&#39; &gt;&gt; ner_chunk: [&#39;severe fever&#39;,&#39;sore throat&#39;,&#39;stomach pain&#39;,&#39;headache&#39;] &gt;&gt; assertion_filtered: [&#39;severe fever&#39;,&#39;sore throat&#39;] 2.7.3 We are glad to announce that Spark NLP for Healthcare 2.7.3 has been released! Highlights: Introducing a brand-new RelationExtractionDL Annotator – Achieving SOTA results in clinical relation extraction using BioBert. Massive Improvements &amp; feature enhancements in De-Identification module: Introduction of faker augmentation in Spark NLP for Healthcare to generate random data for obfuscation in de-identification module. Brand-new annotator for Structured De-Identification. Drug Normalizer: Normalize medication-related phrases (dosage, form and strength) and abbreviations in text and named entities extracted by NER models. Confidence scores in assertion output : just like NER output, assertion models now also support confidence scores for each prediction. Cosine similarity metrics in entity resolvers to get more informative and semantically correct results. AuxLabel in the metadata of entity resolvers to return additional mappings. New Relation Extraction models to extract relations between body parts and clinical entities. New Entity Resolver models to extract billable medical codes. New Clinical Pretrained NER models. Bug fixes &amp; general improvements. Matching the version with Spark NLP open-source v2.7.3. 1. Improvements in De-Identification Module: Integration of faker library to automatically generate random data like names, dates, addresses etc so users dont have to specify dummy data (custom obfuscation files can still be used). It also improves the obfuscation results due to a bigger pool of random values. How to use: Set the flag setObfuscateRefSource to faker deidentification = DeIdentification() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_chunk&quot;]) .setOutputCol(&quot;deidentified&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) For more details: Check out this notebook 2. Structured De-Identification Module: Introduction of a new annotator to handle de-identification of structured data. it allows users to define a mapping of columns and their obfuscation policy. Users can also provide dummy data and map them to columns they want to replace values in. How to use: obfuscator = StructuredDeidentification (spark,{&quot;NAME&quot;:&quot;PATIENT&quot;,&quot;AGE&quot;:&quot;AGE&quot;}, obfuscateRefSource = &quot;faker&quot;) obfuscator_df = obfuscator.obfuscateColumns(df) obfuscator_df.select(&quot;NAME&quot;,&quot;AGE&quot;).show(truncate=False) Example: Input Data: Name Age Cecilia Chapman 83 Iris Watson 9 Bryar Pitts 98 Theodore Lowe 16 Calista Wise 76 Deidentified: Name Age Menne Erdôs 20 Longin Robinson 31 Flynn Fiedlerová 50 John Wakeland 21 Vanessa Andersson 12 For more details: Check out this notebook. 3. Introducing SOTA relation extraction model using BioBert A brand-new end-to-end trained BERT model, resulting in massive improvements. Another new annotator (ReChunkFilter) is also developed for this new model to allow syntactic features work well with BioBert to extract relations. How to use: re_ner_chunk_filter = RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) .setRelationPairs(pairs) .setMaxSyntacticDistance(4) re_model = RelationExtractionDLModel() .pretrained(“redl_temporal_events_biobert”, &quot;en&quot;, &quot;clinical/models&quot;) .setPredictionThreshold(0.9) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) Benchmarks: on benchmark datasets model Spark NLP ML model Spark NLP DL model benchmark re_temporal_events_clinical 68.29 71.0 80.2 1 re_clinical 56.45 69.2 68.2 2 re_human_pheotype_gene_clinical - 87.9 67.2 3 re_drug_drug_interaction - 72.1 83.8 4 re_chemprot 76.69 94.1 83.64 5 on in-house annotations model Spark NLP ML model Spark NLP DL model re_bodypart_problem 84.58 85.7 re_bodypart_procedure 61.0 63.3 re_date_clinical 83.0 84.0 re_bodypart_direction 93.5 92.5 For more details: Check out the notebook or modelshub. 4. Drug Normalizer: Standardize units of drugs and handle abbreviations in raw text or drug chunks identified by any NER model. This normalization significantly improves performance of entity resolvers. How to use: drug_normalizer = DrugNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;document_normalized&quot;) .setPolicy(&quot;all&quot;) #all/abbreviations/dosages Examples: drug_normalizer.transform(&quot;adalimumab 54.5 + 43.2 gm”) &gt;&gt;&gt; &quot;adalimumab 97700 mg&quot; Changes: combine 54.5 + 43.2 and normalize gm to mg drug_normalizer.transform(&quot;Agnogenic one half cup”) &gt;&gt;&gt; &quot;Agnogenic 0.5 oral solution&quot; Changes: replace one half to the 0.5, normalize cup to the oral solution drug_normalizer.transform(&quot;interferon alfa-2b 10 million unit ( 1 ml ) injec”) &gt;&gt;&gt; &quot;interferon alfa - 2b 10000000 unt ( 1 ml ) injection &quot; Changes: convert 10 million unit to the 10000000 unt, replace injec with injection For more details: Check out this notebook 5. Assertion models to support confidence in output: Just like NER output, assertion models now also provides confidence scores for each prediction. chunks entities assertion confidence a headache PROBLEM present 0.9992 anxious PROBLEM conditional 0.9039 alopecia PROBLEM absent 0.9992 pain PROBLEM absent 0.9238 .setClasses() method is deprecated in AssertionDLApproach and users do not need to specify number of classes while training, as it will be inferred from the dataset. 6. New Relation Extraction Models: We are also releasing new relation extraction models to link the clinical entities to body parts and dates. These models are trained using binary relation extraction approach for better accuracy. - re_bodypart_direction : Relation Extraction between Body Part and Direction entities. Example: Text: “MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia” relations entity1 chunk1 entity2 chunk2 confidence 1 Direction upper bodyPart brain stem 0.999 0 Direction upper bodyPart cerebellum 0.999 0 Direction upper bodyPart basil ganglia 0.999 0 bodyPart brain stem Direction left 0.999 0 bodyPart brain stem Direction right 0.999 1 Direction left bodyPart cerebellum 1.0 0 Direction left bodyPart basil ganglia 0.976 0 bodyPart cerebellum Direction right 0.953 1 Direction right bodyPart basil ganglia 1.0 - re_bodypart_problem : Relation Extraction between Body Part and Problem entities. Example: Text: “No neurologic deficits other than some numbness in his left hand.” relation entity1 chunk1 entity2 chunk2 confidence 0 Symptom neurologic deficits bodyPart hand 1 1 Symptom numbness bodyPart hand 1 - re_bodypart_proceduretest : Relation Extraction between Body Part and Procedure, Test entities. Example: Text: “TECHNIQUE IN DETAIL: After informed consent was obtained from the patient and his mother, the chest was scanned with portable ultrasound.” relation entity1 chunk1 entity2 chunk2 confidence 1 bodyPart chest Test portable ultrasound 0.999 -re_date_clinical : Relation Extraction between Date and different clinical entities. Example: Text: “This 73 y/o patient had CT on 1/12/95, with progressive memory and cognitive decline since 8/11/94.” relations entity1 chunk1 entity2 chunk2 confidence 1 Test CT Date 1/12/95 1.0 1 Symptom progressive memory and cognitive decline Date 8/11/94 1.0 How to use: re_model = RelationExtractionModel() .pretrained(&quot;re_bodypart_direction&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setMaxSyntacticDistance(4) .setRelationPairs([‘Internal_organ_or_component’, ‘Direction’]) For more details: Check out the notebook or modelshub. New matching scheme for entity resolvers - improved accuracy: Adding the option to use cosine similarity to resolve entities and find closest matches, resulting in better, more semantically correct results. 7. New Resolver Models using JSL SBERT: sbiobertresolve_icd10cm_augmented sbiobertresolve_cpt_augmented sbiobertresolve_cpt_procedures_augmented sbiobertresolve_icd10cm_augmented_billable_hcc sbiobertresolve_hcc_augmented Returning auxilary columns mapped to resolutions: Chunk entity resolver and sentence entity resolver now returns auxilary data that is mapped the resolutions during training. This will allow users to get multiple resolutions with single model without using any other annotator in the pipeline (In order to get billable codes otherwise there needs to be other modules in the same pipeline) Example: sbiobertresolve_icd10cm_augmented_billable_hcc Input Text: “bladder cancer” idx chunks code resolutions all_codes billable hcc_status hcc_score all_distances 0 bladder cancer C679 [‘bladder cancer’, ‘suspected bladder cancer’, ‘cancer in situ of urinary bladder’, ‘tumor of bladder neck’, ‘malignant tumour of bladder neck’] [‘C679’, ‘Z126’, ‘D090’, ‘D494’, ‘C7911’] [‘1’, ‘1’, ‘1’, ‘1’, ‘1’] [‘1’, ‘0’, ‘0’, ‘0’, ‘1’] [‘11’, ‘0’, ‘0’, ‘0’, ‘8’] [‘0.0000’, ‘0.0904’, ‘0.0978’, ‘0.1080’, ‘0.1281’] sbiobertresolve_cpt_augmented Input Text: “ct abdomen without contrast” idx cpt code distance resolutions 0 74150 0.0802 Computed tomography, abdomen; without contrast material 1 65091 0.1312 Evisceration of ocular contents; without implant 2 70450 0.1323 Computed tomography, head or brain; without contrast material 3 74176 0.1333 Computed tomography, abdomen and pelvis; without contrast material 4 74185 0.1343 Magnetic resonance imaging without contrast 5 77059 0.1343 Magnetic resonance imaging without contrast 8. New Pretrained Clinical NER Models NER Radiology Input Text: “Bilateral breast ultrasound was subsequently performed, which demonstrated an ovoid mass measuring approximately 0.5 x 0.5 x 0.4 cm in diameter located within the anteromedial aspect of the left shoulder. This mass demonstrates isoechoic echotexture to the adjacent muscle, with no evidence of internal color flow. This may represent benign fibrous tissue or a lipoma.” idx chunks entities 0 Bilateral Direction 1 breast BodyPart 2 ultrasound ImagingTest 3 ovoid mass ImagingFindings 4 0.5 x 0.5 x 0.4 Measurements 5 cm Units 6 anteromedial aspect Direction 7 left Direction 8 shoulder BodyPart 9 mass ImagingFindings 10 isoechoic echotexture ImagingFindings 11 muscle BodyPart 12 internal color flow ImagingFindings 13 benign fibrous tissue ImagingFindings 14 lipoma Disease_Syndrome_Disorder 2.7.2 We are glad to announce that Spark NLP for Healthcare 2.7.2 has been released ! In this release, we introduce the following features: Far better accuracy for resolving medication terms to RxNorm codes: ondansetron 8 mg tablet&#39; -&gt; &#39;312086 Far better accuracy for resolving diagnosis terms to ICD-10-CM codes: TIA -&gt; transient ischemic attack (disorder) ‘S0690’ New ability to map medications to pharmacological actions (PA): &#39;metformin&#39; -&gt; ‘Hypoglycemic Agents’ 2 new greedy named entity recognition models for medication details: ner_drugs_greedy: ‘magnesium hydroxide 100mg/1ml PO’ ` ner_posology _greedy: ‘12 units of insulin lispro’ ` New model to classify the gender of a patient in a given medical note: &#39;58yo patient with a family history of breast cancer&#39; -&gt; ‘female’ And starting customized spark sessions with rich parameters params = {&quot;spark.driver.memory&quot;:&quot;32G&quot;, &quot;spark.kryoserializer.buffer.max&quot;:&quot;2000M&quot;, &quot;spark.driver.maxResultSize&quot;:&quot;2000M&quot;} spark = sparknlp_jsl.start(secret, params=params) State-of-the-art accuracy is achieved using new healthcare-tuned BERT Sentence Embeddings (s-Bert). The following sections include more details, metrics, and examples. Named Entity Recognizers for Medications A new medication NER (ner_drugs_greedy) that joins the drug entities with neighboring entities such as dosage, route, form and strength; and returns a single entity drug. This greedy NER model would be highly useful if you want to extract a drug with its context and then use it to get a RxNorm code (drugs may get different RxNorm codes based on the dosage and strength information). Metrics label tp fp fn prec rec f1 I-DRUG 37423 4179 3773 0.899 0.908 0.904 B-DRUG 29699 2090 1983 0.934 0.937 0.936 A new medication NER (ner_posology_greedy) that joins the drug entities with neighboring entities such as dosage, route, form and strength. It also returns all the other medication entities even if not related to (or joined with) a drug. Now we have five different medication-related NER models. You can see the outputs from each model below: Text = ‘‘The patient was prescribed 1 capsule of Advil 10 mg for 5 days and magnesium hydroxide 100mg/1ml suspension PO. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.’’ a. ner_drugs_greedy   chunks begin end entities 0 1 capsule of Advil 10 mg 27 50 DRUG 1 magnesium hydroxide 100mg/1ml PO 67 98 DRUG 2 40 units of insulin glargine 168 195 DRUG 3 12 units of insulin lispro 207 232 DRUG b. ner_posology_greedy   chunks begin end entities 0 1 capsule of Advil 10 mg 27 50 DRUG 1 magnesium hydroxide 100mg/1ml PO 67 98 DRUG 2 for 5 days 52 61 DURATION 3 40 units of insulin glargine 168 195 DRUG 4 at night 197 204 FREQUENCY 5 12 units of insulin lispro 207 232 DRUG 6 with meals 234 243 FREQUENCY 7 metformin 1000 mg 250 266 DRUG 8 two times a day 268 282 FREQUENCY c. ner_drugs   chunks begin end entities 0 Advil 40 44 DrugChem 1 magnesium hydroxide 67 85 DrugChem 2 metformin 261 269 DrugChem d.ner_posology   chunks begin end entities 0 1 27 27 DOSAGE 1 capsule 29 35 FORM 2 Advil 40 44 DRUG 3 10 mg 46 50 STRENGTH 4 for 5 days 52 61 DURATION 5 magnesium hydroxide 67 85 DRUG 6 100mg/1ml 87 95 STRENGTH 7 PO 97 98 ROUTE 8 40 units 168 175 DOSAGE 9 insulin glargine 180 195 DRUG 10 at night 197 204 FREQUENCY 11 12 units 207 214 DOSAGE 12 insulin lispro 219 232 DRUG 13 with meals 234 243 FREQUENCY 14 metformin 250 258 DRUG 15 1000 mg 260 266 STRENGTH 16 two times a day 268 282 FREQUENCY e. ner_drugs_large   chunks begin end entities 0 Advil 10 mg 40 50 DRUG 1 magnesium hydroxide 100mg/1ml PO. 67 99 DRUG 2 insulin glargine 180 195 DRUG 3 insulin lispro 219 232 DRUG 4 metformin 1000 mg 250 266 DRUG Patient Gender Classification This model detects the gender of the patient in the clinical document. It can classify the documents into Female, Male and Unknown. We release two models: ‘Classifierdl_gender_sbert’ (more accurate, works with licensed sbiobert_base_cased_mli) ‘Classifierdl_gender_biobert’ (works with biobert_pubmed_base_cased) The models are trained on more than four thousands clinical documents (radiology reports, pathology reports, clinical visits etc.), annotated internally. Metrics (Classifierdl_gender_sbert)   precision recall f1-score support Female 0.9224 0.8954 0.9087 239 Male 0.7895 0.8468 0.8171 124 Text= ‘‘social history: shows that does not smoke cigarettes or drink alcohol, lives in a nursing home. family history: shows a family history of breast cancer.’’ gender_classifier.annotate(text)[&#39;class&#39;][0] &gt;&gt; `Female` See this Colab notebook for further details. a. classifierdl_gender_sbert document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) .setMaxSentenceLength(512) gender_classifier = ClassifierDLModel .pretrained(&#39;classifierdl_gender_sbert&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;document&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;class&quot;) gender_pred_pipeline = Pipeline( stages = [ document, sbert_embedder, gender_classifier ]) b. classifierdl_gender_biobert documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) clf_tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) biobert_embeddings = BertEmbeddings().pretrained(&#39;biobert_pubmed_base_cased&#39;) .setInputCols([&quot;document&quot;,&#39;token&#39;]) .setOutputCol(&quot;bert_embeddings&quot;) biobert_embeddings_avg = SentenceEmbeddings() .setInputCols([&quot;document&quot;, &quot;bert_embeddings&quot;]) .setOutputCol(&quot;sentence_bert_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) genderClassifier = ClassifierDLModel.pretrained(&#39;classifierdl_gender_biobert&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;document&quot;, &quot;sentence_bert_embeddings&quot;]) .setOutputCol(&quot;gender&quot;) gender_pred_pipeline = Pipeline( stages = [ documentAssembler, clf_tokenizer, biobert_embeddings, biobert_embeddings_avg, genderClassifier ]) New ICD10CM and RxCUI resolvers powered by s-Bert embeddings The advent of s-Bert sentence embeddings changed the landscape of Clinical Entity Resolvers completely in Spark NLP. Since s-Bert is already tuned on MedNLI (medical natural language inference) dataset, it is now capable of populating the chunk embeddings in a more precise way than before. We now release two new resolvers: sbiobertresolve_icd10cm_augmented (augmented with synonyms, four times richer than previous resolver accuracy: 73% for top-1 (exact match), 89% for top-5 (previous accuracy was 59% and 64% respectively) sbiobertresolve_rxcui (extract RxNorm concept unique identifiers to map with ATC or durg families) accuracy: 71% for top-1 (exact match), 72% for top-5 (previous accuracy was 22% and 41% respectively) a. ICD10CM augmented resolver Text = “This is an 82 year old male with a history of prior tobacco use , hypertension , chronic renal insufficiency , COPD , gastritis , and TIA who initially presented to Braintree with a non-ST elevation MI and Guaiac positive stools , transferred to St . Margaret&#39;s Center for Women &amp; Infants for cardiac catheterization with PTCA to mid LAD lesion complicated by hypotension and bradycardia requiring Atropine , IV fluids and transient dopamine possibly secondary to vagal reaction , subsequently transferred to CCU for close monitoring , hemodynamically stable at the time of admission to the CCU . “   chunk begin end code term 0 hypertension 66 77 I10 hypertension 1 chronic renal insufficiency 81 107 N189 chronic renal insufficiency 2 COPD 111 114 J449 copd - chronic obstructive pulmonary disease 3 gastritis 118 126 K2970 gastritis 4 TIA 134 136 S0690 transient ischemic attack (disorder) 5 a non-ST elevation MI 180 200 I219 silent myocardial infarction (disorder) 6 Guaiac positive stools 206 227 K921 guaiac-positive stools 7 mid LAD lesion 330 343 I2102 stemi involving left anterior descending coronary artery 8 hypotension 360 370 I959 hypotension 9 bradycardia 376 386 O9941 bradycardia b. RxCUI resolver Text= “He was seen by the endocrinology service and she was discharged on 50 mg of eltrombopag oral at night, 5 mg amlodipine with meals, and metformin 1000 mg two times a day . “   chunk begin end code term 0 50 mg of eltrombopag oral 67 91 825427 eltrombopag 50 MG Oral Tablet 1 5 mg amlodipine 103 117 197361 amlodipine 5 MG Oral Tablet 2 metformin 1000 mg 135 151 861004 metformin hydrochloride 1000 MG Oral Tablet Using this new resolver and some other resources like Snomed Resolver, RxTerm, MESHPA and ATC dictionary, you can link the drugs to the pharmacological actions (PA), ingredients and the disease treated with that. Code sample: (after getting the chunk from ChunkConverter) c2doc = Chunk2Doc().setInputCols(&quot;ner_chunk&quot;).setOutputCol(&quot;ner_chunk_doc&quot;) sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) icd10_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_icd10cm_augmented&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;icd10cm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) See the notebook for details. 2.7.1 We are glad to announce that Spark NLP for Healthcare 2.7.1 has been released ! In this release, we introduce the following features: 1. Sentence BioBert and Bluebert Transformers that are fine tuned on MedNLI dataset. Sentence Transformers offers a framework that provides an easy method to compute dense vector representations for sentences and paragraphs (also known as sentence embeddings). The models are based on BioBert and BlueBert, and are tuned specifically to meaningful sentence embeddings such that sentences with similar meanings are close in vector space. These are the first PyTorch based models we managed to port into Spark NLP. Here is how you can load these: sbiobert_embeddins = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) sbluebert_embeddins = BertSentenceEmbeddings .pretrained(&quot;sbluebert_base_cased_mli&quot;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) 2. SentenceEntityResolvers powered by s-Bert embeddings. The advent of s-Bert sentence embeddings changed the landscape of Clinical Entity Resolvers completely in Spark NLP. Since s-Bert is already tuned on MedNLI (medical natural language inference) dataset, it is now capable of populating the chunk embeddings in a more precise way than before. Using sbiobert_base_cased_mli, we trained the following Clinical Entity Resolvers: sbiobertresolve_icd10cm sbiobertresolve_icd10pcs sbiobertresolve_snomed_findings (with clinical_findings concepts from CT version) sbiobertresolve_snomed_findings_int (with clinical_findings concepts from INT version) sbiobertresolve_snomed_auxConcepts (with Morph Abnormality, Procedure, Substance, Physical Object, Body Structure concepts from CT version) sbiobertresolve_snomed_auxConcepts_int (with Morph Abnormality, Procedure, Substance, Physical Object, Body Structure concepts from INT version) sbiobertresolve_rxnorm sbiobertresolve_icdo sbiobertresolve_cpt Code sample: (after getting the chunk from ChunkConverter) c2doc = Chunk2Doc().setInputCols(&quot;ner_chunk&quot;).setOutputCol(&quot;ner_chunk_doc&quot;) sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) snomed_ct_resolver = SentenceEntityResolverModel .pretrained(&quot;sbiobertresolve_snomed_findings&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_ct_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) Output:   chunks begin end code resolutions 2 COPD 113 116 13645005 copd - chronic obstructive pulmonary disease 8 PTCA 324 327 373108000 post percutaneous transluminal coronary angioplasty (finding) 16 close monitoring 519 534 417014005 on examination - vigilance See the notebook for details. 3. We are releasing the following pretrained clinical NER models: ner_drugs_large (trained with medications dataset, and extracts drugs with the dosage, strength, form and route at once as a single entity; entities: drug) ner_deid_sd_large (extracts PHI entities, trained with augmented dataset) ner_anatomy_coarse (trained with enriched anatomy NER dataset; entities: anatomy) ner_anatomy_coarse_biobert chunkresolve_ICD10GM_2021 (German ICD10GM resolver) We are also releasing two new NER models: ner_aspect_based_sentiment (extracts positive, negative and neutral aspects about restaurants from the written feedback given by reviewers. ) ner_financial_contract (extract financial entities from contracts. See the notebook for details.) 2.7.0 We are glad to announce that Spark NLP for Healthcare 2.7 has been released ! In this release, we introduce the following features: 1. Text2SQL Text2SQL Annotator that translates natural language text into SQL queries against a predefined database schema, which is one of the most sought-after features of NLU. With the help of a pretrained text2SQL model, you will be able to query your database without writing a SQL query: Example 1 Query: What is the name of the nurse who has the most appointments? Generated SQL query from the model: SELECT T1.Name FROM Nurse AS T1 JOIN Appointment AS T2 ON T1.EmployeeID = T2.PrepNurse GROUP BY T2.prepnurse ORDER BY count(*) DESC LIMIT 1 Response:   Name 0 Carla Espinosa Example 2 Query: How many patients do each physician take care of? List their names and number of patients they take care of. Generated SQL query from the model: SELECT T1.Name, count(*) FROM Physician AS T1 JOIN Patient AS T2 ON T1.EmployeeID = T2.PCP GROUP BY T1.Name Response:   Name count(*) 0 Christopher Turk 1 1 Elliot Reid 2 2 John Dorian 1 For now, it only comes with one pretrained model (trained on Spider dataset) and new pretrained models will be released soon. Check out the Colab notebook to see more examples and run on your data. 2. SentenceEntityResolvers In addition to ChunkEntityResolvers, we now release our first BioBert-based entity resolvers using the SentenceEntityResolver annotator. It’s fully trainable and comes with several pretrained entity resolvers for the following medical terminologies: CPT: biobertresolve_cpt ICDO: biobertresolve_icdo ICD10CM: biobertresolve_icd10cm ICD10PCS: biobertresolve_icd10pcs LOINC: biobertresolve_loinc SNOMED_CT (findings): biobertresolve_snomed_findings SNOMED_INT (clinical_findings): biobertresolve_snomed_findings_int RXNORM (branded and clinical drugs): biobertresolve_rxnorm_bdcd Example: text = &#39;He has a starvation ketosis but nothing significant for dry oral mucosa&#39; df = get_icd10_codes (light_pipeline_icd10, &#39;icd10cm_code&#39;, text)   chunks begin end code 0 a starvation ketosis 7 26 E71121 1 dry oral mucosa 66 80 K136 Check out the Colab notebook to see more examples and run on your data. You can also train your own entity resolver using any medical terminology like MedRa and UMLS. Check this notebook to learn more about training from scratch. 3. ChunkMerge Annotator In order to use multiple NER models in the same pipeline, Spark NLP Healthcare has ChunkMerge Annotator that is used to return entities from each NER model by overlapping. Now it has a new parameter to avoid merging overlapping entities (setMergeOverlapping) to return all the entities regardless of char indices. It will be quite useful to analyze what every NER module returns on the same text. 4. Starting SparkSession We now support starting SparkSession with a different version of the open source jar and not only the one it was built against by sparknlp_jsl.start(secret, public=&quot;x.x.x&quot;) for extreme cases. 5. Biomedical NERs We are releasing 3 new biomedical NER models trained with clinical embeddings (all one single entity models) ner_bacterial_species (comprising of Linneaus and Species800 datasets) ner_chemicals (general purpose and bio chemicals, comprising of BC4Chem and BN5CDR-Chem) ner_diseases_large (comprising of ner_disease, NCBI_Disease and BN5CDR-Disease) We are also releasing the biobert versions of the several clinical NER models stated below: ner_clinical_biobert ner_anatomy_biobert ner_bionlp_biobert ner_cellular_biobert ner_deid_biobert ner_diseases_biobert ner_events_biobert ner_jsl_biobert ner_chemprot_biobert ner_human_phenotype_gene_biobert ner_human_phenotype_go_biobert ner_posology_biobert ner_risk_factors_biobert Metrics (micro averages excluding O’s):   model_name clinical_glove_micro biobert_micro 0 ner_chemprot_clinical 0.816 0.803 1 ner_bionlp 0.748 0.808 2 ner_deid_enriched 0.934 0.918 3 ner_posology 0.915 0.911 4 ner_events_clinical 0.801 0.809 5 ner_clinical 0.873 0.884 6 ner_posology_small 0.941   7 ner_human_phenotype_go_clinical 0.922 0.932 8 ner_drugs 0.964   9 ner_human_phenotype_gene_clinical 0.876 0.870 10 ner_risk_factors 0.728   11 ner_cellular 0.813 0.812 12 ner_posology_large 0.921   13 ner_anatomy 0.851 0.831 14 ner_deid_large 0.942   15 ner_diseases 0.960 0.966 In addition to these, we release two new German NER models: ner_healthcare_slim (‘TIME_INFORMATION’, ‘MEDICAL_CONDITION’, ‘BODY_PART’, ‘TREATMENT’, ‘PERSON’, ‘BODY_PART’) ner_traffic (extract entities regarding traffic accidents e.g. date, trigger, location etc.) 6. PICO Classifier Successful evidence-based medicine (EBM) applications rely on answering clinical questions by analyzing large medical literature databases. In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components: Participants/Problem (P) (e.g., diabetic patients), Intervention (I) (e.g., insulin), Comparison (C) (e.g., placebo) and Outcome (O) (e.g., blood glucose levels). Spark NLP now introduces a pretrained PICO Classifier that is trained with Biobert embeddings. Example: text = “There appears to be no difference in smoking cessation effectiveness between 1mg and 0.5mg varenicline.” pico_lp_pipeline.annotate(text)[&#39;class&#39;][0] ans: CONCLUSIONS 2.6.2 Overview We are very happy to announce that version 2.6.2 of Spark NLP Enterprise is ready to be installed and used. We are making available Named Entity Recognition, Sentence Classification and Entity Resolution models to analyze Adverse Drug Events in natural language text from clinical domains. Models NERs We are pleased to announce that we have a brand new named entity recognition (NER) model for Adverse Drug Events (ADE) to extract ADE and DRUG entities from a given text. ADE NER will have four versions in the library, trained with different size of word embeddings: ner_ade_bioert (768d Bert embeddings) ner_ade_clinicalbert (768d Bert embeddings) ner_ade_clinical (200d clinical embeddings) ner_ade_healthcare (100d healthcare embeddings) More information and examples here We are also releasing our first clinical pretrained classifier for ADE classification tasks. This new ADE classifier is trained on various ADE datasets, including the mentions in tweets to represent the daily life conversations as well. So it works well on the texts coming from academic context, social media and clinical notes. It’s trained with Clinical Biobert embeddings, which is the most powerful contextual language model in the clinical domain out there. Classifiers ADE classifier will have two versions in the library, trained with different Bert embeddings: classifierdl_ade_bioert (768d BioBert embeddings) classifierdl_adee_clinicalbert (768d ClinicalBert embeddings) More information and examples here Pipeline By combining ADE NER and Classifier, we are releasing a new pretrained clinical pipeline for ADE tasks to save you from building pipelines from scratch. Pretrained pipelines are already fitted using certain annotators and transformers according to various use cases and you can use them as easy as follows: pipeline = PretrainedPipeline(&#39;explain_clinical_doc_ade&#39;, &#39;en&#39;, &#39;clinical/models&#39;) pipeline.annotate(&#39;my string&#39;) explain_clinical_doc_ade is bundled with ner_ade_clinicalBert, and classifierdl_ade_clinicalBert. It can extract ADE and DRUG clinical entities, and then assign ADE status to a text (True means ADE, False means not related to ADE). More information and examples here Entity Resolver We are releasing the first Entity Resolver for Athena (Automated Terminology Harmonization, Extraction and Normalization for Analytics, http://athena.ohdsi.org/) to extract concept ids via standardized medical vocabularies. For now, it only supports conditions section and can be used to map the clinical conditions with the corresponding standard terminology and then get the concept ids to store them in various database schemas. It is named as chunkresolve_athena_conditions_healthcare. We added slim versions of several clinical NER models that are trained with 100d healthcare word embeddings, which is lighter and smaller in size. ner_healthcare assertion_dl_healthcare ner_posology_healthcare ner_events_healthcare Graph Builder Spark NLP Licensed version has several DL based annotators (modules) such as NerDL, AssertionDL, RelationExtraction and GenericClassifier, and they are all based on Tensorflow (tf) with custom graphs. In order to make the creating and customizing the tf graphs for these models easier for our licensed users, we added a graph builder to the Python side of the library. Now you can customize your graphs and use them in the respected models while training a new DL model. from sparknlp_jsl.training import tf_graph tf_graph.build(&quot;relation_extraction&quot;,build_params={&quot;input_dim&quot;: 6000, &quot;output_dim&quot;: 3, &#39;batch_norm&#39;:1, &quot;hidden_layers&quot;: [300, 200], &quot;hidden_act&quot;: &quot;relu&quot;, &#39;hidden_act_l2&#39;:1}, model_location=&quot;.&quot;, model_filename=&quot;re_with_BN&quot;) More information and examples here 2.6.0 Overview We are honored to announce that Spark NLP Enterprise 2.6.0 has been released. The first time ever, we release three pretrained clinical pipelines to save you from building pipelines from scratch. Pretrained pipelines are already fitted using certain annotators and transformers according to various use cases. The first time ever, we are releasing 3 licensed German models for healthcare and Legal domains. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Models Pretrained Pipelines: The first time ever, we release three pretrained clinical pipelines to save you from building pipelines from scratch. Pretrained pipelines are already fitted using certain annotators and transformers according to various use cases and you can use them as easy as follows: pipeline = PretrainedPipeline(&#39;explain_clinical_doc_carp&#39;, &#39;en&#39;, &#39;clinical/models&#39;) pipeline.annotate(&#39;my string&#39;) Pipeline descriptions: explain_clinical_doc_carp a pipeline with ner_clinical, assertion_dl, re_clinical and ner_posology. It will extract clinical and medication entities, assign assertion status and find relationships between clinical entities. explain_clinical_doc_era a pipeline with ner_clinical_events, assertion_dl and re_temporal_events_clinical. It will extract clinical entities, assign assertion status and find temporal relationships between clinical entities. recognize_entities_posology a pipeline with ner_posology. It will only extract medication entities. More information and examples are available here: https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/11.Pretrained_Clinical_Pipelines.ipynb. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Pretrained Named Entity Recognition and Relationship Extraction Models (English) RE models: re_temporal_events_clinical re_temporal_events_enriched_clinical re_human_phenotype_gene_clinical re_drug_drug_interaction_clinical re_chemprot_clinical NER models: ner_human_phenotype_gene_clinical ner_human_phenotype_go_clinical ner_chemprot_clinical More information and examples here: https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/10.Clinical_Relation_Extraction.ipynb &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Pretrained Named Entity Recognition and Relationship Extraction Models (German) The first time ever, we are releasing 3 licensed German models for healthcare and Legal domains. German Clinical NER model for 19 clinical entities German Legal NER model for 19 legal entities German ICD-10GM More information and examples here: https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/14.German_Healthcare_Models.ipynb https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/15.German_Legal_Model.ipynb &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Other Pretrained Models We now have Named Entity Disambiguation model out of the box. Disambiguation models map words of interest, such as names of persons, locations and companies, from an input text document to corresponding unique entities in a target Knowledge Base (KB). https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/12.Named_Entity_Disambiguation.ipynb Due to ongoing requests about Clinical Entity Resolvers, we release a notebook to let you see how to train an entity resolver using an open source dataset based on Snomed. https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/13.Snomed_Entity_Resolver_Model_Training.ipynb &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.5.5 Overview We are very happy to release Spark NLP for Healthcare 2.5.5 with a new state-of-the-art RelationExtraction annotator to identify relationships between entities coming from our pretrained NER models. This is also the first release to support Relation Extraction with the following two (2) models: re_clinical and re_posology in the clinical/models repository. We also include multiple bug fixes as usual. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features RelationExtraction annotator that receives WORD_EMBEDDINGS, POS, CHUNK, DEPENDENCY and returns the CATEGORY of the relationship and a confidence score. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements AssertionDL Annotator now keeps logs of the metrics while training DeIdentification now has a default behavior of merging entities close in Levenshtein distance with setConsistentObfuscation and setSameEntityThreshold params. DeIdentification now has a specific parameter setObfuscateDate to obfuscate dates (which will be otherwise just masked). The only formats obfuscated when the param is true will be the ones present in dateFormats param. NerConverterInternal now has a greedyMode param that will merge all contiguous tags of the same type regardless of boundary tags like “B”,”E”,”S”. AnnotationToolJsonReader includes mergeOverlapping parameter to merge (or not) overlapping entities from the Annotator jsons i.e. not included in the assertion list. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes DeIdentification documentation bug fix (typo) DeIdentification training bug fix in obfuscation dictionary IOBTagger now has the correct output type NAMED_ENTITY &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Deprecations EnsembleEntityResolver has been deprecated Models We have 2 new english Relationship Extraction model for Clinical and Posology NERs: re_clinical: with ner_clinical and embeddings_clinical re_posology: with ner_posology and embeddings_clinical &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.5.3 Overview We are pleased to announce the release of Spark NLP for Healthcare 2.5.3. This time we include four (4) new Annotators: FeatureAssembler, GenericClassifier, Yake Keyword Extractor and NerConverterInternal. We also include helper classes to read datasets from CodiEsp and Cantemist Spanish NER Challenges. This is also the first release to support the following models: ner_diag_proc (spanish), ner_neoplasms (spanish), ner_deid_enriched (english). We have also included Bugifxes and Enhancements for AnnotationToolJsonReader and ChunkMergeModel. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features FeatureAssembler Transformer: Receives a list of column names containing numerical arrays and concatenates them to form one single feature_vector annotation GenericClassifier Annotator: Receives a feature_vector annotation and outputs a category annotation Yake Keyword Extraction Annotator: Receives a token annotation and outputs multi-token keyword annotations NerConverterInternal Annotator: Similar to it’s open source counterpart in functionality, performs smarter extraction for complex tokenizations and confidence calculation Readers for CodiEsp and Cantemist Challenges &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements AnnotationToolJsonReader includes parameter for preprocessing pipeline (from Document Assembling to Tokenization) AnnotationToolJsonReader includes parameter to discard specific entity types &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes ChunkMergeModel now prioritizes highest number of different entities when coverage is the same &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Models We have 2 new spanish models for Clinical Entity Recognition: ner_diag_proc and ner_neoplasms We have a new english Named Entity Recognition model for deidentification: ner_deid_enriched &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.5.2 Overview We are really happy to bring you Spark NLP for Healthcare 2.5.2, with a couple new features and several enhancements in our existing annotators. This release was mainly dedicated to generate adoption in our AnnotationToolJsonReader, a connector that provide out-of-the-box support for out Annotation Tool and our practices. Also the ChunkMerge annotator has ben provided with extra functionality to remove entire entity types and to modify some chunk’s entity type We also dedicated some time in finalizing some refactorization in DeIdentification annotator, mainly improving type consistency and case insensitive entity dictionary for obfuscation. Thanks to the community for all the feedback and suggestions, it’s really comfortable to navigate together towards common functional goals that keep us agile in the SotA. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features Brand new IOBTagger Annotator NerDL Metrics provides an intuitive DataFrame API to calculate NER metrics at tag (token) and entity (chunk) level &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements AnnotationToolJsonReader includes parameters for document cleanup, sentence boundaries and tokenizer split chars AnnotationToolJsonReader uses the task title if present and uses IOBTagger annotator AnnotationToolJsonReader has improved alignment in assertion train set generation by using an alignTol parameter as tollerance in chunk char alignment DeIdentification refactorization: Improved typing and replacement logic, case insensitive entities for obfuscation ChunkMerge Annotator now handles: Drop all chunks for an entity Replace entity name Change entity type for a specific (chunk, entity) pair Drop specific (chunk, entity) pairs caseSensitive param to EnsembleEntityResolver Output logs for AssertionDLApproach loss Disambiguator is back with improved dependency management &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes Bugfix in python when Annotators shared domain parts across public and internal Bugfix in python when ChunkMerge annotator was loaded from disk ChunkMerge now weights the token coverage correctly when multiple multi-token entities overlap &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.5.0 Overview We are happy to bring you Spark NLP for Healthcare 2.5.0 with new Annotators, Models and Data Readers. Model composition and iteration is now faster with readers and annotators designed for real world tasks. We introduce ChunkMerge annotator to combine all CHUNKS extracted by different Entity Extraction Annotators. We also introduce an Annotation Reader for JSL AI Platform’s Annotation Tool. This release is also the first one to support the models: ner_large_clinical, ner_events_clinical, assertion_dl_large, chunkresolve_loinc_clinical, deidentify_large And of course we have fixed some bugs. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features AnnotationToolJsonReader is a new class that imports a JSON from AI Platform’s Annotation Tool an generates NER and Assertion training datasets ChunkMerge Annotator is a new functionality that merges two columns of CHUNKs handling overlaps with a very straightforward logic: max coverage, max # entities ChunkMerge Annotator handles inputs from NerDLModel, RegexMatcher, ContextualParser, TextMatcher A DeIdentification pretrained model can now work in ‘mask’ or ‘obfuscate’ mode &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements DeIdentification Annotator has a more consistent API: mode param with values (‘mask’l’obfuscate’) to drive its behavior dateFormats param a list of string values to to select which dateFormats to obfuscate (and which to just mask) DeIdentification Annotator no longer automatically obfuscates dates. Obfuscation is now driven by mode and dateFormats params A DeIdentification pretrained model can now work in ‘mask’ or ‘obfuscate’ mode &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes DeIdentification Annotator now correctly deduplicates protected entities coming from NER / Regex DeIdentification Annotator now indexes chunks correctly after merging them AssertionDLApproach Annotator can now be trained with the graph in any folder specified by setting graphFolder param AssertionDLApproach now has the setClasses param setter in Python wrapper JVM Memory and Kryo Max Buffer size increased to 32G and 2000M respectively in sparknlp_jsl.start(secret) function &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.4.6 Overview We release Spark NLP for Healthcare 2.4.6 to fix some minor bugs. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes Updated IDF value calculation to be probabilistic based log[(N - df_t) / df_t + 1] as opposed to log[N / df_t] TFIDF cosine distance was being calculated with the rooted norms rather than with the original squared norms Validation of label cols is now performed at the beginning of EnsembleEntityResolver Environment Variable for License value named jsl.settings.license Now DocumentLogRegClassifier can be serialized from Python (bug introduced with the implementation of RecursivePipelines, LazyAnnotator attribute) &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.4.5 Overview We are glad to announce Spark NLP for Healthcare 2.4.5. As a new feature we are happy to introduce our new EnsembleEntityResolver which allows our Entity Resolution architecture to scale up in multiple orders of magnitude and handle datasets of millions of records on a sub-log computation increase We also enhanced our ChunkEntityResolverModel with 5 new distance calculations with weighting-array and aggregation-strategy params that results in more levers to finetune its performance against a given dataset. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features EnsembleEntityResolver consisting of an integrated TFIDF-Logreg classifier in the first layer + Multiple ChunkEntityResolvers in the second layer (one per each class) Five (5) new distances calculations for ChunkEntityResolver, namely: Token Based: TFIDF-Cosine, Jaccard, SorensenDice Character Based: JaroWinkler and Levenshtein Weight parameter that works as a multiplier for each distance result to be considered during their aggregation Three (3) aggregation strategies for the enabled distance in a particular instance, namely: AVERAGE, MAX and MIN &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements ChunkEntityResolver can now compute distances over all the neighbours found and return the metadata just for the best alternatives that meet the threshold; before it would calculate them over the neighbours and return them all in the metadata ChunkEntityResolver now has an extramassPenalty parameter to accoun for penalization of token-length difference in compared strings Metadata for the ChunkEntityResolver has been updated accordingly to reflect all new features StringDistances class has been included in utils to aid in the calculation and organization of different types of distances for Strings HasFeaturesJsl trait has been included to support the serialization of Features including [T] &lt;: AnnotatorModel[T] types &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes Frequency calculation for WMD in ChunkEntityResolver has been adjusted to account for real word count representation AnnotatorType for DocumentLogRegClassifier has been changed to CATEGORY to align with classifiers in Open Source library &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Deprecations Legacy EntityResolver{Approach, Model} classes have been deprecated in favor of ChunkEntityResolver classes ChunkEntityResolverSelector classes has been deprecated in favor of EnsembleEntityResolver &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.4.2 Overview We are glad to announce Spark NLP for Healthcare 2.4.2. As a new feature we are happy to introduce our new Disambiguation Annotator, which will let the users resolve different kind of entities based on Knowledge bases provided in the form of Records in a RocksDB database. We also enhanced / fixed DocumentLogRegClassifier, ChunkEntityResolverModel and ChunkEntityResolverSelector Annotators. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features Disambiguation Annotator (NerDisambiguator and NerDisambiguatorModel) which accepts annotator types CHUNK and SENTENCE_EMBEDDINGS and returns DISAMBIGUATION annotator type. This output annotation type includes all the matches in the result and their similarity scores in the metadata. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements ChunkEntityResolver Annotator now supports both EUCLIDEAN and COSINE distance for the KNN search and WMD calculation. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes Fixed a bug in DocumentLogRegClassifier Annotator to support its serialization to disk. Fixed a bug in ChunkEntityResolverSelector Annotator to group by both SENTENCE and CHUNK at the time of forwarding tokens and embeddings to the lazy annotators. Fixed a bug in ChunkEntityResolverModel in which the same exact embeddings was not included in the neighbours. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.4.1 Overview Introducing Spark NLP for Healthcare 2.4.1 after all the feedback we received in the form of issues and suggestions on our different communication channels. Even though 2.4.0 was very stable, version 2.4.1 is here to address minor bug fixes that we summarize in the following lines. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes Changing the license Spark property key to be “jsl” instead of “sparkjsl” as the latter generates inconsistencies Fix the alignment logic for tokens and chunks in the ChunkEntityResolverSelector because when tokens and chunks did not have the same begin-end indexes the resolution was not executed &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.4.0 Overview We are glad to announce Spark NLP for Healthcare 2.4.0. This is an important release because of several refactorizations achieved in the core library, plus the introduction of several state of the art algorithms, new features and enhancements. We have included several architecture and performance improvements, that aim towards making the library more robust in terms of storage handling for Big Data. In the NLP aspect, we have introduced a ContextualParser, DocumentLogRegClassifier and a ChunkEntityResolverSelector. These last two Annotators also target performance time and memory consumption by lowering the order of computation and data loaded to memory in each step when designed following a hierarchical pattern. We have put a big effort on this one, so please enjoy and share your comments. Your words are always welcome through all our different channels. Thank you very much for your important doubts, bug reports and feedback; they are always welcome and much appreciated. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features BigChunkEntityResolver Annotator: New experimental approach to reduce memory consumption at expense of disk IO. ContextualParser Annotator: New entity parser that works based on context parameters defined in a JSON file. ChunkEntityResolverSelector Annotator: New AnnotatorModel that takes advantage of the RecursivePipelineModel + LazyAnnotator pattern to annotate with different LazyAnnotators at runtime. DocumentLogregClassifier Annotator: New Annotator that provides a wrapped TFIDF Vectorizer + LogReg Classifier for TOKEN AnnotatorTypes (either at Document level or Chunk level) &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements normalizedColumn Param is no longer required in ChunkEntityResolver Annotator (defaults to the labelCol Param value). ChunkEntityResolverMetadata now has more data to infer whether the match is meaningful or not. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes Fixed a bug on ContextSpellChecker Annotator where unrecognized tokens would cause an exception if not in vocabulary. Fixed a bug on ChunkEntityResolver Annotator where undetermined results were coming out of negligible confidence scores for matches. Fixed a bug on ChunkEntityResolver Annotator where search would fail if the neighbours Param was grater than the number of nodes in the tree. Now it returns up to the number of nodes in the tree. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Deprecations OCR Moves to its own JSL Spark OCR project. &lt;/div&gt; Infrastructure Spark NLP License is now required to utilize the library. Please follow the instructions on the shared email.",
    "url": "/docs/en/licensed_release_notes",
    "relUrl": "/docs/en/licensed_release_notes"
  },
  "54": {
    "id": "54",
    "title": "Training",
    "content": "Training Datasets These are classes to load common datasets to train annotators for tasks such as Relation Model, Assertion models and more. Annotation tool json reader. The annotation tool json reader is a reader that generate a assertion train set from the json from annotations labs exports. Input File Format: [ { &quot;completions&quot;: [ { &quot;created_ago&quot;: &quot;2020-05-18T20:48:18.117Z&quot;, &quot;created_username&quot;: &quot;admin&quot;, &quot;id&quot;: 3001, &quot;lead_time&quot;: 19.255, &quot;result&quot;: [ { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;o752YyB2g9&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 12, &quot;labels&quot;: [ &quot;AsPresent&quot; ], &quot;start&quot;: 3, &quot;text&quot;: &quot;have faith&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;wf2U3o7I6T&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 24, &quot;labels&quot;: [ &quot;AsPresent&quot; ], &quot;start&quot;: 16, &quot;text&quot;: &quot; to trust&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;Q3BkU5eZNx&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 40, &quot;labels&quot;: [ &quot;AsPresent&quot; ], &quot;start&quot;: 35, &quot;text&quot;: &quot;to the&quot; } } ] } ], &quot;created_at&quot;: &quot;2020-05-18 20:47:53&quot;, &quot;created_by&quot;: &quot;andres.fernandez&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;To have faith is to trust yourself to the water&quot; }, &quot;id&quot;: 3 }, { &quot;completions&quot;: [ { &quot;created_ago&quot;: &quot;2020-05-17T17:52:41.563Z&quot;, &quot;created_username&quot;: &quot;andres.fernandez&quot;, &quot;id&quot;: 1, &quot;lead_time&quot;: 31.449, &quot;result&quot;: [ { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;IQjoZJNKEv&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 12, &quot;labels&quot;: [ &quot;Disease&quot; ], &quot;start&quot;: 3, &quot;text&quot;: &quot;have faith&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;tHsbn4oYy5&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 46, &quot;labels&quot;: [ &quot;Treatment&quot; ], &quot;start&quot;: 42, &quot;text&quot;: &quot;water&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;IJHkc9bxJ-&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 12, &quot;labels&quot;: [ &quot;AsPresent&quot; ], &quot;start&quot;: 0, &quot;text&quot;: &quot;To have faith&quot; } } ] } ], &quot;created_at&quot;: &quot;2020-05-17 17:52:02&quot;, &quot;created_by&quot;: &quot;andres.fernandez&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;To have faith is to trust yourself to the water&quot; }, &quot;id&quot;: 0 }, { &quot;completions&quot;: [ { &quot;created_ago&quot;: &quot;2020-05-17T17:57:19.402Z&quot;, &quot;created_username&quot;: &quot;andres.fernandez&quot;, &quot;id&quot;: 1001, &quot;lead_time&quot;: 15.454, &quot;result&quot;: [ { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;j_lT0zwtrJ&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 46, &quot;labels&quot;: [ &quot;Disease&quot; ], &quot;start&quot;: 20, &quot;text&quot;: &quot;trust yourself to the water&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;e1FuGWu7EQ&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 33, &quot;labels&quot;: [ &quot;AsPresent&quot; ], &quot;start&quot;: 19, &quot;text&quot;: &quot; trust yourself&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;q0MCSM9SXz&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 12, &quot;labels&quot;: [ &quot;Treatment&quot; ], &quot;start&quot;: 0, &quot;text&quot;: &quot;To have faith&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;9R7dvPphPX&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 12, &quot;labels&quot;: [ &quot;AsPresent&quot; ], &quot;start&quot;: 0, &quot;text&quot;: &quot;To have faith&quot; } } ] } ], &quot;created_at&quot;: &quot;2020-05-17 17:52:54&quot;, &quot;created_by&quot;: &quot;andres.fernandez&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;To have faith is to trust yourself to the water&quot; }, &quot;id&quot;: 1, &quot;predictions&quot;: [] } ] Constructor Parameters: assertion_labels: The assertions labels are used for the training dataset creation. excluded_labels: The assertions labels that are excluded for the training dataset creation. split_chars: The split chars that are used in the default tokenizer. context_chars: The context chars that are used in the default tokenizer. SDDLPath: The context chars that are used in the default tokenizer. Parameters for readDataset: spark: Initiated Spark Session with Spark NLP path: Path to the resource Refer to the documentation for more details on the API: Python API: Scala API: AnnotationToolJsonReader Show Example PythonScala from sparknlp_jsl.training import AnnotationToolJsonReader assertion_labels = [&quot;AsPresent&quot;,&quot;Absent&quot;] excluded_labels = [&quot;Treatment&quot;] split_chars = [&quot; &quot;, &quot; -&quot;] context_chars = [&quot;.&quot;, &quot;,&quot;, &quot;;&quot;, &quot;:&quot;, &quot;!&quot;, &quot;?&quot;, &quot;*&quot;, &quot;-&quot;, &quot;(&quot;, &quot;)&quot;, &quot; &quot;&quot;, &quot;&#39;&quot;,&quot;+&quot;,&quot;%&quot;,&quot;&#39;&quot;] SDDLPath = &quot;&quot; rdr = AnnotationToolJsonReader(assertion_labels = assertion_labels, excluded_labels = excluded_labels, split_chars = split_chars, context_chars = context_chars,SDDLPath=SDDLPath) path = &quot;src/test/resources/anc-pos-corpus-small/test-training.txt&quot; df = rdr.readDataset(spark, json_path) assertion_df = rdr.generateAssertionTrainSet(df) assertion_df.show() +--+--++--++ | text| target| label|start|end| +--+--++--++ |To have faith is ...| To have faith|AsPresent| 0| 2| |To have faith is ...| have faith|AsPresent| 1| 2| |To have faith is ...| to trust|AsPresent| 4| 5| |To have faith is ...| to the|AsPresent| 7| 8| |To have faith is ...| yourself|AsPresent| 6| 6| |To have faith is ...| To have faith|AsPresent| 0| 2| |To have faith is ...|trust yourself|AsPresent| 5| 6| +--+--++--++ import com.johnsnowlabs.nlp.training.POS val filename = &quot;src/test/resources/json_import.json&quot; val reader = new AnnotationToolJsonReader(assertionLabels=List(&quot;AsPresent&quot;,&quot;Absent&quot;).asJava, splitChars=List(&quot; &quot;, &quot; -&quot;).asJava, excludedLabels = List(&quot;Treatment&quot;).asJava) val df = reader.readDataset(ResourceHelper.spark, filename) val assertionDf = reader.generateAssertionTrainSet(df) assertionDf.show() +--+--++--++ | text| target| label|start|end| +--+--++--++ |To have faith is ...| To have faith|AsPresent| 0| 2| |To have faith is ...| have faith|AsPresent| 1| 2| |To have faith is ...| to trust|AsPresent| 4| 5| |To have faith is ...| to the|AsPresent| 7| 8| |To have faith is ...| yourself|AsPresent| 6| 6| |To have faith is ...| To have faith|AsPresent| 0| 2| |To have faith is ...|trust yourself|AsPresent| 5| 6| +--+--++--++ Assertion Trains AssertionDL, a deep Learning based approach used to extract Assertion Status from extracted entities and text. AssertionDLApproach Train a Assertion Model algorithm using deep learning. The training data should have annotations columns of type DOCUMENT, CHUNK, WORD_EMBEDDINGS, the labelcolumn (The assertion status that you want to predict), the start (the start index for the term that has the assertion status), the end column (the end index for the term that has the assertion status).This model use a deep learning to predict the entity. Excluding the label, this can be done with for example a SentenceDetector, a Chunk , a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). Input Annotator Types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output Annotator Type: ASSERTION Python API: AssertionDLApproach Scala API: AssertionDLApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline document_assembler = DocumentAssembler().setInputCol(&#39;text&#39;).setOutputCol(&#39;document&#39;) sentence_detector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) POSTag = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) chunker = Chunker() .setInputCols([&quot;pos&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;(&lt;NN&gt;)+&quot;]) pubmed = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) assertion_status = AssertionDLApproach() .setInputCols(&quot;sentence&quot;, &quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) .setLabelCol(&quot;label&quot;) .setLearningRate(0.01) .setDropout(0.15) .setBatchSize(16) .setEpochs(3) .setValidationSplit(0.2) .setIncludeConfidence(True) pipeline = Pipeline().setStages([ document_assembler, sentence_detector, tokenizer, POSTag, chunker, pubmed, assertion_status ]) conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) // This CoNLL dataset already includes the sentence, token, pos and label column with their respective annotator types. // If a custom dataset is used, these need to be defined. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.{Chunker, Tokenizer} import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotator.PerceptronModel import com.johnsnowlabs.nlp.annotators.assertion.dl.AssertionDLModel import com.johnsnowlabs.nlp.annotator.NerCrfApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val POSTag = PerceptronModel .pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val chunker = new Chunker() .setInputCols(Array(&quot;pos&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;(&lt;NN&gt;)+&quot;)) val pubmed = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val assertionStatus = new AssertionDLApproach() .setInputCols(&quot;sentence&quot;, &quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) .setLabelCol(&quot;label&quot;) .setLearningRate(0.01f) .setDropout(0.15f) .setBatchSize(16) .setEpochs(3) .setValidationSplit(0.2f) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, POSTag, chunker, pubmed, assertionStatus )) datasetPath = &quot;/../src/test/resources/rsAnnotations-1-120-random.csv&quot; train_data = SparkContextForTest.spark.read.option(&quot;header&quot;, &quot;true&quot;).csv(path=&quot;file:///&quot; + os.getcwd() + datasetPath) val pipelineModel = pipeline.fit(trainingData) AssertionLogRegApproach Train a Assertion Model algorithm using a regression log model. The training data should have annotations columns of type DOCUMENT, CHUNK, WORD_EMBEDDINGS, the labelcolumn (The assertion status that you want to predict), the start (the start index for the term that has the assertion status), the end column (the end index for the term that has the assertion status).This model use a deep learning to predict the entity. Excluding the label, this can be done with for example a SentenceDetector, a Chunk , a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). Input Annotator Types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output Annotator Type: ASSERTION Python API: AssertionLogRegApproach Scala API: AssertionLogRegApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline document_assembler = DocumentAssembler().setInputCol(&#39;text&#39;).setOutputCol(&#39;document&#39;) sentence_detector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) POSTag = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) chunker = Chunker() .setInputCols([&quot;pos&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;(&lt;NN&gt;)+&quot;]) pubmed = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) assertion_status = AssertionLogRegApproach() .setInputCols(&quot;sentence&quot;, &quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) .setLabelCol(&quot;label&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setEpochs(3) pipeline = Pipeline().setStages([ document_assembler, sentence_detector, tokenizer, POSTag, chunker, pubmed, assertion_status ]) conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) // This CoNLL dataset already includes the sentence, token, pos and label column with their respective annotator types. // If a custom dataset is used, these need to be defined. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.{Chunker, Tokenizer} import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotator.PerceptronModel import com.johnsnowlabs.nlp.annotators.assertion.dl.AssertionDLModel import com.johnsnowlabs.nlp.annotator.NerCrfApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val POSTag = PerceptronModel .pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val chunker = new Chunker() .setInputCols(Array(&quot;pos&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;(&lt;NN&gt;)+&quot;)) val pubmed = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val assertion = new AssertionLogRegApproach() .setLabelCol(&quot;label&quot;) .setInputCols(&quot;document&quot;, &quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, POSTag, chunker, pubmed, assertion )) datasetPath = &quot;/../src/test/resources/rsAnnotations-1-120-random.csv&quot; train_data = SparkContextForTest.spark.read.option(&quot;header&quot;, &quot;true&quot;).csv(path=&quot;file:///&quot; + os.getcwd() + datasetPath) val pipelineModel = pipeline.fit(trainingData) Token Classification These are annotators that can be trained to recognize named entities in text. MedicalNer This Named Entity recognition annotator allows to train generic NER model based on Neural Networks. The architecture of the neural network is a Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. For instantiated/pretrained models, see NerDLModel. The training data should be a labeled Spark Dataset, in the format of CoNLL 2003 IOB with Annotation type columns. The data should have columns of type DOCUMENT, TOKEN, WORD_EMBEDDINGS and an additional label column of annotator type NAMED_ENTITY. Excluding the label, this can be done with for example a SentenceDetector, a Tokenizer and a WordEmbeddingsModel with clinical embeddings (any clinical word embeddings can be chosen). Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: MedicalNerApproach Scala API: MedicalNerApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp_jsl.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline # First extract the prerequisites for the NerDLApproach documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) clinical_embeddings = WordEmbeddingsModel.pretrained(&#39;embeddings_clinical&#39;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Then the training can start nerTagger = MedicalNerApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(2) .setBatchSize(64) .setRandomSeed(0) .setVerbose(1) .setValidationSplit(0.2) .setEvaluationLogExtended(True) .setEnableOutputLogs(True) .setIncludeConfidence(True) .setOutputLogsPath(&#39;ner_logs&#39;) .setGraphFolder(&#39;medical_ner_graphs&#39;) .setEnableMemoryOptimizer(True) #&gt;&gt; if you have a limited memory and a large conll file, you can set this True to train batch by batch pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, clinical_embeddings, nerTagger ]) # We use the text and labels from the CoNLL dataset conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.ner.MedicalNerApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline // First extract the prerequisites for the NerDLApproach val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger =new MedicalNerApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(5) .setLr(0.003f) .setBatchSize(8) .setRandomSeed(0) .setVerbose(1) .setEvaluationLogExtended(false) .setEnableOutputLogs(false) .setIncludeConfidence(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) // We use the text and labels from the CoNLL dataset val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) Text Classification These are annotators that can be trained to classify text into different classes, such as sentiment. DocumentLogRegClassifier Trains a model to classify documents with a Logarithmic Regression algorithm. Training data requires columns for text and their label. The result is a trained GenericClassifierModel. Input Annotator Types: TOKEN Output Annotator Type: CATEGORY Python API: DocumentLogRegClassifierApproach Scala API: DocumentLogRegClassifierApproach Show Example PythonScala import sparknlp from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline document_assembler = DocumentAssembler() .setInputCols(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) stopwords_cleaner = StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) stemmer = Stemmer() .setInputCols(&quot;cleanTokens&quot;) .setOutputCol(&quot;stem&quot;) gen_clf = DocumentLogRegClassifierApproach() .setLabelColumn(&quot;category&quot;) .setInputCols(&quot;stem&quot;) .setOutputCol(&quot;prediction&quot;) pipeline = Pipeline().setStages([ document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, logreg ]) clf_model = pipeline.fit(data) import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.ner.MedicalNerApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline // First extract the prerequisites for the NerDLApproach val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger =new MedicalNerApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(5) .setLr(0.003f) .setBatchSize(8) .setRandomSeed(0) .setVerbose(1) .setEvaluationLogExtended(false) .setEnableOutputLogs(false) .setIncludeConfidence(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) // We use the text and labels from the CoNLL dataset val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) GenericClassifier Trains a TensorFlow model for generic classification of feature vectors. It takes FEATURE_VECTOR annotations from FeaturesAssembler as input, classifies them and outputs CATEGORY annotations. Please see the Parameters section for required training parameters. For a more extensive example please see the Spark NLP Workshop. Input Annotator Types: FEATURE_VECTOR Output Annotator Type: CATEGORY Python API: GenericClassifierApproach Scala API: GenericClassifierApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline features_asm = FeaturesAssembler() .setInputCols([&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;]) .setOutputCol(&quot;features&quot;) gen_clf = GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001) .setFixImbalance(True) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2) # keep 20% of the data for validation purposes pipeline = Pipeline().setStages([ features_asm, gen_clf ]) clf_model = pipeline.fit(data) import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.ner.MedicalNerApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline // First extract the prerequisites for the NerDLApproach val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger =new MedicalNerApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(5) .setLr(0.003f) .setBatchSize(8) .setRandomSeed(0) .setVerbose(1) .setEvaluationLogExtended(false) .setEnableOutputLogs(false) .setIncludeConfidence(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) // We use the text and labels from the CoNLL dataset val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) Relation Models RelationExtractionApproach Trains a Relation Extraction Model to predict attributes and relations for entities in a sentence. Relation Extraction is the key component for building relation knowledge graphs, and it is of crucial significance to natural language processing applications such as structured search, sentiment analysis, question answering, and summarization. The dataset will be a csv with the following that contains the following columns (sentence,chunk1,firstCharEnt1,lastCharEnt1,label1,chunk2,firstCharEnt2,lastCharEnt2,label2,rel), This annotator can be don with for example: Excluding the rel, this can be done with for example a SentenceDetector, a Tokenizer and a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). a Chunk can be created using the firstCharEnt1, lastCharEnt1,chunk1, label1 columns and firstCharEnt2, lastCharEnt2, chunk2, label2 columns An example of that dataset can be found in the following link i2b2_clinical_dataset sentence,chunk1,firstCharEnt1,lastCharEnt1,label1,chunk2,firstCharEnt2,lastCharEnt2,label2,rel Previous studies have reported the association of prodynorphin (PDYN) promoter polymorphism with temporal lobe epilepsy (TLE) susceptibility, but the results remain inconclusive.,PDYN,64,67,GENE,epilepsy,111,118,PHENOTYPE,0 The remaining cases, clinically similar to XLA, are autosomal recessive agammaglobulinemia (ARA).,XLA,43,45,GENE,autosomal recessive,52,70,PHENOTYPE,0 YAP/TAZ have been reported to be highly expressed in malignant tumors.,YAP,19,21,GENE,tumors,82,87,PHENOTYPE,0 Apart from that, no additional training data is needed. Input Annotator Types: WORD_EMBEDDINGS, POS, CHUNK, DEPENDENCY Output Annotator Type: CATEGORY Python API: RelationExtractionApproach Scala API: RelationExtractionApproach Show Example PythonScala import functools import numpy as np import pyspark.sql.functions as F import pyspark.sql.types as T from sparknlp.base import annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) @F.udf(T.ArrayType(annotationType)) def createTrainAnnotations(begin1, end1, begin2, end2, chunk1, chunk2, label1, label2): entity1 = sparknlp.annotation.Annotation(&quot;chunk&quot;, begin1, end1, chunk1, {&#39;entity&#39;: label1.upper(), &#39;sentence&#39;: &#39;0&#39;}, []) entity2 = sparknlp.annotation.Annotation(&quot;chunk&quot;, begin2, end2, chunk2, {&#39;entity&#39;: label2.upper(), &#39;sentence&#39;: &#39;0&#39;}, []) entity1.annotatorType = &quot;chunk&quot; entity2.annotatorType = &quot;chunk&quot; return [entity1, entity2] data = spark.read.option(&quot;header&quot;,&quot;true&quot;).format(&quot;csv&quot;).load(&quot;i2b2_clinical_rel_dataset.csv&quot;) data = data .withColumn(&quot;begin1i&quot;, F.expr(&quot;cast(firstCharEnt1 AS Int)&quot;)) .withColumn(&quot;end1i&quot;, F.expr(&quot;cast(lastCharEnt1 AS Int)&quot;)) .withColumn(&quot;begin2i&quot;, F.expr(&quot;cast(firstCharEnt2 AS Int)&quot;)) .withColumn(&quot;end2i&quot;, F.expr(&quot;cast(lastCharEnt2 AS Int)&quot;)) .where(&quot;begin1i IS NOT NULL&quot;) .where(&quot;end1i IS NOT NULL&quot;) .where(&quot;begin2i IS NOT NULL&quot;) .where(&quot;end2i IS NOT NULL&quot;) .withColumn( &quot;train_ner_chunks&quot;, createTrainAnnotations( &quot;begin1i&quot;, &quot;end1i&quot;, &quot;begin2i&quot;, &quot;end2i&quot;, &quot;chunk1&quot;, &quot;chunk2&quot;, &quot;label1&quot;, &quot;label2&quot; ).alias(&quot;train_ner_chunks&quot;, metadata={&#39;annotatorType&#39;: &quot;chunk&quot;})) documentAssembler = DocumentAssembler() .setInputCol(&quot;sentence&quot;) .setOutputCol(&quot;sentences&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;token&quot;) words_embedder = WordEmbeddingsModel() .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) pos_tagger = PerceptronModel() .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;pos_tags&quot;) dependency_parser = DependencyParserModel() .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) reApproach = RelationExtractionApproach() .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setLabelColumn(&quot;rel&quot;) .setEpochsNumber(70) .setBatchSize(200) .setDropout(0.5) .setLearningRate(0.001) .setModelFile(&quot;/content/RE_in1200D_out20.pb&quot;) .setFixImbalance(True) .setFromEntity(&quot;begin1i&quot;, &quot;end1i&quot;, &quot;label1&quot;) .setToEntity(&quot;begin2i&quot;, &quot;end2i&quot;, &quot;label2&quot;) .setOutputLogsPath(&#39;/content&#39;) train_pipeline = Pipeline(stages=[ documenter, tokenizer, words_embedder, pos_tagger, dependency_parser, reApproach ]) rel_model = train_pipeline.fit(data) import com.johnsnowlabs.nlp.{DocumentAssembler} import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.ner.{MedicalNerModel, NerConverter} import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel package com.johnsnowlabs.nlp.annotators.re.RelationExtractionApproach() import org.apache.spark.ml.Pipeline import org.apache.spark.sql.functions._ val data = spark.read.option(&quot;header&quot;,true).csv(&quot;src/test/resources/re/gene_hpi.csv&quot;).limit(10) def createTrainAnnotations = udf { ( begin1:Int, end1:Int, begin2:Int, end2:Int, chunk1:String, chunk2:String, label1:String, label2:String) =&gt; { val an1 = Annotation(CHUNK,begin1,end1,chunk1,Map(&quot;entity&quot; -&gt; label1.toUpperCase,&quot;sentence&quot; -&gt; &quot;0&quot;)) val an2 = Annotation(CHUNK,begin2,end2,chunk2,Map(&quot;entity&quot; -&gt; label2.toUpperCase,&quot;sentence&quot; -&gt; &quot;0&quot;)) Seq(an1,an2) } } val metadataBuilder: MetadataBuilder = new MetadataBuilder() val meta = metadataBuilder.putString(&quot;annotatorType&quot;, CHUNK).build() val dataEncoded = data .withColumn(&quot;begin1i&quot;, expr(&quot;cast(firstCharEnt1 AS Int)&quot;)) .withColumn(&quot;end1i&quot;, expr(&quot;cast(lastCharEnt1 AS Int)&quot;)) .withColumn(&quot;begin2i&quot;, expr(&quot;cast(firstCharEnt2 AS Int)&quot;)) .withColumn(&quot;end2i&quot;, expr(&quot;cast(lastCharEnt2 AS Int)&quot;)) .where(&quot;begin1i IS NOT NULL&quot;) .where(&quot;end1i IS NOT NULL&quot;) .where(&quot;begin2i IS NOT NULL&quot;) .where(&quot;end2i IS NOT NULL&quot;) .withColumn( &quot;train_ner_chunks&quot;, createTrainAnnotations( col(&quot;begin1i&quot;), col(&quot;end1i&quot;), col(&quot;begin2i&quot;), col(&quot;end2i&quot;), col(&quot;chunk1&quot;), col(&quot;chunk2&quot;), col(&quot;label1&quot;), col(&quot;label2&quot;) ).as(&quot;train_ner_chunks&quot;,meta)) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;sentence&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentences&quot;)) .setOutputCol(&quot;tokens&quot;) val embedder = ParallelDownload(WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;embeddings&quot;)) val posTagger = ParallelDownload(PerceptronModel .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;posTags&quot;)) val nerTagger = ParallelDownload(MedicalNerModel .pretrained(&quot;ner_events_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner_tags&quot;)) val nerConverter = new NerConverter() .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;)) .setOutputCol(&quot;nerChunks&quot;) val depencyParser = ParallelDownload(DependencyParserModel .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;posTags&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;dependencies&quot;)) val re = new RelationExtractionApproach() .setInputCols(Array(&quot;embeddings&quot;, &quot;posTags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;)) .setOutputCol(&quot;rel&quot;) .setLabelColumn(&quot;target_rel&quot;) .setEpochsNumber(30) .setBatchSize(200) .setlearningRate(0.001f) .setValidationSplit(0.05f) .setFromEntity(&quot;begin1i&quot;, &quot;end1i&quot;, &quot;label1&quot;) .setToEntity(&quot;end2i&quot;, &quot;end2i&quot;, &quot;label2&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embedder, posTagger, nerTagger, nerConverter, depencyParser, re).parallelDownload) val model = pipeline.fit(dataEncoded) Entity Resolution Those models predict what are the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.). SentenceEntityResolver Contains all the parameters and methods to train a SentenceEntityResolverModel. The model transforms a dataset with Input Annotation type SENTENCE_EMBEDDINGS, coming from e.g. BertSentenceEmbeddings and returns the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.) To use pretrained models please use SentenceEntityResolverModel and see the Models Hub for available models. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: ENTITY Python API: SentenceEntityResolverApproach Scala API: SentenceEntityResolverApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Training a SNOMED resolution model using BERT sentence embeddings # Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. documentAssembler = DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) bertEmbeddings = BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;bert_embeddings&quot;) snomedTrainingPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, bertEmbeddings ]) snomedTrainingModel = snomedTrainingPipeline.fit(data) snomedData = snomedTrainingModel.transform(data).cache() # Then the Resolver can be trained with bertExtractor = SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols([&quot;bert_embeddings&quot;]) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(False) snomedModel = bertExtractor.fit(snomedData) // Training a SNOMED resolution model using BERT sentence embeddings // Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. val documentAssembler = new DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val bertEmbeddings = BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;bert_embeddings&quot;) val snomedTrainingPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, bertEmbeddings )) val snomedTrainingModel = snomedTrainingPipeline.fit(data) val snomedData = snomedTrainingModel.transform(data).cache() // Then the Resolver can be trained with val bertExtractor = new SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols(&quot;bert_embeddings&quot;) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(false) val snomedModel = bertExtractor.fit(snomedData) ChunkEntityResolver Contains all the parameters and methods to train a ChunkEntityResolverModel. It transform a dataset with two Input Annotations of types TOKEN and WORD_EMBEDDINGS, coming from e.g. ChunkTokenizer and ChunkEmbeddings Annotators and returns the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.) To use pretrained models please use ChunkEntityResolverModel and see the Models Hub for available models. Input Annotator Types: TOKEN, WORD_EMBEDDINGS Output Annotator Type: ENTITY Python API: ChunkEntityResolverApproach Scala API: ChunkEntityResolverApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Training a SNOMED model # Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data # and their labels. document = DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) chunk = Doc2Chunk() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;chunk&quot;) token = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_healthcare_100d&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) chunkEmb = ChunkEmbeddings() .setInputCols([&quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;chunk_embeddings&quot;) snomedTrainingPipeline = Pipeline().setStages([ document, chunk, token, embeddings, chunkEmb ]) snomedTrainingModel = snomedTrainingPipeline.fit(data) snomedData = snomedTrainingModel.transform(data).cache() # Then the Resolver can be trained with snomedExtractor = ChunkEntityResolverApproach() .setInputCols([&quot;token&quot;, &quot;chunk_embeddings&quot;]) .setOutputCol(&quot;recognized&quot;) .setNeighbours(1000) .setAlternatives(25) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setEnableWmd(True).setEnableTfidf(True).setEnableJaccard(True) .setEnableSorensenDice(True).setEnableJaroWinkler(True).setEnableLevenshtein(True) .setDistanceWeights([1, 2, 2, 1, 1, 1]) .setAllDistancesMetadata(True) .setPoolingStrategy(&quot;MAX&quot;) .setThreshold(1e32) model = snomedExtractor.fit(snomedData) // Training a SNOMED resolution model using BERT sentence embeddings // Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. val documentAssembler = new DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val bertEmbeddings = BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;bert_embeddings&quot;) val snomedTrainingPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, bertEmbeddings )) val snomedTrainingModel = snomedTrainingPipeline.fit(data) val snomedData = snomedTrainingModel.transform(data).cache() // Then the Resolver can be trained with val bertExtractor = new SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols(&quot;bert_embeddings&quot;) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(false) val snomedModel = bertExtractor.fit(snomedData)",
    "url": "/docs/en/licensed_training",
    "relUrl": "/docs/en/licensed_training"
  },
  "55": {
    "id": "55",
    "title": "Version Compatibility",
    "content": "Spark NLP for Healthcare Spark NLP (Public) 3.3.4 3.3.4 3.3.2 3.3.2 3.3.1 3.3.1 3.3.0 3.3.0 3.2.3 3.2.3 3.2.2 3.2.2 3.2.1 3.2.1 3.2.0 3.2.1 3.1.3 3.1.3 3.1.2 3.1.2 3.1.1 3.1.0 3.1.0 3.1.0 3.0.3 3.0.3 3.0.2 3.0.2 3.0.1 3.0.1 3.0.0 3.0.1 2.7.6 2.7.4 2.7.5 2.7.4 2.7.4 2.7.3 2.7.3 2.7.3 2.7.2 2.6.5 2.7.1 2.6.4 2.7.0 2.6.3 2.6.2 2.6.2 2.6.0 2.6.0 2.5.5 2.5.5 2.5.3 2.5.3 2.5.2 2.5.2 2.5.0 2.5.0 2.4.7 2.4.5 2.4.6 2.4.5 2.4.5 2.4.5 2.4.2 2.4.2 2.4.1 2.4.1 2.4.0 2.4.0 2.3.6 2.3.6 2.3.5 2.3.5 2.3.4 2.3.4 Spark NLP for Healthcare Spark OCR 3.3.2 3.9.0 3.3.1 3.9.0",
    "url": "/docs/en/licensed_version_compatibility",
    "relUrl": "/docs/en/licensed_version_compatibility"
  },
  "56": {
    "id": "56",
    "title": "Medical Risk Scoring",
    "content": "",
    "url": "/medical_risk_scoring",
    "relUrl": "/medical_risk_scoring"
  },
  "57": {
    "id": "57",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/middle_eastern_languages",
    "relUrl": "/middle_eastern_languages"
  },
  "58": {
    "id": "58",
    "title": "Experiment Tracking",
    "content": "Serialization and Experiment Tracking with MLFlow (Python) About MLFLow Spark NLP uses Spark MLlib Pipelines, what are natively supported by MLFlow. MLFlow is, as stated in their official webpage, an open source platform for the machine learning lifecycle, that includes: Mlflow Tracking: Record and query experiments: code, data, config, and results MLflow Projects: Package data science code in a format to reproduce runs on any platform MLflow Models: Deploy machine learning models in diverse serving environments Model Registry: Store, annotate, discover, and manage models in a central repository MLFlow is also integrated in Databricks, so you will be able to track your experiments in any Databricks environment, and even use MLFLow Model Registry to serve models for production purposes, using the REST API (see section “Productionizing Spark NLP”). We will be using in this documentation Jupyter Notebook syntax. Available configurations There are several ways of deploying a MLFlow Model Registry: 1) Scenario 1: MLflow on localhost with no Tracking Server: This scenario uses a localhost folder (./mlruns by default) to serialize and store your models, but there is no tracking server available (version tracking will be disabled). 2) Scenario 2: MLflow on localhost with a Tracking Server This scenario uses a localhost folder (./mlruns by default) to serialize and store your mdoels, and a database as a Tracking Sever. It uses SQLAlchemy under the hood, so the following databases are supported: mssql, postgresql, mysql, sqlite. We are going to show how to implement this scenario with a mysql database. 3) Scenario 3: MLflow on remote with a Tracking Server This scenario is a remote version of Scenario 2. It uses a remote S3 bucket to serialize and store your mdoels, and a database as a Tracking Sever. Again, it uses SQLAlchemy for the Tracking Server under the hood, so the following databases are supported: mssql, postgresql, mysql, sqlite. In this case, you can use any service as AWS RDS or Azure SQL Database. Requirements As we said before, we are going to showcase Scenario 2. Since we want to have a Experiment Tracking Server with mysql, we will need to install in our server the requirements for it. !sudo apt-get install -y python-mysqldb mysql-server libmysqlclient-dev Also, let’s install a mysql Python interface library, called pymsql, to access mysql databases. !pip install mysqlclient pymysql We will also need MLFlow (this example was tested with version 1.21.0) !pip install mlflow Finally, make sure you follow the Spark NLP installation, available here Instantiating a MySQL database We are going to use Docker to instantiate a MySQL container with a persistent volume, but you can install it directly on your machine without Docker. To do that, we will need to have installed (feel free to skip this step if you will install MySql without Docker): Docker Docker-compose In our case, I used this docker-compose.yml file to instantiate a mysql database with a persistent volume: version: &#39;3&#39; services: # MySQL mflow_models: container_name: mlflow_models image: mysql:8.0 command: mysqld --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: mlflow_models MYSQL_USER: jsl MYSQL_PASSWORD: passpass MYSQL_ALLOW_EMPTY_PASSWORD: &quot;yes&quot; ports: - &#39;3306:3306&#39; volumes: - &#39;./docker/db/data:/var/lib/mysql&#39; - &#39;./docker/db/my.cnf:/etc/mysql/conf.d/my.cnf&#39; - &#39;./docker/db/sql:/docker-entrypoint-initdb.d&#39; Just by executing the following command in the folder where your docker-compose.yml file is, you will have your MySQL engine, with a mlflow_models database running and prepared for MLFlow Experiment Tracking: !sudo docker-compose up -d . Make sure it’s running using the following command: `!docker ps | grep -o mlflow_models Connection string You will need a connection string that will tell MLFlow (SQLAlchemy) how to reach that database. Connections strings in SQLALchemy have this format: &lt;dialect&gt;+&lt;driver&gt;://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt; In our case, we declare a CONNECTION_STRING var as: CONNECTION_STRING = f&quot;mysql+pymysql://root:root@localhost:3306/mlflow_models&quot; Imports Let’s now import all the libraries we will need. Generic imports import json import os from sklearn.metrics import classification_report import time import mlflow from mlflow.models.signature import infer_signature from urllib.parse import urlparse import pandas as pd import glob Spark NLP imports import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline import pyspark.sql.functions as F from sparknlp.training import CoNLL from pyspark.sql import SparkSession Setting the connection string in MLFLow Now that we have imported mlflow, let’s set the connection string we had prepared before. mlflow.set_tracking_uri(CONNECTION_STRING) mlflow.get_tracking_uri() # This checks if it was set properly Constant with pip_requirements MLFLow requires either a conda_env (conda environment) definition of the requirements of your models, or a pip_requirements list with all pip libraries. We will use this second way, so let’s prepare the list with Spark NLP and MLFlow: PIP_REQUIREMENTS = [f&quot;sparknlp=={sparknlp.version()}&quot;, f&quot;mlflow=={mlflow.__version__}&quot;] PIP_REQUIREMENTS # This checks if it was set properly Training a NERDLApproach() We will be showcasing the serialization and experiment tracking of NERDLApproach(). There is one specific util that is able to parse the log of that approach in order to extract the metrics and charts. Let’s get it. Ner Log Parser Util !wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/utils/ner_image_log_parser.py Now, let’s import the library: import ner_image_log_parser Starting a SparkNLP session It’s important we create a Spark NLP Session using the Session Builder, since we need to specify the jars not only of Spark NLP, but also of MLFlow. def start(): builder = SparkSession.builder .appName(&quot;Spark NLP Licensed&quot;) .master(&quot;local[80]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;256G&quot;) .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.driver.maxResultSize&quot;,&quot;4000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.2,org.mlflow:mlflow-spark:1.21.0&quot;) return builder.getOrCreate() spark = start() Training dataset preparation Let’s download some training and test datasets: !wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.train !wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.testa TRAIN_DATASET = &quot;eng.train&quot; TEST_DATASET = &quot;eng.testa&quot; Let’s read the training dataset: training_data = CoNLL().readDataset(spark, TRAIN_DATASET) training_data.show(3) Let’s get the size: %%time TRAINING_SIZE = training_data.count() TRAINING_SIZE Hyperparameters configuration Let’s configure our hyperparameter values. MODEL_NAME = &#39;&#39; # Add your model name here. Example: clinical_ner EXPERIMENT_NAME = &#39;&#39; # Add your experiment name here. Example: testing_dropout OUTPUT_DIR = f&quot;{MODEL_NAME}_{EXPERIMENT_NAME}_output&quot; # Output folder of all your model artifacts MODEL_DIR = f&quot;model&quot; # Name of the folder where the MLFlow model will be stored MAX_EPOCHS = 10 # Adapt me to your experiment LEARNING_RATE = 0.003 # Adapt me to your experiment BATCH_SIZE = 2048 # Adapt me to your experiment RANDOM_SEED = 0 # Adapt me to your experiment VALIDATION_SPLIT = 0.1 # Adapt me to your experiment Creating the experiment Now, we are ready to instantiate an experiment in MLFlow EXPERIMENT_ID = mlflow.create_experiment(f&quot;{MODEL_NAME}_{EXPERIMENT_NAME}&quot;) Each time you want to test a different thing, change the EXPERIMENT_NAME and rerun the line above to create a new entry in the experiment. By changing the experiment name, a new experiment ID will be generated. Each experiment ID groups all runs in separates folder inside ./mlruns. Pipeline creation document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&#39;document&#39;]) .setOutputCol(&#39;sentence&#39;) token = Tokenizer() .setInputCols([&#39;sentence&#39;]) .setOutputCol(&#39;token&#39;) embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) ner_approach = NerDLApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(MAX_EPOCHS) .setLr(LEARNING_RATE) .setBatchSize(BATCH_SIZE) .setRandomSeed(RANDOM_SEED) .setVerbose(1) .setEnableOutputLogs(True) .setIncludeConfidence(True) .setIncludeAllConfidenceScores(True) .setEvaluationLogExtended(True) .setOutputLogsPath(OUTPUT_DIR) .setValidationSplit(VALIDATION_SPLIT) Let’s create a preprocessing pipeline without the NerDLApproach(): ner_preprocessing_pipeline = Pipeline(stages=[ document, sentence, token, embeddings ]) And a training pipeline with it: ner_training_pipeline = Pipeline(stages = ner_preprocessing_pipeline.getStages() + [ner_approach]) Preparing inference objects Now, let’s prepare the inference as well, since we will train and infer afterwards, and store all the results of training and inference as artifacts in our MLFlow object. Test dataset preparation test_data = CoNLL().readDataset(spark, TEST_DATASET) Setting the names of the inference objects INFERENCE_NAME = &quot;inference.parquet&quot; # This is the name of the results inference on the test dataset, serialized in parquet, CLASSIFICATION_REPORT_LOG_NAME = &quot;classification_report.txt&quot; # Name of the classification report from scikit-learn on Ner Entities PREC_REC_F1_NAME = &quot;precrecf1.jpg&quot; # Name of the precision-recall-f1 file MACRO_MICRO_AVG_NAME = &quot;macromicroavg.jpg&quot; # Name of the macro-micro-average file LOSS_NAME = &quot;loss.jpg&quot; # Name of the loss plot file Now, let’s run the experiment The experiment has already been created before (see “Creating the experiment” section). So we take the ID and start a run. Each time you run execute this cell, you will get a different run for the same experiment. If you want to change the experiment id (and name), go back to “Hyperparameters configuration”. As mentioned before, by changing the experiment name, a new experiment ID will be generated. Each experiment ID groups all runs in separates folder inside ./mlruns. with mlflow.start_run(experiment_id=EXPERIMENT_ID) as run: # Printing RUN and EXPERIMENT ID # ============================== print(f&quot;Model name: {MODEL_NAME}&quot;) RUN_ID = run.info.run_id print(f&quot;Run id: {RUN_ID}&quot;) EXPERIMENT_ID = run.info.experiment_id print(f&quot;Experiment id: {EXPERIMENT_ID}&quot;) # Training the model # ================== print(&quot;Starting training...&quot;) start = time.time() ner_model = ner_training_pipeline.fit(training_data) end = time.time() ELAPSED_SEC_TRAINING = end - start print(&quot;- Finished!&quot;) # Saving the model in TensorFlow (ready to be loaded using NerDLModel.load) # ============================== print(&quot;Saving the model...&quot;) ner_model.stages[-1].write().overwrite().save(f&quot;{OUTPUT_DIR}/{MODEL_DIR}/{MODEL_NAME}&quot;) print(&quot;- Finished!&quot;) # Loading the model (to check everything worked) # ============================== print(&quot;Loading back the model...&quot;) loaded_ner_model = NerDLModel.load(f&quot;{OUTPUT_DIR}/{MODEL_DIR}/{MODEL_NAME}&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) # Creating the inference pipeline with the loaded model # ============================== ner_prediction_pipeline = Pipeline(stages = ner_preprocessing_pipeline.getStages() + [loaded_ner_model]) # Triggering inference # ============================== print(&quot;Starting inference...&quot;) prediction_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) prediction_model = ner_prediction_pipeline.fit(prediction_data) start = time.time() prediction_model.transform(test_data).write.mode(&#39;overwrite&#39;).parquet(f&quot;{OUTPUT_DIR}/{INFERENCE_NAME}&quot;) end = time.time() ELAPSED_SEC_INFERENCE = end - start print(&quot;- Finished!&quot;) # Calculating NER metrics from logs using scikit-learn &#39;classification_report&#39; # ============================== print(&quot;Starting metric calculation...&quot;) predictions = spark.read.parquet(f&quot;{OUTPUT_DIR}/{INFERENCE_NAME}&quot;) preds_df = predictions.select(F.explode(F.arrays_zip(&#39;token.result&#39;,&#39;label.result&#39;,&#39;ner.result&#39;)).alias(&quot;cols&quot;)) .select(F.expr(&quot;cols[&#39;0&#39;]&quot;).alias(&quot;token&quot;), F.expr(&quot;cols[&#39;1&#39;]&quot;).alias(&quot;ground_truth&quot;), F.expr(&quot;cols[&#39;2&#39;]&quot;).alias(&quot;prediction&quot;)).toPandas() preds_df = preds_df.fillna(value=&#39;O&#39;) with open(f&#39;{OUTPUT_DIR}/{CLASSIFICATION_REPORT_LOG_NAME}&#39;, &#39;w&#39;) as f: metrics = classification_report(preds_df[&#39;ground_truth&#39;], preds_df[&#39;prediction&#39;]) f.write(metrics) metrics_dict = classification_report(preds_df[&#39;ground_truth&#39;], preds_df[&#39;prediction&#39;], output_dict=True) print(&quot;- Finished!&quot;) # Printing metrics # ============================== print(f&quot;Training dataset size: {TRAINING_SIZE}&quot;) print(f&quot;Training time (sec): {ELAPSED_SEC_TRAINING}&quot;) print(f&quot;Inference dataset size: {TEST_SIZE}&quot;) print(f&quot;Inference time (sec): {ELAPSED_SEC_INFERENCE}&quot;) print(f&quot;Metrics: n&quot;) print(metrics) # Logging all our params, metrics, charts and artifacts using MLFlow # - log_param: logs a configuration param # - log_artifacts: logs a folder and all its files # - log_artifact: adds a file # - log_metric: logs a metric, what allows you use the MLFlow UI to visually compare results # ============================== print(&quot;Logging params, artifacts, metrics and charts in MLFlow&quot;) mlflow.log_param(&quot;training_size&quot;, TRAINING_SIZE) mlflow.log_param(&quot;training_time&quot;, ELAPSED_SEC_TRAINING) mlflow.log_param(&quot;model_name&quot;, MODEL_NAME) mlflow.log_param(&quot;test_size&quot;, TEST_SIZE) mlflow.log_param(&quot;test_time&quot;, ELAPSED_SEC_INFERENCE) mlflow.log_param(&quot;run_id&quot;, RUN_ID) mlflow.log_param(&quot;max_epochs&quot;, MAX_EPOCHS) mlflow.log_param(&quot;learning_rate&quot;, LEARNING_RATE) mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE) mlflow.log_param(&quot;random_seed&quot;, RANDOM_SEED) mlflow.log_param(&quot;validation_split&quot;, VALIDATION_SPLIT) for file in glob.glob(f&quot;{OUTPUT_DIR}/*.log&quot;): images = {} images.update(ner_image_log_parser.get_charts(file, img_prec_rec_f1_path=f&quot;{OUTPUT_DIR}/{PREC_REC_F1_NAME}&quot;, img_macro_micro_avg_path=f&quot;{OUTPUT_DIR}/{MACRO_MICRO_AVG_NAME}&quot;)) images.update(ner_image_log_parser.loss_plot(file, img_loss_path=f&quot;{OUTPUT_DIR}/{LOSS_NAME}&quot;)) mlflow.log_artifacts(OUTPUT_DIR) mlflow.log_artifact(TRAIN_DATASET) mlflow.log_artifact(TEST_DATASET) for k,v in metrics_dict.items(): if isinstance(v, dict): for kv, vv in v.items(): mlflow.log_metric(f&quot;{k}_{kv}&quot;, vv) else: mlflow.log_metric(k, v) print(&quot;- Finished!&quot;) print(&quot;Logging the model in MLFlow&quot;) # ============================== # Logging the model to be explored in the MLFLow UI tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme # Model registry does not work with file store if tracking_url_type_store != &quot;file&quot;: # Register the model # There are other ways to use the Model Registry, which depends on the use case, # please refer to the doc for more information: # https://mlflow.org/docs/latest/model-registry.html#api-workflow mlflow.spark.log_model(ner_model, f&quot;{MODEL_NAME}_{EXPERIMENT_ID}_{RUN_ID}&quot;, registered_model_name=MODEL_NAME, pip_requirements=PIP_REQUIREMENTS) else: mlflow.spark.log_model(ner_model, f&quot;{MODEL_NAME}_{EXPERIMENT_ID}_{RUN_ID}&quot;, pip_requirements=PIP_REQUIREMENTS) print(&quot;- Finished!&quot;) # Saving the model, in case you want to export it # ============================== print(&quot;Saving the model...&quot;) input_example = predictions.select(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;).limit(1).toPandas() mlflow.spark.save_model(loaded_ner_model, MODEL_NAME, pip_requirements=PIP_REQUIREMENTS, input_example=input_example) print(&quot;- Finished!&quot;) This is an example of the output generated: Model name: NER_base_2048_mlflow Run id: 5f8601fbfc664b3b91c7c61cde31e16d Experiment id: 2 Starting training... - Finished! Saving the model... - Finished! Loading back the model... Starting inference... - Finished! Starting metric calculation... - Finished! Training dataset size: 14041 Training time (sec): 12000.3835768699646 Inference dataset size: 3250 Inference time (sec): 2900.713200330734253 Metrics: precision recall f1-score support B-LOC 0.85 0.82 0.83 1837 B-MISC 0.86 0.83 0.81 922 B-ORG 0.81 0.83 0.82 1341 B-PER 0.86 0.81 0.80 1842 I-LOC 0.80 0.80 0.80 257 I-MISC 0.80 0.80 0.80 346 I-ORG 0.83 0.89 0.80 751 I-PER 0.86 0.83 0.82 1307 O 0.81 0.98 0.84 43792 accuracy 0.87 52395 macro avg 0.88 0.83 0.88 52395 weighted avg 0.84 0.87 0.85 52395 Logging params, artifacts, metrics and charts in MLFlow - Finished! Logging the model in MLFlow Registered model &#39;NER_base_2048_mlflow&#39; already exists. Creating a new version of this model... 2021/11/25 11:51:24 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: NER_base_2048_mlflow, version 2 Created version &#39;2&#39; of model &#39;NER_base_2048_mlflow&#39;. - Finished! Saving the model... - Finished! MLFLow UI to check results Now, we just need to launch the MLFLow UI to see: All the experiments All the runs in each experiment The automatic versioning in the Tracking Server database in MySQL THe MLFlow model, and the TensorFlow version as well The UI for comparing the metrics we set using log_metrics The UI for visualizing the image artifacts we have logged (charts) etc !mlflow ui --backend-store-uri $CONNECTION_STRING Some example screenshots",
    "url": "/docs/en/mlflow",
    "relUrl": "/docs/en/mlflow"
  },
  "59": {
    "id": "59",
    "title": "Models",
    "content": "Pretrained Models Pretrained Models have moved to Models Hub. Please follow this link for the updated list of all models and pipelines: Models Hub How to use Pretrained Models Online You can follow this approach to use Spark NLP pretrained models: # load NER model trained by deep learning approach and GloVe word embeddings ner_dl = NerDLModel.pretrained(&#39;ner_dl&#39;) # load NER model trained by deep learning approach and BERT word embeddings ner_bert = NerDLModel.pretrained(&#39;ner_dl_bert&#39;) The default language is en, so for other laguages you should set the language: // load French POS tagger model trained by Universal Dependencies val french_pos = PerceptronModel.pretrained(&quot;pos_ud_gsd&quot;, lang=&quot;fr&quot;) // load Italain LemmatizerModel val italian_lemma = LemmatizerModel.pretrained(&quot;lemma_dxc&quot;, lang=&quot;it&quot;) Offline If you have any trouble using online pipelines or models in your environment (maybe it’s air-gapped), you can directly download them for offline use. After downloading offline models/pipelines and extracting them, here is how you can use them iside your code (the path could be a shared storage like HDFS in a cluster): Loading PerceptronModel annotator model inside Spark NLP Pipeline val french_pos = PerceptronModel.load(&quot;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) Public Models If you wish to use a pre-trained model for a specific annotator in your pipeline, you need to use the annotator which is mentioned under Model following with pretrained(name, lang) function. Example to load a pretraiand BERT model or NER model: bert = BertEmbeddings.pretrained(name=&#39;bert_base_cased&#39;, lang=&#39;en&#39;) ner_onto = NerDLModel.pretrained(name=&#39;ner_dl_bert&#39;, lang=&#39;en&#39;) NOTE: build means the model can be downloaded or loaded for that specific version or above. For instance, 2.4.0 can be used in all the releases after 2.4.x but not before. Dutch - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 nl Download PerceptronModel (POS UD) pos_ud_alpino 2.5.0 nl Download NerDLModel (glove_100d) wikiner_6B_100 2.5.0 nl Download NerDLModel (glove_6B_300) wikiner_6B_300 2.5.0 nl Download NerDLModel (glove_840B_300) wikiner_840B_300 2.5.0 nl Download English - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma_antbnc 2.0.2 en Download PerceptronModel (POS) pos_anc 2.0.2 en Download PerceptronModel (POS UD) pos_ud_ewt 2.2.2 en Download NerCrfModel (NER with GloVe) ner_crf 2.4.0 en Download NerDLModel (NER with GloVe) ner_dl 2.4.3 en Download NerDLModel (NER with BERT) ner_dl_bert 2.4.3 en Download NerDLModel (OntoNotes with GloVe 100d) onto_100 2.4.0 en Download NerDLModel (OntoNotes with GloVe 300d) onto_300 2.4.0 en Download SymmetricDeleteModel (Spell Checker) spellcheck_sd 2.0.2 en Download NorvigSweetingModel (Spell Checker) spellcheck_norvig 2.0.2 en Download ViveknSentimentModel (Sentiment) sentiment_vivekn 2.0.2 en Download DependencyParser (Dependency) dependency_conllu 2.0.8 en Download TypedDependencyParser (Dependency) dependency_typed_conllu 2.0.8 en Download Embeddings Model Name Build Lang Offline WordEmbeddings (GloVe) glove_100d 2.4.0 en Download BertEmbeddings bert_base_uncased 2.4.0 en Download BertEmbeddings bert_base_cased 2.4.0 en Download BertEmbeddings bert_large_uncased 2.4.0 en Download BertEmbeddings bert_large_cased 2.4.0 en Download ElmoEmbeddings elmo 2.4.0 en Download UniversalSentenceEncoder (USE) tfhub_use 2.4.0 en Download UniversalSentenceEncoder (USE) tfhub_use_lg 2.4.0 en Download AlbertEmbeddings albert_base_uncased 2.5.0 en Download AlbertEmbeddings albert_large_uncased 2.5.0 en Download AlbertEmbeddings albert_xlarge_uncased 2.5.0 en Download AlbertEmbeddings albert_xxlarge_uncased 2.5.0 en Download XlnetEmbeddings xlnet_base_cased 2.5.0 en Download XlnetEmbeddings xlnet_large_cased 2.5.0 en Download Classification Model Name Build Lang Offline ClassifierDL (with tfhub_use) classifierdl_use_trec6 2.5.0 en Download ClassifierDL (with tfhub_use) classifierdl_use_trec50 2.5.0 en Download SentimentDL (with tfhub_use) sentimentdl_use_imdb 2.5.0 en Download SentimentDL (with tfhub_use) sentimentdl_use_twitter 2.5.0 en Download SentimentDL (with glove_100d) sentimentdl_glove_imdb 2.5.0 en Download French - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.0.2 fr Download PerceptronModel (POS UD) pos_ud_gsd 2.0.2 fr Download NerDLModel (glove_840B_300) wikiner_840B_300 2.0.2 fr Download Feature Description   Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura   POS Trained by PerceptronApproach annotator on the Universal Dependencies   NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities   German - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.0.8 de Download PerceptronModel (POS UD) pos_ud_hdt 2.0.8 de Download NerDLModel (glove_840B_300) wikiner_840B_300 2.4.0 de Download Feature Description Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Italian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma_dxc 2.0.2 it Download ViveknSentimentAnalysis (Sentiment) sentiment_dxc 2.0.2 it Download PerceptronModel (POS UD) pos_ud_isdt 2.0.8 it Download NerDLModel (glove_840B_300) wikiner_840B_300 2.4.0 it Download Feature Description Lemma Trained by Lemmatizer annotator on DXC Technology dataset POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Norwegian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 nb Download PerceptronModel (POS UD) pos_ud_nynorsk 2.5.0 nn Download PerceptronModel (POS UD) pos_ud_bokmaal 2.5.0 nb Download NerDLModel (glove_100d) norne_6B_100 2.5.0 no Download NerDLModel (glove_6B_300) norne_6B_300 2.5.0 no Download NerDLModel (glove_840B_300) norne_840B_300 2.5.0 no Download Polish - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 pl Download PerceptronModel (POS UD) pos_ud_lfg 2.5.0 pl Download NerDLModel (glove_100d) wikiner_6B_100 2.5.0 pl Download NerDLModel (glove_6B_300) wikiner_6B_300 2.5.0 pl Download NerDLModel (glove_840B_300) wikiner_840B_300 2.5.0 pl Download Portuguese - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 pt Download PerceptronModel (POS UD) pos_ud_bosque 2.5.0 pt Download NerDLModel (glove_100d) wikiner_6B_100 2.5.0 pt Download NerDLModel (glove_6B_300) wikiner_6B_300 2.5.0 pt Download NerDLModel (glove_840B_300) wikiner_840B_300 2.5.0 pt Download Russian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.4.4 ru Download PerceptronModel (POS UD) pos_ud_gsd 2.4.4 ru Download NerDLModel (glove_100d) wikiner_6B_100 2.4.4 ru Download NerDLModel (glove_6B_300) wikiner_6B_300 2.4.4 ru Download NerDLModel (glove_840B_300) wikiner_840B_300 2.4.4 ru Download Feature Description Lemma Trained by Lemmatizer annotator on the Universal Dependencies POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Spanish - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.4.0 es Download PerceptronModel (POS UD) pos_ud_gsd 2.4.0 es Download NerDLModel (glove_100d) wikiner_6B_100 2.4.0 es Download NerDLModel (glove_6B_300) wikiner_6B_300 2.4.0 es Download NerDLModel (glove_840B_300) wikiner_840B_300 2.4.0 es Download Feature Description Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Bulgarian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 bg Download PerceptronModel (POS UD) pos_ud_btb 2.5.0 bg Download Czech - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 cs Download PerceptronModel (POS UD) pos_ud_pdt 2.5.0 cs Download Greek - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 el Download PerceptronModel (POS UD) pos_ud_gdt 2.5.0 el Download Finnish - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 fi Download PerceptronModel (POS UD) pos_ud_tdt 2.5.0 fi Download Hungarian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 hu Download PerceptronModel (POS UD) pos_ud_szeged 2.5.0 hu Download Romanian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 ro Download PerceptronModel (POS UD) pos_ud_rrt 2.5.0 ro Download Slovak - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 sk Download PerceptronModel (POS UD) pos_ud_snk 2.5.0 sk Download Swedish - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 sv Download PerceptronModel (POS UD) pos_ud_tal 2.5.0 sv Download Turkish - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 tr Download PerceptronModel (POS UD) pos_ud_imst 2.5.0 tr Download Ukrainian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 uk Download PerceptronModel (POS UD) pos_ud_iu 2.5.0 uk Download Multi-language Model Name Build Lang Offline WordEmbeddings (GloVe) glove_840B_300 2.4.0 xx Download WordEmbeddings (GloVe) glove_6B_300 2.4.0 xx Download BertEmbeddings (multi_cased) bert_multi_cased 2.4.0 xx Download LanguageDetectorDL ld_wiki_7 2.5.2 xx Download LanguageDetectorDL ld_wiki_20 2.5.2 xx Download The model with 7 languages: Czech, German, English, Spanish, French, Italy, and Slovak The model with 20 languages: Bulgarian, Czech, German, Greek, English, Spanish, Finnish, French, Croatian, Hungarian, Italy, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Swedish, Turkish, and Ukrainian Please follow this link for the updated list: https://github.com/JohnSnowLabs/spark-nlp-models",
    "url": "/models",
    "relUrl": "/models"
  },
  "60": {
    "id": "60",
    "title": "Available Models and Pipelines",
    "content": "",
    "url": "/models",
    "relUrl": "/models"
  },
  "61": {
    "id": "61",
    "title": "NLP Models Hub",
    "content": "The Annotation Lab 1.8.0 offers a tight integration with NLP Models Hub. Any compatible NER model and Embeddings can be downloaded and made available to the Annotation Lab users for preannotations either from within the application or via manual upload. Models Hub page can be accessed via the left navigation menu by users in the UserAdmins group. The Models Hub page has three tabs that hold information about models and embeddings. Models Hub Tab This tab lists all pretrained NER models and embeddings from NLP Models Hub which are compatible with Spark NLP 2.7.5 and which are defined for English language. By selecting one or multiple items from the grid view, users can download them to the Annotation Lab. The licensed/Healthcare models and embeddings are available to download only when a valid license is uploaded. One restriction that we impose on models download/upload relates to the available disk space. Any model download requires that the double of its size is available on the local storage. If not enough space is available the download cannot proceed. Disk usage view, search, and filter features are available on the upper section of the Models Hub page. Available Models Tab All the models available in the Annotation Lab are listed in this tab. The models are either trained within the Annotation Lab, uploaded to Annotation Lab by admin users, or downloaded from NLP Models Hub. General information about the models like labels/categories and the source (downloaded or trained or uploaded) of the model can be viewed. It is possible to delete any model or redownload failed ones by using the overflow menu on the top right corner of each model. Available Embeddings Tab This tab lists all embeddings available to the Annotation Lab together with information on their source and date of upload/download. Like models, any compatible embeddings can be downloaded from NLP Models Hub. By default, glove_100d embeddings are included in the deployment. Custom Models/Embeddings Upload Custom NER models or embeddings can be uploaded using the Upload button present in the top right corner of the page. The labels predicted by the uploaded NER model need to be specified using the Model upload form. The models&amp; embeddings to upload need to be Spark NLP compatible. All available models are listed in the Spark NLP Pipeline Config on the Setup Page of any project and are ready to be included in the Labeling Config for preannotation.",
    "url": "/docs/en/alab/models_hub",
    "relUrl": "/docs/en/alab/models_hub"
  },
  "62": {
    "id": "62",
    "title": "NLP Server",
    "content": "This is a ready to use NLP Server for analyzing text documents using NLU library. Over 4000+ industry grade NLP Models in 300+ Languages are available to use via a simple and intuitive UI, without writing a line of code. For more expert users and more complex tasks, NLP Server also provides a REST API that can be used to process high amounts of data. The models, refered to as spells, are provided by the NLU library and powered by the most widely used NLP library in the industry, Spark NLP. NLP Server is free for everyone to download and use. There is no limitation in the amount of text to analyze. You can setup NLP-Server as a Docker Machine in any enviroment or get it via the AWS Marketplace in just 1 click. Web UI The Web UI is accessible at the following URL: http://localhost:5000/ It allows a very simple and intuitive interaction with the NLP Server. As a first step the user chooses the spell from the first dropdown. All NLU spells are available. Then the user has to provide a text document for analysis. This can be done by either copy/pasting text on the text box, or by uploading a csv/json file. After selecting the grouping option, the user clicks on the Preview button to get the results for the first 10 rows of text. REST API NLP Server includes a REST API which can be used to process any amount of data using NLU. Once you deploy the NLP Server, you can access the API documentation at the following URL http://localhost:5000/docs. How to use in Python import requests # Invoke Processing with tokenization spell r = requests.post(f&#39;http://localhost:5000/api/results&#39;,json={&quot;spell&quot;: &quot;tokenize&quot;,&quot;data&quot;: &quot;I love NLU! &lt;3&quot;}) # Use the uuid to get your processed data uuid = r.json()[&#39;uuid&#39;] # Get status of processing r = requests.get(f&#39;http://localhost:5000/api/results/{uuid}/status&#39;).json &gt;&gt;&gt; {&#39;status&#39;: {&#39;code&#39;: &#39;success&#39;, &#39;message&#39;: None}} # Get results r = requests.get(f&#39;http://localhost:5000/api/results/{uuid}&#39;).json() &gt;&gt;&gt; {&#39;sentence&#39;: {&#39;0&#39;: [&#39;I love NLU! &lt;3&#39;]}, &#39;document&#39;: {&#39;0&#39;: &#39;I love NLU! &lt;3&#39;}, &#39;token&#39;: {&#39;0&#39;: [&#39;I&#39;, &#39;love&#39;, &#39;NLU&#39;, &#39;!&#39;, &#39;&lt;3&#39;]}}",
    "url": "/docs/en/nlp_server/nlp_server",
    "relUrl": "/docs/en/nlp_server/nlp_server"
  },
  "63": {
    "id": "63",
    "title": "Spark OCR",
    "content": "Spark OCR is another commercial extension of Spark NLP for optical character recognition from images, scanned PDF documents, Microsoft DOCX and DICOM files. If you want to try it out on your own documents click on the below button: Try Free Spark OCR is built on top of Apache Spark and offers the following capabilities: Image pre-processing algorithms to improve text recognition results: Adaptive thresholding &amp; denoising Skew detection &amp; correction Adaptive scaling Layout Analysis &amp; region detection Image cropping Removing background objects Text recognition, by combining NLP and OCR pipelines: Extracting text from images (optical character recognition) Support English, German, French, Spanish, Russian, Vietnamese and Arabic languages Extracting data from tables Recognizing and highlighting named entities in PDF documents Masking sensitive text in order to de-identify images Table detection and recognition from images Signature detection Visual document understanding Document classification Visual NER Output generation in different formats: PDF, images, or DICOM files with annotated or masked entities Digital text for downstream processing in Spark NLP or other libraries Structured data formats (JSON and CSV), as files or Spark data frames Scale out: distribute the OCR jobs across multiple nodes in a Spark cluster. Frictionless unification of OCR, NLP, ML &amp; DL pipelines. Spark OCR Workshop If you prefer learning by example, click the button below to checkout the workshop repository full of fresh examples. Spark OCR Workshop Below, you can follow a more theoretical and thorough quick start guide. Quickstart Examples Images The following code example creates an OCR Pipeline for processing image(s). The image file(s) can contain complex layout like columns, tables, images inside. PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers._ val imagePath = &quot;path to image files&quot; // Read image files as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) // Transform binary content to image val binaryToImage = new BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) // OCR val ocr = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) // Define Pipeline val pipeline = new Pipeline() pipeline.setStages(Array( binaryToImage, ocr )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = modelPipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image files&quot; # Read image files as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) # Transform binary content to image binaryToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # OCR ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) # Define Pipeline pipeline = PipelineModel(stages=[ binaryToImage, ocr ]) data = pipeline.transform(df) data.show() Scanned PDF files Next sample provides an example of OCR Pipeline for processing PDF files containing image data. In this case, the PdfToImage transformer is used to convert PDF file to a set of images. PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers._ val imagePath = &quot;path to pdf files&quot; // Read pdf files as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) // Transform PDF file to the image val pdfToImage = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) // OCR val ocr = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( pdfToImage, ocr )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = modelPipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to pdf files&quot; # Read pdf files as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) # Transform PDF file to the image pdfToImage = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # OCR ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) # Define pipeline pipeline = PipelineModel(stages=[ pdfToImage, ocr ]) data = pipeline.transform(df) data.show() PDF files (scanned or text) In the following code example we will create OCR Pipeline for processing PDF files that contain text or image data. For each PDF file, this pipeline will: extract the text from document and save it to the text column if text contains less than 10 characters (so the document isn’t PDF with text layout) it will process the PDF file as a scanned document: convert PDF file to an image detect and split image to regions run OCR and save output to the text column PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers._ val imagePath = &quot;path to PDF files&quot; // Read PDF files as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) // Extract text from PDF text layout val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) // In case of `text` column contains less then 10 characters, // pipeline run PdfToImage as fallback method val pdfToImage = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setFallBackCol(&quot;text&quot;) .setMinSizeBeforeFallback(10) // OCR val ocr = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( pdfToText, pdfToImage, ocr )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = modelPipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to PDF files&quot; # Read PDF files as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) # Extract text from PDF text layout pdfToText = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) # In case of `text` column contains less then 10 characters, # pipeline run PdfToImage as fallback method pdfToImage = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setFallBackCol(&quot;text&quot;) .setMinSizeBeforeFallback(10) # OCR ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) # Define pipeline pipeline = PipelineModel(stages=[ pdfToText, pdfToImage, ocr, ]) data = pipeline.transform(df) data.show() Images (streaming mode) Next code segments provide an example of streaming OCR pipeline. It processes images and stores results to memory table. PythonScala val imagePath = &quot;path folder with images&quot; val batchDataFrame = spark.read.format(&quot;binaryFile&quot;).load(imagePath).limit(1) val pipeline = new Pipeline() pipeline.setStages(Array( binaryToImage, binarizer, ocr )) val modelPipeline = pipeline.fit(batchDataFrame) // Read files in streaming mode val dataFrame = spark.readStream .format(&quot;binaryFile&quot;) .schema(batchDataFrame.schema) .load(imagePath) // Call pipeline and store results to &#39;results&#39; memory table val query = modelPipeline.transform(dataFrame) .select(&quot;text&quot;, &quot;exception&quot;) .writeStream .format(&quot;memory&quot;) .queryName(&quot;results&quot;) .start() imagePath = &quot;path folder with images&quot; batchDataFrame = spark.read.format(&quot;binaryFile&quot;).load(imagePath).limit(1) pipeline = Pipeline() pipeline.setStages(Array( binaryToImage, binarizer, ocr )) modelPipeline = pipeline.fit(batchDataFrame) # Read files in streaming mode dataFrame = spark.readStream .format(&quot;binaryFile&quot;) .schema(batchDataFrame.schema) .load(imagePath) # Call pipeline and store results to &#39;results&#39; memory table query = modelPipeline.transform(dataFrame) .select(&quot;text&quot;, &quot;exception&quot;) .writeStream() .format(&quot;memory&quot;) .queryName(&quot;results&quot;) .start() For getting results from memory table following code could be used: PythonScala spark.table(&quot;results&quot;).select(&quot;path&quot;, &quot;text&quot;).show() spark.table(&quot;results&quot;).select(&quot;path&quot;, &quot;text&quot;).show() More details about Spark Structured Streaming could be found in spark documentation. Advanced Topics Error Handling Pipeline execution would not be interrupted in case of the runtime exceptions while processing some records. In this case OCR transformers would fill exception column that contains transformer name and exception. NOTE: Storing runtime errors to the exception field allows to process batch of files. Output Here is an output with exception when try to process js file using OCR pipeline: PythonScala result.select(&quot;path&quot;, &quot;text&quot;, &quot;exception&quot;).show(2, false) result.select(&quot;path&quot;, &quot;text&quot;, &quot;exception&quot;).show(2, False) +-+-+--+ |path |text |exception | +-+-+--+ |file:jquery-1.12.3.js | |BinaryToImage_c0311dc62161: Can&#39;t open file as image.| |file:image.png |I prefer the morning flight through Denver |null | +-+-+--+ Performance In case of big count of text PDF’s in dataset need have manual partitioning for avoid skew in partitions and effective utilize resources. For example the randomization could be used.",
    "url": "/docs/en/ocr",
    "relUrl": "/docs/en/ocr"
  },
  "64": {
    "id": "64",
    "title": "Installation",
    "content": "Spark OCR is built on top of Apache Spark. Currently, it supports 3.0., 2.4. and 2.3.* versions of Spark. It is recommended to have basic knowledge of the framework and a working environment before using Spark OCR. Refer to Spark documentation to get started with Spark. Spark OCR required: Scala 2.11 or 2.12 related to the Spark version Python 3.7 + (in case using PySpark) Before you start, make sure that you have: Spark OCR jar file (or secret for download it) Spark OCR python wheel file License key If you don’t have a valid subscription yet and you want to test out the Spark OCR library press the button below: Try Free Spark OCR from Scala You can start a spark REPL with Scala by running in your terminal a spark-shell including the com.johnsnowlabs.nlp:spark-ocr_2.11:1.0.0 package: spark-shell --jars #### The #### is a secret url only avaliable for license users. If you have purchansed a license but did not receive it please contact us at info@johnsnowlabs.com. Start Spark OCR Session The following code will initialize the spark session in case you have run the jupyter notebook directly. If you have started the notebook using pyspark this cell is just ignored. Initializing the spark session takes some seconds (usually less than 1 minute) as the jar from the server needs to be loaded. The #### in .config(“spark.jars”, “####”) is a secret code, if you have not received it please contact us at info@johnsnowlabs.com. import org.apache.spark.sql.SparkSession val spark = SparkSession .builder() .appName(&quot;Spark OCR&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;4G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;2G&quot;) .config(&quot;spark.jars&quot;, &quot;####&quot;) .getOrCreate() Spark OCR from Python Install Python package Install python package using pip: pip install spark-ocr==1.8.0.spark24 --extra-index-url #### --ignore-installed The #### is a secret url only avaliable for license users. If you have purchansed a license but did not receive it please contact us at info@johnsnowlabs.com. Start Spark OCR Session Manually from pyspark.sql import SparkSession spark = SparkSession .builder .appName(&quot;Spark OCR&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;4G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;2G&quot;) .config(&quot;spark.jars&quot;, &quot;https://pypi.johnsnowlabs.com/####&quot;) .getOrCreate() Using Start function Another way to initialize SparkSession with Spark OCR to use start function in Python. Start function has following params: Param name Type Default Description secret string None Secret for download Spark OCR jar file jar_path string None Path to jar file in case you need to run spark session offline extra_conf SparkConf None Extra spark configuration master_url string local[*] Spark master url nlp_version string None Spark NLP version for add it Jar to session nlp_internal boolean/string None Run Spark session with Spark NLP Internal if set to ‘True’ or specify version nlp_secret string None Secret for get Spark NLP Internal jar keys_file string keys.json Name of the json file with license, secret and aws keys For start Spark session with Spark NLP please specify version of it in nlp_version param. Example: from sparkocr import start spark = start(secret=secret, nlp_version=&quot;2.4.4&quot;) Databricks The installation process to Databricks includes following steps: Installing Spark OCR library to Databricks and attaching it to the cluster Same step for Spark OCR python wheel file Adding license key Adding cluster init script for install dependencies Please look databricks python helpers for simplify install init script. Example notebooks: Spark OCR Databricks python notebooks Spark OCR Databricks Scala notebooks",
    "url": "/docs/en/ocr_install",
    "relUrl": "/docs/en/ocr_install"
  },
  "65": {
    "id": "65",
    "title": "Object detection",
    "content": "ImageHandwrittenDetector ImageHandwrittenDetector is a DL model for detect handwritten text on the image. It based on Cascade Region-based CNN network. Detector support following labels: ‘signature’ ‘date’ ‘name’ ‘title’ ‘address’ ‘others’ Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description scoreThreshold float 0.5 Score threshold for output regions. outputLabels Array[String]   White list for output labels. labels Array[String]   List of labels Output Columns Param name Type Default Column Data Description outputCol string table_regions array of [Coordinaties]ocr_structures#coordinate-schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect signature val signature_detector = ImageHandwrittenDetector .pretrained(&quot;image_signature_detector_gsa0628&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;signature_regions&quot;) val draw_regions = new ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;signature_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, signature_detector, draw_regions ]) val data = pipeline.transform(df) data.storeImage(&quot;image_with_regions&quot;) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect signature signature_detector = ImageHandwrittenDetector .pretrained(&quot;image_signature_detector_gsa0628&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;signature_regions&quot;) draw_regions = ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;signature_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, signature_detector, draw_regions ]) data = pipeline.transform(df) display_images(data, &quot;image_with_regions&quot;) Output:",
    "url": "/docs/en/ocr_object_detection",
    "relUrl": "/docs/en/ocr_object_detection"
  },
  "66": {
    "id": "66",
    "title": "Spark OCR 2.3.x (Licensed)",
    "content": "Spark NLP comes with an OCR module that can read both PDF files and scanned images (requires Tesseract 4.x+). Installation Installing Tesseract As mentioned above, if you are dealing with scanned images instead of test-selectable PDF files you need to install tesseract 4.x+ on all the nodes in your cluster. Here how you can install it on Ubuntu/Debian: apt-get install tesseract-ocr In Databricks this command may result in installing tesseract 3.x instead of version 4.x. You can simply run this init script to install tesseract 4.x in your Databricks cluster: #!/bin/bash sudo apt-get install -y g++ # or clang++ (presumably) sudo apt-get install -y autoconf automake libtool sudo apt-get install -y pkg-config sudo apt-get install -y libpng-dev sudo apt-get install -y libjpeg8-dev sudo apt-get install -y libtiff5-dev sudo apt-get install -y zlib1g-dev ​ wget http://www.leptonica.org/source/leptonica-1.74.4.tar.gz tar xvf leptonica-1.74.4.tar.gz cd leptonica-1.74.4 ./configure make sudo make install ​ git clone --single-branch --branch 4.1 https://github.com/tesseract-ocr/tesseract.git cd tesseract ./autogen.sh ./configure make sudo make install sudo ldconfig ​ tesseract -v Quick start Let’s read a PDF file: import com.johnsnowlabs.nlp._ val ocrHelper = new OcrHelper() //If you do this locally you can use file:/// or hdfs:/// if the files are hosted in Hadoop val dataset = ocrHelper.createDataset(spark, &quot;/tmp/sample_article.pdf&quot;) If you are trying to extract text from scanned images in the format of PDF, please keep in mind to use these configs: ocrHelper.setPreferredMethod(&quot;image&quot;) ocrHelper.setFallbackMethod(false) ocrHelper.setMinSizeBeforeFallback(0) Configuration setPreferredMethod(text/image = text) either text or image will work. Defaults to text. Text mode works better and faster for digital or text scanned PDFs setFallbackMethod(boolean) on true, when text or image fail, it will fallback to the alternate method setMinSizeBeforeFallback(int = 1) number of characters to have at a minimum, before falling back. setPageSegMode(int = 3) image mode page segmentation mode setEngineMode(int = 1) image mode engine mode setPageIteratorLevel(int = 0) image mode page iteratior level setScalingFactor(float) Specifies the scaling factor to apply to images, in both axes, before OCR. It can scale up the image(factor &gt; 1.0) or scale it down(factor &lt; 1.0) setSplitPages(boolean = true) Whether to split pages into different rows and documents setSplitRegions(boolean = true) Whether to split by document regions. Works only in image mode. Enables split pages as well. setIncludeConfidence(boolean = false) setAutomaticSkewCorrection(use: boolean, half_angle: double = 5.0, resolution: double = 1.0) setAutomaticSizeCorrection(use: boolean, desired_size: int = 34) setEstimateNoise(string) image mode estimator noise level useErosion(use: boolean, kernel_size: int = 2, kernel_shape: Int = 0) image mode erosion Utilizing Spark NLP OCR Module Spark NLP OCR Module is not included within Spark NLP. It is not an annotator and not an extension to Spark ML. You can use OcrHelper to directly create spark dataframes from PDF. This will hold entire documents in single rows, meant to be later processed by a SentenceDetector. This way, you won’t be breaking the content in rows as if you were reading a standard document. Metadata columns are added automatically and will include page numbers, file name and other useful information per row. Python code from pyspark.sql import SparkSession from sparknlp.ocr import OcrHelper from sparknlp import DocumentAssembler data = OcrHelper().createDataset(spark = spark, input_path = &quot;/your/example.pdf&quot; ) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;) annotations = documentAssembler.transform(data) annotations.columns [&#39;text&#39;, &#39;pagenum&#39;, &#39;method&#39;, &#39;noiselevel&#39;, &#39;confidence&#39;, &#39;positions&#39;, &#39;filename&#39;, &#39;document&#39;] Scala code import com.johnsnowlabs.nlp.util.io.OcrHelper import com.johnsnowlabs.nlp.DocumentAssembler val myOcrHelper = new OcrHelper val data = myOcrHelper.createDataset(spark, &quot;/your/example.pdf&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;) val annotations = documentAssembler.transform(data) annotations.columns Array[String] = Array(text, pagenum, method, noiselevel, confidence, positions, filename, document) … where the text column of the annotations spark dataframe includes the text content of the PDF, pagenum the page number, etc… Creating an Array of Strings from PDF (For LightPipeline) Another way, would be to simply create an array of strings. This is useful for example if you are parsing a small amount of pdf files and would like to use LightPipelines instead. See an example below. Scala code import com.johnsnowlabs.nlp.util.io.OcrHelper import com.johnsnowlabs.nlp.{DocumentAssembler,LightPipeline} import com.johnsnowlabs.nlp.annotator.SentenceDetector import org.apache.spark.ml.Pipeline val myOcrHelper = new OcrHelper val raw = myOcrHelper.createMap(&quot;/pdfs/&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val lightPipeline = new LightPipeline(new Pipeline().setStages(Array(documentAssembler, sentenceDetector)).fit(Seq.empty[String].toDF(&quot;text&quot;))) val annotations = ligthPipeline.annotate(raw.values.toArray) Now to get the whole first PDF content in your /pdfs/ folder you can use: annotations(0)(&quot;document&quot;)(0) and to get the third sentence found in that first pdf: annotations(0)(&quot;sentence&quot;)(2) To get from the fifth pdf the second sentence: annotations(4)(&quot;sentence&quot;)(1) Similarly, the whole content of the fifth pdf can be retrieved by: annotations(4)(&quot;document&quot;)(0)",
    "url": "/docs/en/ocr_old",
    "relUrl": "/docs/en/ocr_old"
  },
  "67": {
    "id": "67",
    "title": "Pipeline components",
    "content": "PDF processing Next section describes the transformers that deal with PDF files with the purpose of extracting text and image data from PDF files. PdfToText PDFToText extracts text from selectable PDF (with text layout). Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PDF document originCol string path path to the original file Parameters Param name Type Default Description splitPage bool true whether it needed to split document to pages textStripper   TextStripperType.PDF_TEXT_STRIPPER   sort bool false Sort text during extraction with TextStripperType.PDF_LAYOUT_STRIPPER partitionNum int 0 Force repartition dataframe if set to value more than 0. onlyPageNum bool false Extract only page numbers. extractCoordinates bool false Extract coordinates and store to the positions column storeSplittedPdf bool false Store one page pdf’s for process it using PdfToImage. Output Columns Param name Type Default Column Data Description outputCol string text extracted text pageNumCol string pagenum page number or 0 when splitPage = false NOTE: For setting parameters use setParamName method. Example PythonScala import com.johnsnowlabs.ocr.transformers.PdfToText val pdfPath = &quot;path to pdf with text layout&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val transformer = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;pagenum&quot;) .setSplitPage(true) val data = transformer.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() from sparkocr.transformers import * pdfPath = &quot;path to pdf with text layout&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) transformer = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;pagenum&quot;) .setSplitPage(true) data = transformer.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() Output: +-+-+ |pagenum|text | +-+-+ |0 |This is a page. | |1 |This is another page. | |2 |Yet another page. | +-+-+ PdfToImage PdfToImage renders PDF to an image. To be used with scanned PDF documents. Output dataframe contains total_pages field with total number of pages. For process pdf with big number of pages prefer to split pdf by setting splitNumBatch param. Number of partitions should be equal number of cores/executors. Input Columns Param name Type Default Column Data Description inputCol string content binary representation of the PDF document originCol string path path to the original file fallBackCol string text extracted text from previous method for detect if need to run transformer as fallBack Parameters Param name Type Default Description splitPage bool true whether it needed to split document to pages minSizeBeforeFallback int 10 minimal count of characters to extract to decide, that the document is the PDF with text layout imageType ImageType ImageType.TYPE_BYTE_GRAY type of the image resolution int 300 Output image resolution in dpi keepInput boolean false Keep input column in dataframe. By default it is dropping. partitionNum int 0 Number of Spark RDD partitions (0 value - without repartition) binarization boolean false Enable/Disable binarization image after extract image. binarizationParams Array[String] null Array of Binarization params in key=value format. splitNumBatch int 0 Number of partitions or size of partitions, related to the splitting strategy. partitionNumAfterSplit int 0 Number of Spark RDD partitions after splitting pdf document (0 value - without repartition). splittingStategy SplittingStrategy SplittingStrategy.FIXED_SIZE_OF_PARTITION Splitting strategy. Output Columns Param name Type Default Column Data Description outputCol string image extracted image struct (Image schema) pageNumCol string pagenum page number or 0 when splitPage = false Example: PythonScala import com.johnsnowlabs.ocr.transformers.PdfToImage val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToImage = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;pagenum&quot;) .setSplitPage(true) val data = pdfToImage.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() from sparkocr.transformers import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdfToImage = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;pagenum&quot;) .setSplitPage(true) data = pdfToImage.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() ImageToPdf ImageToPdf transform image to Pdf document. If dataframe contains few records for same origin path, it groups image by origin column and create multipage PDF document. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string content binary representation of the PDF document Example: Read images and store it as single page PDF documents. PythonScala import com.johnsnowlabs.ocr.transformers._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(imagePath) // Define transformer for convert to Image struct val binaryToImage = new BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) // Define transformer for store to PDF val imageToPdf = new ImageToPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;content&quot;) // Call transformers val image_df = binaryToImage.transform(df) val pdf_df = pdfToImage.transform(image_df) pdf_df.select(&quot;content&quot;).show() from sparkocr.transformers import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) # Define transformer for convert to Image struct binaryToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for store to PDF imageToPdf = ImageToPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;content&quot;) # Call transformers image_df = binaryToImage.transform(df) pdf_df = pdfToImage.transform(image_df) pdf_df.select(&quot;content&quot;).show() TextToPdf TextToPdf renders ocr results to PDF document as text layout. Each symbol will render to same position with same font size as in original image or PDF. If dataframe contains few records for same origin path, it groups image by origin column and create multipage PDF document. Input Columns Param name Type Default Column Data Description inputCol string positions column with positions struct inputImage string image image struct (Image schema) inputText string text column name with recognized text originCol string path path to the original file inputContent string content column name with binary representation of original PDF file Output Columns Param name Type Default Column Data Description outputCol string pdf binary representation of the PDF document Example: Read PDF document, run OCR and render results to PDF document. PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers._ val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToImage = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image_raw&quot;) .setResolution(400) val binarizer = new ImageBinarizer() .setInputCol(&quot;image_raw&quot;) .setOutputCol(&quot;image&quot;) .setThreshold(130) val ocr = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setIgnoreResolution(false) .setPageSegMode(PageSegmentationMode.SPARSE_TEXT) .setConfidenceThreshold(60) val textToPdf = new TextToPdf() .setInputCol(&quot;positions&quot;) .setInputImage(&quot;image&quot;) .setOutputCol(&quot;pdf&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( pdfToImage, binarizer, ocr, textToPdf )) val modelPipeline = pipeline.fit(df) val pdf = modelPipeline.transform(df) val pdfContent = pdf.select(&quot;pdf&quot;).collect().head.getAs[Array[Byte]](0) // store to file val tmpFile = Files.createTempFile(suffix=&quot;.pdf&quot;).toAbsolutePath.toString val fos = new FileOutputStream(tmpFile) fos.write(pdfContent) fos.close() println(tmpFile) from sparkocr.transformers import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_image = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image_raw&quot;) binarizer = ImageBinarizer() .setInputCol(&quot;image_raw&quot;) .setOutputCol(&quot;image&quot;) .setThreshold(130) ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setIgnoreResolution(False) .setPageSegMode(PageSegmentationMode.SPARSE_TEXT) .setConfidenceThreshold(60) textToPdf = TextToPdf() .setInputCol(&quot;positions&quot;) .setInputImage(&quot;image&quot;) .setOutputCol(&quot;pdf&quot;) pipeline = PipelineModel(stages=[ pdf_to_image, binarizer, ocr, textToPdf ]) result = pipeline.transform(df).collect() # Store to file for debug with open(&quot;test.pdf&quot;, &quot;wb&quot;) as file: file.write(result[0].pdf) PdfAssembler PdfAssembler group single page PDF documents by the filename and assemble muliplepage PDF document. Input Columns Param name Type Default Column Data Description inputCol string page_pdf binary representation of the PDF document originCol string path path to the original file pageNumCol string pagenum for compatibility with another transformers Output Columns Param name Type Default Column Data Description outputCol string pdf binary representation of the PDF document Example: PythonScala import java.io.FileOutputStream import java.nio.file.Files import com.johnsnowlabs.ocr.transformers._ val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdf_to_image = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setKeepInput(True) // Run OCR and render results to PDF val ocr = new ImageToTextPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;pdf_page&quot;) // Assemble multipage PDF val pdf_assembler = new PdfAssembler() .setInputCol(&quot;pdf_page&quot;) .setOutputCol(&quot;pdf&quot;) // Create pipeline val pipeline = new Pipeline() .setStages(Array( pdf_to_image, ocr, pdf_assembler )) val pdf = pipeline.fit(df).transform(df) val pdfContent = pdf.select(&quot;pdf&quot;).collect().head.getAs[Array[Byte]](0) // store to pdf file val tmpFile = Files.createTempFile(&quot;with_regions_&quot;, s&quot;.pdf&quot;).toAbsolutePath.toString val fos = new FileOutputStream(tmpFile) fos.write(pdfContent) fos.close() println(tmpFile) from pyspark.ml import PipelineModel from sparkocr.transformers import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_image = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setKeepInput(True) # Run OCR and render results to PDF ocr = ImageToTextPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;pdf_page&quot;) # Assemble multipage PDF pdf_assembler = PdfAssembler() .setInputCol(&quot;pdf_page&quot;) .setOutputCol(&quot;pdf&quot;) pipeline = PipelineModel(stages=[ pdf_to_image, ocr, pdf_assembler ]) pdf = pipeline.transform(df) pdfContent = pdf.select(&quot;pdf&quot;).collect().head.getAs[Array[Byte]](0) # store pdf to file with open(&quot;test.pdf&quot;, &quot;wb&quot;) as file: file.write(pdfContent[0].pdf) PdfDrawRegions PdfDrawRegions transformer for drawing regions to Pdf document. Input Columns Param name Type Default Column Data Description inputCol string content binary representation of the PDF document originCol string path path to the original file inputRegionsCol string region input column which contain regions Parameters Param name Type Default Description lineWidth integer 1 line width for draw regions Output Columns Param name Type Default Column Data Description outputCol string pdf_regions binary representation of the PDF document Example: PythonScala import java.io.FileOutputStream import java.nio.file.Files import com.johnsnowlabs.ocr.transformers._ import com.johnsnowlabs.nlp.{DocumentAssembler, SparkAccessor} import com.johnsnowlabs.nlp.annotators._ import com.johnsnowlabs.nlp.util.io.ReadAs val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) val positionFinder = new PositionFinder() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;coordinates&quot;) .setPageMatrixCol(&quot;positions&quot;) .setMatchingWindow(10) .setPadding(2) val pdfDrawRegions = new PdfDrawRegions() .setInputRegionsCol(&quot;coordinates&quot;) // Create pipeline val pipeline = new Pipeline() .setStages(Array( pdfToText, documentAssembler, sentenceDetector, tokenizer, entityExtractor, positionFinder, pdfDrawRegions )) val pdfWithRegions = pipeline.fit(df).transform(df) val pdfContent = pdfWithRegions.select(&quot;pdf_regions&quot;).collect().head.getAs[Array[Byte]](0) // store to pdf to tmp file val tmpFile = Files.createTempFile(&quot;with_regions_&quot;, s&quot;.pdf&quot;).toAbsolutePath.toString val fos = new FileOutputStream(tmpFile) fos.write(pdfContent) fos.close() println(tmpFile) from pyspark.ml import Pipeline from sparkocr.transformers import * from sparknlp.annotator import * from sparknlp.base import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;page&quot;) .setSplitPage(False) document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) entity_extractor = TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;./sparkocr/resources/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) position_finder = PositionFinder() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;coordinates&quot;) .setPageMatrixCol(&quot;positions&quot;) .setMatchingWindow(10) .setPadding(2) draw = PdfDrawRegions() .setInputRegionsCol(&quot;coordinates&quot;) .setOutputCol(&quot;pdf_with_regions&quot;) .setInputCol(&quot;content&quot;) .setLineWidth(1) pipeline = Pipeline(stages=[ pdf_to_text, document_assembler, sentence_detector, tokenizer, entity_extractor, position_finder, draw ]) pdfWithRegions = pipeline.fit(df).transform(df) pdfContent = pdfWithRegions.select(&quot;pdf_regions&quot;).collect().head.getAs[Array[Byte]](0) # store to pdf to tmp file with open(&quot;test.pdf&quot;, &quot;wb&quot;) as file: file.write(pdfContent[0].pdf_regions) Results: PdfToTextTable Extract tables from Pdf document page. Input is a column with binary representation of PDF document. As output generate column with tables and tables text chunks coordinates (rows/cols). Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PDF document originCol string path path to the original file Parameters Param name Type Default Description pageIndex integer -1 Page index to extract Tables. guess bool false A logical indicating whether to guess the locations of tables on each page. method string decide Identifying the prefered method of table extraction: basic, spreadsheet. Output Columns Param name Type Default Column Data Description outputCol TableContainer tables Extracted tables Example: PythonScala import java.io.FileOutputStream import java.nio.file.Files import com.johnsnowlabs.ocr.transformers._ import com.johnsnowlabs.nlp.{DocumentAssembler, SparkAccessor} import com.johnsnowlabs.nlp.annotators._ import com.johnsnowlabs.nlp.util.io.ReadAs val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToTextTable = new PdfToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;table&quot;) .pdf_to_text_table.setPageIndex(1) .pdf_to_text_table.setMethod(&quot;basic&quot;) table = pdfToTextTable.transform(df) // Show first row table.select(table[&quot;table.chunks&quot;].getItem(1)[&quot;chunkText&quot;]).show(1, False) from pyspark.ml import Pipeline from sparkocr.transformers import * from sparknlp.annotator import * from sparknlp.base import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text_table = PdfToTextTable() pdf_to_text_table.setInputCol(&quot;content&quot;) pdf_to_text_table.setOutputCol(&quot;table&quot;) pdf_to_text_table.setPageIndex(1) pdf_to_text_table.setMethod(&quot;basic&quot;) table = pdf_to_text_table.transform(df) # Show first row table.select(table[&quot;table.chunks&quot;].getItem(1)[&quot;chunkText&quot;]).show(1, False) Output: ++ |table.chunks AS chunks#760[1].chunkText | ++ |[Mazda RX4, 21.0, 6, , 160.0, 110, 3.90, 2.620, 16.46, 0, 1, 4, 4]| ++ DOCX processing Next section describes the transformers that deal with DOCX files with the purpose of extracting text and table data from it. DocToText DocToText extracts text from the DOCX document. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the DOCX document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string text extracted text pageNumCol string pagenum for compatibility with another transformers NOTE: For setting parameters use setParamName method. Example PythonScala import com.johnsnowlabs.ocr.transformers.DocToText val docPath = &quot;path to docx with text layout&quot; // Read DOCX file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new DocToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) val data = transformer.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() from sparkocr.transformers import * docPath = &quot;path to docx with text layout&quot; # Read DOCX file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = DocToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) data = transformer.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() DocToTextTable DocToTextTable extracts table data from the DOCX documents. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PDF document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol TableContainer tables Extracted tables NOTE: For setting parameters use setParamName method. Example PythonScala import com.johnsnowlabs.ocr.transformers.DocToTextTable val docPath = &quot;path to docx with text layout&quot; // Read DOCX file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new DocToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;tables&quot;) val data = transformer.transform(df) data.select(&quot;tables&quot;).show() from sparkocr.transformers import * docPath = &quot;path to docx with text layout&quot; # Read DOCX file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = DocToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;tables&quot;) data = transformer.transform(df) data.select(&quot;tables&quot;).show() DocToPdf DocToPdf convert DOCX document to PDF document. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the DOCX document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string text binary representation of the PDF document NOTE: For setting parameters use setParamName method. Example PythonScala import com.johnsnowlabs.ocr.transformers.DocToPdf val docPath = &quot;path to docx with text layout&quot; // Read DOCX file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new DocToPdf() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;pdf&quot;) val data = transformer.transform(df) data.select(&quot;pdf&quot;).show() from sparkocr.transformers import * docPath = &quot;path to docx with text layout&quot; # Read DOCX file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = DocToPdf() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;pdf&quot;) data = transformer.transform(df) data.select(&quot;pdf&quot;).show() PptToTextTable PptToTextTable extracts table data from the PPT and PPTX documents. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PPT document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol TableContainer tables Extracted tables NOTE: For setting parameters use setParamName method. Example PythonScala import com.johnsnowlabs.ocr.transformers.PptToTextTable val docPath = &quot;path to docx with text layout&quot; // Read PPT file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new PptToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;tables&quot;) val data = transformer.transform(df) data.select(&quot;tables&quot;).show() from sparkocr.transformers import * docPath = &quot;path to docx with text layout&quot; # Read PPT file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = PptToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;tables&quot;) data = transformer.transform(df) data.select(&quot;tables&quot;).show() PptToPdf PptToPdf convert PPT and PPTX document to PDF document. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PPT document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string text binary representation of the PDF document NOTE: For setting parameters use setParamName method. Example PythonScala import com.johnsnowlabs.ocr.transformers.PptToPdf val docPath = &quot;path to docx with text layout&quot; // Read PPT file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new PptToPdf() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;pdf&quot;) val data = transformer.transform(df) data.select(&quot;pdf&quot;).show() from sparkocr.transformers import * docPath = &quot;path to PPT with text layout&quot; # Read DOCX file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = PptToPdf() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;pdf&quot;) data = transformer.transform(df) data.select(&quot;pdf&quot;).show() Dicom processing DicomToImage DicomToImage transforms dicom object (loaded as binary file) to image struct. Input Columns Param name Type Default Column Data Description inputCol string content binary dicom object originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string image extracted image struct (Image schema) pageNumCol integer pagenum page (image) number begin from 0 metadataCol string metadata Output column name for dicom metatdata ( json formatted ) Scala example: PythonScala import com.johnsnowlabs.ocr.transformers.DicomToImage val dicomPath = &quot;path to dicom files&quot; // Read dicom file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(dicomPath) val dicomToImage = new DicomToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setMetadataCol(&quot;meta&quot;) val data = dicomToImage.transform(df) data.select(&quot;image&quot;, &quot;pagenum&quot;, &quot;meta&quot;).show() from sparkocr.transformers import * dicomPath = &quot;path to dicom files&quot; # Read dicom file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(dicomPath) dicomToImage = DicomToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setMetadataCol(&quot;meta&quot;) data = dicomToImage.transform(df) data.select(&quot;image&quot;, &quot;pagenum&quot;, &quot;meta&quot;).show() ImageToDicom ImageToDicom transforms image to Dicom document. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) originCol string path path to the original file metadataCol string metadata dicom metatdata ( json formatted ) Output Columns Param name Type Default Column Data Description outputCol string dicom binary dicom object Scala example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageToDicom val imagePath = &quot;path to image file&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToDicom = new ImageToDicom() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;dicom&quot;) val data = imageToDicom.transform(df) data.select(&quot;dicom&quot;).show() from sparkocr.transformers import * imagePath = &quot;path to image file&quot; # Read image file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(imagePath) binaryToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) image_df = binaryToImage.transform(df) imageToDicom = ImageToDicom() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;dicom&quot;) data = imageToDicom.transform(image_df) data.select(&quot;dicom&quot;).show() Image pre-processing Next section describes the transformers for image pre-processing: scaling, binarization, skew correction, etc. BinaryToImage BinaryToImage transforms image (loaded as binary file) to image struct. Input Columns Param name Type Default Column Data Description inputCol string content binary representation of the image originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string image extracted image struct (Image schema) Scala example: PythonScala import com.johnsnowlabs.ocr.transformers.BinaryToImage val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(imagePath) val binaryToImage = new BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) val data = binaryToImage.transform(df) data.select(&quot;image&quot;).show() from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(imagePath) binaryToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) data = binaryToImage.transform(df) data.select(&quot;image&quot;).show() GPUImageTransformer GPUImageTransformer allows to run image pre-processing operations on GPU. It supports following operations: Scaling Otsu thresholding Huang thresholding Erosion Dilation GPUImageTransformer allows to add few operations. For add operations need to call one of the methods with params: Method name Params Description addScalingTransform factor Scale image by scaling factor. addOtsuTransform   The automatic thresholder utilizes the Otsu threshold method. addHuangTransform   The automatic thresholder utilizes the Huang threshold method. addDilateTransform width, height Computes the local maximum of a pixels rectangular neighborhood. The rectangles size is specified by its half-width and half-height. addErodeTransform width, height Computes the local minimum of a pixels rectangular neighborhood. The rectangles size is specified by its half-width and half-height Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description imageType ImageType ImageType.TYPE_BYTE_BINARY Type of the output image gpuName string ”” GPU device name. Output Columns Param name Type Default Column Data Description outputCol string transformed_image image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.GPUImageTransformer import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new GPUImageTransformer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;transformed_image&quot;) .addHuangTransform() .addScalingTransform(3) .addDilateTransform(2, 2) .setImageType(ImageType.TYPE_BYTE_BINARY) val data = transformer.transform(df) data.storeImage(&quot;transformed_image&quot;) from sparkocr.transformers import * from sparkocr.enums import ImageType from sparkocr.utils import display_images imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) transformer = GPUImageTransformer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;transformed_image&quot;) .addHuangTransform() .addScalingTransform(3) .addDilateTransform(2, 2) .setImageType(ImageType.TYPE_BYTE_BINARY) pipeline = PipelineModel(stages=[ binary_to_image, transformer ]) result = pipeline.transform(df) display_images(result, &quot;transformed_image&quot;) ImageBinarizer ImageBinarizer transforms image to binary color schema by threshold. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description threshold int 170   Output Columns Param name Type Default Column Data Description outputCol string binarized_image image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageBinarizer import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val binirizer = new ImageBinarizer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;binary_image&quot;) .setThreshold(100) val data = binirizer.transform(df) data.storeImage(&quot;binary_image&quot;) from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) binirizer = ImageBinarizer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;binary_image&quot;) .setThreshold(100) data = binirizer.transform(df) data.show() Original image: Binarized image with 100 threshold: ImageAdaptiveBinarizer Supported Methods: OTSU Gaussian local thresholding. Thresholds the image using a locally adaptive threshold that is computed using a local square region centered on each pixel. The threshold is equal to the gaussian weighted sum of the surrounding pixels times the scale. Sauvola Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description width float 90 Width of square region. method TresholdingMethod TresholdingMethod.GAUSSIAN Method used to determine adaptive threshold. scale float 1.1f Scale factor used to adjust threshold. imageType ImageType ImageType.TYPE_BYTE_BINARY Type of the output image Output Columns Param name Type Default Column Data Description outputCol string binarized_image image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val binirizer = new ImageAdaptiveBinarizer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;binary_image&quot;) .setWidth(100) .setScale(1.1) val data = binirizer.transform(df) data.storeImage(&quot;binary_image&quot;) from pyspark.ml import PipelineModel from sparkocr.transformers import * from sparkocr.utils import display_image imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) adaptive_thresholding = ImageAdaptiveBinarizer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;binarized_image&quot;) .setWidth(100) .setScale(1.1) pipeline = PipelineModel(stages=[ binary_to_image, adaptive_thresholding ]) result = pipeline.transform(df) for r in result.select(&quot;image&quot;, &quot;corrected_image&quot;).collect(): display_image(r.image) display_image(r.corrected_image) ImageAdaptiveThresholding Compute a threshold mask image based on local pixel neighborhood and apply it to image. Also known as adaptive or dynamic thresholding. The threshold value is the weighted mean for the local neighborhood of a pixel subtracted by a constant. Supported methods: GAUSSIAN MEAN MEDIAN WOLF SINGH Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description blockSize int 170 Odd size of pixel neighborhood which is used to calculate the threshold value (e.g. 3, 5, 7, …, 21, …). method AdaptiveThresholdingMethod AdaptiveThresholdingMethod.GAUSSIAN Method used to determine adaptive threshold for local neighbourhood in weighted mean image. offset int   Constant subtracted from weighted mean of neighborhood to calculate the local threshold value. Default offset is 0. mode string   The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to ‘constant’ cval int   Value to fill past edges of input if mode is ‘constant’. Output Columns Param name Type Default Column Data Description outputCol string binarized_image image struct (Image schema) Example: PythonScala // Implemented only for Python from pyspark.ml import PipelineModel from sparkocr.transformers import * from sparkocr.utils import display_image imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) adaptive_thresholding = ImageAdaptiveThresholding() .setInputCol(&quot;scaled_image&quot;) .setOutputCol(&quot;binarized_image&quot;) .setBlockSize(21) .setOffset(73) pipeline = PipelineModel(stages=[ binary_to_image, adaptive_thresholding ]) result = pipeline.transform(df) for r in result.select(&quot;image&quot;, &quot;corrected_image&quot;).collect(): display_image(r.image) display_image(r.corrected_image) Original image: Binarized image: ImageScaler ImageScaler scales image by provided scale factor or needed output size. It supports keeping original ratio of image by padding the image in case fixed output size. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description scaleFactor double 1.0 scale factor keepRatio boolean false Keep original ratio of image width int 0 Output width of image height int 0 Outpu height of imgae Output Columns Param name Type Default Column Data Description outputCol string scaled_image scaled image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageScaler import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageScaler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;scaled_image&quot;) .setScaleFactor(0.5) val data = transformer.transform(df) data.storeImage(&quot;scaled_image&quot;) from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) transformer = ImageScaler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;scaled_image&quot;) .setScaleFactor(0.5) data = transformer.transform(df) data.show() ImageAdaptiveScaler ImageAdaptiveScaler detects font size and scales image for have desired font size. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description desiredSize int 34 desired size of font in pixels Output Columns Param name Type Default Column Data Description outputCol string scaled_image scaled image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageAdaptiveScaler import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageAdaptiveScaler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;scaled_image&quot;) .setDesiredSize(34) val data = transformer.transform(df) data.storeImage(&quot;scaled_image&quot;) from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) transformer = ImageAdaptiveScaler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;scaled_image&quot;) .setDesiredSize(34) data = transformer.transform(df) data.show() ImageSkewCorrector ImageSkewCorrector detects skew of the image and rotates it. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description rotationAngle double 0.0 rotation angle automaticSkewCorrection boolean true enables/disables adaptive skew correction halfAngle double 5.0 half the angle(in degrees) that will be considered for correction resolution double 1.0 The step size(in degrees) that will be used for generating correction angle candidates Output Columns Param name Type Default Column Data Description outputCol string corrected_image corrected image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageSkewCorrector import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageSkewCorrector() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;corrected_image&quot;) .setAutomaticSkewCorrection(true) val data = transformer.transform(df) data.storeImage(&quot;corrected_image&quot;) from sparkocr.transformers import * val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageSkewCorrector() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;corrected_image&quot;) .setAutomaticSkewCorrection(true) val data = transformer.transform(df) data.show() Original image: Corrected image: ImageNoiseScorer ImageNoiseScorer computes noise score for each region. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) inputRegionsCol string regions regions Parameters Param name Type Default Description method NoiseMethod string NoiseMethod.RATIO method of computation noise score Output Columns Param name Type Default Column Data Description outputCol string noisescores noise score for each region Example: PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.{ImageNoiseScorer, ImageLayoutAnalyzer} import com.johnsnowlabs.ocr.NoiseMethod import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect regions val layoutAnalyzer = new ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) // Define transformer for compute noise level for each region val noisescorer = new ImageNoiseScorer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;noiselevel&quot;) .setInputRegionsCol(&quot;regions&quot;) .setMethod(NoiseMethod.VARIANCE) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( layoutAnalyzer, noisescorer )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = modelPipeline.transform(df) data.select(&quot;path&quot;, &quot;noiselevel&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * from sparkocr.enums import NoiseMethod imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) # Define transformer for detect regions layoutAnalyzer = ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) # Define transformer for compute noise level for each region noisescorer = ImageNoiseScorer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;noiselevel&quot;) .setInputRegionsCol(&quot;regions&quot;) .setMethod(NoiseMethod.VARIANCE) # Define pipeline pipeline = Pipeline() pipeline.setStages(Array( layoutAnalyzer, noisescorer )) data = pipeline.transform(df) data.select(&quot;path&quot;, &quot;noiselevel&quot;).show() Output: ++--+ |path |noiselevel | ++--+ |file:./noisy.png |[32.01805641767766, 32.312916551193354, 29.99257352247787, 30.62470388308217]| ++--+ ImageRemoveObjects python only ImageRemoveObjects for remove background objects. It support removing: objects less then elements of font with minSizeFont size objects less then minSizeObject holes less then minSizeHole objects more then maxSizeObject Input Columns Param name Type Default Column Data Description inputCol string None image struct (Image schema) Parameters Param name Type Default Description minSizeFont int 10 Min size font in pt. minSizeObject int None Min size of object which will keep on image [*]. connectivityObject int 0 The connectivity defining the neighborhood of a pixel. minSizeHole int None Min size of hole which will keep on image[ *]. connectivityHole int 0 The connectivity defining the neighborhood of a pixel. maxSizeObject int None Max size of object which will keep on image [*]. connectivityMaxObject int 0 The connectivity defining the neighborhood of a pixel. [*] : None value disables removing objects. Output Columns Param name Type Default Column Data Description outputCol string None scaled image struct (Image schema) Example: PythonScala // Implemented only for Python from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) remove_objects = ImageRemoveObjects() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;corrected_image&quot;) .setMinSizeObject(20) pipeline = PipelineModel(stages=[ binary_to_image, remove_objects ]) data = pipeline.transform(df) ImageMorphologyOperation python only ImageMorphologyOperationis a transformer for applying morphological operations to image. It supports following operation: Erosion Dilation Opening Closing Input Columns Param name Type Default Column Data Description inputCol string None image struct (Image schema) Parameters Param name Type Default Description operation MorphologyOperationType MorphologyOperationType.OPENING Operation type kernelShape KernelShape KernelShape.DISK Kernel shape. kernelSize int 1 Kernel size in pixels. Output Columns Param name Type Default Column Data Description outputCol string None scaled image struct (Image schema) Example: PythonScala // Implemented only for Python from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setOperation(MorphologyOperationType.OPENING) adaptive_thresholding = ImageAdaptiveThresholding() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;corrected_image&quot;) .setBlockSize(75) .setOffset(0) opening = ImageMorphologyOperation() .setInputCol(&quot;corrected_image&quot;) .setOutputCol(&quot;opening_image&quot;) .setkernelSize(1) pipeline = PipelineModel(stages=[ binary_to_image, adaptive_thresholding, opening ]) result = pipeline.transform(df) for r in result.select(&quot;image&quot;, &quot;corrected_image&quot;).collect(): display_image(r.image) display_image(r.corrected_image) Original image: Opening image: ImageCropper ImageCropperis a transformer for cropping image. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description cropRectangle Rectangle Rectangle(0,0,0,0) Image rectangle. cropSquareType CropSquareType CropSquareType.TOP_LEFT Type of square. Output Columns Param name Type Default Column Data Description outputCol string cropped_image scaled image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageAdaptiveScaler import com.johnsnowlabs.ocr.OcrContext.implicits._ import java.awt.Rectangle val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val rectangle: Rectangle = new Rectangle(0, 0, 200, 110) val cropper: ImageCropper = new ImageCropper() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cropped_image&quot;) .setCropRectangle(rectangle) val data = transformer.transform(df) data.storeImage(&quot;cropped_image&quot;) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setOperation(MorphologyOperationType.OPENING) cropper = ImageCropper() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cropped_image&quot;) .setCropRectangle((0, 0, 200, 110)) pipeline = PipelineModel(stages=[ binary_to_image, cropper ]) result = pipeline.transform(df) for r in result.select(&quot;image&quot;, &quot;cropped_image&quot;).collect(): display_image(r.image) display_image(r.cropped_image) Splitting image to regions ImageLayoutAnalyzer ImageLayoutAnalyzer analyzes the image and determines regions of text. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description pageSegMode PageSegmentationMode AUTO page segmentation mode pageIteratorLevel PageIteratorLevel BLOCK page iteration level ocrEngineMode EngineMode LSTM_ONLY OCR engine mode Output Columns Param name Type Default Column Data Description outputCol string region array of [Coordinaties]ocr_structures#coordinate-schema) Example: PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.{ImageSplitRegions, ImageLayoutAnalyzer} import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect regions val layoutAnalyzer = new ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) val data = layoutAnalyzer.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect regions layout_analyzer = ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, layout_analyzer ]) data = pipeline.transform(df) data.show() ImageSplitRegions ImageSplitRegions splits image to regions. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) inputRegionsCol string region array of [Coordinaties]ocr_structures#coordinate-schema) Parameters Param name Type Default Description explodeCols Array[string]   Columns which need to explode Output Columns Param name Type Default Column Data Description outputCol string region_image image struct (Image schema) Example: PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.{ImageSplitRegions, ImageLayoutAnalyzer} import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect regions val layoutAnalyzer = new ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) val splitter = new ImageSplitRegions() .setInputCol(&quot;image&quot;) .setRegionCol(&quot;regions&quot;) .setOutputCol(&quot;region_image&quot;) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( layoutAnalyzer, splitter )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = pipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect regions layout_analyzer = ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) splitter = ImageSplitRegions() .setInputCol(&quot;image&quot;) .setRegionCol(&quot;regions&quot;) .setOutputCol(&quot;region_image&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, layout_analyzer, splitter ]) data = pipeline.transform(df) data.show() ImageDrawAnnotations ImageDrawAnnotations draw annotations with label and score to the image. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) inputChunksCol string region array of Annotation Parameters Param name Type Default Description lineWidth Int 4 Line width for draw rectangles fontSize Int 12 Font size for render labels and score rectColor Color Color.black Color of lines Output Columns Param name Type Default Column Data Description outputCol string image_with_chunks image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToHocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val tokenizer = new HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) val draw_annotations = new ImageDrawAnnotations() .setInputCol(&quot;image&quot;) .setInputChunksCol(&quot;token&quot;) .setOutputCol(&quot;image_with_annotations&quot;) .setFilledRect(False) .setFontSize(40) .setRectColor(Color.red) val pipeline = new Pipeline() pipeline.setStages(Array( imageToHocr, tokenizer, draw_annotations )) val modelPipeline = pipeline.fit(df) val result = modelPipeline.transform(df) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) tokenizer = HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) draw_annotations = ImageDrawAnnotations() .setInputCol(&quot;image&quot;) .setInputChunksCol(&quot;token&quot;) .setOutputCol(&quot;image_with_annotations&quot;) .setFilledRect(False) .setFontSize(40) .setRectColor(Color.red) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr, tokenizer, image_with_annotations ]) result = pipeline.transform(df) ImageDrawRegions ImageDrawRegions draw regions with label and score to the image. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) inputRegionsCol string region array of [Coordinaties]ocr_structures#coordinate-schema) Parameters Param name Type Default Description lineWidth Int 4 Line width for draw rectangles fontSize Int 12 Font size for render labels and score Output Columns Param name Type Default Column Data Description outputCol string image_with_regions image struct (Image schema) Example: PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.{ImageSplitRegions, ImageLayoutAnalyzer} import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect regions val layoutAnalyzer = new ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) val draw = new ImageDrawRegions() .setInputCol(&quot;image&quot;) .setRegionCol(&quot;regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( layoutAnalyzer, draw )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = pipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect regions layout_analyzer = ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) draw = ImageDrawRegions() .setInputCol(&quot;image&quot;) .setRegionCol(&quot;regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, layout_analyzer, draw ]) data = pipeline.transform(df) data.show() Characters recognition Next section describes the estimators for OCR ImageToText ImageToText runs OCR for input image, return recognized text to outputCol and positions with font size to ‘positionsCol’ column. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description pageSegMode PageSegmentationMode AUTO page segmentation mode pageIteratorLevel PageIteratorLevel BLOCK page iteration level ocrEngineMode EngineMode LSTM_ONLY OCR engine mode language Language Language.ENG language confidenceThreshold int 0 Confidence threshold. ignoreResolution bool false Ignore resolution from metadata of image. ocrParams array of strings [] Array of Ocr params in key=value format. pdfCoordinates bool false Transform coordinates in positions to PDF points. modelData string   Path to the local model data. modelType ModelType ModelType.BASE Model type downloadModelData bool false Download model data from JSL S3 withSpaces bool false Include spaces to output positions. Output Columns Param name Type Default Column Data Description outputCol string text Recognized text positionsCol string positions Positions of each block of text (related to pageIteratorLevel) in PageMatrix Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageToText import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setOcrParams(Array(&quot;preserve_interword_spaces=1&quot;)) val data = transformer.transform(df) print(data.select(&quot;text&quot;).collect()[0].text) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setOcrParams([&quot;preserve_interword_spaces=1&quot;, ]) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr ]) data = pipeline.transform(df) data.show() Image: Output: FOREWORD Electronic design engineers are the true idea men of the electronic industries. They create ideas and use them in their designs, they stimu- late ideas in other designers, and they borrow and adapt ideas from others. One could almost say they feed on and grow on ideas. ImageToTextPdf ImageToTextPdf runs OCR for input image, render recognized text to the PDF as an invisible text layout with an original image. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) originCol string path path to the original file pageNumCol string pagenum for compatibility with another transformers Parameters Param name Type Default Description ocrParams array of strings [] Array of Ocr params in key=value format. Output Columns Param name Type Default Column Data Description outputCol string pdf Recognized text rendered to PDF PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageToTextPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;pdf&quot;) val data = transformer.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToTextPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;pdf&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr ]) data = pipeline.transform(df) data.show() ImageToHocr ImageToHocr runs OCR for input image, return recognized text and bounding boxes to outputCol column in HOCR format. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description pageSegMode PageSegmentationMode AUTO page segmentation mode pageIteratorLevel PageIteratorLevel BLOCK page iteration level ocrEngineMode EngineMode LSTM_ONLY OCR engine mode language string eng language ignoreResolution bool true Ignore resolution from metadata of image. ocrParams array of strings [] Array of Ocr params in key=value format. Output Columns Param name Type Default Column Data Description outputCol string hocr Recognized text Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageToHocr import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val data = transformer.transform(df) print(data.select(&quot;hocr&quot;).collect()[0].hocr) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr ]) data = pipeline.transform(df) data.show() Image: Output: &lt;div class=&#39;ocr_page&#39; id=&#39;page_1&#39; title=&#39;image &quot;&quot;; bbox 0 0 1280 467; ppageno 0&#39;&gt; &lt;div class=&#39;ocr_carea&#39; id=&#39;block_1_1&#39; title=&quot;bbox 516 80 780 114&quot;&gt; &lt;p class=&#39;ocr_par&#39; id=&#39;par_1_1&#39; lang=&#39;eng&#39; title=&quot;bbox 516 80 780 114&quot;&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_1&#39; title=&quot;bbox 516 80 780 114; baseline 0 -1; x_size 44; x_descenders 11; x_ascenders 11&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_1&#39; title=&#39;bbox 516 80 780 114; x_wconf 96&#39;&gt;FOREWORD&lt;/span&gt; &lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;div class=&#39;ocr_carea&#39; id=&#39;block_1_2&#39; title=&quot;bbox 40 237 1249 425&quot;&gt; &lt;p class=&#39;ocr_par&#39; id=&#39;par_1_2&#39; lang=&#39;eng&#39; title=&quot;bbox 40 237 1249 425&quot;&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_2&#39; title=&quot;bbox 122 237 1249 282; baseline 0.001 -12; x_size 45; x_descenders 12; x_ascenders 13&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_2&#39; title=&#39;bbox 122 237 296 270; x_wconf 96&#39;&gt;Electronic&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_3&#39; title=&#39;bbox 308 237 416 281; x_wconf 96&#39;&gt;design&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_4&#39; title=&#39;bbox 428 243 588 282; x_wconf 96&#39;&gt;engineers&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_5&#39; title=&#39;bbox 600 250 653 271; x_wconf 96&#39;&gt;are&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_6&#39; title=&#39;bbox 665 238 718 271; x_wconf 96&#39;&gt;the&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_7&#39; title=&#39;bbox 731 246 798 272; x_wconf 97&#39;&gt;true&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_8&#39; title=&#39;bbox 810 238 880 271; x_wconf 96&#39;&gt;idea&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_9&#39; title=&#39;bbox 892 251 963 271; x_wconf 96&#39;&gt;men&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_10&#39; title=&#39;bbox 977 238 1010 272; x_wconf 96&#39;&gt;of&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_11&#39; title=&#39;bbox 1021 238 1074 271; x_wconf 96&#39;&gt;the&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_12&#39; title=&#39;bbox 1086 239 1249 272; x_wconf 96&#39;&gt;electronic&lt;/span&gt; &lt;/span&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_3&#39; title=&quot;bbox 41 284 1248 330; baseline 0.002 -13; x_size 44; x_descenders 11; x_ascenders 12&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_13&#39; title=&#39;bbox 41 284 214 318; x_wconf 96&#39;&gt;industries.&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_14&#39; title=&#39;bbox 227 284 313 328; x_wconf 96&#39;&gt;They&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_15&#39; title=&#39;bbox 324 292 427 319; x_wconf 96&#39;&gt;create&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_16&#39; title=&#39;bbox 440 285 525 319; x_wconf 96&#39;&gt;ideas&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_17&#39; title=&#39;bbox 537 286 599 318; x_wconf 96&#39;&gt;and&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_18&#39; title=&#39;bbox 611 298 668 319; x_wconf 96&#39;&gt;use&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_19&#39; title=&#39;bbox 680 286 764 319; x_wconf 96&#39;&gt;them&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_20&#39; title=&#39;bbox 777 291 808 319; x_wconf 96&#39;&gt;in&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_21&#39; title=&#39;bbox 821 286 900 319; x_wconf 96&#39;&gt;their&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_22&#39; title=&#39;bbox 912 286 1044 330; x_wconf 96&#39;&gt;designs,&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_23&#39; title=&#39;bbox 1058 286 1132 330; x_wconf 93&#39;&gt;they&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_24&#39; title=&#39;bbox 1144 291 1248 320; x_wconf 92&#39;&gt;stimu-&lt;/span&gt; &lt;/span&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_4&#39; title=&quot;bbox 42 332 1247 378; baseline 0.002 -14; x_size 44; x_descenders 12; x_ascenders 12&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_25&#39; title=&#39;bbox 42 332 103 364; x_wconf 97&#39;&gt;late&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_26&#39; title=&#39;bbox 120 332 204 365; x_wconf 96&#39;&gt;ideas&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_27&#39; title=&#39;bbox 223 337 252 365; x_wconf 96&#39;&gt;in&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_28&#39; title=&#39;bbox 271 333 359 365; x_wconf 96&#39;&gt;other&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_29&#39; title=&#39;bbox 376 333 542 377; x_wconf 96&#39;&gt;designers,&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_30&#39; title=&#39;bbox 561 334 625 366; x_wconf 96&#39;&gt;and&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_31&#39; title=&#39;bbox 643 334 716 377; x_wconf 96&#39;&gt;they&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_32&#39; title=&#39;bbox 734 334 855 366; x_wconf 96&#39;&gt;borrow&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_33&#39; title=&#39;bbox 873 334 934 366; x_wconf 96&#39;&gt;and&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_34&#39; title=&#39;bbox 954 335 1048 378; x_wconf 96&#39;&gt;adapt&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_35&#39; title=&#39;bbox 1067 334 1151 367; x_wconf 96&#39;&gt;ideas&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_36&#39; title=&#39;bbox 1169 334 1247 367; x_wconf 96&#39;&gt;from&lt;/span&gt; &lt;/span&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_5&#39; title=&quot;bbox 40 379 1107 425; baseline 0.002 -13; x_size 45; x_descenders 12; x_ascenders 12&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_37&#39; title=&#39;bbox 40 380 151 412; x_wconf 96&#39;&gt;others.&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_38&#39; title=&#39;bbox 168 383 238 412; x_wconf 96&#39;&gt;One&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_39&#39; title=&#39;bbox 252 379 345 412; x_wconf 96&#39;&gt;could&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_40&#39; title=&#39;bbox 359 380 469 413; x_wconf 96&#39;&gt;almost&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_41&#39; title=&#39;bbox 483 392 537 423; x_wconf 96&#39;&gt;say&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_42&#39; title=&#39;bbox 552 381 626 424; x_wconf 96&#39;&gt;they&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_43&#39; title=&#39;bbox 641 381 712 414; x_wconf 96&#39;&gt;feed&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_44&#39; title=&#39;bbox 727 393 767 414; x_wconf 96&#39;&gt;on&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_45&#39; title=&#39;bbox 783 381 845 414; x_wconf 96&#39;&gt;and&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_46&#39; title=&#39;bbox 860 392 945 425; x_wconf 97&#39;&gt;grow&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_47&#39; title=&#39;bbox 959 393 999 414; x_wconf 96&#39;&gt;on&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_48&#39; title=&#39;bbox 1014 381 1107 414; x_wconf 95&#39;&gt;ideas.&lt;/span&gt; &lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; ImageBrandsToText ImageBrandsToText runs OCR for specified brands of input image, return recognized text to outputCol and positions with font size to ‘positionsCol’ column. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description pageSegMode PageSegmentationMode AUTO page segmentation mode pageIteratorLevel PageIteratorLevel BLOCK page iteration level ocrEngineMode EngineMode LSTM_ONLY OCR engine mode language string eng language confidenceThreshold int 0 Confidence threshold. ignoreResolution bool true Ignore resolution from metadata of image. ocrParams array of strings [] Array of Ocr params in key=value format. brandsCoords string   Json with coordinates of brands. Output Columns Param name Type Default Column Data Description outputCol structure image_brands Structure with recognized text from brands. textCol string text Recognized text positionsCol string positions Positions of each block of text (related to pageIteratorLevel) Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageToText import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageBrandsToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setBrandsCoordsStr( &quot;&quot;&quot; [ { &quot;name&quot;:&quot;part_one&quot;, &quot;rectangle&quot;:{ &quot;x&quot;:286, &quot;y&quot;:65, &quot;width&quot;:542, &quot;height&quot;:342 } }, { &quot;name&quot;:&quot;part_two&quot;, &quot;rectangle&quot;:{ &quot;x&quot;:828, &quot;y&quot;:65, &quot;width&quot;:1126, &quot;height&quot;:329 } } ] &quot;&quot;&quot;.stripMargin) val data = transformer.transform(df) print(data.select(&quot;text&quot;).collect()[0].text) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageBrandsToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setBrandsCoords(&quot;&quot;&quot;[ { &quot;name&quot;:&quot;part_one&quot;, &quot;rectangle&quot;:{ &quot;x&quot;:286, &quot;y&quot;:65, &quot;width&quot;:542, &quot;height&quot;:342 } }, { &quot;name&quot;:&quot;part_two&quot;, &quot;rectangle&quot;:{ &quot;x&quot;:828, &quot;y&quot;:65, &quot;width&quot;:1126, &quot;height&quot;:329 } } ]&quot;&quot;&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr ]) data = pipeline.transform(df) data.show() Other Next section describes the extra transformers PositionFinder PositionFinder find position of input text entities in original document. Input Columns Param name Type Default Column Data Description inputCols string image Input annotations columns pageMatrixCol string   Column name for Page Matrix schema Parameters Param name Type Default Description matchingWindow int 10 Textual range to match in context, applies in both direction windowPageTolerance boolean true whether or not to increase tolerance as page number grows padding int 5 padding for area Output Columns Param name Type Default Column Data Description outputCol string   Name of output column for store coordinates. Example: PythonScala import com.johnsnowlabs.ocr.transformers._ import com.johnsnowlabs.nlp.{DocumentAssembler, SparkAccessor} import com.johnsnowlabs.nlp.annotators._ import com.johnsnowlabs.nlp.util.io.ReadAs val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) val positionFinder = new PositionFinder() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;coordinates&quot;) .setPageMatrixCol(&quot;positions&quot;) .setMatchingWindow(10) .setPadding(2) // Create pipeline val pipeline = new Pipeline() .setStages(Array( pdfToText, documentAssembler, sentenceDetector, tokenizer, entityExtractor, positionFinder )) val results = pipeline.fit(df).transform(df) results.show() from pyspark.ml import Pipeline from sparkocr.transformers import * from sparknlp.annotator import * from sparknlp.base import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;page&quot;) .setSplitPage(False) document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) entity_extractor = TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;./sparkocr/resources/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) position_finder = PositionFinder() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;coordinates&quot;) .setPageMatrixCol(&quot;positions&quot;) .setMatchingWindow(10) .setPadding(2) pipeline = Pipeline(stages=[ pdf_to_text, document_assembler, sentence_detector, tokenizer, entity_extractor, position_finder ]) results = pipeline.fit(df).transform(df) results.show() UpdateTextPosition UpdateTextPosition update output text and keep old coordinates of original document. Input Columns Param name Type Default Column Data Description inputCol string positions Сolumn name with original positions struct InputText string replace_text Column name for New Text to replace Old one Output Columns Param name Type Default Column Data Description outputCol string output_positions Name of output column for updated positions struct. Example: PythonScala import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.spell.norvig.NorvigSweetingModel import com.johnsnowlabs.nlp.{DocumentAssembler, TokenAssembler} import com.johnsnowlabs.ocr.transformers._ import org.apache.spark.ml.Pipeline val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val token = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;tokens&quot;) val spell = NorvigSweetingModel.pretrained(&quot;spellcheck_norvig&quot;, &quot;en&quot;) .setInputCols(&quot;tokens&quot;) .setOutputCol(&quot;spell&quot;) val tokenAssem = new TokenAssembler() .setInputCols(&quot;spell&quot;) .setOutputCol(&quot;newDocs&quot;) val updatedText = new UpdateTextPosition() .setInputCol(&quot;positions&quot;) .setOutputCol(&quot;output_positions&quot;) .setInputText(&quot;newDocs.result&quot;) val pipeline = new Pipeline() .setStages(Array( pdfToText, documentAssembler, sentence, token, spell, tokenAssem, updatedText )) val results = pipeline.fit(df).transform(df) results.show() from pyspark.ml import Pipeline from sparkocr.transformers import * from sparknlp.annotator import * from sparknlp.base import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;page&quot;) .setSplitPage(False) document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;tokens&quot;) spell = NorvigSweetingModel().pretrained(&quot;spellcheck_norvig&quot;, &quot;en&quot;) .setInputCols(&quot;tokens&quot;) .setOutputCol(&quot;spell&quot;) tokenAssem = TokenAssembler() .setInputCols(&quot;spell&quot;) .setOutputCol(&quot;newDocs&quot;) updatedText = UpdateTextPosition() .setInputCol(&quot;positions&quot;) .setOutputCol(&quot;output_positions&quot;) .setInputText(&quot;newDocs.result&quot;) pipeline = Pipeline(stages=[ document_assembler, sentence_detector, tokenizer, spell, tokenAssem, updatedText ]) results = pipeline.fit(df).transform(df) results.show() FoundationOneReportParser FoundationOneReportParser is a transformer for parsing FoundationOne reports. Current implementation support parsing patient info, genomic, biomarker findings and gene lists from appendix. Output format is json. Input Columns Param name Type Default Column Data Description inputCol string text Сolumn name with text of report originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string report Name of output column with report in json format. Example: PythonScala import com.johnsnowlabs.ocr.transformers._ import org.apache.spark.ml.Pipeline val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) .setTextStripper(TextStripperType.PDF_LAYOUT_TEXT_STRIPPER) val genomicsParser = new FoundationOneReportParser() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;report&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( pdfToText, genomicsParser )) val modelPipeline = pipeline.fit(df) val report = modelPipeline.transform(df) from pyspark.ml import Pipeline from sparkocr.transformers import * from sparkocr.enums import TextStripperType pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text = PdfToText() pdf_to_text.setInputCol(&quot;content&quot;) pdf_to_text.setOutputCol(&quot;text&quot;) pdf_to_text.setSplitPage(False) pdf_to_text.setTextStripper(TextStripperType.PDF_LAYOUT_TEXT_STRIPPER) genomic_parser = FoundationOneReportParser() genomic_parser.setInputCol(&quot;text&quot;) genomic_parser.setOutputCol(&quot;report&quot;) report = genomic_parser.transform(pdf_to_text.transform(df)).collect() Output: { &quot;Patient&quot; : { &quot;disease&quot; : &quot;Unknown primary melanoma&quot;, &quot;name&quot; : &quot;Lekavich Gloria&quot;, &quot;date_of_birth&quot; : &quot;11 November 1926&quot;, &quot;sex&quot; : &quot;Female&quot;, &quot;medical_record&quot; : &quot;11111&quot; }, &quot;Physician&quot; : { &quot;ordering_physician&quot; : &quot;Genes Pinley&quot;, &quot;medical_facility&quot; : &quot;Health Network Cancer Institute&quot;, &quot;additional_recipient&quot; : &quot;Nath&quot;, &quot;medical_facility_id&quot; : &quot;202051&quot;, &quot;pathologist&quot; : &quot;Manqju Nwath&quot; }, &quot;Specimen&quot; : { &quot;specimen_site&quot; : &quot;Rectum&quot;, &quot;specimen_id&quot; : &quot;AVS 1A&quot;, &quot;specimen_type&quot; : &quot;Slide&quot;, &quot;date_of_collection&quot; : &quot;20 March 2015&quot;, &quot;specimen_received&quot; : &quot;30 March 2015 &quot; }, &quot;Biomarker_findings&quot; : [ { &quot;name&quot; : &quot;Tumor Mutation Burden&quot;, &quot;state&quot; : &quot;TMB-Low (3Muts/Mb)&quot;, &quot;actionability&quot; : &quot;No therapies or clinical trials. &quot; } ], &quot;Genomic_findings&quot; : [ { &quot;name&quot; : &quot;FLT3&quot;, &quot;state&quot; : &quot;amplification&quot;, &quot;therapies_with_clinical_benefit_in_patient_tumor_type&quot; : [ &quot;none&quot; ], &quot;therapies_with_clinical_benefit_in_other_tumor_type&quot; : [ &quot;Sorafenib&quot;, &quot;Sunitinib&quot;, &quot;Ponatinib&quot; ] } ], &quot;Appendix&quot; : { &quot;dna_gene_list&quot; : [ &quot;ABL1&quot;, &quot;ACVR1B&quot;, &quot;AKT1&quot;, .... ], &quot;dna_gene_list_rearrangement&quot; : [ &quot;ALK&quot;, &quot;BCL2&quot;, &quot;BCR&quot;, .... ], &quot;additional_assays&quot; : [ &quot;Tumor Mutation Burden (TMB)&quot;, &quot;Microsatellite Status (MS)&quot; ] } } HocrDocumentAssembler HocrDocumentAssembler prepares data into a format that is processable by Spark NLP. Output Annotator Type: DOCUMENT Input Columns Param name Type Default Column Data Description inputCol string hocr Сolumn name with HOCR of the document Output Columns Param name Type Default Column Data Description outputCol string document Name of output column. Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToHocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val hocrDocumentAssembler = HocrDocumentAssembler() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;document&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( imageToHocr, hocrDocumentAssembler )) val modelPipeline = pipeline.fit(df) val result = modelPipeline.transform(df) result.select(&quot;document&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) hocr_document_assembler = HocrDocumentAssembler() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;document&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr, hocr_document_assembler ]) result = pipeline.transform(df) result.select(&quot;document&quot;).show() Output: +--+ | document | +--+ | [[document, 0, 4392, Patient Nam Financial Numbe Random Hospital...| +--+ HocrTokenizer HocrTokenizer prepares into a format that is processable by Spark NLP. HocrTokenizer puts to metadata coordinates and ocr confidence. Output Annotator Type: TOKEN Input Columns Param name Type Default Column Data Description inputCol string hocr Сolumn name with HOCR of the document. Output Columns Param name Type Default Column Data Description outputCol string token Name of output column. Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToHocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val tokenizer = HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( imageToHocr, tokenizer )) val modelPipeline = pipeline.fit(df) val result = modelPipeline.transform(df) result.select(&quot;token&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) tokenizer = HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr, tokenizer ]) result = pipeline.transform(df) result.select(&quot;token&quot;).show() Output: +--+ | token | +--+ | [[token, 0, 6, patient, [x -&gt; 2905, y -&gt; 527, height -&gt; 56, | | confidence -&gt; 95, word -&gt; Patient, width -&gt; 230], []], [token, 8, | |10, nam, [x -&gt; 3166, y -&gt; 526, height -&gt; 55, confidence -&gt; 95, word | |-&gt; Nam, width -&gt; 158], []] ... | +--+",
    "url": "/docs/en/ocr_pipeline_components",
    "relUrl": "/docs/en/ocr_pipeline_components"
  },
  "68": {
    "id": "68",
    "title": "Spark OCR release notes",
    "content": "3.9.0 Release date: 20-10-2021 Overview Improve visualization and support Spark NLP. New Features Added HocrTokenizer Added HocrDocumentAssembler Added ImageDrawAnnotations Added support Arabic language in ImageToText and ImageToHocr Enhancements Added postprocessing to the ImageTableDetector Added Spark NLP by default to spark session in start function Changed default value for ignoreResolution param in ImageToText Updated license-validator. Added support floating license and set AWS keys from license. Added ‘whiteList’ param to the VisualDocumentNER New and updated notebooks Spark OCR HOCR Visual Document NER 3.8.0 Release date: 14-09-2021 Overview Support Microsoft PPT and PPTX documents. New Features Added PptToPdf transformer for convert PPT and PPTX slides to the PDF document. Added PptToTextTable transformer for extract tables from PPT and PPTX slides. New and updated notebooks Convert PPT to PDF (New) Extract tables from PPT (New) 3.7.0 Release date: 30-08-2021 Overview Improve table recognition and render OCR results to the PDF with original image New Features Added ImageToTextPdf transformer for store recognized text to the searchable PDF with original image Added PdfAssembler for assemble multipage PDF document from single page PDF documents Enhancements Added support dbfs for store models. This allow to use models on Databricks. Improved ImageTableCellDetector algorithms Added params for tune ImageTableCellDetector algorithms Added possibility to render detected lines to the original image in ImageTableCellDetector Added support store recognized results to CSV in ImageCellsToTextTable Added display_table and display_tables functions Added display_pdf_file function for display pdf in embedded pdf viewer Updated license validator New and updated notebooks Process multiple page scanned PDF (New) Image Table Detection example Image Cell Recognition example Image Table Recognition Tables Recognition from PDF 3.6.0 Release date: 05-08-2021 Overview Handwritten detection and improve visualization. New Features Added ImageHandwrittenDetector for detect ‘signature’, ‘date’, ‘name’, ‘title’, ‘address’ and others handwritten text. Added rendering labels and scores in ImageDrawRegions. Added possibility to scale image to fixed size in ImageScaler with keeping original ratio. Enhancements Support new version of pip for installing python package Added support string labels for detectors Added an auto inferencing of the input shape for detector models New license validator Bugfixes Fixed display BGR images in display functions New and updated notebooks Image Signature Detection example Image Handwritten Detection example Image Scaler example 3.5.0 Release date: 15-07-2021 Overview Improve table detection and table recognition. More details please read in Extract Tabular Data from PDF in Spark OCR New Features Added new method to ImageTableCellDetector which support borderless tables and combined tables. Added Wolf and Singh adaptive binarization methods to the ImageAdaptiveThresholding. Enhancements Added possibility to use different type of images as input for ImageTableDetector. Added display_pdf and display_images_horizontal util functions. New notebooks Tables Recognition from PDF Pdf de-identification on Databricks Dicom de-identification on Databricks 3.4.0 Release date: 30-06-2021 Overview Signature Detection in image-based documents. More details please read in Signature Detection in Spark OCR New Features ImageSignatureDetector is a DL model for detect signature on the image. New notebooks Image Signature Detection example 3.3.0 Release date: 14-06-2021 Overview Table detection and recognition for scanned documents. For table detection we added ImageTableDetector. It based on CascadeTabNet which used Cascade mask Region-based CNN High-Resolution Network (Cascade mask R-CNN HRNet). Model was pre-trained on the COCO dataset and fine tuned on ICDAR 2019 competitions dataset for table detection. It demonstrates state of the art results for ICDAR 2013 and TableBank. And top results for ICDAR 2019. More details please read in Table Detection &amp; Extraction in Spark OCR New Features ImageTableDetector is a DL model for detect tables on the image. ImageTableCellDetector is a transformer for detect regions of cells in the table image. ImageCellsToTextTable is a transformer for extract text from the detected cells. New notebooks Image Table Detection example Image Cell Recognition example Image Table Recognition 3.2.0 Release date: 28-05-2021 Overview Multi-modal visual document understanding, built on the LayoutLM architecture. It achieves new state-of-the-art accuracy in several downstream tasks, including form understanding and receipt understanding. New Features VisualDocumentNER is a DL model for NER problem using text and layout data. Currently available pre-trained model on the SROIE dataset. Enhancements Added support SPARK_OCR_LICENSE env key for read license. Update dependencies and sync Spark versions with Spark NLP. Bugfixes Fixed an issue that some ImageReaderSpi plugins are unavailable in the fat jar. New notebooks Visual Document NER 3.1.0 Release date: 16-04-2021 Overview Image processing on GPU. It is in 3..5 times faster than on CPU. More details please read in GPU image preprocessing in Spark OCR New Features GPUImageTransformer with support: scaling, erosion, delation, Otsu and Huang thresholding. Added display_images util function for display images from Spark DataFrame in Jupyter notebooks. Enhancements Improve display_image util function. Bug fixes Fixed issue with extra dependencies in start function New notebooks GPU image processing 3.0.0 Release date: 02-04-2021 Overview We are very excited to release Spark OCR 3.0.0! Spark OCR 3.0.0 extends the support for Apache Spark 3.0.x and 3.1.x major releases on Scala 2.12 with both Hadoop 2.7. and 3.2. We will support all 4 major Apache Spark and PySpark releases of 2.3.x, 2.4.x, 3.0.x, and 3.1.x. Spark OCR started to support Tensorflow models. First model is VisualDocumentClassifier. New Features Support for Apache Spark and PySpark 3.0.x on Scala 2.12 Support for Apache Spark and PySpark 3.1.x on Scala 2.12 Support 9x new Databricks runtimes: Databricks 7.3 Databricks 7.3 ML GPU Databricks 7.4 Databricks 7.4 ML GPU Databricks 7.5 Databricks 7.5 ML GPU Databricks 7.6 Databricks 7.6 ML GPU Databricks 8.0 Databricks 8.0 ML (there is no GPU in 8.0) Databricks 8.1 Support 2x new EMR 6.x: EMR 6.1.0 (Apache Spark 3.0.0 / Hadoop 3.2.1) EMR 6.2.0 (Apache Spark 3.0.1 / Hadoop 3.2.1) VisualDocumentClassifier model for classification documents using text and layout data. Added support Vietnamese language. New notebooks Visual Document Classifier 1.11.0 Release date: 25-02-2021 Overview Support German, French, Spanish and Russian languages. Improving PositionsFinder and ImageToText for better support de-identification. New Features Loading model data from S3 in ImageToText. Added support German, French, Spanish, Russian languages in ImageToText. Added different OCR model types: Base, Best, Fast in ImageToText. Enhancements Added spaces symbols to the output positions in the ImageToText transformer. Eliminate python-levensthein from dependencies for simplify installation. Bugfixes Fixed issue with extracting coordinates in in ImageToText. Fixed loading model data on cluster in yarn mode. New notebooks Languages Support Image DeIdentification 1.10.0 Release date: 20-01-2021 Overview Support Microsoft Docx documents. New Features Added DocToText transformer for extract text from DOCX documents. Added DocToTextTable transformer for extract table data from DOCX documents. Added DocToPdf transformer for convert DOCX documents to PDF format. Bugfixes Fixed issue with loading model data on some cluster configurations 1.9.0 Release date: 11-12-2020 Overview Extension of FoundationOne report parser and support HOCR output format. New Features Added ImageToHocr transformer for recognize text from image and store it to HOCR format. Added parsing gene lists from ‘Appendix’ in FoundationOneReportParser transformer. 1.8.0 Release date: 20-11-2020 Overview Optimisation performance for processing multipage PDF documents. Support up to 10k pages per document. New Features Added ImageAdaptiveBinarizer Scala transformer with support: Gaussian local thresholding Otsu thresholding Sauvola local thresholding Added possibility to split pdf to small documents for optimize processing in PdfToImage. Enhancements Added applying binarization in PdfToImage for optimize memory usage. Added pdfCoordinates param to the ImageToText transformer. Added ‘total_pages’ field to the PdfToImage transformer. Added different splitting strategies to the PdfToImage transformer. Simplified paging PdfToImage when run it with splitting to small PDF. Added params to the PdfToText for disable extra functionality. Added master_url param to the python start function. 1.7.0 Release date: 22-09-2020 Overview Support Spark 2.3.3. Bugfixes Restored read JPEG2000 image 1.6.0 Release date: 05-09-2020 Overview Support parsing data from tables for selectable PDFs. New Features Added PdfToTextTable transformer for extract tables from Pdf document per each page. Added ImageCropper transformer for crop images. Added ImageBrandsToText transformer for detect text in defined areas. 1.5.0 Release date: 22-07-2020 Overview FoundationOne report parsing support. Enhancements Optimized memory usage during image processing New Features Added FoundationOneReportParser which support parsing patient info, genomic and biomarker findings. 1.4.0 Release date: 23-06-2020 Overview Added support Dicom format and improved support image morphological operations. Enhancements Updated start function. Improved support Spark NLP internal. ImageMorphologyOpening and ImageErosion are removed. Improved existing transformers for support de-identification Dicom documents. Added possibility to draw filled rectangles to ImageDrawRegions. New Features Support reading and writing Dicom documents. Added ImageMorphologyOperation transformer which support: erosion, dilation, opening and closing operations. Bugfixes Fixed issue in ImageToText related to extraction coordinates. 1.3.0 Release date: 22-05-2020 Overview New functionality for de-identification problem. Enhancements Renamed TesseractOCR to ImageToText. Simplified installation. Added check license from SPARK_NLP_LICENSE env varibale. New Features Support storing for binaryFormat. Added support storing Image and PDF files. Support selectable pdf for TextToPdf transformer. Added UpdateTextPosition transformer. 1.2.0 Release date: 08-04-2020 Overview Improved support Databricks and processing selectable pdfs. Enhancements Adapted Spark OCR for run on Databricks. Added rewriting positions in ImageToText when run together with PdfToText. Added ‘positionsCol’ param to ImageToText. Improved support Spark NLP. Changed start function. New Features Added showImage implicit to Dataframe for display images in Scala Databricks notebooks. Added display_images function for display images in Python Databricks notebooks. Added propagation selectable pdf file in TextToPdf. Added ‘inputContent’ param to ‘TextToPdf’. 1.1.2 Release date: 09-03-2020 Overview Minor improvements and fixes Enhancements Improved messages during license validation Bugfixes Fixed dependencies issue 1.1.1 Release date: 06-03-2020 Overview Integration with license server. Enhancements Added license validation. License can be set in following waysq: Environment variable. Set variable ‘JSL_OCR_LICENSE’. System property. Set property ‘jsl.sparkocr.settings.license’. Application.conf file. Set property ‘jsl.sparkocr.settings.license’. Added auto renew license using jsl license server. 1.1.0 Release date: 03-03-2020 Overview This release contains improvements for preprocessing image before run OCR and added possibility to store results to PDF for keep original formatting. New Features Added auto calculation maximum size of objects for removing in ImageRemoveObjects. This improvement avoids to remove . and affect symbols with dots (i, !, ?). Added minSizeFont param to ImageRemoveObjects transformer for activate this functional. Added ocrParams parameter to ImageToText transformer for set any ocr params. Added extraction font size in ImageToText Added TextToPdf transformer for render text with positions to pdf file. Enhancements Added setting resolution in ImageToText. And added ignoreResolution param with default true value to ImageToText transformer for back compatibility. Added parsing resolution from image metadata in BinaryToImage transformer. Added storing resolution in PrfToImage transformer. Added resolution field to Image schema. Updated ‘start’ function for set ‘PYSPARK_PYTHON’ env variable. Improve auto-scaling/skew correction: improved access to images values removing unnecessary copies of images adding more test cases improving auto-correlation in auto-scaling. 1.0.0 Release date: 12-02-2020 Overview Spark NLP OCR functionality was reimplemented as set of Spark ML transformers and moved to separate Spark OCR library. New Features Added extraction coordinates of each symbol in ImageToText Added ImageDrawRegions transformer Added ImageToPdf transformer Added ImageMorphologyOpening transformer Added ImageRemoveObjects transformer Added ImageAdaptiveThresholding transformer Enhancements Reimplement main functionality as Spark ML transformers Moved DrawRectangle functionality to PdfDrawRegions transformer Added ‘start’ function with support SparkMonitor initialization Moved PositionFinder to Spark OCR Bugfixes Fixed bug with transforming complex pdf to image",
    "url": "/docs/en/ocr_release_notes",
    "relUrl": "/docs/en/ocr_release_notes"
  },
  "69": {
    "id": "69",
    "title": "Structures and helpers",
    "content": "Schemas Image Schema Images are loaded as a DataFrame with a single column called “image.” It is a struct-type column, that contains all information about image: image: struct (nullable = true) | |-- origin: string (nullable = true) | |-- height: integer (nullable = false) | |-- width: integer (nullable = false) | |-- nChannels: integer (nullable = false) | |-- mode: integer (nullable = false) | |-- resolution: integer (nullable = true) | |-- data: binary (nullable = true) Fields Field name Type Description origin string source URI height integer image height in pixels width integer image width in pixels nChannels integer number of color channels mode ImageType the data type and channel order the data is stored in resolution integer Resolution of image in dpi data binary image data in a binary format NOTE: Image data stored in a binary format. Image data is represented as a 3-dimensional array with the dimension shape (height, width, nChannels) and array values of type t specified by the mode field. Coordinate Schema element: struct (containsNull = true) | | |-- index: integer (nullable = false) | | |-- page: integer (nullable = false) | | |-- x: float (nullable = false) | | |-- y: float (nullable = false) | | |-- width: float (nullable = false) | | |-- height: float (nullable = false) Field name Type Description index integer Chunk index page integer Page number x float The lower left x coordinate y float The lower left y coordinate width float The width of the rectangle height float The height of the rectangle score float The score of the object label string The label of the object PageMatrix Schema element: struct (containsNull = true) | | |-- mappings: array[struct] (nullable = false) Field name Type Description mappings Array[Mapping] Array of mappings Mapping Schema element: struct (containsNull = true) | | |-- c: string (nullable = false) | | |-- p: integer (nullable = false) | | |-- x: float (nullable = false) | | |-- y: float (nullable = false) | | |-- width: float (nullable = false) | | |-- height: float (nullable = false) | | |-- fontSize: integer (nullable = false) Field name Type Description c string Character p integer Page number x float The lower left x coordinate y float The lower left y coordinate width float The width of the rectangle height float The height of the rectangle fontSize integer Font size in points Enums PageSegmentationMode OSD_ONLY: Orientation and script detection only. AUTO_OSD: Automatic page segmentation with orientation and script detection. AUTO_ONLY: Automatic page segmentation, but no OSD, or OCR. AUTO: Fully automatic page segmentation, but no OSD. SINGLE_COLUMN: Assume a single column of text of variable sizes. SINGLE_BLOCK_VERT_TEXT: Assume a single uniform block of vertically aligned text. SINGLE_BLOCK: Assume a single uniform block of text. SINGLE_LINE: Treat the image as a single text line. SINGLE_WORD: Treat the image as a single word. CIRCLE_WORD: Treat the image as a single word in a circle. SINGLE_CHAR: Treat the image as a single character. SPARSE_TEXT: Find as much text as possible in no particular order. SPARSE_TEXT_OSD: Sparse text with orientation and script detection. EngineMode TESSERACT_ONLY: Legacy engine only. OEM_LSTM_ONLY: Neural nets LSTM engine only. TESSERACT_LSTM_COMBINED: Legacy + LSTM engines. DEFAULT: Default, based on what is available. PageIteratorLevel BLOCK: Block of text/image/separator line. PARAGRAPH: Paragraph within a block. TEXTLINE: Line within a paragraph. WORD: Word within a text line. SYMBOL: Symbol/character within a word. Language ENG: English FRA: French SPA: Spanish RUS: Russian DEU: German VIE: Vietnamese ARA: Arabic ModelType BASE: Block of text/image/separator line. BEST: Paragraph within a block. FAST: Line within a paragraph. ImageType TYPE_BYTE_GRAY TYPE_BYTE_BINARY TYPE_3BYTE_BGR TYPE_4BYTE_ABGR NoiseMethod VARIANCE RATIO KernelShape SQUARE DIAMOND DISK OCTAHEDRON OCTAGON STAR MorphologyOperationType OPENING CLOSING EROSION DILATION CropSquareType TOP_LEFT TOP_CENTER TOP_RIGHT CENTER_LEFT CENTER CENTER_RIGHT BOTTOM_LEFT BOTTOM_CENTER BOTTOM_RIGHT SplittingStrategy FIXED_NUMBER_OF_PARTITIONS FIXED_SIZE_OF_PARTITION AdaptiveThresholdingMethod GAUSSIAN MEAN MEDIAN WOLF SINGH TresholdingMethod GAUSSIAN OTSU SAUVOLA WOLF CellDetectionAlgos CONTOURS - Detect cells in bordered tables MORPHOPS - Detected calls in: bordered, borderless and combined tables TableOutputFormat TABLE - Table struct format CSV - Comma separated CSV OCR implicits asImage asImage transforms binary content to Image schema. Parameters Param name Type Default Description outputCol string image output column name contentCol string content input column name with binary content pathCol string path input column name with path to original file Example: import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) df.show() storeImage storeImage stores the image(s) to tmp location and return Dataset with path(s) to stored image files. Parameters Param name Type Default Description inputColumn string   input column name with image struct formatName string png image format name prefix string sparknlp_ocr_ prefix for output file Example: import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) df.storeImage(&quot;image&quot;) showImages Show images on Databrics notebook. Parameters Param name Type Default Description field string image input column name with image struct limit integer 5 count of rows for display width string “800” width of image show_meta boolean true enable/disable displaying methadata of image Jupyter Python helpers display_image Show single image with methadata in Jupyter notebook. Parameters Param name Type Default Description width string “600” width of image show_meta boolean true enable/disable displaying methadata of image Example: from sparkocr.utils import display_image from sparkocr.transformers import BinaryToImage images_path = &quot;/tmp/ocr/images/*.tif&quot; images_example_df = spark.read.format(&quot;binaryFile&quot;).load(images_path).cache() display_image(BinaryToImage().transform(images_example_df).collect()[0].image) display_images Show images from dataframe. Parameters Param name Type Default Description field string image input column name with image struct limit integer 5 count of rows for display width string “600” width of image show_meta boolean true enable/disable displaying methadata of image Example: from sparkocr.utils import display_images from sparkocr.transformers import BinaryToImage images_path = &quot;/tmp/ocr/images/*.tif&quot; images_example_df = spark.read.format(&quot;binaryFile&quot;).load(images_path).cache() display_images(BinaryToImage().transform(images_example_df), limit=3) display_images_horizontal Show one or more images per row from dataframe. Parameters Param name Type Default Description fields string image comma separated input column names with image struct limit integer 5 count of rows for display width string “600” width of image show_meta boolean true enable/disable displaying methadata of image Example: from sparkocr.utils import display_images_horizontal display_images_horizontal(df_with_few_image_fields, fields=&quot;images, image_with_regions&quot;, limit=10) display_pdf Show pdf from dataframe. Parameters Param name Type Default Description field string content input column with binary representation of pdf limit integer 5 count of rows for display width string “600” width of image show_meta boolean true enable/disable displaying methadata of image Example: from sparkocr.utils import display_pdf pdf_df = spark.read.format(&quot;binaryFile&quot;).load(pdf_path) display_pdf(pdf_df) display_pdf_file Show pdf file using embedded pdf viewer. Parameters Param name Type Default Description pdf string   Path to the file name size integer size=(600, 500) count of rows for display Example: from sparkocr.utils import display_pdf_file display_pdf_file(&quot;path to the pdf file&quot;) Example output: display_table Display table from the dataframe. display_tables Display tables from the dataframe. It is useful for display results of table recognition from the multipage documents/few tables per page. Example output: Databricks Python helpers display_images Show images from dataframe. Parameters Param name Type Default Description field string image input column name with image struct limit integer 5 count of rows for display width string “800” width of image show_meta boolean true enable/disable displaying methadata of image Example: from sparkocr.databricks import display_images from sparkocr.transformers import BinaryToImage images_path = &quot;/tmp/ocr/images/*.tif&quot; images_example_df = spark.read.format(&quot;binaryFile&quot;).load(images_path).cache() display_images(BinaryToImage().transform(images_example_df), limit=3)",
    "url": "/docs/en/ocr_structures",
    "relUrl": "/docs/en/ocr_structures"
  },
  "70": {
    "id": "70",
    "title": "Table recognition",
    "content": "ImageTableDetector ImageTableDetector is a DL model for detect tables on the image. It based on CascadeTabNet which used Cascade mask Region-based CNN High-Resolution Network (Cascade mask R-CNN HRNet). Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description scoreThreshold float 0.9 Score threshold for output regions. applyCorrection boolean false Enable correction of results. Output Columns Param name Type Default Column Data Description outputCol string table_regions array of [Coordinaties]ocr_structures#coordinate-schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect tables val table_detector = ImageTableDetector .pretrained(&quot;general_model_table_detection_v2&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;table_regions&quot;) val draw_regions = new ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;table_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, table_detector, draw_regions ]) val data = pipeline.transform(df) data.storeImage(&quot;image_with_regions&quot;) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect tables table_detector = ImageTableDetector .pretrained(&quot;general_model_table_detection_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;table_regions&quot;) draw_regions = ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;table_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, table_detector, draw_regions ]) data = pipeline.transform(df) display_images(data, &quot;image_with_regions&quot;) Output: ImageTableCellDetector ImageTableCellDetector detect cells on image with table. It based on image processing algorithm by detecting horizontal and vertical lines. Current implementation support few algorithm for extract cells: CellDetectionAlgos.CONTOURS works only for bordered tables. CellDetectionAlgos.MORPHOPS works for bordered, borderless and combined tables. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description algoType CellDetectionAlgos CellDetectionAlgos.CONTOURS Algorithm for detect cells. algoParams string row_treshold=0.05,row_treshold_wide=1.0, row_min_wide=5,column_treshold=0.05, column_treshold_wide=5,column_min_wide=5 Parameters of ‘MORPHOPS’ cells detection algorithm drawDetectedLines boolean false Enable to draw detected lines to the output image keepOriginalLines boolean false Keep original images on the output image Output Columns Param name Type Default Column Data Description outputCol string cells array of coordinates of cells outputImageCol string output_image output image Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect cells val transformer = new ImageTableCellDetector() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cells&quot;) val data = transformer.transform(df) data.select(&quot;cells&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect cells transformer = ImageTableCellDetector .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cells&quot;) .setAlgoParams(&quot;row_treshold=0.05&quot;) pipeline = PipelineModel(stages=[ binary_to_image, transformer ]) data = pipeline.transform(df) data.select(&quot;cells&quot;).show() Image: Output:* +-+ | cells | +-+ ||[[[[15, 17, 224, 53]], [[241, 17, 179, 53]], [[423, 17, | | 194, 53]], [[619, 17, 164, 53]] .... | +-+ ImageCellsToTextTable ImageCellsToTextTable runs OCR for cells regions on image, return recognized text to outputCol as TableContainer structure. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) cellsCol string celss Array of cells Parameters Param name Type Default Description strip bool true Strip output text. margin bool 1 Margin of cells in pixelx. pageSegMode PageSegmentationMode AUTO page segmentation mode ocrEngineMode EngineMode LSTM_ONLY OCR engine mode language Language Language.ENG language ocrParams array of strings [] Array of Ocr params in key=value format. pdfCoordinates bool false Transform coordinates in positions to PDF points. modelData string   Path to the local model data. modelType ModelType ModelType.BASE Model type downloadModelData bool false Download model data from JSL S3 outputFormat TableOutputFormat TableOutputFormat.TABLE Output format Output Columns Param name Type Default Column Data Description outputCol string table Recognized text as TableContainer Example: PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect cells val cell_detector = new ImageTableCellDetector() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cells&quot;) val table_recognition = new ImageCellsToTextTable() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;tables&quot;) .setMargin(2) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( cell_detector, table_recognition )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val results = modelPipeline.transform(df) results.select(&quot;tables&quot;) .withColumn(&quot;cells&quot;, explode(col(&quot;tables.chunks&quot;))) .select((0 until 7).map(i =&gt; col(&quot;cells&quot;)(i).getField(&quot;chunkText&quot;).alias(s&quot;col$i&quot;)): _*) .show(false) from pyspark.ml import PipelineModel import pyspark.sql.functions as f from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() binary_to_image.setImageType(ImageType.TYPE_BYTE_GRAY) binary_to_image.setInputCol(&quot;content&quot;) cell_detector = TableCellDetector() cell_detector.setInputCol(&quot;image&quot;) cell_detector.setOutputCol(&quot;cells&quot;) cell_detector.setKeepInput(True) table_recognition = ImageCellsToTextTable() table_recognition.setInputCol(&quot;image&quot;) table_recognition.setCellsCol(&#39;cells&#39;) table_recognition.setMargin(2) table_recognition.setStrip(True) table_recognition.setOutputCol(&#39;table&#39;) pipeline = PipelineModel(stages=[ binary_to_image, cell_detector, table_recognition ]) result = pipeline.transform(df) results.select(&quot;table&quot;) .withColumn(&quot;cells&quot;, f.explode(f.col(&quot;table.chunks&quot;))) .select([f.col(&quot;cells&quot;)[i].getField(&quot;chunkText&quot;).alias(f&quot;col{i}&quot;) for i in range(0, 7)]) .show(20, False) Image: Output: +-+-+--++--++-+ |col0 |col1 |col2 |col3 |col4 |col5 |col6 | +-+-+--++--++-+ |Order Date|Region |Rep |Item |Units|Unit Cost|Total | |1/23/10 |Ontario|Kivell |Binder|50 |$19.99 |$999.50| |2/9/10 |Ontario|Jardine |Pencil|36 |$4.99 |$179.64| |2/26/10 |Ontario|Gill |Pen |27 |$19.99 |$539.73| |3/15/10 |Alberta|Sorvino |Pencil|56 |$2.99 |$167.44| |4/1/10 |Quebec |Jones |Binder|60 |$4.99 |$299.40| |4/18/10 |Ontario|Andrews |Pencil|75 |$1.99 |$149.25| |5/5/10 |Ontario|Jardine |Pencil|90 |$4.99 |$449.10| |5/22/10 |Alberta|Thompson|Pencil|32 |$1.99 |$63.68 | +-+-+--++--++-+",
    "url": "/docs/en/ocr_table_recognition",
    "relUrl": "/docs/en/ocr_table_recognition"
  },
  "71": {
    "id": "71",
    "title": "Visual document understanding",
    "content": "NLP models are great at processing digital text, but many real-word applications use documents with more complex formats. For example, healthcare systems often include visual lab results, sequencing reports, clinical trial forms, and other scanned documents. When we only use an NLP approach for document understanding, we lose layout and style information - which can be vital for document image understanding. New advances in multi-modal learning allow models to learn from both the text in documents (via NLP) and visual layout (via computer vision). We provide multi-modal visual document understanding, built on Spark OCR based on the LayoutLM architecture. It achieves new state-of-the-art accuracy in several downstream tasks, including form understanding (from 70.7 to 79.3), receipt understanding (from 94.0 to 95.2) and document image classification (from 93.1 to 94.4). Please check also webinar: Visual Document Understanding with Multi-Modal Image &amp; Text Mining in Spark OCR 3 VisualDocumentClassifier VisualDocumentClassifier is a DL model for classification documents using text and layout data. Currently available pretrained model on the Tabacco3482 dataset. Input Columns Param name Type Default Column Data Description inputCol string hocr Сolumn name with HOCR of the document Parameters Param name Type Default Description maxSentenceLength int 128 Maximum sentence length. caseSensitive boolean false Determines whether model is case sensitive. confidenceThreshold float 0f Confidence threshold. Output Columns Param name Type Default Column Data Description labelCol string label Name of output column with the predicted label. confidenceCol string confidence Name of output column with confidence. Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToHocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val visualDocumentClassifier = VisualDocumentClassifier .pretrained(&quot;visual_document_classifier_tobacco3482&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setMaxSentenceLength(128) .setInputCol(&quot;hocr&quot;) .setLabelCol(&quot;label&quot;) .setConfidenceCol(&quot;conf&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( imageToHocr, visualDocumentClassifier )) val modelPipeline = pipeline.fit(df) val result = modelPipeline.transform(df) result.select(&quot;label&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) document_classifier = VisualDocumentClassifier() .pretrained(&quot;visual_document_classifier_tobacco3482&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setMaxSentenceLength(128) .setInputCol(&quot;hocr&quot;) .setLabelCol(&quot;label&quot;) .setConfidenceCol(&quot;conf&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr, document_classifier, ]) result = pipeline.transform(df) result.select(&quot;label&quot;).show() Output: ++ | label| ++ |Letter| ++ VisualDocumentNER VisualDocumentNER is a DL model for NER documents using text and layout data. Currently available pre-trained model on the SROIE dataset. Input Columns Param name Type Default Column Data Description inputCol string hocr Сolumn name with HOCR of the document Parameters Param name Type Default Description maxSentenceLength int 512 Maximum sentence length. caseSensitive boolean false Determines whether model is case sensitive. whiteList Array[String]   Whitelist of output labels Output Columns Param name Type Default Column Data Description outputCol string entities Name of output column with entities Annotation. Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToHocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val visualDocumentNER = VisualDocumentNER .pretrained(&quot;visual_document_NER_SROIE0526&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) .setMaxSentenceLength(512) .setInputCol(&quot;hocr&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( imageToHocr, visualDocumentNER )) val modelPipeline = pipeline.fit(df) val result = modelPipeline.transform(df) result.select(&quot;entities&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) document_ner = VisualDocumentNer() .pretrained(&quot;visual_document_NER_SROIE0526&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) .setMaxSentenceLength(512) .setInputCol(&quot;hocr&quot;) .setLabelCol(&quot;label&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr, document_ner, ]) result = pipeline.transform(df) result.select(&quot;entities&quot;).show() Output: +-+ |entities | +-+ |[[entity, 0, 0, O, [word -&gt; 0£0, token -&gt; 0£0], []], [entity, 0, 0, | | B-COMPANY, [word -&gt; AEON, token -&gt; aeon], []], [entity, 0, 0, B-COMPANY,| | [word -&gt; CO., token -&gt; co], ... | +-+",
    "url": "/docs/en/ocr_visual_document_understanding",
    "relUrl": "/docs/en/ocr_visual_document_understanding"
  },
  "72": {
    "id": "72",
    "title": "Pipelines",
    "content": "Pretrained Pipelines have moved to Models Hub. Please follow this link for the updated list of all models and pipelines: Models Hub English NOTE: noncontrib pipelines are compatible with Windows operating systems. Pipelines Name Explain Document ML explain_document_ml Explain Document DL explain_document_dl Explain Document DL Win explain_document_dl_noncontrib Explain Document DL Fast explain_document_dl_fast Explain Document DL Fast Win explain_document_dl_fast_noncontrib Recognize Entities DL recognize_entities_dl Recognize Entities DL Win recognize_entities_dl_noncontrib OntoNotes Entities Small onto_recognize_entities_sm OntoNotes Entities Large onto_recognize_entities_lg Match Datetime match_datetime Match Pattern match_pattern Match Chunk match_chunks Match Phrases match_phrases Clean Stop clean_stop Clean Pattern clean_pattern Clean Slang clean_slang Check Spelling check_spelling Analyze Sentiment analyze_sentiment Analyze Sentiment DL analyze_sentimentdl_use_imdb Analyze Sentiment DL analyze_sentimentdl_use_twitter Dependency Parse dependency_parse explain_document_ml import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;The Paris metro will soon enter the 21st century, ditching single-use paper tickets for rechargeable electronic cards.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_ml,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 7 more fields] ++--+--+--+--+--+--+--+--+ | id| text| document| sentence| token| checked| lemmas| stems| pos| ++--+--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...| | 2|The Paris metro w...|[[document, 0, 11...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...| ++--+--+--+--+--+--+--+--+ */ explain_document_dl import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_dl,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 10 more fields] ++--+--+--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| checked| lemma| stem| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[token, 0, 5, Go...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...| | 2|The Paris metro w...|[[document, 0, 11...|[[token, 0, 2, Th...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 4, 8, Pa...| ++--+--+--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Google, TensorFlow] | |[Donald John Trump, United States]| +-+ */ recognize_entities_dl import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;recognize_entities_dl&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(entity_recognizer_dl,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 6 more fields] ++--+--+--+--+--+--+--+ | id| text| document| sentence| token| embeddings| ner| ner_converter| ++--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 5, Go...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...| | 2|Donald John Trump...|[[document, 0, 92...|[[document, 0, 92...|[[token, 0, 5, Do...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 16, D...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Google, TensorFlow] | |[Donald John Trump, United States]| +-+ */ onto_recognize_entities_sm Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the OntoNotes corpus and supports the identification of 18 entities. import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London, from 2008 to 2016, before rejoining Parliament. &quot;), (2, &quot;A little less than a decade later, dozens of self-driving startups have cropped up while automakers around the world clamor, wallet in hand, to secure their place in the fast-moving world of fully automated transportation.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;onto_recognize_entities_sm&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.1.0 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(onto_recognize_entities_sm,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 6 more fields] ++--+--+--+--+--+--+ | id| text| document| token| embeddings| ner| entities| ++--+--+--+--+--+--+ | 1|Johnson first ent...|[[document, 0, 17...|[[token, 0, 6, Jo...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 6, Jo...| | 2|A little less tha...|[[document, 0, 22...|[[token, 0, 0, A,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 32, A...| ++--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* ++ |result | ++ |[Johnson, first, 2001, Parliament, eight years, London, 2008 to 2016, Parliament]| |[A little less than a decade later, dozens] | ++ */ onto_recognize_entities_lg Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the OntoNotes corpus and supports the identification of 18 entities. import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London, from 2008 to 2016, before rejoining Parliament. &quot;), (2, &quot;A little less than a decade later, dozens of self-driving startups have cropped up while automakers around the world clamor, wallet in hand, to secure their place in the fast-moving world of fully automated transportation.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;onto_recognize_entities_lg&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.1.0 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(onto_recognize_entities_lg,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 6 more fields] ++--+--+--+--+--+--+ | id| text| document| token| embeddings| ner| entities| ++--+--+--+--+--+--+ | 1|Johnson first ent...|[[document, 0, 17...|[[token, 0, 6, Jo...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 6, Jo...| | 2|A little less tha...|[[document, 0, 22...|[[token, 0, 0, A,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 32, A...| ++--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Johnson, first, 2001, Parliament, eight years, London, 2008, 2016, Parliament]| |[A little less than a decade later, dozens] | +-+ */ match_datetime DateMatcher yyyy/MM/dd import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;I would like to come over and see you in 01/02/2019.&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;match_datetime&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(match_datetime,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 4 more fields] ++--+--+--+--+--+ | id| text| document| sentence| token| date| ++--+--+--+--+--+ | 1|I would like to c...|[[document, 0, 51...|[[document, 0, 51...|[[token, 0, 0, I,...|[[date, 41, 50, 2...| | 2|Donald John Trump...|[[document, 0, 92...|[[document, 0, 92...|[[token, 0, 5, Do...|[[date, 24, 36, 1...| ++--+--+--+--+--+ */ annotation.select(&quot;date.result&quot;).show(false) /* ++ |result | ++ |[2019/01/02]| |[1946/06/14]| ++ */ match_pattern RegexMatcher (match phone numbers) import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;You should call Mr. Jon Doe at +33 1 79 01 22 89&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;match_pattern&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(match_pattern,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 4 more fields] ++--+--+--+--+--+ | id| text| document| sentence| token| regex| ++--+--+--+--+--+ | 1|You should call M...|[[document, 0, 47...|[[document, 0, 47...|[[token, 0, 2, Yo...|[[chunk, 31, 47, ...| ++--+--+--+--+--+ */ annotation.select(&quot;regex.result&quot;).show(false) /* +-+ |result | +-+ |[+33 1 79 01 22 89]| +-+ */ match_chunks The pipeline uses regex &lt;DT/&gt;?/&lt;JJ/&gt;*&lt;NN&gt;+ import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;The book has many chapters&quot;), (2, &quot;the little yellow dog barked at the cat&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;match_chunks&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(match_chunks,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 5 more fields] ++--+--+--+--+--+--+ | id| text| document| sentence| token| pos| chunk| ++--+--+--+--+--+--+ | 1|The book has many...|[[document, 0, 25...|[[document, 0, 25...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[chunk, 0, 7, Th...| | 2|the little yellow...|[[document, 0, 38...|[[document, 0, 38...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[chunk, 0, 20, t...| ++--+--+--+--+--+--+ */ annotation.select(&quot;chunk.result&quot;).show(false) /* +--+ |result | +--+ |[The book] | |[the little yellow dog, the cat]| +--+ */ French Pipelines Name Explain Document Large explain_document_lg Explain Document Medium explain_document_md Entity Recognizer Large entity_recognizer_lg Entity Recognizer Medium entity_recognizer_md Feature Description NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura POS Trained by PerceptronApproach annotator on the Universal Dependencies Size Model size indicator, md and lg. The large pipeline uses glove_840B_300 and the medium uses glove_6B_300 WordEmbeddings French explain_document_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_lg&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,fr,public/models) testData: org.apache.spark.sql.DataFrame = [id: bigint, text: string] annotation: org.apache.spark.sql.DataFrame = [id: bigint, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[token, 0, 12, C...|[[pos, 0, 12, ADV...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[token, 0, 7, Em...|[[pos, 0, 7, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /*+-+ |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ French explain_document_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_md&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_md,fr,public/models) testData: org.apache.spark.sql.DataFrame = [id: bigint, text: string] annotation: org.apache.spark.sql.DataFrame = [id: bigint, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[token, 0, 12, C...|[[pos, 0, 12, ADV...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[token, 0, 7, Em...|[[pos, 0, 7, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, au CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ French entity_recognizer_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_lg&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ French entity_recognizer_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_md&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /*+-+ |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, au CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ Italian Pipelines Name Explain Document Large explain_document_lg Explain Document Medium explain_document_md Entity Recognizer Large entity_recognizer_lg Entity Recognizer Medium entity_recognizer_md Feature Description NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Lemma Trained by Lemmatizer annotator on DXC Technology dataset POS Trained by PerceptronApproach annotator on the Universal Dependencies Size Model size indicator, md and lg. The large pipeline uses glove_840B_300 and the medium uses glove_6B_300 WordEmbeddings Italian explain_document_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_lg&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[token, 0, 1, La...|[[pos, 0, 1, DET,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 3, 6, FI...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[token, 0, 4, Re...|[[pos, 0, 4, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +--+ |result | +--+ |[FIFA, Zidane, Materazzi] | |[Reims, Domani, Mondiali femminili]| +--+ */ Italian explain_document_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_md&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[token, 0, 1, La...|[[pos, 0, 1, DET,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 9, La...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[token, 0, 4, Re...|[[pos, 0, 4, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[La FIFA, Zidane, Materazzi]| |[Reims, Domani, Mondiali] | +-+ */ Italian entity_recognizer_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_lg&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 3, 6, FI...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +--+ |result | +--+ |[FIFA, Zidane, Materazzi] | |[Reims, Domani, Mondiali femminili]| +--+ */ Italian entity_recognizer_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_md&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 9, La...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[La FIFA, Zidane, Materazzi]| |[Reims, Domani, Mondiali] | +-+ */ Spanish Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.4.0 es   Download Explain Document Medium explain_document_md 2.4.0 es   Download Explain Document Large explain_document_lg 2.4.0 es   Download Entity Recognizer Small entity_recognizer_sm 2.4.0 es   Download Entity Recognizer Medium entity_recognizer_md 2.4.0 es   Download Entity Recognizer Large entity_recognizer_lg 2.4.0 es   Download Feature Description Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Size Model size indicator, sm, md, and lg. The small pipelines use glove_100d, the medium pipelines use glove_6B_300, and large pipelines use glove_840B_300 WordEmbeddings Russian Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.4.4 ru   Download Explain Document Medium explain_document_md 2.4.4 ru   Download Explain Document Large explain_document_lg 2.4.4 ru   Download Entity Recognizer Small entity_recognizer_sm 2.4.4 ru   Download Entity Recognizer Medium entity_recognizer_md 2.4.4 ru   Download Entity Recognizer Large entity_recognizer_lg 2.4.4 ru   Download Feature Description Lemma Trained by Lemmatizer annotator on the Universal Dependencies POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Dutch Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 nl   Download Explain Document Medium explain_document_md 2.5.0 nl   Download Explain Document Large explain_document_lg 2.5.0 nl   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 nl   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 nl   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 nl   Download Norwegian Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 no   Download Explain Document Medium explain_document_md 2.5.0 no   Download Explain Document Large explain_document_lg 2.5.0 no   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 no   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 no   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 no   Download Polish Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 pl   Download Explain Document Medium explain_document_md 2.5.0 pl   Download Explain Document Large explain_document_lg 2.5.0 pl   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 pl   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 pl   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 pl   Download Portuguese Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 pt   Download Explain Document Medium explain_document_md 2.5.0 pt   Download Explain Document Large explain_document_lg 2.5.0 pt   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 pt   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 pt   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 pt   Download Multi-language Pipeline Name Build lang Description Offline LanguageDetectorDL detect_language_7 2.5.2 xx   Download LanguageDetectorDL detect_language_20 2.5.2 xx   Download The model with 7 languages: Czech, German, English, Spanish, French, Italy, and Slovak The model with 20 languages: Bulgarian, Czech, German, Greek, English, Spanish, Finnish, French, Croatian, Hungarian, Italy, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Swedish, Turkish, and Ukrainian How to use Online To use Spark NLP pretrained pipelines, you can call PretrainedPipeline with pipeline’s name and its language (default is en): pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;) Same in Scala val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;, lang=&quot;en&quot;) Offline If you have any trouble using online pipelines or models in your environment (maybe it’s air-gapped), you can directly download them for offline use. After downloading offline models/pipelines and extracting them, here is how you can use them iside your code (the path could be a shared storage like HDFS in a cluster): val advancedPipeline = PipelineModel.load(&quot;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&quot;) // To use the loaded Pipeline for prediction advancedPipeline.transform(predictionDF)",
    "url": "/docs/en/pipelines",
    "relUrl": "/docs/en/pipelines"
  },
  "73": {
    "id": "73",
    "title": "Preannotations with Spark NLP",
    "content": "Annotation Lab offers out-of-the-box support for Named Entity Recognition, Classification and Assertion Status Preannotations. Those are extremely useful for bootstraping any annotation project, as the annotation team does not start the labeling from scratch but can leverage the existing knowledge transfer from domain experts to models. This way, the annotation efforts are significantly reduced. For running preannotation on one or several tasks, the Project Owner or the Manager must select the target tasks and can click on the Preannotate Button from the upper right side of the Tasks List Page. This will display a popup with information regarding the last deployment including the list of models deployed and the labels they predict. This information is very important, especially when multiple users are doing training and deployment in parallel. So before doing preannotations on your tasks, carefully check the list of currently deployed models and their labels. If needed, users can deploy the models defined in the current project (based on the current Labeling Config) by clicking the “Deploy” button. After the deployment is complete, the preannotation can be triggered. Pretrained Models On the project setup screen you can find a Spark NLP pipeline config widget which lists all available models together with the labels those are predicting. By simply selecting the relevant labels for your project and clicking the add button you can add the predefined labels to your project and take advantage of the Spark NLP auto labeling capabilities. In the below example we are reusing the posology model that comes with 7 labels related to drugs. Pipeline Limitations Loading too many models in the preannotation server is not memory efficient and may not be practically required. Starting from version 1.8.0, Annotation Lab supports maximum of five different models to be used for the preannotation server deployment. Another restriction for the server deployment is that two models trained on different embeddings cannot be used together in the same project. The Labeling Config will throw validation errors in any of the cases above and hence Labeling Config cannot be saved and the preannotation server deployment will fail.",
    "url": "/docs/en/alab/preannotations",
    "relUrl": "/docs/en/alab/preannotations"
  },
  "74": {
    "id": "74",
    "title": "Productionizing Spark NLP",
    "content": "Productionizing Spark NLP in Databricks This documentation page will describe how to use Databricks to run Spark NLP Pipelines for production purposes. About Databricks Databricks is an enterprise software company founded by the creators of Apache Spark. The company has also created MLflow, the Serialization and Experiment tracking library you can use (inside or outside databricks), as described in the section “Experiment Tracking”. Databricks develops a web-based platform for working with Spark, that provides automated cluster management and IPython-style notebooks. Their infrastructured is provided for training and production purposes, and is integrated in cloud platforms as Azure and AWS. Spark NLP is a proud partner of Databricks and we offer a seamless integration with them - see Install on Databricks. All Spark NLP capabilities run in Databricks, including MLFlow serialization and Experiment tracking, what can be used for serving Spark NLP for production purposes. About MLFlow MLFlow is a serialization and Experiment Tracking platform, which also natively suports Spark NLP. We have a documentation entry about MLFlow in the “Experiment Tracking” section. It’s highly recommended that you take a look before moving forward in this document, since we will use some of the concepts explained there. We will use MLFlow serialization to serve our Spark NLP models. Creating a cluster in Databricks As mentioned before, Spark NLP offers a seamless integration with Databricks. To create a cluster, please follow the instructions in Install on Databricks. That cluster can be then replicated (cloned) for production purposes later on. Configuring Databricks for Spark NLP and MLFlow In Databricks Runtime Version, select any Standard runtime, not ML ones.. These ones add their version of MLFlow, and some incompatibilities may arise. For this example, we have used 8.3 (includes Apache Spark 3.1.1, Scala 2.12) The cluster instantiated is prepared to use Spark NLP, but to make it production-ready using MLFlow, we need to add the MLFlow jar, in addition to the Spark NLP jar, as shown in the “Experiment Tracking” section. In that case, we did it instantiating adding both jars (&quot;spark.jars.packages&quot;:&quot; com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.2,org.mlflow:mlflow-spark:1.21.0&quot;) into the SparkSession. However, in Databricks, you don’t instantiate programatically a session, but you configure it in the Compute screen, selecting your Spark NLP cluster, and then going to Configuration -&gt; Advanced Options -&gt; Sparl -&gt; Spark Config, as shown in the following image: In addition to Spark Config, we need to add the Spark NLP and MLFlow libraries to the Cluster. You can do that by going to Libraries inside your cluster. Make sure you have spark-nlp and mlflow. If not, you can install them either using PyPI or Maven artifacts. In the image below you can see the PyPI alternative: Creating a notebook You are ready to create a notebook in Databricks and attach it to the recently created cluster. To do that, go to Create - Notebook, and select the cluster you want in the dropdown above your notebook. Make sure you have selected the cluster with the right Spark NLP + MLFlow configuration. To check everything is ok, run the following lines: 1) To check the session is running: spark 2) To check jars are in the session: spark.sparkContext.getConf().get(&#39;spark.jars.packages&#39;) You should see the following output from the last line (versions may differ depending on which ones you used to configure your cluster) Out[2]: &#39;com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.2,org.mlflow:mlflow-spark:1.21.0&#39; Logging the experiment in Databricks using MLFlow As explained in the “Experiment Tracking” section, MLFlow can log Spark MLLib / NLP Pipelines as experiments, to carry out runs on them, track versions, etc. MLFlow is natively integrated in Databricks, so we can leverage the mlflow.spark.log_model() function of the Spark flavour of MLFlow, to start tracking our Spark NLP pipelines. Let’s first import our libraries… import mlflow import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import pandas as pd from sparknlp.training import CoNLL import pyspark from pyspark.sql import SparkSession Then, create a Lemmatization pipeline: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = LemmatizerModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;prediction&quot;) # It&#39;s mandatory to call it prediction pipeline = Pipeline(stages=[ documentAssembler, tokenizer, lemmatizer ]) IMPORTANT: Last output column of the last component in the pipeline should be called prediction. Finally, let’s log the experiment. In the “Experiment Tracking” section, we used the pip_requirements parameter in the log_model() function to set the required libraries: But we mentioned using conda is also available. Let’s use conda in this example: conda_env = { &#39;channels&#39;: [&#39;conda-forge&#39;], &#39;dependencies&#39;: [ &#39;python=3.8.8&#39;, { &quot;pip&quot;: [ &#39;pyspark==3.1.1&#39;, &#39;mlflow==1.21.0&#39;, &#39;spark-nlp==3.3.2&#39; ] } ], &#39;name&#39;: &#39;mlflow-env&#39; } With this conda environment, we are ready to log our pipeline: mlflow.spark.log_model(p_model, &quot;lemmatizer&quot;, conda_env=conda_env) You should see an output similar to this one: (6) Spark Jobs (1) MLflow run Logged 1 run to an experiment in MLflow. Learn more Experiment UI On the top right corner of your notebook, you will see the Experiment widget, and inside, as shown in the image below. You can also access Experiments UI if you switch your environment from “Data Science &amp; Engineering” to “Machine Learning”, on the left panel… … or clicking on the “experiment” word in the cell output (it’s a link!) Once in the experiment UI, you will see the following screen, where your experiments are tracked. If you click on the Start Time cell of your experiment, you will reach the registered MLFlow run. On the left panel you will see the MLFlow model and some other artifacts, as the conda.yml and pip_requirements.txt that manage the dependencies of your models. On the right panel, you will see two snippets, about how to call to the model for inference internally from Databricks. 1) Snippet for calling with a Pandas Dataframe: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; # Load model as a Spark UDF. loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model) # Predict on a Spark DataFrame. columns = list(df.columns) df.withColumn(&#39;predictions&#39;, loaded_model(*columns)).collect() 2) Snippet for calling with a Spark Dataframe. We won’t include it in this documentation because that snippet does not include SPark NLP specificities. To make it work, the correct snippet should be: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model) # Predict on a Spark DataFrame. res_spark = loaded_model.predict(df_1_spark.rdd) IMPORTANT: You will only get the last column (prediction) results, which is a list of Rows of Annotation Types. To convert the result list into a Spark Dataframe, use the following schema: import pyspark.sql.types as T import pyspark.sql.functions as f annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) And then, get the results (for example, in res_spark) and apply the schema: spark_res = spark.createDataFrame(res_pandas[0], schema=annotationType) Calling the experiment for production purposes 1. Internally, if the data is in Databricks If your data lies in Datalake, in Spark Tables, or any other internal storage in Databricks, you just need to use the previous snippets (depending if you want to use Pandas or Spark Dataframes), and you are ready to go. Example for Spark Dataframes: Try to use Spark Dataframes by default, since converting from Spark Dataframes into Pandas triggers a collect() first, removing all the parallelism capabilities of Spark Dataframes. The next logical step is to create Notebooks to be called programatically using the snippets above, running into production clusters. There are two ways to do this: using Batch Inference or using Jobs. 2. Internally, using Batch Inference (with Spark Tables) If we come back to the experiment ui, you will see, above the Pandas and Spark snippets, a button with the text “Register Model”. If you do that, you will register the experiment to be called externally, either for Batch Inference or with a REST API (we will get there!). After clicking the Register Model button, you will see a link instead of the button, that will enabled after some seconds. By clicking that link, you will be redirected to the Model Inference screen. This new screen has a button on the top right, that says “Use model for inference”. By clicking on it, you will see two options: Batch Inference or REST API. Batch inference requires a Spark Table for input, and another for output, and after configuring them, what you will see is an auto-generated notebook to be executed on-demand, programatically or with crons, that is prepared to load the environment and do the inference, getting the text fron the input table and storing the results in the output table. This is an example of how the notebook looks like: 3. Externally, with the MLFlow Serve REST API Instead of chosing a Batch Inference, you can select REST API. This will lead you to another screen, when the model will be loaded for production purposes in an independent cluster. Once deployed, you will be able to: 1) Check the endpoint URL to consume the model externally; 2) Test the endpoint writing a json (in our example, ‘text’ is our first input col of the pipeline, so it shoud look similar to: {&quot;text&quot;: &quot;This is a test of how the lemmatizer works&quot;} You can see the response in the same screen. 3) Check what is the Python code or cURL command to do that very same thing programatically. By just using that Python code, you can already consume it for production purposes from any external web app. IMPORTANT: As per 26/11/2021, there is an issue being studied by Databricks team, regarding the creation on the fly of job clusters to serve MLFlow models. There is not a way to configure the Spark Session, so the jars are not loaded and the model fails to start. This will be fixed in later versions of Databricks. In the meantime, see a workaround in point 4. 4. Databricks Jobs asynchronous REST API Creating the notebook for the job And last, but not least, another approach to consume models for production purposes. the Jobs API. Databricks has its own API for managing jobs, that allows you to instantiate any notebook or script as a job, run it, stop it, and manage all the life cycle. And you can configure the cluster where this job will run before hand, what prevents having the issue described in point 3. To do that: 1) Create a new production cluster, as described before, cloning you training environment but adapting it to your needs for production purposes. Make sure the Spark Config is right, as described at the beginning of this documentation. 2) Create a new notebook. Always check that the jars are in the session: spark.sparkContext.getConf().get(&#39;spark.jars.packages&#39;) 3) Add the Spark NLP imports. import mlflow import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import pandas as pd from sparknlp.training import CoNLL import pyspark from pyspark.sql import SparkSession import pyspark.sql.types as T import pyspark.sql.functions as f import json 4) Let’s define that an input param called text will be sent in the request. Let’s get the text from that parameter using dbutils. input = &quot;&quot; try: input = dbutils.widgets.get(&quot;text&quot;) print(&#39;&quot;text&quot; input found: &#39; + input) except: print(&#39;Unable to run: dbutils.widgets.get(&quot;text&quot;). Setting it to NOT_SET&#39;) input = &quot;NOT_SET&quot; Right now, the input text will be in input var. You can trigger an exception or set the input to some default value if the parameter does not come in the request. 5) Let’s create a Spark Dataframe with the input df = spark.createDataFrame([[input]]).toDF(&#39;text&#39;) 6) And now, we just need to use the snippet for Spark Dataframe to consume MLFlow models, described above: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model) # Predict on a Spark DataFrame. res_spark = loaded_model.predict(df_1_spark.rdd) import pyspark.sql.types as T import pyspark.sql.functions as f annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) spark_res = spark.createDataFrame(res_spark[0], schema=annotationType) 7) Let’s transform our lemmatized tokens from the Dataframe into a list of strings: l = spark_res.select(&quot;result&quot;).collect() txt_results = [x[&#39;result&#39;] for x in l] 8) And finally, let’s use again dbutils to tell Databricks to spin off the run and return an exit parameter: the list of token strings. dbutils.notebook.exit(json.dumps({ &quot;status&quot;: &quot;OK&quot;, &quot;results&quot;: txt_results })) Configuring the job Last, but not least. We need to precreate the job, so that we run it from the API. We could do that using the API as well, but we will show you how to do it using the UI. On the left panel, go to Jobs and then Create Job. In the jobs screen, you will see you job created. It’s not running, it’s prepared to be called on demand, programatically or in the interface, with a text input param. Let’s see how to do that: Running the job 1) In the jobs screen, if you click on the job, you will enter the Job screen, and be able to set your text input parameter and run the job manually. You can use this for testing purpores, but the interesting part is calling it externally, using the Databricks Jobs API. 2) Using the Databricks Jobs API, from for example, Postman. POST HTTP request URL: https://[your_databricks_instance]/api/2.1/jobs/run-now Authorization: [use Bearer Token. You can get it from Databricks, Settings, User Settings, Generate New Token.] Body: { &quot;job_id&quot;: [job_id, check it in the Jobs screen], &quot;notebook_params&quot;: {&quot;text&quot;: &quot;This is an example of how well the lemmatizer works&quot;} } As it’s an asynchronous call, it will return the number a number of run, but no results. You will need to query for results using the number of the run and the following url https://[your_databricks_instance]/2.1/jobs/runs/get-output You will get a big json, but the most relevant info, the output, will be up to the end: {&quot;notebook_output&quot;: { &quot;status&quot;: &quot;OK&quot;, &quot;results&quot;: [&quot;This&quot;, &quot;is&quot;, &quot;a&quot;, &quot;example&quot;, &quot;of&quot;, &quot;how&quot;, &quot;lemmatizer&quot;, &quot;work&quot;] }} The notebook will be prepared in the job, but idle, until you call it programatically, what will instantiate a run. Check the Jobs API for more information about what you can do with it and how to adapt it to your solutions for production purposes.",
    "url": "/docs/en/production-readiness",
    "relUrl": "/docs/en/production-readiness"
  },
  "75": {
    "id": "75",
    "title": "Project Setup",
    "content": "Every project in Annotation Lab should have the following information: a unique name and a short description; a team of annotators, reviewers and a manager which will colaborate on the project; a configuration which specifies the type of annotations that will be created. To create a new project, click on the Create Project button on the Home Page and choose a name for it. The project can include a short description and annotation instructions/guidelines. Share your project with the annotation team When working in teams, projects can be shared with other team members. The user who creates a project is called a Project Owner. He/She has complete visibility and ownership of the project for its entire lifecycle. If the Project Owner is removed from the user database, then all his/her projects are transfered to a new project owner. The Project Owner can edit the project configuration, can import/export tasks, can create a project team that will work on his project and can access project analytics. When defining the project team, a project owner has access to three distinct roles: Annotator, Reviewer, and Manager. These are very useful for most of the workflows that our users follow. An Annotator is able to see the tasks which have been assigned to him or her and can create annotations on the documents. The Reviewer is able to see the work of the annotators and approve it or reject in case he finds issues that need to be solved. The Manager is able to see the work of the Annotators and of the Reviewers and he can assign tasks to team members. This is useful for eliminating work overlap and for a better management of the work load. To add a user to your project team, navigate to the Project Setup page. On the Manage Project Team tab, start typing the name of a user in the available text box. This will populate a list of available users having the username start with the caracters you typed. From the dropdown select the user you want to add to your team. Select a role for the user and click on the “Add to team” button. Supported Project Types We currently support multiple predefined project configurations. The most popular ones are Text Classification and Named Entity Recognition. Create a setup from scratch or customize a predefined one according to your needs. For customizing a predefined configuration, click on the corresponding link in the table above and then navigate to the Labeling config widget and manually edit/update it to contain the labels you need. After you finish editing the labels you want to define for your project click the “Save” button. Text Classification Project The Annotation Lab offers two types of classification widgets: The first one supports single choice labels. You can activate it by choosing Text Classification from the list of predefined projects. The labels can be changed by directly editing them in the Labeling Config XML style widget. The updates will be automatically reflected in the right side preview. The second configuration offers support for multi-class classification. It can be activated by clicking on the Multi classification link in the list of predefined configurations. This option will add to the labeling config widget multiple checkboxes, grouped by headers. The names of the choices and well as the headers are customizable. You can also add new choices if necessary. Named Entity Recognition Project Named Entity Recognition (NER) refers to the identification and classification of entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. The Annotation Lab offers support for two types of labels: Simple labels for NER or assertion models; Binary relations for relation extraction models. Assertion Status Project The syntax for defining an Assertion Status label is the same as for the NER labels, with an additional attribute - assertion which should be set to true (see example below). This is convention defined by Annotation Lab users which we exploited for identifying the labels to include in the training and prediction of Assertion Models. A simple Labeling Config with Assertion Status defined should look like the following: &lt;View&gt; &lt;Labels name=&quot;ner&quot; toName=&quot;text&quot;&gt; &lt;Label value=&quot;Medicine&quot; background=&quot;orange&quot; hotkey=&quot;_&quot;/&gt; &lt;Label value=&quot;Condition&quot; background=&quot;orange&quot; hotkey=&quot;_&quot;/&gt; &lt;Label value=&quot;Procedure&quot; background=&quot;green&quot; hotkey=&quot;8&quot;/&gt; &lt;Label value=&quot;Absent&quot; assertion=&quot;true&quot; background=&quot;red&quot; hotkey=&quot;Z&quot;/&gt; &lt;Label value=&quot;Past&quot; assertion=&quot;true&quot; background=&quot;red&quot; hotkey=&quot;X&quot;/&gt; &lt;/Labels&gt; &lt;View style=&quot;height: 250px; overflow: auto;&quot;&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;/&gt; &lt;/View&gt; &lt;/View&gt; Notice assertion=”true” in Absent and Past labels, which marks each of those labels as Assertion Status Labels. Labels customization Names of the labels must be carefully chosen so they are easy to understand by the annotators. Highlighting colors can be assigned to each labels by either specifying the color name or the color code. Shortcuts keys can be assigned to each label to make the annotation process easier and faster. &lt;Labels name=&quot;ner&quot; toName=&quot;text&quot;&gt; &lt;Label value=&quot;Cancer&quot; background=&quot;red&quot; hotkey=&quot;c&quot;/&gt; &lt;Label value=&quot;TumorSize&quot; background=&quot;blue&quot; hotkey=&quot;t&quot;/&gt; &lt;Label value=&quot;TumorLocation&quot; background=&quot;pink&quot; hotkey=&quot;l&quot;/&gt; &lt;Label value=&quot;Symptom&quot; background=&quot;#dda0dd&quot; hotkey=&quot;z&quot;/&gt; &lt;/Labels&gt; Relations The Annotation Lab also offers support for relation extraction. Relations are introduced by simply specifying their label. &lt;Relations&gt; &lt;Relation value=&quot;CancerSize&quot; /&gt; &lt;Relation value=&quot;CancerLocation&quot;/&gt; &lt;Relation value=&quot;MetastasisLocation&quot;/&gt; &lt;/Relations&gt; No other constraints can currently be enforced on the labels linked by the defined relations so the annotators must be extra careful and follow the annotation guidelines that specify how the defined relations can be used. Project Validations Annotation Lab validates the defined configurations before allowing users to save them. Error messages are shown on the preview section. For instance, if the labels names do not align with the lables of the corresponding models an error is displayed. The same happens if reserved shortcut keys are used or if incompatible pretrained models are selected for preannotations. Delete a Project To delete a project, on the Home screen click on the settings icon for that project. This will redirect you to the Project Setup screen where you will have the Delete Project button under the Project Instructions text area. Click on the Delete Project button and confirm the deletion by clicking on the Yes button on the popup. Best practices Use Google Chrome. While Annotation Lab will work on other web browsers, we recommend using it on Google Chrome to ensure the best experience. Use one browser window at a time. Using the Annotation Lab web application via multiple sessions in parallel (i.e., multiple browser windows/tabs) connected with the same user account is not recommended.",
    "url": "/docs/en/alab/project_setup",
    "relUrl": "/docs/en/alab/project_setup"
  },
  "76": {
    "id": "76",
    "title": "Annotation Lab",
    "content": "Lightning fast annotation A highly efficient annotation tool for all enterprise teams who need to: annotate documents; build ML models &amp; get them to production; All that without writing a line of code! Included with every Spark NLP subscription, or can be purchased on its own. Try Free Productivity Bootstrap annotations with Spark NLP models Keep Annotators in the zone Reach agreement quickly Auto NLP Active learning - no data scientist required Deliver a model not just annotations Build for high compliance environments Teamwork Projects &amp; teams Customizable workflows High security Resources General tutorials Annotation best practices Tips and tricks",
    "url": "/docs/en/alab/quickstart",
    "relUrl": "/docs/en/alab/quickstart"
  },
  "77": {
    "id": "77",
    "title": "Quick Start",
    "content": "Requirements &amp; Setup Spark NLP is built on top of Apache Spark 3.x. For using Spark NLP you need: Java 8 Apache Spark 3.1.x (or 3.0.x, or 2.4.x, or 2.3.x) It is recommended to have basic knowledge of the framework and a working environment before using Spark NLP. Please refer to Spark documentation to get started with Spark. Install Spark NLP in Python Scala and Java Databricks EMR Join our Slack channel Join our channel, to ask for help and share your feedback. Developers and users can help each other getting started here. Spark NLP Slack Spark NLP in Action Make sure to check out our demos built by Streamlit to showcase Spark NLP in action: Spark NLP Demo Spark NLP Workshop If you prefer learning by example, check this repository: Spark NLP Workshop It is full of fresh examples and even a docker container if you want to skip installation. Below, you can follow into a more theoretical and thorough quick start guide. Where to go next If you need more detailed information about how to install Spark NLP you can check the Installation page Detailed information about Spark NLP concepts, annotators and more may be found HERE",
    "url": "/docs/en/quickstart",
    "relUrl": "/docs/en/quickstart"
  },
  "78": {
    "id": "78",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/recognize_biomedical_entities",
    "relUrl": "/recognize_biomedical_entities"
  },
  "79": {
    "id": "79",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/recognize_clinical_entities",
    "relUrl": "/recognize_clinical_entities"
  },
  "80": {
    "id": "80",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/recognize_entitie",
    "relUrl": "/recognize_entitie"
  },
  "81": {
    "id": "81",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/recognize_social_determinants",
    "relUrl": "/recognize_social_determinants"
  },
  "82": {
    "id": "82",
    "title": "Release Notes",
    "content": "2.3.0 Highlights Multipage PDF annotation. Annotation Lab 2.3.0 supports the complete flow of import, annotation, and export for multi-page PDF files. Users have two options for importing a new PDF file into the Visual NER project: Import PDF file from local storage Add a link to the PDF file in the file attribute. After import, the user can see a task on the task page with a file name. On the labeling page, the user can view the PDF file with pagination so that the user can annotate the PDF one page at a time. After completing the annotation, the user can submit a task and it is ready to be exported to JSON and the user can import this exported file into any Visual NER project. [Note: Export in COCO format is not yet supported for PDF file] Redesign of the Project Setup Page. With the addition of many new features on every release, the Project Setup page became crowded and difficult to digest by users. In this release we have split its main components into multiple tabs: Project Description, Sharing, Configuration, and Training. Train and Test Dataset. Project Owner/Manager can tag the tasks that will be used for train and for test purposes. For this, two predefined tags will be available in all projects: Train and Test. Enhanced Relation Annotation. The user experience while annotating relations on the Labeling page has been improved. Annotators can now select the desired relation(s) by clicking the plus “+” sign present next to the relations arrow. Other UX improvements: Multiple items selection with Shift Key in Models Hub and Tasks page, Click instead of hover on more options in Models Hub and Tasks page, Tabbed View on the ModelsHub page. Bug fixes Validations related to Training Settings across different types of projects were fixed. It is not very common to upload an expired license given a valid license is already present. But in case users did that there was an issue while using a license in the spark session because only the last uploaded license was used. Now it has been fixed to use any valid license no matter the order of upload. Sometimes search and filters in the ModelsHub page were not working. Also, there was an issue while removing defined labels on the Upload Models Form. Both of these issues are fixed. 2.2.2 Highlights Support for pretrained Relation Extraction and Assertion Status models. A valid Spark NLP for HealthCare License is needed to download pretrained models via the Models Hub page. After download, they can be added to the Project Config and used for preannotations. Support for uploading local images. Until this version, only images from remote URLs could be uploaded for Image projects. With this version the Annotation Lab supports uploading images from you local storage/computer. It is possible to either import one image or multiple images by zipping them together. The maximum image file size is 16 MB. If you need to upload files exceding the default configuration, please contact your system administrator who will change the limit size in the installation artifact and run the upgrade script. Improved support for Visual NER projects. A sample task can be imported from the Import page by clicking the “Add Sample Task” button. Also default config for the Visual NER project contains zoom feature which supports maximum possible width for low resolution images when zooming. Improved Relation Labeling. Creating numerous relations in a single task can look a bit clumsy. The limited space in Labeling screen, the relation arrows and different relation types all at once could create difficulty to visualize them properly. We improved the UX for this feature: Spaces between two lines if relations are present Ability to Filter by certain relations When hovered on one relation, only that is focused Miscellaneous. Generally when a first completion in a task is submitted, it is very likely for that completion to be the ground truth for that task. Starting with this version, the first submitted completion gets automatically starred. Hitting submit button on next completion, annotator are asked to either just submit or submit and star it. Bug fixes On restart of the Annotation Lab machine/VM all Downloaded models (from Models Hub) compatible with Spark NLP 3.1 version were deleted. We have now fixed this issue. Going forward, it is user’s responsibility to remove any incompatible models. Those will only be marked as “Incompatible” in Models Hub. This version also fixes some reported issues in training logs. The CONLL exports were including Assertion Status labels too. Going forward Assertion Status labels will be excluded given correct Project Config is setup. Read more 2.1.0 Highlights A new project configuration “Visual NER Labeling” was added, which provides the skeleton for text annotation on scanned images. Project Owners or Project Manager can train open-source models too. The UI components and navigation of Annotation Lab - as a SPA - continues to improve its performance. The application has an increased performance (security and bug fixes, general optimizations). More models &amp; embeddings included in the Annotation Lab image used for deployments. This should reduce the burden for system admins during the installation in air-gapped or enterprise environments. Easier way to add relations. Project Owners and Managers can see the proper status of tasks, taking into account their own completions. Security Fixes. We understand and take the security issues as the highest priority. On every release, we run our artifacts and images through series of security testings (Static Code analysis, PenTest, Images Vulnerabilities Test, AWS AMI Scan Test). This version resolves a few critical issues that were recently identified in Python Docker image we use. We have upgraded it to a higher version. Along with this upgrade, we have also refactored our codebase to pass our standard Static Code Analysis. Bug fixes An issue with using Uploaded models was fixed so any uploaded models can be loaded in Project Config and used for preannotation. Issues related to error messages when uploading a valid Spark OCR license and when trying to train NER models while Spark OCR license was expired are now fixed. The issue with exporting annotations in COCO format for image projects was fixed. Project Owners and Managers should be able to export COCO format which also includes images used for annotations. The bug reports related to unexpected scrolling of the Labeling page, issues in Swagger documentation, and typos in some hover texts are now fixed. Read more 2.0.1 Highlights Inter-Annotation Agreement Charts. To get a measure of how well multiple annotators can make the same annotation decision for a certain category, we are shipping seven different charts. To see these charts users can click on the third tab “Inter-Annotator Agreement” of the Analytics Dashboard of NER projects. There are dropdown boxes to change annotators for comparison purposes. It is also possible to download the data of some charts in CSV format by clicking the download button present at the bottom right corner of each of them. Updated CONLL Export. In previous versions, numerous files were created based on Tasks and Completions. There were issues in the Header and no sentences were detected. Also, some punctuations were not correctly exported or were missing. The new CONLL export implementation results in a single file and fixes all the above issues. As in previous versions, if only Starred completions are needed in the exported file, users can select the “Only ground truth” checkbox. Search tasks by label. Now, it is possible to list the tasks based on some annotation criteria. Examples of supported queries: “label: ABC”, “label: ABC=DEF”, “choice: Mychoice”, “label: ABC=DEF”. Validation of labels and models is done beforehand. An error message is shown if the label is incompatible with models. Transfer Learning support for Training Models. Now its is possible to continue model training from an already available model. If a Medical NER model is present in the system, the project owner or manager can go to Advanced Options settings of the Training section in the Setup Page and choose it to Fine Tune the model. When Fine Tuning is enabled, the embeddings that were used to train the model need to be present in the system. If present, it will be automatically selected, otherwise users need to go to the Models Hub page and download or upload it. Training Community Models without the need of License. In previous versions, Annotation Lab didn’t allow training without the presence of Spark NLP for Healthcare license. But now the training with community embeddings is allowed even without the presence of Valid license. Support for custom training scripts. If users want to change the default Training script present within the Annotation Lab, they can upload their own training pipeline. In the Training section of the Project Setup Page, only admin users can upload the training scripts. At the moment we are supporting the NER custom training script only. Users can now see a proper message on the Modelshub page when annotationlab is not connected to the internet (AWS S3 to be more precise). This happens in air-gapped environments or some issues in the enterprise network. Users now have the option to download the trained models from the Models Hub page. The download option is available under the overflow menu of each Model on the “Available Models” tab. Training Live Logs are improved in terms of content and readability. Not all Embeddings present in the Models Hub are supported by NER and Assertion Status Training. These are now properly validated from the UI. Conflict when trying to use deleted embeddings. The existence of the embeddings in training as well as in deployment is ensured and a readable message is shown to users. Support for adding custom CA certificate chain. Follow the instructions described in instruction.md file present in the installation artifact. Bug fixes When multiple paged OCR file was imported using Spark OCR, the task created did not have pagination. Due to a bug in the Assertion Status script, the training was not working at all. Any AdminUser could delete the main “admin” user as well as itself. We have added proper validation to avoid such situations. Read more",
    "url": "/docs/en/alab/release_notes",
    "relUrl": "/docs/en/alab/release_notes"
  },
  "83": {
    "id": "83",
    "title": "Spark NLP release notes",
    "content": "3.3.0 John Snow Labs Spark-NLP 3.3.0: New ALBERT, XLNet, RoBERTa, XLM-RoBERTa, and Longformer for Token Classification, 50x times faster to save models, new ways to discover pretrained models and pipelines, new state-of-the-art models, and lots more! Overview We are very excited to release Spark NLP 🚀 3.3.0! This release comes with new ALBERT, XLNet, RoBERTa, XLM-RoBERTa, and Longformer existing or fine-tuned models for Token Classification on HuggingFace 🤗 , up to 50x times faster saving Spark NLP models &amp; pipelines, no more 2G limitation for the size of imported TensorFlow models, lots of new functions to filter and display pretrained models &amp; pipelines inside Spark NLP, bug fixes, and more! We are proud to say Spark NLP 3.3.0 is still compatible across all major releases of Apache Spark used locally, by all Cloud providers such as EMR, and all managed services such as Databricks. The major releases of Apache Spark include Apache Spark 3.0.x/3.1.x (spark-nlp), Apache Spark 2.4.x (spark-nlp-spark24), and Apache Spark 2.3.x (spark-nlp-spark23). As always, we would like to thank our community for their feedback, questions, and feature requests. Major features and improvements NEW: Starting Spark NLP 3.3.0 release there will be no limitation of size when you import TensorFlow models! You can now import TF Hub &amp; HuggingFace models larger than 2 Gigabytes of size. NEW: Up to 50x faster saving Spark NLP models and pipelines! We have improved the way we package TensorFlow SavedModel while saving Spark NLP models &amp; pipelines. For instance, it used to take up to 10 minutes to save the xlm_roberta_base model before Spark NLP 3.3.0, and now it only takes up to 15 seconds! NEW: Introducing AlbertForTokenClassification annotator in Spark NLP 🚀. AlbertForTokenClassification can load ALBERT Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. This annotator is compatible with all the models trained/fine-tuned by using AlbertForTokenClassification or TFAlbertForTokenClassification in HuggingFace 🤗 NEW: Introducing XlnetForTokenClassification annotator in Spark NLP 🚀. XlnetForTokenClassification can load XLNet Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. This annotator is compatible with all the models trained/fine-tuned by using XLNetForTokenClassificationet or TFXLNetForTokenClassificationet in HuggingFace 🤗 NEW: Introducing RoBertaForTokenClassification annotator in Spark NLP 🚀. RoBertaForTokenClassification can load RoBERTa Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. This annotator is compatible with all the models trained/fine-tuned by using RobertaForTokenClassification or TFRobertaForTokenClassification in HuggingFace 🤗 NEW: Introducing XlmRoBertaForTokenClassification annotator in Spark NLP 🚀. XlmRoBertaForTokenClassification can load XLM-RoBERTa Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. This annotator is compatible with all the models trained/fine-tuned by using XLMRobertaForTokenClassification or TFXLMRobertaForTokenClassification in HuggingFace 🤗 NEW: Introducing LongformerForTokenClassification annotator in Spark NLP 🚀. LongformerForTokenClassification can load Longformer Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. This annotator is compatible with all the models trained/fine-tuned by using LongformerForTokenClassification or TFLongformerForTokenClassification in HuggingFace 🤗 NEW: Introducing new ResourceDownloader functions to easily look for pretrained models &amp; pipelines inside Spark NLP (Python and Scala). You can filter models or pipelines via language, version, or the name of the annotator from sparknlp.pretrained import * # display and filter all available pretrained pipelines ResourceDownloader.showPublicPipelines() ResourceDownloader.showPublicPipelines(lang=&quot;en&quot;) ResourceDownloader.showPublicPipelines(lang=&quot;en&quot;, version=&quot;3.2.0&quot;) # display and filter all available pretrained pipelines ResourceDownloader.showPublicModels() ResourceDownloader.showPublicModels(&quot;NerDLModel&quot;, &quot;3.2.0&quot;) ResourceDownloader.showPublicModels(&quot;NerDLModel&quot;, &quot;en&quot;) ResourceDownloader.showPublicModels(&quot;XlmRoBertaEmbeddings&quot;, &quot;xx&quot;) +--+++ | Model | lang | version | +--+++ | xlm_roberta_base | xx | 3.1.0 | | twitter_xlm_roberta_base | xx | 3.1.0 | | xlm_roberta_xtreme_base | xx | 3.1.3 | | xlm_roberta_large | xx | 3.3.0 | +--+++ # remove all the downloaded models &amp; pipelines to free up storage ResourceDownloader.clearCache() # display all available annotators that can be saved as a Model ResourceDownloader.showAvailableAnnotators() Welcoming Databricks Runtime 9.1 LTS, 9.1 ML, and 9.1 ML with GPU Bug Fixes Fix a bug in RoBertaEmbeddings when all special tokens were identical Fix a bug in RoBertaEmbeddings when a special token contained valid regex Fix a bug that leads to memory leak inside NorvigSweeting spell checker. This issue caused issues with pretrained pipelines such as explain_document_ml and explain_document_dl due to some inputs Fix the wrong types being assigned to minCount and classCount in Python for ContextSpellCheckerApproach annotator Fix explain_document_ml pretrained pipeline for Spark NLP 3.x on Apache Spark 2.x Fix WordSegmenterModel wordseg_best model for Thai language Fix WordSegmenterModel wordseg_large model for Chinese language Models and Pipelines Spark NLP 3.3.0 comes with: New ALBERT, RoBERTa, XLNet, and XLM-RoBERTa for Token Classification models New XLM-RoBERTa models in Luganda, Kinyarwanda, Igbo, Hausa, and Amharic languages New Transformer Models Model Name Build Lang RoBertaForTokenClassification roberta_large_token_classifier_ontonotes 3.3.0 en RoBertaForTokenClassification roberta_large_token_classifier_conll03 3.3.0 en RoBertaForTokenClassification roberta_base_token_classifier_ontonotes 3.3.0 en RoBertaForTokenClassification roberta_base_token_classifier_conll03 3.3.0 en RoBertaForTokenClassification distilroberta_base_token_classifier_ontonotes 3.3.0 en RoBertaForTokenClassification roberta_token_classifier_zwnj_base_ner 3.3.0 fa XlmRoBertaForTokenClassification xlm_roberta_token_classifier_ner_40_lang 3.3.0 xx AlbertForTokenClassification albert_xlarge_token_classifier_conll03 3.3.0 en AlbertForTokenClassification albert_large_token_classifier_conll03 3.3.0 en AlbertForTokenClassification albert_base_token_classifier_conll03 3.3.0 en XlnetForTokenClassification xlnet_large_token_classifier_conll03 3.3.0 en XlnetForTokenClassification xlnet_base_token_classifier_conll03 3.3.0 en XlmRoBertaEmbeddings xlm_roberta_large 3.3.0 xx XlmRoBertaEmbeddings xlm_roberta_base_finetuned_luganda 3.3.0 lg XlmRoBertaEmbeddings xlm_roberta_base_finetuned_kinyarwanda 3.3.0 rw XlmRoBertaEmbeddings xlm_roberta_base_finetuned_igbo 3.3.0 ig XlmRoBertaEmbeddings xlm_roberta_base_finetuned_hausa 3.3.0 ha XlmRoBertaEmbeddings xlm_roberta_base_finetuned_amharic 3.3.0 am The complete list of all 3700+ models &amp; pipelines in 200+ languages is available on Models Hub. New Notebooks Import hundreds of models in different languages to Spark NLP Spark NLP HuggingFace Notebooks Colab AlbertForTokenClassification HuggingFace in Spark NLP - AlbertForTokenClassification RoBertaForTokenClassification HuggingFace in Spark NLP - RoBertaForTokenClassification XlmRoBertaForTokenClassification HuggingFace in Spark NLP - XlmRoBertaForTokenClassification Documentation TF Hub &amp; HuggingFace to Spark NLP Models Hub with new models Spark NLP documentation Spark NLP Scala APIs Spark NLP Python APIs Spark NLP Workshop notebooks Spark NLP publications Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.3.0 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.3.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.3.0 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.3.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.3.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.3.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.3.0 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.3.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.3.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.3.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.3.0 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.3.0.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.3.0.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.3.0.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.3.0.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.3.0.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.3.0.jar 3.2.3 John Snow Labs Spark-NLP 3.2.3: New Transformers and Training documentation, Improved GraphExtraction, new Japanese models, new multilingual Transformer models, enhancements, and bug fixes Overview We are pleased to release Spark NLP 🚀 3.2.3! This release comes with new and completed documentation for all Transformers and Trainable annotators in Spark NLP, new Japanese NER and Embeddings models, new multilingual Transformer models, code enhancements, and bug fixes. As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add delimiter feature to CoNLL() class to support other delimiters in CoNLL files https://github.com/JohnSnowLabs/spark-nlp/pull/5934 Add support for IOB in addition to IOB2 format in GraphExtraction annotator https://github.com/JohnSnowLabs/spark-nlp/pull/6101 Change YakeModel output type from KEYWORD to CHUNK to have more available features after the YakeModel annotator such as Chunk2Doc or ChunkEmbeddings https://github.com/JohnSnowLabs/spark-nlp/pull/6065 Welcoming Databricks Runtime 9.0, 9.0 ML, and 9.0 ML with GPU A new and completed Transformer page description default model’s name link to Models Hub link to notebook on Spark NLP Workshop link to Python APIs link to Scala APIs link to source code and unit test Examples in Python and Scala for Prediction Training Raw Embeddings A new and completed Training page Training Datasets Text Processing Spell Checkers Token Classification Text Classification External Trainable Models Bug Fixes &amp; Enhancements Fix the default language for XlmRoBertaSentenceEmbeddings pretrained model in Python https://github.com/JohnSnowLabs/spark-nlp/pull/6057 Fix SentenceEmbeddings issue concatenating sentences instead of each correspondent sentence https://github.com/JohnSnowLabs/spark-nlp/pull/6060 Fix GraphExctraction usage in LightPipeline https://github.com/JohnSnowLabs/spark-nlp/pull/6101 Fix compatibility issue in explain_document_ml pipeline Better import process for corrupted merges file in Longformer tokenizer https://github.com/JohnSnowLabs/spark-nlp/pull/6083 Models and Pipelines Spanish, Greek, Swedish, Dutch, German, French, Romanian, and Japanese BERT Embeddings (Word and Sentence) Model Name Build Lang BertEmbeddings bert_base_uncased_legal 3.2.2 en BertEmbeddings bert_base_uncased 3.2.2 es BertEmbeddings bert_base_cased 3.2.2 es BertEmbeddings bert_base_uncased 3.2.2 el BertEmbeddings bert_base_cased 3.2.2 sv BertEmbeddings bert_base_cased 3.2.2 nl BertSentenceEmbeddings sent_bert_base_uncased_legal 3.2.2 en BertSentenceEmbeddings sent_bert_base_uncased 3.2.2 es BertSentenceEmbeddings sent_bert_base_cased 3.2.2 es BertSentenceEmbeddings sent_bert_base_uncased 3.2.2 el BertSentenceEmbeddings sent_bert_base_cased 3.2.2 sv BertSentenceEmbeddings sent_bert_base_cased 3.2.2 nl BertSentenceEmbeddings sent_bert_base_cased 3.2.2 de Other multilingual models Model Name Build Lang WordEmbeddingsModel japanese_cc_300d 3.2.2 ja NerDLModel ner_ud_gsd_cc_300d 3.2.2 ja NerDLModel ner_ud_gsd_xlm_roberta_base 3.2.2 ja BertForTokenClassification bert_token_classifier_ner_ud_gsd 3.2.2 ja BertForTokenClassification bert_token_classifier_ner_btc 3.2.2 en ClassifierDLModel classifierdl_bert_sentiment 3.2.2 de ClassifierDLModel classifierdl_bert_sentiment 3.2.2 fr The complete list of all 3700+ models &amp; pipelines in 200+ languages is available on Models Hub. Models Hub for the community by community Serve Your Spark NLP Models for Free! You can host and share your Spark NLP models &amp; pipelines publicly with everyone to reuse them with one line of code! Models Hub is open to everyone to upload their models and pipelines, showcase their work, and share them with others. Please visit the following page for more information: https://modelshub.johnsnowlabs.com/ Documentation TF Hub &amp; HuggingFace to Spark NLP Models Hub with new models Spark NLP in Action Spark NLP documentation Spark NLP Scala APIs Spark NLP Python APIs Spark NLP Workshop notebooks Spark NLP publications Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.2.3 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.2.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.2.3 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.2.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.2.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.2.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.2.3 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.2.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.2.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.2.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.2.3 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.2.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.2.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.3&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.2.3.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.2.3.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.2.3.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.2.3.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.2.3.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.2.3.jar 3.2.2 John Snow Labs Spark-NLP 3.2.2: Models Hub for the community by the community, new RoBERTa and XLM-RoBERTa Sentence Embeddings, 40 new models in 20 languages, bug fixes, and more! Overview We are pleased to release Spark NLP 🚀 3.2.2! This release comes with accessible Models Hub to our community to host their models and pipelines for free, new RoBERTa and XLM-RoBERTa Sentence Embeddings, over 40 new models and pipelines in 20+ languages, bug fixes, and more As always, we would like to thank our community for their feedback, questions, and feature requests. New Features A new RoBertaSentenceEmbeddings annotator for sentence embeddings used in SentimentDL, ClassifierDL, and MultiClassifierDL annotators A new XlmRoBertaSentenceEmbeddings annotator for sentence embeddings used in SentimentDL, ClassifierDL, and MultiClassifierDL annotators Add support for AWS MFA via Spark NLP configuration Add new AWS configs to Spark NLP configuration when using a private S3 bucket to store logs for training models or access TF graphs needed in NerDLApproach spark.jsl.settings.aws.credentials.access_key_id spark.jsl.settings.aws.credentials.secret_access_key spark.jsl.settings.aws.credentials.session_token spark.jsl.settings.aws.s3_bucket spark.jsl.settings.aws.region Models Hub for the community, by the community Serve Your Spark NLP Models for Free! You can host and share your Spark NLP models &amp; pipelines publicly with everyone to reuse them with one line of code! We are opening Models Hub to everyone to upload their models and pipelines, showcase their work, and share them with others. Please visit the following page for more information: https://modelshub.johnsnowlabs.com/ Bug Fixes &amp; Enhancements Improve loading merges file for RoBERTa tokenizer Remove batchSize param from broadcast in XlmRoBertaEmbeddings to be set after it is created Preserve previously generated metadata in BertSentenceEmbeddings annotator Set elmo as a default poolingLayer in ElmoEmbeddings Fix special tokens ids in XlmRoBertaEmbeddings annotator Fix distilbert_base_token_classifier_ontonotes model Fix distilbert_base_token_classifier_conll03 model Fix distilbert_base_token_classifier_few_nerd model Fix distilbert_token_classifier_persian_ner model Fix ner_conll_longformer_base_4096 model Models and Pipelines Spark NLP 3.2.2 comes with new Turkish text classifier pipelines, Expert BERT Word and Sentence embeddings such as wiki books and PubMed, new BERT model for 17 Indian languages, and Sentence Detection models for 15 new languages. Pipelines Name Build Lang classifierdl_berturk_cyberbullying_pipeline 3.1.3 tr classifierdl_bert_news_pipeline 3.1.3 de classifierdl_electra_questionpair_pipeline 3.2.0 en classifierdl_bert_news_pipeline 3.2.0 tr Named Entity Recognition Model Name Build Lang NerDLModel ner_conll_elmo 3.2.2 en NerDLModel ner_conll_albert_base_uncased 3.2.2 en NerDLModel ner_conll_albert_large_uncased 3.2.2 en NerDLModel ner_conll_xlnet_base_cased 3.2.2 en BERT Embeddings Model Name Build Lang BertEmbeddings bert_muril 3.2.0 xx BertEmbeddings bert_wiki_books_sst2 3.2.0 en BertEmbeddings bert_wiki_books_squad2 3.2.0 en BertEmbeddings bert_wiki_books_qqp 3.2.0 en BertEmbeddings bert_wiki_books_qnli 3.2.0 en BertEmbeddings bert_wiki_books_mnli 3.2.0 en BertEmbeddings bert_wiki_books 3.2.0 en BertEmbeddings bert_pubmed_squad2 3.2.0 en BertEmbeddings bert_pubmed 3.2.0 en BertSentenceEmbeddings sent_bert_wiki_books_sst2 3.2.0 en BertSentenceEmbeddings sent_bert_wiki_books_squad2 3.2.0 en BertSentenceEmbeddings sent_bert_wiki_books_qqp 3.2.0 en BertSentenceEmbeddings sent_bert_wiki_books_qnli 3.2.0 en BertSentenceEmbeddings sent_bert_wiki_books_mnli 3.2.0 en BertSentenceEmbeddings sent_bert_wiki_books 3.2.0 en BertSentenceEmbeddings sent_bert_pubmed_squad2 3.2.0 en BertSentenceEmbeddings sent_bert_pubmed 3.2.0 en BertSentenceEmbeddings sent_bert_muril 3.2.0 xx Sentence Detection Yiddish, Ukrainian, Telugu, Tamil, Somali, Sindhi, Russian, Punjabi, Nepali, Marathi, Malayalam, Kannada, Indonesian, Gujrati, Bosnian Model Name Build Lang SentenceDetectorDLModel sentence_detector_dl 3.2.0 yi SentenceDetectorDLModel sentence_detector_dl 3.2.0 uk SentenceDetectorDLModel sentence_detector_dl 3.2.0 te SentenceDetectorDLModel sentence_detector_dl 3.2.0 ta SentenceDetectorDLModel sentence_detector_dl 3.2.0 so SentenceDetectorDLModel sentence_detector_dl 3.2.0 sd SentenceDetectorDLModel sentence_detector_dl 3.2.0 ru SentenceDetectorDLModel sentence_detector_dl 3.2.0 pa SentenceDetectorDLModel sentence_detector_dl 3.2.0 ne SentenceDetectorDLModel sentence_detector_dl 3.2.0 mr SentenceDetectorDLModel sentence_detector_dl 3.2.0 ml SentenceDetectorDLModel sentence_detector_dl 3.2.0 kn SentenceDetectorDLModel sentence_detector_dl 3.2.0 id SentenceDetectorDLModel sentence_detector_dl 3.2.0 gu SentenceDetectorDLModel sentence_detector_dl 3.2.0 bs The complete list of all 3700+ models &amp; pipelines in 200+ languages is available on Models Hub. Documentation TF Hub &amp; HuggingFace to Spark NLP Models Hub with new models Spark NLP in Action Spark NLP documentation Spark NLP Scala APIs Spark NLP Python APIs Spark NLP Workshop notebooks Spark NLP publications Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.2.2 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.2.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.2.2 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.2.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.2.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.2.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.2.2 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.2.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.2.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.2.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.2.2 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.2.2.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.2.2.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.2.2.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.2.2.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.2.2.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.2.2.jar 3.2.1 John Snow Labs Spark-NLP 3.2.1: Patch release Patch release Fix unsupported model error in pretrained function for LongformerEmbeddings, BertForTokenClassification, and DistilBertForTokenClassification https://github.com/JohnSnowLabs/spark-nlp/issues/5947 Documentation TF Hub &amp; HuggingFace to Spark NLP Models Hub with new models Spark NLP publications Spark NLP in Action Spark NLP documentation Spark NLP Scala APIs Spark NLP Python APIs Spark NLP Workshop notebooks Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.2.1 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.2.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.2.1 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.2.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.2.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.2.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.2.1 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.2.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.2.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.2.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.2.1 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.2.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.2.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.1&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.2.1.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.2.1.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.2.1.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.2.1.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.2.1.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.2.1.jar 3.2.0 John Snow Labs Spark-NLP 3.2.0: New Longformer embeddings, BERT and DistilBERT for Token Classification, GraphExctraction, Spark NLP Configurations, new state-of-the-art multilingual NER models, and lots more! Overview We are very excited to release Spark NLP 🚀 3.2.0! This is a big release with new Longformer models for long documents, BertForTokenClassification &amp; DistilBertForTokenClassification for existing or fine-tuned models on HuggingFace, GraphExctraction &amp; GraphFinisher to find relevant relationships between words, support for multilingual Date Matching, new Pydoc for Python APIs, and so many more! As always, we would like to thank our community for their feedback, questions, and feature requests. Major features and improvements NEW: Introducing LongformerEmbeddings annotator. Longformer is a transformer model for long documents. Longformer is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. It supports sequences of length up to 4,096. We have trained two NER models based on Longformer Base and Large embeddings: Model Accuracy F1 Test F1 Dev ner_conll_longformer_base_4096 94.75% 90.09 94.22 ner_conll_longformer_large_4096 95.79% 91.25 94.82 NEW: Introducing BertForTokenClassification annotator. BertForTokenClassification can load BERT Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. This annotator is compatible with all the models trained/fine-tuned by using BertForTokenClassification or TFBertForTokenClassification in HuggingFace 🤗 NEW: Introducing DistilBertForTokenClassification annotator. DistilBertForTokenClassification can load BERT Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. This annotator is compatible with all the models trained/fine-tuned by using DistilBertForTokenClassification or TFDistilBertForTokenClassification in HuggingFace 🤗 NEW: Introducing GraphExctraction and GraphFinisher annotators to extract a dependency graph between entities. The GraphExtraction class takes e.g. extracted entities from a NerDLModel and creates a dependency tree that describes how the entities relate to each other. For that, a triple store format is used. Nodes represent the entities and the edges represent the relations between those entities. The graph can then be used to find relevant relationships between words NEW: Introducing support for multilingual DateMatcher and MultiDateMatcher annotators. These two annotators will support English, French, Italian, Spanish, German, and Portuguese languages NEW: Introducing new Python APIs and fully documented Pydoc NEW: Introducing new Spark NLP configurations via spark.conf() by deprecating application.conf usage. You can easily change Spark NLP configurations in SparkSession. For more examples please visti Spark NLP Configuration Add support for Amazon S3 to log_folder Spark NLP config and outputLogsPath param in NerDLApproach, ClassifierDlApproach, MultiClassifierDlApproach, and SentimentDlApproach annotators Added examples to all Spark NLP Scaladoc Added examples to all Spark NLP Pydoc Welcoming new Databricks runtimes to our Spark NLP family: Databricks 8.4 ML &amp; GPU Fix printing a wrong version return in sparknlp.version() Models and Pipelines Spark NLP 3.2.0 comes with new LongformerEmbeddings, BertForTokenClassification, and DistilBertForTokenClassification annotators. New Longformer Models Model Name Build Lang LongformerEmbeddings longformer_base_4096 3.2.0 en LongformerEmbeddings longformer_large_4096 3.2.0 en Featured NerDL Models New NER models for CoNLL (4 entities) and OntoNotes (18 entities) trained by using BERT, RoBERTa, DistilBERT, XLM-RoBERTa, and Longformer Embeddings: Model Name Build Lang NerDLModel ner_ontonotes_roberta_base 3.2.0 en NerDLModel ner_ontonotes_roberta_large 3.2.0 en NerDLModel ner_ontonotes_distilbert_base_cased 3.2.0 en NerDLModel ner_conll_bert_base_cased 3.2.0 en NerDLModel ner_conll_distilbert_base_cased 3.2.0 en NerDLModel ner_conll_roberta_base 3.2.0 en NerDLModel ner_conll_roberta_large 3.2.0 en NerDLModel ner_conll_xlm_roberta_base 3.2.0 en NerDLModel ner_conll_longformer_base_4096 3.2.0 en NerDLModel ner_conll_longformer_large_4096 3.2.0 en BERT and DistilBERT for Token Classification New BERT and DistilBERT fine-tuned for the Named Entity Recognition (NER) in English, Persian, Spanish, Swedish, and Turkish: Model Name Build Lang BertForTokenClassification bert_base_token_classifier_conll03 3.2.0 en BertForTokenClassification bert_large_token_classifier_conll03 3.2.0 en BertForTokenClassification bert_base_token_classifier_ontonote 3.2.0 en BertForTokenClassification bert_large_token_classifier_ontonote 3.2.0 en BertForTokenClassification bert_token_classifier_parsbert_armanner 3.2.0 fa BertForTokenClassification bert_token_classifier_parsbert_ner 3.2.0 fa BertForTokenClassification bert_token_classifier_parsbert_peymaner 3.2.0 fa BertForTokenClassification bert_token_classifier_turkish_ner 3.2.0 tr BertForTokenClassification bert_token_classifier_spanish_ner 3.2.0 es BertForTokenClassification bert_token_classifier_swedish_ner 3.2.0 sv BertForTokenClassification bert_base_token_classifier_few_nerd 3.2.0 en DistilBertForTokenClassification distilbert_base_token_classifier_few_nerd 3.2.0 en DistilBertForTokenClassification distilbert_base_token_classifier_conll03 3.2.0 en DistilBertForTokenClassification distilbert_base_token_classifier_ontonotes 3.2.0 en DistilBertForTokenClassification distilbert_token_classifier_persian_ner 3.2.0 fa The complete list of all 3700+ models &amp; pipelines in 200+ languages is available on Models Hub. New Notebooks Import hundreds of models in different languages to Spark NLP Spark NLP HuggingFace Notebooks Colab LongformerEmbeddings HuggingFace in Spark NLP - Longformer BertForTokenClassification HuggingFace in Spark NLP - BertForTokenClassification DistilBertForTokenClassification HuggingFace in Spark NLP - DistilBertForTokenClassification You can visit Import Transformers in Spark NLP for more info New Multilingual DateMatcher and MultiDateMatcher Spark NLP Jupyter Notebooks MultiDateMatcher Date Matcehr in English MultiDateMatcher Date Matcehr in French MultiDateMatcher Date Matcehr in German MultiDateMatcher Date Matcehr in Italian MultiDateMatcher Date Matcehr in Portuguese MultiDateMatcher Date Matcehr in Spanish GraphExtraction Graph Extraction Intro GraphExtraction Graph Extraction GraphExtraction Graph Extraction Explode Entities Documentation TF Hub &amp; HuggingFace to Spark NLP Models Hub with new models Spark NLP publications Spark NLP in Action Spark NLP documentation Spark NLP Scala APIs Spark NLP Python APIs Spark NLP Workshop notebooks Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.2.0 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.2.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.2.0 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.2.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.2.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.2.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.2.0 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.2.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.2.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.2.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.2.0 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.2.0.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.2.0.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.2.0.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.2.0.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.2.0.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.2.0.jar 3.1.3 John Snow Labs Spark-NLP 3.1.3: TF Hub support, new multilingual NER models for 40 languages, state-of-the-art multilingual sentence embeddings for 100+ languages, and bug fixes! Overview We are pleased to release Spark NLP 🚀 3.1.3! In this release, we bring notebooks to easily import models for BERT and ALBERT models from TF Hub into Spark NLP, new multilingual NER models for 40 languages with a fine-tuned XLM-RoBERTa model, and new state-of-the-art document/sentence embeddings models for English and 100+ languages! As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Support BERT models from TF Hub to Spark NLP Support BERT for sentence embeddings from TF Hub to Spark NLP Support ALBERT models from TF Hub to Spark NLP Welcoming new Databricks 8.4 / 8.4 ML/GPU runtimes to Spark NLP platforms New Models We have trained multilingual NER models by using the entire XTREME (40 languages) and WIKINER (8 languages). Multilingual Named Entity Recognition: Model Name Build Lang NerDLModel ner_xtreme_xlm_roberta_xtreme_base 3.1.3 xx NerDLModel ner_xtreme_glove_840B_300 3.1.3 xx NerDLModel ner_wikiner_xlm_roberta_base 3.1.3 xx NerDLModel ner_wikiner_glove_840B_300 3.1.3 xx NerDLModel ner_mit_movie_simple_distilbert_base_cased 3.1.3 en NerDLModel ner_mit_movie_complex_distilbert_base_cased 3.1.3 en NerDLModel ner_mit_movie_complex_bert_base_cased 3.1.3 en Fine-tuned XLM-RoBERTa base model by randomly masking 15% of XTREME dataset: Model Name Build Lang XlmRoBertaEmbeddings xlm_roberta_xtreme_base 3.1.3 xx New Universal Sentence Encoder trained with CMLM (English &amp; 100+ languages): The models extend the BERT transformer architecture and that is why we use them with BertSentenceEmbeddings. Model Name Build Lang BertSentenceEmbeddings sent_bert_use_cmlm_en_base 3.1.3 en BertSentenceEmbeddings sent_bert_use_cmlm_en_large 3.1.3 en BertSentenceEmbeddings sent_bert_use_cmlm_multi_base 3.1.3 xx BertSentenceEmbeddings sent_bert_use_cmlm_multi_base_br 3.1.3 xx Benchmark We used BERT base, large, and the new Universal Sentence Encoder trained with CMLM extending the BERT transformer architecture to train ClassifierDL with News dataset: (120k training examples - 10 Epochs - 512 max sequence - Nvidia Tesla P100) Model Accuracy F1 Duration tfhub_use 0.90 0.89 10 min tfhub_use_lg 0.91 0.90 24 min sent_bert_base_cased 0.92 0.90 35 min sent_bert_large_cased 0.93 0.91 75 min sent_bert_use_cmlm_en_base 0.934 0.91 36 min sent_bert_use_cmlm_en_large 0.945 0.92 72 min The complete list of all 3700+ models &amp; pipelines in 200+ languages is available on Models Hub. Bug Fixes Fix serialization issue in NorvigSweetingModel Fix the issue with BertSentenceEmbeddings model in TF v2 Update ArrayType structure to fix Finisher failing to clean up some annotators New Notebooks Spark NLP TF Hub Notebooks BertEmbeddings TF Hub in Spark NLP - BERT BertSentenceEmbeddings TF Hub in Spark NLP - BERT Sentence AlbertEmbeddings TF Hub in Spark NLP - ALBERT Documentation HuggingFace &amp; TF Hub to Spark NLP Models Hub with new models Spark NLP publications Spark NLP in Action Spark NLP documentation Spark NLP Workshop notebooks Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.1.3 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.3 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.3 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.1.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.1.3 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.3&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.1.3.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.1.3.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.1.3.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.1.3.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.1.3.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.1.3.jar 3.1.2 John Snow Labs Spark-NLP 3.1.2: New and improved XLNet with support for external Transformers, better documentation, bug fixes, and other improvements! Overview We are pleased to release Spark NLP 🚀 3.1.2! We have a new and much-improved XLNet annotator with support for HuggingFace 🤗 models in Spark NLP. We managed to make XlnetEmbeddings almost 5x times faster on GPU compare to prior releases! As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Migrate XlnetEmbeddings to TensorFlow v2. This allows the importing of HuggingFace XLNet models to Spark NLP Migrate XlnetEmbeddings to BatchAnnotate to allow better performance on accelerated hardware such as GPU Dynamically extract special tokens from SentencePiece model in XlmRoBertaEmbeddings Add setIncludeAllConfidenceScores param in NerDLModel to merge confidence scores per label to only predicted label Fully updated Annotators page with full examples in Python and Scala Fully update Transformers page for all the transformers in Spark NLP Bug Fixes &amp; Enhancements Fix issue with SymmetricDeleteModel Fix issue with encoding unknown bytes in RoBertaEmbeddings Fix issue with multi-lingual UniversalSentenceEncoder models Sync params between Python and Scala for ContextSpellChecker change setWordMaxDist to setWordMaxDistance in Scala change setLMClasses to setLanguageModelClasses in Scala change setWordMaxDist to setWordMaxDistance in Scala change setBlackListMinFreq to setCompoundCount in Scala change setClassThreshold to setClassCount in Scala change setWeights to setWeightedDistPath in Scala change setInitialBatchSize to setBatchSize in Python Sync params between Python and Scala for ViveknSentimentApproach change setCorpusPrune to setPruneCorpus in Scala Sync params between Python and Scala for RegexMatcher change setRules to setExternalRules in Scala Sync params between Python and Scala for WordSegmenterApproach change setPosCol to setPosColumn change setIterations to setNIterations Sync params between Python and Scala for ViveknSentimentApproach change setCorpusPrune to setPruneCorpus Sync params between Python and Scala for PerceptronApproach change setPosCol to setPosColumn Fix typos in docs: https://github.com/JohnSnowLabs/spark-nlp/pull/5766 and https://github.com/JohnSnowLabs/spark-nlp/pull/5775 thanks to @brollb Performance Improvements Introducing a new batch annotation technique implemented in Spark NLP 3.1.2 for XlnetEmbeddings annotator to radically improve prediction/inferencing performance. From now on the batchSize for these annotators means the number of rows that can be fed into the models for prediction instead of sentences per row. You can control the throughput when you are on accelerated hardware such as GPU to fully utilize it. Backward compatibility We have migrated XlnetEmbeddings to TensorFlow v2, the earlier models prior to 3.1.2 won’t work after this release. We have already updated the models and uploaded them on Models Hub. You can use pretrained() that takes care of it automatically or please make sure you download the new models manually. Documentation HuggingFace to Spark NLP Models Hub with new models Spark NLP publications Spark NLP in Action Spark NLP documentation Spark NLP Workshop notebooks Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.1.2 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.2 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.2 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.1.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.1.2 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.2&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.1.2.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.1.2.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.1.2.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.1.2.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.1.2.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.1.2.jar 3.1.1 John Snow Labs Spark-NLP 3.1.1: New and improved ALBERT with support for external Transformers, real-time metrics in Python notebooks, bug fixes, and many more improvements! Overview We are pleased to release Spark NLP 🚀 3.1.1! We have a new and much-improved ALBERT annotator with support for HuggingFace 🤗 models in Spark NLP. We managed to make AlbertEmbeddings almost 7x times faster on GPU compare to prior releases! As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Migrate AlbertEmbeddings to TensorFlow v2. This allows the importing of HuggingFace ALBERT models to Spark NLP Migrate AlbertEmbeddings to BatchAnnotate to allow better performance on accelerated hardware such as GPU Enable stdout/stderr in real-time for child processes via sparknlp.start(). Thanks to PySpark 3.x, this is now possible with sparknlp.start(real_time_output=True) to have the outputs of Spark NLP (such as metrics during training) right in your Jupyter, Colab, and Kaggle notebooks. Complete examples for all annotators in Scaladoc APIs https://github.com/JohnSnowLabs/spark-nlp/pull/5668 Bug Fixes &amp; Enhancements Fix YakeModel issue with empty token https://github.com/JohnSnowLabs/spark-nlp/pull/5683 thanks to @shaddoxac Fix getAnchorDateMonth method in DateMatcher and MultiDateMatcher https://github.com/JohnSnowLabs/spark-nlp/pull/5693 Fix the broken PubTutor class in Python https://github.com/JohnSnowLabs/spark-nlp/pull/5702 Fix relative dates in DateMatcher and MultiDateMatcher such as day after tomorrow or day before yesterday https://github.com/JohnSnowLabs/spark-nlp/pull/5706 Add isPaddedToken param to PubTutor https://github.com/JohnSnowLabs/spark-nlp/pull/5702 Fix issue with logger inside session on some setup https://github.com/JohnSnowLabs/spark-nlp/pull/5715 Add signatures to TF session to handle inputs/outputs more dynamically in BertEmbeddings, DistilBertEmbeddings, RoBertaEmbeddings, and XlmRoBertaEmbeddings https://github.com/JohnSnowLabs/spark-nlp/pull/5715 Fix XlmRoBertaEmbeddings issue with init_all_tables https://github.com/JohnSnowLabs/spark-nlp/pull/5715 Add missing YakeModel from annotators Add missing random seed param to ClassifierDLApproach, MultiClassifierDLApproach, and SentimentDLApproach https://github.com/JohnSnowLabs/spark-nlp/pull/5697 Make the Java Exceptions appear before Py4J exceptions for ease of debugging in Python https://github.com/JohnSnowLabs/spark-nlp/pull/5709 Make sure batchSize set in NerDLModel is the same internally to feed TensorFlow https://github.com/JohnSnowLabs/spark-nlp/pull/5716 Fix a typo in documentation https://github.com/JohnSnowLabs/spark-nlp/pull/5664 thanks to @roger-yu-ds Performance Improvements Introducing a new batch annotation technique implemented in Spark NLP 3.1.1 for AlbertEmbeddings annotator to radically improve prediction/inferencing performance. From now on the batchSize for these annotators means the number of rows that can be fed into the models for prediction instead of sentences per row. You can control the throughput when you are on accelerated hardware such as GPU to fully utilize it. Performance achievements by using Spark NLP 2.x/3.0.x vs. Spark NLP 3.1.1 (Performed on a Databricks cluster) Spark NLP 2.x/3.0.x vs. 3.1.1 CPU GPU ALBERT Base 22% 340% Albert Large 20% 770% We will update this benchmark table in future pre-releases. Backward compatibility We have migrated AlbertEmbeddings to TensorFlow v2, the earlier models prior to 3.1.1 won’t work after this release. We have already updated the models and uploaded them on Models Hub. You can use pretrained() that takes care of it automatically or please make sure you download the new models manually. Documentation HuggingFace to Spark NLP Models Hub with new models Spark NLP publications Spark NLP in Action Spark NLP documentation Spark NLP Workshop notebooks Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.1.1 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.1 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.1 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.1.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.1.1 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.1&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.1.1.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.1.1.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.1.1.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.1.1.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.1.1.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.1.1.jar 3.1.0 John Snow Labs Spark-NLP 3.1.0: Over 2600+ new models and pipelines in 200+ languages, new DistilBERT, RoBERTa, and XLM-RoBERTa transformers, support for external Transformers, and lots more! Overview We are very excited to release Spark NLP 🚀 3.1.0! This is one of our biggest releases with lots of models, pipelines, and groundworks for future features that we are so proud to share it with our community. Spark NLP 3.1.0 comes with over 2600+ new pretrained models and pipelines in over 200+ languages, new DistilBERT, RoBERTa, and XLM-RoBERTa annotators, support for HuggingFace 🤗 (Autoencoding) models in Spark NLP, and extends support for new Databricks and EMR instances. As always, we would like to thank our community for their feedback, questions, and feature requests. Major features and improvements NEW: Introducing DistiBertEmbeddings annotator. DistilBERT is a small, fast, cheap, and light Transformer model trained by distilling BERT base. It has 40% fewer parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances NEW: Introducing RoBERTaEmbeddings annotator. RoBERTa (Robustly Optimized BERT-Pretraining Approach) models deliver state-of-the-art performance on NLP/NLU tasks and a sizable performance improvement on the GLUE benchmark. With a score of 88.5, RoBERTa reached the top position on the GLUE leaderboard NEW: Introducing XlmRoBERTaEmbeddings annotator. XLM-RoBERTa (Unsupervised Cross-lingual Representation Learning at Scale) is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data with 100 different languages. It also outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model NEW: Introducing support for HuggingFace exported models in equivalent Spark NLP annotators. Starting this release, you can easily use the saved_model feature in HuggingFace within a few lines of codes and import any BERT, DistilBERT, RoBERTa, and XLM-RoBERTa models to Spark NLP. We will work on the remaining annotators and extend this support to the rest with each release - For more information please visit this discussion NEW: Migrate MarianTransformer to BatchAnnotate to control the throughput when you are on accelerated hardware such as GPU to fully utilize it Upgrade to TensorFlow v2.4.1 with native support for Java to take advantage of many optimizations for CPU/GPU and new features/models introduced in TF v2.x Update to CUDA11 and cuDNN 8.0.2 for GPU support Implement ModelSignatureManager to automatically detect inputs, outputs, save and restore tensors from SavedModel in TF v2. This allows Spark NLP 3.1.x to extend support for external Encoders such as HuggingFace and TF Hub (coming soon!) Implement a new BPE tokenizer for RoBERTa and XLM models. This tokenizer will use the custom tokens from Tokenizer or RegexTokenizer and generates token pieces, encodes, and decodes the results Welcoming new Databricks runtimes to our Spark NLP family: Databricks 8.1 ML &amp; GPU Databricks 8.2 ML &amp; GPU Databricks 8.3 ML &amp; GPU Welcoming a new EMR 6.x series to our Spark NLP family: EMR 6.3.0 (Apache Spark 3.1.1 / Hadoop 3.2.1) Added examples to Spark NLP Scaladoc Models and Pipelines Spark NLP 3.1.0 comes with over 2600+ new pretrained models and pipelines in over 200 languages available for Windows, Linux, and macOS users. Featured Transformers: Model Name Build Lang BertEmbeddings bert_base_dutch_cased 3.1.0 nl BertEmbeddings bert_base_german_cased 3.1.0 de BertEmbeddings bert_base_german_uncased 3.1.0 de BertEmbeddings bert_base_italian_cased 3.1.0 it BertEmbeddings bert_base_italian_uncased 3.1.0 it BertEmbeddings bert_base_turkish_cased 3.1.0 tr BertEmbeddings bert_base_turkish_uncased 3.1.0 tr BertEmbeddings chinese_bert_wwm 3.1.0 zh BertEmbeddings bert_base_chinese 3.1.0 zh DistilBertEmbeddings distilbert_base_cased 3.1.0 en DistilBertEmbeddings distilbert_base_uncased 3.1.0 en DistilBertEmbeddings distilbert_base_multilingual_cased 3.1.0 xx RoBertaEmbeddings roberta_base 3.1.0 en RoBertaEmbeddings roberta_large 3.1.0 en RoBertaEmbeddings distilroberta_base 3.1.0 en XlmRoBertaEmbeddings xlm_roberta_base 3.1.0 xx XlmRoBertaEmbeddings twitter_xlm_roberta_base 3.1.0 xx Featured Translation Models: Model Name Build Lang MarianTransformer Chinese to Vietnamese 3.1.0 xx MarianTransformer Chinese to Ukrainian 3.1.0 xx MarianTransformer Chinese to Dutch 3.1.0 xx MarianTransformer Chinese to English 3.1.0 xx MarianTransformer Chinese to Finnish 3.1.0 xx MarianTransformer Chinese to Italian 3.1.0 xx MarianTransformer Yoruba to English 3.1.0 xx MarianTransformer Yapese to French 3.1.0 xx MarianTransformer Waray to Spanish 3.1.0 xx MarianTransformer Ukrainian to English 3.1.0 xx MarianTransformer Hindi to Urdu 3.1.0 xx MarianTransformer Italian to Ukrainian 3.1.0 xx MarianTransformer Italian to Icelandic 3.1.0 xx Transformers in Spark NLP: Import hundreds of models in different languages to Spark NLP Spark NLP HuggingFace Notebooks BertEmbeddings HuggingFace in Spark NLP - BERT BertSentenceEmbeddings HuggingFace in Spark NLP - BERT Sentence DistilBertEmbeddings HuggingFace in Spark NLP - DistilBERT RoBertaEmbeddings HuggingFace in Spark NLP - RoBERTa XlmRoBertaEmbeddings HuggingFace in Spark NLP - XLM-RoBERTa The complete list of all 3700+ models &amp; pipelines in 200+ languages is available on Models Hub. Backward compatibility We have updated our MarianTransformer annotator to be compatible with TF v2 models. This change is not compatible with previous models/pipelines. However, we have updated and uploaded all the models and pipelines for 3.1.x release. You can either use MarianTransformer.pretrained(MODEL_NAME) and it will automatically download the compatible model or you can visit Models Hub to download the compatible models for offline use via MarianTransformer.load(PATH) Documentation HuggingFace to Spark NLP Models Hub with new models Spark NLP publications Spark NLP in Action Spark NLP documentation Spark NLP Workshop notebooks Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.1.0 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.0 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.0 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.1.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.1.0 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.1.0.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.1.0.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.1.0.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.1.0.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.1.0.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.1.0.jar 3.0.3 John Snow Labs Spark-NLP 3.0.3: New T5 features for longer and more accurate text generation, new multi-lingual models &amp; pipelines, bug fixes, and other improvements! Overview We are glad to release Spark NLP 3.0.3! We have added some new features to our T5 Transformer annotator to help with longer and more accurate text generation, trained some new multi-lingual models and pipelines in Farsi, Hebrew, Korean, and Turkish, and fixed some bugs in this release. As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add 6 new features to T5Transformer for longer and better text generation doSample: Whether or not to use sampling; use greedy decoding otherwise temperature: The value used to module the next token probabilities topK: The number of highest probability vocabulary tokens to keep for top-k-filtering topP: If set to float &lt; 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation repetitionPenalty: The parameter for repetition penalty. 1.0 means no penalty. See CTRL: A Conditional Transformer Language Model for Controllable Generation paper for more details noRepeatNgramSize: If set to int &gt; 0, all ngrams of that size can only occur once Spark NLP 3.0.3 is compatible with the new Databricks 8.2 (ML) runtime Spark NLP 3.0.3 is compatible with the new EMR 5.33.0 (with Zeppelin 0.9.0) release Bug Fixes Fix ChunkEmbeddings Array out of bounds exception https://github.com/JohnSnowLabs/spark-nlp/pull/2796 Fix pretrained tfhub_use_multi and tfhub_use_multi_lg models in UniversalSentenceEncoder https://github.com/JohnSnowLabs/spark-nlp/pull/2827 Fix anchorDateMonth in Python that resulted in 1 additional month and case sensitivity to some relative dates like next friday or next Friday https://github.com/JohnSnowLabs/spark-nlp/pull/2848 Models and Pipelines New multilingual models and pipelines for Farsi, Hebrew, Korean, and Turkish Model Name Build Lang ClassifierDLModel classifierdl_bert_news 3.0.2 tr UniversalSentenceEncoder tfhub_use_multi 3.0.0 xx UniversalSentenceEncoder tfhub_use_multi_lg 3.0.0 xx Pipeline Name Build Lang PretrainedPipeline recognize_entities_dl 3.0.0 fa PretrainedPipeline explain_document_lg 3.0.2 he PretrainedPipeline explain_document_lg 3.0.2 ko The complete list of all 1100+ models &amp; pipelines in 192+ languages is available on Models Hub. Documentation and Notebooks Add a new Offline section to docs Installing Spark NLP and Spark OCR in air-gapped networks (offline mode) Models Hub with new models Spark NLP publications Spark NLP in Action Spark NLP documentation Spark NLP Workshop notebooks Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.0.3 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.3 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.3 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.0.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.0.3 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.3&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.0.3.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.0.3.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.0.3.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.0.3.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.0.3.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.0.3.jar 3.0.2 John Snow Labs Spark-NLP 3.0.2: New multilingual models, confidence scores for entities and all NER tags, first support for community models, bug fixes, and other improvements! Overview We are glad to release Spark NLP 3.0.2! We have added some new features, improvements, trained some new multi-lingual models, and fixed some bugs in this release. As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Experimental support for community models and pipelines (uploaded by users) https://github.com/JohnSnowLabs/spark-nlp/pull/2743 Provide confidence scores for all available tags in NerDLModel and NerCrfModel https://github.com/JohnSnowLabs/spark-nlp/pull/2760 # NerDLModel and NerCrfModel before 3.0.2 [[named_entity, 0, 4, B-LOC, [word -&gt; Japan, confidence -&gt; 0.9998], []] # Now in Spark NLP 3.0.2 [[named_entity, 0, 4, B-LOC, [B-LOC -&gt; 0.9998, I-ORG -&gt; 0.0, I-MISC -&gt; 0.0, I-LOC -&gt; 0.0, I-PER -&gt; 0.0, B-MISC -&gt; 0.0, B-ORG -&gt; 1.0E-4, word -&gt; Japan, O -&gt; 0.0, B-PER -&gt; 0.0], []] Calculate confidence score for entities in NerConverter https://github.com/JohnSnowLabs/spark-nlp/pull/2784 [chunk, 30, 41, Barack Obama, [entity -&gt; PERSON, sentence -&gt; 0, chunk -&gt; 0, confidence -&gt; 0.94035] Enhancements Add proper conversions for Scala 2.11/2.12 in ContextSpellChecker to use models from Spark 2.x in Spark 3.x https://github.com/JohnSnowLabs/spark-nlp/pull/2758 Refactoring SentencePiece encoding in AlbertEmbeddings and XlnetEmbeddings https://github.com/JohnSnowLabs/spark-nlp/pull/2777 Bug Fixes Fix an exception in NerConverter when the documents/sentences don’t carry the used tokens in NerDLModel https://github.com/JohnSnowLabs/spark-nlp/pull/2784 thanks to @rahulraina7 Fix an exception in AlbertEmbeddings when the original tokens are longer than the piece tokens https://github.com/JohnSnowLabs/spark-nlp/pull/2777 Models and Pipelines New multilingual models for Afrikaans, Welsh, Maltese, Tamil, and Vietnamese Model Name Build Lang PerceptronModel pos_afribooms 3.0.0 af LemmatizerModel lemma 3.0.0 cy LemmatizerModel lemma 3.0.0 mt LemmatizerModel lemma 3.0.0 af LemmatizerModel lemma 3.0.0 ta LemmatizerModel lemma 3.0.0 vi The complete list of all 1100+ models &amp; pipelines in 192+ languages is available on Models Hub. Documentation and Notebooks Add a new Offline section to docs Models Hub with new models Spark NLP publications Spark NLP in Action Spark NLP documentation Spark NLP Workshop notebooks Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.0.2 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.2 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.2 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.0.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.0.2 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.0.2.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.0.2.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.0.2.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.0.2.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.0.2.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.0.2.jar 3.0.1 John Snow Labs Spark-NLP 3.0.1: New parameters in Normalizer, bug fixes and other improvements! Overview We are glad to release Spark NLP 3.0.1! We have made some improvements, added 1 line bash script to set up Google Colab and Kaggle kernel for Spark NLP 3.x, and improved our Models Hub filtering to help our community to have easier access to over 1300 pretrained models and pipelines in over 200+ languages. As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add minLength and maxLength parameters to Normalizer annotator https://github.com/JohnSnowLabs/spark-nlp/pull/2614 1 line to setup Google Colab 1 line to setup Kaggle Kernel Enhancements Adjust shading rule for amazon AWS to support sub-projects from Spark NLP Fat JAR https://github.com/JohnSnowLabs/spark-nlp/pull/2613 Fix the missing variables in BertSentenceEmbeddings https://github.com/JohnSnowLabs/spark-nlp/pull/2615 Restrict loading Sentencepiece ops only to supported models https://github.com/JohnSnowLabs/spark-nlp/pull/2623 improve dependency management and resolvers https://github.com/JohnSnowLabs/spark-nlp/pull/2479 Documentation and Notebooks Models Hub with new models Spark NLP publications Spark NLP in Action Spark NLP documentation Spark NLP Workshop notebooks Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.0.1 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.1 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.1 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.0.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.0.1 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.0.1.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.0.1.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.0.1.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.0.1.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.0.1.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.0.1.jar 3.0.0 John Snow Labs Spark-NLP 3.0.0: Supporting Spark 3.x, Scala 2.12, more Databricks runtimes, more EMR versions, performance improvements &amp; lots more Overview We are very excited to release Spark NLP 3.0.0! This has been one of the biggest releases we have ever done and we are so proud to share this with our community. Spark NLP 3.0.0 extends the support for Apache Spark 3.0.x and 3.1.x major releases on Scala 2.12 with both Hadoop 2.7. and 3.2. We will support all 4 major Apache Spark and PySpark releases of 2.3.x, 2.4.x, 3.0.x, and 3.1.x helping the community to migrate from earlier Apache Spark versions to newer releases without being worried about Spark NLP support. As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Support for Apache Spark and PySpark 3.0.x on Scala 2.12 Support for Apache Spark and PySpark 3.1.x on Scala 2.12 Migrate to TensorFlow v2.3.1 with native support for Java to take advantage of many optimizations for CPU/GPU and new features/models introduced in TF v2.x Welcoming 9x new Databricks runtimes to our Spark NLP family: Databricks 7.3 Databricks 7.3 ML GPU Databricks 7.4 Databricks 7.4 ML GPU Databricks 7.5 Databricks 7.5 ML GPU Databricks 7.6 Databricks 7.6 ML GPU Databricks 8.0 Databricks 8.0 ML (there is no GPU in 8.0) Databricks 8.1 Beta Welcoming 2x new EMR 6.x series to our Spark NLP family: EMR 6.1.0 (Apache Spark 3.0.0 / Hadoop 3.2.1) EMR 6.2.0 (Apache Spark 3.0.1 / Hadoop 3.2.1) Starting Spark NLP 3.0.0 the default packages for CPU and GPU will be based on Apache Spark 3.x and Scala 2.12 (spark-nlp and spark-nlp-gpu will be compatible only with Apache Spark 3.x and Scala 2.12) Starting Spark NLP 3.0.0 we have two new packages to support Apache Spark 2.4.x and Scala 2.11 (spark-nlp-spark24 and spark-nlp-gpu-spark24) Spark NLP 3.0.0 still is and will be compatible with Apache Spark 2.3.x and Scala 2.11 (spark-nlp-spark23 and spark-nlp-gpu-spark23) Adding a new param to sparknlp.start() function in Python for Apache Spark 2.4.x (spark24=True) Adding a new param to adjust Driver memory in sparknlp.start() function (memory=&quot;16G&quot;) Performance Improvements Introducing a new batch annotation technique implemented in Spark NLP 3.0.0 for NerDLModel, BertEmbeddings, and BertSentenceEmbeddings annotators to radically improve prediction/inferencing performance. From now on the batchSize for these annotators means the number of rows that can be fed into the models for prediction instead of sentences per row. You can control the throughput when you are on accelerated hardware such as GPU to fully utilize it. Performance achievements by using Spark NLP 3.0.0 vs. Spark NLP 2.7.x on CPU and GPU: (Performed on a Databricks cluster) Spark NLP 3.0.0 vs. 2.7.x PySpark 3.x on CPU PySpark 3.x on GPU BertEmbeddings (bert-base) +10% +550% (6.6x) BertEmbeddings (bert-large) +12%. +690% (7.9x) NerDLModel +185% +327% (4.2x) Breaking changes There are only 6 annotators that are not compatible to be used with both Scala 2.11 (Apache Spark 2.3 and Apache Spark 2.4) and Scala 2.12 (Apache Spark 3.x) at the same time. You can either train and use them on Apache Spark 2.3.x/2.4.x or train and use them on Apache Spark 3.x. TokenizerModel PerceptronApproach (POS Tagger) WordSegmenter DependencyParser TypedDependencyParser NerCrfModel The rest of our models/pipelines can be used on all Apache Spark and Scala major versions without any issue. We have already retrained and uploaded all the exiting pretrained for Part of Speech and WordSegmenter models in Apache Spark 3.x and Scala 2.12. We will continue doing this as we see existing models which are not compatible with Apache Spark 3.x and Scala 2.12. NOTE: You can always use the .pretrained() function which seamlessly will find the compatible and most recent models to download for you. It will download and extract them in your HOME DIRECTORY ~/cached_pretrained/. More info: https://github.com/JohnSnowLabs/spark-nlp/discussions/2562 Deprecated Starting Spark NLP 3.0.0 release we no longer publish any artifacts on spark-packages and we continue to host all the artifacts only Maven Repository. Documentation Apache Spark Migration Guide PySpark Migration Guide “Spark NLP: Natural language understanding at scale” published paper Spark NLP publications Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Spark NLP Workshop notebooks Models Hub with new models Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.0.0 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.0 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.0 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.0.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.0.0 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.0.0.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.0.0.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.0.0.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.0.0.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.0.0.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-gpu-assembly-3.0.0.jar 2.7.5 John Snow Labs Spark-NLP 2.7.5: Supporting more EMR versions and other improvements! Overview We are glad to release Spark NLP 2.7.5 release! Starting this release we no longer ship Hadoop AWS and AWS Java SDK dependencies. This change allows users to avoid any conflicts in AWS environments and also results in more EMR 5.x versions support. As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Support more EMR 5.x versions emr-5.20.0 emr-5.21.0 emr-5.21.1 emr-5.22.0 emr-5.23.0 emr-5.24.0 emr-5.24.1 emr-5.25.0 emr-5.26.0 emr-5.27.0 emr-5.28.0 emr-5.29.0 emr-5.30.0 emr-5.30.1 emr-5.31.0 emr-5.32.0 Bugfixes Fix BigDecimal error in NerDL when includeConfidence is true Enhancements Shade Hadoop AWS and AWS Java SDK dependencies Documentation and Notebooks “Spark NLP: Natural language understanding at scale” published paper Spark NLP publications Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Spark NLP Workshop notebooks Models Hub with new models New Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==2.7.5 #Conda conda install -c johnsnowlabs spark-nlp==2.7.5 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.5 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.5 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.5 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.5 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-2.7.5.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-2.7.5.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-2.7.5.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-gpu-assembly-2.7.5.jar 2.7.4 John Snow Labs Spark-NLP 2.7.4: New Bengali NER and Word Embeddings models, new Intent Prediction models, bug fixes, and other improvements! Overview We are glad to release Spark NLP 2.7.4 release! This release comes with a few bug fixes, enhancements, and 4 new pretrained models. As always, we would like to thank our community for their feedback, questions, and feature requests. Bugfixes Fix Tensors with a 0 dimension issue in ClassifierDL and SentimentDL thanks to @pradeepgowda21 https://github.com/JohnSnowLabs/spark-nlp/pull/2288 Fix index error in TokenAssembler https://github.com/JohnSnowLabs/spark-nlp/pull/2289 Fix MatchError in DateMatcher and MultiDateMatcher annotators https://github.com/JohnSnowLabs/spark-nlp/pull/2297 Fix setOutputAsArray and its default value for valueSplitSymbol in Finisher annotator https://github.com/JohnSnowLabs/spark-nlp/pull/2290 Enhancements Implement missing frequencyThreshold and ambiguityThreshold params in WordSegmenterApproach annotator https://github.com/JohnSnowLabs/spark-nlp/pull/2308 Downgrade Hadoop from 3.2 to 2.7 which caused an issue with S3 https://github.com/JohnSnowLabs/spark-nlp/pull/2310 Update Apache HTTP Client https://github.com/JohnSnowLabs/spark-nlp/pull/2312 Models and Pipelines Model Name Build Lang NerDLModel bengali_cc_300d 2.7.3 bn WordEmbeddingsModel bengaliner_cc_300d 2.7.3 bn NerDLModel nerdl_snips_100d 2.7.3 en ClassifierDLModel classifierdl_use_snips 2.7.3 en The complete list of all 1100+ models &amp; pipelines in 192+ languages is available on Models Hub. Compatibility Starting today, we have moved all of the Fat JARs hosted on our S3 to the auxdata.johnsnowlabs.com/public/jars/ location. We have also fixed the links in the previous releases. Documentation and Notebooks “Spark NLP: Natural language understanding at scale” published paper Spark NLP publications Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Spark NLP Workshop notebooks Models Hub with new models New Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==2.7.4 #Conda conda install -c johnsnowlabs spark-nlp==2.7.4 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.4 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.4 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.4 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.4 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-2.7.4.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-2.7.4.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-2.7.4.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-gpu-assembly-2.7.4.jar 2.7.3 John Snow Labs Spark-NLP 2.7.3: 18 new state-of-the-art transformer-based OntoNotes models and pipelines, new support for Bengali NER and Hindi Word Embeddings, and other improvements! Overview We are glad to release Spark NLP 2.7.3 release! This release comes with a couple of bug fixes, enhancements, and 20+ pretrained models and pipelines including support for Bengali Named Entity Recognition, Hindi Word Embeddings, and state-of-the-art transformer based OntoNotes models and pipelines! As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add anchorDateYear, anchorDateMonth, and anchorDateDay to DateMatcher and MultiDateMatcher to be used for relative dates extraction Bugfixes Fix the default value for action parameter in Python wrapper for DocumentNormalizer annotator Fix Lemmatizer pretrained models published in 2021 Enhancements Improve T5Transformer performance on documents with many sentences Models and Pipelines This release comes with support for Bengali Named Entity Recognition and Hindi Word Embeddings. We are also announcing the release of 18 new state-of-the-art transformer based OntoNotes models and pipelines! These models are trained by using Transformers pretrained models such as BERT Tiny, BERT Mini, BERT Small, BERT Medium, BERT Base, BERT Large, ELECTRA Small, ELECTRA Base, and ELECTRA Large. New Bengali and Hindi Models: Model Name Build Lang NerDLModel ner_jifs_glove_840B_300d 2.7.0 bn WordEmbeddingsModel hindi_cc_300d 2.7.0 hi New Transformer-based OntoNotes Models &amp; Pipelines: Model Name Build Lang NerDLModel onto_small_bert_L2_128 2.7.0 en NerDLModel onto_small_bert_L4_256 2.7.0 en NerDLModel onto_small_bert_L4_512 2.7.0 en NerDLModel onto_small_bert_L8_512 2.7.0 en NerDLModel onto_bert_base_cased 2.7.0 en NerDLModel onto_bert_large_cased 2.7.0 en NerDLModel onto_electra_small_uncased 2.7.0 en NerDLModel onto_electra_base_uncased 2.7.0 en NerDLModel onto_electra_large_uncased 2.7.0 en Pipeline Build Lang onto_recognize_entities_bert_tiny 2.7.0 en onto_recognize_entities_bert_mini 2.7.0 en onto_recognize_entities_bert_small 2.7.0 en onto_recognize_entities_bert_medium 2.7.0 en onto_recognize_entities_bert_base 2.7.0 en onto_recognize_entities_bert_large 2.7.0 en onto_recognize_entities_electra_small 2.7.0 en onto_recognize_entities_electra_base 2.7.0 en onto_recognize_entities_electra_large 2.7.0 en OntoNotes Benchmark: SYSTEM YEAR LANGUAGE ONTONOTES Spark NLP v2.7 2021 Python/Scala/Java/R 90.0 (test F1) 92.5 (dev F1) spaCy 3.0 (RoBERTa) 2020 Python 89.7 (dev F1) Stanza (StanfordNLP) 2020 Python 88.8 (dev F1) Flair 2018 Python 89.7 Documentation and Notebooks “Spark NLP: Natural language understanding at scale” published paper Spark NLP publications Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Spark NLP Workshop notebooks Models Hub with new models New Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==2.7.3 #Conda conda install -c johnsnowlabs spark-nlp==2.7.3 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.3 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.3 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-2.7.3.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-2.7.3.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-2.7.3.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-gpu-assembly-2.7.3.jar 2.7.2 John Snow Labs Spark-NLP 2.7.2: New multilingual models, GPU support to train a Spell Checker, bug fixes, and other improvements! Overview We are glad to release Spark NLP 2.7.2 release! This release comes with a couple of bug fixes, enhancements, and 25+ pretrained models and pipelines in Amharic, Bengali, Bhojpuri, Japanese, and Korean languages. As always, we would like to thank our community for their feedback, questions, and feature requests. Bugfixes Fix casual mask calculations resulting in bad translation in MarianTransformer https://github.com/JohnSnowLabs/spark-nlp/pull/2149 Fix Serialization issue in the cluster while training ContextSpellChecker https://github.com/JohnSnowLabs/spark-nlp/pull/2167 Fix calculating CHUNK spans based on the sentences’ boundaries in RegexMatcher https://github.com/JohnSnowLabs/spark-nlp/pull/2150 Enhancements Add GPU support for training ContextSpellChecker https://github.com/JohnSnowLabs/spark-nlp/pull/2167 Adding Scalatest ability to control tests by tags https://github.com/JohnSnowLabs/spark-nlp/pull/2156 Models and Pipelines The 2.7.x release comes with over 720+ new pretrained models and pipelines available for Windows, Linux, and macOS users. New Text Classifier models: Model Name Build Lang SentimentDLModel sentimentdl_glove_imdb 2.7.1 en SentimentDLModel sentimentdl_use_imdb 2.7.1 en SentimentDLModel sentimentdl_use_twitter 2.7.1 en ClassifierDLMode classifierdl_use_spam 2.7.1 en ClassifierDLModel classifierdl_use_sarcasm 2.7.1 en ClassifierDLModel classifierdl_use_fakenews 2.7.1 en ClassifierDLModel classifierdl_use_emotion 2.7.1 en ClassifierDLModel classifierdl_use_cyberbullying 2.7.1 en MultiClassifierDLModel multiclassifierdl_use_toxic_sm 2.7.1 en MultiClassifierDLModel multiclassifierdl_use_toxic 2.7.1 en MultiClassifierDLModel multiclassifierdl_use_e2e 2.7.1 en New Multi-lingual models: Some of the new models for Amharic, Bengali, Bhojpuri, Japanese, and Korean languages Model Name Build Lang SentimentDLModel sentiment_jager_use 2.7.1 th SentimentDLModel sentimentdl_urduvec_imdb 2.7.1 ur LemmatizerModel lemma 2.7.0 am LemmatizerModel lemma 2.7.0 bn LemmatizerModel lemma 2.7.0 bh LemmatizerModel lemma 2.7.0 ja LemmatizerModel lemma 2.7.0 ko PerceptronModel pos_ud_att 2.7.0 am PerceptronModel pos_ud_bhtb 2.7.0 bh PerceptronModel pos_msri 2.7.0 bn PerceptronModel pos_lst20 2.7.0 th WordSegmenterModel wordseg_best 2.7.0 th NerDLModel ner_lst20_glove_840B_300d 2.7.0 th The complete list of all 1100+ models &amp; pipelines in 192+ languages is available on Models Hub. Documentation and Notebooks Spark NLP publications Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Spark NLP Workshop notebooks Models Hub with new models New Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==2.7.2 #Conda conda install -c johnsnowlabs spark-nlp==2.7.2 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.2 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.2 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-2.7.2.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-2.7.2.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-2.7.2.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-gpu-assembly-2.7.2.jar 2.7.1 John Snow Labs Spark-NLP 2.7.1: New T5 models, new TREC pipelines, bug fixes, and other improvements! Overview We are glad to release Spark NLP 2.7.1 towards making 2.7 stable release! This release comes with 3 new optimized T5 models, 2 new TREC pipelines, a few bug fixes, and other improvements. We highly recommend all users to upgrade to 2.7.1 for more stability while paying attention to the backward compatibility notice. As always, we would like to thank our community for their feedback, questions, and feature requests. Bugfixes Fix default pretrained model T5Transformer https://github.com/JohnSnowLabs/spark-nlp/pull/2068 Fix default pretrained model WordSegmenter https://github.com/JohnSnowLabs/spark-nlp/pull/2068 Fix missing reference to WordSegmenter in ResourceDwonloader https://github.com/JohnSnowLabs/spark-nlp/pull/2068 Fix T5Transformer models crashing due to unknown task https://github.com/JohnSnowLabs/spark-nlp/pull/2070 Fix the issue of reading and writing ClassifierDL, SentimentDL, and MultiClassifierDL models introduced in the 2.7.0 release https://github.com/JohnSnowLabs/spark-nlp/pull/2081 Enhancements Export new T5 models with optimized Encoder/Decoder https://github.com/JohnSnowLabs/spark-nlp/pull/2074 Add support for alternative tagging with the positional parser in RegexTokenizer https://github.com/JohnSnowLabs/spark-nlp/pull/2077 Refactor AssertAnnotations https://github.com/JohnSnowLabs/spark-nlp/pull/2079 Backward compatibility In order to fix the issue of Classifiers in the clusters, we had to export new TF models and change the read/write functions of these annotators. This caused any model trained prior to the 2.7.0 release not to be compatible with 2.7.1 and require retraining including pre-trained models. (we are re-training all the existing text classification models with 2.7.1) Models and Pipelines The 2.7.x release comes with over 720+ new pretrained models and pipelines available for Windows, Linux, and macOS users. New optimized T5 models: Model Name Build Lang T5Transformer t5_small 2.7.1 en T5Transformer t5_base 2.7.1 en T5Transformer google_t5_small_ssm_nq 2.7.1 en Question classification of open-domain and fact-based questions: Model Name Build Lang ClassifierDL classifierdl_use_trec6 2.7.1 en ClassifierDL classifierdl_use_trec50 2.7.1 en ClassifierDL classifierdl_use_trec6_pipeline 2.7.1 en ClassifierDL classifierdl_use_trec50_pipeline 2.7.1 en The complete list of all 1100+ models &amp; pipelines in 192+ languages is available on Models Hub. Documentation and Notebooks New RegexTokenizer Notebook New T5 Summarization &amp; Question Answering Notebook New T5 Translation Notebook New Marian Translation Notebook Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Update the entire spark-nlp-workshop notebooks Update Models Hub with new models New Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==2.7.1 #Conda conda install -c johnsnowlabs spark-nlp==2.7.1 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.1 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.1 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-2.7.1.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-2.7.1.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-2.7.1.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-gpu-assembly-2.7.1.jar 2.7.0 John Snow Labs Spark-NLP 2.7.0: New T5 and MarianMT seq2seq transformers, detect up to 375 languages, word segmentation, over 720+ models and pipelines, support for 192+ languages, and many more! Overview We are very excited to release Spark NLP 2.7.0! This has been one of the biggest releases we have ever done that we are so proud to share it with our community! In this release, we are bringing support to state-of-the-art Seq2Seq and Text2Text transformers. We have developed annotators for Google T5 (Text-To-Text Transfer Transformer) and MarianMNT for Neural Machine Translation with over 646 pretrained models and pipelines. This release also comes with a refactored and brand new models for language detection and identification. They are more accurate, faster, and support up to 375 languages. The 2.7.0 release has over 720+ new pretrained models and pipelines while extending our support of multi-lingual models to 192+ languages such as Chinese, Japanese, Korean, Arabic, Persian, Urdu, and Hebrew. As always, we would like to thank our community for their feedback and support. Major features and improvements NEW: Introducing MarianTransformer annotator for machine translation based on MarianNMT models. Marian is an efficient, free Neural Machine Translation framework mainly being developed by the Microsoft Translator team (646+ pretrained models &amp; pipelines in 192+ languages) NEW: Introducing T5Transformer annotator for Text-To-Text Transfer Transformer (Google T5) models to achieve state-of-the-art results on multiple NLP tasks such as Translation, Summarization, Question Answering, Sentence Similarity, and so on NEW: Introducing brand new and refactored language detection and identification models. The new LanguageDetectorDL is faster, more accurate, and supports up to 375 languages NEW: Introducing WordSegmenter annotator, a trainable annotator for word segmentation of languages without any rule-based tokenization such as Chinese, Japanese, or Korean NEW: Introducing DocumentNormalizer annotator cleaning content from HTML or XML documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters NEW: Spark NLP Display for visualization of different types of annotations Add support for new multi-lingual models in UniversalSentenceEncoder annotator Add support to Lemmatizer to be trained directly from a DataFrame instead of a text file Add training helper to transform CoNLL-U into Spark NLP annotator type columns Bugfixes and Enhancements Fix all the known issues in ClassifierDL, SentimentDL, and MultiClassifierDL annotators in a Cluster NerDL enhancements for memory optimization and logging during the training with the test dataset SentenceEmbeddings annotator now reuses the storageRef of any embeddings used in prior Fix dropout in SentenceDetectorDL models for more deterministic results. Both English and Multi-lingual models are retrained for the 2.7.0 release Fix Python dataType Annotation Upgrade to Apache Spark 2.4.7 Models and Pipelines The 2.7.0 release comes with over 720+ new pretrained models and pipelines available for Windows, Linux, and macOS users. Selected T5 and Marian models Model Name Build Lang T5Transformer google_t5_small_ssm_nq 2.7.0 en T5Transformer t5_small 2.7.0 en MarianTransformer opus-mt-en-aav 2.7.0 xx MarianTransformer opus-mt-en-af 2.7.0 xx MarianTransformer opus-mt-en-afa 2.7.0 xx MarianTransformer opus-mt-en-alv 2.7.0 xx MarianTransformer opus-mt-en-ar 2.7.0 xx MarianTransformer opus-mt-en-az 2.7.0 xx MarianTransformer opus-mt-en-bat 2.7.0 xx MarianTransformer opus-mt-en-bcl 2.7.0 xx MarianTransformer opus-mt-en-bem 2.7.0 xx MarianTransformer opus-mt-en-ber 2.7.0 xx MarianTransformer opus-mt-en-bg 2.7.0 xx MarianTransformer opus-mt-en-bi 2.7.0 xx MarianTransformer opus-mt-en-bnt 2.7.0 xx MarianTransformer opus-mt-en-bzs 2.7.0 xx MarianTransformer opus-mt-en-ca 2.7.0 xx MarianTransformer opus-mt-en-ceb 2.7.0 xx MarianTransformer opus-mt-en-cel 2.7.0 xx MarianTransformer opus-mt-en-chk 2.7.0 xx MarianTransformer opus-mt-en-cpf 2.7.0 xx MarianTransformer opus-mt-en-cpp 2.7.0 xx MarianTransformer opus-mt-en-crs 2.7.0 xx MarianTransformer opus-mt-en-cs 2.7.0 xx MarianTransformer opus-mt-en-cus 2.7.0 xx MarianTransformer opus-mt-en-cy 2.7.0 xx MarianTransformer opus-mt-en-da 2.7.0 xx MarianTransformer opus-mt-en-de 2.7.0 xx MarianTransformer opus-mt-en-dra 2.7.0 xx MarianTransformer opus-mt-en-ee 2.7.0 xx MarianTransformer opus-mt-en-efi 2.7.0 xx MarianTransformer opus-mt-en-el 2.7.0 xx MarianTransformer opus-mt-en-eo 2.7.0 xx MarianTransformer opus-mt-en-es 2.7.0 xx MarianTransformer opus-mt-en-et 2.7.0 xx MarianTransformer opus-mt-en-eu 2.7.0 xx MarianTransformer opus-mt-en-euq 2.7.0 xx MarianTransformer opus-mt-en-fi 2.7.0 xx MarianTransformer opus-mt-en-fiu 2.7.0 xx MarianTransformer opus-mt-en-fj 2.7.0 xx MarianTransformer opus-mt-en-fr 2.7.0 xx MarianTransformer opus-mt-en-ga 2.7.0 xx MarianTransformer opus-mt-en-gaa 2.7.0 xx MarianTransformer opus-mt-en-gem 2.7.0 xx MarianTransformer opus-mt-en-gil 2.7.0 xx MarianTransformer opus-mt-en-gl 2.7.0 xx MarianTransformer opus-mt-en-gmq 2.7.0 xx MarianTransformer opus-mt-en-gmw 2.7.0 xx MarianTransformer opus-mt-en-grk 2.7.0 xx MarianTransformer opus-mt-en-guw 2.7.0 xx MarianTransformer opus-mt-en-gv 2.7.0 xx MarianTransformer opus-mt-en-ha 2.7.0 xx MarianTransformer opus-mt-en-he 2.7.0 xx MarianTransformer opus-mt-en-hi 2.7.0 xx MarianTransformer opus-mt-en-hil 2.7.0 xx MarianTransformer opus-mt-en-ho 2.7.0 xx MarianTransformer opus-mt-en-ht 2.7.0 xx MarianTransformer opus-mt-en-hu 2.7.0 xx MarianTransformer opus-mt-en-hy 2.7.0 xx MarianTransformer opus-mt-en-id 2.7.0 xx MarianTransformer opus-mt-en-ig 2.7.0 xx MarianTransformer opus-mt-en-iir 2.7.0 xx MarianTransformer opus-mt-en-ilo 2.7.0 xx MarianTransformer opus-mt-en-inc 2.7.0 xx MarianTransformer opus-mt-en-ine 2.7.0 xx MarianTransformer opus-mt-en-is 2.7.0 xx MarianTransformer opus-mt-en-iso 2.7.0 xx MarianTransformer opus-mt-en-it 2.7.0 xx MarianTransformer opus-mt-en-itc 2.7.0 xx MarianTransformer opus-mt-en-jap 2.7.0 xx MarianTransformer opus-mt-en-kg 2.7.0 xx MarianTransformer opus-mt-en-kj 2.7.0 xx MarianTransformer opus-mt-en-kqn 2.7.0 xx MarianTransformer opus-mt-en-kwn 2.7.0 xx MarianTransformer opus-mt-en-kwy 2.7.0 xx MarianTransformer opus-mt-en-lg 2.7.0 xx MarianTransformer opus-mt-en-ln 2.7.0 xx MarianTransformer opus-mt-en-loz 2.7.0 xx MarianTransformer opus-mt-en-lu 2.7.0 xx MarianTransformer opus-mt-en-lua 2.7.0 xx MarianTransformer opus-mt-en-lue 2.7.0 xx MarianTransformer opus-mt-en-lun 2.7.0 xx MarianTransformer opus-mt-en-luo 2.7.0 xx MarianTransformer opus-mt-en-lus 2.7.0 xx MarianTransformer opus-mt-en-map 2.7.0 xx MarianTransformer opus-mt-en-mfe 2.7.0 xx MarianTransformer opus-mt-en-mg 2.7.0 xx MarianTransformer opus-mt-en-mh 2.7.0 xx MarianTransformer opus-mt-en-mk 2.7.0 xx MarianTransformer opus-mt-en-mkh 2.7.0 xx MarianTransformer opus-mt-en-ml 2.7.0 xx MarianTransformer opus-mt-en-mos 2.7.0 xx MarianTransformer opus-mt-en-mr 2.7.0 xx MarianTransformer opus-mt-en-mt 2.7.0 xx MarianTransformer opus-mt-en-mul 2.7.0 xx MarianTransformer opus-mt-en-ng 2.7.0 xx MarianTransformer opus-mt-en-nic 2.7.0 xx MarianTransformer opus-mt-en-niu 2.7.0 xx MarianTransformer opus-mt-en-nl 2.7.0 xx MarianTransformer opus-mt-en-nso 2.7.0 xx MarianTransformer opus-mt-en-ny 2.7.0 xx MarianTransformer opus-mt-en-nyk 2.7.0 xx Chinese models Models Name Build Lang WordSegmenterModel wordseg_weibo 2.7.0 zh WordSegmenterModel wordseg_pku 2.7.0 zh WordSegmenterModel wordseg_msra 2.7.0 zh WordSegmenterModel wordseg_large 2.7.0 zh WordSegmenterModel wordseg_ctb9 2.7.0 zh PerceptronModel pos_ud_gsd 2.7.0 zh PerceptronModel pos_ctb9 2.7.0 zh NerDLModel ner_msra_bert_768d 2.7.0 zh NerDLModel ner_weibo_bert_768d 2.7.0 zh Arabic models Models Name Build Lang StopWordsCleaner stopwords_ar 2.7.0 ar LemmatizerModel lemma 2.7.0 ar PerceptronModel pos_ud_padt 2.7.0 ar WordEmbeddingsModel arabic_w2v_cc_300d 2.7.0 ar NerDLModel aner_cc_300d 2.7.0 ar Persian models Models Name Build Lang StopWordsCleaner stopwords_fa 2.7.0 fa LemmatizerModel lemma 2.7.0 fa PerceptronModel pos_ud_perdt 2.7.0 fa WordEmbeddingsModel persian_w2v_cc_300d 2.7.0 fa NerDLModel personer_cc_300d 2.7.0 fa The complete list of all 1100+ models &amp; pipelines in 192+ languages is available on Models Hub. Documentation and Notebooks Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Update the entire spark-nlp-workshop notebooks Update Models Hub with new models New Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==2.7.0 #Conda conda install -c johnsnowlabs spark-nlp==2.7.0 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.0 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.0 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.7.0.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.7.0.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.7.0.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.7.0.jar 2.6.5 John Snow Labs Spark-NLP 2.6.5: A few bug fixes and other improvements! Overview We are glad to release Spark NLP 2.6.5! This release comes with a few bug fixes before we move to a new major version. As always, we would like to thank our community for their feedback, questions, and feature requests. Bugfixes Fix a bug in batching sentences in BertSentenceEmbeddings Fix AttributeError when trying to load a saved EmbeddingsFinisher in Python Enhancements Improve handling exceptions in DocumentAssmbler when the user uses a corrupted DataFrame Documentation and Notebooks Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Update the entire spark-nlp-workshop notebooks A brand new 1-hour Spark NLP workshop Update Model Hubs with new models Installation Python #PyPI pip install spark-nlp==2.6.5 #Conda conda install -c johnsnowlabs spark-nlp==2.6.5 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.5 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.5 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.5 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.5 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.5&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.6.5.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.6.5.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.6.5.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.6.5.jar 2.6.4 John Snow Labs Spark-NLP 2.6.4: A few bug fixes and other improvements! Overview We are glad to release Spark NLP 2.6.4! This release comes with a few bug fixes before we move to a new major version. As always, we would like to thank our community for their feedback, questions, and feature requests. Bugfixes Fix loading from a local folder with no access to the cache folder https://github.com/JohnSnowLabs/spark-nlp/pull/1141 Fix NullPointerException in DocumentAssembler when there are null in the rows https://github.com/JohnSnowLabs/spark-nlp/pull/1145 Fix dynamic padding in BertSentenceEmbeddings https://github.com/JohnSnowLabs/spark-nlp/pull/1162 Documentation and Notebooks Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Update the entire spark-nlp-workshop notebooks A brand new 1-hour Spark NLP workshop Update Model Hubs with new models Installation Python #PyPI pip install spark-nlp==2.6.4 #Conda conda install -c johnsnowlabs spark-nlp==2.6.4 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.4 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.4 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.4 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.4 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.6.4.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.6.4.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.6.4.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.6.4.jar 2.6.3 John Snow Labs Spark-NLP 2.6.3: New refactored NerDL with memory optimization, bug fixes, and other improvements! Overview We are glad to release Spark NLP 2.6.3! This release comes with a refactored NerDLApproach that allows users to train their NerDL on any size of the CoNLL file regardless of the memory limitations. We also have some bug fixes and improvements in the 2.6.3 release. Spark NLP has a new and improved Website for its documentation and models. We have been moving our 330+ pretrained models and pipelines into Models Hubs and we would appreciate your feedback! :) As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add enableMemoryOptimizer to allow training NerDLApproach on a dataset larger than the memory Add option to explode sentences in SentenceDetectorDL Enhancements Improve POS (AveragedPerceptron) performance Improve Norvig Spell Checker performance Bugfixes Fix SentenceDetectorDL unsupported model error in pretrained function Fix a race condition in LRU algorithm that can cause NullPointerException during a LightPipeline operation with embeddings Fix max sequence length calculation in BertEmbeddings and BertSentenceEmbeddings Fix threshold in YakeModel on Python side Documentation and Notebooks Spark NLP training certification notebooks for Google Colab and Databricks A brand new 1-hour Spark NLP workshop Update Model Hubs with new models in Spark NLP 2.6.3 Update documentation for release of Spark NLP 2.6.3 Update the entire spark-nlp-workshop notebooks for Spark NLP 2.6.3 Installation Python #PyPI pip install spark-nlp==2.6.3 #Conda conda install -c johnsnowlabs spark-nlp==2.6.3 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.3 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.3 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.6.3.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.6.3.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.6.3.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.6.3.jar 2.6.2 John Snow Labs Spark-NLP 2.6.2: New SentenceDetectorDL, improved BioBERT models, new Models Hub, and other improvements! Overview We are glad to release Spark NLP 2.6.2! This release comes with a brand new SentenceDetectorDL (SDDL) that is based on a general-purpose neural network model for sentence boundary detection with higher accuracy. In addition, we are releasing 12 new and improved BioBERT models for BertEmbeddings and BertSentenceEembeddings used for sequence and text classifications. Spark NLP has a new and improved Website for its documentation and models. We have been moving our 330+ pretrained models and pipelines into Models Hubs and we would appreciate your feedback! :) As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Introducing a new SentenceDetectorDL (trainable) for sentence boundary detection Dedicated Models Hub for all pretrained models &amp; pipelines Enhancements Improved BioBERT models quality for BertEmbeddings (it achieves higher accuracy in sequence classification) Improved Sentence BioBERT models quality for BertSentenceEmbeddings (it achieves higher accuracy in text classification) Improve loadSavedModel in BertEmbeddings and BertSentenceEmbeddings Add unit test to MultiClassifierDL annotator Better error handling in SentimentDLApproach Bugfixes Fix BERT LaBSE model for BertSentenceEmbeddings Fix loadSavedModel for BertSentenceEmbeddings in Python Deprecations DeepSentenceDetector is deprecated in favor of SentenceDetectorDL Models Model Name Build Lang BertEmbeddings biobert_pubmed_base_cased 2.6.2 en BertEmbeddings biobert_pubmed_large_cased 2.6.2 en BertEmbeddings biobert_pmc_base_cased 2.6.2 en BertEmbeddings biobert_pubmed_pmc_base_cased 2.6.2 en BertEmbeddings biobert_clinical_base_cased 2.6.2 en BertEmbeddings biobert_discharge_base_cased 2.6.2 en BertSentenceEmbeddings sent_biobert_pubmed_base_cased 2.6.2 en BertSentenceEmbeddings sent_biobert_pubmed_large_cased 2.6.2 en BertSentenceEmbeddings sent_biobert_pmc_base_cased 2.6.2 en BertSentenceEmbeddings sent_biobert_pubmed_pmc_base_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_clinical_base_cased 2.6.2 en BertSentenceEmbeddings sent_biobert_discharge_base_cased 2.6.2 en The complete list of all 330+ models &amp; pipelines in 46+ languages is available here. Documentation and Notebooks New notebook to use SentenceDetectorDL Update Model Hubs with new models in Spark NLP 2.6.2 Update documentation for release of Spark NLP 2.6.2 Update the entire spark-nlp-workshop notebooks for Spark NLP 2.6.2 Installation Python #PyPI pip install spark-nlp==2.6.2 #Conda conda install -c johnsnowlabs spark-nlp==2.6.2 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.2 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.2 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.6.2.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.6.2.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.6.2.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.6.2.jar 2.6.1 John Snow Labs Spark-NLP 2.6.1: New Portuguese BERT models, import any BERT models to Spark NLP, and a bug-fix for ClassifierDL Overview We are glad to release Spark NLP 2.6.1! This release comes with new Portuguese BERT models, a notebook to demonstrate how to import any BERT models to Spark NLP, and a fix for ClassifierDL which was introduced in the 2.6.0 release that resulted in low accuracy during training. As always, we would like to thank our community for their feedback, questions, and feature requests. Bugfixes Fix lower accuracy in ClassifierDL introduced in 2.6.0 release Models and Pipelines Model Name Build Lang BertEmbeddings bert_portuguese_base_cased 2.6.0 pt BertEmbeddings bert_portuguese_large_cased 2.6.0 pt The complete list of all 330+ models &amp; pipelines in 46+ languages is available here. Documentation and Notebooks New notebook to import BERT checkpoints into Spark NLP New notebook to extract keywords Update documentation for release of Spark NLP 2.6.1 Update the entire spark-nlp-models repository with new pre-trained models and pipelines Update the entire spark-nlp-workshop notebooks for Spark NLP 2.6.1 Installation Python #PyPI pip install spark-nlp==2.6.1 #Conda conda install -c johnsnowlabs spark-nlp==2.6.1 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.1 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.1 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.6.1.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.6.1.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.6.1.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.6.1.jar 2.6.0 John Snow Labs Spark-NLP 2.6.0: New multi-label classifier, BERT sentence embeddings, unsupervised keyword extractions, over 110 pretrained pipelines, models, Transformers, and more! Overview We are very excited to finally release Spark NLP 2.6.0! This has been one of the biggest releases we have ever made and we are so proud to share it with our community! This release comes with a brand new MultiClassifierDL for multi-label text classification, BertSentenceEmbeddings with 42 models, unsupervised keyword extractions annotator, and adding 28 new pretrained Transformers such as Small BERT, CovidBERT, ELECTRA, and the state-of-the-art language-agnostic BERT Sentence Embedding model(LaBSE). The 2.6.0 release has over 110 new pretrained models, pipelines, and Transformers with extending full support for Danish, Finnish, and Swedish languages. Major features and improvements NEW: A new MultiClassifierDL annotator for multi-label text classification built by using Bidirectional GRU and CNN inside TensorFlow that supports up to 100 classes NEW: A new BertSentenceEmbeddings annotator with 42 available pre-trained models for sentence embeddings used in SentimentDL, ClassifierDL, and MultiClassifierDL annotators NEW: A new YakeModel annotator for an unsupervised, corpus-independent, domain, and language-independent and single-document keyword extraction algorithm NEW: Integrate 24 new Small BERT models where the smallest model is 24x times smaller and 28x times faster compare to BERT base models NEW: Add 3 new ELECTRA small, base, and large models NEW: Add 4 new Finnish BERT models for BertEmbeddings and BertSentenceEmbeddings Improve BertEmbeddings memory consumption by 30% Improve BertEmbeddings performance by more than 70% with a new built-in dynamic shape inputs Remove the poolingLayer parameter in BertEmbeddings in favor of sequence_output that is provided by TF Hub models for new BERT models Add validation loss, validation accuracy, validation F1, and validation True Positive Rate during the training in MultiClassifierDL Add parameter to enable/disable list detection in SentenceDetector Unify the loggings in ClassifierDL and SentimentDL during training Bugfixes Fix Tokenization bug with Bigrams in the exception list Fix the versioning error in second SBT projects causing models not being found via pretrained function Fix logging to file in NerDLApproach, ClassifierDL, SentimentDL, and MultiClassifierDL on HDFS Fix ignored modified tokens in BertEmbeddings, now it will consider modified tokens instead of originals Models and Pipelines This release comes with over 100+ new pretrained models and pipelines available for Windows, Linux, and macOS users. The complete list of all 330+ models &amp; pipelines in 46+ languages is available here. Some selected Transformers: Model Name Build Lang BertEmbeddings electra_small_uncased 2.6.0 en BertEmbeddings electra_base_uncased 2.6.0 en BertEmbeddings electra_large_uncased 2.6.0 en BertEmbeddings covidbert_large_uncased 2.6.0 en BertEmbeddings small_bert_L2_128 2.6.0 en BertEmbeddings small_bert_L4_128 2.6.0 en BertEmbeddings small_bert_L6_128 2.6.0 en BertEmbeddings small_bert_L8_128 2.6.0 en BertEmbeddings small_bert_L10_128 2.6.0 en BertEmbeddings small_bert_L12_128 2.6.0 en BertEmbeddings small_bert_L2_256 2.6.0 en BertEmbeddings small_bert_L4_256 2.6.0 en BertEmbeddings small_bert_L6_256 2.6.0 en BertEmbeddings small_bert_L8_256 2.6.0 en BertEmbeddings small_bert_L10_256 2.6.0 en BertEmbeddings small_bert_L12_256 2.6.0 en BertEmbeddings small_bert_L2_512 2.6.0 en BertEmbeddings small_bert_L4_512 2.6.0 en BertEmbeddings small_bert_L6_512 2.6.0 en BertEmbeddings small_bert_L8_512 2.6.0 en BertEmbeddings small_bert_L10_512 2.6.0 en BertEmbeddings small_bert_L12_512 2.6.0 en BertEmbeddings small_bert_L2_768 2.6.0 en BertEmbeddings small_bert_L4_768 2.6.0 en BertEmbeddings small_bert_L6_768 2.6.0 en BertEmbeddings small_bert_L8_768 2.6.0 en BertEmbeddings small_bert_L10_768 2.6.0 en BertEmbeddings small_bert_L12_768 2.6.0 en BertEmbeddings bert_finnish_cased 2.6.0 fi BertEmbeddings bert_finnish_uncased 2.6.0 fi BertSentenceEmbeddings sent_bert_finnish_cased 2.6.0 fi BertSentenceEmbeddings sent_bert_finnish_uncased 2.6.0 fi BertSentenceEmbeddings sent_electra_small_uncased 2.6.0 en BertSentenceEmbeddings sent_electra_base_uncased 2.6.0 en BertSentenceEmbeddings sent_electra_large_uncased 2.6.0 en BertSentenceEmbeddings sent_bert_base_uncased 2.6.0 en BertSentenceEmbeddings sent_bert_base_cased 2.6.0 en BertSentenceEmbeddings sent_bert_large_uncased 2.6.0 en BertSentenceEmbeddings sent_bert_large_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_pubmed_base_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_pubmed_large_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_pmc_base_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_pubmed_pmc_base_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_clinical_base_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_discharge_base_cased 2.6.0 en BertSentenceEmbeddings sent_covidbert_large_uncased 2.6.0 en BertSentenceEmbeddings sent_small_bert_L2_128 2.6.0 en BertSentenceEmbeddings sent_small_bert_L4_128 2.6.0 en BertSentenceEmbeddings sent_small_bert_L6_128 2.6.0 en BertSentenceEmbeddings sent_small_bert_L8_128 2.6.0 en BertSentenceEmbeddings sent_small_bert_L10_128 2.6.0 en BertSentenceEmbeddings sent_small_bert_L12_128 2.6.0 en BertSentenceEmbeddings sent_small_bert_L2_256 2.6.0 en BertSentenceEmbeddings sent_small_bert_L4_256 2.6.0 en BertSentenceEmbeddings sent_small_bert_L6_256 2.6.0 en BertSentenceEmbeddings sent_small_bert_L8_256 2.6.0 en BertSentenceEmbeddings sent_small_bert_L10_256 2.6.0 en BertSentenceEmbeddings sent_small_bert_L12_256 2.6.0 en BertSentenceEmbeddings sent_small_bert_L2_512 2.6.0 en BertSentenceEmbeddings sent_small_bert_L4_512 2.6.0 en BertSentenceEmbeddings sent_small_bert_L6_512 2.6.0 en BertSentenceEmbeddings sent_small_bert_L8_512 2.6.0 en BertSentenceEmbeddings sent_small_bert_L10_512 2.6.0 en BertSentenceEmbeddings sent_small_bert_L12_512 2.6.0 en BertSentenceEmbeddings sent_small_bert_L2_768 2.6.0 en BertSentenceEmbeddings sent_small_bert_L4_768 2.6.0 en BertSentenceEmbeddings sent_small_bert_L6_768 2.6.0 en BertSentenceEmbeddings sent_small_bert_L8_768 2.6.0 en BertSentenceEmbeddings sent_small_bert_L10_768 2.6.0 en BertSentenceEmbeddings sent_small_bert_L12_768 2.6.0 en BertSentenceEmbeddings sent_bert_multi_cased 2.6.0 xx BertSentenceEmbeddings labse 2.6.0 xx Danish pipelines Pipeline Name Build Lang Explain Document Small explain_document_sm 2.6.0 da Explain Document Medium explain_document_md 2.6.0 da Explain Document Large explain_document_lg 2.6.0 da Entity Recognizer Small entity_recognizer_sm 2.6.0 da Entity Recognizer Medium entity_recognizer_md 2.6.0 da Entity Recognizer Large entity_recognizer_lg 2.6.0 da Finnish pipelines Pipeline Name Build Lang Explain Document Small explain_document_sm 2.6.0 fi Explain Document Medium explain_document_md 2.6.0 fi Explain Document Large explain_document_lg 2.6.0 fi Entity Recognizer Small entity_recognizer_sm 2.6.0 fi Entity Recognizer Medium entity_recognizer_md 2.6.0 fi Entity Recognizer Large entity_recognizer_lg 2.6.0 fi Swedish pipelines Pipeline Name Build Lang Explain Document Small explain_document_sm 2.6.0 sv Explain Document Medium explain_document_md 2.6.0 sv Explain Document Large explain_document_lg 2.6.0 sv Entity Recognizer Small entity_recognizer_sm 2.6.0 sv Entity Recognizer Medium entity_recognizer_md 2.6.0 sv Entity Recognizer Large entity_recognizer_lg 2.6.0 sv Documentation and Notebooks New notebook for training multi-label Toxic comments New notebook for training multi-label E2E Challenge Update documentation for release of Spark NLP 2.6.0 Update the entire spark-nlp-models repository with new pre-trained models and pipelines Update the entire spark-nlp-workshop notebooks for Spark NLP 2.6.0 Installation Python #PyPI pip install spark-nlp==2.6.0 #Conda conda install -c johnsnowlabs spark-nlp==2.6.0 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.0 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.0 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.6.0.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.6.0.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.6.0.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.6.0.jar 2.5.5 John Snow Labs Spark-NLP 2.5.5: 28 new Lemma and POS models in 14 languages, bug fixes, and lots of new notebooks! Overview We are excited to release Spark NLP 2.5.5 with 28 new pretrained models for Lemma and POS in 14 languages, bug fixes, new notebooks, and more! As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add getClasses() function to NerDLModel Add getClasses() function to ClassifierDLModel Add getClasses() function to SentimentDLModel Example: ner_model = NerDLModel.pretrained(&#39;onto_100&#39;) print(ner_model.getClasses()) #[&#39;O&#39;, &#39;B-CARDINAL&#39;, &#39;B-EVENT&#39;, &#39;I-EVENT&#39;, &#39;B-WORK_OF_ART&#39;, &#39;I-WORK_OF_ART&#39;, &#39;B-ORG&#39;, &#39;B-DATE&#39;, &#39;I-DATE&#39;, &#39;I-ORG&#39;, &#39;B-GPE&#39;, &#39;B-PERSON&#39;, &#39;B-PRODUCT&#39;, &#39;B-NORP&#39;, &#39;B-ORDINAL&#39;, &#39;I-PERSON&#39;, &#39;B-MONEY&#39;, &#39;I-MONEY&#39;, &#39;I-GPE&#39;, &#39;B-LOC&#39;, &#39;I-LOC&#39;, &#39;I-CARDINAL&#39;, &#39;B-FAC&#39;, &#39;I-FAC&#39;, &#39;B-LAW&#39;, &#39;I-LAW&#39;, &#39;B-TIME&#39;, &#39;I-TIME&#39;, &#39;B-PERCENT&#39;, &#39;I-PERCENT&#39;, &#39;I-NORP&#39;, &#39;I-PRODUCT&#39;, &#39;B-QUANTITY&#39;, &#39;I-QUANTITY&#39;, &#39;B-LANGUAGE&#39;, &#39;I-ORDINAL&#39;, &#39;I-LANGUAGE&#39;, &#39;X&#39;] Enhancements Improve max sequence length calculation in BertEmbeddings and XlnetEmbeddings Bugfixes Fix a bug in RegexTokenizer in Python Fix StopWordsCleaner exception in Python when pretrained() is used Fix max sequence length issue in AlbertEmbeddings and SentencePiece generation Fix HDFS support for setGaphFolder param in NerDLApproach Models We have added 28 new pretrained models for Lemma and POS in 14 languages: Model Name Build Lang LemmatizerModel (Lemmatizer) lemma 2.5.5 br LemmatizerModel (Lemmatizer) lemma 2.5.5 ca LemmatizerModel (Lemmatizer) lemma 2.5.5 da LemmatizerModel (Lemmatizer) lemma 2.5.5 ga LemmatizerModel (Lemmatizer) lemma 2.5.5 hi LemmatizerModel (Lemmatizer) lemma 2.5.5 hy LemmatizerModel (Lemmatizer) lemma 2.5.5 eu LemmatizerModel (Lemmatizer) lemma 2.5.5 mr LemmatizerModel (Lemmatizer) lemma 2.5.5 yo LemmatizerModel (Lemmatizer) lemma 2.5.5 la LemmatizerModel (Lemmatizer) lemma 2.5.5 lv LemmatizerModel (Lemmatizer) lemma 2.5.5 sl LemmatizerModel (Lemmatizer) lemma 2.5.5 gl LemmatizerModel (Lemmatizer) lemma 2.5.5 id PerceptronModel (POS UD) pos_ud_keb 2.5.5 br PerceptronModel (POS UD) pos_ud_ancora 2.5.5 ca PerceptronModel (POS UD) pos_ud_ddt 2.5.5 da PerceptronModel (POS UD) pos_ud_idt 2.5.5 ga PerceptronModel (POS UD) pos_ud_hdtb 2.5.5 hi PerceptronModel (POS UD) pos_ud_armtdp 2.5.5 hy PerceptronModel (POS UD) pos_ud_bdt 2.5.5 eu PerceptronModel (POS UD) pos_ud_ufal 2.5.5 mr PerceptronModel (POS UD) pos_ud_ytb 2.5.5 yo PerceptronModel (POS UD) pos_ud_llct 2.5.5 la PerceptronModel (POS UD) pos_ud_lvtb 2.5.5 lv PerceptronModel (POS UD) pos_ud_ssj 2.5.5 sl PerceptronModel (POS UD) pos_ud_treegal 2.5.5 gl PerceptronModel (POS UD) pos_ud_gsd 2.5.5 id Languages: Armenian, Basque, Breton, Catalan, Danish, Galician, Hindi, Indonesian, Irish, Latin, Latvian, Marathi, Slovenian, Yoruba Documentation and Notebooks New notebook for pretrained StopWordsCleaner New notebook to Detect entities in German language New notebook to Detect entities in English language New notebook to Detect entities in Spanish language New notebook to Detect entities in French language New notebook to Detect entities in Italian language New notebook to Detect entities in Norwegian language New notebook to Detect entities in Polish language New notebook to Detect entities in Portugese language New notebook to Detect entities in Russian language Update documentation for release of Spark NLP 2.5.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Update the entire spark-nlp-workshop notebooks for Spark NLP 2.5.x Installation Python #PyPI pip install spark-nlp==2.5.5 #Conda conda install -c johnsnowlabs spark-nlp==2.5.5 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.5 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.5 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.5.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.5.5 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.5.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.5.5 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.5.5.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.5.5.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.5.5.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.5.5.jar 2.5.4 John Snow Labs Spark-NLP 2.5.4: Supporting Apache Spark 2.3, 43 new models and 26 new languages, new RegexTokenizer, lots of new notebooks, and more Overview We are excited to release Spark NLP 2.5.4 with the full support of Apache Spark 2.3.x, adding 43 new pre-trained models for stop words cleaning, supporting 26 new languages, a new RegexTokenizer annotator and more! As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add support for Apache Spark 2.3.x including new Maven artifacts and full support of all pre-trained models/pipelines Add 43 new pre-trained models in 43 languages to StopWordsCleaner annotator Introduce a new RegexTokenizer to split text by regex pattern Enhancements Retrained 6 new BioBERT and ClinicalBERT models Add a new param spark23 to start() function to start the session for Apache Spark 2.3.x Bugfixes Add missing library for SentencePiece used by AlbertEmbeddings and XlnetEmbeddings on Windows Fix ModuleNotFoundError in LanguageDetectorDL pipelines in Python Models We have added 43 new pre-trained models in 43 languages for StopWordsCleaner. Some selected models: Afrikaans - Models Model Name Build Lang Offline StopWordsCleaner stopwords_af 2.5.4 af Download Arabic - Models Model Name Build Lang Offline StopWordsCleaner stopwords_ar 2.5.4 ar Download Armenian - Models Model Name Build Lang Offline StopWordsCleaner stopwords_hy 2.5.4 hy Download Basque - Models Model Name Build Lang Offline StopWordsCleaner stopwords_eu 2.5.4 eu Download Bengali - Models Model Name Build Lang Offline StopWordsCleaner stopwords_bn 2.5.4 bn Download Breton - Models Model Name Build Lang Offline StopWordsCleaner stopwords_br 2.5.4 br Download Documentation and Notebooks New notebook for Language detection and identification New notebook for Classify text according to TREC classes New notebook for Detect Spam messages New notebook for Detect fake news New notebook for Find sentiment in text New notebook for Detect bullying in tweets New notebook for Detect Emotions in text New notebook for Detect Sarcasm in text Update the entire spark-nlp-models repository with new pre-trained models and pipelines Update the entire spark-nlp-workshop notebooks for Spark NLP 2.5.x Update documentation for release of Spark NLP 2.5.x Installation Python #PyPI pip install spark-nlp==2.5.4 #Conda conda install -c johnsnowlabs spark-nlp==2.5.4 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.4 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.4 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.5.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.5.4 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.5.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.5.4 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.5.4.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.5.4.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.5.4.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.5.4.jar 2.5.3 John Snow Labs Spark-NLP 2.5.3: Detect Fake news, emotions, spams, and more classification models, enhancements, and bug fixes Overview We are very happy to release Spark NLP 2.5.3 with 5 new pre-trained ClassifierDL models for multi-class text classification. There are also bug-fixes and other enhancements introduced in this release which were reported and requested by Spark NLP users. As always, we thank our community for their feedback, questions, and feature requests. New Features TextMatcher now can construct the chunks from tokens instead of the original documents via buildFromTokens param CoNLLGenerator now is accessible in Python Bugfixes Fix a bug in ContextSpellChecker resulting in IllegalArgumentException Enhancements Improve RocksDB connection to support different storage capabilities Improve parameters naming convention in ContextSpellChecker Add NerConverter to documentation Fix multi-language tabs in documentation Models We have added 5 new pre-trained ClassifierDL models for multi-class text classification. Model Name Build Lang Description Offline ClassifierDLModel classifierdl_use_spam 2.5.3 en Detect if a message is spam or not Download ClassifierDLModel classifierdl_use_fakenews 2.5.3 en Classify if a news is fake or real Download ClassifierDLModel classifierdl_use_emotion 2.5.3 en Detect Emotions in TweetsDetect Emotions in Tweets Download ClassifierDLModel classifierdl_use_cyberbullying 2.5.3 en Classify if a tweet is bullying Download ClassifierDLModel classifierdl_use_sarcasm 2.5.3 en Identify sarcastic tweets Download Documentation Update documentation for release of Spark NLP 2.5.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.5.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation Python #PyPI pip install spark-nlp==2.5.3 #Conda conda install -c johnsnowlabs spark-nlp==2.5.3 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.3 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.3 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.3&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.5.3.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.5.3.jar 2.5.2 John Snow Labs Spark-NLP 2.5.2: New Language Detection annotator, enhancements, and bug fixes Overview We are very happy to release Spark NLP 2.5.2 with a new state-of-the-art LanguageDetectorDL annotator to detect and identify up to 20 languages. There are also bug-fixes and other enhancements introduced in this release which were reported and requested by Spark NLP users. As always, we thank our community for their feedback, questions, and feature requests. New Features Introducing a new LanguageDetectorDL state-of-the-art annotator to detect and identify languages in documents and sentences Add a new param entityValue to TextMatcher to add custom value inside metadata. Useful in post-processing when there are multiple TextMatcher annotators with multiple dictionaries https://github.com/JohnSnowLabs/spark-nlp/issues/920 Bugfixes Add missing TensorFlow graphs to train ContextSpellChecker annotator https://github.com/JohnSnowLabs/spark-nlp/issues/912 Fix misspelled param in classThreshold param in ContextSpellChecker annotator https://github.com/JohnSnowLabs/spark-nlp/issues/911 Fix a bug where setGraphFolder in NerDLApproach annotator couldn’t find a graph on Databricks (DBFS) https://github.com/JohnSnowLabs/spark-nlp/issues/739 Fix a bug in NerDLApproach when includeConfidence was set to true https://github.com/JohnSnowLabs/spark-nlp/issues/917 Fix a bug in BertEmbeddings https://github.com/JohnSnowLabs/spark-nlp/issues/906 https://github.com/JohnSnowLabs/spark-nlp/issues/918 Enhancements Improve TF backend in ContextSpellChecker annotator Pipelines and Models We have added 4 new LanguageDetectorDL models and pipelines to detect and identify up to 20 languages: The model with 7 languages: Czech, German, English, Spanish, French, Italy, and Slovak The model with 20 languages: Bulgarian, Czech, German, Greek, English, Spanish, Finnish, French, Croatian, Hungarian, Italy, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Swedish, Turkish, and Ukrainian Model Name Build Lang Offline LanguageDetectorDL ld_wiki_7 2.5.2 xx Download LanguageDetectorDL ld_wiki_20 2.5.2 xx Download Pipeline Name Build Lang Offline LanguageDetectorDL detect_language_7 2.5.2 xx Download LanguageDetectorDL detect_language_20 2.5.2 xx Download Documentation Update documentation for release of Spark NLP 2.5.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.5.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation Python #PyPI pip install spark-nlp==2.5.2 #Conda conda install -c johnsnowlabs spark-nlp==2.5.2 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.2 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.2 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.5.2.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.5.2.jar 2.5.1 John Snow Labs Spark-NLP 2.5.1: Adding support for 6 new BioBERT and ClinicalBERT models Overview We are very excited to extend Spark NLP support to 6 new BERT models for medical and clinical documents. We have also updated our documentation for 2.5.x releases, notebooks in our workshop, and made some enhancements in this release. As always, we thank our community for their feedback and questions in our Slack channel. New Features Add Python support for PubTator reader to convert automatic annotations of the biomedical datasets into DataFrame Add 6 new pre-trained BERT models from BioBERT and ClinicalBERT Models We have added 6 new BERT models for medical and clinical purposes. The 4 BERT pre-trained models are from BioBERT and the other 2 are coming from ClinicalBERT models: Model Name Build Lang Offline BertEmbeddings biobert_pubmed_base_cased 2.5.0 en Download BertEmbeddings biobert_pubmed_large_cased 2.5.0 en Download BertEmbeddings biobert_pmc_base_cased 2.5.0 en Download BertEmbeddings biobert_pubmed_pmc_base_cased 2.5.0 en Download BertEmbeddings biobert_clinical_base_cased 2.5.0 en Download BertEmbeddings biobert_discharge_base_cased 2.5.0 en Download Enhancements Add unit tests for XlnetEmbeddings Add unit tests for AlbertEmbeddings Add unit tests for ContextSpellChecker Documentation Update documentation for release of Spark NLP 2.5.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.5.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation Python #PyPI pip install spark-nlp==2.5.1 #Conda conda install -c johnsnowlabs spark-nlp==2.5.1 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.1 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.1 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.5.1.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.5.1.jar 2.5.0 John Snow Labs Spark-NLP 2.5.0: ALBERT &amp; XLNet transformers, state-of-the-art spell checker, multi-class sentiment detector, 80+ new models &amp; pipelines in 14 new languages &amp; more Overview When we started planning for Spark NLP 2.5.0 release a few months ago the world was a different place! We have been blown away by the use of Natural Language Processing for early outbreak detections, question-answering chatbot services, text analysis of medical records, monitoring efforts to minimize the virus spread, and many more. In that spirit, we are honored to announce Spark NLP 2.5.0 release! Witnessing the world coming together to fight coronavirus has driven us to deliver perhaps one of the biggest releases we have ever made. As always, we thank our community for their feedback, bug reports, and contributions that made this release possible. Major features and improvements NEW: A new AlbertEmbeddings annotator with 4 available pre-trained models NEW: A new XlnetEmbeddings annotator with 2 available pre-trained models NEW: A new ContextSpellChecker annotator, the state-of-the-art annotator for spell checking NEW: A new SentimentDL annotator for multi-class sentiment analysis. This annotator comes with 2 available pre-trained models trained on IMDB and Twitter datasets NEW: Support for 14 new languages with 80+ pretrained models and pipelines! Add new PubTator reader to convert automatic annotations of the biomedical datasets into DataFrame Introducing a new outputLogsPath param for NerDLApproach, ClassifierDLApproach and SentimentDLApproach annotators Refactored CoNLLGenerator to actually use NER labels from the DataFrame Unified params in NerDLModel in both Scala and Python Extend and complete Scaladoc APIs for all the annotators Bugfixes Fix position of tokens in Normalizer Fix Lemmatizer exception on a bad input Fix annotator logs failing on object storage file systems like DBFS Models and Pipelines Spark NLP 2.5.0 comes with 87 new pretrained models and pipelines in 14 new languages available for all Windows, Linux, and macOS users. We added new languages such as Dutch, Norwegian. Polish, Portuguese, Bulgarian, Czech, Greek, Finnish, Hungarian, Romanian, Slovak, Swedish, Turkish, and Ukrainian. The complete list of 160+ models &amp; pipelines in 22+ languages is available here. Featured Pretrained Pipelines Dutch - Pipelines Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 nl   Download Explain Document Medium explain_document_md 2.5.0 nl   Download Explain Document Large explain_document_lg 2.5.0 nl   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 nl   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 nl   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 nl   Download Norwegian - Pipelines Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 no   Download Explain Document Medium explain_document_md 2.5.0 no   Download Explain Document Large explain_document_lg 2.5.0 no   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 no   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 no   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 no   Download Polish - Pipelines Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 pl   Download Explain Document Medium explain_document_md 2.5.0 pl   Download Explain Document Large explain_document_lg 2.5.0 pl   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 pl   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 pl   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 pl   Download Portuguese - Pipelines Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 pt   Download Explain Document Medium explain_document_md 2.5.0 pt   Download Explain Document Large explain_document_lg 2.5.0 pt   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 pt   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 pt   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 pt   Download Documentation Update documentation for release of Spark NLP 2.5.0 Update the entire spark-nlp-workshop notebooks for Spark NLP 2.5.0 Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation Python #PyPI pip install spark-nlp==2.5.0 #Conda conda install -c johnsnowlabs spark-nlp==2.5.0 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.5.0.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.5.0.jar 2.4.5 John Snow Labs Spark-NLP 2.4.5: Supporting more Databricks runtimes and YARN in cluster mode Overview We are very excited to extend Spark NLP support to 6 new Databricks runtimes and add support to Cloudera and EMR YARN cluster-mode. As always, we thank our community for their feedback and questions in our Slack channel. New Features Extend Spark NLP support for Databricks runtimes: 6.2 6.2 ML 6.3 6.3 ML 6.4 6.4 ML 6.5 6.5 ML Add support for cluster-mode in Cloudera and EMR YARN clusters New splitPattern param in Tokenizer to split tokens by regex rules Bugfixes Fix ClassifierDLModel save and load in Python Fix ClassifierDL TensorFlow session reuse Fix Normalizer positions of new tokens Documentation Update documentation for release of Spark NLP 2.4.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.4.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation Python #PyPI pip install spark-nlp==2.4.5 #Conda conda install -c johnsnowlabs spark-nlp==2.4.5 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.4.5.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.4.5.jar 2.4.4 John Snow Labs Spark-NLP 2.4.4: The very first native multi-class text classifier and pre-trained models and pipelines in Russian Overview We are very excited to release the very first multi-class text classifier in Spark NLP v2.4.4! We have built a generic ClassifierDL annotator that uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 50 classes. We are also happy to announce the support of yet another language: Russian! We have trained and prepared 5 pre-trained models and 6 pre-trained pipelines in Russian. NOTE: ClassifierDL is an experimental feature in 2.4.4 before it becomes stable in 2.4.5 release. We have worked hard to aim for simplicity and we are looking forward to your feedback as always. We will add more examples by the upcoming days: Examples: Python and Scala New Features Introducing a generic multi-class text classifier: ClassifierDL. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 50 classes. 5 new pretrained Russian models (Lemma, POS, 3x NER) 6 new pretrained Russian pipelines Models: Model name language LemmatizerModel (Lemmatizer) lemma ru PerceptronModel (POS UD) pos_ud_gsd ru NerDLModel wikiner_6B_100 ru NerDLModel wikiner_6B_300 ru NerDLModel wikiner_840B_300 ru Pipelines: Pipeline name language Explain Document (Small) explain_document_sm ru Explain Document (Medium) explain_document_md ru Explain Document (Large) explain_document_lg ru Entity Recognizer (Small) entity_recognizer_sm ru Entity Recognizer (Medium) entity_recognizer_md ru Entity Recognizer (Large) entity_recognizer_lg ru Evaluation: wikiner_6B_100 with conlleval.pl Accuracy Precision Recall F1-Score 97.76% 88.85% 88.55% 88.70 wikiner_6B_300 with conlleval.pl Accuracy Precision Recall F1-Score 97.78% 89.09% 88.51% 88.80 wikiner_840B_300 with conlleval.pl Accuracy Precision Recall F1-Score 97.85% 89.85% 89.11% 89.48 Example: import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val pipeline = PretrainedPipeline(&quot;explain_document_sm&quot;, lang=&quot;ru&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Пик распространения коронавируса и вызываемой им болезни Covid-19 в Китае прошел, заявил в четверг агентству Синьхуа официальный представитель Госкомитета по гигиене и здравоохранению КНР Ми Фэн.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() Enhancements Add param to NerConverter to override modified tokens instead of original tokens UniversalSentenceEncoder and SentenceEmbeddings are now accepting storageRef Bugfixes Fix TokenAssembler Fix NerConverter exception when NerDL is trained with different tagging style than IOB/IOB2 Normalizer now recomputes the index of tokens when it removes characters from a text Documentation Update documentation for release of Spark NLP 2.4.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.4.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation Python #PyPI pip install spark-nlp==2.4.4 #Conda conda install -c johnsnowlabs spark-nlp==2.4.4 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.4 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.4 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.4&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.4.4.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.4.4.jar 2.4.3 John Snow Labs Spark-NLP 2.4.3: Minor bug fix in Python Overview This minor release fixes a bug on our Python side that was introduced in 2.4.2 release. As always, we thank our community for their feedback and questions in our Slack channel. NOTE: We highly recommend our Python users to update to 2.4.3 release. Bugfixes Fix Python imports which resulted in AttributeError: module ‘sparknlp’ has no attribute Documentation Update documentation for release of Spark NLP 2.4.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.4.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation PyPI pip install spark-nlp==2.4.3 Conda conda install -c johnsnowlabs spark-nlp==2.4.3 spark-shell spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.3 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.3 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.4.3.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.4.3.jar 2.4.2 John Snow Labs Spark-NLP 2.4.2: Minor bug fixes and improvements Overview This minor release fixes a few bugs in some of our annotators reported by our community. As always, we thank our community for their feedback and questions in our Slack channel. Bugfixes Fix UniversalSentenceEncoder.pretrained() that failed in Python Fix ElmoEmbeddings.pretrained() that failed in Python Fix ElmoEmbeddings poolingLayer param to be a string as expected Fix ChunkEmbeddings to preserve chunk’s index Fix NGramGenerator and missing chunk metadata New Features Add GPU support param in Spark NLP start function: sparknlp.start(gpu=true) Improve create_model.py to create custom TF graph for NerDLApproach Documentation Update documentation for release of Spark NLP 2.4.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.4.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation PyPI pip install spark-nlp==2.4.2 Conda conda install -c johnsnowlabs spark-nlp==2.4.2 spark-shell spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.2 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.2 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.4.2.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.4.2.jar 2.4.1 John Snow Labs Spark-NLP 2.4.1: Bug fixes and the very first Spanish models &amp; pipelines Overview This minor release fixes a few bugs in some of the annotators reported by our community. As always, we thank our community for their feedback on our Slack channel. Models &amp; Pipelines 5 new pretrained Spanish models (Lemma, POS, 3x NER) 6 new pretrained Spanish pipelines Models: Model name language LemmatizerModel (Lemmatizer) lemma es PerceptronModel (POS UD) pos_ud_gsd es NerDLModel wikiner_6B_100 es NerDLModel wikiner_6B_300 es NerDLModel wikiner_840B_300 es Pipelines: Pipeline name language Explain Document (Small) explain_document_sm es Explain Document (Medium) explain_document_md es Explain Document (Large) explain_document_lg es Entity Recognizer (Small) entity_recognizer_sm es Entity Recognizer (Medium) entity_recognizer_md es Entity Recognizer (Large) entity_recognizer_lg es Evaluation: wikiner_6B_100 with conlleval.pl Accuracy Precision Recall F1-Score 98.35% 88.97% 88.64% 88.80 wikiner_6B_300 with conlleval.pl Accuracy Precision Recall F1-Score 98.38% 89.42% 89.03% 89.22 wikiner_840B_300 with conlleval.pl Accuracy Precision Recall F1-Score 98.46% 89.74% 89.43% 89.58 Example import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val pipeline = PretrainedPipeline(&quot;explain_document_sm&quot;, lang=&quot;es&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Ésta se convertiría en una amistad de por vida, y Peleo, conociendo la sabiduría de Quirón , más adelante le confiaría la educación de su hijo Aquiles.&quot;), (2, &quot;Durante algo más de 200 años el territorio de la actual Bolivia constituyó la Real Audiencia de Charcas, uno de los centros más prósperos y densamente poblados de los virreinatos españoles.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() More info on pre-trained models and pipelines Bugfixes Improve ChunkEmbeddings annotator and fix the empty chunk result Fix UniversalSentenceEncoder crashing on empty Tensor Fix NorvigSweetingModel missing sentenceId that results in NGramsGenerator crashing Fix missing storageRef in embeddings’ column for ElmoEmbeddings annotator Documentation Update documentation for release of Spark NLP 2.4.x Add new features such as ElmoEmbeddings and UniversalSentenceEncoder Add multiple programming languages for demos and examples Update the entire spark-nlp-models repository with new pre-trained models and pipelines 2.4.0 John Snow Labs Spark-NLP 2.4.0: New TensorFlow 1.15, Universal Sentence Encoder, Elmo, faster Word Embeddings &amp; more We are very excited to finally release Spark NLP v2.4.0! This has been one of the largest releases we have ever made since the inception of the library! The new release of Spark NLP 2.4.0 has been migrated to TensorFlow 1.15.0 which takes advantage of the latest deep learning technologies and pre-trained models. Major features and improvements NEW: TensorFlow 1.15.0 now works behind Spark NLP. This brings implicit improvements in performance, accuracy, and functionalities NEW: UniversalSentenceEncoder annotator with 2 pre-trained models from TF Hub NEW: ElmoEmbeddings with a pre-trained model from TF Hub NEW: All our pre-trained models are now cross-platform! NEW: For the first time, all the multi-lingual models and pipelines are available for Windows users (French, German and Italian) NEW: MultiDateMatcher capable of matching more than one date per sentence (Extends DateMatcher algorithm) NEW: BigTextMatcher works best with large amounts of input data BertEmbeddings improvements with 5 new models from TF Hub RecursivePipelineModel as an enhanced PipelineModel allows Annotators to access previous annotators in the pipeline for more ML strategies LazyAnnotators: A new Param in Annotators allows them to stand idle in the Pipeline and do nothing. Can be called by other Annotators in a RecursivePipeline RocksDB is now available as a flexible API called Storage. Allows any annotator to have it’s own distributed local index database Now our Tensorflow pre-trained models are cross-platform. Enabling multi-language models and other improvements to Windows users. Improved IO performance in general for handling embeddings Improved cache cleanup and GC by liberating open files utilized in RocksDB (to be improved further) Tokenizer and SentenceDetector Params minLength and MaxLength to filter out annotations outside these bounds Tokenizer improvements in splitChars and simplified rules DateMatcher improvements TextMatcher improvements preload algorithm information within the model for faster prediction Annotators the utilize embeddings have now a strict validation to be using exactly the embeddings they were trained with Improvements in the API allow Annotators with Storage to save and load their RocksDB database independently and let it be shared across Annotators and let it be shared across Annotators Models and Pipelines Spark NLP 2.4.0 comes with new models including Universal Sentence Encoder, BERT, and Elmo models from TF Hub. In addition, our multilingual pipelines are now available for Windows as same as Linux and macOS users. Models Name UniversalSentenceEncoder tf_use UniversalSentenceEncoder tf_use_lg BertEmbeddings bert_large_cased BertEmbeddings bert_large_uncased BertEmbeddings bert_base_cased BertEmbeddings bert_base_uncased BertEmbeddings bert_multi_cased ElmoEmbeddings elmo NerDLModel onto_100 NerDLModel onto_300 Pipelines Name Language Explain Document Large explain_document_lg fr Explain Document Medium explain_document_md fr Entity Recognizer Large entity_recognizer_lg fr Entity Recognizer Medium entity_recognizer_md fr Explain Document Large explain_document_lg de Explain Document Medium explain_document_md de Entity Recognizer Large entity_recognizer_lg de Entity Recognizer Medium entity_recognizer_md de Explain Document Large explain_document_lg it Explain Document Medium explain_document_md it Entity Recognizer Large entity_recognizer_lg it Entity Recognizer Medium entity_recognizer_md it Example: # Import Spark NLP from sparknlp.base import * from sparknlp.annotator import * from sparknlp.pretrained import PretrainedPipeline import sparknlp # Start Spark Session with Spark NLP # If you already have a SparkSession (Zeppelin, Databricks, etc.) # you can skip this spark = sparknlp.start() # Download a pre-trained pipeline pipeline = PretrainedPipeline(&#39;explain_document_md&#39;, lang=&#39;fr&#39;) # Your testing dataset text = &quot;&quot;&quot; Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale. &quot;&quot;&quot; # Annotate your testing dataset result = pipeline.annotate(text) # What&#39;s in the pipeline list(result.keys()) # result: # [&#39;entities&#39;, &#39;lemma&#39;, &#39;document&#39;, &#39;pos&#39;, &#39;token&#39;, &#39;ner&#39;, &#39;embeddings&#39;, &#39;sentence&#39;] # Check the results result[&#39;entities&#39;] # entities: # [&#39;Emmanuel Jean-Michel Frédéric Macron&#39;, &#39;Jean-Michel Macron&#39;, &quot;CHU d&#39;Amiens4&quot;, &#39;Françoise Noguès&#39;, &#39;Sécurité sociale&#39;] Backward incompatibilities Please note that in 2.4.0 we have added storageRef parameter to our WordEmbeddogs. This means every WordEmbeddingsModel will now have storageRef which is also bound to NerDLModel trained by that embeddings. This assures users won’t use a NerDLModel with a wrong WordEmbeddingsModel. Example: val embeddings = new WordEmbeddings() .setStoragePath(&quot;/tmp/glove.6B.100d.txt&quot;, ReadAs.TEXT) .setDimension(100) .setStorageRef(&quot;glove_100d&quot;) // Use or save this WordEmbeddings with storageRef .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) If you save theWordEmbeddings model the storageRef will be glove_100d. If you ever train any NerDLApproach the glove_100d will bind to that NerDLModel. If you have already WordEmbeddingsModels saved from earlier versions, you either need to re-save them with storageRed or you can manually add this param in their metadata/. The same advice works for the NerDLModel from earlier versions. Installation Python #PyPI pip install spark-nlp==2.4.0 #Conda conda install -c johnsnowlabs spark-nlp==2.4.0 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.0 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.0 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.4.0.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.4.0.jar Bugfixes Fixed splitChars in Tokenizer Fixed PretrainedPipeline in Python to allow accessing the inner PipelineModel in the instance Fixes in Chunk and SentenceEmbeddings to better deal with empty cleaned-up Annotations Documentation and examples We have a new Developer section for those who are interested in contributing to Spark NLP Developer We have updated our workshop repository with more notebooks Workshop",
    "url": "/docs/en/release_notes",
    "relUrl": "/docs/en/release_notes"
  },
  "84": {
    "id": "84",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/resolve_entities_codes",
    "relUrl": "/resolve_entities_codes"
  },
  "85": {
    "id": "85",
    "title": "Security and Privacy",
    "content": "We understand and take the security issues as the highest priority. On every release, all our artifacts and images ran through a series of security testing - Static Code analysis, Pen Test, Images Vulnerabilities Test, AWS AMI Scan Test. Every identified critical issue is remediated, code gets refactored to pass our standard Static Code Analysis. Role-based access Role-based access control is available for all Annotation Lab deployments. By default, all projects are private to the user who created them – the project owner. If necessary, project owners can add other users to the project and define their role(s) among annotator, reviewer, manager. The three roles supported by Annotation Lab offer different levels of task and feature visibility. Annotators can only see tasks assigned to them and their own completions. Reviewers can see the work of annotators who created completions for the tasks assigned to them. Annotators and reviewers do not have access to task import or annotation export nor to the Models Hub page. Managers have higher level of access. They can see all tasks content, can assign work to annotators and reviewers, can import tasks, export annotations, see completions created by team members or download models. When creating the annotation team, make sure the appropriate role is assigned to each team member according to the Need-To-Know Basis. Screen capture is not disabled, and given the high adoption of mobile technologies, team members can easily take pictures of the data. This is why, when dealing with sensitive documents, it is advisable to conduct periodical HIPPA/GDPR training with the annotation team to avoid data breaches. Data sharing Annotation Lab runs locally - all computation and model training run inside the boundaries of your deployment environment. The content related to any tasks within your projects is NOT SHARED with anyone. The Annotation Lab does not call home. Access to internet is used ONLY when downloading models from the NLP Models Hub. Document processing - OCR, pre-annotation, training, fine-tuning- runs entirely on your environment. Secure user access to Annotation Lab Access to Annotation Lab is restricted to users who are given access by an admin or project manager. Each user has an account; when created, passwords are enforced to best practice security policy. Annotation Lab keeps track of who has access to the defined projects and their actions regarding completions creation, cloning, submission, and starring. See User Management Page for more details. API access to Annotation Lab Access to Annotation Lab REST API requires an access token that is specific to a user account. To obtain your access token please follow the steps illustrated here. Complete project audit trail Annotation Lab keeps trail for all created completions. It is not possible for annotators or reviewers to delete any completions and only managers and project owners are able to remove tasks. Application development cycle The Annotation Lab development cycle currently includes static code analysis; everything is assembled as docker images whom are being scanned for vulnerabilities before being published. We are currently implementing web vulnerability scanning.",
    "url": "/docs/en/alab/security",
    "relUrl": "/docs/en/alab/security"
  },
  "86": {
    "id": "86",
    "title": "Annotation Settings",
    "content": "The labeling screen offers some configurable features. For example, you can modify the layout of the screen, hide or show predictions, annotations, or the results panel, hide or show various controls and buttons. It is also possible to keep a label selected after creating a region, display labels on bounding boxes, polygons and other regions while labeling, and show line numbers for text labeling. Enable labeling hotkeys This option specifies if the hotkeys assigned to taxonomy labels should be used for activating them during the annotation process. Show hotkey tooltips When the enable labelling hotkeys is turned on, it is possible to also show the hotkey tooltips on some of the buttons - the ones which have hotkeys assigned. Show labels hotkey tooltips When this option is checked tooltips will be displayed on mouse over for the buttons that have hotkeys assigned. Show labels inside the regions When this option is checked the labels assigned to each annotated region are displayed on the respective region as shown in the image below. Keep label selected after creating a region This option helps users quickly annotate sequences of the same label by keeping the label selected after the annotation of a region. With the option unchecked: With the option checked: Select regions after creating This option keeps the annotated region selected after annotation. In this way, it will be easier for users to quickly change the assigned label for the last selected region if necessary. Show line numbers for Text This option adds line counts for the text to annotate as illustrated below. Label all occurrences of selected text When checked, this option allow users to annotate all occurences of a label in the current task in one step.",
    "url": "/docs/en/alab/settings",
    "relUrl": "/docs/en/alab/settings"
  },
  "87": {
    "id": "87",
    "title": "Spark NLP",
    "content": "Requirements &amp; Setup Spark NLP is built on top of Apache Spark 3.x. For using Spark NLP you need: Java 8 Apache Spark 3.1.x (or 3.0.x, or 2.4.x, or 2.3.x) It is recommended to have basic knowledge of the framework and a working environment before using Spark NLP. Please refer to Spark documentation to get started with Spark. Install Spark NLP in Python Scala and Java Databricks EMR Join our Slack channel Join our channel, to ask for help and share your feedback. Developers and users can help each other getting started here. Spark NLP Slack Spark NLP in Action Make sure to check out our demos built by Streamlit to showcase Spark NLP in action: Spark NLP Demo Spark NLP Workshop If you prefer learning by example, check this repository: Spark NLP Workshop It is full of fresh examples and even a docker container if you want to skip installation. Below, you can follow into a more theoretical and thorough quick start guide. Where to go next If you need more detailed information about how to install Spark NLP you can check the Installation page Detailed information about Spark NLP concepts, annotators and more may be found HERE",
    "url": "/docs/en/spark-nlp",
    "relUrl": "/docs/en/spark-nlp"
  },
  "88": {
    "id": "88",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/split_clean_medical_text",
    "relUrl": "/split_clean_medical_text"
  },
  "89": {
    "id": "89",
    "title": "Home Page",
    "content": "When you login to the Annotation Lab, you will be presented with the list of available projects you have created (My Projects) or that have been shared with you (Shared With Me). Some basic information are shown for each project the total number of tasks and the creation date. The list of projects can be sorted ascending or descending on the project creation date. Project Grouping As the number of projects can grow significantly over time, for an easier management and organization of those, the Annotation Lab allows project grouping. As such, it is possible to assign a project to an existing group or to a new group. Each group can be assigned a color which will be used to highlight projects included in that group. Once a project is assigned to a group, the group name proceeds the name of the project. At any time a project can be remove from one group and added to another group. The list of visible projects can be filtered based on group name, or using the search functionality which applies to both group name and project name.",
    "url": "/docs/en/alab/start_page",
    "relUrl": "/docs/en/alab/start_page"
  },
  "90": {
    "id": "90",
    "title": "Tasks",
    "content": "The Tasks screen shows a list of all documents that have been imported into the current project. Under each task you can see meta data about the task: the time of import, the user who imported the task and the annotators and reviewers assigned to the task. Task Assignment Project Owners/Managers can assign tasks to annotator(s) and reviewer(s) in order to better plan/distribute project work. Annotators and Reviewers can only view tasks that are assigned to them which means there is no chance of accidental work overlap. For assigning a task to an annotator, from the task page select one or more tasks and from the Assign dropdown choose an annotator. You can only assign a task to annotators that have already been added to the project. For adding an annotator to the project, go to the Setup page and share the project with the annotator by giving him/her the update right. Once an annotator is assigned to a task his/her name will be listed below the task name on the tasks screen. When upgrading from an older version of the Annotation Lab, the annotators will no longer have access to the tasks they worked on unless they will be assigned to those explicitely by the admin user who created the project. Once they are assigned, they can resume work and no information will be lost. Task Status At high level, each task can have one of the following statuses: Incomplete, when none of the assigned annotators has started working on the task. In Progress, when at least one of the assigned annotators has submitted at least one completion for this task. Submitted, when all annotators which were assigned to the task have submitted a completion which is set as ground truth (starred). Reviewed, in the case there is a reviewer assigned to the task, and the reviewer has reviewed and accepted the submited completion. To Correct, in the case the assigned reviewer has rejected the completion created by the Annotator. The status of a task varies according to the type of account the logged in user has (his/her visibility over the project) and according to the tasks that have been assigned to him/her. For Project Owner, Manager and Reviewer On the Analytics page and Tasks page, the Project Owner/Manager/Reviewer will see the general overview of the projects which will take into consideration the task level statuses as follows: Incomplete - Assigned annotators have not started working on this task In Progress - At least one annotator still has not starred (marked as ground truth) one submitted completion Submitted - All annotators that are assigned to the task have starred (marked as ground truth) one submitted completion Reviewed - Reviewer has approved all starred submitted completions for the task For Annotators On the Annotator’s Task page, the task status will be shown with regards to the context of the logged-in Annotator’s work. As such, if the same task is assigned to two annotators then: if annotator1 is still working and not submitted the task, then he/she will see task status as In-progress if annotator2 submits the task from his/her side then he/she will see task status as Submitted The following statuses are available on the Annotator’s view. Incomplete – Current logged-in annotator has not started working on this task. In Progress - At least one saved/submitted completions exist, but there is no starred submitted completion. Submitted - Annotator has at least one starred submitted completion. Reviewed - Reviewer has approved the starred submitted completion for the task. To Correct - Reviewer has rejected the submitted work. In this case, the star is removed from the reviewed completion. The annotator should start working on the task and resubmit. Note: The status of a task is maintained/available only for the annotators assigned to the task. When multiple Annotators are assigned to a task, the reviewer will see the task as submitted when all annotators submit and star their completions. Otherwise, if one of the assigned Annotators has not submitted or has not starred one completion, then the Reviewer will see the task as In Progress. View As Feature For users that have multiple roles (e.g. Annotator and Reviewer or Reviewer and Manager), the task status can get confusing. In order to elliminate all posible confusions, the View As filter was added. When selecting View As Annotator option, the task statuses are updated as if the only role the currently logged-in user has is Annotator. The same applies to View As Reviewer and View as Manager. This option is available only if the currently logged-in user has multiple roles. Task Filters As normally annotation projects involve a large number of tasks, the Task page includes filtering and sorting options which will help the user identify the tasks he/she needs faster. Tasks can be sorted by time of import ascending or descending. Tasks can be filtered by the assigned tags, by the user who imported the task and by the status. There is also a search functionality which will identify the tasks having a given string on their name. The number of tasks visible on the screeen is customizable by selecting the predefined values from the Tasks per page drop-down. Search tasks by label:token Annotation Lab supports the identification of tasks based on the annotations they include. Currently supported search queries: label: ABC -&gt; returns all tasks that have at least one completion containing a chunk with label ABC label: ABC=DEF -&gt; returns all tasks that have at least one completion containing the text DEF labeled as ABC choice: Mychoice -&gt; returns all tasks that have at least one completion which classified as Mychoice Search functionality is case insensitive, thus the following queries label: ABC=DEF , label: Abc=Def or label: abc=def are considered equivalent. Example: Consider a system with 3 tasks which are annotated as below: Search-query “label:person” will list as results task 0 and task 1. Search-query “label:location” will list as result task 2. Search-query “label:person=the water” will listas result task 1. Comments Comment can be added to each task by Project Owner or Manager. This is done by clicking the comment icon present on the rightmost side of each Task in the Tasks List page. It is important to notice that these comments are visible to everyone who can view the particular task. *Dark icon = A comment is present on the task. Light icon = No comment is present",
    "url": "/docs/en/alab/tasks",
    "relUrl": "/docs/en/alab/tasks"
  },
  "91": {
    "id": "91",
    "title": "Third Party Projects",
    "content": "There are third party projects that can integrate with Spark NLP. These packages need to be installed separately to be used. If you’d like to integrate your application with Spark NLP, please send us a message! Logging Comet Comet is a meta machine learning platform designed to help AI practitioners and teams build reliable machine learning models for real-world applications by streamlining the machine learning model lifecycle. By leveraging Comet, users can track, compare, explain and reproduce their machine learning experiments. Comet can easily integrated into the Spark NLP workflow with the a dedicated logging class CometLogger to log training and evaluation metrics, pipeline parameters and NER visualization made with sparknlp-display. For more information see the User Guide and for more examples see the Spark NLP Workshop. Python API: CometLogger Show Example # Metrics while training an annotator can be logged with for example: import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.logging.comet import CometLogger spark = sparknlp.start() OUTPUT_LOG_PATH = &quot;./run&quot; logger = CometLogger() document = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) embds = ( UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) ) multiClassifier = ( MultiClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;labels&quot;) .setBatchSize(128) .setLr(1e-3) .setThreshold(0.5) .setShufflePerEpoch(False) .setEnableOutputLogs(True) .setOutputLogsPath(OUTPUT_LOG_PATH) .setMaxEpochs(1) ) logger.monitor(logdir=OUTPUT_LOG_PATH, model=multiClassifier) trainDataset = spark.createDataFrame( [(&quot;Nice.&quot;, [&quot;positive&quot;]), (&quot;That&#39;s bad.&quot;, [&quot;negative&quot;])], schema=[&quot;text&quot;, &quot;labels&quot;], ) pipeline = Pipeline(stages=[document, embds, multiClassifier]) pipeline.fit(trainDataset) logger.end() # If you are using a jupyter notebook, it is possible to display the live web # interface with logger.experiment.display(tab=&#39;charts&#39;)",
    "url": "/docs/en/third-party-projects",
    "relUrl": "/docs/en/third-party-projects"
  },
  "92": {
    "id": "92",
    "title": "Training",
    "content": "Training Datasets These are classes to load common datasets to train annotators for tasks such as part-of-speech tagging, named entity recognition, spell checking and more. {% include_relative training_entries/pos.md %} {% include_relative training_entries/conll.md %} {% include_relative training_entries/conllu.md %} {% include_relative training_entries/pubtator.md %} Spell Checkers Dataset (Corpus) In order to train a Norvig or Symmetric Spell Checkers, we need to get corpus data as a spark dataframe. We can read a plain text file and transforms it to a spark dataset. Example: {% include programmingLanguageSelectScalaPython.html %} train_corpus = spark.read .text(&quot;./sherlockholmes.txt&quot;) .withColumnRenamed(&quot;value&quot;, &quot;text&quot;) val trainCorpus = spark.read .text(&quot;./sherlockholmes.txt&quot;) .select(trainCorpus.col(&quot;value&quot;).as(&quot;text&quot;)) Text Processing These are annotators that can be trained to process text for tasks such as dependency parsing, lemmatisation, part-of-speech tagging, sentence detection and word segmentation. {% include_relative training_entries/DependencyParser.md %} {% include_relative training_entries/Lemmatizer.md %} {% include_relative training_entries/Perceptron.md %} {% include_relative training_entries/SentenceDetectorDL.md %} {% include_relative training_entries/TypedDependencyParser.md %} {% include_relative training_entries/WordSegmenter.md %} Spell Checkers These are annotators that can be trained to correct text. {% include_relative training_entries/ContextSpellChecker.md %} {% include_relative training_entries/NorvigSweeting.md %} {% include_relative training_entries/SymmetricDelete.md %} Token Classification These are annotators that can be trained to recognize named entities in text. {% include_relative training_entries/NerCrf.md %} {% include_relative training_entries/NerDL.md %} Text Classification These are annotators that can be trained to classify text into different classes, such as sentiment. {% include_relative training_entries/ClassifierDL.md %} {% include_relative training_entries/MultiClassifierDL.md %} {% include_relative training_entries/SentimentDL.md %} {% include_relative training_entries/ViveknSentiment.md %} Text Representation These are annotators that can be trained to turn text into a numerical representation. {% include_relative training_entries/Doc2VecApproach.md %} External Trainable Models These are annotators that are trained in an external library, which are then loaded into Spark NLP. {% include_relative training_entries/BertForTokenClassification.md %} {% include_relative training_entries/DistilBertForTokenClassification.md %} TensorFlow Graphs NER DL uses Char CNNs - BiLSTM - CRF Neural Network architecture. Spark NLP defines this architecture through a Tensorflow graph, which requires the following parameters: Tags Embeddings Dimension Number of Chars Spark NLP infers these values from the training dataset used in NerDLApproach annotator and tries to load the graph embedded on spark-nlp package. Currently, Spark NLP has graphs for the most common combination of tags, embeddings, and number of chars values: Tags Embeddings Dimension 10 100 10 200 10 300 10 768 10 1024 25 300 All of these graphs use an LSTM of size 128 and number of chars 100 In case, your train dataset has a different number of tags, embeddings dimension, number of chars and LSTM size combinations shown in the table above, NerDLApproach will raise an IllegalArgumentException exception during runtime with the message below: Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Check https://nlp.johnsnowlabs.com/docs/en/graph for instructions to generate the required graph. To overcome this exception message we have to follow these steps: Clone spark-nlp github repo Run python file create_models with number of tags, embeddings dimension and number of char values mentioned on your exception message error. cd spark-nlp/python/tensorflow export PYTHONPATH=lib/ner python create_models.py [number_of_tags] [embeddings_dimension] [number_of_chars] [output_path] This will generate a graph on the directory defined on `output_path argument. Retry training with NerDLApproach annotator but this time use the parameter setGraphFolder with the path of your graph. Note: Make sure that you have Python 3 and Tensorflow 1.15.0 installed on your system since create_models requires those versions to generate the graph successfully. Note: We also have a notebook in the same directory if you prefer Jupyter notebook to cerate your custom graph.",
    "url": "/docs/en/training",
    "relUrl": "/docs/en/training"
  },
  "93": {
    "id": "93",
    "title": "Transformers",
    "content": "{% assign parent_path = “en/transformer_entries” %} {% for file in site.static_files %} {% if file.path contains parent_path %} {% assign file_name = file.path | remove: parent_path | remove: “/” | prepend: “transformer_entries/” %} {% include_relative {{ file_name }} %} {% endif %} {% endfor %} Import Transformers into Spark NLP Overview We have extended support for HuggingFace 🤗 and TF Hub exported models since 3.1.0 to equivalent Spark NLP 🚀 annotators. Starting this release, you can easily use the saved_model feature in HuggingFace within a few lines of codes and import any BERT, DistilBERT, RoBERTa, XLM-RoBERTa, Longformer, BertForTokenClassification, DistilBertForTokenClassification, AlbertForTokenClassification, RoBertaForTokenClassification, XlmRoBertaForTokenClassification, XlnetForTokenClassification, LongformerForTokenClassification, BertForSequenceClassification, and DistilBertForSequenceClassification models to Spark NLP. We will work on the remaining annotators and extend this support to the rest with each release 😊 Compatibility Spark NLP: The equivalent annotator in Spark NLP TF Hub: Models from TF Hub HuggingFace: Models from HuggingFace Model Architecture: Which architecture is compatible with that annotator Flags: Fully supported ✅ Partially supported (requires workarounds) ✔️ Under development ❎ Not supported ❌ Spark NLP TF Hub HuggingFace Model Architecture BertEmbeddings ✅ ✅ BERT - Small BERT - ELECTRA BertSentenceEmbeddings ✅ ✅ BERT - Small BERT - ELECTRA DistilBertEmbeddings   ✅ DistilBERT RoBertaEmbeddings   ✅ RoBERTa - DistilRoBERTa XlmRoBertaEmbeddings   ✅ XLM-RoBERTa AlbertEmbeddings ✅ ✅ ALBERT XlnetEmbeddings   ✅ XLNet LongformerEmbeddings   ✅ Longformer ElmoEmbeddings ❎ ❎   UniversalSentenceEncoder ❎     BertForTokenClassification   ✅ TFBertForTokenClassification DistilBertForTokenClassification   ✅ TFDistilBertForTokenClassification AlbertForTokenClassification   ✅ TFAlbertForTokenClassification RoBertaForTokenClassification   ✅ TFRobertaForTokenClassification XlmRoBertaForTokenClassification   ✅ TFXLMRobertaForTokenClassification XlnetForTokenClassification   ✅ TFXLNetForTokenClassificationet LongformerForTokenClassification   ✅ TFLongformerForTokenClassification BertForSequenceClassification   ✅ TFBertForSequenceClassification DistilBertForSequenceClassification   ✅ TFDistilBertForSequenceClassification T5Transformer   ❌   MarianTransformer   ❌   Example Notebooks HuggingFace to Spark NLP Spark NLP HuggingFace Notebooks Colab BertEmbeddings HuggingFace in Spark NLP - BERT BertSentenceEmbeddings HuggingFace in Spark NLP - BERT Sentence DistilBertEmbeddings HuggingFace in Spark NLP - DistilBERT RoBertaEmbeddings HuggingFace in Spark NLP - RoBERTa XlmRoBertaEmbeddings HuggingFace in Spark NLP - XLM-RoBERTa AlbertEmbeddings HuggingFace in Spark NLP - ALBERT XlnetEmbeddings HuggingFace in Spark NLP - XLNet LongformerEmbeddings HuggingFace in Spark NLP - Longformer BertForTokenClassification HuggingFace in Spark NLP - BertForTokenClassification DistilBertForTokenClassification HuggingFace in Spark NLP - DistilBertForTokenClassification AlbertForTokenClassification HuggingFace in Spark NLP - AlbertForTokenClassification RoBertaForTokenClassification HuggingFace in Spark NLP - RoBertaForTokenClassification XlmRoBertaForTokenClassification HuggingFace in Spark NLP - XlmRoBertaForTokenClassification BertForSequenceClassification HuggingFace in Spark NLP - BertForSequenceClassification DistilBertForSequenceClassification HuggingFace in Spark NLP - DistilBertForSequenceClassification TF Hub to Spark NLP Spark NLP TF Hub Notebooks Colab BertEmbeddings TF Hub in Spark NLP - BERT BertSentenceEmbeddings TF Hub in Spark NLP - BERT Sentence AlbertEmbeddings TF Hub in Spark NLP - ALBERT",
    "url": "/docs/en/transformers",
    "relUrl": "/docs/en/transformers"
  },
  "94": {
    "id": "94",
    "title": "Video Tutorials",
    "content": "Add a new user. Ida Lucente - January, 2021 Update password from User Profile. Ida Lucente - January, 2021 Collect the client secret. Ida Lucente - January, 2021 Setup 2FA. Ida Lucente - January, 2021 API usage example. Ida Lucente - January, 2021",
    "url": "/docs/en/alab/tutorials",
    "relUrl": "/docs/en/alab/tutorials"
  },
  "95": {
    "id": "95",
    "title": "User Management",
    "content": "Basic user management features are included in the Annotation Lab. The admin user can add or remove a user from the data base or can edit user information if necessary. This feature is available by navigating to the lower left side menu and selecting User Management feature. All users that have beed added to the current Annotation Lab instance can be seen on the Users screen. First Name, Last Name and e-mail address information should be available for all users. An admin user can edit those information, add a user to a group or change a user’s password. User Details Annotation Lab stores basic information for each user, such as: the First Name, Last Name, E-mail address. Those can be edited from the User Details page by any Admin User. User Groups Currently two user groups are available: Annotators and UserAdmins. By default a new user is added to the Annotators group. This means the user will not have access to any admin features such as: User Management or Settings. For adding a user to the Admin group, an admin user needs to navigate to the Users screen, click on the edit button for the concerned user, then click on the Groups tab and check the Admin checkbox. Reset User Credentials An admin user can change the login credentials for another user, by navigating to the User Credentials tab and by defining a new (temporary) password. For extra protection, the admin user can also enforce the password change on next user login.",
    "url": "/docs/en/alab/user_management",
    "relUrl": "/docs/en/alab/user_management"
  },
  "96": {
    "id": "96",
    "title": "Version Compatibility",
    "content": "Healthcare NLP Spark OCR Spark NLP 3.0.0 3.4.0 3.0.0 3.0.1 3.4.0 3.0.1 3.0.2 3.4.0 3.0.2 3.0.3 3.4.0 3.0.3 3.1.0 3.4.1 3.1.0 3.1.1 3.4.1 3.1.1 3.1.2 3.4.1 3.1.2 3.1.3 3.5.0 3.1.3 3.1.3 3.6.0 3.1.3 3.2.0 3.7.0 3.2.0 3.2.1 3.8.0 3.2.1",
    "url": "/docs/en/version_compatibility",
    "relUrl": "/docs/en/version_compatibility"
  },
  "97": {
    "id": "97",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/visual_document_understanding",
    "relUrl": "/visual_document_understanding"
  },
  "98": {
    "id": "98",
    "title": "Visual NER",
    "content": "Annotating text included in image documents (e.g. scanned documents) is a common use case in many verticals but comes with several challenges. With the new Visual NER Labeling config, we aim to ease the work of annotators by allowing them to simply select text from an image and assign the corresponding label to it. This feature is powered by Spark OCR 3.5.0; thus a valid Spark OCR license is required to get access to it. Here is how this can be used: Upload a valid Spark OCR license. See how to do this here. Create a new project, specify a name for your project, add team members if necessary, and from the list of predefined templates (Default Project Configs) choose “Visual NER Labeling”. Update the configuration if necessary. This might be useful if you want to use other labels than the currently defined ones. Click the save button. While saving the project, a confirmation dialog is displayed to let you know that the Spark OCR pipeline for Visual NER is being deployed. Import the tasks you want to annotate (images). Start annotating text on top of the image by clicking on the text tokens or by drawing bounding boxes on top of chunks or image areas. Export annotations in your preferred format. The entire process is illustrated below: Support for multi-page PDF documents When a valid Saprk OCR license is available, Annotation Lab offers support for multi-page PDF annotation. The complete flow of import, annotation, and export for multi-page PDF files is currently supported. Users have two options for importing a new PDF file into the Visual NER project Import PDF file from local storage; Add a link to the PDF file in the file attribute. After import, the task becomes available on the Tasks Page. The title of the new task is the name of the imported file. On the labeling page, the PDF file is displayed with pagination so that annotators can annotate on the PDF document one page at a time.",
    "url": "/docs/en/alab/visual_ner",
    "relUrl": "/docs/en/alab/visual_ner"
  },
  "99": {
    "id": "99",
    "title": "Workflow Setup",
    "content": "When a team of people work together on a large annotation project, the tasks can be organized into a multi-step workflow for an easier management of the team collaboration. This is also necessary when the project has strict requirements on the labels: e.g. the same document must be labeled by multiple annotators; the annotations must be checked by a senior annotator. In this situation, a workflow can be setup using the task tagging functionality provided by the Annotation Lab. Then can be used for splitting work across the team but also for differentiating between first-level annotators and second-level reviewers. A full audit trail is kept on each action performed by all actors. Each saved entry is stored along with an authenticated user and a time stamp, and the user interface includes a visual comparison between versions. To add a tag, select a task and press Tags &gt; Add more. Tasks can be filtered by tags, making it easier to identify, for example, which documents are completed and which ones need to be reviewed.",
    "url": "/docs/en/alab/workflow",
    "relUrl": "/docs/en/alab/workflow"
  },
  "100": {
    "id": "100",
    "title": "Workflow Setup",
    "content": "When a team of people work together on a large annotation project, the tasks can be organized into a multi-step workflow for an easier management of the team collaboration. This is also necessary when the project has strict requirements on the labels: e.g. the same document must be labeled by multiple annotators; the annotations must be checked by a senior annotator. In this situation, a workflow can be setup using the task tagging functionality provided by the Annotation Lab. Then can be used for splitting work across the team but also for differentiating between first-level annotators and second-level reviewers. A full audit trail is kept on each action performed by all actors. Each saved entry is stored along with an authenticated user and a time stamp, and the user interface includes a visual comparison between versions. To add a tag, select a task and press Tags &gt; Add more. Tasks can be filtered by tags, making it easier to identify, for example, which documents are completed and which ones need to be reviewed.",
    "url": "/docs/en/workflow",
    "relUrl": "/docs/en/workflow"
  }
  
}
