{
  "0": {
    "id": "0",
    "title": "404",
    "content": "404 Page not found :( &lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; Translate Zande (individual language) to Swedish Pipeline- Spark NLP Model HomeDocsLearnModelsDemo John Snow Labs Jun 04, 2021 Translate Zande (individual language) to Swedish Pipelineopen_sourcepipelineseq2seqtranslationznesvxxmultilingual Description Marian is an efficient, free Neural Machine Translation framework written in pure C++ with minimal dependencies. It is mainly being developed by the Microsoft Translator team. Many academic (most notably the University of Edinburgh and in the past the Adam Mickiewicz University in Poznań) and commercial contributors help with its development. It is currently the engine behind the Microsoft Translator Neural Machine Translation services and being deployed by many companies, organizations and research projects (see below for an incomplete list). source languages: zne target languages: sv Live Demo Open in Colab Download How to use PythonScalaNLU from sparknlp.pretrained import PretrainedPipeline pipeline = PretrainedPipeline(&quot;translate_zne_sv&quot;, lang = &quot;xx&quot;) pipeline.annotate(&quot;Your sentence to translate!&quot;) import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val pipeline = new PretrainedPipeline(&quot;translate_zne_sv&quot;, lang = &quot;xx&quot;) pipeline.annotate(&quot;Your sentence to translate!&quot;) import nlu text = [&quot;text to translate&quot;] translate_df = nlu.load(&#39;xx.Zande (individual language).translate_to.Swedish&#39;).predict(text, output_level=&#39;sentence&#39;) translate_df Model Information Model Name: translate_zne_sv Type: pipeline Compatibility: Spark NLP 3.1.0+ License: Open Source Edition: Official Language: xx Data Source https://huggingface.co/Helsinki-NLP/opus-mt-zne-sv Included Models DocumentAssembler SentenceDetectorDLModel MarianTransformer PREVIOUSTranslate Zande (individual language) to French Pipeline © 2021 John Snow Labs Inc. Terms of Service | Privacy Policy &lt;/html&gt;",
    "url": "/404.html",
    "relUrl": "/404.html"
  },
  "1": {
    "id": "1",
    "title": "Active Learning",
    "content": "A Project Owner or a Manager can use the completed tasks (completions) from a project for training a new Spark NLP model. The training feature can be found on the Setup page. Named Entity Recognition Projects Named Entity Recognition (NER) projects usually include several labels. When the annotation team has generated a relevant sample of training data/examples for each one of the labels the Project Owner/Manager can use this data to train an DL model which can then be used to predict the labels on new tasks. The NER models can be easily trained as illustrated below. The “Train Now” button (See Arrow 6) can be used to trigger training of a new model when no other trainings or preannotations are in progress. Otherwise, the button is disabled. Information on the training progress is shown in the top right corner of Model Training tab. Here the user can get indications on the success or failure message depending on how the last training ended. When triggering the training, users are prompted to choose either to immediately deploy models or just do training. If immediate deployment is chosen, then the Labeling config is updated according to the name of the new model (item 1 on the above image). It is possible to download training logs by clicking on the download logs icon (see item 9 on the above image) of the recently trained NER model which includes information like training parameters and TF graph used along with precision, recall, f1 score, etc. Training parameters It is possible to tune the most common training parameters (Validation split ratio, Epoch, Learning rate, Decay, Dropout, and Batch) by editing their values on the popup window activated by the gear icon (see item 4 on the above image). It is also possible to train a model by using a sublist of tasks with predefined tags. This is done by specifing the targeted Tags on the Training Parameters popup window (last option). Selection of Completions During the annotation project lifetime, normally not all tasks/completions are ready to be used as a training dataset. This is why the training process selects completions based on their status: Filter tasks by tags (if defined in Training Parameters window, otherwise all tasks are considered) For completed tasks, completions to be taken into account are also selected based on the following criteria: If a task has a completion accepted by a reviewer this is selected for training and all others are ignored Completions rejected by a Reviewer are not used for training If no reviewer is assigned to a task that has multiple submitted completions the most recent completion is selected for training purpose Assertion Status Projects NER configurations for the healthcare domain are often mixed with Assertion Status labels. In this case Annotation Lab offers support for training both types of models in one go. After the training is complete, the models will be listed in the Spark NLP Pipeline Config. On mouse over the model name in the Spark NLP pipeline config, the user can see more information about the model such as when it was trained and if the training was manually initiated or by the Active Learning process. Once the model(s) has been trained, the project configuration will be automatically updated to reference the new model for prediction. Notice below, for the Assertion Status **** tag the addition of model attribute to indicate which model will be used for task preannotation for this label. &lt;Label value=&quot;Absent&quot; assertion=&quot;true&quot; model=&quot;assertion_jsl_annotation_manual.model&quot;/&gt; &lt;Label value=&quot;Past&quot; assertion=&quot;true&quot; model=&quot;assertion_jsl_annotation_manual.model&quot;/&gt; It is not possible to mark a label as an Assertion Status label and use a NER model to predict it. A validation error is shown in the Interface Preview in case an invalid Assertion model is used. The Annotation Lab only allows the use of one single Assertion Status model in the same project. Classification Project Models Training Annotation Lab supports two types of classification training Single Choice Classification and Multi-Choice Classification. For doing so, it uses three important attributes of the **** tag to drive the Classification Models training and preannotation. Those are **name**, **choice** and **train**. Attribute name The attribute name allows the naming of the different choices present in the project configuration, and thus the training of separate models based on the same project annotations. For example, in the sample configuration illustrated below, the name=”age” attribute, tells the system to only consider age-related classification information when training an Age Classifier. The value specified by the name attribute is also used to name the resulting Classification model (classification_age_annotation_manual). Attribute choice The choice attribute specifies the type of model that will be trained: multiple or single. For example, in the Labeling Config below, Age and Gender are Single Choice Classification categories while the Smoking Status is Multi-Choice Classification. Depending upon the value of this attribute, the respective model will be trained as a Single Choice Classifier or Multi-Choice Classifier. &lt;View&gt; &lt;View style=&quot;overflow: auto;&quot;&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;/&gt; &lt;/View&gt; &lt;Header value=&quot;Smoking Status&quot;/&gt; &lt;Choices name=&quot;smokingstatus&quot; toName=&quot;text&quot; choice=&quot;multiple&quot; showInLine=&quot;true&quot;&gt; &lt;Choice value=&quot;Smoker&quot;/&gt; &lt;Choice value=&quot;Past Smoker&quot;/&gt; &lt;Choice value=&quot;Nonsmoker&quot;/&gt; &lt;/Choices&gt; &lt;Header value=&quot;Age&quot;/&gt; &lt;Choices name=&quot;age&quot; toName=&quot;text&quot; choice=&quot;single&quot; showInLine=&quot;true&quot;&gt; &lt;Choice value=&quot;Child (less than 18y)&quot; hotkey=&quot;c&quot;/&gt; &lt;Choice value=&quot;Adult (19-50y)&quot; hotkey=&quot;a&quot;/&gt; &lt;Choice value=&quot;Aged (50+y)&quot; hotkey=&quot;o&quot;/&gt; &lt;/Choices&gt; &lt;Header value=&quot;Gender&quot;/&gt; &lt;Choices name=&quot;gender&quot; toName=&quot;text&quot; choice=&quot;single&quot; showInLine=&quot;true&quot;&gt; &lt;Choice value=&quot;Female&quot; hotkey=&quot;f&quot;/&gt; &lt;Choice value=&quot;Male&quot; hotkey=&quot;m&quot;/&gt; &lt;/Choices&gt; &lt;/View&gt; Attribute train This version of Annotation Lab restricts the training of two or more Classification Models at the same time. If there are multiple Classification categories in a project (like the one above), only the category whose name comes first in alphabetical order will be trained by default. In the above example, based on the value of the name attribute, we conclude that the Age classifier model is trained. The model to be trained can also be specified by setting the train=”true” attribute for the targeted tag (like the one defined in Gender category below). &lt;View&gt; &lt;View style=&quot;overflow: auto;&quot;&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;/&gt; &lt;/View&gt; &lt;Header value=&quot;Smoking Status&quot;/&gt; &lt;Choices name=&quot;smokingstatus&quot; toName=&quot;text&quot; choice=&quot;multiple&quot; showInLine=&quot;true&quot;&gt; ... &lt;/Choices&gt; &lt;Header value=&quot;Age&quot;/&gt; &lt;Choices name=&quot;age&quot; toName=&quot;text&quot; choice=&quot;single&quot; showInLine=&quot;true&quot;&gt; ... &lt;/Choices&gt; &lt;Header value=&quot;Gender&quot;/&gt; &lt;Choices name=&quot;gender&quot; train=&quot;true&quot; toName=&quot;text&quot; choice=&quot;single&quot; showInLine=&quot;true&quot;&gt; ... &lt;/Choices&gt; &lt;/View&gt; The trained classification models are also available on the Spark NLP pipeline config list. Mixed Projects If a project is set up to include Classification, Named Entity Recognition and Assertion Status labels and the three kinds of annotations are present in the training data, it is possible to train three models: one for Named Entity Recognition, one for Assertion Status, and one for Classification at the same time. The training logs from all three trainings can be downloaded at once by clicking the download button present in the Training section of the Setup Page. The newly trained models will be added to the Spark NLP pipeline config. Active Learning Project Owners or Managers can enable the Active Learning feature by clicking on the corresponding Switch (item 7 on the above image) available on Model Training tab. If this feature is enabled, the NER training gets triggered automatically on every 50 new completions. It is also possible to change the completions frequency by dropdown (8) which is visible only when Active Learning is enabled. While enabling this feature, users are asked whether they want to deploy the newly trained model right after the training process or not. If the user chooses not to automatically deploy the newly trained model, this can be done on demand by navigating to the Spark NLP pipeline config and filtering the model by name of the project (3) and select that new model trained by Active Learning. This will update the Labeling Config (name of the model in tag is changed). Hovering on each trained model will show the training date and time. If the user opts for deploying the model after the training, the Project Configuration is automatically updated for each label that is not associated with a pretrained Spark NLP model, the model information is updated with the name of the new model. If there is any mistake in the name of models, the validation error is displayed in the Interface Preview Section present on the right side of the Labeling Config area.",
    "url": "/docs/en/active_learning",
    "relUrl": "/docs/en/active_learning"
  },
  "2": {
    "id": "2",
    "title": "Annotation Lab",
    "content": "Lightning fast annotation A highly efficient annotation tool for all enterprise teams who need to: annotate documents; build ML models &amp; get them to production; All that without writing a line of code! Included with every Spark NLP subscription, or can be purchased on its own. Try Free Productivity Bootstrap annotations with Spark NLP models Keep Annotators in the zone Reach agreement quickly Auto NLP Active learning - no data scientist required Deliver a model not just annotations Build for high compliance environments Teamwork Projects &amp; teams Customizable workflows High security Resources General tutorials Annotation best practices Tips and tricks",
    "url": "/docs/en/alab",
    "relUrl": "/docs/en/alab"
  },
  "3": {
    "id": "3",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/analyze_non_english_medical_text",
    "relUrl": "/analyze_non_english_medical_text"
  },
  "4": {
    "id": "4",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/analyze_non_english_text_documents",
    "relUrl": "/analyze_non_english_text_documents"
  },
  "5": {
    "id": "5",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/analyze_spelling_grammar",
    "relUrl": "/analyze_spelling_grammar"
  },
  "6": {
    "id": "6",
    "title": "Text Annotation",
    "content": "Overview The Annotator Lab is designed to keep a human expert as productive as possible. It minimizes the number of mouse clicks, keystrokes, and eye movements in the main workflow, based on iterative feedback from daily users. Keyboard shortcuts are supported for all annotations – this enables having one hand on keyboard, one hand on mouse, and eyes on screen at all time. One-click completion and automated switching to the next task keeps experts in the loop. On the upper side of the Labeling screen, you can find the list of labels defined for the project. In the center of the screen the content of the task is displayed. On the right side there are several widgets: Completions Predictions Results Completions A completion is a list of annotations manually defined by a user for a given task. When the work on a task is done (e.g. all entities have been highlighted in the document or the task has been assigned one or more classes in the case of classification projects) the user clicks on the Save button. Starting Annotation Lab 1.2.0, we introduced the idea of completion submission. In the past, annotators could change or delete completions as many times as they wanted with no restriction. From now on, a submitted completion is no longer editable and cannot be deleted. Creating a new copy of the submitted completion is the only option to edit it. An annotator can modify or delete his/her completions only if the completions are not submitted yet. This is an important feature for ensuring a complete audit trail of all user actions. Now, it is possible to track the history and details of any deleted completions, which was not possible in previous releases. This means it is possible to see the name of the completion creator, date of creation, and deletion. Predictions A prediction is a list of annotations created automatically, via the use of Spark NLP pretrained models. Predictions are created using the “Preannotate” button form the Task view. Predictions are read only - users can see them but cannot modify them in any way. Results The results widget has two sections. The first section - Regions - gives a list overview of all annotated chunks. When you click on one annotation it gets automatically highlighted in the document. Annotations can be edited or removed if necessary. The second section - Relations - lists all the relations that have been labeled. When the user moves the mouse over one relation it is highlighted in the document. NER Labels To extract information using NER labels, first click on the label to select it or press the shortcut key assigned to it and then, with the mouse, select the relevant part of the text. Wrong extractions can be easily edited by clicking on them to select them and then by selecting the new label you want to assign to the text. For deleting a label, select it by clicking on it and press backspace. Assertion Labels To add an assertion label to an extracted entity, select the label and use it to do the same exact extraction as the NER label. After this, the extracted entity will have two labels: one for NER and one for assertion. In the example below, the chunks “heart disease”, “kidney disease”, “stroke” etc. ware extracted using first a blue yellow label and then a red assertion label (Absent). Relation Extraction Creating relations with the Annotation Lab is very simple. First select one labeled entity, then press r and select the second labeled entity. You can add a label to the relation, change its direction or delete it using the relation box. Pre-annotations When you setup a project to use existing Spark NLP models for pre-annotation, you can run the designated models on all of your tasks by pressing the Preannotate button located on the upper side of the task view. As a result, all predicted labels for a given task will be available in the Prediction widget, on the main annotation screen. The predictions are not editable, you can only view them and navigate them or compare them with older predictions. However, you can create a new completion based on a given prediction. All labels and relations from such a new completion are now editable.",
    "url": "/docs/en/annotation",
    "relUrl": "/docs/en/annotation"
  },
  "7": {
    "id": "7",
    "title": "Annotators",
    "content": "How to read this section All annotators in Spark NLP share a common interface, this is: Annotation: Annotation(annotatorType, begin, end, result, meta-data, embeddings) AnnotatorType: some annotators share a type. This is not only figurative, but also tells about the structure of the metadata map in the Annotation. This is the one referred in the input and output of annotators. Inputs: Represents how many and which annotator types are expected in setInputCols(). These are column names of output of other annotators in the DataFrames. Output Represents the type of the output in the column setOutputCol(). There are two types of Annotators: Approach: AnnotatorApproach extend Estimators, which are meant to be trained through fit() Model: AnnotatorModel extend from Transformers, which are meant to transform DataFrames through transform() Model suffix is explicitly stated when the annotator is the result of a training process. Some annotators, such as Tokenizer are transformers, but do not contain the word Model since they are not trained annotators. Model annotators have a pretrained() on it’s static object, to retrieve the public pre-trained version of a model. pretrained(name, language, extra_location) -&gt; by default, pre-trained will bring a default model, sometimes we offer more than one model, in this case, you may have to use name, language or extra location to download them. The types are: AnnotatorType AnnotatorType DOCUMENT = “document” DATE = “date” TOKEN = “token” ENTITY = “entity” WORDPIECE = “wordpiece” NEGEX = “negex” WORD_EMBEDDINGS = “word_embeddings” DEPENDENCY = “dependency” SENTENCE_EMBEDDINGS = “sentence_embeddings” KEYWORD = “keyword” CATEGORY = “category” LABELED_DEPENDENCY = “labeled_dependency” SENTIMENT = “sentiment” LANGUAGE = “language” POS = “pos” CHUNK = “chunk” NAMED_ENTITY = “named_entity”   Annotator Description Version Tokenizer Identifies tokens with tokenization open standards Opensource WordSegmenter Trainable annotator for word segmentation of languages without any rule-based tokenization such as Chinese, Japanese, or Korean Opensource Normalizer Removes all dirty characters from text Opensource DocumentNormalizer Cleaning content from HTML or XML documents Opensource Stemmer Returns hard-stems out of words with the objective of retrieving the meaningful part of the word Opensource Lemmatizer Retrieves lemmas out of words with the objective of returning a base dictionary word Opensource StopWordsCleaner This annotator excludes from a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences Opensource RegexMatcher Uses a reference file to match a set of regular expressions and put them inside a provided key. Opensource TextMatcher Annotator to match entire phrases (by token) provided in a file against a Document Opensource Chunker Matches a pattern of part-of-speech tags in order to return meaningful phrases from document Opensource NGramGenerator integrates Spark ML NGram function into Spark ML with a new cumulative feature to also generate range ngrams like the scikit-learn library Opensource DateMatcher Reads from different forms of date and time expressions and converts them to a provided date format Opensource MultiDateMatcher Reads from multiple different forms of date and time expressions and converts them to a provided date format Opensource SentenceDetector Finds sentence bounds in raw text. Applies rules from Pragmatic Segmenter Opensource POSTagger Sets a Part-Of-Speech tag to each word within a sentence. Opensource ViveknSentimentDetector Scores a sentence for a sentiment Opensource SentimentDetector Scores a sentence for a sentiment Opensource WordEmbeddings Word Embeddings lookup annotator that maps tokens to vectors Opensource BertEmbeddings BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture Opensource BertSentenceEmbeddings This annotator generates sentence embeddings from all BERT models Opensource ElmoEmbeddings Computes contextualized word representations using character-based word representations and bidirectional LSTMs Opensource AlbertEmbeddings Computes contextualized word representations using “A Lite” implementation of BERT algorithm by applying parameter-reduction techniques Opensource XlnetEmbeddings Computes contextualized word representations using combination of Autoregressive Language Model and Permutation Language Model Opensource UniversalSentenceEncoder Encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks. Opensource SentenceEmbeddings utilizes WordEmbeddings or BertEmbeddings to generate sentence or document embeddings Opensource ChunkEmbeddings utilizes WordEmbeddings or BertEmbeddings to generate chunk embeddings from either Chunker, NGramGenerator, or NerConverter outputs Opensource ClassifierDL Multi-class Text Classification. ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 100 classes Opensource MultiClassifierDL Multi-label Text Classification. MultiClassifierDL uses a Bidirectional GRU with Convolution model that we have built inside TensorFlow and supports up to 100 classes. Opensource SentimentDL Multi-class Sentiment Analysis Annotator. SentimentDL is an annotator for multi-class sentiment analysis. This annotator comes with 2 available pre-trained models trained on IMDB and Twitter datasets Opensource T5Transformer for Text-To-Text Transfer Transformer (Google T5) models to achieve state-of-the-art results on multiple NLP tasks such as Translation, Summarization, Question Answering, Sentence Similarity, and so on Opensource MarianTransformer Neural Machine Translation based on MarianNMT models being developed by the Microsoft Translator team Opensource LanguageDetectorDL State-of-the-art language detection and identification annotator trained by using TensorFlow/keras neural networks Opensource YakeModel Yake is an Unsupervised, Corpus-Independent, Domain and Language-Independent and Single-Document keyword extraction algorithm. Opensource NerDL Named Entity recognition annotator allows for a generic model to be trained by utilizing a deep learning algorithm (Char CNNs - BiLSTM - CRF - word embeddings) Opensource NerCrf Named Entity recognition annotator allows for a generic model to be trained by utilizing a CRF machine learning algorithm Opensource NorvigSweeting SpellChecker This annotator retrieves tokens and makes corrections automatically if not found in an English dictionary Opensource SymmetricDelete SpellChecker This spell checker is inspired on Symmetric Delete algorithm Opensource Context SpellChecker Implements Noisy Channel Model Spell Algorithm. Correction candidates are extracted combining context information and word information Opensource DependencyParser Unlabeled parser that finds a grammatical relation between two words in a sentence Opensource TypedDependencyParser Labeled parser that finds a grammatical relation between two words in a sentence Opensource PubTator reader Converts automatic annotations of the biomedical datasets into Spark DataFrame Opensource Tokenizer Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. Output Annotator Type: Token Input Annotator Types: Document Note: all these APIs receive regular expressions so please make sure that you escape special characters according to Java conventions. Example: PythonScala tokenizer = Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;token&quot;) .setSplitChars([&#39;-&#39;]) .setContextChars([&#39;(&#39;, &#39;)&#39;, &#39;?&#39;, &#39;!&#39;]) .setExceptions([&quot;New York&quot;, &quot;e-mail&quot;]) .setSplitPattern(&quot;&#39;&quot;) .setMaxLength(0) .setMaxLength(99999) .setCaseSensitiveExceptions(False) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) .setContextChars(Array(&quot;(&quot;, &quot;)&quot;, &quot;?&quot;, &quot;!&quot;)) .setSplitChars(Array(&#39;-&#39;)) .setExceptions([&quot;New York&quot;, &quot;e-mail&quot;]) .setSplitPattern(&quot;&#39;&quot;) .setMaxLength(0) .setMaxLength(99999) .setCaseSensitiveExceptions(False) API: Tokenizer Source: Tokenizer DocumentNormalizer (Text cleaning) Annotator which normalizes raw text from tagged text, e.g. scraped web pages or xml documents, from document type columns into Sentence. Output Annotator Type: Document Input Annotator Types: Document Example: PythonScala documentNormalizer = DocumentNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;normalizedDocument&quot;) .setPatterns(cleanUpPatterns) .setPolicy(removalPolicy) val documentNormalizer = new DocumentNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;normalizedDocument&quot;) .setPatterns(cleanUpPatterns) .setPolicy(removalPolicy) API: DocumentNormalizer Source: DocumentNormalizer Normalizer (Text cleaning) Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary Output Annotator Type: Token Input Annotator Types: Token Example: PythonScala normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) .setLowercase(True) .setCleanupPatterns([&quot;[^ w d s]&quot;]) .setSlangMatchCase(False) val normalizer = new Normalizer() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;normalized&quot;) .setLowercase(True) .setCleanupPatterns([&quot;[^ w d s]&quot;]) .setSlangMatchCase(False) API: Normalizer Source: Normalizer Stemmer Returns hard-stems out of words with the objective of retrieving the meaningful part of the word Output Annotator Type: Token Input Annotator Types: Token Example: PythonScala stemmer = Stemmer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;stem&quot;) .setLanguage(&quot;English&quot;) val stemmer = new Stemmer() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;stem&quot;) .setLanguage(&quot;English&quot;) API: Stemmer Source: Stemmer Lemmatizer Retrieves lemmas out of words with the objective of returning a base dictionary word Output Annotator Type: Token Input Annotator Types: Token Example: PythonScala # Uncomment to Download the Dictionary # !wget -q https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt lemmatizer = Lemmatizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;./AntBNC_lemmas_ver_001.txt&quot;, value_delimiter =&quot; t&quot;, key_delimiter = &quot;-&gt;&quot;) // Uncomment to Download the Dictionary // !wget -q https://raw.githubusercontent.com/mahavivo/vocabulary/master/lemmas/AntBNC_lemmas_ver_001.txt val lemmatizer = new Lemmatizer() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;./AntBNC_lemmas_ver_001.txt&quot;, value_delimiter =&quot; t&quot;, key_delimiter = &quot;-&gt;&quot;) API: Lemmatizer Source: Lemmatizer StopWordsCleaner This annotator excludes from a sequence of strings (e.g. the output of a Tokenizer(), Normalizer(), Lemmatizer(), and Stemmer()) and drops all the stop words from the input sequences. Output Annotator Type: token Input Annotator Types: token Example: PythonScala stop_words_cleaner = StopWordsCleaner() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setStopWords([&quot;this&quot;, &quot;is&quot;, &quot;and&quot;]) .setCaseSensitive(False) val stopWordsCleaner = new StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setStopWords(Array(&quot;this&quot;, &quot;is&quot;, &quot;and&quot;)) .setCaseSensitive(false) NOTE: If you need to setStopWords from a text file, you can first read and convert it into an array of string as follows. PythonScala # your stop words text file, each line is one stop word stopwords = sc.textFile(&quot;/tmp/stopwords/english.txt&quot;).collect() # simply use it in StopWordsCleaner stopWordsCleaner = StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setStopWords(stopwords) .setCaseSensitive(False) # or you can use pretrained models for StopWordsCleaner stopWordsCleaner = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) // your stop words text file, each line is one stop word val stopwords = sc.textFile(&quot;/tmp/stopwords/english.txt&quot;).collect() // simply use it in StopWordsCleaner val stopWordsCleaner = new StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setStopWords(stopwords) .setCaseSensitive(false) // or you can use pretrained models for StopWordsCleaner val stopWordsCleaner = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) API: StopWordsCleaner Source: StopWordsCleaner RegexMatcher Uses a reference file to match a set of regular expressions and put them inside a provided key. File must be comma separated. Output Annotator Type: Regex Input Annotator Types: Document Example: PythonScala # For example, here are some Regex Rules which you can write in regex_rules.txt rules = &#39;&#39;&#39; renal s w+, started with &#39;renal&#39; cardiac s w+, started with &#39;cardiac&#39; w*ly b, ending with &#39;ly&#39; S* d+ S*, match any word that contains numbers ( d+).?( d*) s*(mg|ml|g), match medication metrics &#39;&#39;&#39; regex_matcher = RegexMatcher() .setStrategy(&quot;MATCH_ALL&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;regex&quot;) .setExternalRules(path=&#39;./regex_rules.txt&#39;, delimiter=&#39;,&#39;) val regexMatcher = new RegexMatcher() .setStrategy(&quot;MATCH_ALL&quot;) .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;regex&quot;) API: RegexMatcher Source: RegexMatcher RegexMatcherModel TextMatcher (Phrase matching) Annotator to match entire phrases (by token) provided in a file against a Document Output Annotator Type: Entity Input Annotator Types: Document, Token Example: PythonScala # For example, here are some entities and they are stored in sport_entities.txt entities = [&#39;soccer&#39;, &#39;world cup&#39;, &#39;Messi&#39;, &#39;FC Barcelona&#39;, &#39;cricket&#39;, &#39;Dhoni&#39;] entity_extractor = TextMatcher() .setInputCols([&quot;inputCol&quot;]) .setOutputCol(&quot;entity&quot;) .setEntities(&quot;/path/to/file/sport_entities.txt&quot;) .setEntityValue(&#39;sport_entity&#39;) .setCaseSensitive(True) .setMergeOverlapping(False) // Assume following are our entities and they are stored in sport_entities.txt entities = (&quot;soccer&quot;, &quot;world cup&quot;, &quot;Messi&quot;, &quot;FC Barcelona&quot;, &quot;cricket&quot;, &quot;Dhoni&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;inputCol&quot;) .setOutputCol(&quot;entity&quot;) .setEntities(&quot;/path/to/file/myentities.txt&quot;) .setEntityValue(&quot;sport_entity&quot;) .setCaseSensitive(true) .setMergeOverlapping(false) API: TextMatcher Source: TextMatcher TextMatcherModel Chunker This annotator matches a pattern of part-of-speech tags in order to return meaningful phrases from document Output Annotator Type: Chunk Input Annotator Types: Document, POS Example: PythonScala chunker = Chunker() .setInputCols([&quot;document&quot;, &quot;pos&quot;]) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;‹NNP›+&quot;, &quot;‹DT|PP $›?‹JJ›*‹NN›&quot;]) val chunker = new Chunker() .setInputCols(Array(&quot;document&quot;, &quot;pos&quot;)) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;‹NNP›+&quot;, &quot;‹DT|PP $›?‹JJ›*‹NN›&quot;)) API: Chunker Source: Chunker NGramGenerator NGramGenerator annotator takes as input a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Stemmer, Lemmatizer, and StopWordsCleaner). The parameter n is used to determine the number of terms in each n-gram. The output will consist of a sequence of n-grams where each n-gram is represented by a space-delimited string of n consecutive words with annotatorType CHUNK same as the Chunker annotator. Output Annotator Type: CHUNK Input Annotator Types: TOKEN Reference: NGramGenerator Example: PythonScala ngrams_cum = NGramGenerator() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;ngrams&quot;) .setN(2) .setEnableCumulative(True) .setDelimiter(&quot;_&quot;) # Default is space val nGrams = new NGramGenerator() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;ngrams&quot;) .setN(2) .setEnableCumulative(true) .setDelimiter(&quot;_&quot;) // Default is space DateMatcher Reads from different forms of date and time expressions and converts them to a provided date format. Extracts only ONE date per sentence. Use with sentence detector for more matches. Output Annotator Type: Date Input Annotator Types: Document Reads the following kind of dates: Format Format Format 1978-01-28 last wednesday 5 am tomorrow 1984/04/02 today 0600h 1/02/1980 tomorrow 06:00 hours 2/28/79 yesterday 6pm The 31st of April in the year 2008 next week at 7.30 5:30 a.m. Fri, 21 Nov 1997 next week at 5 Jan 21, ‘97 next month 12:59 Sun, Nov 21 next year 1988/11/23 6pm jan 1st day after 23:59 next thursday the day before   Example: PythonScala date_matcher = DateMatcher() .setInputCols(&#39;document&#39;) .setOutputCol(&quot;date&quot;) .setDateFormat(&quot;yyyy/MM/dd&quot;) val dateMatcher = new DateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setFormat(&quot;yyyyMM&quot;) API: DateMatcher Source: DateMatcher MultiDateMatcher Reads from multiple different forms of date and time expressions and converts them to a provided date format. Extracts multiple dates per sentence. Output Annotator Type: Date Input Annotator Types: Document Reads the following kind of dates: Format Format Format 1978-01-28 jan 1st day after 1984/04/02 next thursday the day before 1978-01-28 last wednesday 0600h 1988/11/23 6pm today 06:00 hours 1/02/1980 tomorrow 6pm 2/28/79 yesterday 5:30 a.m. The 31st of April in the year 2008 at 5 next week at 7.30 Fri, 21 Nov 1997 next week 12:59 Jan 21, ‘97 next month 23:59 Sun, Nov 21 next year 5 am tomorrow Example: PythonScala date_matcher = MultiDateMatcher() .setInputCols(&#39;document&#39;) .setOutputCol(&quot;date&quot;) .setDateFormat(&quot;yyyy/MM/dd&quot;) val dateMatcher = new MultiDateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setFormat(&quot;yyyyMM&quot;) API: MultiDateMatcher Source: MultiDateMatcher SentenceDetector Finds sentence bounds in raw text. Applies rules from Pragmatic Segmenter. Output Annotator Type: Sentence Input Annotator Types: Document Example: PythonScala sentence_detector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) API: SentenceDetector Source: SentenceDetector POSTagger (Part of speech tagger) Sets a POS tag to each word within a sentence. Its train data (train_pos) is a spark dataset of POS format values with Annotation columns. Output Annotator Type: POS Input Annotator Types: Document, Token Example: PythonScala pos_tagger = PerceptronApproach() .setInputCols([&quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;pos&quot;) .setNIterations(2) .setFrequencyThreshold(30) val posTagger = new PerceptronApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;pos&quot;) .setNIterations(2) .setFrequencyThreshold(30) API: PerceptronApproach Source: PerceptronApproach PerceptronModel ViveknSentimentDetector Scores a sentence for a sentiment Output Annotator Type: sentiment Input Annotator Types: Document, Token Example: Train your own model PythonScala sentiment_detector = ViveknSentimentApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;sentiment&quot;) .setSentimentCol(&quot;sentiment_label&quot;) .setCorpusPrune(0) .setImportantFeatureRatio(16.66) val sentimentDetector = new ViveknSentimentApproach() .setInputCols(Array(&quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;vivekn&quot;) .setSentimentCol(&quot;sentiment_label&quot;) .setCorpusPrune(0) .setImportantFeatureRatio(16.66) Use a pretrained model PythonScala sentiment_detector = ViveknSentimentModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;sentiment&quot;) val sentimentDetector = new ViveknSentimentModel.pretrained .setInputCols(Array(&quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;vivekn&quot;) API: ViveknSentimentApproach Source: ViveknSentimentApproach ViveknSentimentModel SentimentDetector (Sentiment analysis) Scores a sentence for a sentiment Output Annotator Type: Sentiment Input Annotator Types: Document, Token Example: PythonScala sentiment_detector = SentimentDetector() .setInputCols([&quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;sentiment&quot;) .setPositiveMultiplier(1.0) .setNegativeMultiplier(-1.0) .setIncrementMultiplier(2.0) .setDecrementMultiplier(-2.0) .setReverseMultiplier(-1.0) val sentimentDetector = new SentimentDetector .setInputCols(Array(&quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;sentiment&quot;) .setPositiveMultiplier(1.0) .setNegativeMultiplier(-1.0) .setIncrementMultiplier(2.0) .setDecrementMultiplier(-2.0) .setReverseMultiplier(-1.0) API: SentimentDetector Reference: SentimentDetector SentimentDetectorModel WordEmbeddings Word Embeddings lookup annotator that maps tokens to vectors Output Annotator Type: Word_Embeddings Input Annotator Types: Document, Token Example: PythonScala embeddings = WordEmbeddings() .setStoragePath(&quot;/tmp/glove.6B.100d.txt&quot;, &quot;TEXT&quot;) .setDimension(100) .setStorageRef(&quot;glove_100d&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) # or you can use the pretrained models for WordEmbeddings embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddings = new WordEmbeddings() .setStoragePath(&quot;/tmp/glove.6B.100d.txt&quot;, &quot;TEXT&quot;) .setDimension(100) .setStorageRef(&quot;glove_100d&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // or you can use the pretrained models for WordEmbeddings val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) There are also two convenient functions to retrieve the embeddings coverage with respect to the transformed dataset: withCoverageColumn(dataset, embeddingsCol, outputCol): Adds a custom column with word coverage stats for the embedded field: (coveredWords, totalWords, coveragePercentage). This creates a new column with statistics for each row. overallCoverage(dataset, embeddingsCol): Calculates overall word coverage for the whole data in the embedded field. This returns a single coverage object considering all rows in the field. API: WordEmbeddings Source: WordEmbeddings WordEmbeddingsModel BertEmbeddings BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture You can find the pre-trained models for BertEmbeddings in the Spark NLP Models repository Output Annotator Type: Word_Embeddings Input Annotator Types: Document, Token Example: PythonScala bert = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;bert&quot;) val bert = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;bert&quot;) API: BertEmbeddings Source: BertEmbeddings BertSentenceEmbeddings BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture You can find the pre-trained models for BertEmbeddings in the Spark NLP Models repository Output Annotator Type: Sentence_Embeddings Input Annotator Types: Document Example: How to use pretrained BertEmbeddings: PythonScala bert = BertSentencembeddings.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;bert_sentence_embeddings&quot;) val bert = BertEmbeddings.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;bert_sentence_embeddings&quot;) API: BertSentenceEmbeddings Source: BertSentenceEmbeddings ElmoEmbeddings Computes contextualized word representations using character-based word representations and bidirectional LSTMs You can find the pre-trained model for ElmoEmbeddings in the Spark NLP Models repository Output Annotator Type: Word_Embeddings Input Annotator Types: Document, Token Example: PythonScala # Online - Download the pretrained model elmo = ElmoEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;elmo&quot;) # Offline - Download the pretrained model manually and extract it elmo = ElmoEmbeddings.load(&quot;/elmo_en_2.4.0_2.4_1580488815299&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;elmo&quot;) val elmo = ElmoEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;elmo&quot;) .setPoolingLayer(&quot;elmo&quot;) // word_emb, lstm_outputs1, lstm_outputs2 or elmo API: ElmoEmbeddings Source: ElmoEmbeddings AlbertEmbeddings Computes contextualized word representations using “A Lite” implementation of BERT algorithm by applying parameter-reduction techniques You can find the pre-trained model for AlbertEmbeddings in the Spark NLP Models repository Output Annotator Type: Word_Embeddings Input Annotator Types: Document, Token Examples: PythonScala # Online - Download the pretrained model albert = AlbertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;albert&quot;) # Offline - Download the pretrained model manually and extract it albert = AlbertEmbeddings.load(&quot;/albert_base_uncased_en_2.5.0_2.4_1588073363475&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;albert&quot;) val albert = AlbertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;albert&quot;) API: AlbertEmbeddings Source: AlbertEmbeddings XlnetEmbeddings Computes contextualized word representations using combination of Autoregressive Language Model and Permutation Language Model You can find the pre-trained model for XlnetEmbeddings in the Spark NLP Models repository Output Annotator Type: Word_Embeddings Input Annotator Types: Document, Token Example: How to use pretrained XlnetEmbeddings: PythonScala # Online - Download the pretrained model xlnet = XlnetEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;xlnet&quot;) # Offline - Download the pretrained model manually and extract it xlnet = XlnetEmbeddings.load(&quot;/xlnet_large_cased_en_2.5.0_2.4_1588074397954&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;xlnet&quot;) val xlnet = XlnetEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;xlnet&quot;) API: XlnetEmbeddings Source: XlnetEmbeddings UniversalSentenceEncoder The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks. Output Annotator Type: SENTENCE_EMBEDDINGS Input Annotator Types: Document Example: PythonScala use = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;use_embeddings&quot;) val use = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;use_embeddings&quot;) API: UniversalSentenceEncoder Source: UniversalSentenceEncoder SentenceEmbeddings This annotator converts the results from WordEmbeddings, BertEmbeddings, ElmoEmbeddings, AlbertEmbeddings, or XlnetEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols). Output Annotator Type: SENTENCE_EMBEDDINGS Input Annotator Types: Document Example: PythonScala sentence_embeddings = SentenceEmbeddings() .setInputCols([&quot;document&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) val embeddingsSentence = new SentenceEmbeddings() .setInputCols(Array(&quot;document&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;sentence_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) NOTE: If you choose document as your input for Tokenizer, WordEmbeddings/BertEmbeddings, and SentenceEmbeddings then it averages/sums all the embeddings into one array of embeddings. However, if you choose sentence as inputCols then for each sentence SentenceEmbeddings generates one array of embeddings. TIP: Here is how you can explode and convert these embeddings into Vectors or what’s known as Feature column so it can be used in Spark ML regression or clustering functions PythonScala from org.apache.spark.ml.linal import Vector, Vectors from pyspark.sql.functions import udf # Let&#39;s create a UDF to take array of embeddings and output Vectors @udf(Vector) def convertToVectorUDF(matrix): return Vectors.dense(matrix.toArray.map(_.toDouble)) # Now let&#39;s explode the sentence_embeddings column and have a new feature column for Spark ML pipelineDF.select(explode(&quot;sentence_embeddings.embeddings&quot;).as(&quot;sentence_embedding&quot;)) .withColumn(&quot;features&quot;, convertToVectorUDF(&quot;sentence_embedding&quot;)) import org.apache.spark.ml.linalg.{Vector, Vectors} // Let&#39;s create a UDF to take array of embeddings and output Vectors val convertToVectorUDF = udf((matrix : Seq[Float]) =&gt; { Vectors.dense(matrix.toArray.map(_.toDouble)) }) // Now let&#39;s explode the sentence_embeddings column and have a new feature column for Spark ML pipelineDF.select(explode($&quot;sentence_embeddings.embeddings&quot;).as(&quot;sentence_embedding&quot;)) .withColumn(&quot;features&quot;, convertToVectorUDF($&quot;sentence_embedding&quot;)) API: SentenceEmbeddings Source: SentenceEmbeddings ChunkEmbeddings This annotator utilizes WordEmbeddings or BertEmbeddings to generate chunk embeddings from either Chunker, NGramGenerator, or NerConverter outputs. Output Annotator Type: CHUNK Input Annotator Types: CHUNK, Word_Embeddings Example: PythonScala chunk_embeddings = ChunkEmbeddings() .setInputCols([&quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;chunk_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) val chunkSentence = new ChunkEmbeddings() .setInputCols(Array(&quot;chunk&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;chunk_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) TIP: Here is how you can explode and convert these embeddings into Vectors or what’s known as Feature column so it can be used in Spark ML regression or clustering functions PythonScala from org.apache.spark.ml.linal import Vector, Vectors from pyspark.sql.functions import udf // Let&#39;s create a UDF to take array of embeddings and output Vectors @udf(Vector) def convertToVectorUDF(matrix): return Vectors.dense(matrix.toArray.map(_.toDouble)) import org.apache.spark.ml.linalg.{Vector, Vectors} // Let&#39;s create a UDF to take array of embeddings and output Vectors val convertToVectorUDF = udf((matrix : Seq[Float]) =&gt; { Vectors.dense(matrix.toArray.map(_.toDouble)) }) // Now let&#39;s explode the sentence_embeddings column and have a new feature column for Spark ML pipelineDF.select(explode($&quot;chunk_embeddings.embeddings&quot;).as(&quot;chunk_embeddings_exploded&quot;)) .withColumn(&quot;features&quot;, convertToVectorUDF($&quot;chunk_embeddings_exploded&quot;)) API: ChunkEmbeddings Source: ChunkEmbeddings ClassifierDL (Multi-class Text Classification) ClassifierDL is a generic Multi-class Text Classification. ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 100 classes Output Annotator Type: CATEGORY Input Annotator Types: SENTENCE_EMBEDDINGS NOTE: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. NOTE: UniversalSentenceEncoder, BertSentenceEmbeddings, or SentenceEmbeddings can be used for the inputCol Example: PythonScala docClassifier = ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(0.5) .setDropout(0.5) val docClassifier = new ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(5e-3f) .setDropout(0.5f) Please refer to existing notebooks for more examples. API: ClassifierDLApproach Source: ClassifierDLApproach ClassifierDLModel MultiClassifierDL (Multi-label Text Classification) MultiClassifierDL is a Multi-label Text Classification. MultiClassifierDL uses a Bidirectional GRU with Convolution model that we have built inside TensorFlow and supports up to 100 classes. The input to MultiClassifierDL is Sentence Embeddings such as state-of-the-art UniversalSentenceEncoder, BertSentenceEmbeddings, or SentenceEmbeddings Output Annotator Type: CATEGORY Input Annotator Types: SENTENCE_EMBEDDINGS NOTE: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. NOTE: UniversalSentenceEncoder, BertSentenceEmbeddings, or SentenceEmbeddings can be used for the inputCol Example: PythonScala docMultiClassifier = MultiClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(0.5) val docMultiClassifier = new MultiClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(5e-3f) Please refer to existing notebooks for more examples. API: MultiClassifierDLApproach Source: MultiClassifierDLApproach MultiClassifierDLModel SentimentDL (Multi-class Sentiment Analysis annotator) SentimentDL is an annotator for multi-class sentiment analysis. This annotator comes with 2 available pre-trained models trained on IMDB and Twitter datasets Output Annotator Type: CATEGORY Input Annotator Types: SENTENCE_EMBEDDINGS NOTE: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. NOTE: UniversalSentenceEncoder, BertSentenceEmbeddings, or SentenceEmbeddings can be used for the inputCol Example: PythonScala sentimentClassifier = SentimentDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(0.5) .setDropout(0.5) val sentimentClassifier = new SentimentDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(5e-3f) .setDropout(0.5f) Please refer to existing notebooks for more examples. API: SentimentDLApproach Source: SentimentDLApproach SentimentDLModel LanguageDetectorDL (Language Detection and Identiffication) LanguageDetectorDL is a state-of-the-art language detection and identification annotator trained by using TensorFlow/keras neural networks. Output Annotator Type: LANGUAGE Input Annotator Types: DOCUMENT or SENTENCE Example: PythonScala languageDetector = LanguageDetectorDL.pretrained(&quot;ld_wiki_20&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;language&quot;) .setThreshold(0.3) .setCoalesceSentences(True) val languageDetector = LanguageDetectorDL.pretrained(&quot;ld_wiki_20&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;language&quot;) .setThreshold(0.3f) .setCoalesceSentences(true) API: LanguageDetectorDL Source: LanguageDetectorDL YakeModel (Keywords Extraction) Yake is an Unsupervised, Corpus-Independent, Domain and Language-Independent and Single-Document keyword extraction algorithm. sExtracting keywords from texts has become a challenge for individuals and organizations as the information grows in complexity and size. The need to automate this task so that text can be processed in a timely and adequate manner has led to the emergence of automatic keyword extraction tools. Yake is a novel feature-based system for multi-lingual keyword extraction, which supports texts of different sizes, domain or languages. Unlike other approaches, Yake does not rely on dictionaries nor thesauri, neither is trained against any corpora. Instead, it follows an unsupervised approach which builds upon features extracted from the text, making it thus applicable to documents written in different languages without the need for further knowledge. This can be beneficial for a large number of tasks and a plethora of situations where access to training corpora is either limited or restricted. The algorithm makes use of the position of a sentence and token. Therefore, to use the annotator, the text should be first sent through a Sentence Boundary Detector and then a tokenizer. You can tweak the following parameters to get the best result from the annotator. Output Annotator Type: KEYWORD Input Annotator Types: TOKEN Example: PythonScala keywords = YakeModel() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;keywords&quot;) .setMinNGrams(1) .setMaxNGrams(3) .setNKeywords(20) .setStopWords(stopwords) val keywords = new YakeModel() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;keywords&quot;) .setMinNGrams(1) .setMaxNGrams(3) .setNKeywords(20) .setStopWords(stopwords) API: YakeModel Source: YakeModel NER CRF (Named Entity Recognition CRF annotator) This Named Entity recognition annotator allows for a generic model to be trained by utilizing a CRF machine learning algorithm. Its train data (train_ner) is either a labeled or an external CoNLL 2003 IOB based spark dataset with Annotations columns. Also the user has to provide word embeddings annotation column. Optionally the user can provide an entity dictionary file for better accuracy Output Annotator Type: Named_Entity Input Annotator Types: Document, Token, POS, Word_Embeddings Example: PythonScala nerTagger = NerCrfApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMinEpochs(1) .setMaxEpochs(20) .setLossEps(1e-3) .setDicts([&quot;ner-corpus/dict.txt&quot;]) .setL2(1) .setC0(1250000) .setRandomSeed(0) .setVerbose(2) val nerTagger = new NerCrfApproach() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;) .setLabelColumn(&quot;label&quot;) .setMinEpochs(1) .setMaxEpochs(3) .setC0(34) .setL2(3.0) .setOutputCol(&quot;ner&quot;) API: NerCrfApproach Source: NerCrfApproach NerCrfModel NER DL (Named Entity Recognition Deep Learning annotator) This Named Entity recognition annotator allows to train generic NER model based on Neural Networks. Its train data (train_ner) is either a labeled or an external CoNLL 2003 IOB based spark dataset with Annotations columns. Also the user has to provide word embeddings annotation column. Neural Network architecture is Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. Output Annotator Type: Named_Entity Input Annotator Types: Document, Token, Word_Embeddings Note: Please check here in case you get an IllegalArgumentException error with a description such as: Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Generate graph by python code in python/tensorflow/ner/create_models before usage and use setGraphFolder Param to point to output. Example: PythonScala nerTagger = NerDLApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) .setRandomSeed(0) .setVerbose(2) val nerTagger = new NerDLApproach() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) .setLabelColumn(&quot;label&quot;) .setMaxEpochs(120) .setRandomSeed(0) .setPo(0.03f) .setLr(0.2f) .setDropout(0.5f) .setBatchSize(9) .setVerbose(Verbose.Epochs) API: NerDLApproach Source: NerDLApproach NerDLModel NER Converter (Converts IOB or IOB2 representation of NER to user-friendly) NER Converter used to finalize work of NER annotators. Combines entites with types B-, I- and etc. to the Chunks with Named entity in the metadata field (if LightPipeline is used can be extracted after fullAnnotate()) This NER converter can be used to the output of a NER model into the ner chunk format. Output Annotator Type: Chunk Input Annotator Types: Document, Token, Named_Entity Example: PythonScala nerConverter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_src&quot;]) .setOutputCol(&quot;ner_chunk&quot;) val nerConverter = new NerConverter() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner_src&quot;) .setOutputCol(&quot;ner_chunk&quot;) API: NerConverter Source: NerConverter Norvig SpellChecker This annotator retrieves tokens and makes corrections automatically if not found in an English dictionary Output Annotator Type: Token Input Annotator Types: Token Inputs: Any text for corpus. A list of words for dictionary. A comma separated custom dictionary. Train Data: train_corpus is a spark dataset of text content Example: PythonScala spell_checker = NorvigSweetingApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;checked&quot;) .setDictionary(&quot;coca2017.txt&quot;, &quot;[a-zA-Z]+&quot;) val symSpellChecker = new NorvigSweetingApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;checked&quot;) .setDictionary(&quot;coca2017.txt&quot;, &quot;[a-zA-Z]+&quot;) API: NorvigSweetingApproach Source: NorvigSweetingApproach NorvigSweetingModel Symmetric SpellChecker This spell checker is inspired on Symmetric Delete algorithm. It retrieves tokens and utilizes distance metrics to compute possible derived words Output Annotator Type: Token Input Annotator Types: Token Inputs: Any text for corpus. A list of words for dictionary. A comma separated custom dictionary. Train Data: train_corpus is a spark dataset of text content Example: PythonScala spell_checker = SymmetricDeleteApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;spell&quot;) val spellChecker = new SymmetricDeleteApproach() .setInputCols(Array(&quot;normalized&quot;)) .setOutputCol(&quot;spell&quot;) API: SymmetricDeleteApproach Source: SymmetricDeleteApproach SymmetricDeleteModel Context SpellChecker Implements Noisy Channel Model Spell Algorithm. Correction candidates are extracted combining context information and word information Output Annotator Type: Token Input Annotator Types: Token Inputs: Any text for corpus. A list of words for dictionary. A comma separated custom dictionary. Train Data: train_corpus is a spark dataset of text content Example: PythonScala spell_checker = ContextSpellCheckerApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;spell&quot;) .fit(train_corpus) .setErrorThreshold(4.0) .setTradeoff(6.0) val spellChecker = new ContextSpellCheckerApproach() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;spell&quot;) .fit(trainCorpus) .setErrorThreshold(4.0) .setTradeoff(6.0) API: ContextSpellCheckerApproach Source: ContextSpellCheckerApproach ContextSpellCheckerModel Dependency Parsers Dependency parser provides information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The following diagram illustrates a dependency-style analysis using the standard graphical method favored in the dependency-parsing community. Relations among the words are illustrated above the sentence with directed, labeled arcs from heads to dependents. We call this a typed dependency structure because the labels are drawn from a fixed inventory of grammatical relations. It also includes a root node that explicitly marks the root of the tree, the head of the entire structure. [1] Untyped Dependency Parser (Unlabeled grammatical relation) Unlabeled parser that finds a grammatical relation between two words in a sentence. Its input is a directory with dependency treebank files. Output Annotator Type: Dependency Input Annotator Types: Document, POS, Token Example: PythonScala dependency_parser = DependencyParserApproach() .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency&quot;) .setDependencyTreeBank(&quot;file://parser/dependency_treebank&quot;) .setNumberOfIterations(10) val dependencyParser = new DependencyParserApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;)) .setOutputCol(&quot;dependency&quot;) .setDependencyTreeBank(&quot;parser/dependency_treebank&quot;) .setNumberOfIterations(10) API: DependencyParserApproach Source: DependencyParserApproach DependencyParserModel Typed Dependency Parser (Labeled grammatical relation) Labeled parser that finds a grammatical relation between two words in a sentence. Its input is a CoNLL2009 or ConllU dataset. Output Annotator Type: Labeled Dependency Input Annotator Types: Token, POS, Dependency Example: PythonScala typed_dependency_parser = TypedDependencyParserApproach() .setInputCols([&quot;token&quot;, &quot;pos&quot;, &quot;dependency&quot;]) .setOutputCol(&quot;labdep&quot;) .setConll2009(&quot;file://conll2009/eng.train&quot;) .setNumberOfIterations(10) val typedDependencyParser = new TypedDependencyParserApproach() .setInputCols(Array(&quot;token&quot;, &quot;pos&quot;, &quot;dependency&quot;)) .setOutputCol(&quot;labdep&quot;) .setConll2009(&quot;conll2009/eng.train&quot;)) API: TypedDependencyParserApproach Source: TypedDependencyParserApproach TypedDependencyParserModel References [1] Speech and Language Processing. Daniel Jurafsky &amp; James H. Martin. 2018",
    "url": "/docs/en/annotators",
    "relUrl": "/docs/en/annotators"
  },
  "8": {
    "id": "8",
    "title": "Helper functions",
    "content": "Spark NLP Annotation functions The functions presented here help users manipulate annotations, by providing both UDFs and dataframe utilities to deal with them more easily Python In python, the functions are straight forward and have both UDF and Dataframe applications map_annotations(f, output_type: DataType) UDF that applies f(). Requires output DataType from pyspark.sql.types map_annotations_strict(f) UDF that apples an f() method that returns a list of Annotations map_annotations_col(dataframe: DataFrame, f, column: str, output_column: str, annotatyon_type: str, output_type: DataType = Annotation.arrayType()) applies f() to column from dataframe map_annotations_cols(dataframe: DataFrame, f, columns: str, output_column: str, annotatyon_type: str, output_type: DataType = Annotation.arrayType()) applies f() to columns from dataframe filter_by_annotations_col(dataframe, f, column) applies a boolean filter f() to column from dataframe explode_annotations_col(dataframe: DataFrame, column, output_column) explodes annotation column from dataframe Scala In Scala, importing inner functions brings implicits that allow these functions to be applied directly on top of the dataframe mapAnnotations(function: Seq[Annotation] =&gt; T, outputType: DataType) mapAnnotationsStrict(function: Seq[Annotation] =&gt; Seq[Annotation]) mapAnnotationsCol[T: TypeTag](column: String, outputCol: String,annotatorType: String, function: Seq[Annotation] =&gt; T) mapAnnotationsCol[T: TypeTag](cols: Seq[String], outputCol: String,annotatorType: String, function: Seq[Annotation] =&gt; T) eachAnnotationsCol[T: TypeTag](column: String, function: Seq[Annotation] =&gt; Unit) def explodeAnnotationsCol[T: TypeTag](column: String, outputCol: String) Imports: PythonScala from sparknlp.functions import * from sparknlp.annotation import Annotation import com.johnsnowlabs.nlp.functions._ import com.johnsnowlabs.nlp.Annotation Examples: Complete usage examples can be seen here: https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/234-release-candidate/jupyter/annotation/english/spark-nlp-basics/spark-nlp-basics-functions.ipynb PythonScala val modified = data.mapAnnotationsCol(&quot;pos&quot;, &quot;mod_pos&quot;,&quot;pos&quot; ,(_: Seq[Annotation]) =&gt; { &quot;hello world&quot; }) def my_annoation_map_function(annotations): return list(map(lambda a: Annotation( &#39;my_own_type&#39;, a.begin, a.end, a.result, {&#39;my_key&#39;: &#39;custom_annotation_data&#39;}, []), annotations)) result.select( map_annotations(my_annoation_map_function, Annotation.arrayType())(&#39;token&#39;) ).toDF(&quot;my output&quot;).show(truncate=False)",
    "url": "/docs/en/auxiliary",
    "relUrl": "/docs/en/auxiliary"
  },
  "9": {
    "id": "9",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/classify_documents",
    "relUrl": "/classify_documents"
  },
  "10": {
    "id": "10",
    "title": "General Concepts",
    "content": "Concepts Spark ML provides a set of Machine Learning applications that can be build using two main components: Estimators and Transformers. The Estimators have a method called fit() which secures and trains a piece of data to such application. The Transformer is generally the result of a fitting process and applies changes to the the target dataset. These components have been embedded to be applicable to Spark NLP. Pipelines are a mechanism for combining multiple estimators and transformers in a single workflow. They allow multiple chained transformations along a Machine Learning task. For more information please refer to Spark ML library. Annotation The basic result of a Spark NLP operation is an annotation. It’s structure includes: annotatorType: the type of annotator that generated the current annotation begin: the begin of the matched content relative to raw-text end: the end of the matched content relative to raw-text result: the main output of the annotation metadata: content of matched result and additional information embeddings: (new in 2.0) contains vector mappings if required This object is automatically generated by annotators after a transform process. No manual work is required. However, it is important to clearly understand the structure of an annotation to be able too efficiently use it. Annotators Annotators are the spearhead of NLP functions in Spark NLP. There are two forms of annotators: Annotator Approaches: are those who represent a Spark ML Estimator and require a training stage. They have a function called fit(data) which trains a model based on some data. They produce the second type of annotator which is an annotator model or transformer. Annotator Models: are spark models or transformers, meaning they have a transform(data) function. This function takes as input a dataframe to which it adds a new column containing the result of the current annotation. All transformers are additive, meaning they append to current data, never replace or delete previous information. Both forms of annotators can be included in a Pipeline. All annotators included in a Pipeline will be automatically executed in the defined order and will transform the data accordingly. A Pipeline is turned into a PipelineModel after the fit() stage. The Pipeline can be saved to disk and re-loaded at any time. Common Functions setInputCols(column_names): Takes a list of column names of annotations required by this annotator. Those are generated by the annotators which precede the current annotator in the pipeline. setOutputCol(column_name): Defines the name of the column containing the result of the current annotator. Use this name as an input for other annotators down the pipeline requiring the outputs generated by the current annotator. Quickly annotate some text You can run these examples using Python or Scala. The easiest way to run the python examples is by starting a pyspark jupyter notebook including the spark-nlp package: $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.7 -y $ conda activate sparknlp # spark-nlp by default is based on pyspark 3.x $ pip install spark-nlp==3.1.0 pyspark==3.1.1 jupyter $ jupyter notebook Explain Document ML Spark NLP offers a variety of pretrained pipelines that will help you get started, and get a sense of how the library works. We are constantly working on improving the available content. You can checkout a demo application of the Explain Document ML pipeline here: View Demo Downloading and using a pretrained pipeline Explain Document ML (explain_document_ml) is a pretrained pipeline that does a little bit of everything NLP related. Let’s try it out in scala. Note that the first time you run the below code it might take longer since it downloads the pretrained pipeline from our servers! PythonScala import sparknlp sparknlp.start() from sparknlp.pretrained import PretrainedPipeline explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) annotations = explain_document_pipeline.annotate(&quot;We are very happy about SparkNLP&quot;) print(annotations) OUTPUT: { &#39;stem&#39;: [&#39;we&#39;, &#39;ar&#39;, &#39;veri&#39;, &#39;happi&#39;, &#39;about&#39;, &#39;sparknlp&#39;], &#39;checked&#39;: [&#39;We&#39;, &#39;are&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], &#39;lemma&#39;: [&#39;We&#39;, &#39;be&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], &#39;document&#39;: [&#39;We are very happy about SparkNLP&#39;], &#39;pos&#39;: [&#39;PRP&#39;, &#39;VBP&#39;, &#39;RB&#39;, &#39;JJ&#39;, &#39;IN&#39;, &#39;NNP&#39;], &#39;token&#39;: [&#39;We&#39;, &#39;are&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], &#39;sentence&#39;: [&#39;We are very happy about SparkNLP&#39;] } import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) OUTPUT: explain_document_ml download started this may take some time. Approximate size to download 9.4 MB Download done! Loading the resource. explain_document_pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_ml,en,public/models) val annotations = explainDocumentPipeline.annotate(&quot;We are very happy about SparkNLP&quot;) println(annotations) OUTPUT: Map( stem -&gt; List(we, ar, veri, happi, about, sparknlp), checked -&gt; List(We, are, very, happy, about, SparkNLP), lemma -&gt; List(We, be, very, happy, about, SparkNLP), document -&gt; List(We are very happy about SparkNLP), pos -&gt; ArrayBuffer(PRP, VBP, RB, JJ, IN, NNP), token -&gt; List(We, are, very, happy, about, SparkNLP), sentence -&gt; List(We are very happy about SparkNLP) ) As you can see the explain_document_ml is able to annotate any “document” providing as output a list of stems, check-spelling, lemmas, part of speech tags, tokens and sentence boundary detection and all this “out-of-the-box”!. Using a pretrained pipeline with spark dataframes You can also use the pipeline with a spark dataframe. You just need to create first a spark dataframe with a column named “text” that will work as the input for the pipeline and then use the .transform() method to run the pipeline over that dataframe and store the outputs of the different components in a spark dataframe. Remember than when starting jupyter notebook from pyspark or when running the spark-shell for scala, a Spark Session is started in the background by default within the namespace ‘scala’. PythonScala import sparknlp sparknlp.start() sentences = [ [&#39;Hello, this is an example sentence&#39;], [&#39;And this is a second sentence.&#39;] ] # spark is the Spark Session automatically started by pyspark. data = spark.createDataFrame(sentences).toDF(&quot;text&quot;) # Download the pretrained pipeline from Johnsnowlab&#39;s servers explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) OUTPUT: explain_document_ml download started this may take some time. Approx size to download 9.4 MB [OK!] # Transform &#39;data&#39; and store output in a new &#39;annotations_df&#39; dataframe annotations_df = explain_document_pipeline.transform(data) # Show the results annotations_df.show() OUTPUT: +--+--+--+--+--+--+--+--+ | text| document| sentence| token| checked| lemma| stem| pos| +--+--+--+--+--+--+--+--+ |Hello, this is an...|[[document, 0, 33...|[[document, 0, 33...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, he...|[[pos, 0, 4, UH, ...| |And this is a sec...|[[document, 0, 29...|[[document, 0, 29...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, an...|[[pos, 0, 2, CC, ...| +--+--+--+--+--+--+--+--+ val data = Seq( &quot;Hello, this is an example sentence&quot;, &quot;And this is a second sentence&quot;) .toDF(&quot;text&quot;) data.show(truncate=false) OUTPUT: ++ |text | ++ |Hello, this is an example set | |And this is a second sentence.| ++ val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) val annotations_df = explainDocumentPipeline.transform(data) annotations_df.show() OUTPUT: +--+--+--+--+--+--+--+--+ | text| document| sentence| token| checked| lemma| stem| pos| +--+--+--+--+--+--+--+--+ |Hello, this is an...|[[document, 0, 33...|[[document, 0, 33...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, he...|[[pos, 0, 4, UH, ...| |And this is a sec...|[[document, 0, 29...|[[document, 0, 29...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, an...|[[pos, 0, 2, CC, ...| +--+--+--+--+--+--+--+--+ Manipulating pipelines The output of the previous DataFrame was in terms of Annotation objects. This output is not really comfortable to deal with, as you can see by running the code: PythonScala annotations_df.select(&quot;token&quot;).show(truncate=False) OUTPUT: +--+ |token | +--+ |[[token, 0, 4, Hello, [sentence -&gt; 0], [], []], [token, 5, 5, ,, [sentence -&gt; 0], [], []], [token, 7, 10, this, [sentence -&gt; 0], [], []], [token, 12, 13, is, [sentence -&gt; 0], [], []], [token, 15, 16, an, [sentence -&gt; 0], [], []], [token, 18, 24, example, [sentence -&gt; 0], [], []], [token, 26, 33, sentence, [sentence -&gt; 0], [], []]]| |[[token, 0, 2, And, [sentence -&gt; 0], [], []], [token, 4, 7, this, [sentence -&gt; 0], [], []], [token, 9, 10, is, [sentence -&gt; 0], [], []], [token, 12, 12, a, [sentence -&gt; 0], [], []], [token, 14, 19, second, [sentence -&gt; 0], [], []], [token, 21, 28, sentence, [sentence -&gt; 0], [], []], [token, 29, 29, ., [sentence -&gt; 0], [], []]] | +--+ annotations_df.select(&quot;token&quot;).show(truncate=false) OUTPUT: +--+ |token | +--+ |[[token, 0, 4, Hello, [sentence -&gt; 0], [], []], [token, 5, 5, ,, [sentence -&gt; 0], [], []], [token, 7, 10, this, [sentence -&gt; 0], [], []], [token, 12, 13, is, [sentence -&gt; 0], [], []], [token, 15, 16, an, [sentence -&gt; 0], [], []], [token, 18, 24, example, [sentence -&gt; 0], [], []], [token, 26, 33, sentence, [sentence -&gt; 0], [], []]]| |[[token, 0, 2, And, [sentence -&gt; 0], [], []], [token, 4, 7, this, [sentence -&gt; 0], [], []], [token, 9, 10, is, [sentence -&gt; 0], [], []], [token, 12, 12, a, [sentence -&gt; 0], [], []], [token, 14, 19, second, [sentence -&gt; 0], [], []], [token, 21, 28, sentence, [sentence -&gt; 0], [], []], [token, 29, 29, ., [sentence -&gt; 0], [], []]] | +--+ What if we want to deal with just the resulting annotations? We can use the Finisher annotator, retrieve the Explain Document ML pipeline, and add them together in a Spark ML Pipeline. Remember that pretrained pipelines expect the input column to be named “text”. PythonScala from sparknlp import Finisher from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline finisher = Finisher().setInputCols([&quot;token&quot;, &quot;lemma&quot;, &quot;pos&quot;]) explain_pipeline_model = PretrainedPipeline(&quot;explain_document_ml&quot;).model pipeline = Pipeline() .setStages([ explain_pipeline_model, finisher ]) sentences = [ [&#39;Hello, this is an example sentence&#39;], [&#39;And this is a second sentence.&#39;] ] data = spark.createDataFrame(sentences).toDF(&quot;text&quot;) model = pipeline.fit(data) annotations_finished_df = model.transform(data) annotations_finished_df.select(&#39;finished_token&#39;).show(truncate=False) OUTPUT: +-+ |finished_token | +-+ |[Hello, ,, this, is, an, example, sentence]| |[And, this, is, a, second, sentence, .] | +-+ scala&gt; import com.johnsnowlabs.nlp.Finisher scala&gt; import org.apache.spark.ml.Pipeline scala&gt; val finisher = new Finisher().setInputCols(&quot;token&quot;, &quot;lemma&quot;, &quot;pos&quot;) scala&gt; val explainPipelineModel = PretrainedPipeline(&quot;explain_document_ml&quot;).model scala&gt; val pipeline = new Pipeline(). setStages(Array( explainPipelineModel, finisher )) scala&gt; val data = Seq( &quot;Hello, this is an example sentence&quot;, &quot;And this is a second sentence&quot;) .toDF(&quot;text&quot;) scala&gt; val model = pipeline.fit(data) scala&gt; val annotations_df = model.transform(data) scala&gt; annotations_df.select(&quot;finished_token&quot;).show(truncate=false) OUTPUT: +-+ |finished_token | +-+ |[Hello, ,, this, is, an, example, sentence]| |[And, this, is, a, second, sentence, .] | +-+ Setup your own pipeline Annotator types Every annotator has a type. Those annotators that share a type, can be used interchangeably, meaning you could use any of them when needed. For example, when a token type annotator is required by another annotator, such as a sentiment analysis annotator, you can either provide a normalized token or a lemma, as both are of type token. Necessary imports Since version 1.5.0 we are making necessary imports easy to reach, base._ will include general Spark NLP transformers and concepts, while annotator._ will include all annotators that we currently provide. We also need Spark ML pipelines. PythonScala from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline DocumentAssembler: Getting data in In order to get through the NLP process, we need to get raw data annotated. There is a special transformer that does this for us: the DocumentAssembler, it creates the first annotation of type Document which may be used by annotators down the road. PythonScala documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val documentAssembler = new DocumentAssembler(). setInputCol(&quot;text&quot;). setOutputCol(&quot;document&quot;) Sentence detection and tokenization In this quick example, we now proceed to identify the sentences in the input document. SentenceDetector requires a Document annotation, which is provided by the DocumentAssembler output, and it’s itself a Document type token. The Tokenizer requires a Document annotation type. That means it works both with DocumentAssembler or SentenceDetector output. In the following example we use the sentence output. PythonScala sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;Sentence&quot;) regexTokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) val sentenceDetector = new SentenceDetector(). setInputCols(Array(&quot;document&quot;)). setOutputCol(&quot;sentence&quot;) val regexTokenizer = new Tokenizer(). setInputCols(Array(&quot;sentence&quot;)). setOutputCol(&quot;token&quot;) Spark NLP also includes another special transformer, called Finisher to show tokens in a human language. finisher = Finisher() .setInputCols([&quot;token&quot;]) .setCleanAnnotations(False) val finisher = new Finisher(). setInputCols(&quot;token&quot;). setCleanAnnotations(false) Finisher: Getting data out At the end of each pipeline or any stage that was done by Spark NLP, you may want to get results out whether onto another pipeline or simply write them on disk. The Finisher annotator helps you to clean the metadata (if it’s set to true) and output the results into an array: PythonScala finisher = Finisher() .setInputCols([&quot;token&quot;]) .setIncludeMetadata(True) val finisher = new Finisher() .setInputCols(&quot;token&quot;) .setIncludeMetadata(true) If you need to have a flattened DataFrame (each sub-array in a new column) from any annotations other than struct type columns, you can use explode function from Spark SQL. You can also use Apache Spark functions (SQL) to manipulate the output DataFrame in any way you need. Here we combine the tokens and NER results together: import pyspark.sql.functions as F df.withColumn(&quot;tmp&quot;, F.explode(&quot;chunk&quot;)).select(&quot;tmp.*&quot;) finisher.withColumn(&quot;newCol&quot;, explode(arrays_zip($&quot;finished_token&quot;, $&quot;finished_ner&quot;))) import org.apache.spark.sql.functions._ df.withColumn(&quot;tmp&quot;, explode(col(&quot;chunk&quot;))).select(&quot;tmp.*&quot;) Using Spark ML Pipeline Now we want to put all this together and retrieve the results, we use a Pipeline for this. We use the same data in fit() that we will use in transform since none of the pipeline stages have a training stage. PythonScala pipeline = Pipeline() .setStages([ documentAssembler, sentenceDetector, regexTokenizer, finisher ]) OUTPUT: +-+ |finished_token | +-+ |[hello, ,, this, is, an, example, sentence]| +-+ val pipeline = new Pipeline(). setStages(Array( documentAssembler, sentenceDetector, regexTokenizer, finisher )) val data = Seq(&quot;hello, this is an example sentence&quot;).toDF(&quot;text&quot;) val annotations = pipeline. fit(data). transform(data).toDF(&quot;text&quot;)) annotations.select(&quot;finished_token&quot;).show(truncate=false) OUTPUT: +-+ |finished_token | +-+ |[hello, ,, this, is, an, example, sentence]| +-+ Using Spark NLP’s LightPipeline LightPipeline is a Spark NLP specific Pipeline class equivalent to Spark ML Pipeline. The difference is that it’s execution does not hold to Spark principles, instead it computes everything locally (but in parallel) in order to achieve fast results when dealing with small amounts of data. This means, we do not input a Spark Dataframe, but a string or an Array of strings instead, to be annotated. To create Light Pipelines, you need to input an already trained (fit) Spark ML Pipeline. It’s transform() stage is converted into annotate() instead. PythonScala from sparknlp.base import LightPipeline explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) lightPipeline = LightPipeline(explain_document_pipeline.model) OUTPUT: explain_document_ml download started this may take some time. Approx size to download 9.4 MB [OK!] lightPipeline.annotate(&quot;Hello world, please annotate my text&quot;) OUTPUT: {&#39;stem&#39;: [&#39;hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;pleas&#39;, &#39;annot&#39;, &#39;my&#39;, &#39;text&#39;], &#39;checked&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;my&#39;, &#39;text&#39;], &#39;lemma&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;i&#39;, &#39;text&#39;], &#39;document&#39;: [&#39;Hello world, please annotate my text&#39;], &#39;pos&#39;: [&#39;UH&#39;, &#39;NN&#39;, &#39;,&#39;, &#39;VB&#39;, &#39;NN&#39;, &#39;PRP$&#39;, &#39;NN&#39;], &#39;token&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;my&#39;, &#39;text&#39;], &#39;sentence&#39;: [&#39;Hello world, please annotate my text&#39;]} import com.johnsnowlabs.nlp.base._ val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) val lightPipeline = new LightPipeline(explainDocumentPipeline.model) lightPipeline.annotate(&quot;Hello world, please annotate my text&quot;) OUTPUT: Map[String,Seq[String]] = Map( stem -&gt; List(hello, world, ,, pleas, annot, my, text), checked -&gt; List(Hello, world, ,, please, annotate, my, tex), lemma -&gt; List(Hello, world, ,, please, annotate, i, text), document -&gt; List(Hello world, please annotate my text), pos -&gt; ArrayBuffer(UH, NN, ,, VB, NN, PRP$, NN), token -&gt; List(Hello, world, ,, please, annotate, my, text), sentence -&gt; List(Hello world, please annotate my text) ) Training annotators Training methodology Training your own annotators is a key concept when dealing with real life scenarios. Any of the annotators provided above, such as pretrained pipelines and models, can be applied out-of-the-box to a specific use case, but better results are obtained when they are fine-tuned to your specific use-case. Dealing with real life problems ofter requires training your own models. In Spark NLP, we support three ways of training a custom annotator: Train from a dataset. Most annotators are capable of training from a dataset passed to fit() method just as Spark ML does. Annotators that use the suffix Approach are such trainable annotators. Training from fit() is the standard behavior in Spark ML. Annotators have different schema requirements for training. Check the reference to see what are the requirements of each annotators. Training from an external source: Some of our annotators train from an external file or folder passed to the annotator as a param. You will see such ones as setCorpus() or setDictionary() param setter methods, allowing you to configure the input to use. You can set Spark NLP to read them as Spark datasets or LINE_BY_LINE which is usually faster for small files. Last but not least, some of our annotators are Deep Learning based. These models may be trained with the standard AnnotatorApproach API just like any other annotator. For more advanced users, we also allow importing your own graphs or even training from Python and converting them into an AnnotatorModel. Spark NLP Imports base includes general Spark NLP transformers and concepts, annotator includes all annotators that we currently provide, embeddings includes word embedding annotators. Example: PythonScala from sparknlp.base import * from sparknlp.annotator import * from sparknlp.embeddings import * import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ Spark ML Pipelines SparkML Pipelines are a uniform structure that helps creating and tuning practical machine learning pipelines. Spark NLP integrates with them seamlessly so it is important to have this concept handy. Once a Pipeline is trained with fit(), it becomes a PipelineModel Example: PythonScala from pyspark.ml import Pipeline pipeline = Pipeline().setStages([...]) import org.apache.spark.ml.Pipeline new Pipeline().setStages(Array(...)) LightPipeline LightPipelines are Spark ML pipelines converted into a single machine but multithreaded task, becoming more than 10x times faster for smaller amounts of data (small is relative, but 50k sentences is roughly a good maximum). To use them, simply plug in a trained (fitted) pipeline. Example: PythonScala from sparknlp.base import LightPipeline LightPipeline(someTrainedPipeline).annotate(someStringOrArray) import com.johnsnowlabs.nlp.LightPipeline new LightPipeline(somePipelineModel).annotate(someStringOrArray)) Functions: annotate(string or string[]): returns dictionary list of annotation results fullAnnotate(string or string[]): returns dictionary list of entire annotations content For more details please refer to Using Spark NLP’s LightPipelines. RecursivePipeline Recursive pipelines are SparkNLP specific pipelines that allow a Spark ML Pipeline to know about itself on every Pipeline Stage task, allowing annotators to utilize this same pipeline against external resources to process them in the same way the user decides. Only some of our annotators take advantage of this. RecursivePipeline behaves exactly the same as normal Spark ML pipelines, so they can be used with the same intention. Example: PythonScala from sparknlp.annotator import * recursivePipeline = RecursivePipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, lemmatizer, finisher ]) import com.johnsnowlabs.nlp.RecursivePipeline val recursivePipeline = new RecursivePipeline() .setStages(Array( documentAssembler, sentenceDetector, tokenizer, lemmatizer, finisher )) Params and Features Annotator parameters SparkML uses ML Params to store pipeline parameter maps. In SparkNLP, we also use Features, which are a way to store parameter maps that are larger than just a string or a boolean. These features are serialized as either Parquet or RDD objects, allowing much faster and scalable annotator information. Features are also broadcasted among executors for better performance.",
    "url": "/docs/en/concepts",
    "relUrl": "/docs/en/concepts"
  },
  "11": {
    "id": "11",
    "title": "Contribute",
    "content": "Refer to our GitHub page to take a look at the GH Issues, as the project is yet small. You can create in there your own issues to either work on them yourself or simply propose them. Feel free to clone the repository locally and submit pull requests so we can review them and work together. feedback, ideas and bug reports testing and development training and testing nlp corpora documentation and research Help is always welcome, for any further questions, contact nlp@johnsnowlabs.com. Your own annotator model Creating your first annotator transformer should not be hard, here are a few guidelines to get you started. Lets assume we want a wrapper annotator, which puts a character surrounding tokens provided by a Tokenizer WordWrapper uid is utilized for transformer serialization, AnnotatorModel[MyAnnotator] will contain the common annotator logic We need to use standard constructor for java and python compatibility class WordWrapper(override val uid: String) extends AnnotatorModel[WordWrapper] { def this() = this(Identifiable.randomUID(&quot;WORD_WRAPPER&quot;)) } Annotator attributes This annotator is not flexible if we don’t provide parameters import com.johnsnowlabs.nlp.AnnotatorType._ override val annotatorType: AnnotatorType = TOKEN override val requiredAnnotatorTypes: Array[AnnotatorType] = Array[AnnotatorType](TOKEN) Annotator parameters This annotator is not flexible if we don’t provide parameters protected val character: Param[String] = new Param(this, &quot;character&quot;, &quot;this is the character used to wrap a token&quot;) def setCharacter(value: String): this.type = set(pattern, value) def getCharacter: String = $(pattern) setDefault(character, &quot;@&quot;) Annotator logic Here is how we act, annotations will automatically provide our required annotations We generally use annotatorType for metadata keys override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = { annotations.map(annotation =&gt; { Annotation( annotatorType, annotation.begin, annotation.end, Map(annotatorType -&gt; $(character) + annotation.result + $(character)) }) }",
    "url": "/contribute",
    "relUrl": "/contribute"
  },
  "12": {
    "id": "12",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/deidentification",
    "relUrl": "/deidentification"
  },
  "13": {
    "id": "13",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/demo",
    "relUrl": "/demo"
  },
  "14": {
    "id": "14",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/detect_sentiment_emotion",
    "relUrl": "/detect_sentiment_emotion"
  },
  "15": {
    "id": "15",
    "title": "Developers Guideline",
    "content": "Spark NLP is an open-source library and everyone’s contribution is welcome! In this section we provide a guide on how to setup your environment using IntelliJ IDEA for a smoother start. You can also check our video tutorials available on our YouTube channel: https://www.youtube.com/johnsnowlabs Setting up the Environment Import to IntelliJ IDEA Setup Spark NLP development environment. This section will cover library set up for IntelliJ IDEA. Before you begin, make sure what you have Java and Spark installed in your system. We suggest that you have installed jdk 8 and Apache Spark 2.4.x. To check installation run: java -version and spark-submit --version Next step is to open IntelliJ IDEA. On the Welcome to IntelliJ IDEA screen you will see ability to Check out from Version Controle. Log in into your github account in pop up. After select from a list Spark NLP repo url: https://github.com/JohnSnowLabs/spark-nlp and press clone button. If you don’t see url in the list, clone or fork repo first to your Github account and try again. When the repo cloned IDE will detect SBT file with dependencies. Click Yes to start import from sbt. In the Import from sbt pop up make sure you have JDK 8 detected. Click Ok to proceed and download required resources. If you already had dependences installed you may see the pop up Not empty folder, click Ok to ignore it and reload resources. IntelliJ IDEA will be open and it will start syncing SBT project. It make take some time, you will see the progress in the build output panel in the bottom of the screen. To see the project panel in the left press Alt+1. Next step is to install Python plugin to the IntelliJ IDEA. To do this, open File -&gt; Settings -&gt; Plugins, type Python in the search and select Python plugin by JetBrains. Install this plugin by clicking Install button. After this steps you can check project structure in the File -&gt; Project Structure -&gt; Modules. Make sure what you have spark-nlp and spark-nlp-build folders and no errors in the exported dependencies. In the Project settings check what project SDK is set to 1.8 and in Platform Settings -&gt; SDK&#39;s you have Java installation as well as Python installation. If you don’t see Python installed in the SDK&#39;s tab click + button, add Python SDK with new virtual environment in the project folder with Python 3.x. Compiling, assembly and unit testing Run tests in Scala Click Add configuration in the Top right corner. In the pop up click on the + and look for sbt task. In the Name field put Test. In the Tasks field write down test. After you can disable checkbox in Use sbt shell to have more custom configurations. In the VM parameters increase the memory by changing -Xmx1024M to -Xmx10G and click Ok. If everything was set up correctly you suhould see unabled green button Run ‘Test’ in the top right. Click on it to start running the tests. This algorithm will Run all tests under spark-nlp/src/test/scala/com.johnsnowlabs/ Copy tasks After you created task, click Edit configuration. Select target task and instead of + button you can click copy in the same menu. It will recreate all settings from parent task and create a new task. You can do it for Scala or for Python tasks. Run individual tests Open test file you want to run. For example, spark-nlp/src/test/scala/com.johnsnowlabs/nlp/FinisherTestSpec.scala. Right click on the class name and select Copy reference. It will copy to you buffer classpath - com.johnsnowlabs.nlp.FinisherTestSpec. Copy existing Scala task and Name it as FinisherTest. In the Tasks field write down &quot;testOnly *classpath*&quot; -&gt; &quot;testOnly com.johnsnowlabs.nlp.FinisherTestSpec&quot; and click Ok to save individual scala test run configuration. Press play button to run individual test. Debugging tests To run tests in debug mode click Debug button (next to play button). In this mode task will stop at the given break points. Run tests in Python To run Python test, first you need to configure project structure. Go to File -&gt; Project Settings -&gt; Modules, click on the + button and select New Module. In the pop up choose Python on left menu, select Python SDK from created virtual environment and click Next. Enter python in the Module name and click Finish. After you need to add Spark dependencies. Select created Python module and click on the + button in the Dependencies part. Choose Jars or directories… and find the find installation path of spark (usually the folder name is spark-2.4.5-bin-hadoop2.7). In the Spark folder go to the python/libs and select pyspark.zip to the project. Do the same for another file in the same folder - py4j-0.10.7-src.zip. All available tests are in spark-nlp/python/run-tests.py. Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and look for Python. In the Script path locate file spark-nlp/python/run-tests.py. Also you need to add SPARK_HOME environment variable to the project. Choose Environment variables and add new variable SPARK_HOME. Insert installation path of spark to the Value field. Click Ok to save and close pop up and click Ok to confirm new task creation. Before running the tests we need to install requered python dependencies in the new virtual environment. Select in the bottom menu Terminal and activate your environment with command source venv/bin/activate after install packages by running pip install pyspark numpy Compiling jar Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and select sbt task. In the Name field put AssemblyCopy. In the Tasks field write down assemblyAndCopy. After you can disable checkbox in Use sbt shell to have more custom configurations. In the VM parameters increase the memory by changing -Xmx1024M to -Xmx6G and click Ok. You can find created jar in the folder spark-nlp/python/lib/sparknlp.jar Note: Assembly command creates a fat jars, that includes all dependencies within Compiling pypi, whl Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and select sbt task. In the Name field put AssemblyAndCopyForPyPi. In the Tasks field write down assemblyAndCopyForPyPi. Then you go to spark-nlp/python/ directory and run: python setup.py sdist bdist_wheel You can find created whl and tar.gz in the folder spark-nlp/python/dist/. Use this files to install spark-nlp locally: pip install spark_nlp-2.x.x-py3-none-any.whl",
    "url": "/docs/en/developers",
    "relUrl": "/docs/en/developers"
  },
  "16": {
    "id": "16",
    "title": "Spark NLP Display",
    "content": "Getting started Spark NLP Display is an open-source python library for visualizing the annotations generated with Spark NLP. It currently offers out-of-the-box suport for the following types of annotations: Dependency Parser Named Entity Recognition Entity Resolution Relation Extraction Assertion Status The ability to quickly visualize the entities/relations/assertion statuses, etc. generated using Spark NLP is a very useful feature for speeding up the development process as well as for understanding the obtained results. Getting all of this in a one liner is extremelly convenient especially when running Jupyter notebooks which offers full support for html visualizations. The visualisation classes work with the outputs returned by both Pipeline.transform() function and LightPipeline.fullAnnotate(). Install Spark NLP Display You can install the Spark NLP Display library via pip by using: pip install spark-nlp-display A complete guideline on how to use the Spark NLP Display library is available here. Visualize a dependency tree For visualizing a dependency trees generated with DependencyParserApproach you can use the following code. from sparknlp_display import DependencyParserVisualizer dependency_vis = DependencyParserVisualizer() dependency_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe. pos_col = &#39;pos&#39;, #specify the pos column dependency_col = &#39;dependency&#39;, #specify the dependency column dependency_type_col = &#39;dependency_type&#39; #specify the dependency type column ) The following image gives an example of html output that is obtained for a test sentence: Visualize extracted named entities The NerVisualizer highlights the named entities that are identified by Spark NLP and also displays their labels as decorations on top of the analyzed text. The colors assigned to the predicted labels can be configured to fit the particular needs of the application. from sparknlp_display import NerVisualizer ner_vis = NerVisualizer() ner_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe label_col=&#39;entities&#39;, #specify the entity column document_col=&#39;document&#39; #specify the document column (default: &#39;document&#39;) labels=[&#39;PER&#39;] #only allow these labels to be displayed. (default: [] - all labels will be displayed) ) ## To set custom label colors: ner_vis.set_label_colors({&#39;LOC&#39;:&#39;#800080&#39;, &#39;PER&#39;:&#39;#77b5fe&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences: Visualize relations The RelationExtractionVisualizer can be used to visualize the relations predicted by Spark NLP. The two entities involved in a relation will be highlighted and their label will be displayed. Also a directed and labeled arc(line) will be used to connect the two entities. from sparknlp_display import RelationExtractionVisualizer re_vis = RelationExtractionVisualizer() re_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe relation_col = &#39;relations&#39;, #specify relations column document_col = &#39;document&#39;, #specify document column show_relations=True #display relation names on arrows (default: True) ) The following image gives an example of html output that is obtained for a couple of test sentences: Visualize assertion status The AssertionVisualizer is a special type of NerVisualizer that also displays on top of the labeled entities the assertion status that was infered by a Spark NLP model. from sparknlp_display import AssertionVisualizer assertion_vis = AssertionVisualizer() assertion_vis.display(pipeline_result[0], label_col = &#39;entities&#39;, #specify the ner result column assertion_col = &#39;assertion&#39; #specify assertion column document_col = &#39;document&#39; #specify the document column (default: &#39;document&#39;) ) ## To set custom label colors: assertion_vis.set_label_colors({&#39;TREATMENT&#39;:&#39;#008080&#39;, &#39;problem&#39;:&#39;#800080&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences: Visualize entity resolution Entity resolution refers to the normalization of named entities predicted by Spark NLP with respect to standard terminologies such as ICD-10, SNOMED, RxNorm etc. You can read more about the available entity resolvers here. The EntityResolverVisualizer will automatically display on top of the NER label the standard code (ICD10 CM, PCS, ICDO; CPT) that corresponds to that entity as well as the short description of the code. If no resolution code could be identified a regular NER-type of visualization will be displayed. from sparknlp_display import EntityResolverVisualizer er_vis = EntityResolverVisualizer() er_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe label_col=&#39;entities&#39;, #specify the ner result column resolution_col = &#39;resolution&#39; document_col=&#39;document&#39; #specify the document column (default: &#39;document&#39;) ) ## To set custom label colors: er_vis.set_label_colors({&#39;TREATMENT&#39;:&#39;#800080&#39;, &#39;PROBLEM&#39;:&#39;#77b5fe&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences:",
    "url": "/docs/en/display",
    "relUrl": "/docs/en/display"
  },
  "17": {
    "id": "17",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/east_asian_languages",
    "relUrl": "/east_asian_languages"
  },
  "18": {
    "id": "18",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/enhance_low_quality_images",
    "relUrl": "/enhance_low_quality_images"
  },
  "19": {
    "id": "19",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/european_languages",
    "relUrl": "/european_languages"
  },
  "20": {
    "id": "20",
    "title": "Evaluation",
    "content": "Spark NLP Evaluation This module includes tools to evaluate the accuracy of annotators and visualize the parameters used on training. It includes specific metrics for each annotator and its training time. The results will display on the console or to an MLflow tracking UI. Just with a simple import you can start using eval module. Check how to setup MLflow UI See here on eval folder if you want to check specific running examples. Example: PythonScala from sparknlp.eval import * import com.johnsnowlabs.nlp.eval._ Evaluating Norvig Spell Checker You can evaluate this spell checker either by training an annotator or by using a pretrained model. spark: Spark session. trainFile: A corpus of documents with correctly spell words. testFile: A corpus of documents with misspells words. groundTruthFile: The same corpus used on testFile but with correctly spell words. Train File Example: Any document that you prefer with correctly spell words. Test File Example: My siter go to Munich. Ground Truth File Example: My sister goes to Munich. Example for annotator: PythonScala spell = NorvigSweetingApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;checked&quot;) .setDictionary(dictionary_file) norvigSpellEvaluation = NorvigSpellEvaluation(spark, test_file, ground_truth_file) norvigSpellEvaluation.computeAccuracyAnnotator(train_file, spell) val spell = new NorvigSweetingApproach() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;checked&quot;) .setDictionary(dictionary_file) val norvigSpellEvaluation = new NorvigSpellEvaluation(spark, testFile, groundTruthFile) norvigSpellEvaluation.computeAccuracyAnnotator(trainFile, spell) Example for pretrained model: PythonScala spell = NorvigSweetingModel.pretrained() norvigSpellEvaluation = NorvigSpellEvaluation(spark, test_file, ground_truth_file) norvigSpellEvaluation.computeAccuracyModel(spell) val spell = NorvigSweetingModel.pretrained() val norvigSpellEvaluation = new NorvigSpellEvaluation(spark, testFile, groundTruthFile) norvigSpellEvaluation.computeAccuracyModel(spell) Evaluating Symmetric Spell Checker You can evaluate this spell checker either by training an annotator or by using a pretrained model. spark: Spark session trainFile: A corpus of documents with correctly spell words. testFile: A corpus of documents with misspells words. groundTruthFile: The same corpus used on testFile but with correctly spell words. Train File Example: Any document that you prefer with correctly spell words. Test File Example: My siter go to Munich. Ground Truth File Example: My sister goes to Munich. Example for annotator: PythonScala spell = SymmetricDeleteApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;checked&quot;) .setDictionary(dictionary_file) symSpellEvaluation = SymSpellEvaluation(spark, test_file, ground_truth_file) symSpellEvaluation.computeAccuracyAnnotator(train_file, spell) val spell = new SymmetricDeleteApproach() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;checked&quot;) val symSpellEvaluation = new SymSpellEvaluation(spark, testFile, groundTruthFile) symSpellEvaluation.computeAccuracyAnnotator(trainFile, spell) Example for pretrained model: PythonScala spell = SymmetricDeleteModel.pretrained() symSpellEvaluation = NorvigSpellEvaluation(spark, test_file, ground_truth_file) symSpellEvaluation.computeAccuracyModel(spell) val spell = SymmetricDeleteModel.pretrained() val symSpellEvaluation = new SymSpellEvaluation(spark, testFile, groundTruthFile) symSpellEvaluation.computeAccuracyModel(spell) Evaluating NER DL You can evaluate NER DL when training an annotator. spark: Spark session. trainFile: Files with labeled NER entities for training. testFile: Files with labeled NER entities for testing. These files are used to evaluate the model. So, it’s used for prediction and the labels as ground truth. tagLevel: The granularity of tagging when measuring accuracy on entities. Set “IOB” to include inside and beginning, empty to ignore it. For example to display accuracy for entity I-PER and B-PER set “IOB” whereas just for entity PER set it as an empty string. Example: PythonScala embeddings = WordEmbeddings() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setEmbeddingsSource(&quot;glove.6B.100d.txt&quot;, 100, &quot;TEXT&quot;) ner_approach = NerDLApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) .setRandomSeed(0) nerDLEvaluation = NerDLEvaluation(spark, test_File, tag_level) nerDLEvaluation.computeAccuracyAnnotator(train_file, ner_approach, embeddings) val embeddings = new WordEmbeddings() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setEmbeddingsSource(&quot;glove.6B.100d.txt&quot;, 100, WordEmbeddingsFormat.TEXT) val nerApproach = new NerDLApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) .setRandomSeed(0) val nerDLEvaluation = new NerDLEvaluation(spark, testFile, tagLevel) nerDLEvaluation.computeAccuracyAnnotator(trainFile, nerApproach, embeddings) Example for pretrained model: PythonScala ner_dl = NerDLModel.pretrained() nerDlEvaluation = NerDLEvaluation(spark, test_File, tag_level) nerDlEvaluation.computeAccuracyModel(ner_dl) val nerDl = NerDLModel.pretrained() val nerDlEvaluation = NerDLEvaluation(spark, testFile, tagLevel) nerDlEvaluation.computeAccuracyModel(nerDl) Evaluating NER CRF You can evaluate NER CRF when training an annotator. spark: Spark session. trainFile: Files with labeled NER entities for training. testFile: Files with labeled NER entities for testing. These files are used to evaluate the model. So, it’s used for prediction and the labels as ground truth. format: The granularity of tagging when measuring accuracy on entities. Set “IOB” to include inside and beginning, empty to ignore it. For example to display accuracy for entity I-PER and B-PER set “IOB” whereas just for entity PER set it as an empty string. Example: PythonScala embeddings = WordEmbeddings() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setEmbeddingsSource(&quot;glove.6B.100d.txt&quot;, 100, &quot;TEXT&quot;) ner_approach = NerCrfApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) .setRandomSeed(0) nerCrfEvaluation = NerCrfEvaluation(spark, test_File, tag_level) nerCrfEvaluation.computeAccuracyAnnotator(train_file, ner_approach, embeddings) val embeddings = new WordEmbeddings() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setEmbeddingsSource(&quot;./glove.6B.100d.txt &quot;, 100, WordEmbeddingsFormat.TEXT) .setCaseSensitive(true) val nerTagger = new NerCrfApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;pos&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) val nerCrfEvaluation = new NerCrfEvaluation(testFile, format) nerCrfEvaluation.computeAccuracyAnnotator(trainFile, nerTagger, embeddings) Example for pretrained model: PythonScala ner_crf = NerCrfModel.pretrained() nerCrfEvaluation = NerCrfEvaluation(spark, test_File, tag_level) nerCrfEvaluation.computeAccuracyModel(ner_crf) nerCrf = NerCrfModel.pretrained() nerCrfEvaluation = NerCrfEvaluation(spark, testFile, tagLevel) nerCrfEvaluation.computeAccuracyModel(nerCrf) Evaluating POS Tagger You can evaluate POS either by training an annotator or by using a pretrained model. spark: Spark session. trainFile: A labeled POS file see and example here. testFile: A CoNLL-U format file. Example for annotator: PythonScala pos_tagger = PerceptronApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) .setNIterations(2) posEvaluation = POSEvaluation(spark, test_file) posEvaluation.computeAccuracyAnnotator(train_file, pos_tagger) val posTagger = new PerceptronApproach() .setInputCols(Array(&quot;document&quot;, &quot;token&quot;)) .setOutputCol(&quot;pos&quot;) .setNIterations(2) val posEvaluation = new POSEvaluation(spark, testFile) posEvaluation.computeAccuracyAnnotator(trainFile, posTagger)",
    "url": "/docs/en/evaluation",
    "relUrl": "/docs/en/evaluation"
  },
  "21": {
    "id": "21",
    "title": "Examples",
    "content": "Showcasing notebooks and codes of how to use Spark NLP in Python and Scala. Python Setup $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.7 -y $ conda activate sparknlp $ pip install spark-nlp==3.1.0 pyspark==3.1.1 Colab setup import os # Install java ! apt-get update -qq ! apt-get install -y openjdk-8-jdk-headless -qq &gt; /dev/null os.environ[&quot;JAVA_HOME&quot;] = &quot;/usr/lib/jvm/java-8-openjdk-amd64&quot; os.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;/bin:&quot; + os.environ[&quot;PATH&quot;] ! java -version # Install pyspark ! pip install -q spark-nlp==3.1.0 pyspark==3.1.1 Notebooks Tutorials and trainings Jupyter Notebooks Databricks Notebooks",
    "url": "/docs/en/examples",
    "relUrl": "/docs/en/examples"
  },
  "22": {
    "id": "22",
    "title": "Export Data",
    "content": "The completions and predictions are stored in a database for fast search and access. Completions and predictions can be exported into the formats described below. JSON You can convert and export the completions and predictions to json format by using the JSON option on the Export page. The obtained format is the following: [ { &quot;completions&quot;: [], &quot;predictions&quot;: [ { &quot;created_username&quot;: &quot;SparkNLP Pre-annotation&quot;, &quot;result&quot;: [ { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;7HGzTLkNUA&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 3554, &quot;labels&quot;: [ &quot;Symptom_Name&quot; ], &quot;start&quot;: 3548, &quot;text&quot;: &quot;snores&quot; } } ], &quot;created_ago&quot;: &quot;2020-11-09T14:44:57.713743Z&quot;, &quot;id&quot;: 2001 } ], &quot;created_at&quot;: &quot;2020-11-09 14:41:39&quot;, &quot;created_by&quot;: &quot;admin&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;Cardiovascular / Pulmonary nSample Name: Angina - Consult nDescription: Patient had a recurrent left arm pain after her stent, three days ago, and this persisted after two sublingual nitroglycerin. n(Medical Transcription Sample Report) nHISTORY OF PRESENT ILLNESS: The patient is a 68-year-old woman whom I have been following, who has had angina. In any case today, she called me because she had a recurrent left arm pain after her stent, three days ago, and this persisted after two sublingual nitroglycerin when I spoke to her.&quot;, &quot;title&quot;: &quot;sample_document3.txt&quot;, &quot;pre_annotation&quot;: true }, &quot;id&quot;: 2 }] CSV Results are stored in comma-separated tabular file with column names specified by “from_name” “to_name” values TSV Results are stored in tab-separated tabular file with column names specified by “from_name” “to_name” values CONLL2003 Results will be generated in the following format: -DOCSTART- -X- O Sample -X- _ O Type -X- _ O Medical -X- _ O Specialty: -X- _ O Endocrinology -X- _ O Sample -X- _ O Name: -X- _ O Diabetes -X- _ B-Diagnosis Mellitus -X- _ I-Diagnosis Followup -X- _ O Description: -X- _ O Return -X- _ O visit -X- _ O to -X- _ O the -X- _ O endocrine -X- _ O clinic -X- _ O for -X- _ O followup -X- _ O management -X- _ O of -X- _ O type -X- _ O 1 -X- _ O diabetes -X- _ O mellitus -X- _ O Plan -X- _ O today -X- _ O is -X- _ O to -X- _ O make -X- _ O adjustments -X- _ O to -X- _ O her -X- _ O pump -X- _ O based -X- _ O on -X- _ O a -X- _ O total -X- _ O daily -X- _ B-FREQUENCY dose -X- _ O of -X- _ O 90 -X- _ O units -X- _ O of -X- _ O insulin -X- _ O …",
    "url": "/docs/en/export",
    "relUrl": "/docs/en/export"
  },
  "23": {
    "id": "23",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/extract_relationships",
    "relUrl": "/extract_relationships"
  },
  "24": {
    "id": "24",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/extract_tables_structured_data",
    "relUrl": "/extract_tables_structured_data"
  },
  "25": {
    "id": "25",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/extract_text_from_documents",
    "relUrl": "/extract_text_from_documents"
  },
  "26": {
    "id": "26",
    "title": "Tensorflow Graph",
    "content": "NER DL uses Char CNNs - BiLSTM - CRF Neural Network architecture. Spark NLP defines this architecture through a Tensorflow graph, which requires the following parameters: Tags Embeddings Dimension Number of Chars Spark NLP infers these values from the training dataset used in NerDLApproach annotator and tries to load the graph embedded on spark-nlp package. Currently, Spark NLP has graphs for the most common combination of tags, embeddings, and number of chars values: Tags Embeddings Dimension 10 100 10 200 10 300 10 768 10 1024 25 300 All of these graphs use an LSTM of size 128 and number of chars 100 In case, your train dataset has a different number of tags, embeddings dimension, number of chars and LSTM size combinations shown in the table above, NerDLApproach will raise an IllegalArgumentException exception during runtime with the message below: Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Check https://nlp.johnsnowlabs.com/docs/en/graph for instructions to generate the required graph. To overcome this exception message we have to follow these steps: Clone spark-nlp github repo Run python file create_models with number of tags, embeddings dimension and number of char values mentioned on your exception message error. cd spark-nlp/python/tensorflow export PYTHONPATH=lib/ner python create_models.py [number_of_tags] [embeddings_dimension] [number_of_chars] [output_path] This will generate a graph on the directory defined on `output_path argument. Retry training with NerDLApproach annotator but this time use the parameter setGraphFolder with the path of your graph. Note: Make sure that you have Python 3 and Tensorflow 1.15.0 installed on your system since create_models requires those versions to generate the graph successfully. Note: We also have a notebook in the same directory if you prefer Jupyter notebook to cerate your custom graph.",
    "url": "/docs/en/graph",
    "relUrl": "/docs/en/graph"
  },
  "27": {
    "id": "27",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/identify_translate_languages",
    "relUrl": "/identify_translate_languages"
  },
  "28": {
    "id": "28",
    "title": "Import Documents",
    "content": "Once a new project is created and its configuration is saved, the user is redirected to the Import page. Here the user has multiple options for importing tasks. Plain text file When you upload a text file, a task will be created for each line of that file. In other words the text is split by the new line character and a new task is created for each line. Json file For bulk importing a list of documents you can use the json import option. The expected format is illustrated in the image below. It consists of a list of dictionaries, each with 2 keys-values pairs (“text” and “title”). [{ &quot;text&quot;: &quot;Task text content.&quot;, &quot;title&quot;:&quot;Task title&quot;}] CSV, TSV file When CSV / TSV formatted text file is used, column names are interpreted as task data keys: Task text content, Task title this is a first task, Colon Cancer.txt this is a second task, Breast radiation therapy.txt Import annotated tasks When importing tasks that already contain annotations (e.g. exported from another project, with predictions generated by pre-trained models) the user has the option to overwrite completions/predictions or to skip the tasks that are already imported into the project.",
    "url": "/docs/en/import",
    "relUrl": "/docs/en/import"
  },
  "29": {
    "id": "29",
    "title": "Spark NLP: <span>State of the Art Natural Language Processing</span>",
    "content": "",
    "url": "/",
    "relUrl": "/"
  },
  "30": {
    "id": "30",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/infer_meaning_intent",
    "relUrl": "/infer_meaning_intent"
  },
  "31": {
    "id": "31",
    "title": "Installation",
    "content": "Spark NLP Cheat Sheet # Install Spark NLP from PyPI pip install spark-nlp==3.1.0 # Install Spark NLP from Anacodna/Conda conda install -c johnsnowlabs spark-nlp # Load Spark NLP with Spark Shell spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0 # Load Spark NLP with PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0 # Load Spark NLP with Spark Submit spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0 # Load Spark NLP as external JAR after compiling and building Spark NLP by `sbt assembly` spark-shell --jar spark-nlp-assembly-3.1.0 Python Spark NLP supports Python 3.6.x and 3.7.x if you are using PySpark 2.3.x or 2.4.x and Python 3.8.x if you are using PySpark 3.x. Quick Install Let’s create a new Conda environment to manage all the dependencies there. You can use Python Virtual Environment if you prefer or not have any enviroment. $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.7 -y $ conda activate sparknlp $ pip install spark-nlp==3.1.0 pyspark==3.1.1 Of course you will need to have jupyter installed in your system: pip install jupyter Now you should be ready to create a jupyter notebook running from terminal: jupyter notebook Start Spark NLP Session from python If you need to manually start SparkSession because you have other configuraations and sparknlp.start() is not including them, you can manually start the SparkSession: spark = SparkSession.builder .appName(&quot;Spark NLP&quot;) .master(&quot;local[4]&quot;) .config(&quot;spark.driver.memory&quot;,&quot;16G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0&quot;) .getOrCreate() Scala and Java Maven Spark NLP supports Scala 2.11.x if you are using Apache Spark 2.3.x or 2.4.x and Scala 2.12.x if you are using Apache Spark 3.0.x or 3.1.x. Our packages are deployed to Maven central. To add any of our packages as a dependency in your application you can follow these coordinates: spark-nlp on Apache Spark 3.x: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark24 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark24 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.0/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; SBT spark-nlp on Apache Spark 3.x.x: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp&quot; % &quot;3.1.0&quot; spark-nlp-gpu: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-gpu&quot; % &quot;3.1.0&quot; spark-nlp on Apache Spark 2.4.x: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-spark24&quot; % &quot;3.1.0&quot; spark-nlp-gpu: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-gpu-spark24&quot; % &quot;3.1.0&quot; spark-nlp on Apache Spark 2.3.x: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-spark23 libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-spark23&quot; % &quot;3.1.0&quot; spark-nlp-gpu: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu-spark23 libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-gpu-spark23&quot; % &quot;3.1.0&quot; Maven Central: https://mvnrepository.com/artifact/com.johnsnowlabs.nlp Google Colab Notebook Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account. Run the following code in Google Colab notebook and start using spark-nlp right away. # This is only to setup PySpark and Spark NLP on Colab !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash This script comes with the two options to define pyspark and spark-nlp versions via options: # -p is for pyspark # -s is for spark-nlp # by default they are set to the latest !bash colab.sh -p 3.1.1 -s 3.1.0 Spark NLP quick start on Google Colab is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines. Kaggle Kernel Run the following code in Kaggle Kernel and start using spark-nlp right away. # Let&#39;s setup Kaggle for Spark NLP and PySpark !wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash Spark NLP quick start on Kaggle Kernel is a live demo on Kaggle Kernel that performs named entity recognitions by using Spark NLP pretrained pipeline. Databricks Support Spark NLP 3.1.0 has been tested and is compatible with the following runtimes: CPU: 5.5 LTS 5.5 LTS ML 6.4 6.4 ML 7.3 7.3 ML 7.4 7.4 ML 7.5 7.5 ML 7.6 7.6 ML 8.0 8.0 ML 8.1 8.1 ML 8.2 8.2 ML 8.3 8.3 ML GPU: 8.1 ML &amp; GPU 8.2 ML &amp; GPU 8.3 ML &amp; GPU NOTE: Spark NLP 3.1.x is based on TensorFlow 2.4.x which is compatible with CUDA11 and cuDNN 8.0.2. The only Databricks runtimes supporting CUDA 11. are 8.1 ML with GPU, 8.2 ML with GPU, and 8.3 ML with GPU. Install Spark NLP on Databricks Create a cluster if you don’t have one already On a new cluster or existing one you need to add the following to the Advanced Options -&gt; Spark tab: spark.kryoserializer.buffer.max 2000M spark.serializer org.apache.spark.serializer.KryoSerializer In Libraries tab inside your cluster you need to follow these steps: 3.1. Install New -&gt; PyPI -&gt; spark-nlp -&gt; Install 3.2. Install New -&gt; Maven -&gt; Coordinates -&gt; com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0 -&gt; Install Now you can attach your notebook to the cluster and use Spark NLP! Databricks Notebooks You can view all the Databricks notebooks from this address: https://johnsnowlabs.github.io/spark-nlp-workshop/databricks/index.html Note: You can import these notebooks by using their URLs. EMR Support Spark NLP 3.1.0 has been tested and is compatible with the following EMR releases: emr-5.20.0 emr-5.21.0 emr-5.21.1 emr-5.22.0 emr-5.23.0 emr-5.24.0 emr-5.24.1 emr-5.25.0 emr-5.26.0 emr-5.27.0 emr-5.28.0 emr-5.29.0 emr-5.30.0 emr-5.30.1 emr-5.31.0 emr-5.32.0 emr-6.1.0 emr-6.2.0 emr-6.3.0 Full list of Amazon EMR 5.x releases Full list of Amazon EMR 6.x releases NOTE: The EMR 6.0.0 is not supported by Spark NLP 3.1.0 How to create EMR cluster via CLI To lanuch EMR cluster with Apache Spark/PySpark and Spark NLP correctly you need to have bootstrap and software configuration. A sample of your bootstrap script #!/bin/bash set -x -e echo -e &#39;export PYSPARK_PYTHON=/usr/bin/python3 export HADOOP_CONF_DIR=/etc/hadoop/conf export SPARK_JARS_DIR=/usr/lib/spark/jars export SPARK_HOME=/usr/lib/spark&#39; &gt;&gt; $HOME/.bashrc &amp;&amp; source $HOME/.bashrc sudo python3 -m pip install awscli boto spark-nlp set +x exit 0 A sample of your software configuration in JSON on S3 (must be public access): [{ &quot;Classification&quot;: &quot;spark-env&quot;, &quot;Configurations&quot;: [{ &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;PYSPARK_PYTHON&quot;: &quot;/usr/bin/python3&quot; } }] }, { &quot;Classification&quot;: &quot;spark-defaults&quot;, &quot;Properties&quot;: { &quot;spark.yarn.stagingDir&quot;: &quot;hdfs:///tmp&quot;, &quot;spark.yarn.preserve.staging.files&quot;: &quot;true&quot;, &quot;spark.kryoserializer.buffer.max&quot;: &quot;2000M&quot;, &quot;spark.serializer&quot;: &quot;org.apache.spark.serializer.KryoSerializer&quot;, &quot;spark.driver.maxResultSize&quot;: &quot;0&quot;, &quot;spark.jars.packages&quot;: &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0&quot; } } ] A sample of AWS CLI to launch EMR cluster: .sh aws emr create-cluster --name &quot;Spark NLP 3.1.0&quot; --release-label emr-6.2.0 --applications Name=Hadoop Name=Spark Name=Hive --instance-type m4.4xlarge --instance-count 3 --use-default-roles --log-uri &quot;s3://&lt;S3_BUCKET&gt;/&quot; --bootstrap-actions Path=s3://&lt;S3_BUCKET&gt;/emr-bootstrap.sh,Name=custome --configurations &quot;https://&lt;public_access&gt;/sparknlp-config.json&quot; --ec2-attributes KeyName=&lt;your_ssh_key&gt;,EmrManagedMasterSecurityGroup=&lt;security_group_with_ssh&gt;,EmrManagedSlaveSecurityGroup=&lt;security_group_with_ssh&gt; --profile &lt;aws_profile_credentials&gt; Docker Support For having Spark NLP, PySpark, Jupyter, and other ML/DL dependencies as a Docker image you can use the following template: #Download base image ubuntu 18.04 FROM ubuntu:18.04 ENV NB_USER jovyan ENV NB_UID 1000 ENV HOME /home/${NB_USER} ENV PYSPARK_PYTHON=python3 ENV PYSPARK_DRIVER_PYTHON=python3 RUN apt-get update &amp;&amp; apt-get install -y tar wget bash rsync gcc libfreetype6-dev libhdf5-serial-dev libpng-dev libzmq3-dev python3 python3-dev python3-pip unzip pkg-config software-properties-common graphviz RUN adduser --disabled-password --gecos &quot;Default user&quot; --uid ${NB_UID} ${NB_USER} # Install OpenJDK-8 RUN apt-get update &amp;&amp; apt-get install -y openjdk-8-jdk &amp;&amp; apt-get install -y ant &amp;&amp; apt-get clean; # Fix certificate issues RUN apt-get update &amp;&amp; apt-get install ca-certificates-java &amp;&amp; apt-get clean &amp;&amp; update-ca-certificates -f; # Setup JAVA_HOME -- useful for docker commandline ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/ RUN export JAVA_HOME RUN echo &quot;export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/&quot; &gt;&gt; ~/.bashrc RUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* RUN pip3 install --upgrade pip # You only need pyspark and spark-nlp paclages to use Spark NLP # The rest of the PyPI packages are here as examples RUN pip3 install --no-cache-dir pyspark==3.1.1 spark-nlp==3.1.0 notebook==5.* numpy pandas mlflow Keras scikit-spark scikit-learn scipy matplotlib pydot tensorflow==2.4.1 graphviz # Make sure the contents of our repo are in ${HOME} RUN mkdir -p /home/jovyan/tutorials RUN mkdir -p /home/jovyan/jupyter COPY data ${HOME}/data COPY jupyter ${HOME}/jupyter COPY tutorials ${HOME}/tutorials RUN jupyter notebook --generate-config COPY jupyter_notebook_config.json /home/jovyan/.jupyter/jupyter_notebook_config.json USER root RUN chown -R ${NB_UID} ${HOME} USER ${NB_USER} WORKDIR ${HOME} # Specify the default command to run CMD [&quot;jupyter&quot;, &quot;notebook&quot;, &quot;--ip&quot;, &quot;0.0.0.0&quot;] Finally, use jupyter_notebook_config.json for the password: { &quot;NotebookApp&quot;: { &quot;password&quot;: &quot;sha1:65adaa6ffb9c:36df1c2086ef294276da703667d1b8ff38f92614&quot; } } Windows Support In order to fully take advantage of Spark NLP on Windows (8 or 10), you need to setup/install Apache Spark, Apache Hadoop, and Java correctly by following the following instructions: https://github.com/JohnSnowLabs/spark-nlp/discussions/1022 How to correctly install Spark NLP on Windows 8 and 10 Follow the below steps: Download OpenJDK from here: https://adoptopenjdk.net/?variant=openjdk8&amp;jvmVariant=hotspot; Make sure it is 64-bit Make sure you install it in the root C: java Windows . During installation after changing the path, select setting Path Download winutils and put it in C: hadoop bin https://github.com/cdarlint/winutils/blob/master/hadoop-2.7.3/bin/winutils.exe; Download Anaconda 3.6 from Archive: https://repo.anaconda.com/archive/Anaconda3-2020.02-Windows-x86_64.exe; Download Apache Spark 3.1.1 and extract it in C: spark Set the env for HADOOP_HOME to C: hadoop and SPARK_HOME to C: spark Set Paths for %HADOOP_HOME% bin and %SPARK_HOME% bin Install C++ https://www.microsoft.com/en-us/download/confirmation.aspx?id=14632 Create C: temp and C: temp hive Fix permissions: C: Users maz&gt;%HADOOP_HOME% bin winutils.exe chmod 777 /tmp/hive C: Users maz&gt;%HADOOP_HOME% bin winutils.exe chmod 777 /tmp/ Either create a conda env for python 3.6, install pyspark==3.1.1 spark-nlp numpy and use Jupyter/python console, or in the same conda env you can go to spark bin for pyspark –packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0. Offline Spark NLP library and all the pre-trained models/pipelines can be used entirely offline with no access to the Internet. If you are behind a proxy or a firewall with no access to the Maven repository (to download packages) or/and no access to S3 (to automatically download models and pipelines), you can simply follow the instructions to have Spark NLP without any limitations offline: Instead of using the Maven package, you need to load our Fat JAR Instead of using PretrainedPipeline for pretrained pipelines or the .pretrained() function to download pretrained models, you will need to manually download your pipeline/model from Models Hub, extract it, and load it. Example of SparkSession with Fat JAR to have Spark NLP offline: spark = SparkSession.builder .appName(&quot;Spark NLP&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;,&quot;16G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars&quot;, &quot;/tmp/spark-nlp-assembly-3.1.0.jar&quot;) .getOrCreate() You can download provided Fat JARs from each release notes, please pay attention to pick the one that suits your environment depending on the device (CPU/GPU) and Apache Spark version (2.3.x, 2.4.x, and 3.x) If you are local, you can load the Fat JAR from your local FileSystem, however, if you are in a cluster setup you need to put the Fat JAR on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., hdfs:///tmp/spark-nlp-assembly-3.1.0.jar) Example of using pretrained Models and Pipelines in offline: # instead of using pretrained() for online: # french_pos = PerceptronModel.pretrained(&quot;pos_ud_gsd&quot;, lang=&quot;fr&quot;) # you download this model, extract it, and use .load french_pos = PerceptronModel.load(&quot;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) # example for pipelines # instead of using PretrainedPipeline # pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;) # you download this pipeline, extract it, and use PipelineModel PipelineModel.load(&quot;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&quot;) Since you are downloading and loading models/pipelines manually, this means Spark NLP is not downloading the most recent and compatible models/pipelines for you. Choosing the right model/pipeline is on you If you are local, you can load the model/pipeline from your local FileSystem, however, if you are in a cluster setup you need to put the model/pipeline on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., hdfs:///tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/)",
    "url": "/docs/en/install",
    "relUrl": "/docs/en/install"
  },
  "32": {
    "id": "32",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/languages_africa",
    "relUrl": "/languages_africa"
  },
  "33": {
    "id": "33",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/languages_india",
    "relUrl": "/languages_india"
  },
  "34": {
    "id": "34",
    "title": "",
    "content": "",
    "url": "/latest.html",
    "relUrl": "/latest.html"
  },
  "35": {
    "id": "35",
    "title": "Learn",
    "content": "Introductions to Spark NLP Videos State of the Art Natural Language Processing at Scale. David Talby - April 13, 2020 Spark NLP: State of the art natural language processing at scale. David Talby - 4 Jun 2020 What is Spark NLP. John Snow Labs - 30 Jul 2019 Apache Spark NLP Extending Spark ML to Deliver Fast, Scalable, and Unified Natural Language Process. David Talby - 6 May 2019 Natural Language Understanding at Scale with Spark Native NLP, Spark ML &amp;TensorFlow with Alex Thomas. Alex Thomas - 26 Oct 2017 Articles Introducing the Natural Language Processing Library for Apache SparkDavid Talby - October 19, 2017 Improving Clinical Document Understanding on COVID-19 Research with Spark NLPVeysel Kocaman, David Talby - 7 December, 2020 A Google Colab Notebook Introducing Spark NLPVeysel Kocaman - September, 2020 State-of-the-art Natural Language Processing at ScaleDavid Talby - April 13, 2020 How to Wrap Your Head Around Spark NLPMustafa Aytuğ Kaya - August 25, 2020 5 Reasons Why Spark NLP Is The Most Widely Used Library In EnterprisesAmbika Choudhury - May 28, 2019 My Experience with SparkNLP Workshop &amp; CertificationAngelina Maria Leigh - August 17, 2020 Out of the box Spark NLP models in actionDia Trambitas - August 14, 2020 Get started with Machine Learning in Java using Spark NLPWill Price - August 27, 2020 SPARK NLP 3: MASSIVE SPEEDUPS &amp; THE LATEST COMPUTE PLATFORMSMaziyar Panahi - March 25, 2021 SPARK NLP 2.7: 720+ NEW MODELS &amp; PIPELINES FOR 192 LANGUAGES!David Talby - January 05, 2021 Python’s NLU Library Videos John Snow Labs NLU: Become a Data Science Superhero with One Line of Python code. Christian Kasim Loan - November, 2020 Making State of the Art Natural Language Processing Easier to Apply. David Talby - 3 Nov 2020 Articles 1 line to GLOVE Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to XLNET Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to ALBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to COVIDBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to ELECTRA Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to BioBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 Line of Code, 350 + NLP Models with John Snow Labs’ NLU in PythonChristian Kasim Loan - September 21, 2020 Easy sentence similarity with BERT Sentence Embeddings using John Snow Labs NLUChristian Kasim Loan - November 20, 2020 Training Deep Learning NLP Classifier TutorialChristian Kasim Loan - November 20, 2020 1 Python Line for ELMo Word Embeddings and t-SNE plots with John Snow Labs’ NLUChristian Kasim Loan - October 24, 2020 1 line of Python code for BERT, ALBERT, ELMO, ELECTRA, XLNET, GLOVE, Part of Speech with NLU and t-SNEChristian Kasim Loan - September 21, 2020 1 line to BERT Word Embeddings with NLU in PythonChristian Kasim Loan - September 21, 2020 Question answering, intent classification, aspect based ner, and new multilingual models in python’s NLU libraryChristian Kasim Loan - February 12, 2021 Intent and action classification, analyze chinese news and crypto market, 200+ languages &amp; answer questions with NLU 1.1.3Christian Kasim Loan - March 02, 2021 Hindi wordembeddings, bengali named entity recognition, 30+ new models, analyze crypto news with NLU 1.1.2Christian Kasim Loan - February 18, 2021 Named Entity Recognition Videos State-of-the-art Clinical Named Entity Recognition in Spark NLP Workshop - Veysel Kocaman Train your own NerDL. John Snow Labs - 7 Oct 2019 Articles State-of-the-art named entity recognition with BERTVeysel Kocaman - February 26th, 2020 State-of-the-art Named Entity Recognition in Spark NLPVeysel Kocaman Spark NLP in action: intelligent, high-accuracy fact extraction from long financial documentsSaif Addin Ellafi - May 5, 2020 Named Entity Recognition (NER) with BERT in Spark NLPVeysel Kocaman - Mar 4, 2020 Document Classification Videos Spark NLP in Action: Learning to read Life Science research - Saif Addin Ellafi. Saif Addin Ellafi - 1 Aug 2018 State of the art emotion and sentiment analysis with Spark NLP (Data science Salon). Dia Trambitas - December 1, 2020 Articles GloVe, ELMo &amp; BERT. A guide to state-of-the-art text classification using Spark NLP Ryan Burke - March 16, 2021 Distributed Topic Modelling using Spark NLP and Spark MLLib(LDA)Satish Silveri - June 11, 2020 Text Classification in Spark NLP with Bert and Universal Sentence EncodersVeysel Kocaman - April 12, 2020 Classification of Unstructured Documents into the Environmental, Social &amp; Governance (ESG) TaxonomyAlina Petukhova - May, 2020 Using Spark NLP to build a drug discovery knowledge graph for COVID-19Vishnu Vettrivel, Alexander Thomas - October 8, 2020 Build Text Categorization Model with Spark NLPSatish Silveri - Jul 8 2020 Topic Modelling with PySpark and Spark NLPMaria Obedkova - May 29 2020 Spark NLP Tasks &amp; Pipelines Videos Spark NLP Annotators, Annotations and Pipelines. John Snow Labs - 23 Oct 2019 Your first Spark NLP Pipeline. John Snow Labs - 23 Oct 2019 Natural Language Understanding at Scale with Spark NLP | DSS 2020. Veysel Kocaman - December 12, 2020 Articles Cleaning and extracting text from HTML/XML documents by using Spark NLPStefano Lori - January 13 2021 NER model with ELMo Embeddings in 5 minutes in Spark-NLPChristian Kasim Loan - Jule 2020 Applying Context Aware Spell Checking in Spark NLPAlberto Andreotti - May 2020 Spark nlp 2.5 delivers state-of-the-art accuracy for spell checking and sentiment analysisIda Lucente - May 12, 2020 Spark NLP 2.4: More Accurate NER, OCR, and Entity ResolutionIda Lucente - February 14, 2020 Introduction to Spark NLP: Foundations and Basic Components (Part-I)Veysel Kocaman - Sep 29, 2019 Introducing Spark NLP: Why would we need another NLP library (Part-I)Veysel Kocaman - October 22, 2019 Introducing Spark NLP: basic components and underlying technologies (Part-III)Veysel Kocaman - December 2, 2019 Explain document DL – Spark NLP pretrained pipelineVeysel Kocaman - January 15, 2020 Spark NLP Walkthrough, powered by TensorFlowSaif Addin Ellafi - Nov 19, 2018 Natural Language Processing with PySpark and Spark-NLPAllison Honold - Feb 5, 2020 Spark NLP for Healthcare Videos Advancing the State of the Art in Applied Natural Language Processing | Healthcare NLP Summit 2021. David Talby - 21 Apr 2021 Connecting the Dots in Clinical Document Understanding &amp; Information Extraction I Health NLP Summit. Veysel Kocaman - 15 Apr 2021 Advanced Natural Language Processing with Apache Spark NLP. David Talby - 20 Aug 2020 Applying State-of-the-art Natural Language Processing for Personalized Healthcare. David Talby - April 13, 2020 Apache Spark NLP for Healthcare: Lessons Learned Building Real-World Healthcare AI Systems. Veysel Kocaman - 9 Jul 2020 SNOMED entity resolver. John Snow Labs - 31 Jul 2020 NLP and its applications in Healthcare. Veysel Kocaman - 17 May 2020 Lessons Learned Building Real-World Healthcare AI Systems. Veysel Kocaman - April 13, 2020 Application of Spark NLP for Development of Multi-Modal Prediction Model from EHR | Healthcare NLP. Sutanay Choudhury - 14 Apr 2021 Best Practices in Improving NLP Accuracy for Clinical Use Cases I Healthcare NLP Summit 2021. Rajesh Chamarthi, Veysel Kocaman - 15 Apr 2021 Articles Named Entity Recognition for Healthcare with SparkNLP NerDL and NerCRFMaggie Yilmaz - Jul 20 2020 Roche automates knowledge extraction from pathology reports with Spark NLPCase Study Spark NLP in action: Improving patient flow forecastingCase Study Using Spark NLP to Enable Real-World Evidence (RWE) and Clinical Decision Support in OncologyVeysel Kocaman - April 13, 2020 Applying State-of-the-art Natural Language Processing for Personalized HealthcareDavid Talby - April 13, 2020 Automated Mapping of Clinical Entities from Natural Language Text to Medical TerminologiesAndrés Fernández - April 29 2020 Contextual Parser in Spark NLP: Extracting Medical Entities ContextuallyAlina Petukhova - May 28 2020 Deep6 accelerates clinical trial recruitment with Spark NLPCase Study SelectData uses AI to better understand home health patientsCase Study Explain Clinical Document Spark NLP Pretrained PipelineVeysel Kocaman - January 20, 2020 Introducing Spark NLP: State of the art NLP Package (Part-II)Veysel Kocaman - January 20, 2020 Automated Adverse Drug Event (ADE) Detection from Text in Spark NLP with BioBertVeysel Kocaman - Octover 4, 2020 Normalize drug names and dosage units with spark NLPDavid Cecchini - February 23, 2021 Spark NLP for healthcare 2.7.3 with biobert extraction models, higher accuracy, de-identification, new radiology ner model &amp; moreVeysel Kocaman - February 09, 2021 Spark OCR &amp; De-Identification Videos Maximizing Text Recognition Accuracy with Image Transformers in Spark OCR. Mykola Melnyk - June 24, 2020 Accurate de-identification, obfuscation, and editing of scanned medical documents and images. Alina Petukhova - August 19, 2020 Accurate De-Identification of Structured &amp; Unstructured Medical Data at Scale. Julio Bonis - March 18, 2020 Articles A Unified CV, OCR &amp; NLP Model Pipeline for Document Understanding at DocuSignPatrick Beukema, Michael Chertushkin - October 6, 2020 Scaling High-Accuracy Text Extraction from Images using Spark OCR on DatabricksMikola Melnyk - July 2, 2020 Spark NLP at Scale Videos Turbocharging State-of-the-art Natural Language Processing on Ray. David Talby - October 3, 2020 Articles Big Data Analysis of Meetup Events using Spark NLP, Kafka and Vegas VisualizationAndrei Deuşteanu - August 25, 2020 Setup Spark NLP on Databricks in 2 Minutes and get the taste of scalable NLPChristian Kasim Loan - May 25, 2020 Real-time trending topic detection using Spark NLP, Kafka and Vegas VisualizationValentina Crisan - Oct 15, 2020 Mueller Report for Nerds! Spark meets NLP with TensorFlow and BERTMaziyar Panahi - May 1, 2019 Spark in Docker in Kubernetes: A Practical Approach for Scalable NLPJürgen Schmidl - Jan 18 2020 Running Spark NLP in Docker Container for Named Entity Recognition and Other NLP FeaturesYuefeng Zhang - Jun 5 2020 Annotation Lab Videos Accelerating Clinical Data Abstraction and Real-World Data Curation with Active Learning, Dia Trambitas - Apr 15, 2021 MLOPS Veysel &amp; Dia. Dia Trambitas, Veysel Kocaman - July 16, 2020 Best Practices &amp; Tools for Accurate Document Annotation and Data Abstraction. Dia Trambitas - May 27, 2020 Articles John Snow Labs’ data annotator &amp; active learning for human-in-the-loop AI is now included with all subscriptionsIda Lucente - May 26, 2020 Auto NLP: Pretrain, Tune &amp; Deploy State-of-the-art Models Without CodingDia Trambitas - October 6, 2020 Lesson Learned annotating training data for healthcare NLP projectsRebecca Leung, Marianne Mak - October 8, 2020 Task review workflows in the annotation labDia Trambitas - March 08, 2021 The annotation lab 1.1 is here with improvements to speed, accuracy, and productivityIda Lucente - January 20, 2021 Tips and tricks on how to annotate assertion in clinical textsMauro Nievas Offidani - November 24, 2020 Spark NLP Benchmarks Articles Biomedical Named Entity Recognition at ScaleVeysel Kocaman, David Talby - November 12, 2020 NLP Industry Survey Analysis: the industry landscape of natural language use cases in 2020Paco Nathan - October 6, 2020 Comparing the Functionality of Open Source Natural Language Processing LibrariesMaziyar Panahi and David Talby - April 7, 2019 SpaCy or Spark NLP — A Benchmarking ComparisonMustafa Aytuğ Kaya - Aug 27, 2020 Comparing production-grade NLP libraries: Training Spark-NLP and spaCy pipelinesSaif Addin Ellafi - February 28, 2018 Comparing production-grade NLP libraries: Running Spark-NLP and spaCy pipelinesSaif Addin Ellafi - February 28, 2018 Comparing production-grade NLP libraries: Accuracy, performance, and scalabilitySaif Addin Ellafi - February 28, 2018 Spark NLP Awards Articles John Snow Labs is healthcare tech outlook’s 2020 healthcare analytics provider of the yearIda Lucente - July 14, 2020 John Snow Labs wins the 2020 artificial intelligence excellence awardIda Lucente - April 27, 2020 John Snow Labs is named ‘2019 ai platform of the yearIda Lucente - August 14, 2019 Spark NLP is the world’s most widely used nlp library by enterprise practitionersIda Lucente - May 6, 2019 John Snow Labs’ spark nlp wins “most significant open source project” at the strata data awardsIda Lucente April 1 - 2019 John Snow Labs named “artificial intelligence solution provider of the year” by cio reviewIda Lucente - February 7, 2019",
    "url": "/learn",
    "relUrl": "/learn"
  },
  "36": {
    "id": "36",
    "title": "Spark NLP for Healthcare Annotators",
    "content": "A Spark NLP for Healthcare subscription includes access to several pretrained annotators. Check out the Spark NLP Annotators page for more information. AssertionDL Approach scaladocs Model scaladocs A Deep Learning based approach is used to extract Assertion Status from extracted entities and text. AssertionDLModel requires DOCUMENT, CHUNK and WORD_EMBEDDINGS type annotator inputs, which can be obtained by e.g a DocumentAssembler, NerConverter and WordEmbeddingsModel. The result is an assertion status annotation for each recognized entity. Possible values include “present”, “absent”, “hypothetical”, “conditional”, “associated_with_other_person” etc. For pretrained models please see the Models Hub for available models. Input types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output type: ASSERTION Show Example PythonScala // Define pipeline stages to extract NER chunks first val data = Seq( &quot;Patient with severe fever and sore throat&quot;, &quot;Patient shows no stomach pain&quot;, &quot;She was maintained on an epidural and PCA for pain control.&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setOutputCol(&quot;embeddings&quot;) val nerModel = MedicalNerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;).setOutputCol(&quot;ner&quot;) val nerConverter = new NerConverter().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;).setOutputCol(&quot;ner_chunk&quot;) // Then a pretrained AssertionDLModel is used to extract the assertion status val clinicalAssertion = AssertionDLModel.pretrained(&quot;assertion_dl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion )) val assertionModel = assertionPipeline.fit(data) // Show results val result = assertionModel.transform(data) result.selectExpr(&quot;ner_chunk.result&quot;, &quot;assertion.result&quot;).show(3, truncate=false) +--+--+ |result |result | +--+--+ |[severe fever, sore throat] |[present, present] | |[stomach pain] |[absent] | |[an epidural, PCA, pain control]|[present, present, hypothetical]| +--+--+ # Define pipeline stages to extract NER chunks first data = spark.createDataFrame([ [&quot;Patient with severe fever and sore throat&quot;], [&quot;Patient shows no stomach pain&quot;], [&quot;She was maintained on an epidural and PCA for pain control.&quot;]]).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setOutputCol(&quot;embeddings&quot;) nerModel = MedicalNerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]).setOutputCol(&quot;ner&quot;) nerConverter = NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]).setOutputCol(&quot;ner_chunk&quot;) # Then a pretrained AssertionDLModel is used to extract the assertion status clinicalAssertion = AssertionDLModel.pretrained(&quot;assertion_dl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion ]) assertionModel = assertionPipeline.fit(data) # Show results result = assertionModel.transform(data) result.selectExpr(&quot;ner_chunk.result&quot;, &quot;assertion.result&quot;).show(3, truncate=False) +--+--+ |result |result | +--+--+ |[severe fever, sore throat] |[present, present] | |[stomach pain] |[absent] | |[an epidural, PCA, pain control]|[present, present, hypothetical]| +--+--+ AssertionFilterer Model scaladocs Filters entities coming from ASSERTION type annotations and returns the CHUNKS. Filters can be set via a white list on the extracted chunk, the assertion or a regular expression. White list for assertion is enabled by default. To use chunk white list, criteria has to be set to &quot;isin&quot;. For regex, criteria has to be set to &quot;regex&quot;. Input types: DOCUMENT, CHUNK, ASSERTION Output type: CHUNK Show Example PythonScala // To see how the assertions are extracted, see the example for AssertionDLModel. // Define an extra step where the assertions are filtered val assertionFilterer = new AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;assertion&quot;) .setWhiteList(&quot;present&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion, assertionFilterer )) val assertionModel = assertionPipeline.fit(data) val result = assertionModel.transform(data) // Show results: result.selectExpr(&quot;ner_chunk.result&quot;, &quot;assertion.result&quot;).show(3, truncate=false) +--+--+ |result |result | +--+--+ |[severe fever, sore throat] |[present, present] | |[stomach pain] |[absent] | |[an epidural, PCA, pain control]|[present, present, hypothetical]| +--+--+ result.select(&quot;filtered.result&quot;).show(3, truncate=false) ++ |result | ++ |[severe fever, sore throat]| |[] | |[an epidural, PCA] | ++ # To see how the assertions are extracted, see the example for AssertionDLModel. # Define an extra step where the assertions are filtered assertionFilterer = AssertionFilterer() .setInputCols([&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;]) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;assertion&quot;) .setWhiteList([&quot;present&quot;]) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion, assertionFilterer ]) assertionModel = assertionPipeline.fit(data) result = assertionModel.transform(data) # Show results: result.selectExpr(&quot;ner_chunk.result&quot;, &quot;assertion.result&quot;).show(3, truncate=False) +--+--+ |result |result | +--+--+ |[severe fever, sore throat] |[present, present] | |[stomach pain] |[absent] | |[an epidural, PCA, pain control]|[present, present, hypothetical]| +--+--+ result.select(&quot;filtered.result&quot;).show(3, truncate=False) ++ |result | ++ |[severe fever, sore throat]| |[] | |[an epidural, PCA] | ++ AssertionLogReg Approach scaladocs Model scaladocs This annotator classifies each clinically relevant named entity into its assertion: type: “present”, “absent”, “hypothetical”, “conditional”, “associated_with_other_person”, etc. Input types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output type: ASSERTION Show Example PythonScala // Training with Glove Embeddings // First define pipeline stages to extract embeddings and text chunks val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val glove = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;word_embeddings&quot;) .setCaseSensitive(false) val chunkAssembler = new Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) // Then the AssertionLogRegApproach model is defined. Label column is needed in the dataset for training. val assertion = new AssertionLogRegApproach() .setLabelCol(&quot;label&quot;) .setInputCols(&quot;document&quot;, &quot;chunk&quot;, &quot;word_embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, assertion )) val assertionModel = assertionPipeline.fit(dataset) # Training with Glove Embeddings # First define pipeline stages to extract embeddings and text chunks documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) glove = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) .setCaseSensitive(False) chunkAssembler = Doc2Chunk() .setInputCols([&quot;document&quot;]) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) # Then the AssertionLogRegApproach model is defined. Label column is needed in the dataset for training. assertion = AssertionLogRegApproach() .setLabelCol(&quot;label&quot;) .setInputCols([&quot;document&quot;, &quot;chunk&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, assertion ]) assertionModel = assertionPipeline.fit(dataset) Chunk2Token API scaladocs A feature transformer that converts the input array of strings (annotatorType CHUNK) into an array of chunk-based tokens (annotatorType TOKEN). When the input is empty, an empty array is returned. This Annotator is specially convenient when using NGramGenerator annotations as inputs to WordEmbeddingsModels Input types: CHUNK Output type: TOKEN Show Example PythonScala // Define a pipeline for generating n-grams val data = Seq((&quot;A 63-year-old man presents to the hospital ...&quot;)).toDF(&quot;text&quot;) val document = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val token = new Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val ngrammer = new NGramGenerator() .setN(2) .setEnableCumulative(false) .setInputCols(&quot;token&quot;) .setOutputCol(&quot;ngrams&quot;) .setDelimiter(&quot;_&quot;) // Stage to convert n-gram CHUNKS to TOKEN type val chunk2Token = new Chunk2Token().setInputCols(&quot;ngrams&quot;).setOutputCol(&quot;ngram_tokens&quot;) val trainingPipeline = new Pipeline().setStages(Array(document, sentenceDetector, token, ngrammer, chunk2Token)).fit(data) val result = trainingPipeline.transform(data).cache() result.selectExpr(&quot;explode(ngram_tokens)&quot;).show(5, false) +-+ |col | +-+ |{token, 3, 15, A_63-year-old, {sentence -&gt; 0, chunk -&gt; 0}, []} | |{token, 5, 19, 63-year-old_man, {sentence -&gt; 0, chunk -&gt; 1}, []}| |{token, 17, 28, man_presents, {sentence -&gt; 0, chunk -&gt; 2}, []} | |{token, 21, 31, presents_to, {sentence -&gt; 0, chunk -&gt; 3}, []} | |{token, 30, 35, to_the, {sentence -&gt; 0, chunk -&gt; 4}, []} | +-+ # Define a pipeline for generating n-grams data = spark.createDataFrame([[&quot;A 63-year-old man presents to the hospital ...&quot;]]).toDF(&quot;text&quot;) document = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) token = Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) ngrammer = NGramGenerator() .setN(2) .setEnableCumulative(False) .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;ngrams&quot;) .setDelimiter(&quot;_&quot;) # Stage to convert n-gram CHUNKS to TOKEN type chunk2Token = Chunk2Token().setInputCols([&quot;ngrams&quot;]).setOutputCol(&quot;ngram_tokens&quot;) trainingPipeline = Pipeline(stages=[document, sentenceDetector, token, ngrammer, chunk2Token]).fit(data) result = trainingPipeline.transform(data).cache() result.selectExpr(&quot;explode(ngram_tokens)&quot;).show(5, False) +-+ |col | +-+ |{token, 3, 15, A_63-year-old, {sentence -&gt; 0, chunk -&gt; 0}, []} | |{token, 5, 19, 63-year-old_man, {sentence -&gt; 0, chunk -&gt; 1}, []}| |{token, 17, 28, man_presents, {sentence -&gt; 0, chunk -&gt; 2}, []} | |{token, 21, 31, presents_to, {sentence -&gt; 0, chunk -&gt; 3}, []} | |{token, 30, 35, to_the, {sentence -&gt; 0, chunk -&gt; 4}, []} | +-+ ChunkEntityResolver Approach scaladocs Model scaladocs Contains all the parameters and methods to train a ChunkEntityResolverModel. It transform a dataset with two Input Annotations of types TOKEN and WORD_EMBEDDINGS, coming from e.g. ChunkTokenizer and ChunkEmbeddings Annotators and returns the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.) To use pretrained models please use ChunkEntityResolverModel and see the Models Hub for available models. Input types: TOKEN, WORD_EMBEDDINGS Output type: ENTITY Show Example PythonScala // Training a SNOMED model // Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data // and their labels. val document = new DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) val chunk = new Doc2Chunk() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;chunk&quot;) val token = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_healthcare_100d&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val chunkEmb = new ChunkEmbeddings() .setInputCols(&quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;chunk_embeddings&quot;) val snomedTrainingPipeline = new Pipeline().setStages(Array( document, chunk, token, embeddings, chunkEmb )) val snomedTrainingModel = snomedTrainingPipeline.fit(data) val snomedData = snomedTrainingModel.transform(data).cache() // Then the Resolver can be trained with val snomedExtractor = new ChunkEntityResolverApproach() .setInputCols(&quot;token&quot;, &quot;chunk_embeddings&quot;) .setOutputCol(&quot;recognized&quot;) .setNeighbours(1000) .setAlternatives(25) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setEnableWmd(true).setEnableTfidf(true).setEnableJaccard(true) .setEnableSorensenDice(true).setEnableJaroWinkler(true).setEnableLevenshtein(true) .setDistanceWeights(Array(1, 2, 2, 1, 1, 1)) .setAllDistancesMetadata(true) .setPoolingStrategy(&quot;MAX&quot;) .setThreshold(1e32) val model = snomedExtractor.fit(snomedData) # Training a SNOMED model # Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data # and their labels. document = DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) chunk = Doc2Chunk() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;chunk&quot;) token = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_healthcare_100d&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) chunkEmb = ChunkEmbeddings() .setInputCols([&quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;chunk_embeddings&quot;) snomedTrainingPipeline = Pipeline(stages=[ document, chunk, token, embeddings, chunkEmb ]) snomedTrainingModel = snomedTrainingPipeline.fit(data) snomedData = snomedTrainingModel.transform(data).cache() # Then the Resolver can be trained with snomedExtractor = ChunkEntityResolverApproach() .setInputCols([&quot;token&quot;, &quot;chunk_embeddings&quot;]) .setOutputCol(&quot;recognized&quot;) .setNeighbours(1000) .setAlternatives(25) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setEnableWmd(True).setEnableTfidf(True).setEnableJaccard(True) .setEnableSorensenDice(True).setEnableJaroWinkler(True).setEnableLevenshtein(True) .setDistanceWeights(Array(1, 2, 2, 1, 1, 1)) .setAllDistancesMetadata(True) .setPoolingStrategy(&quot;MAX&quot;) .setThreshold(1e32) model = snomedExtractor.fit(snomedData) ChunkFilterer Model scaladocs Filters entities coming from CHUNK annotations. Filters can be set via a white list of terms or a regular expression. White list criteria is enabled by default. To use regex, criteria has to be set to regex. Input types: DOCUMENT,CHUNK Output type: CHUNK Show Example PythonScala // Filtering POS tags // First pipeline stages to extract the POS tags are defined val data = Seq(&quot;Has a past history of gastroenteritis and stomach pain, however patient ...&quot;).toDF(&quot;text&quot;) val docAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val chunker = new Chunker() .setInputCols(&quot;pos&quot;, &quot;sentence&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;(&lt;NN&gt;)+&quot;)) // Then the chunks can be filtered via a white list. Here only terms with &quot;gastroenteritis&quot; remain. val chunkerFilter = new ChunkFilterer() .setInputCols(&quot;sentence&quot;,&quot;chunk&quot;) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList(&quot;gastroenteritis&quot;) val pipeline = new Pipeline().setStages(Array( docAssembler, sentenceDetector, tokenizer, posTagger, chunker, chunkerFilter)) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk)&quot;).show(truncate=false) ++ |col | ++ |{chunk, 11, 17, history, {sentence -&gt; 0, chunk -&gt; 0}, []} | |{chunk, 22, 36, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 42, 53, stomach pain, {sentence -&gt; 0, chunk -&gt; 2}, []} | |{chunk, 64, 70, patient, {sentence -&gt; 0, chunk -&gt; 3}, []} | |{chunk, 81, 110, stomach pain now.We don&#39;t care, {sentence -&gt; 0, chunk -&gt; 4}, []}| |{chunk, 118, 132, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 5}, []} | ++ result.selectExpr(&quot;explode(filtered)&quot;).show(truncate=false) +-+ |col | +-+ |{chunk, 22, 36, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 118, 132, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 5}, []}| +-+ # Filtering POS tags # First pipeline stages to extract the POS tags are defined data = spark.createDataFrame([[&quot;Has a past history of gastroenteritis and stomach pain, however patient ...&quot;]]).toDF(&quot;text&quot;) docAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) posTagger = PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) chunker = Chunker() .setInputCols([&quot;pos&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;(&lt;NN&gt;)+&quot;]) # Then the chunks can be filtered via a white list. Here only terms with &quot;gastroenteritis&quot; remain. chunkerFilter = ChunkFilterer() .setInputCols([&quot;sentence&quot;,&quot;chunk&quot;]) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList([&quot;gastroenteritis&quot;]) pipeline = Pipeline(stages=[ docAssembler, sentenceDetector, tokenizer, posTagger, chunker, chunkerFilter]) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk)&quot;).show(truncate=False) ++ |col | ++ |{chunk, 11, 17, history, {sentence -&gt; 0, chunk -&gt; 0}, []} | |{chunk, 22, 36, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 42, 53, stomach pain, {sentence -&gt; 0, chunk -&gt; 2}, []} | |{chunk, 64, 70, patient, {sentence -&gt; 0, chunk -&gt; 3}, []} | |{chunk, 81, 110, stomach pain now.We don&#39;t care, {sentence -&gt; 0, chunk -&gt; 4}, []}| |{chunk, 118, 132, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 5}, []} | ++ result.selectExpr(&quot;explode(filtered)&quot;).show(truncate=False) +-+ |col | +-+ |{chunk, 22, 36, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 118, 132, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 5}, []}| +-+ ChunkMerge Approach scaladocs Model scaladocs Merges NER Chunks by prioritizing overlapping indices (chunks with longer lengths and highest information will be kept from each ner model). Labels can be changed by setReplaceDictResource. Input types: CHUNK, CHUNK Output type: CHUNK Show Example PythonScala // Define a pipeline with 2 different NER models with a ChunkMergeApproach at the end val data = Seq((&quot;A 63-year-old man presents to the hospital ...&quot;)).toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;), new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;), new Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;), WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setOutputCol(&quot;embs&quot;), MedicalNerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;).setOutputCol(&quot;jsl_ner&quot;), new NerConverter().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;jsl_ner&quot;).setOutputCol(&quot;jsl_ner_chunk&quot;), MedicalNerModel.pretrained(&quot;ner_bionlp&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;).setOutputCol(&quot;bionlp_ner&quot;), new NerConverter().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;bionlp_ner&quot;) .setOutputCol(&quot;bionlp_ner_chunk&quot;), new ChunkMergeApproach().setInputCols(&quot;jsl_ner_chunk&quot;, &quot;bionlp_ner_chunk&quot;).setOutputCol(&quot;merged_chunk&quot;) )) // Show results val result = pipeline.fit(data).transform(data).cache() result.selectExpr(&quot;explode(merged_chunk) as a&quot;) .selectExpr(&quot;a.begin&quot;,&quot;a.end&quot;,&quot;a.result as chunk&quot;,&quot;a.metadata.entity as entity&quot;) .show(5, false) +--++--++ |begin|end|chunk |entity | +--++--++ |5 |15 |63-year-old|Age | |17 |19 |man |Gender | |64 |72 |recurrent |Modifier | |98 |107|cellulitis |Diagnosis| |110 |119|pneumonias |Diagnosis| +--++--++ # Define a pipeline with 2 different NER models with a ChunkMergeApproach at the end data = spark.createDataFrame([[&quot;A 63-year-old man presents to the hospital ...&quot;]]).toDF(&quot;text&quot;) pipeline = Pipeline(stages=[ DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;), SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;), Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;), WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setOutputCol(&quot;embs&quot;), MedicalNerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;]).setOutputCol(&quot;jsl_ner&quot;), NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;jsl_ner&quot;]).setOutputCol(&quot;jsl_ner_chunk&quot;), MedicalNerModel.pretrained(&quot;ner_bionlp&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;]).setOutputCol(&quot;bionlp_ner&quot;), NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;bionlp_ner&quot;]) .setOutputCol(&quot;bionlp_ner_chunk&quot;), ChunkMergeApproach().setInputCols([&quot;jsl_ner_chunk&quot;, &quot;bionlp_ner_chunk&quot;]).setOutputCol(&quot;merged_chunk&quot;) ]) # Show results result = pipeline.fit(data).transform(data).cache() result.selectExpr(&quot;explode(merged_chunk) as a&quot;) .selectExpr(&quot;a.begin&quot;,&quot;a.end&quot;,&quot;a.result as chunk&quot;,&quot;a.metadata.entity as entity&quot;) .show(5, False) +--++--++ |begin|end|chunk |entity | +--++--++ |5 |15 |63-year-old|Age | |17 |19 |man |Gender | |64 |72 |recurrent |Modifier | |98 |107|cellulitis |Diagnosis| |110 |119|pneumonias |Diagnosis| +--++--++ ContextualParser Approach scaladocs Model scaladocs Creates a model, that extracts entity from a document based on user defined rules. Rule matching is based on a RegexMatcher defined in a JSON file. It is set through the parameter setJsonPath(). In this JSON file, regex is defined that you want to match along with the information that will output on metadata field. Additionally, a dictionary can be provided with setDictionary to map extracted entities to a unified representation. The first column of the dictionary file should be the representation with following columns the possible matches. An example JSON file regex_token.json can look like this: { &quot;entity&quot;: &quot;Stage&quot;, &quot;ruleScope&quot;: &quot;sentence&quot;, &quot;regex&quot;: &quot;[cpyrau]?[T][0-9X?][a-z^cpyrau]*&quot;, &quot;matchScope&quot;: &quot;token&quot; } Which extracts the stage code on a sentence level. Input types: DOCUMENT, TOKEN Output type: CHUNK Show Example PythonScala // Pipeline could then be defined like this val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) // Define the parser (json file needs to be provided) val data = Seq(&quot;A patient has liver metastases pT1bN0M0 and the T5 primary site may be colon or... &quot;).toDF(&quot;text&quot;) val contextualParser = new ContextualParserApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;entity&quot;) .setJsonPath(&quot;/path/to/regex_token.json&quot;) .setCaseSensitive(true) .setContextMatch(false) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, contextualParser )) val result = pipeline.fit(data).transform(data) // Show Results result.selectExpr(&quot;explode(entity)&quot;).show(5, truncate=false) +-+ |col | +-+ |{chunk, 32, 39, pT1bN0M0, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 0}, []} | |{chunk, 49, 50, T5, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 0}, []} | |{chunk, 148, 156, cT4bcN2M1, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 1}, []}| |{chunk, 189, 194, T?N3M1, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 2}, []} | |{chunk, 316, 323, pT1bN0M0, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 3}, []} | +-+ # Pipeline could then be defined like this documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) # Define the parser (json file needs to be provided) data = spark.createDataFrame([[&quot;A patient has liver metastases pT1bN0M0 and the T5 primary site may be colon or... &quot;]]).toDF(&quot;text&quot;) contextualParser = ContextualParserApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;entity&quot;) .setJsonPath(&quot;/path/to/regex_token.json&quot;) .setCaseSensitive(True) .setContextMatch(False) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, contextualParser ]) result = pipeline.fit(data).transform(data) # Show Results result.selectExpr(&quot;explode(entity)&quot;).show(5, truncate=False) +-+ |col | +-+ |{chunk, 32, 39, pT1bN0M0, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 0}, []} | |{chunk, 49, 50, T5, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 0}, []} | |{chunk, 148, 156, cT4bcN2M1, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 1}, []}| |{chunk, 189, 194, T?N3M1, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 2}, []} | |{chunk, 316, 323, pT1bN0M0, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 3}, []} | +-+ DeIdentificator Approach scaladocs Model scaladocs Identifies potential pieces of content with personal information about patients and remove them by replacing with semantic tags. Ideally this annotator works in conjunction with Demographic Named EntityRecognizers that can be trained either using TextMatchers, RegexMatchers, DateMatchers, NerCRFs or NerDLs Example of pipeline for deidentification. Input types: DOCUMENT, TOKEN, CHUNK Output type: DOCUMENT Show Example PythonScala val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(true) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) // Ner entities val clinical_sensitive_entities = MedicalNerModel.pretrained(&quot;ner_deid_enriched&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)).setOutputCol(&quot;ner&quot;) val nerConverter = new NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_con&quot;) // Deidentification val deIdentification = new DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) // file with custom regex pattern for custom entities .setRegexPatternsDictionary(&quot;path/to/dic_regex_patterns_main_categories.txt&quot;) // file with custom obfuscator names for the entities .setObfuscateRefFile(&quot;path/to/obfuscate_fixed_entities.txt&quot;) .setRefFileFormat(&quot;csv&quot;) .setRefSep(&quot;#&quot;) .setMode(&quot;obfuscate&quot;) .setDateFormats(Array(&quot;MM/dd/yy&quot;,&quot;yyyy-MM-dd&quot;)) .setObfuscateDate(true) .setDateTag(&quot;DATE&quot;) .setDays(5) .setObfuscateRefSource(&quot;file&quot;) // Pipeline val data = Seq( &quot;# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09.&quot; ).toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, clinical_sensitive_entities, nerConverter, deIdentification )) val result = pipeline.fit(data).transform(data) // Show Results result.select(&quot;dei.result&quot;).show(truncate = false) +--+ |result | +--+ |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , &lt;AGE&gt; years-old , Record date : 2079-11-14.]| +--+ documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(True) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Ner entities clinical_sensitive_entities = MedicalNerModel .pretrained(&quot;ner_deid_enriched&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]).setOutputCol(&quot;ner&quot;) nerConverter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_con&quot;) # Deidentification deIdentification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) # file with custom regex pattern for custom entities .setRegexPatternsDictionary(&quot;path/to/dic_regex_patterns_main_categories.txt&quot;) # file with custom obfuscator names for the entities .setObfuscateRefFile(&quot;path/to/obfuscate_fixed_entities.txt&quot;) .setRefFileFormat(&quot;csv&quot;) .setRefSep(&quot;#&quot;) .setMode(&quot;obfuscate&quot;) .setDateFormats(Array(&quot;MM/dd/yy&quot;,&quot;yyyy-MM-dd&quot;)) .setObfuscateDate(True) .setDateTag(&quot;DATE&quot;) .setDays(5) .setObfuscateRefSource(&quot;file&quot;) # Pipeline data = spark.createDataFrame([ [&quot;# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09.&quot;] ]).toDF(&quot;text&quot;) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, clinical_sensitive_entities, nerConverter, deIdentification ]) result = pipeline.fit(data).transform(data) # Show Results result.select(&quot;dei.result&quot;).show(truncate = False) +--+ |result | +--+ |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , &lt;AGE&gt; years-old , Record date : 2079-11-14.]| +--+ DocumentLogRegClassifier Approach scaladocs Model scaladocs A convenient TFIDF-LogReg classifier that accepts “token” input type and outputs “selector”; an input type mainly used in RecursivePipelineModels Input types: TOKEN Output type: CATEGORY Show Example PythonScala // Define pipeline stages to prepare the data val document_assembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) val stopwords_cleaner = new StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val stemmer = new Stemmer() .setInputCols(&quot;cleanTokens&quot;) .setOutputCol(&quot;stem&quot;) // Define the document classifier and fit training data to it val logreg = new DocumentLogRegClassifierApproach() .setInputCols(&quot;stem&quot;) .setLabelCol(&quot;category&quot;) .setOutputCol(&quot;prediction&quot;) val pipeline = new Pipeline().setStages(Array( document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, logreg )) val model = pipeline.fit(trainingData) # Define pipeline stages to prepare the data document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) stopwords_cleaner = StopWordsCleaner() .setInputCols([&quot;normalized&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) stemmer = Stemmer() .setInputCols([&quot;cleanTokens&quot;]) .setOutputCol(&quot;stem&quot;) # Define the document classifier and fit training data to it logreg = DocumentLogRegClassifierApproach() .setInputCols([&quot;stem&quot;]) .setLabelCol(&quot;category&quot;) .setOutputCol(&quot;prediction&quot;) pipeline = Pipeline(stages=[ document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, logreg ]) model = pipeline.fit(trainingData) DrugNormalizer API scaladocs Annotator which normalizes raw text from clinical documents, e.g. scraped web pages or xml documents, from document type columns into Sentence. Removes all dirty characters from text following one or more input regex patterns. Can apply non wanted character removal which a specific policy. Can apply lower case normalization. See Spark NLP Workshop for more examples of usage. Input types: DOCUMENT Output type: DOCUMENT Show Example PythonScala val data = Seq( (&quot;Sodium Chloride/Potassium Chloride 13bag&quot;), (&quot;interferon alfa-2b 10 million unit ( 1 ml ) injec&quot;), (&quot;aspirin 10 meq/ 5 ml oral sol&quot;) ).toDF(&quot;text&quot;) val document = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val drugNormalizer = new DrugNormalizer().setInputCols(&quot;document&quot;).setOutputCol(&quot;document_normalized&quot;) val trainingPipeline = new Pipeline().setStages(Array(document, drugNormalizer)) val result = trainingPipeline.fit(data).transform(data) result.selectExpr(&quot;explode(document_normalized.result) as normalized_text&quot;).show(false) +-+ |normalized_text | +-+ |Sodium Chloride / Potassium Chloride 13 bag | |interferon alfa - 2b 10000000 unt ( 1 ml ) injection| |aspirin 2 meq/ml oral solution | +-+ data = spark.createDataFrame([ [&quot;Sodium Chloride/Potassium Chloride 13bag&quot;], [&quot;interferon alfa-2b 10 million unit ( 1 ml ) injec&quot;], [&quot;aspirin 10 meq/ 5 ml oral sol&quot;] ]).toDF(&quot;text&quot;) document = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) drugNormalizer = DrugNormalizer().setInputCols([&quot;document&quot;]).setOutputCol(&quot;document_normalized&quot;) trainingPipeline = Pipeline(stages=[document, drugNormalizer]) result = trainingPipeline.fit(data).transform(data) result.selectExpr(&quot;explode(document_normalized.result) as normalized_text&quot;).show(truncate=False) +-+ |normalized_text | +-+ |Sodium Chloride / Potassium Chloride 13 bag | |interferon alfa - 2b 10000000 unt ( 1 ml ) injection| |aspirin 2 meq/ml oral solution | +-+ GenericClassifierApproach Approach scaladocs Model scaladocs Trains a TensorFlow model for generic classification of feature vectors. It takes FEATURE_VECTOR annotations from FeaturesAssembler as input, classifies them and outputs CATEGORY annotations. Please see the Parameters section for required training parameters. For a more extensive example please see the Spark NLP Workshop. Input types: FEATURE_VECTOR Output type: CATEGORY Show Example PythonScala val features_asm = new FeaturesAssembler() .setInputCols(Array(&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;)) .setOutputCol(&quot;features&quot;) val gen_clf = new GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols(&quot;features&quot;) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001f) .setFixImbalance(true) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2f) // keep 20% of the data for validation purposes val pipeline = new Pipeline().setStages(Array( features_asm, gen_clf )) val clf_model = pipeline.fit(data) features_asm = FeaturesAssembler() .setInputCols([&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;]) .setOutputCol(&quot;features&quot;) gen_clf = GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setLearningRate(0.001) .setFixImbalance(True) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2) # keep 20% of the data for validation purposes pipeline = Pipeline(stages=[ features_asm, gen_clf ]) clf_model = pipeline.fit(data) IOBTagger API scaladocs Merges token tags and NER labels from chunks in the specified format. For example output columns as inputs from NerConverter and Tokenizer can be used to merge. Input types: TOKEN, CHUNK Output type: NAMED_ENTITY Show Example PythonScala // Pipeline stages are defined where NER is done. NER is converted to chunks. val data = Seq((&quot;A 63-year-old man presents to the hospital ...&quot;)).toDF(&quot;text&quot;) val docAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setOutputCol(&quot;embs&quot;) val nerModel = MedicalNerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;).setOutputCol(&quot;ner&quot;) val nerConverter = new NerConverter().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;).setOutputCol(&quot;ner_chunk&quot;) // Define the IOB tagger, which needs tokens and chunks as input. Show results. val iobTagger = new IOBTagger().setInputCols(&quot;token&quot;, &quot;ner_chunk&quot;).setOutputCol(&quot;ner_label&quot;) val pipeline = new Pipeline().setStages(Array(docAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, iobTagger)) result.selectExpr(&quot;explode(ner_label) as a&quot;) .selectExpr(&quot;a.begin&quot;,&quot;a.end&quot;,&quot;a.result as chunk&quot;,&quot;a.metadata.word as word&quot;) .where(&quot;chunk!=&#39;O&#39;&quot;).show(5, false) +--++--+--+ |begin|end|chunk |word | +--++--+--+ |5 |15 |B-Age |63-year-old| |17 |19 |B-Gender |man | |64 |72 |B-Modifier |recurrent | |98 |107|B-Diagnosis|cellulitis | |110 |119|B-Diagnosis|pneumonias | +--++--+--+ # Pipeline stages are defined where NER is done. NER is converted to chunks. data = spark.createDataFrame([[&quot;A 63-year-old man presents to the hospital ...&quot;]]).toDF(&quot;text&quot;) docAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setOutputCol(&quot;embs&quot;) nerModel = MedicalNerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;]).setOutputCol(&quot;ner&quot;) nerConverter = NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]).setOutputCol(&quot;ner_chunk&quot;) # Define the IOB tagger, which needs tokens and chunks as input. Show results. iobTagger = IOBTagger().setInputCols([&quot;token&quot;, &quot;ner_chunk&quot;]).setOutputCol(&quot;ner_label&quot;) pipeline = Pipeline(stages=[docAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, iobTagger]) result.selectExpr(&quot;explode(ner_label) as a&quot;) .selectExpr(&quot;a.begin&quot;,&quot;a.end&quot;,&quot;a.result as chunk&quot;,&quot;a.metadata.word as word&quot;) .where(&quot;chunk!=&#39;O&#39;&quot;).show(5, False) +--++--+--+ |begin|end|chunk |word | +--++--+--+ |5 |15 |B-Age |63-year-old| |17 |19 |B-Gender |man | |64 |72 |B-Modifier |recurrent | |98 |107|B-Diagnosis|cellulitis | |110 |119|B-Diagnosis|pneumonias | +--++--+--+ NerChunker API scaladocs Extracts phrases that fits into a known pattern using the NER tags. Useful for entity groups with neighboring tokens when there is no pretrained NER model to address certain issues. A Regex needs to be provided to extract the tokens between entities. Input types: DOCUMENT, NAMED_ENTITY Output type: CHUNK Show Example PythonScala // Defining pipeline stages for NER val data= Seq(&quot;She has cystic cyst on her kidney.&quot;).toDF(&quot;text&quot;) val documentAssembler=new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector=new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(false) val tokenizer=new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val ner = MedicalNerModel.pretrained(&quot;ner_radiology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) .setIncludeConfidence(true) // Define the NerChunker to combine to chunks val chunker = new NerChunker() .setInputCols(Array(&quot;sentence&quot;,&quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers(Array(&quot;&lt;ImagingFindings&gt;.*&lt;BodyPart&gt;&quot;)) val pipeline=new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner, chunker )) val result = pipeline.fit(data).transform(data) // Show results: result.selectExpr(&quot;explode(arrays_zip(ner.metadata , ner.result))&quot;) .selectExpr(&quot;col[&#39;0&#39;].word as word&quot; , &quot;col[&#39;1&#39;] as ner&quot;).show(truncate=false) ++--+ |word |ner | ++--+ |She |O | |has |O | |cystic|B-ImagingFindings| |cyst |I-ImagingFindings| |on |O | |her |O | |kidney|B-BodyPart | |. |O | ++--+ result.select(&quot;ner_chunk.result&quot;).show(truncate=false) ++ |result | ++ |[cystic cyst on her kidney]| ++ # Defining pipeline stages for NER data= spark.createDataFrame([[&quot;She has cystic cyst on her kidney.&quot;]]).toDF(&quot;text&quot;) documentAssembler= DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector= SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(False) tokenizer= Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) ner = MedicalNerModel.pretrained(&quot;ner_radiology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) .setIncludeConfidence(True) # Define the NerChunker to combine to chunks chunker = NerChunker() .setInputCols([&quot;sentence&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers([&quot;&lt;ImagingFindings&gt;.*&lt;BodyPart&gt;&quot;]) pipeline= Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner, chunker ]) result = pipeline.fit(data).transform(data) # Show results: result.selectExpr(&quot;explode(arrays_zip(ner.metadata , ner.result))&quot;) .selectExpr(&quot;col[&#39;0&#39;].word as word&quot; , &quot;col[&#39;1&#39;] as ner&quot;).show(truncate=False) ++--+ |word |ner | ++--+ |She |O | |has |O | |cystic|B-ImagingFindings| |cyst |I-ImagingFindings| |on |O | |her |O | |kidney|B-BodyPart | |. |O | ++--+ result.select(&quot;ner_chunk.result&quot;).show(truncate=False) ++ |result | ++ |[cystic cyst on her kidney]| ++ NerConverterInternal API scaladocs Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. Chunks with no associated entity (tagged “O”) are filtered. See also Inside–outside–beginning (tagging) for more information. Input types: DOCUMENT, TOKEN, NAMED_ENTITY Output type: CHUNK Show Example PythonScala // The output of a MedicalNerModel follows the Annotator schema and looks like this after the transformation. result.selectExpr(&quot;explode(ner_result)&quot;).show(5, false) +--+ |col | +--+ |{named_entity, 3, 3, O, {word -&gt; A, confidence -&gt; 0.994}, []} | |{named_entity, 5, 15, B-Age, {word -&gt; 63-year-old, confidence -&gt; 1.0}, []}| |{named_entity, 17, 19, B-Gender, {word -&gt; man, confidence -&gt; 0.9858}, []} | |{named_entity, 21, 28, O, {word -&gt; presents, confidence -&gt; 0.9952}, []} | |{named_entity, 30, 31, O, {word -&gt; to, confidence -&gt; 0.7063}, []} | +--+ // After the converter is used: result.selectExpr(&quot;explode(ner_converter_result)&quot;).show(5, false) +--+ |col | +--+ |{chunk, 5, 15, 63-year-old, {entity -&gt; Age, sentence -&gt; 0, chunk -&gt; 0}, []} | |{chunk, 17, 19, man, {entity -&gt; Gender, sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 64, 72, recurrent, {entity -&gt; Modifier, sentence -&gt; 0, chunk -&gt; 2}, []} | |{chunk, 98, 107, cellulitis, {entity -&gt; Diagnosis, sentence -&gt; 0, chunk -&gt; 3}, []} | |{chunk, 110, 119, pneumonias, {entity -&gt; Diagnosis, sentence -&gt; 0, chunk -&gt; 4}, []}| +--+ # The output of a MedicalNerModel follows the Annotator schema and looks like this after the transformation. result.selectExpr(&quot;explode(ner_result)&quot;).show(5, False) +--+ |col | +--+ |{named_entity, 3, 3, O, {word -&gt; A, confidence -&gt; 0.994}, []} | |{named_entity, 5, 15, B-Age, {word -&gt; 63-year-old, confidence -&gt; 1.0}, []}| |{named_entity, 17, 19, B-Gender, {word -&gt; man, confidence -&gt; 0.9858}, []} | |{named_entity, 21, 28, O, {word -&gt; presents, confidence -&gt; 0.9952}, []} | |{named_entity, 30, 31, O, {word -&gt; to, confidence -&gt; 0.7063}, []} | +--+ # After the converter is used: result.selectExpr(&quot;explode(ner_converter_result)&quot;).show(5, False) +--+ |col | +--+ |{chunk, 5, 15, 63-year-old, {entity -&gt; Age, sentence -&gt; 0, chunk -&gt; 0}, []} | |{chunk, 17, 19, man, {entity -&gt; Gender, sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 64, 72, recurrent, {entity -&gt; Modifier, sentence -&gt; 0, chunk -&gt; 2}, []} | |{chunk, 98, 107, cellulitis, {entity -&gt; Diagnosis, sentence -&gt; 0, chunk -&gt; 3}, []} | |{chunk, 110, 119, pneumonias, {entity -&gt; Diagnosis, sentence -&gt; 0, chunk -&gt; 4}, []}| +--+ NerDisambiguator API scaladocs Links words of interest, such as names of persons, locations and companies, from an input text document to a corresponding unique entity in a target Knowledge Base (KB). Words of interest are called Named Entities (NEs), mentions, or surface forms. The model needs extracted CHUNKS and SENTENCE_EMBEDDINGS type input from e.g. SentenceEmbeddings and NerConverter. Input types: CHUNK, SENTENCE_EMBEDDINGS Output type: DISAMBIGUATION Show Example PythonScala // Extracting Person identities // First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. val data = Seq(&quot;The show also had a contestant named Donald Trump who later defeated Christina Aguilera ...&quot;) .toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val word_embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val sentence_embeddings = new SentenceEmbeddings() .setInputCols(&quot;sentence&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val ner_model = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val ner_converter = new NerConverter() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList(&quot;PER&quot;) // Then the extracted entities can be disambiguated. val disambiguator = new NerDisambiguator() .setS3KnowledgeBaseName(&quot;i-per&quot;) .setInputCols(&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;) .setOutputCol(&quot;disambiguation&quot;) .setNumFirstChars(5) val nlpPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, word_embeddings, sentence_embeddings, ner_model, ner_converter, disambiguator)) val model = nlpPipeline.fit(data) val result = model.transform(data) // Show results result.selectExpr(&quot;explode(disambiguation)&quot;) .selectExpr(&quot;col.metadata.chunk as chunk&quot;, &quot;col.result as result&quot;).show(5, false) +++ |chunk |result | +++ |Donald Trump |http://en.wikipedia.org/?curid=4848272, http://en.wikipedia.org/?curid=31698421, http://en.wikipedia.org/?curid=55907961| |Christina Aguilera|http://en.wikipedia.org/?curid=144171, http://en.wikipedia.org/?curid=6636454 | +++ # Extracting Person identities # First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. data = spark.createDataFrame([[&quot;The show also had a contestant named Donald Trump who later defeated Christina Aguilera ...&quot;]]) .toDF(&quot;text&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) sentence_embeddings = SentenceEmbeddings() .setInputCols([&quot;sentence&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) ner_model = NerDLModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&quot;PER&quot;]) # Then the extracted entities can be disambiguated. disambiguator = NerDisambiguator() .setS3KnowledgeBaseName(&quot;i-per&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;disambiguation&quot;) .setNumFirstChars(5) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, word_embeddings, sentence_embeddings, ner_model, ner_converter, disambiguator]) model = nlpPipeline.fit(data) result = model.transform(data) # Show results result.selectExpr(&quot;explode(disambiguation)&quot;) .selectExpr(&quot;col.metadata.chunk as chunk&quot;, &quot;col.result as result&quot;).show(5, False) +++ |chunk |result | +++ |Donald Trump |http://en.wikipedia.org/?curid=4848272, http://en.wikipedia.org/?curid=31698421, http://en.wikipedia.org/?curid=55907961| |Christina Aguilera|http://en.wikipedia.org/?curid=144171, http://en.wikipedia.org/?curid=6636454 | +++ ReIdentification Approach scaladocs Model scaladocs Reidentifies obfuscated entities by DeIdentification. This annotator requires the outputs from the deidentification as input. Input columns need to be the deidentified document and the deidentification mappings set with DeIdentification.setMappingsColumn. To see how the entities are deidentified, please refer to the example of that annotator. Input types: DOCUMENT,CHUNK Output type: DOCUMENT Show Example PythonScala // Define the reidentification stage and transform the deidentified documents val reideintification = new ReIdentification() .setInputCols(&quot;dei&quot;, &quot;protectedEntities&quot;) .setOutputCol(&quot;reid&quot;) .transform(result) // Show results result.select(&quot;dei.result&quot;).show(truncate=false) +--+ |result | +--+ |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , &lt;AGE&gt; years-old , Record date : 2079-11-14.]| +--+ reideintification.selectExpr(&quot;explode(reid.result)&quot;).show(truncate=false) +--+ |col | +--+ |# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09.| +--+ # Define the reidentification stage and transform the deidentified documents reideintification = ReIdentification() .setInputCols([&quot;dei&quot;, &quot;protectedEntities&quot;]) .setOutputCol(&quot;reid&quot;) .transform(result) # Show results result.select(&quot;dei.result&quot;).show(truncate=False) +--+ |result | +--+ |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , &lt;AGE&gt; years-old , Record date : 2079-11-14.]| +--+ reideintification.selectExpr(&quot;explode(reid.result)&quot;).show(truncate=False) +--+ |col | +--+ |# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09.| +--+ RelationExtraction Approach scaladocs Model scaladocs Extracts and classifies instances of relations between named entities. Input types: WORD_EMBEDDINGS, POS, CHUNK, DEPENDENCY Output type: CATEGORY Show Example PythonScala // Defining pipeline stages to extract entities first val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;tokens&quot;) val embedder = WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;embeddings&quot;) val posTagger = PerceptronModel .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;posTags&quot;) val nerTagger = MedicalNerModel .pretrained(&quot;ner_events_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner_tags&quot;) val nerConverter = new NerConverter() .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;)) .setOutputCol(&quot;nerChunks&quot;) val depencyParser = DependencyParserModel .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(Array(&quot;document&quot;, &quot;posTags&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;dependencies&quot;) // Then define `RelationExtractionApproach` and training parameters val re = new RelationExtractionApproach() .setInputCols(Array(&quot;embeddings&quot;, &quot;posTags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;)) .setOutputCol(&quot;relations_t&quot;) .setLabelColumn(&quot;target_rel&quot;) .setEpochsNumber(300) .setBatchSize(200) .setlearningRate(0.001f) .setModelFile(&quot;path/to/graph_file.pb&quot;) .setFixImbalance(true) .setValidationSplit(0.05f) .setFromEntity(&quot;from_begin&quot;, &quot;from_end&quot;, &quot;from_label&quot;) .setToEntity(&quot;to_begin&quot;, &quot;to_end&quot;, &quot;to_label&quot;) val finisher = new Finisher() .setInputCols(Array(&quot;relations_t&quot;)) .setOutputCols(Array(&quot;relations&quot;)) .setCleanAnnotations(false) .setValueSplitSymbol(&quot;,&quot;) .setAnnotationSplitSymbol(&quot;,&quot;) .setOutputAsArray(false) // Define complete pipeline and start training val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embedder, posTagger, nerTagger, nerConverter, depencyParser, re, finisher)) val model = pipeline.fit(trainData) # Defining pipeline stages to extract entities first documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;tokens&quot;) embedder = WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) posTagger = PerceptronModel .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;posTags&quot;) nerTagger = MedicalNerModel .pretrained(&quot;ner_events_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) nerConverter = NerConverter() .setInputCols([&quot;document&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;]) .setOutputCol(&quot;nerChunks&quot;) depencyParser = DependencyParserModel .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;document&quot;, &quot;posTags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) # Then define `RelationExtractionApproach` and training parameters re = RelationExtractionApproach() .setInputCols([&quot;embeddings&quot;, &quot;posTags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations_t&quot;) .setLabelColumn(&quot;target_rel&quot;) .setEpochsNumber(300) .setBatchSize(200) .setLearningRate(0.001) .setModelFile(&quot;path/to/graph_file.pb&quot;) .setFixImbalance(True) .setValidationSplit(0.05) .setFromEntity(&quot;from_begin&quot;, &quot;from_end&quot;, &quot;from_label&quot;) .setToEntity(&quot;to_begin&quot;, &quot;to_end&quot;, &quot;to_label&quot;) finisher = Finisher() .setInputCols([&quot;relations_t&quot;]) .setOutputCols([&quot;relations&quot;]) .setCleanAnnotations(False) .setValueSplitSymbol(&quot;,&quot;) .setAnnotationSplitSymbol(&quot;,&quot;) .setOutputAsArray(False) # Define complete pipeline and start training pipeline = Pipeline(stages=[ documentAssembler, tokenizer, embedder, posTagger, nerTagger, nerConverter, depencyParser, re, finisher]) model = pipeline.fit(trainData) RelationExtractionDL Model scaladocs Extracts and classifies instances of relations between named entities. In contrast with RelationExtractionModel, RelationExtractionDLModel is based on BERT. For pretrained models please see the Models Hub for available models. Input types: CHUNK, DOCUMENT Output type: CATEGORY Show Example PythonScala // Relation Extraction between body parts // This is a continuation of the RENerChunksFilter example. See that class on how to extract the relation chunks. // Define the extraction model val re_ner_chunk_filter = new RENerChunksFilter() .setInputCols(&quot;ner_chunks&quot;, &quot;dependencies&quot;) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs(Array(&quot;internal_organ_or_component-direction&quot;)) val re_model = RelationExtractionDLModel.pretrained(&quot;redl_bodypart_direction_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setPredictionThreshold(0.5f) .setInputCols(&quot;re_ner_chunks&quot;, &quot;sentences&quot;) .setOutputCol(&quot;relations&quot;) val trained_pipeline = new Pipeline().setStages(Array( documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_ner_chunk_filter, re_model )) val data = Seq(&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;).toDF(&quot;text&quot;) val result = trained_pipeline.fit(data).transform(data) // Show results result.selectExpr(&quot;explode(relations) as relations&quot;) .select( &quot;relations.metadata.chunk1&quot;, &quot;relations.metadata.entity1&quot;, &quot;relations.metadata.chunk2&quot;, &quot;relations.metadata.entity2&quot;, &quot;relations.result&quot; ) .where(&quot;result != 0&quot;) .show(truncate=false) +++-+++ |chunk1|entity1 |chunk2 |entity2 |result| +++-+++ |upper |Direction|brain stem |Internal_organ_or_component|1 | |left |Direction|cerebellum |Internal_organ_or_component|1 | |right |Direction|basil ganglia|Internal_organ_or_component|1 | +++-+++ # Relation Extraction between body parts # This is a continuation of the RENerChunksFilter example. See that class on how to extract the relation chunks. # Define the extraction model re_ner_chunk_filter = RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs([&quot;internal_organ_or_component-direction&quot;]) re_model = RelationExtractionDLModel.pretrained(&quot;redl_bodypart_direction_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setPredictionThreshold(0.5) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) trained_pipeline = Pipeline(stages=[ documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_ner_chunk_filter, re_model ]) data = spark.createDataFrame([[&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;]]).toDF(&quot;text&quot;) result = trained_pipeline.fit(data).transform(data) # Show results result.selectExpr(&quot;explode(relations) as relations&quot;) .select( &quot;relations.metadata.chunk1&quot;, &quot;relations.metadata.entity1&quot;, &quot;relations.metadata.chunk2&quot;, &quot;relations.metadata.entity2&quot;, &quot;relations.result&quot; ) .where(&quot;result != 0&quot;) .show(truncate=False) +++-+++ |chunk1|entity1 |chunk2 |entity2 |result| +++-+++ |upper |Direction|brain stem |Internal_organ_or_component|1 | |left |Direction|cerebellum |Internal_organ_or_component|1 | |right |Direction|basil ganglia|Internal_organ_or_component|1 | +++-+++ RENerChunksFilter API scaladocs Filters and outputs combinations of relations between extracted entities, for further processing. This annotator is especially useful to create inputs for the RelationExtractionDLModel. Input types: CHUNK, DEPENDENCY Output type: CHUNK Show Example PythonScala // Define pipeline stages to extract entities val documenter = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentencer = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;tokens&quot;) val words_embedder = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;) .setOutputCol(&quot;embeddings&quot;) val pos_tagger = PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;) .setOutputCol(&quot;pos_tags&quot;) val dependency_parser = DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;) .setOutputCol(&quot;dependencies&quot;) val clinical_ner_tagger = MedicalNerModel.pretrained(&quot;jsl_ner_wip_greedy_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner_tags&quot;) val ner_chunker = new NerConverter() .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;) .setOutputCol(&quot;ner_chunks&quot;) // Define the relation pairs and the filter val relationPairs = Array(&quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot;) val re_ner_chunk_filter = new RENerChunksFilter() .setInputCols(&quot;ner_chunks&quot;, &quot;dependencies&quot;) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs(Array(&quot;internal_organ_or_component-direction&quot;)) val trained_pipeline = new Pipeline().setStages(Array( documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_ner_chunk_filter )) val data = Seq(&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;).toDF(&quot;text&quot;) val result = trained_pipeline.fit(data).transform(data) // Show results result.selectExpr(&quot;explode(re_ner_chunks) as re_chunks&quot;) .selectExpr(&quot;re_chunks.begin&quot;, &quot;re_chunks.result&quot;, &quot;re_chunks.metadata.entity&quot;, &quot;re_chunks.metadata.paired_to&quot;) .show(6, truncate=false) +--+-+++ |begin|result |entity |paired_to| +--+-+++ |35 |upper |Direction |41 | |41 |brain stem |Internal_organ_or_component|35 | |35 |upper |Direction |59 | |59 |cerebellum |Internal_organ_or_component|35 | |35 |upper |Direction |81 | |81 |basil ganglia|Internal_organ_or_component|35 | +--+-+++ # Define pipeline stages to extract entities documenter = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencer = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;tokens&quot;) words_embedder = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) pos_tagger = PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;pos_tags&quot;) dependency_parser = DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) clinical_ner_tagger = MedicalNerModel.pretrained(&quot;jsl_ner_wip_greedy_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) ner_chunker = NerConverter() .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;]) .setOutputCol(&quot;ner_chunks&quot;) # Define the relation pairs and the filter relationPairs = [&quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot;] re_ner_chunk_filter = RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs([&quot;internal_organ_or_component-direction&quot;]) trained_pipeline = Pipeline(stages=[ documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_ner_chunk_filter ]) data = spark.createDataFrame([[&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;]]).toDF(&quot;text&quot;) result = trained_pipeline.fit(data).transform(data) # Show results result.selectExpr(&quot;explode(re_ner_chunks) as re_chunks&quot;) .selectExpr(&quot;re_chunks.begin&quot;, &quot;re_chunks.result&quot;, &quot;re_chunks.metadata.entity&quot;, &quot;re_chunks.metadata.paired_to&quot;) .show(6, truncate=False) +--+-+++ |begin|result |entity |paired_to| +--+-+++ |35 |upper |Direction |41 | |41 |brain stem |Internal_organ_or_component|35 | |35 |upper |Direction |59 | |59 |cerebellum |Internal_organ_or_component|35 | |35 |upper |Direction |81 | |81 |basil ganglia|Internal_organ_or_component|35 | +--+-+++ SentenceEntityResolver Approach scaladocs Model scaladocs Assigns a standard code (ICD10 CM, PCS, ICDO; CPT) to sentence embeddings pooled over chunks from TextMatchers or the NER Models. This annotator is particularly handy when workING with BertSentenceEmbeddings from the upstream chunks. Input types: SENTENCE_EMBEDDINGS Output type: ENTITY Show Example PythonScala // Training a SNOMED resolution model using BERT sentence embeddings // Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. val documentAssembler = new DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) val bertEmbeddings = BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;bert_embeddings&quot;) val snomedTrainingPipeline = new Pipeline().setStages(Array( documentAssembler, bertEmbeddings )) val snomedTrainingModel = snomedTrainingPipeline.fit(data) val snomedData = snomedTrainingModel.transform(data).cache() // Then the Resolver can be trained with val bertExtractor = new SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols(&quot;bert_embeddings&quot;) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(false) val snomedModel = bertExtractor.fit(snomedData) # Training a SNOMED resolution model using BERT sentence embeddings # Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. documentAssembler = DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) bertEmbeddings = BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;bert_embeddings&quot;) snomedTrainingPipeline = Pipeline(stages=[ documentAssembler, bertEmbeddings ]) snomedTrainingModel = snomedTrainingPipeline.fit(data) snomedData = snomedTrainingModel.transform(data).cache() # Then the Resolver can be trained with bertExtractor = SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols([&quot;bert_embeddings&quot;]) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(False) snomedModel = bertExtractor.fit(snomedData)",
    "url": "/docs/en/licensed_annotators",
    "relUrl": "/docs/en/licensed_annotators"
  },
  "37": {
    "id": "37",
    "title": "Spark NLP for Healthcare",
    "content": "Getting started Spark NLP for Healthcare is a commercial extension of Spark NLP for clinical and biomedical text mining. If you don’t have a Spark NLP for Healthcare subscription yet, you can ask for a free trial by clicking on the button below. Try Free Spark NLP for Healthcare provides healthcare-specific annotators, pipelines, models, and embeddings for: Clinical entity recognition Clinical Entity Linking Entity normalization Assertion Status Detection De-identification Relation Extraction Spell checking &amp; correction note: If you are going to use any pretrained licensed NER model, you don’t need to install licensed libray. As long as you have the AWS keys and license keys in your environment, you will be able to use licensed NER models with Spark NLP public library. For the other licensed pretrained models like AssertionDL, Deidentification, Entity Resolvers and Relation Extraction models, you will need to install Spark NLP Enterprise as well. The library offers access to several clinical and biomedical transformers: JSL-BERT-Clinical, BioBERT, ClinicalBERT, GloVe-Med, GloVe-ICD-O. It also includes over 50 pre-trained healthcare models, that can recognize the following entities (any many more): Clinical - support Signs, Symptoms, Treatments, Procedures, Tests, Labs, Sections Drugs - support Name, Dosage, Strength, Route, Duration, Frequency Risk Factors- support Smoking, Obesity, Diabetes, Hypertension, Substance Abuse Anatomy - support Organ, Subdivision, Cell, Structure Organism, Tissue, Gene, Chemical Demographics - support Age, Gender, Height, Weight, Race, Ethnicity, Marital Status, Vital Signs Sensitive Data- support Patient Name, Address, Phone, Email, Dates, Providers, Identifiers Install Spark NLP for Healthcare You can install the Spark NLP for Healthcare package by using: pip install -q spark-nlp-jsl==${version} --extra-index-url https://pypi.johnsnowlabs.com/${secret.code} --upgrade {version} is the version part of the {secret.code} ({secret.code}.split(&#39;-&#39;)[0]) (i.e. 2.6.0) The {secret.code} is a secret code that is only available to users with valid/trial license. If you did not receive it yet, please contact us at info@johnsnowlabs.com. Setup AWS-CLI Credentials for licensed pretrained models Starting from Spark NLP for Healthcare version 2.4.2, you need to first setup your AWS credentials to be able to access the private repository for John Snow Labs Pretrained Models. You can do this setup via Amazon AWS Command Line Interface (AWSCLI). Instructions about how to install AWSCLI are available at: Installing the AWS CLI Make sure you configure your credentials with aws configure following the instructions at: Configuring the AWS CLI Please substitute the ACCESS_KEY and SECRET_KEY with the credentials you have received from your Customer Owner (CO). If you need your credentials contact us at info@johnsnowlabs.com. Start Spark NLP for Healthcare Session from Python The following will initialize the spark session in case you have run the jupyter notebook directly. If you have started the notebook using pyspark this cell is just ignored. Initializing the spark session takes some seconds (usually less than 1 minute) as the jar from the server needs to be loaded. The {secret-code} is a secret string you should have received from your Customer Owner (CO). If you have not received them, please contact us at info@johnsnowlabs.com. You can either use our convenience function to start your Spark Session that will use standard configuration arguments: import sparknlp_jsl spark = sparknlp_jsl.start(&quot;{secret.code}&quot;) Or use the SparkSession module for more flexibility: from pyspark.sql import SparkSession spark = SparkSession.builder .appName(&quot;Spark NLP Enterprise&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;,&quot;16&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;1000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.6&quot;) .config(&quot;spark.jars&quot;, &quot;https://pypi.johnsnowlabs.com/${secret.code}/spark-nlp-jsl-${version}.jar&quot;) .getOrCreate() If you want to download the source files (jar and whl files) locally, you can follow the instructions here. Install Spark NLP for Healthcare on Databricks Create a cluster if you don’t have one already On a new cluster or existing one you need to add the following to the Advanced Options -&gt; Spark tab, in Spark.Config box: spark.kryoserializer.buffer.max 1000M spark.serializer org.apache.spark.serializer.KryoSerializer Please add the following to the Advanced Options -&gt; Spark tab, in Environment Variables box: AWS_ACCESS_KEY_ID=xxx AWS_SECRET_ACCESS_KEY=yyy SPARK_NLP_LICENSE=zzz (OPTIONAL) If the environment variables used to setup the AWS Access/Secret keys are conflicting with the credential provider chain in Databricks, you may not be able to access to other s3 buckets. To access both JSL repos with JSL AWS keys as well as your own s3 bucket with your own AWS keys), you need to use the following script, copy that to dbfs folder, then go to the Databricks console (init scripts menu) to add the init script for your cluster as follows: %scala val script = &quot;&quot;&quot; #!/bin/bash echo &quot;******** Inject Spark NLP AWS Profile Credentials ******** &quot; mkdir ~/.aws/ cat &lt;&lt; EOF &gt; ~/.aws/credentials [spark_nlp] aws_access_key_id=&lt;YOUR_AWS_ACCESS_KEY&gt; aws_secret_access_key=&lt;YOUR_AWS_SECRET_KEY&gt; EOF echo &quot;******** End Inject Spark NLP AWS Profile Credentials ******** &quot; &quot;&quot;&quot; In Libraries tab inside your cluster you need to follow these steps: Install New -&gt; PyPI -&gt; spark-nlp -&gt; Install Install New -&gt; Maven -&gt; Coordinates -&gt; com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.5 -&gt; Install Please add following jars: Install New -&gt; Python Whl -&gt; upload https://pypi.johnsnowlabs.com/${secret.code}/spark-nlp-jsl/spark_nlp_jsl-${version}-py3-none-any.whl Install New -&gt; Jar -&gt; upload https://pypi.johnsnowlabs.com/${secret.code}/spark-nlp-jsl-${version}.jar Now you can attach your notebook to the cluster and use Spark NLP!",
    "url": "/docs/en/licensed_install",
    "relUrl": "/docs/en/licensed_install"
  },
  "38": {
    "id": "38",
    "title": "Licensed Models",
    "content": "Pretrained Models We are currently in the process of moving the pretrained models and pipelines to a Model Hub that you can explore here: Models Hub",
    "url": "/docs/en/licensed_models",
    "relUrl": "/docs/en/licensed_models"
  },
  "39": {
    "id": "39",
    "title": "Spark NLP for Healthcare Release Notes",
    "content": "Release Notes Spark NLP Healthcare 3.1.0 We are glad to announce that Spark NLP for Healthcare 3.1.0 has been released! Highlights Improved load time &amp; memory consumption for SentenceResolver models. New JSL Bert Models. JSL SBert Model Speed Benchmark. New ICD10CM resolver models. New Deidentification NER models. New column returned in DeidentificationModel New Reidentification feature New Deidentification Pretrained Pipelines Chunk filtering based on confidence Extended regex dictionary fuctionallity in Deidentification Enhanced RelationExtractionDL Model to create and identify relations between entities across the entire document MedicalNerApproach can now accept a graph file directly. MedicalNerApproach can now accept a user-defined name for log file. More improvements in Scaladocs. Bug fixes in Deidentification module. New notebooks. Sentence Resolver Models load time improvement Sentence resolver models now have faster load times, with a speedup of about 6X when compared to previous versions. Also, the load process now is more memory friendly meaning that the maximum memory required during load time is smaller, reducing the chances of OOM exceptions, and thus relaxing hardware requirements. New JSL SBert Models We trained new sBert models in TF2 and fined tuned on MedNLI, NLI and UMLS datasets with various parameters to cover common NLP tasks in medical domain. You can find the details in the following table. sbiobert_jsl_cased sbiobert_jsl_umls_cased sbert_jsl_medium_uncased sbert_jsl_medium_umls_uncased sbert_jsl_mini_uncased sbert_jsl_mini_umls_uncased sbert_jsl_tiny_uncased sbert_jsl_tiny_umls_uncased JSL SBert Model Speed Benchmark | JSL SBert Model| Base Model | Is Cased | Train Datasets | Inference speed (100 rows) | |-|-|-|-|-| | sbiobert_jsl_cased | biobert_v1.1_pubmed | Cased | medNLI, allNLI| 274,53 | | sbiobert_jsl_umls_cased | biobert_v1.1_pubmed | Cased | medNLI, allNLI, umls | 274,52 | | sbert_jsl_medium_uncased | uncased_L-8_H-512_A-8 | Uncased | medNLI, allNLI| 80,40 | | sbert_jsl_medium_umls_uncased | uncased_L-8_H-512_A-8 | Uncased | medNLI, allNLI, umls | 78,35 | | sbert_jsl_mini_uncased | uncased_L-4_H-256_A-4 | Uncased | medNLI, allNLI| 10,68 | | sbert_jsl_mini_umls_uncased | uncased_L-4_H-256_A-4 | Uncased | medNLI, allNLI, umls | 10,29 | | sbert_jsl_tiny_uncased | uncased_L-2_H-128_A-2 | Uncased | medNLI, allNLI| 4,54 | | sbert_jsl_tiny_umls_uncased | uncased_L-2_H-128_A-2 | Uncased | medNLI, allNL, umls | 4,54 | New ICD10CM resolver models: These models map clinical entities and concepts to ICD10 CM codes using sentence bert embeddings. They also return the official resolution text within the brackets inside the metadata. Both models are augmented with synonyms, and previous augmentations are flexed according to cosine distances to unnormalized terms (ground truths). sbiobertresolve_icd10cm_slim_billable_hcc: Trained with classic sbiobert mli. (sbiobert_base_cased_mli) Models Hub Page : https://nlp.johnsnowlabs.com/2021/05/25/sbiobertresolve_icd10cm_slim_billable_hcc_en.html sbertresolve_icd10cm_slim_billable_hcc_med: Trained with new jsl sbert(sbert_jsl_medium_uncased) Models Hub Page : https://nlp.johnsnowlabs.com/2021/05/25/sbertresolve_icd10cm_slim_billable_hcc_med_en.html Example: ‘bladder cancer’ sbiobertresolve_icd10cm_augmented_billable_hcc chunks code all_codes resolutions all_distances 100x Loop(sec) bladder cancer C679 [C679, Z126, D090, D494, C7911] [bladder cancer, suspected bladder cancer, cancer in situ of urinary bladder, tumor of bladder neck, malignant tumour of bladder neck] [0.0000, 0.0904, 0.0978, 0.1080, 0.1281] 26,9 ` sbiobertresolve_icd10cm_slim_billable_hcc` chunks code all_codes resolutions all_distances 100x Loop(sec) bladder cancer D090 [D090, D494, C7911, C680, C679] [cancer in situ of urinary bladder [Carcinoma in situ of bladder], tumor of bladder neck [Neoplasm of unspecified behavior of bladder], malignant tumour of bladder neck [Secondary malignant neoplasm of bladder], carcinoma of urethra [Malignant neoplasm of urethra], malignant tumor of urinary bladder [Malignant neoplasm of bladder, unspecified]] [0.0978, 0.1080, 0.1281, 0.1314, 0.1284] 20,9 sbertresolve_icd10cm_slim_billable_hcc_med chunks code all_codes resolutions all_distances 100x Loop(sec) bladder cancer C671 [C671, C679, C61, C672, C673] [bladder cancer, dome [Malignant neoplasm of dome of bladder], cancer of the urinary bladder [Malignant neoplasm of bladder, unspecified], prostate cancer [Malignant neoplasm of prostate], cancer of the urinary bladder] [0.0894, 0.1051, 0.1184, 0.1180, 0.1200] 12,8 New Deidentification NER Models We trained four new NER models to find PHI data (protected health information) that may need to be deidentified. ner_deid_generic_augmented and ner_deid_subentity_augmented models are trained with a combination of 2014 i2b2 Deid dataset and in-house annotations as well as some augmented version of them. Compared to the same test set coming from 2014 i2b2 Deid dataset, we achieved a better accuracy and generalisation on some entity labels as summarised in the following tables. We also trained the same models with glove_100d embeddings to get more memory friendly versions. ner_deid_generic_augmented : Detects PHI 7 entities (DATE, NAME, LOCATION, PROFESSION, CONTACT, AGE, ID). Models Hub Page : https://nlp.johnsnowlabs.com/2021/06/01/ner_deid_generic_augmented_en.html entity ner_deid_large (v3.0.3 and before) ner_deid_generic_augmented (v3.1.0) CONTACT 0.8695 0.9592 NAME 0.9452 0.9648 DATE 0.9778 0.9855 LOCATION 0.8755 0.923 ner_deid_subentity_augmented: Detects PHI 23 entities (MEDICALRECORD, ORGANIZATION, DOCTOR, USERNAME, PROFESSION, HEALTHPLAN, URL, CITY, DATE, LOCATION-OTHER, STATE, PATIENT, DEVICE, COUNTRY, ZIP, PHONE, HOSPITAL, EMAIL, IDNUM, SREET, BIOID, FAX, AGE) Models Hub Page : https://nlp.johnsnowlabs.com/2021/06/01/ner_deid_subentity_augmented_en.html entity ner_deid_enriched (v3.0.3 and before) ner_deid_subentity_augmented (v3.1.0) HOSPITAL 0.8519 0.8983 DATE 0.9766 0.9854 CITY 0.7493 0.8075 STREET 0.8902 0.9772 ZIP 0.8 0.9504 PHONE 0.8615 0.9502 DOCTOR 0.9191 0.9347 AGE 0.9416 0.9469 ner_deid_generic_glove: Small version of ner_deid_generic_augmented and detects 7 entities. ner_deid_subentity_glove: Small version of ner_deid_subentity_augmented and detects 23 entities. Example: Scala ... val deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) ... val nlpPipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, word_embeddings, deid_ner, ner_converter)) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) val result = pipeline.fit(Seq.empty[&quot;A. Record date : 2093-01-13, David Hale, M.D., Name : Hendrickson, Ora MR. # 7194334 Date : 01/13/93 PCP : Oliveira, 25 -year-old, Record date : 1-11-2000. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227.&quot;].toDS.toDF(&quot;text&quot;)).transform(data) Python ... deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, word_embeddings, deid_ner, ner_converter]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame(pd.DataFrame({&quot;text&quot;: [&quot;&quot;&quot;A. Record date : 2093-01-13, David Hale, M.D., Name : Hendrickson, Ora MR. # 7194334 Date : 01/13/93 PCP : Oliveira, 25 -year-old, Record date : 1-11-2000. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227.&quot;&quot;&quot;]}))) Results: +--+-+ |chunk |ner_label | +--+-+ |2093-01-13 |DATE | |David Hale |DOCTOR | |Hendrickson, Ora |PATIENT | |7194334 |MEDICALRECORD| |01/13/93 |DATE | |Oliveira |DOCTOR | |25-year-old |AGE | |1-11-2000 |DATE | |Cocke County Baptist Hospital|HOSPITAL | |0295 Keats Street. |STREET | |(302) 786-5227 |PHONE | |Brothers Coal-Mine |ORGANIZATION | +--+-+ New column returned in DeidentificationModel DeidentificationModel now can return a new column to save the mappings between the mask/obfuscated entities and original entities. This column is optional and you can set it up with the .setReturnEntityMappings(True) method. The default value is False. Also, the name for the column can be changed using the following method; .setMappingsColumn(&quot;newAlternativeName&quot;) The new column will produce annotations with the following structure, Annotation( type: chunk, begin: 17, end: 25, result: 47, metadata:{ originalChunk -&gt; 01/13/93 //Original text of the chunk chunk -&gt; 0 // The number of the chunk in the sentence beginOriginalChunk -&gt; 95 // Start index of the original chunk endOriginalChunk -&gt; 102 // End index of the original chunk entity -&gt; AGE // Entity of the chunk sentence -&gt; 2 // Number of the sentence } ) New Reidentification feature With the new ReidetificationModel, the user can go back to the original sentences using the mappings columns and the deidentification sentences. Example: Scala val redeidentification = new ReIdentification() .setInputCols(Array(&quot;mappings&quot;, &quot;deid_chunks&quot;)) .setOutputCol(&quot;original&quot;) Python reDeidentification = ReIdentification() .setInputCols([&quot;mappings&quot;,&quot;deid_chunks&quot;]) .setOutputCol(&quot;original&quot;) New Deidentification Pretrained Pipelines We developed a clinical_deidentification pretrained pipeline that can be used to deidentify PHI information from medical texts. The PHI information will be masked and obfuscated in the resulting text. The pipeline can mask and obfuscate AGE, CONTACT, DATE, ID, LOCATION, NAME, PROFESSION, CITY, COUNTRY, DOCTOR, HOSPITAL, IDNUM, MEDICALRECORD, ORGANIZATION, PATIENT, PHONE, PROFESSION, STREET, USERNAME, ZIP, ACCOUNT, LICENSE, VIN, SSN, DLN, PLATE, IPADDR entities. Models Hub Page : https://nlp.johnsnowlabs.com/2021/05/27/clinical_deidentification_en.html There is also a lightweight version of the same pipeline trained with memory efficient glove_100dembeddings. Here are the model names: clinical_deidentification clinical_deidentification_glove Example: Python: from sparknlp.pretrained import PretrainedPipeline deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;, &quot;en&quot;, &quot;clinical/models&quot;) deid_pipeline.annotate(&quot;Record date : 2093-01-13, David Hale, M.D. IP: 203.120.223.13. The driver&#39;s license no:A334455B. the SSN:324598674 and e-mail: hale@gmail.com. Name : Hendrickson, Ora MR. # 719435 Date : 01/13/93. PCP : Oliveira, 25 years-old. Record date : 2079-11-09, Patient&#39;s VIN : 1HGBH41JXMN109286.&quot;) Scala: import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;,&quot;en&quot;,&quot;clinical/models&quot;) val result = deid_pipeline.annotate(&quot;Record date : 2093-01-13, David Hale, M.D. IP: 203.120.223.13. The driver&#39;s license no:A334455B. the SSN:324598674 and e-mail: hale@gmail.com. Name : Hendrickson, Ora MR. # 719435 Date : 01/13/93. PCP : Oliveira, 25 years-old. Record date : 2079-11-09, Patient&#39;s VIN : 1HGBH41JXMN109286.&quot;) Result: {&#39;sentence&#39;: [&#39;Record date : 2093-01-13, David Hale, M.D.&#39;, &#39;IP: 203.120.223.13.&#39;, &#39;The driver&#39;s license no:A334455B.&#39;, &#39;the SSN:324598674 and e-mail: hale@gmail.com.&#39;, &#39;Name : Hendrickson, Ora MR. # 719435 Date : 01/13/93.&#39;, &#39;PCP : Oliveira, 25 years-old.&#39;, &#39;Record date : 2079-11-09, Patient&#39;s VIN : 1HGBH41JXMN109286.&#39;], &#39;masked&#39;: [&#39;Record date : &lt;DATE&gt;, &lt;DOCTOR&gt;, M.D.&#39;, &#39;IP: &lt;IPADDR&gt;.&#39;, &#39;The driver&#39;s license &lt;DLN&gt;.&#39;, &#39;the &lt;SSN&gt; and e-mail: &lt;EMAIL&gt;.&#39;, &#39;Name : &lt;PATIENT&gt; MR. # &lt;MEDICALRECORD&gt; Date : &lt;DATE&gt;.&#39;, &#39;PCP : &lt;DOCTOR&gt;, &lt;AGE&gt; years-old.&#39;, &#39;Record date : &lt;DATE&gt;, Patient&#39;s VIN : &lt;VIN&gt;.&#39;], &#39;obfuscated&#39;: [&#39;Record date : 2093-01-18, Dr Alveria Eden, M.D.&#39;, &#39;IP: 001.001.001.001.&#39;, &#39;The driver&#39;s license K783518004444.&#39;, &#39;the SSN-400-50-8849 and e-mail: Merilynn@hotmail.com.&#39;, &#39;Name : Charls Danger MR. # J3366417 Date : 01-18-1974.&#39;, &#39;PCP : Dr Sina Sewer, 55 years-old.&#39;, &#39;Record date : 2079-11-23, Patient&#39;s VIN : 6ffff55gggg666777.&#39;], &#39;ner_chunk&#39;: [&#39;2093-01-13&#39;, &#39;David Hale&#39;, &#39;no:A334455B&#39;, &#39;SSN:324598674&#39;, &#39;Hendrickson, Ora&#39;, &#39;719435&#39;, &#39;01/13/93&#39;, &#39;Oliveira&#39;, &#39;25&#39;, &#39;2079-11-09&#39;, &#39;1HGBH41JXMN109286&#39;]} Chunk filtering based on confidence We added a new annotator ChunkFiltererApproach that allows to load a csv with both entities and confidence thresholds. This annotator will produce a ChunkFilterer model. You can load the dictionary with the following property setEntitiesConfidenceResource(). An example dictionary is: TREATMENT,0.7 With that dictionary, the user can filter the chunks corresponding to treatment entities which have confidence lower than 0.7. Example: We have a ner_chunk column and sentence column with the following data: Ner_chunk |[{chunk, 141, 163, the genomicorganization, {entity -&gt; TREATMENT, sentence -&gt; 0, chunk -&gt; 0, confidence -&gt; 0.57785}, []}, {chunk, 209, 267, a candidate gene forType II diabetes mellitus, {entity -&gt; PROBLEM, sentence -&gt; 0, chunk -&gt; 1, confidence -&gt; 0.6614286}, []}, {chunk, 394, 408, byapproximately, {entity -&gt; TREATMENT, sentence -&gt; 1, chunk -&gt; 2, confidence -&gt; 0.7705}, []}, {chunk, 478, 508, single nucleotide polymorphisms, {entity -&gt; TREATMENT, sentence -&gt; 2, chunk -&gt; 3, confidence -&gt; 0.7204666}, []}, {chunk, 559, 581, aVal366Ala substitution, {entity -&gt; TREATMENT, sentence -&gt; 2, chunk -&gt; 4, confidence -&gt; 0.61505}, []}, {chunk, 588, 601, an 8 base-pair, {entity -&gt; TREATMENT, sentence -&gt; 2, chunk -&gt; 5, confidence -&gt; 0.29226667}, []}, {chunk, 608, 625, insertion/deletion, {entity -&gt; PROBLEM, sentence -&gt; 3, chunk -&gt; 6, confidence -&gt; 0.9841}, []}]| +- Sentence [{document, 0, 298, The human KCNJ9 (Kir 3.3, GIRK3) is a member of the G-protein-activated inwardly rectifying potassium (GIRK) channel family.Here we describe the genomicorganization of the KCNJ9 locus on chromosome 1q21-23 as a candidate gene forType II diabetes mellitus in the Pima Indian population., {sentence -&gt; 0}, []}, {document, 300, 460, The gene spansapproximately 7.6 kb and contains one noncoding and two coding exons ,separated byapproximately 2.2 and approximately 2.6 kb introns, respectively., {sentence -&gt; 1}, []}, {document, 462, 601, We identified14 single nucleotide polymorphisms (SNPs), including one that predicts aVal366Ala substitution, and an 8 base-pair, {sentence -&gt; 2}, []}, {document, 603, 626, (bp) insertion/deletion., {sentence -&gt; 3}, []}] We can filter the entities using the following annotator: chunker_filter = ChunkFiltererApproach().setInputCols(&quot;sentence&quot;, &quot;ner_chunk&quot;) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;regex&quot;) .setRegex([&quot;.*&quot;]) .setEntitiesConfidenceResource(&quot;entities_confidence.csv&quot;) Where entities-confidence.csv has the following data: TREATMENT,0.7 PROBLEM,0.9 We can use that chunk_filter: chunker_filter.fit(data).transform(data) Producing the following entities: |[{chunk, 394, 408, byapproximately, {entity -&gt; TREATMENT, sentence -&gt; 1, chunk -&gt; 2, confidence -&gt; 0.7705}, []}, {chunk, 478, 508, single nucleotide polymorphisms, {entity -&gt; TREATMENT, sentence -&gt; 2, chunk -&gt; 3, confidence -&gt; 0.7204666}, []}, {chunk, 608, 625, insertion/deletion, {entity -&gt; PROBLEM, sentence -&gt; 3, chunk -&gt; 6, confidence -&gt; 0.9841}, []}]| As you can see, only the treatment entities with confidence score of more than 0.7, and the problem entities with confidence score of more than 0.9 have been kept in the output. Extended regex dictionary fuctionallity in Deidentification The RegexPatternsDictionary can now use a regex that spawns the 2 previous token and the 2 next tokens. That feature is implemented using regex groups. Examples: Given the sentence The patient with ssn 123123123 we can use the following regex to capture the entitty ssn ( d{9}) Given the sentence The patient has 12 years we can use the following regex to capture the entitty ( d{2}) years Enhanced RelationExtractionDL Model to create and identify relations between entities across the entire document A new option has been added to RENerChunksFilter to support pairing entities from different sentences using .setDocLevelRelations(True), to pass to the Relation Extraction Model. The RelationExtractionDL Model has also been updated to process document-level relations. How to use: re_dl_chunks = RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setDocLevelRelations(True) .setMaxSyntacticDistance(7) .setOutputCol(&quot;redl_ner_chunks&quot;) Examples: Given a document containing multiple sentences: John somkes cigrettes. He also consumes alcohol., now we can generate relation pairs across sentences and relate alcohol with John . Set NER graph explicitely in MedicalNerApproach Now MedicalNerApproach can receives the path to the graph directly. When a graph location is provided through this method, previous graph search behavior is disabled. MedicalNerApproach.setGraphFile(graphFilePath) MedicalNerApproach can now accept a user-defined name for log file. Now MedicalNerApproach can accept a user-defined name for the log file. If not such a name is provided, the conventional naming will take place. MedicalNerApproach.setLogPrefix(&quot;oncology_ner&quot;) This will result in oncology_ner_20210605_141701.log filename being used, in which the 20210605_141701 is a timestamp. New Notebooks A new notebook to reproduce our peer-reviewed NER paper (https://arxiv.org/abs/2011.06315) New databricks case study notebooks. In these notebooks, we showed the examples of how to work with oncology notes dataset and OCR on databricks for both DBr and community edition versions. 3.0.3 We are glad to announce that Spark NLP for Healthcare 3.0.3 has been released! Highlights Five new entity resolution models to cover UMLS, HPO and LIONC terminologies. New feature for random displacement of dates on deidentification model. Five new pretrained pipelines to map terminologies across each other (from UMLS to ICD10, from RxNorm to MeSH etc.) AnnotationToolReader support for Spark 2.3. The tool that helps model training on Spark-NLP to leverage data annotated using JSL Annotation Tool now has support for Spark 2.3. Updated documentation (Scaladocs) covering more APIs, and examples. Five new resolver models: sbiobertresolve_umls_major_concepts: This model returns CUI (concept unique identifier) codes for Clinical Findings, Medical Devices, Anatomical Structures and Injuries &amp; Poisoning terms. sbiobertresolve_umls_findings: This model returns CUI (concept unique identifier) codes for 200K concepts from clinical findings. sbiobertresolve_loinc: Map clinical NER entities to LOINC codes using sbiobert. sbluebertresolve_loinc: Map clinical NER entities to LOINC codes using sbluebert. sbiobertresolve_HPO: This model returns Human Phenotype Ontology (HPO) codes for phenotypic abnormalities encountered in human diseases. It also returns associated codes from the following vocabularies for each HPO code: * MeSH (Medical Subject Headings) * SNOMED * UMLS (Unified Medical Language System ) * ORPHA (international reference resource for information on rare diseases and orphan drugs) * OMIM (Online Mendelian Inheritance in Man) Related Notebook: Resolver Models New feature on Deidentification Module isRandomDateDisplacement(True): Be able to apply a random displacement on obfuscation dates. The randomness is based on the seed. Fix random dates when the format is not correct. Now you can repeat an execution using a seed for dates. Random dates will be based on the seed. Five new healthcare code mapping pipelines: icd10cm_umls_mapping: This pretrained pipeline maps ICD10CM codes to UMLS codes without using any text data. You’ll just feed white space-delimited ICD10CM codes and it will return the corresponding UMLS codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;icd10cm&#39;: [&#39;M89.50&#39;, &#39;R82.2&#39;, &#39;R09.01&#39;], &#39;umls&#39;: [&#39;C4721411&#39;, &#39;C0159076&#39;, &#39;C0004044&#39;]} mesh_umls_mapping: This pretrained pipeline maps MeSH codes to UMLS codes without using any text data. You’ll just feed white space-delimited MeSH codes and it will return the corresponding UMLS codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;mesh&#39;: [&#39;C028491&#39;, &#39;D019326&#39;, &#39;C579867&#39;], &#39;umls&#39;: [&#39;C0970275&#39;, &#39;C0886627&#39;, &#39;C3696376&#39;]} rxnorm_umls_mapping: This pretrained pipeline maps RxNorm codes to UMLS codes without using any text data. You’ll just feed white space-delimited RxNorm codes and it will return the corresponding UMLS codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;rxnorm&#39;: [&#39;1161611&#39;, &#39;315677&#39;, &#39;343663&#39;], &#39;umls&#39;: [&#39;C3215948&#39;, &#39;C0984912&#39;, &#39;C1146501&#39;]} rxnorm_mesh_mapping: This pretrained pipeline maps RxNorm codes to MeSH codes without using any text data. You’ll just feed white space-delimited RxNorm codes and it will return the corresponding MeSH codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;rxnorm&#39;: [&#39;1191&#39;, &#39;6809&#39;, &#39;47613&#39;], &#39;mesh&#39;: [&#39;D001241&#39;, &#39;D008687&#39;, &#39;D019355&#39;]} snomed_umls_mapping: This pretrained pipeline maps SNOMED codes to UMLS codes without using any text data. You’ll just feed white space-delimited SNOMED codes and it will return the corresponding UMLS codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;snomed&#39;: [&#39;733187009&#39;, &#39;449433008&#39;, &#39;51264003&#39;], &#39;umls&#39;: [&#39;C4546029&#39;, &#39;C3164619&#39;, &#39;C0271267&#39;]} Related Notebook: Healthcare Code Mapping 3.0.2 We are very excited to announce that Spark NLP for Healthcare 3.0.2 has been released! This release includes bug fixes and some compatibility improvements. Highlights Dictionaries for Obfuscator were augmented with more than 10K names. Improved support for spark 2.3 and spark 2.4. Bug fixes in DrugNormalizer. New Features Provide confidence scores for all available tags in MedicalNerModel, MedicalNerModel before 3.0.2 [[named_entity, 0, 9, B-PROBLEM, [word -&gt; Pneumonia, confidence -&gt; 0.9998], []] Now in Spark NLP for Healthcare 3.0.2 [[named_entity, 0, 9, B-PROBLEM, [B-PROBLEM -&gt; 0.9998, I-TREATMENT -&gt; 0.0, I-PROBLEM -&gt; 0.0, I-TEST -&gt; 0.0, B-TREATMENT -&gt; 1.0E-4, word -&gt; Pneumonia, B-TEST -&gt; 0.0], []] 3.0.1 We are very excited to announce that Spark NLP for Healthcare 3.0.1 has been released! Highlights: Fixed problem in Assertion Status internal tokenization (reported in Spark-NLP #2470). Fixes in the internal implementation of DeIdentificationModel/Obfuscator. Being able to disable the use of regexes in the Deidentification process Other minor bug fixes &amp; general improvements. DeIdentificationModel Annotator New seed parameter. Now we have the possibility of using a seed to guide the process of obfuscating entities and returning the same result across different executions. To make that possible a new method setSeed(seed:Int) was introduced. Example: Return obfuscated documents in a repeatable manner based on the same seed. Scala deIdentification = DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(true) Python de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(True) This seed controls how the obfuscated values are picked from a set of obfuscation candidates. Fixing the seed allows the process to be replicated. Example: Given the following input to the deidentification: &quot;David Hale was in Cocke County Baptist Hospital. David Hale&quot; If the annotator is set up with a seed of 10: Scala val deIdentification = new DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(true) Python de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(True) The result will be the following for any execution, &quot;Brendan Kitten was in New Megan.Brendan Kitten&quot; Now if we set up a seed of 32, Scala val deIdentification = new DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(32) .setIgnoreRegex(true) Python de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(True) The result will be the following for any execution, &quot;Louise Pear was in Lake Edward.Louise Pear&quot; New ignoreRegex parameter. You can now choose to completely disable the use of regexes in the deidentification process by setting the setIgnoreRegex param to True. Example: Scala DeIdentificationModel.setIgnoreRegex(true) Python DeIdentificationModel().setIgnoreRegex(True) The default value for this param is False meaning that regexes will be used by default. New supported entities for Deidentification &amp; Obfuscation: We added new entities to the default supported regexes: SSN - Social security number. PASSPORT - Passport id. DLN - Department of Labor Number. NPI - National Provider Identifier. C_CARD - The id number for credits card. IBAN - International Bank Account Number. DEA - DEA Registration Number, which is an identifier assigned to a health care provider by the United States Drug Enforcement Administration. We also introduced new Obfuscator cases for these new entities. 3.0.0 We are very excited to announce that Spark NLP for Healthcare 3.0.0 has been released! This has been one of the biggest releases we have ever done and we are so proud to share this with our customers. Highlights: Spark NLP for Healthcare 3.0.0 extends the support for Apache Spark 3.0.x and 3.1.x major releases on Scala 2.12 with both Hadoop 2.7. and 3.2. We now support all 4 major Apache Spark and PySpark releases of 2.3.x, 2.4.x, 3.0.x, and 3.1.x helping the customers to migrate from earlier Apache Spark versions to newer releases without being worried about Spark NLP support. Highlights: Support for Apache Spark and PySpark 3.0.x on Scala 2.12 Support for Apache Spark and PySpark 3.1.x on Scala 2.12 Migrate to TensorFlow v2.3.1 with native support for Java to take advantage of many optimizations for CPU/GPU and new features/models introduced in TF v2.x A brand new MedicalNerModel annotator to train &amp; load the licensed clinical NER models. Two times faster NER and Entity Resolution due to new batch annotation technique. Welcoming 9x new Databricks runtimes to our Spark NLP family: Databricks 7.3 Databricks 7.3 ML GPU Databricks 7.4 Databricks 7.4 ML GPU Databricks 7.5 Databricks 7.5 ML GPU Databricks 7.6 Databricks 7.6 ML GPU Databricks 8.0 Databricks 8.0 ML (there is no GPU in 8.0) Databricks 8.1 Beta Welcoming 2x new EMR 6.x series to our Spark NLP family: EMR 6.1.0 (Apache Spark 3.0.0 / Hadoop 3.2.1) EMR 6.2.0 (Apache Spark 3.0.1 / Hadoop 3.2.1) Starting Spark NLP for Healthcare 3.0.0 the default packages for CPU and GPU will be based on Apache Spark 3.x and Scala 2.12. Deprecated Text2SQL annotator is deprecated and will not be maintained going forward. We are working on a better and faster version of Text2SQL at the moment and will announce soon. 1. MedicalNerModel Annotator Starting Spark NLP for Healthcare 3.0.0, the licensed clinical and biomedical pretrained NER models will only work with this brand new annotator called MedicalNerModel and will not work with NerDLModel in open source version. In order to make this happen, we retrained all the clinical NER models (more than 80) and uploaded to models hub. Example: clinical_ner = MedicalNerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) 2. Speed Improvements A new batch annotation technique implemented in Spark NLP 3.0.0 for NerDLModel,BertEmbeddings, and BertSentenceEmbeddings annotators will be reflected in MedicalNerModel and it improves prediction/inferencing performance radically. From now on the batchSize for these annotators means the number of rows that can be fed into the models for prediction instead of sentences per row. You can control the throughput when you are on accelerated hardware such as GPU to fully utilise it. Here are the overall speed comparison: Now, NER inference and Entity Resolution are two times faster on CPU and three times faster on GPU. 3. JSL Clinical NER Model We are releasing the richest clinical NER model ever, spanning over 80 entities. It has been under development for the last 6 months and we manually annotated more than 4000 clinical notes to cover such a high number of entities in a single model. It has 4 variants at the moment: jsl_ner_wip_clinical jsl_ner_wip_greedy_clinical jsl_ner_wip_modifier_clinical jsl_rd_ner_wip_greedy_clinical Entities: Kidney_Disease, HDL, Diet, Test, Imaging_Technique, Triglycerides, Obesity, Duration, Weight, Social_History_Header, ImagingTest, Labour_Delivery, Disease_Syndrome_Disorder, Communicable_Disease, Overweight, Units, Smoking, Score, Substance_Quantity, Form, Race_Ethnicity, Modifier, Hyperlipidemia, ImagingFindings, Psychological_Condition, OtherFindings, Cerebrovascular_Disease, Date, Test_Result, VS_Finding, Employment, Death_Entity, Gender, Oncological, Heart_Disease, Medical_Device, Total_Cholesterol, ManualFix, Time, Route, Pulse, Admission_Discharge, RelativeDate, O2_Saturation, Frequency, RelativeTime, Hypertension, Alcohol, Allergen, Fetus_NewBorn, Birth_Entity, Age, Respiration, Medical_History_Header, Oxygen_Therapy, Section_Header, LDL, Treatment, Vital_Signs_Header, Direction, BMI, Pregnancy, Sexually_Active_or_Sexual_Orientation, Symptom, Clinical_Dept, Measurements, Height, Family_History_Header, Substance, Strength, Injury_or_Poisoning, Relationship_Status, Blood_Pressure, Drug, Temperature, EKG_Findings, Diabetes, BodyPart, Vaccine, Procedure, Dosage 4. JSL Clinical Assertion Model We are releasing a brand new clinical assertion model, supporting 8 assertion statuses. jsl_assertion_wip Assertion Labels : Present, Absent, Possible, Planned, Someoneelse, Past, Family, Hypotetical 5. Library Version Compatibility Table : Spark NLP for Healthcare 3.0.0 is compatible with Spark NLP 3.0.1 6. Pretrained Models Version Control (Beta): Due to active release cycle, we are adding &amp; training new pretrained models at each release and it might be tricky to maintain the backward compatibility or keep up with the latest models, especially for the users using our models locally in air-gapped networks. We are releasing a new utility class to help you check your local &amp; existing models with the latest version of everything we have up to date. You will not need to specify your AWS credentials from now on. This is the second version of the model checker we released with 2.7.6 and will replace that soon. from sparknlp_jsl.compatibility_beta import CompatibilityBeta compatibility = CompatibilityBeta(spark) print(compatibility.findVersion(&quot;ner_deid&quot;)) 7. Updated Pretrained Models: (requires fresh .pretraned()) None 2.7.6 We are glad to announce that Spark NLP for Healthcare 2.7.6 has been released! Highlights: New pretrained Radiology Assertion Status model to assign Confirmed, Suspected, Negative assertion scopes to imaging findings or any clinical tests. Obfuscating the same sensitive information (patient or doctor name) with the same fake names across the same clinical note. Version compatibility checker for the pretrained clinical models and builds to keep up with the latest development efforts in production. Adding more English names to faker module in Deidentification. Updated &amp; improved clinical SentenceDetectorDL model. New upgrades on ner_deid_large and ner_deid_enriched NER models to cover more use cases with better resolutions. Adding more examples to workshop repo for Scala users to practice more on healthcare annotators. Bug fixes &amp; general improvements. 1. Radiology Assertion Status Model We trained a new assertion model to assign Confirmed, Suspected, Negative assertion scopes to imaging findings or any clinical tests. It will try to assign these statuses to any named entity you would feed to the assertion annotater in the same pipeline. radiology_assertion = AssertionDLModel.pretrained(&quot;assertion_dl_radiology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) text = Blunting of the left costophrenic angle on the lateral view posteriorly suggests a small left pleural effusion. No right-sided pleural effusion or pneumothorax is definitively seen. There are mildly displaced fractures of the left lateral 8th and likely 9th ribs. sentences chunk ner_label sent_id assertion Blunting of the left costophrenic angle on the lateral view posteriorly suggests a small left pleural effusion. Blunting ImagingFindings 0 Confirmed Blunting of the left costophrenic angle on the lateral view posteriorly suggests a small left pleural effusion. effusion ImagingFindings 0 Suspected No right-sided pleural effusion or pneumothorax is definitively seen. effusion ImagingFindings 1 Negative No right-sided pleural effusion or pneumothorax is definitively seen. pneumothorax ImagingFindings 1 Negative There are mildly displaced fractures of the left lateral 8th and likely 9th ribs. displaced fractures ImagingFindings 2 Confirmed You can also use this with AssertionFilterer to return clinical findings from a note only when it is i.e. confirmed or suspected. assertion_filterer = AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;assertion_filtered&quot;) .setWhiteList([&quot;confirmed&quot;,&quot;suspected&quot;]) &gt;&gt; [&quot;displaced fractures&quot;, &quot;effusion&quot;] 2. Obfuscating with the same fake name across the same note: obfuscation = DeIdentification() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_chunk&quot;]) .setOutputCol(&quot;deidentified&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateDate(True) .setSameEntityThreshold(0.8) .setObfuscateRefSource(&quot;faker&quot;) text =&#39;&#39;&#39; Provider: David Hale, M.D. Pt: Jessica Parker David told Jessica that she will need to visit the clinic next month.&#39;&#39;&#39;   sentence obfuscated 0 Provider: David Hale, M.D. Provider: Dennis Perez, M.D. 1 Pt: Jessica Parker Pt: Gerth Bayer 2 David told Jessica that she will need to visit the clinic next month. Dennis told Gerth that she will need to visit the clinic next month. 3. Library Version Compatibility Table : We are releasing the version compatibility table to help users get to see which Spark NLP licensed version is built against which core (open source) version. We are going to release a detailed one after running some tests across the jars from each library. Healthcare Public 2.7.6 2.7.4 2.7.5 2.7.4 2.7.4 2.7.3 2.7.3 2.7.3 2.7.2 2.6.5 2.7.1 2.6.4 2.7.0 2.6.3 2.6.2 2.6.2 2.6.0 2.6.0 2.5.5 2.5.5 2.5.3 2.5.3 2.5.2 2.5.2 2.5.0 2.5.0 2.4.7 2.4.5 2.4.6 2.4.5 2.4.5 2.4.5 2.4.2 2.4.2 2.4.1 2.4.1 2.4.0 2.4.0 2.3.6 2.3.6 2.3.5 2.3.5 2.3.4 2.3.4 4. Pretrained Models Version Control : Due to active release cycle, we are adding &amp; training new pretrained models at each release and it might be tricky to maintain the backward compatibility or keep up with the latest models, especially for the users using our models locally in air-gapped networks. We are releasing a new utility class to help you check your local &amp; existing models with the latest version of everything we have up to date. This is an highly experimental feature of which we plan to improve and add more capability later on. from sparknlp_jsl.check_compatibility import Compatibility checker = sparknlp_jsl.Compatibility() result = checker.find_version(aws_access_key_id=license_keys[&#39;AWS_ACCESS_KEY_ID&#39;], aws_secret_access_key=license_keys[&#39;AWS_SECRET_ACCESS_KEY&#39;], metadata_path=None, model = &#39;all&#39; , # or a specific model name target_version=&#39;all&#39;, cache_pretrained_path=&#39;/home/ubuntu/cache_pretrained&#39;) &gt;&gt; result[&#39;outdated_models&#39;] [{&#39;model_name&#39;: &#39;clinical_ner_assertion&#39;, &#39;current_version&#39;: &#39;2.4.0&#39;, &#39;latest_version&#39;: &#39;2.6.4&#39;}, {&#39;model_name&#39;: &#39;jsl_rd_ner_wip_greedy_clinical&#39;, &#39;current_version&#39;: &#39;2.6.1&#39;, &#39;latest_version&#39;: &#39;2.6.2&#39;}, {&#39;model_name&#39;: &#39;ner_anatomy&#39;, &#39;current_version&#39;: &#39;2.4.2&#39;, &#39;latest_version&#39;: &#39;2.6.4&#39;}, {&#39;model_name&#39;: &#39;ner_aspect_based_sentiment&#39;, &#39;current_version&#39;: &#39;2.6.2&#39;, &#39;latest_version&#39;: &#39;2.7.2&#39;}, {&#39;model_name&#39;: &#39;ner_bionlp&#39;, &#39;current_version&#39;: &#39;2.4.0&#39;, &#39;latest_version&#39;: &#39;2.7.0&#39;}, {&#39;model_name&#39;: &#39;ner_cellular&#39;, &#39;current_version&#39;: &#39;2.4.2&#39;, &#39;latest_version&#39;: &#39;2.5.0&#39;}] &gt;&gt; result[&#39;version_comparison_dict&#39;] [{&#39;clinical_ner_assertion&#39;: {&#39;current_version&#39;: &#39;2.4.0&#39;, &#39;latest_version&#39;: &#39;2.6.4&#39;}}, {&#39;jsl_ner_wip_clinical&#39;: {&#39;current_version&#39;: &#39;2.6.5&#39;, &#39;latest_version&#39;: &#39;2.6.1&#39;}}, {&#39;jsl_ner_wip_greedy_clinical&#39;: {&#39;current_version&#39;: &#39;2.6.5&#39;, &#39;latest_version&#39;: &#39;2.6.5&#39;}}, {&#39;jsl_ner_wip_modifier_clinical&#39;: {&#39;current_version&#39;: &#39;2.6.4&#39;, &#39;latest_version&#39;: &#39;2.6.4&#39;}}, {&#39;jsl_rd_ner_wip_greedy_clinical&#39;: {&#39;current_version&#39;: &#39;2.6.1&#39;,&#39;latest_version&#39;: &#39;2.6.2&#39;}}] 5. Updated Pretrained Models: (requires fresh .pretraned()) ner_deid_large ner_deid_enriched 2.7.5 We are glad to announce that Spark NLP for Healthcare 2.7.5 has been released! Highlights: New pretrained Relation Extraction model to link clinical tests to test results and dates to clinical entities: re_test_result_date Adding two new Admission and Discharge entities to ner_events_clinical and renaming it to ner_events_admission_clinical Improving ner_deid_enriched NER model to cover Doctor and Patient name entities in various context and notations. Bug fixes &amp; general improvements. 1. re_test_result_date : text = “Hospitalized with pneumonia in June, confirmed by a positive PCR of any specimen, evidenced by SPO2 &lt;/= 93% or PaO2/FiO2 &lt; 300 mmHg”   Chunk-1 Entity-1 Chunk-2 Entity-2 Relation 0 pneumonia Problem june Date is_date_of 1 PCR Test positive Test_Result is_result_of 2 SPO2 Test 93% Test_Result is_result_of 3 PaO2/FiO2 Test 300 mmHg Test_Result is_result_of 2. ner_events_admission_clinical : ner_events_clinical NER model is updated &amp; improved to include Admission and Discharge entities. text =”She is diagnosed as cancer in 1991. Then she was admitted to Mayo Clinic in May 2000 and discharged in October 2001”   chunk entity 0 diagnosed OCCURRENCE 1 cancer PROBLEM 2 1991 DATE 3 admitted ADMISSION 4 Mayo Clinic CLINICAL_DEPT 5 May 2000 DATE 6 discharged DISCHARGE 7 October 2001 DATE 3. Improved ner_deid_enriched : PHI NER model is retrained to cover Doctor and Patient name entities even there is a punctuation between tokens as well as all upper case or lowercased. text =”A . Record date : 2093-01-13 , DAVID HALE , M.D . , Name : Hendrickson , Ora MR . # 7194334 Date : 01/13/93 PCP : Oliveira , 25 month years-old , Record date : 2079-11-09 . Cocke County Baptist Hospital . 0295 Keats Street”   chunk entity 0 2093-01-13 MEDICALRECORD 1 DAVID HALE DOCTOR 2 Hendrickson , Ora PATIENT 3 7194334 MEDICALRECORD 4 01/13/93 DATE 5 Oliveira DOCTOR 6 25 AGE 7 2079-11-09 MEDICALRECORD 8 Cocke County Baptist Hospital HOSPITAL 9 0295 Keats Street STREET 2.7.4 We are glad to announce that Spark NLP for Healthcare 2.7.4 has been released! Highlights: Introducing a new annotator to extract chunks with NER tags using regex-like patterns: NerChunker. Introducing two new annotators to filter chunks: ChunkFilterer and AssertionFilterer. Ability to change the entity type in NerConverterInternal without using ChunkMerger (setReplaceDict). In DeIdentification model, ability to use faker and static look-up lists at the same time randomly in Obfuscation mode. New De-Identification NER model, augmented with synthetic datasets to detect uppercased name entities. Bug fixes &amp; general improvements. 1. NerChunker: Similar to what we used to do in POSChunker with POS tags, now we can also extract phrases that fits into a known pattern using the NER tags. NerChunker would be quite handy to extract entity groups with neighboring tokens when there is no pretrained NER model to address certain issues. Lets say we want to extract clinical findings and body parts together as a single chunk even if there are some unwanted tokens between. How to use: ner_model = NerDLModel.pretrained(&quot;ner_radiology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) ner_chunker = NerChunker(). .setInputCols([&quot;sentence&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers([&quot;&lt;IMAGINGFINDINGS&gt;*&lt;BODYPART&gt;&quot;]) text = &#39;She has cystic cyst on her kidney.&#39; &gt;&gt; ner tags: [(cystic, B-IMAGINGFINDINGS), (cyst,I-IMAGINGFINDINGS), (kidney, B-BODYPART) &gt;&gt; ner_chunk: [&#39;cystic cyst on her kidney&#39;] 2. ChunkFilterer: ChunkFilterer will allow you to filter out named entities by some conditions or predefined look-up lists, so that you can feed these entities to other annotators like Assertion Status or Entity Resolvers. It can be used with two criteria: isin and regex. How to use: ner_model = NerDLModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) chunk_filterer = ChunkFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;) .setOutputCol(&quot;chunk_filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList([&#39;severe fever&#39;,&#39;sore throat&#39;]) text = &#39;Patient with severe fever, sore throat, stomach pain, and a headache.&#39; &gt;&gt; ner_chunk: [&#39;severe fever&#39;,&#39;sore throat&#39;,&#39;stomach pain&#39;,&#39;headache&#39;] &gt;&gt; chunk_filtered: [&#39;severe fever&#39;,&#39;sore throat&#39;] 3. AssertionFilterer: AssertionFilterer will allow you to filter out the named entities by the list of acceptable assertion statuses. This annotator would be quite handy if you want to set a white list for the acceptable assertion statuses like present or conditional; and do not want absent conditions get out of your pipeline. How to use: clinical_assertion = AssertionDLModel.pretrained(&quot;assertion_dl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) assertion_filterer = AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;assertion_filtered&quot;) .setWhiteList([&quot;present&quot;]) text = &#39;Patient with severe fever and sore throat, but no stomach pain.&#39; &gt;&gt; ner_chunk: [&#39;severe fever&#39;,&#39;sore throat&#39;,&#39;stomach pain&#39;,&#39;headache&#39;] &gt;&gt; assertion_filtered: [&#39;severe fever&#39;,&#39;sore throat&#39;] 2.7.3 We are glad to announce that Spark NLP for Healthcare 2.7.3 has been released! Highlights: Introducing a brand-new RelationExtractionDL Annotator – Achieving SOTA results in clinical relation extraction using BioBert. Massive Improvements &amp; feature enhancements in De-Identification module: Introduction of faker augmentation in Spark NLP for Healthcare to generate random data for obfuscation in de-identification module. Brand-new annotator for Structured De-Identification. Drug Normalizer: Normalize medication-related phrases (dosage, form and strength) and abbreviations in text and named entities extracted by NER models. Confidence scores in assertion output : just like NER output, assertion models now also support confidence scores for each prediction. Cosine similarity metrics in entity resolvers to get more informative and semantically correct results. AuxLabel in the metadata of entity resolvers to return additional mappings. New Relation Extraction models to extract relations between body parts and clinical entities. New Entity Resolver models to extract billable medical codes. New Clinical Pretrained NER models. Bug fixes &amp; general improvements. Matching the version with Spark NLP open-source v2.7.3. 1. Improvements in De-Identification Module: Integration of faker library to automatically generate random data like names, dates, addresses etc so users dont have to specify dummy data (custom obfuscation files can still be used). It also improves the obfuscation results due to a bigger pool of random values. How to use: Set the flag setObfuscateRefSource to faker deidentification = DeIdentification() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_chunk&quot;]) .setOutputCol(&quot;deidentified&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) For more details: Check out this notebook 2. Structured De-Identification Module: Introduction of a new annotator to handle de-identification of structured data. it allows users to define a mapping of columns and their obfuscation policy. Users can also provide dummy data and map them to columns they want to replace values in. How to use: obfuscator = StructuredDeidentification (spark,{&quot;NAME&quot;:&quot;PATIENT&quot;,&quot;AGE&quot;:&quot;AGE&quot;}, obfuscateRefSource = &quot;faker&quot;) obfuscator_df = obfuscator.obfuscateColumns(df) obfuscator_df.select(&quot;NAME&quot;,&quot;AGE&quot;).show(truncate=False) Example: Input Data: Name Age Cecilia Chapman 83 Iris Watson 9 Bryar Pitts 98 Theodore Lowe 16 Calista Wise 76 Deidentified: Name Age Menne Erdôs 20 Longin Robinson 31 Flynn Fiedlerová 50 John Wakeland 21 Vanessa Andersson 12 For more details: Check out this notebook. 3. Introducing SOTA relation extraction model using BioBert A brand-new end-to-end trained BERT model, resulting in massive improvements. Another new annotator (ReChunkFilter) is also developed for this new model to allow syntactic features work well with BioBert to extract relations. How to use: re_ner_chunk_filter = RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) .setRelationPairs(pairs) .setMaxSyntacticDistance(4) re_model = RelationExtractionDLModel() .pretrained(“redl_temporal_events_biobert”, &quot;en&quot;, &quot;clinical/models&quot;) .setPredictionThreshold(0.9) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) Benchmarks: on benchmark datasets model Spark NLP ML model Spark NLP DL model benchmark re_temporal_events_clinical 68.29 71.0 80.2 1 re_clinical 56.45 69.2 68.2 2 re_human_pheotype_gene_clinical - 87.9 67.2 3 re_drug_drug_interaction - 72.1 83.8 4 re_chemprot 76.69 94.1 83.64 5 on in-house annotations model Spark NLP ML model Spark NLP DL model re_bodypart_problem 84.58 85.7 re_bodypart_procedure 61.0 63.3 re_date_clinical 83.0 84.0 re_bodypart_direction 93.5 92.5 For more details: Check out the notebook or modelshub. 4. Drug Normalizer: Standardize units of drugs and handle abbreviations in raw text or drug chunks identified by any NER model. This normalization significantly improves performance of entity resolvers. How to use: drug_normalizer = DrugNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;document_normalized&quot;) .setPolicy(&quot;all&quot;) #all/abbreviations/dosages Examples: drug_normalizer.transform(&quot;adalimumab 54.5 + 43.2 gm”) &gt;&gt;&gt; &quot;adalimumab 97700 mg&quot; Changes: combine 54.5 + 43.2 and normalize gm to mg drug_normalizer.transform(&quot;Agnogenic one half cup”) &gt;&gt;&gt; &quot;Agnogenic 0.5 oral solution&quot; Changes: replace one half to the 0.5, normalize cup to the oral solution drug_normalizer.transform(&quot;interferon alfa-2b 10 million unit ( 1 ml ) injec”) &gt;&gt;&gt; &quot;interferon alfa - 2b 10000000 unt ( 1 ml ) injection &quot; Changes: convert 10 million unit to the 10000000 unt, replace injec with injection For more details: Check out this notebook 5. Assertion models to support confidence in output: Just like NER output, assertion models now also provides confidence scores for each prediction. chunks entities assertion confidence a headache PROBLEM present 0.9992 anxious PROBLEM conditional 0.9039 alopecia PROBLEM absent 0.9992 pain PROBLEM absent 0.9238 .setClasses() method is deprecated in AssertionDLApproach and users do not need to specify number of classes while training, as it will be inferred from the dataset. 6. New Relation Extraction Models: We are also releasing new relation extraction models to link the clinical entities to body parts and dates. These models are trained using binary relation extraction approach for better accuracy. - re_bodypart_direction : Relation Extraction between Body Part and Direction entities. Example: Text: “MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia” relations entity1 chunk1 entity2 chunk2 confidence 1 Direction upper bodyPart brain stem 0.999 0 Direction upper bodyPart cerebellum 0.999 0 Direction upper bodyPart basil ganglia 0.999 0 bodyPart brain stem Direction left 0.999 0 bodyPart brain stem Direction right 0.999 1 Direction left bodyPart cerebellum 1.0 0 Direction left bodyPart basil ganglia 0.976 0 bodyPart cerebellum Direction right 0.953 1 Direction right bodyPart basil ganglia 1.0 - re_bodypart_problem : Relation Extraction between Body Part and Problem entities. Example: Text: “No neurologic deficits other than some numbness in his left hand.” relation entity1 chunk1 entity2 chunk2 confidence 0 Symptom neurologic deficits bodyPart hand 1 1 Symptom numbness bodyPart hand 1 - re_bodypart_proceduretest : Relation Extraction between Body Part and Procedure, Test entities. Example: Text: “TECHNIQUE IN DETAIL: After informed consent was obtained from the patient and his mother, the chest was scanned with portable ultrasound.” relation entity1 chunk1 entity2 chunk2 confidence 1 bodyPart chest Test portable ultrasound 0.999 -re_date_clinical : Relation Extraction between Date and different clinical entities. Example: Text: “This 73 y/o patient had CT on 1/12/95, with progressive memory and cognitive decline since 8/11/94.” relations entity1 chunk1 entity2 chunk2 confidence 1 Test CT Date 1/12/95 1.0 1 Symptom progressive memory and cognitive decline Date 8/11/94 1.0 How to use: re_model = RelationExtractionModel() .pretrained(&quot;re_bodypart_direction&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setMaxSyntacticDistance(4) .setRelationPairs([‘Internal_organ_or_component’, ‘Direction’]) For more details: Check out the notebook or modelshub. New matching scheme for entity resolvers - improved accuracy: Adding the option to use cosine similarity to resolve entities and find closest matches, resulting in better, more semantically correct results. 7. New Resolver Models using JSL SBERT: sbiobertresolve_icd10cm_augmented sbiobertresolve_cpt_augmented sbiobertresolve_cpt_procedures_augmented sbiobertresolve_icd10cm_augmented_billable_hcc sbiobertresolve_hcc_augmented Returning auxilary columns mapped to resolutions: Chunk entity resolver and sentence entity resolver now returns auxilary data that is mapped the resolutions during training. This will allow users to get multiple resolutions with single model without using any other annotator in the pipeline (In order to get billable codes otherwise there needs to be other modules in the same pipeline) Example: sbiobertresolve_icd10cm_augmented_billable_hcc Input Text: “bladder cancer” idx chunks code resolutions all_codes billable hcc_status hcc_score all_distances 0 bladder cancer C679 [‘bladder cancer’, ‘suspected bladder cancer’, ‘cancer in situ of urinary bladder’, ‘tumor of bladder neck’, ‘malignant tumour of bladder neck’] [‘C679’, ‘Z126’, ‘D090’, ‘D494’, ‘C7911’] [‘1’, ‘1’, ‘1’, ‘1’, ‘1’] [‘1’, ‘0’, ‘0’, ‘0’, ‘1’] [‘11’, ‘0’, ‘0’, ‘0’, ‘8’] [‘0.0000’, ‘0.0904’, ‘0.0978’, ‘0.1080’, ‘0.1281’] sbiobertresolve_cpt_augmented Input Text: “ct abdomen without contrast” idx cpt code distance resolutions 0 74150 0.0802 Computed tomography, abdomen; without contrast material 1 65091 0.1312 Evisceration of ocular contents; without implant 2 70450 0.1323 Computed tomography, head or brain; without contrast material 3 74176 0.1333 Computed tomography, abdomen and pelvis; without contrast material 4 74185 0.1343 Magnetic resonance imaging without contrast 5 77059 0.1343 Magnetic resonance imaging without contrast 8. New Pretrained Clinical NER Models NER Radiology Input Text: “Bilateral breast ultrasound was subsequently performed, which demonstrated an ovoid mass measuring approximately 0.5 x 0.5 x 0.4 cm in diameter located within the anteromedial aspect of the left shoulder. This mass demonstrates isoechoic echotexture to the adjacent muscle, with no evidence of internal color flow. This may represent benign fibrous tissue or a lipoma.” idx chunks entities 0 Bilateral Direction 1 breast BodyPart 2 ultrasound ImagingTest 3 ovoid mass ImagingFindings 4 0.5 x 0.5 x 0.4 Measurements 5 cm Units 6 anteromedial aspect Direction 7 left Direction 8 shoulder BodyPart 9 mass ImagingFindings 10 isoechoic echotexture ImagingFindings 11 muscle BodyPart 12 internal color flow ImagingFindings 13 benign fibrous tissue ImagingFindings 14 lipoma Disease_Syndrome_Disorder 2.7.2 We are glad to announce that Spark NLP for Healthcare 2.7.2 has been released ! In this release, we introduce the following features: Far better accuracy for resolving medication terms to RxNorm codes: ondansetron 8 mg tablet&#39; -&gt; &#39;312086 Far better accuracy for resolving diagnosis terms to ICD-10-CM codes: TIA -&gt; transient ischemic attack (disorder) ‘S0690’ New ability to map medications to pharmacological actions (PA): &#39;metformin&#39; -&gt; ‘Hypoglycemic Agents’ 2 new greedy named entity recognition models for medication details: ner_drugs_greedy: ‘magnesium hydroxide 100mg/1ml PO’ ` ner_posology _greedy: ‘12 units of insulin lispro’ ` New model to classify the gender of a patient in a given medical note: &#39;58yo patient with a family history of breast cancer&#39; -&gt; ‘female’ And starting customized spark sessions with rich parameters params = {&quot;spark.driver.memory&quot;:&quot;32G&quot;, &quot;spark.kryoserializer.buffer.max&quot;:&quot;2000M&quot;, &quot;spark.driver.maxResultSize&quot;:&quot;2000M&quot;} spark = sparknlp_jsl.start(secret, params=params) State-of-the-art accuracy is achieved using new healthcare-tuned BERT Sentence Embeddings (s-Bert). The following sections include more details, metrics, and examples. Named Entity Recognizers for Medications A new medication NER (ner_drugs_greedy) that joins the drug entities with neighboring entities such as dosage, route, form and strength; and returns a single entity drug. This greedy NER model would be highly useful if you want to extract a drug with its context and then use it to get a RxNorm code (drugs may get different RxNorm codes based on the dosage and strength information). Metrics label tp fp fn prec rec f1 I-DRUG 37423 4179 3773 0.899 0.908 0.904 B-DRUG 29699 2090 1983 0.934 0.937 0.936 A new medication NER (ner_posology_greedy) that joins the drug entities with neighboring entities such as dosage, route, form and strength. It also returns all the other medication entities even if not related to (or joined with) a drug. Now we have five different medication-related NER models. You can see the outputs from each model below: Text = ‘‘The patient was prescribed 1 capsule of Advil 10 mg for 5 days and magnesium hydroxide 100mg/1ml suspension PO. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.’’ a. ner_drugs_greedy   chunks begin end entities 0 1 capsule of Advil 10 mg 27 50 DRUG 1 magnesium hydroxide 100mg/1ml PO 67 98 DRUG 2 40 units of insulin glargine 168 195 DRUG 3 12 units of insulin lispro 207 232 DRUG b. ner_posology_greedy   chunks begin end entities 0 1 capsule of Advil 10 mg 27 50 DRUG 1 magnesium hydroxide 100mg/1ml PO 67 98 DRUG 2 for 5 days 52 61 DURATION 3 40 units of insulin glargine 168 195 DRUG 4 at night 197 204 FREQUENCY 5 12 units of insulin lispro 207 232 DRUG 6 with meals 234 243 FREQUENCY 7 metformin 1000 mg 250 266 DRUG 8 two times a day 268 282 FREQUENCY c. ner_drugs   chunks begin end entities 0 Advil 40 44 DrugChem 1 magnesium hydroxide 67 85 DrugChem 2 metformin 261 269 DrugChem d.ner_posology   chunks begin end entities 0 1 27 27 DOSAGE 1 capsule 29 35 FORM 2 Advil 40 44 DRUG 3 10 mg 46 50 STRENGTH 4 for 5 days 52 61 DURATION 5 magnesium hydroxide 67 85 DRUG 6 100mg/1ml 87 95 STRENGTH 7 PO 97 98 ROUTE 8 40 units 168 175 DOSAGE 9 insulin glargine 180 195 DRUG 10 at night 197 204 FREQUENCY 11 12 units 207 214 DOSAGE 12 insulin lispro 219 232 DRUG 13 with meals 234 243 FREQUENCY 14 metformin 250 258 DRUG 15 1000 mg 260 266 STRENGTH 16 two times a day 268 282 FREQUENCY e. ner_drugs_large   chunks begin end entities 0 Advil 10 mg 40 50 DRUG 1 magnesium hydroxide 100mg/1ml PO. 67 99 DRUG 2 insulin glargine 180 195 DRUG 3 insulin lispro 219 232 DRUG 4 metformin 1000 mg 250 266 DRUG Patient Gender Classification This model detects the gender of the patient in the clinical document. It can classify the documents into Female, Male and Unknown. We release two models: ‘Classifierdl_gender_sbert’ (more accurate, works with licensed sbiobert_base_cased_mli) ‘Classifierdl_gender_biobert’ (works with biobert_pubmed_base_cased) The models are trained on more than four thousands clinical documents (radiology reports, pathology reports, clinical visits etc.), annotated internally. Metrics (Classifierdl_gender_sbert)   precision recall f1-score support Female 0.9224 0.8954 0.9087 239 Male 0.7895 0.8468 0.8171 124 Text= ‘‘social history: shows that does not smoke cigarettes or drink alcohol, lives in a nursing home. family history: shows a family history of breast cancer.’’ gender_classifier.annotate(text)[&#39;class&#39;][0] &gt;&gt; `Female` See this Colab notebook for further details. a. classifierdl_gender_sbert document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) .setMaxSentenceLength(512) gender_classifier = ClassifierDLModel .pretrained(&#39;classifierdl_gender_sbert&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;document&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;class&quot;) gender_pred_pipeline = Pipeline( stages = [ document, sbert_embedder, gender_classifier ]) b. classifierdl_gender_biobert documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) clf_tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) biobert_embeddings = BertEmbeddings().pretrained(&#39;biobert_pubmed_base_cased&#39;) .setInputCols([&quot;document&quot;,&#39;token&#39;]) .setOutputCol(&quot;bert_embeddings&quot;) biobert_embeddings_avg = SentenceEmbeddings() .setInputCols([&quot;document&quot;, &quot;bert_embeddings&quot;]) .setOutputCol(&quot;sentence_bert_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) genderClassifier = ClassifierDLModel.pretrained(&#39;classifierdl_gender_biobert&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;document&quot;, &quot;sentence_bert_embeddings&quot;]) .setOutputCol(&quot;gender&quot;) gender_pred_pipeline = Pipeline( stages = [ documentAssembler, clf_tokenizer, biobert_embeddings, biobert_embeddings_avg, genderClassifier ]) New ICD10CM and RxCUI resolvers powered by s-Bert embeddings The advent of s-Bert sentence embeddings changed the landscape of Clinical Entity Resolvers completely in Spark NLP. Since s-Bert is already tuned on MedNLI (medical natural language inference) dataset, it is now capable of populating the chunk embeddings in a more precise way than before. We now release two new resolvers: sbiobertresolve_icd10cm_augmented (augmented with synonyms, four times richer than previous resolver accuracy: 73% for top-1 (exact match), 89% for top-5 (previous accuracy was 59% and 64% respectively) sbiobertresolve_rxcui (extract RxNorm concept unique identifiers to map with ATC or durg families) accuracy: 71% for top-1 (exact match), 72% for top-5 (previous accuracy was 22% and 41% respectively) a. ICD10CM augmented resolver Text = “This is an 82 year old male with a history of prior tobacco use , hypertension , chronic renal insufficiency , COPD , gastritis , and TIA who initially presented to Braintree with a non-ST elevation MI and Guaiac positive stools , transferred to St . Margaret&#39;s Center for Women &amp; Infants for cardiac catheterization with PTCA to mid LAD lesion complicated by hypotension and bradycardia requiring Atropine , IV fluids and transient dopamine possibly secondary to vagal reaction , subsequently transferred to CCU for close monitoring , hemodynamically stable at the time of admission to the CCU . “   chunk begin end code term 0 hypertension 66 77 I10 hypertension 1 chronic renal insufficiency 81 107 N189 chronic renal insufficiency 2 COPD 111 114 J449 copd - chronic obstructive pulmonary disease 3 gastritis 118 126 K2970 gastritis 4 TIA 134 136 S0690 transient ischemic attack (disorder) 5 a non-ST elevation MI 180 200 I219 silent myocardial infarction (disorder) 6 Guaiac positive stools 206 227 K921 guaiac-positive stools 7 mid LAD lesion 330 343 I2102 stemi involving left anterior descending coronary artery 8 hypotension 360 370 I959 hypotension 9 bradycardia 376 386 O9941 bradycardia b. RxCUI resolver Text= “He was seen by the endocrinology service and she was discharged on 50 mg of eltrombopag oral at night, 5 mg amlodipine with meals, and metformin 1000 mg two times a day . “   chunk begin end code term 0 50 mg of eltrombopag oral 67 91 825427 eltrombopag 50 MG Oral Tablet 1 5 mg amlodipine 103 117 197361 amlodipine 5 MG Oral Tablet 2 metformin 1000 mg 135 151 861004 metformin hydrochloride 1000 MG Oral Tablet Using this new resolver and some other resources like Snomed Resolver, RxTerm, MESHPA and ATC dictionary, you can link the drugs to the pharmacological actions (PA), ingredients and the disease treated with that. Code sample: (after getting the chunk from ChunkConverter) c2doc = Chunk2Doc().setInputCols(&quot;ner_chunk&quot;).setOutputCol(&quot;ner_chunk_doc&quot;) sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) icd10_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_icd10cm_augmented&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;icd10cm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) See the notebook for details. 2.7.1 We are glad to announce that Spark NLP for Healthcare 2.7.1 has been released ! In this release, we introduce the following features: 1. Sentence BioBert and Bluebert Transformers that are fine tuned on MedNLI dataset. Sentence Transformers offers a framework that provides an easy method to compute dense vector representations for sentences and paragraphs (also known as sentence embeddings). The models are based on BioBert and BlueBert, and are tuned specifically to meaningful sentence embeddings such that sentences with similar meanings are close in vector space. These are the first PyTorch based models we managed to port into Spark NLP. Here is how you can load these: sbiobert_embeddins = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) sbluebert_embeddins = BertSentenceEmbeddings .pretrained(&quot;sbluebert_base_cased_mli&quot;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) 2. SentenceEntityResolvers powered by s-Bert embeddings. The advent of s-Bert sentence embeddings changed the landscape of Clinical Entity Resolvers completely in Spark NLP. Since s-Bert is already tuned on MedNLI (medical natural language inference) dataset, it is now capable of populating the chunk embeddings in a more precise way than before. Using sbiobert_base_cased_mli, we trained the following Clinical Entity Resolvers: sbiobertresolve_icd10cm sbiobertresolve_icd10pcs sbiobertresolve_snomed_findings (with clinical_findings concepts from CT version) sbiobertresolve_snomed_findings_int (with clinical_findings concepts from INT version) sbiobertresolve_snomed_auxConcepts (with Morph Abnormality, Procedure, Substance, Physical Object, Body Structure concepts from CT version) sbiobertresolve_snomed_auxConcepts_int (with Morph Abnormality, Procedure, Substance, Physical Object, Body Structure concepts from INT version) sbiobertresolve_rxnorm sbiobertresolve_icdo sbiobertresolve_cpt Code sample: (after getting the chunk from ChunkConverter) c2doc = Chunk2Doc().setInputCols(&quot;ner_chunk&quot;).setOutputCol(&quot;ner_chunk_doc&quot;) sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) snomed_ct_resolver = SentenceEntityResolverModel .pretrained(&quot;sbiobertresolve_snomed_findings&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_ct_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) Output:   chunks begin end code resolutions 2 COPD 113 116 13645005 copd - chronic obstructive pulmonary disease 8 PTCA 324 327 373108000 post percutaneous transluminal coronary angioplasty (finding) 16 close monitoring 519 534 417014005 on examination - vigilance See the notebook for details. 3. We are releasing the following pretrained clinical NER models: ner_drugs_large (trained with medications dataset, and extracts drugs with the dosage, strength, form and route at once as a single entity; entities: drug) ner_deid_sd_large (extracts PHI entities, trained with augmented dataset) ner_anatomy_coarse (trained with enriched anatomy NER dataset; entities: anatomy) ner_anatomy_coarse_biobert chunkresolve_ICD10GM_2021 (German ICD10GM resolver) We are also releasing two new NER models: ner_aspect_based_sentiment (extracts positive, negative and neutral aspects about restaurants from the written feedback given by reviewers. ) ner_financial_contract (extract financial entities from contracts. See the notebook for details.) 2.7.0 We are glad to announce that Spark NLP for Healthcare 2.7 has been released ! In this release, we introduce the following features: 1. Text2SQL Text2SQL Annotator that translates natural language text into SQL queries against a predefined database schema, which is one of the most sought-after features of NLU. With the help of a pretrained text2SQL model, you will be able to query your database without writing a SQL query: Example 1 Query: What is the name of the nurse who has the most appointments? Generated SQL query from the model: SELECT T1.Name FROM Nurse AS T1 JOIN Appointment AS T2 ON T1.EmployeeID = T2.PrepNurse GROUP BY T2.prepnurse ORDER BY count(*) DESC LIMIT 1 Response:   Name 0 Carla Espinosa Example 2 Query: How many patients do each physician take care of? List their names and number of patients they take care of. Generated SQL query from the model: SELECT T1.Name, count(*) FROM Physician AS T1 JOIN Patient AS T2 ON T1.EmployeeID = T2.PCP GROUP BY T1.Name Response:   Name count(*) 0 Christopher Turk 1 1 Elliot Reid 2 2 John Dorian 1 For now, it only comes with one pretrained model (trained on Spider dataset) and new pretrained models will be released soon. Check out the Colab notebook to see more examples and run on your data. 2. SentenceEntityResolvers In addition to ChunkEntityResolvers, we now release our first BioBert-based entity resolvers using the SentenceEntityResolver annotator. It’s fully trainable and comes with several pretrained entity resolvers for the following medical terminologies: CPT: biobertresolve_cpt ICDO: biobertresolve_icdo ICD10CM: biobertresolve_icd10cm ICD10PCS: biobertresolve_icd10pcs LOINC: biobertresolve_loinc SNOMED_CT (findings): biobertresolve_snomed_findings SNOMED_INT (clinical_findings): biobertresolve_snomed_findings_int RXNORM (branded and clinical drugs): biobertresolve_rxnorm_bdcd Example: text = &#39;He has a starvation ketosis but nothing significant for dry oral mucosa&#39; df = get_icd10_codes (light_pipeline_icd10, &#39;icd10cm_code&#39;, text)   chunks begin end code 0 a starvation ketosis 7 26 E71121 1 dry oral mucosa 66 80 K136 Check out the Colab notebook to see more examples and run on your data. You can also train your own entity resolver using any medical terminology like MedRa and UMLS. Check this notebook to learn more about training from scratch. 3. ChunkMerge Annotator In order to use multiple NER models in the same pipeline, Spark NLP Healthcare has ChunkMerge Annotator that is used to return entities from each NER model by overlapping. Now it has a new parameter to avoid merging overlapping entities (setMergeOverlapping) to return all the entities regardless of char indices. It will be quite useful to analyze what every NER module returns on the same text. 4. Starting SparkSession We now support starting SparkSession with a different version of the open source jar and not only the one it was built against by sparknlp_jsl.start(secret, public=&quot;x.x.x&quot;) for extreme cases. 5. Biomedical NERs We are releasing 3 new biomedical NER models trained with clinical embeddings (all one single entity models) ner_bacterial_species (comprising of Linneaus and Species800 datasets) ner_chemicals (general purpose and bio chemicals, comprising of BC4Chem and BN5CDR-Chem) ner_diseases_large (comprising of ner_disease, NCBI_Disease and BN5CDR-Disease) We are also releasing the biobert versions of the several clinical NER models stated below: ner_clinical_biobert ner_anatomy_biobert ner_bionlp_biobert ner_cellular_biobert ner_deid_biobert ner_diseases_biobert ner_events_biobert ner_jsl_biobert ner_chemprot_biobert ner_human_phenotype_gene_biobert ner_human_phenotype_go_biobert ner_posology_biobert ner_risk_factors_biobert Metrics (micro averages excluding O’s):   model_name clinical_glove_micro biobert_micro 0 ner_chemprot_clinical 0.816 0.803 1 ner_bionlp 0.748 0.808 2 ner_deid_enriched 0.934 0.918 3 ner_posology 0.915 0.911 4 ner_events_clinical 0.801 0.809 5 ner_clinical 0.873 0.884 6 ner_posology_small 0.941   7 ner_human_phenotype_go_clinical 0.922 0.932 8 ner_drugs 0.964   9 ner_human_phenotype_gene_clinical 0.876 0.870 10 ner_risk_factors 0.728   11 ner_cellular 0.813 0.812 12 ner_posology_large 0.921   13 ner_anatomy 0.851 0.831 14 ner_deid_large 0.942   15 ner_diseases 0.960 0.966 In addition to these, we release two new German NER models: ner_healthcare_slim (‘TIME_INFORMATION’, ‘MEDICAL_CONDITION’, ‘BODY_PART’, ‘TREATMENT’, ‘PERSON’, ‘BODY_PART’) ner_traffic (extract entities regarding traffic accidents e.g. date, trigger, location etc.) 6. PICO Classifier Successful evidence-based medicine (EBM) applications rely on answering clinical questions by analyzing large medical literature databases. In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components: Participants/Problem (P) (e.g., diabetic patients), Intervention (I) (e.g., insulin), Comparison (C) (e.g., placebo) and Outcome (O) (e.g., blood glucose levels). Spark NLP now introduces a pretrained PICO Classifier that is trained with Biobert embeddings. Example: text = “There appears to be no difference in smoking cessation effectiveness between 1mg and 0.5mg varenicline.” pico_lp_pipeline.annotate(text)[&#39;class&#39;][0] ans: CONCLUSIONS 2.6.2 Overview We are very happy to announce that version 2.6.2 of Spark NLP Enterprise is ready to be installed and used. We are making available Named Entity Recognition, Sentence Classification and Entity Resolution models to analyze Adverse Drug Events in natural language text from clinical domains. Models NERs We are pleased to announce that we have a brand new named entity recognition (NER) model for Adverse Drug Events (ADE) to extract ADE and DRUG entities from a given text. ADE NER will have four versions in the library, trained with different size of word embeddings: ner_ade_bioert (768d Bert embeddings) ner_ade_clinicalbert (768d Bert embeddings) ner_ade_clinical (200d clinical embeddings) ner_ade_healthcare (100d healthcare embeddings) More information and examples here We are also releasing our first clinical pretrained classifier for ADE classification tasks. This new ADE classifier is trained on various ADE datasets, including the mentions in tweets to represent the daily life conversations as well. So it works well on the texts coming from academic context, social media and clinical notes. It’s trained with Clinical Biobert embeddings, which is the most powerful contextual language model in the clinical domain out there. Classifiers ADE classifier will have two versions in the library, trained with different Bert embeddings: classifierdl_ade_bioert (768d BioBert embeddings) classifierdl_adee_clinicalbert (768d ClinicalBert embeddings) More information and examples here Pipeline By combining ADE NER and Classifier, we are releasing a new pretrained clinical pipeline for ADE tasks to save you from building pipelines from scratch. Pretrained pipelines are already fitted using certain annotators and transformers according to various use cases and you can use them as easy as follows: pipeline = PretrainedPipeline(&#39;explain_clinical_doc_ade&#39;, &#39;en&#39;, &#39;clinical/models&#39;) pipeline.annotate(&#39;my string&#39;) explain_clinical_doc_ade is bundled with ner_ade_clinicalBert, and classifierdl_ade_clinicalBert. It can extract ADE and DRUG clinical entities, and then assign ADE status to a text (True means ADE, False means not related to ADE). More information and examples here Entity Resolver We are releasing the first Entity Resolver for Athena (Automated Terminology Harmonization, Extraction and Normalization for Analytics, http://athena.ohdsi.org/) to extract concept ids via standardized medical vocabularies. For now, it only supports conditions section and can be used to map the clinical conditions with the corresponding standard terminology and then get the concept ids to store them in various database schemas. It is named as chunkresolve_athena_conditions_healthcare. We added slim versions of several clinical NER models that are trained with 100d healthcare word embeddings, which is lighter and smaller in size. ner_healthcare assertion_dl_healthcare ner_posology_healthcare ner_events_healthcare Graph Builder Spark NLP Licensed version has several DL based annotators (modules) such as NerDL, AssertionDL, RelationExtraction and GenericClassifier, and they are all based on Tensorflow (tf) with custom graphs. In order to make the creating and customizing the tf graphs for these models easier for our licensed users, we added a graph builder to the Python side of the library. Now you can customize your graphs and use them in the respected models while training a new DL model. from sparknlp_jsl.training import tf_graph tf_graph.build(&quot;relation_extraction&quot;,build_params={&quot;input_dim&quot;: 6000, &quot;output_dim&quot;: 3, &#39;batch_norm&#39;:1, &quot;hidden_layers&quot;: [300, 200], &quot;hidden_act&quot;: &quot;relu&quot;, &#39;hidden_act_l2&#39;:1}, model_location=&quot;.&quot;, model_filename=&quot;re_with_BN&quot;) More information and examples here 2.6.0 Overview We are honored to announce that Spark NLP Enterprise 2.6.0 has been released. The first time ever, we release three pretrained clinical pipelines to save you from building pipelines from scratch. Pretrained pipelines are already fitted using certain annotators and transformers according to various use cases. The first time ever, we are releasing 3 licensed German models for healthcare and Legal domains. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Models Pretrained Pipelines: The first time ever, we release three pretrained clinical pipelines to save you from building pipelines from scratch. Pretrained pipelines are already fitted using certain annotators and transformers according to various use cases and you can use them as easy as follows: pipeline = PretrainedPipeline(&#39;explain_clinical_doc_carp&#39;, &#39;en&#39;, &#39;clinical/models&#39;) pipeline.annotate(&#39;my string&#39;) Pipeline descriptions: explain_clinical_doc_carp a pipeline with ner_clinical, assertion_dl, re_clinical and ner_posology. It will extract clinical and medication entities, assign assertion status and find relationships between clinical entities. explain_clinical_doc_era a pipeline with ner_clinical_events, assertion_dl and re_temporal_events_clinical. It will extract clinical entities, assign assertion status and find temporal relationships between clinical entities. recognize_entities_posology a pipeline with ner_posology. It will only extract medication entities. More information and examples are available here: https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/11.Pretrained_Clinical_Pipelines.ipynb. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Pretrained Named Entity Recognition and Relationship Extraction Models (English) RE models: re_temporal_events_clinical re_temporal_events_enriched_clinical re_human_phenotype_gene_clinical re_drug_drug_interaction_clinical re_chemprot_clinical NER models: ner_human_phenotype_gene_clinical ner_human_phenotype_go_clinical ner_chemprot_clinical More information and examples here: https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/10.Clinical_Relation_Extraction.ipynb &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Pretrained Named Entity Recognition and Relationship Extraction Models (German) The first time ever, we are releasing 3 licensed German models for healthcare and Legal domains. German Clinical NER model for 19 clinical entities German Legal NER model for 19 legal entities German ICD-10GM More information and examples here: https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/14.German_Healthcare_Models.ipynb https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/15.German_Legal_Model.ipynb &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Other Pretrained Models We now have Named Entity Disambiguation model out of the box. Disambiguation models map words of interest, such as names of persons, locations and companies, from an input text document to corresponding unique entities in a target Knowledge Base (KB). https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/12.Named_Entity_Disambiguation.ipynb Due to ongoing requests about Clinical Entity Resolvers, we release a notebook to let you see how to train an entity resolver using an open source dataset based on Snomed. https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/13.Snomed_Entity_Resolver_Model_Training.ipynb &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.5.5 Overview We are very happy to release Spark NLP for Healthcare 2.5.5 with a new state-of-the-art RelationExtraction annotator to identify relationships between entities coming from our pretrained NER models. This is also the first release to support Relation Extraction with the following two (2) models: re_clinical and re_posology in the clinical/models repository. We also include multiple bug fixes as usual. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features RelationExtraction annotator that receives WORD_EMBEDDINGS, POS, CHUNK, DEPENDENCY and returns the CATEGORY of the relationship and a confidence score. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements AssertionDL Annotator now keeps logs of the metrics while training DeIdentification now has a default behavior of merging entities close in Levenshtein distance with setConsistentObfuscation and setSameEntityThreshold params. DeIdentification now has a specific parameter setObfuscateDate to obfuscate dates (which will be otherwise just masked). The only formats obfuscated when the param is true will be the ones present in dateFormats param. NerConverterInternal now has a greedyMode param that will merge all contiguous tags of the same type regardless of boundary tags like “B”,”E”,”S”. AnnotationToolJsonReader includes mergeOverlapping parameter to merge (or not) overlapping entities from the Annotator jsons i.e. not included in the assertion list. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes DeIdentification documentation bug fix (typo) DeIdentification training bug fix in obfuscation dictionary IOBTagger now has the correct output type NAMED_ENTITY &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Deprecations EnsembleEntityResolver has been deprecated Models We have 2 new english Relationship Extraction model for Clinical and Posology NERs: re_clinical: with ner_clinical and embeddings_clinical re_posology: with ner_posology and embeddings_clinical &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.5.3 Overview We are pleased to announce the release of Spark NLP for Healthcare 2.5.3. This time we include four (4) new Annotators: FeatureAssembler, GenericClassifier, Yake Keyword Extractor and NerConverterInternal. We also include helper classes to read datasets from CodiEsp and Cantemist Spanish NER Challenges. This is also the first release to support the following models: ner_diag_proc (spanish), ner_neoplasms (spanish), ner_deid_enriched (english). We have also included Bugifxes and Enhancements for AnnotationToolJsonReader and ChunkMergeModel. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features FeatureAssembler Transformer: Receives a list of column names containing numerical arrays and concatenates them to form one single feature_vector annotation GenericClassifier Annotator: Receives a feature_vector annotation and outputs a category annotation Yake Keyword Extraction Annotator: Receives a token annotation and outputs multi-token keyword annotations NerConverterInternal Annotator: Similar to it’s open source counterpart in functionality, performs smarter extraction for complex tokenizations and confidence calculation Readers for CodiEsp and Cantemist Challenges &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements AnnotationToolJsonReader includes parameter for preprocessing pipeline (from Document Assembling to Tokenization) AnnotationToolJsonReader includes parameter to discard specific entity types &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes ChunkMergeModel now prioritizes highest number of different entities when coverage is the same &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Models We have 2 new spanish models for Clinical Entity Recognition: ner_diag_proc and ner_neoplasms We have a new english Named Entity Recognition model for deidentification: ner_deid_enriched &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.5.2 Overview We are really happy to bring you Spark NLP for Healthcare 2.5.2, with a couple new features and several enhancements in our existing annotators. This release was mainly dedicated to generate adoption in our AnnotationToolJsonReader, a connector that provide out-of-the-box support for out Annotation Tool and our practices. Also the ChunkMerge annotator has ben provided with extra functionality to remove entire entity types and to modify some chunk’s entity type We also dedicated some time in finalizing some refactorization in DeIdentification annotator, mainly improving type consistency and case insensitive entity dictionary for obfuscation. Thanks to the community for all the feedback and suggestions, it’s really comfortable to navigate together towards common functional goals that keep us agile in the SotA. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features Brand new IOBTagger Annotator NerDL Metrics provides an intuitive DataFrame API to calculate NER metrics at tag (token) and entity (chunk) level &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements AnnotationToolJsonReader includes parameters for document cleanup, sentence boundaries and tokenizer split chars AnnotationToolJsonReader uses the task title if present and uses IOBTagger annotator AnnotationToolJsonReader has improved alignment in assertion train set generation by using an alignTol parameter as tollerance in chunk char alignment DeIdentification refactorization: Improved typing and replacement logic, case insensitive entities for obfuscation ChunkMerge Annotator now handles: Drop all chunks for an entity Replace entity name Change entity type for a specific (chunk, entity) pair Drop specific (chunk, entity) pairs caseSensitive param to EnsembleEntityResolver Output logs for AssertionDLApproach loss Disambiguator is back with improved dependency management &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes Bugfix in python when Annotators shared domain parts across public and internal Bugfix in python when ChunkMerge annotator was loaded from disk ChunkMerge now weights the token coverage correctly when multiple multi-token entities overlap &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.5.0 Overview We are happy to bring you Spark NLP for Healthcare 2.5.0 with new Annotators, Models and Data Readers. Model composition and iteration is now faster with readers and annotators designed for real world tasks. We introduce ChunkMerge annotator to combine all CHUNKS extracted by different Entity Extraction Annotators. We also introduce an Annotation Reader for JSL AI Platform’s Annotation Tool. This release is also the first one to support the models: ner_large_clinical, ner_events_clinical, assertion_dl_large, chunkresolve_loinc_clinical, deidentify_large And of course we have fixed some bugs. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features AnnotationToolJsonReader is a new class that imports a JSON from AI Platform’s Annotation Tool an generates NER and Assertion training datasets ChunkMerge Annotator is a new functionality that merges two columns of CHUNKs handling overlaps with a very straightforward logic: max coverage, max # entities ChunkMerge Annotator handles inputs from NerDLModel, RegexMatcher, ContextualParser, TextMatcher A DeIdentification pretrained model can now work in ‘mask’ or ‘obfuscate’ mode &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements DeIdentification Annotator has a more consistent API: mode param with values (‘mask’l’obfuscate’) to drive its behavior dateFormats param a list of string values to to select which dateFormats to obfuscate (and which to just mask) DeIdentification Annotator no longer automatically obfuscates dates. Obfuscation is now driven by mode and dateFormats params A DeIdentification pretrained model can now work in ‘mask’ or ‘obfuscate’ mode &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes DeIdentification Annotator now correctly deduplicates protected entities coming from NER / Regex DeIdentification Annotator now indexes chunks correctly after merging them AssertionDLApproach Annotator can now be trained with the graph in any folder specified by setting graphFolder param AssertionDLApproach now has the setClasses param setter in Python wrapper JVM Memory and Kryo Max Buffer size increased to 32G and 2000M respectively in sparknlp_jsl.start(secret) function &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.4.6 Overview We release Spark NLP for Healthcare 2.4.6 to fix some minor bugs. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes Updated IDF value calculation to be probabilistic based log[(N - df_t) / df_t + 1] as opposed to log[N / df_t] TFIDF cosine distance was being calculated with the rooted norms rather than with the original squared norms Validation of label cols is now performed at the beginning of EnsembleEntityResolver Environment Variable for License value named jsl.settings.license Now DocumentLogRegClassifier can be serialized from Python (bug introduced with the implementation of RecursivePipelines, LazyAnnotator attribute) &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.4.5 Overview We are glad to announce Spark NLP for Healthcare 2.4.5. As a new feature we are happy to introduce our new EnsembleEntityResolver which allows our Entity Resolution architecture to scale up in multiple orders of magnitude and handle datasets of millions of records on a sub-log computation increase We also enhanced our ChunkEntityResolverModel with 5 new distance calculations with weighting-array and aggregation-strategy params that results in more levers to finetune its performance against a given dataset. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features EnsembleEntityResolver consisting of an integrated TFIDF-Logreg classifier in the first layer + Multiple ChunkEntityResolvers in the second layer (one per each class) Five (5) new distances calculations for ChunkEntityResolver, namely: Token Based: TFIDF-Cosine, Jaccard, SorensenDice Character Based: JaroWinkler and Levenshtein Weight parameter that works as a multiplier for each distance result to be considered during their aggregation Three (3) aggregation strategies for the enabled distance in a particular instance, namely: AVERAGE, MAX and MIN &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements ChunkEntityResolver can now compute distances over all the neighbours found and return the metadata just for the best alternatives that meet the threshold; before it would calculate them over the neighbours and return them all in the metadata ChunkEntityResolver now has an extramassPenalty parameter to accoun for penalization of token-length difference in compared strings Metadata for the ChunkEntityResolver has been updated accordingly to reflect all new features StringDistances class has been included in utils to aid in the calculation and organization of different types of distances for Strings HasFeaturesJsl trait has been included to support the serialization of Features including [T] &lt;: AnnotatorModel[T] types &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes Frequency calculation for WMD in ChunkEntityResolver has been adjusted to account for real word count representation AnnotatorType for DocumentLogRegClassifier has been changed to CATEGORY to align with classifiers in Open Source library &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Deprecations Legacy EntityResolver{Approach, Model} classes have been deprecated in favor of ChunkEntityResolver classes ChunkEntityResolverSelector classes has been deprecated in favor of EnsembleEntityResolver &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.4.2 Overview We are glad to announce Spark NLP for Healthcare 2.4.2. As a new feature we are happy to introduce our new Disambiguation Annotator, which will let the users resolve different kind of entities based on Knowledge bases provided in the form of Records in a RocksDB database. We also enhanced / fixed DocumentLogRegClassifier, ChunkEntityResolverModel and ChunkEntityResolverSelector Annotators. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features Disambiguation Annotator (NerDisambiguator and NerDisambiguatorModel) which accepts annotator types CHUNK and SENTENCE_EMBEDDINGS and returns DISAMBIGUATION annotator type. This output annotation type includes all the matches in the result and their similarity scores in the metadata. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements ChunkEntityResolver Annotator now supports both EUCLIDEAN and COSINE distance for the KNN search and WMD calculation. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes Fixed a bug in DocumentLogRegClassifier Annotator to support its serialization to disk. Fixed a bug in ChunkEntityResolverSelector Annotator to group by both SENTENCE and CHUNK at the time of forwarding tokens and embeddings to the lazy annotators. Fixed a bug in ChunkEntityResolverModel in which the same exact embeddings was not included in the neighbours. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.4.1 Overview Introducing Spark NLP for Healthcare 2.4.1 after all the feedback we received in the form of issues and suggestions on our different communication channels. Even though 2.4.0 was very stable, version 2.4.1 is here to address minor bug fixes that we summarize in the following lines. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes Changing the license Spark property key to be “jsl” instead of “sparkjsl” as the latter generates inconsistencies Fix the alignment logic for tokens and chunks in the ChunkEntityResolverSelector because when tokens and chunks did not have the same begin-end indexes the resolution was not executed &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; 2.4.0 Overview We are glad to announce Spark NLP for Healthcare 2.4.0. This is an important release because of several refactorizations achieved in the core library, plus the introduction of several state of the art algorithms, new features and enhancements. We have included several architecture and performance improvements, that aim towards making the library more robust in terms of storage handling for Big Data. In the NLP aspect, we have introduced a ContextualParser, DocumentLogRegClassifier and a ChunkEntityResolverSelector. These last two Annotators also target performance time and memory consumption by lowering the order of computation and data loaded to memory in each step when designed following a hierarchical pattern. We have put a big effort on this one, so please enjoy and share your comments. Your words are always welcome through all our different channels. Thank you very much for your important doubts, bug reports and feedback; they are always welcome and much appreciated. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; New Features BigChunkEntityResolver Annotator: New experimental approach to reduce memory consumption at expense of disk IO. ContextualParser Annotator: New entity parser that works based on context parameters defined in a JSON file. ChunkEntityResolverSelector Annotator: New AnnotatorModel that takes advantage of the RecursivePipelineModel + LazyAnnotator pattern to annotate with different LazyAnnotators at runtime. DocumentLogregClassifier Annotator: New Annotator that provides a wrapped TFIDF Vectorizer + LogReg Classifier for TOKEN AnnotatorTypes (either at Document level or Chunk level) &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Enhancements normalizedColumn Param is no longer required in ChunkEntityResolver Annotator (defaults to the labelCol Param value). ChunkEntityResolverMetadata now has more data to infer whether the match is meaningful or not. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Bugfixes Fixed a bug on ContextSpellChecker Annotator where unrecognized tokens would cause an exception if not in vocabulary. Fixed a bug on ChunkEntityResolver Annotator where undetermined results were coming out of negligible confidence scores for matches. Fixed a bug on ChunkEntityResolver Annotator where search would fail if the neighbours Param was grater than the number of nodes in the tree. Now it returns up to the number of nodes in the tree. &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Deprecations OCR Moves to its own JSL Spark OCR project. &lt;/div&gt; Infrastructure Spark NLP License is now required to utilize the library. Please follow the instructions on the shared email.",
    "url": "/docs/en/licensed_release_notes",
    "relUrl": "/docs/en/licensed_release_notes"
  },
  "40": {
    "id": "40",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/middle_eastern_languages",
    "relUrl": "/middle_eastern_languages"
  },
  "41": {
    "id": "41",
    "title": "Models",
    "content": "Pretrained Models Pretrained Models moved to its own dedicated repository. Please follow this link for the updated list: https://github.com/JohnSnowLabs/spark-nlp-models How to use Pretrained Models Online You can follow this approach to use Spark NLP pretrained models: # load NER model trained by deep learning approach and GloVe word embeddings ner_dl = NerDLModel.pretrained(&#39;ner_dl&#39;) # load NER model trained by deep learning approach and BERT word embeddings ner_bert = NerDLModel.pretrained(&#39;ner_dl_bert&#39;) The default language is en, so for other laguages you should set the language: // load French POS tagger model trained by Universal Dependencies val french_pos = PerceptronModel.pretrained(&quot;pos_ud_gsd&quot;, lang=&quot;fr&quot;) // load Italain LemmatizerModel val italian_lemma = LemmatizerModel.pretrained(&quot;lemma_dxc&quot;, lang=&quot;it&quot;) Offline If you have any trouble using online pipelines or models in your environment (maybe it’s air-gapped), you can directly download them for offline use. After downloading offline models/pipelines and extracting them, here is how you can use them iside your code (the path could be a shared storage like HDFS in a cluster): Loading PerceptronModel annotator model inside Spark NLP Pipeline val french_pos = PerceptronModel.load(&quot;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) Public Models If you wish to use a pre-trained model for a specific annotator in your pipeline, you need to use the annotator which is mentioned under Model following with pretrained(name, lang) function. Example to load a pretraiand BERT model or NER model: bert = BertEmbeddings.pretrained(name=&#39;bert_base_cased&#39;, lang=&#39;en&#39;) ner_onto = NerDLModel.pretrained(name=&#39;ner_dl_bert&#39;, lang=&#39;en&#39;) NOTE: build means the model can be downloaded or loaded for that specific version or above. For instance, 2.4.0 can be used in all the releases after 2.4.x but not before. Dutch - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 nl Download PerceptronModel (POS UD) pos_ud_alpino 2.5.0 nl Download NerDLModel (glove_100d) wikiner_6B_100 2.5.0 nl Download NerDLModel (glove_6B_300) wikiner_6B_300 2.5.0 nl Download NerDLModel (glove_840B_300) wikiner_840B_300 2.5.0 nl Download English - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma_antbnc 2.0.2 en Download PerceptronModel (POS) pos_anc 2.0.2 en Download PerceptronModel (POS UD) pos_ud_ewt 2.2.2 en Download NerCrfModel (NER with GloVe) ner_crf 2.4.0 en Download NerDLModel (NER with GloVe) ner_dl 2.4.3 en Download NerDLModel (NER with BERT) ner_dl_bert 2.4.3 en Download NerDLModel (OntoNotes with GloVe 100d) onto_100 2.4.0 en Download NerDLModel (OntoNotes with GloVe 300d) onto_300 2.4.0 en Download SymmetricDeleteModel (Spell Checker) spellcheck_sd 2.0.2 en Download NorvigSweetingModel (Spell Checker) spellcheck_norvig 2.0.2 en Download ViveknSentimentModel (Sentiment) sentiment_vivekn 2.0.2 en Download DependencyParser (Dependency) dependency_conllu 2.0.8 en Download TypedDependencyParser (Dependency) dependency_typed_conllu 2.0.8 en Download Embeddings Model Name Build Lang Offline WordEmbeddings (GloVe) glove_100d 2.4.0 en Download BertEmbeddings bert_base_uncased 2.4.0 en Download BertEmbeddings bert_base_cased 2.4.0 en Download BertEmbeddings bert_large_uncased 2.4.0 en Download BertEmbeddings bert_large_cased 2.4.0 en Download ElmoEmbeddings elmo 2.4.0 en Download UniversalSentenceEncoder (USE) tfhub_use 2.4.0 en Download UniversalSentenceEncoder (USE) tfhub_use_lg 2.4.0 en Download AlbertEmbeddings albert_base_uncased 2.5.0 en Download AlbertEmbeddings albert_large_uncased 2.5.0 en Download AlbertEmbeddings albert_xlarge_uncased 2.5.0 en Download AlbertEmbeddings albert_xxlarge_uncased 2.5.0 en Download XlnetEmbeddings xlnet_base_cased 2.5.0 en Download XlnetEmbeddings xlnet_large_cased 2.5.0 en Download Classification Model Name Build Lang Offline ClassifierDL (with tfhub_use) classifierdl_use_trec6 2.5.0 en Download ClassifierDL (with tfhub_use) classifierdl_use_trec50 2.5.0 en Download SentimentDL (with tfhub_use) sentimentdl_use_imdb 2.5.0 en Download SentimentDL (with tfhub_use) sentimentdl_use_twitter 2.5.0 en Download SentimentDL (with glove_100d) sentimentdl_glove_imdb 2.5.0 en Download French - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.0.2 fr Download PerceptronModel (POS UD) pos_ud_gsd 2.0.2 fr Download NerDLModel (glove_840B_300) wikiner_840B_300 2.0.2 fr Download Feature Description   Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura   POS Trained by PerceptronApproach annotator on the Universal Dependencies   NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities   German - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.0.8 de Download PerceptronModel (POS UD) pos_ud_hdt 2.0.8 de Download NerDLModel (glove_840B_300) wikiner_840B_300 2.4.0 de Download Feature Description Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Italian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma_dxc 2.0.2 it Download ViveknSentimentAnalysis (Sentiment) sentiment_dxc 2.0.2 it Download PerceptronModel (POS UD) pos_ud_isdt 2.0.8 it Download NerDLModel (glove_840B_300) wikiner_840B_300 2.4.0 it Download Feature Description Lemma Trained by Lemmatizer annotator on DXC Technology dataset POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Norwegian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 nb Download PerceptronModel (POS UD) pos_ud_nynorsk 2.5.0 nn Download PerceptronModel (POS UD) pos_ud_bokmaal 2.5.0 nb Download NerDLModel (glove_100d) norne_6B_100 2.5.0 no Download NerDLModel (glove_6B_300) norne_6B_300 2.5.0 no Download NerDLModel (glove_840B_300) norne_840B_300 2.5.0 no Download Polish - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 pl Download PerceptronModel (POS UD) pos_ud_lfg 2.5.0 pl Download NerDLModel (glove_100d) wikiner_6B_100 2.5.0 pl Download NerDLModel (glove_6B_300) wikiner_6B_300 2.5.0 pl Download NerDLModel (glove_840B_300) wikiner_840B_300 2.5.0 pl Download Portuguese - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 pt Download PerceptronModel (POS UD) pos_ud_bosque 2.5.0 pt Download NerDLModel (glove_100d) wikiner_6B_100 2.5.0 pt Download NerDLModel (glove_6B_300) wikiner_6B_300 2.5.0 pt Download NerDLModel (glove_840B_300) wikiner_840B_300 2.5.0 pt Download Russian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.4.4 ru Download PerceptronModel (POS UD) pos_ud_gsd 2.4.4 ru Download NerDLModel (glove_100d) wikiner_6B_100 2.4.4 ru Download NerDLModel (glove_6B_300) wikiner_6B_300 2.4.4 ru Download NerDLModel (glove_840B_300) wikiner_840B_300 2.4.4 ru Download Feature Description Lemma Trained by Lemmatizer annotator on the Universal Dependencies POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Spanish - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.4.0 es Download PerceptronModel (POS UD) pos_ud_gsd 2.4.0 es Download NerDLModel (glove_100d) wikiner_6B_100 2.4.0 es Download NerDLModel (glove_6B_300) wikiner_6B_300 2.4.0 es Download NerDLModel (glove_840B_300) wikiner_840B_300 2.4.0 es Download Feature Description Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Bulgarian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 bg Download PerceptronModel (POS UD) pos_ud_btb 2.5.0 bg Download Czech - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 cs Download PerceptronModel (POS UD) pos_ud_pdt 2.5.0 cs Download Greek - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 el Download PerceptronModel (POS UD) pos_ud_gdt 2.5.0 el Download Finnish - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 fi Download PerceptronModel (POS UD) pos_ud_tdt 2.5.0 fi Download Hungarian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 hu Download PerceptronModel (POS UD) pos_ud_szeged 2.5.0 hu Download Romanian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 ro Download PerceptronModel (POS UD) pos_ud_rrt 2.5.0 ro Download Slovak - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 sk Download PerceptronModel (POS UD) pos_ud_snk 2.5.0 sk Download Swedish - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 sv Download PerceptronModel (POS UD) pos_ud_tal 2.5.0 sv Download Turkish - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 tr Download PerceptronModel (POS UD) pos_ud_imst 2.5.0 tr Download Ukrainian - Models Model Name Build Lang Offline LemmatizerModel (Lemmatizer) lemma 2.5.0 uk Download PerceptronModel (POS UD) pos_ud_iu 2.5.0 uk Download Multi-language Model Name Build Lang Offline WordEmbeddings (GloVe) glove_840B_300 2.4.0 xx Download WordEmbeddings (GloVe) glove_6B_300 2.4.0 xx Download BertEmbeddings (multi_cased) bert_multi_cased 2.4.0 xx Download LanguageDetectorDL ld_wiki_7 2.5.2 xx Download LanguageDetectorDL ld_wiki_20 2.5.2 xx Download The model with 7 languages: Czech, German, English, Spanish, French, Italy, and Slovak The model with 20 languages: Bulgarian, Czech, German, Greek, English, Spanish, Finnish, French, Croatian, Hungarian, Italy, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Swedish, Turkish, and Ukrainian Please follow this link for the updated list: https://github.com/JohnSnowLabs/spark-nlp-models",
    "url": "/docs/en/models",
    "relUrl": "/docs/en/models"
  },
  "42": {
    "id": "42",
    "title": "Available Models and Pipelines",
    "content": "",
    "url": "/models",
    "relUrl": "/models"
  },
  "43": {
    "id": "43",
    "title": "Spark OCR",
    "content": "Spark OCR is another commercial extension of Spark NLP for optical character recognition from images, scanned PDF documents, Microsoft DOCX and DICOM files. If you want to try it out on your own documents click on the below button: Try Free Spark OCR is built on top of Apache Spark and offers the following capabilities: Image pre-processing algorithms to improve text recognition results: Adaptive thresholding &amp; denoising Skew detection &amp; correction Adaptive scaling Layout Analysis &amp; region detection Image cropping Removing background objects Text recognition, by combining NLP and OCR pipelines: Extracting text from images (optical character recognition) Support English, German, French, Spanish, Russian and Vietnamese languages Extracting data from tables Recognizing and highlighting named entities in PDF documents Masking sensitive text in order to de-identify images Table detection and recognition from images Visual document understanding Document classification Visual NER Output generation in different formats: PDF, images, or DICOM files with annotated or masked entities Digital text for downstream processing in Spark NLP or other libraries Structured data formats (JSON and CSV), as files or Spark data frames Scale out: distribute the OCR jobs across multiple nodes in a Spark cluster. Frictionless unification of OCR, NLP, ML &amp; DL pipelines. Spark OCR Workshop If you prefer learning by example, click the button below to checkout the workshop repository full of fresh examples. Spark OCR Workshop Below, you can follow a more theoretical and thorough quick start guide. Quickstart Examples Images The following code example creates an OCR Pipeline for processing image(s). The image file(s) can contain complex layout like columns, tables, images inside. PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers._ val imagePath = &quot;path to image files&quot; // Read image files as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) // Transform binary content to image val binaryToImage = new BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) // OCR val ocr = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) // Define Pipeline val pipeline = new Pipeline() pipeline.setStages(Array( binaryToImage, ocr )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = modelPipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image files&quot; # Read image files as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) # Transform binary content to image binaryToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # OCR ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) # Define Pipeline pipeline = PipelineModel(stages=[ binaryToImage, ocr ]) data = pipeline.transform(df) data.show() Scanned PDF files Next sample provides an example of OCR Pipeline for processing PDF files containing image data. In this case, the PdfToImage transformer is used to convert PDF file to a set of images. PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers._ val imagePath = &quot;path to pdf files&quot; // Read pdf files as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) // Transform PDF file to the image val pdfToImage = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) // OCR val ocr = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( pdfToImage, ocr )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = modelPipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to pdf files&quot; # Read pdf files as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) # Transform PDF file to the image pdfToImage = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # OCR ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) # Define pipeline pipeline = PipelineModel(stages=[ pdfToImage, ocr ]) data = pipeline.transform(df) data.show() PDF files (scanned or text) In the following code example we will create OCR Pipeline for processing PDF files that contain text or image data. For each PDF file, this pipeline will: extract the text from document and save it to the text column if text contains less than 10 characters (so the document isn’t PDF with text layout) it will process the PDF file as a scanned document: convert PDF file to an image detect and split image to regions run OCR and save output to the text column PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers._ val imagePath = &quot;path to PDF files&quot; // Read PDF files as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) // Extract text from PDF text layout val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) // In case of `text` column contains less then 10 characters, // pipeline run PdfToImage as fallback method val pdfToImage = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setFallBackCol(&quot;text&quot;) .setMinSizeBeforeFallback(10) // OCR val ocr = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( pdfToText, pdfToImage, ocr )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = modelPipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to PDF files&quot; # Read PDF files as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) # Extract text from PDF text layout pdfToText = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) # In case of `text` column contains less then 10 characters, # pipeline run PdfToImage as fallback method pdfToImage = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setFallBackCol(&quot;text&quot;) .setMinSizeBeforeFallback(10) # OCR ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) # Define pipeline pipeline = PipelineModel(stages=[ pdfToText, pdfToImage, ocr, ]) data = pipeline.transform(df) data.show() Images (streaming mode) Next code segments provide an example of streaming OCR pipeline. It processes images and stores results to memory table. PythonScala val imagePath = &quot;path folder with images&quot; val batchDataFrame = spark.read.format(&quot;binaryFile&quot;).load(imagePath).limit(1) val pipeline = new Pipeline() pipeline.setStages(Array( binaryToImage, binarizer, ocr )) val modelPipeline = pipeline.fit(batchDataFrame) // Read files in streaming mode val dataFrame = spark.readStream .format(&quot;binaryFile&quot;) .schema(batchDataFrame.schema) .load(imagePath) // Call pipeline and store results to &#39;results&#39; memory table val query = modelPipeline.transform(dataFrame) .select(&quot;text&quot;, &quot;exception&quot;) .writeStream .format(&quot;memory&quot;) .queryName(&quot;results&quot;) .start() imagePath = &quot;path folder with images&quot; batchDataFrame = spark.read.format(&quot;binaryFile&quot;).load(imagePath).limit(1) pipeline = Pipeline() pipeline.setStages(Array( binaryToImage, binarizer, ocr )) modelPipeline = pipeline.fit(batchDataFrame) # Read files in streaming mode dataFrame = spark.readStream .format(&quot;binaryFile&quot;) .schema(batchDataFrame.schema) .load(imagePath) # Call pipeline and store results to &#39;results&#39; memory table query = modelPipeline.transform(dataFrame) .select(&quot;text&quot;, &quot;exception&quot;) .writeStream() .format(&quot;memory&quot;) .queryName(&quot;results&quot;) .start() For getting results from memory table following code could be used: PythonScala spark.table(&quot;results&quot;).select(&quot;path&quot;, &quot;text&quot;).show() spark.table(&quot;results&quot;).select(&quot;path&quot;, &quot;text&quot;).show() More details about Spark Structured Streaming could be found in spark documentation. Advanced Topics Error Handling Pipeline execution would not be interrupted in case of the runtime exceptions while processing some records. In this case OCR transformers would fill exception column that contains transformer name and exception. NOTE: Storing runtime errors to the exception field allows to process batch of files. Output Here is an output with exception when try to process js file using OCR pipeline: PythonScala result.select(&quot;path&quot;, &quot;text&quot;, &quot;exception&quot;).show(2, false) result.select(&quot;path&quot;, &quot;text&quot;, &quot;exception&quot;).show(2, False) +-+-+--+ |path |text |exception | +-+-+--+ |file:jquery-1.12.3.js | |BinaryToImage_c0311dc62161: Can&#39;t open file as image.| |file:image.png |I prefer the morning flight through Denver |null | +-+-+--+ Performance In case of big count of text PDF’s in dataset need have manual partitioning for avoid skew in partitions and effective utilize resources. For example the randomization could be used.",
    "url": "/docs/en/ocr",
    "relUrl": "/docs/en/ocr"
  },
  "44": {
    "id": "44",
    "title": "Installation",
    "content": "Spark OCR is built on top of Apache Spark. Currently, it supports 3.0., 3.1., 2.4.* and 2.3.* versions of Spark. It is recommended to have basic knowledge of the framework and a working environment before using Spark OCR. Refer to Spark documentation to get started with Spark. Spark OCR required: Scala 2.11 or 2.12 related to the Spark version Python 3.+ (in case using PySpark) Before you start, make sure that you have: Spark OCR jar file (or secret for download it) Spark OCR python wheel file License key If you don’t have a valid subscription yet and you want to test out the Spark OCR library press the button below: Try Free Spark OCR from Scala You can start a spark REPL with Scala by running in your terminal a spark-shell including the com.johnsnowlabs.nlp:spark-ocr_2.11:1.0.0 package: spark-shell --jars #### The #### is a secret url only avaliable for license users. If you have purchansed a license but did not receive it please contact us at info@johnsnowlabs.com. Start Spark OCR Session The following code will initialize the spark session in case you have run the jupyter notebook directly. If you have started the notebook using pyspark this cell is just ignored. Initializing the spark session takes some seconds (usually less than 1 minute) as the jar from the server needs to be loaded. The #### in .config(“spark.jars”, “####”) is a secret code, if you have not received it please contact us at info@johnsnowlabs.com. import org.apache.spark.sql.SparkSession val spark = SparkSession .builder() .appName(&quot;Spark OCR&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;4G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;2G&quot;) .config(&quot;spark.jars&quot;, &quot;####&quot;) .getOrCreate() Spark OCR from Python Install Python package Install python package using pip: pip install spark-ocr==1.8.0.spark24 --extra-index-url #### --ignore-installed The #### is a secret url only avaliable for license users. If you have purchansed a license but did not receive it please contact us at info@johnsnowlabs.com. Start Spark OCR Session Manually from pyspark.sql import SparkSession spark = SparkSession .builder .appName(&quot;Spark OCR&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;4G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;2G&quot;) .config(&quot;spark.jars&quot;, &quot;https://pypi.johnsnowlabs.com/####&quot;) .getOrCreate() Using Start function Another way to initialize SparkSession with Spark OCR to use start function in Python. Start function has following params: Param name Type Default Description secret string None Secret for download Spark OCR jar file jar_path string None Path to jar file in case you need to run spark session offline extra_conf SparkConf None Extra spark configuration master_url string local[*] Spark master url nlp_version string None Spark NLP version for add it Jar to session nlp_internal boolean/string None Run Spark session with Spark NLP Internal if set to ‘True’ or specify version nlp_secret string None Secret for get Spark NLP Internal jar keys_file string keys.json Name of the json file with license, secret and aws keys For start Spark session with Spark NLP please specify version of it in nlp_version param. Example: from sparkocr import start spark = start(secret=secret, nlp_version=&quot;2.4.4&quot;) Databricks The installation process to Databricks includes following steps: Installing Spark OCR library to Databricks and attaching it to the cluster Same step for Spark OCR python wheel file Adding license key Adding cluster init script for install dependencies Please look databricks python helpers for simplify install init script. Example notebooks: Spark OCR Databricks python notebooks Spark OCR Databricks Scala notebooks",
    "url": "/docs/en/ocr_install",
    "relUrl": "/docs/en/ocr_install"
  },
  "45": {
    "id": "45",
    "title": "Spark OCR 2.3.x (Licensed)",
    "content": "Spark NLP comes with an OCR module that can read both PDF files and scanned images (requires Tesseract 4.x+). Installation Installing Tesseract As mentioned above, if you are dealing with scanned images instead of test-selectable PDF files you need to install tesseract 4.x+ on all the nodes in your cluster. Here how you can install it on Ubuntu/Debian: apt-get install tesseract-ocr In Databricks this command may result in installing tesseract 3.x instead of version 4.x. You can simply run this init script to install tesseract 4.x in your Databricks cluster: #!/bin/bash sudo apt-get install -y g++ # or clang++ (presumably) sudo apt-get install -y autoconf automake libtool sudo apt-get install -y pkg-config sudo apt-get install -y libpng-dev sudo apt-get install -y libjpeg8-dev sudo apt-get install -y libtiff5-dev sudo apt-get install -y zlib1g-dev ​ wget http://www.leptonica.org/source/leptonica-1.74.4.tar.gz tar xvf leptonica-1.74.4.tar.gz cd leptonica-1.74.4 ./configure make sudo make install ​ git clone --single-branch --branch 4.1 https://github.com/tesseract-ocr/tesseract.git cd tesseract ./autogen.sh ./configure make sudo make install sudo ldconfig ​ tesseract -v Quick start Let’s read a PDF file: import com.johnsnowlabs.nlp._ val ocrHelper = new OcrHelper() //If you do this locally you can use file:/// or hdfs:/// if the files are hosted in Hadoop val dataset = ocrHelper.createDataset(spark, &quot;/tmp/sample_article.pdf&quot;) If you are trying to extract text from scanned images in the format of PDF, please keep in mind to use these configs: ocrHelper.setPreferredMethod(&quot;image&quot;) ocrHelper.setFallbackMethod(false) ocrHelper.setMinSizeBeforeFallback(0) Configuration setPreferredMethod(text/image = text) either text or image will work. Defaults to text. Text mode works better and faster for digital or text scanned PDFs setFallbackMethod(boolean) on true, when text or image fail, it will fallback to the alternate method setMinSizeBeforeFallback(int = 1) number of characters to have at a minimum, before falling back. setPageSegMode(int = 3) image mode page segmentation mode setEngineMode(int = 1) image mode engine mode setPageIteratorLevel(int = 0) image mode page iteratior level setScalingFactor(float) Specifies the scaling factor to apply to images, in both axes, before OCR. It can scale up the image(factor &gt; 1.0) or scale it down(factor &lt; 1.0) setSplitPages(boolean = true) Whether to split pages into different rows and documents setSplitRegions(boolean = true) Whether to split by document regions. Works only in image mode. Enables split pages as well. setIncludeConfidence(boolean = false) setAutomaticSkewCorrection(use: boolean, half_angle: double = 5.0, resolution: double = 1.0) setAutomaticSizeCorrection(use: boolean, desired_size: int = 34) setEstimateNoise(string) image mode estimator noise level useErosion(use: boolean, kernel_size: int = 2, kernel_shape: Int = 0) image mode erosion Utilizing Spark NLP OCR Module Spark NLP OCR Module is not included within Spark NLP. It is not an annotator and not an extension to Spark ML. You can use OcrHelper to directly create spark dataframes from PDF. This will hold entire documents in single rows, meant to be later processed by a SentenceDetector. This way, you won’t be breaking the content in rows as if you were reading a standard document. Metadata columns are added automatically and will include page numbers, file name and other useful information per row. Python code from pyspark.sql import SparkSession from sparknlp.ocr import OcrHelper from sparknlp import DocumentAssembler data = OcrHelper().createDataset(spark = spark, input_path = &quot;/your/example.pdf&quot; ) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;) annotations = documentAssembler.transform(data) annotations.columns [&#39;text&#39;, &#39;pagenum&#39;, &#39;method&#39;, &#39;noiselevel&#39;, &#39;confidence&#39;, &#39;positions&#39;, &#39;filename&#39;, &#39;document&#39;] Scala code import com.johnsnowlabs.nlp.util.io.OcrHelper import com.johnsnowlabs.nlp.DocumentAssembler val myOcrHelper = new OcrHelper val data = myOcrHelper.createDataset(spark, &quot;/your/example.pdf&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;) val annotations = documentAssembler.transform(data) annotations.columns Array[String] = Array(text, pagenum, method, noiselevel, confidence, positions, filename, document) … where the text column of the annotations spark dataframe includes the text content of the PDF, pagenum the page number, etc… Creating an Array of Strings from PDF (For LightPipeline) Another way, would be to simply create an array of strings. This is useful for example if you are parsing a small amount of pdf files and would like to use LightPipelines instead. See an example below. Scala code import com.johnsnowlabs.nlp.util.io.OcrHelper import com.johnsnowlabs.nlp.{DocumentAssembler,LightPipeline} import com.johnsnowlabs.nlp.annotator.SentenceDetector import org.apache.spark.ml.Pipeline val myOcrHelper = new OcrHelper val raw = myOcrHelper.createMap(&quot;/pdfs/&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val lightPipeline = new LightPipeline(new Pipeline().setStages(Array(documentAssembler, sentenceDetector)).fit(Seq.empty[String].toDF(&quot;text&quot;))) val annotations = ligthPipeline.annotate(raw.values.toArray) Now to get the whole first PDF content in your /pdfs/ folder you can use: annotations(0)(&quot;document&quot;)(0) and to get the third sentence found in that first pdf: annotations(0)(&quot;sentence&quot;)(2) To get from the fifth pdf the second sentence: annotations(4)(&quot;sentence&quot;)(1) Similarly, the whole content of the fifth pdf can be retrieved by: annotations(4)(&quot;document&quot;)(0)",
    "url": "/docs/en/ocr_old",
    "relUrl": "/docs/en/ocr_old"
  },
  "46": {
    "id": "46",
    "title": "Pipeline components",
    "content": "PDF processing Next section describes the transformers that deal with PDF files with the purpose of extracting text and image data from PDF files. PdfToText PDFToText extracts text from selectable PDF (with text layout). Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PDF document originCol string path path to the original file Parameters Param name Type Default Description splitPage bool true whether it needed to split document to pages textStripper   TextStripperType.PDF_TEXT_STRIPPER   sort bool false Sort text during extraction with TextStripperType.PDF_LAYOUT_STRIPPER partitionNum int 0 Force repartition dataframe if set to value more than 0. onlyPageNum bool false Extract only page numbers. extractCoordinates bool false Extract coordinates and store to the positions column storeSplittedPdf bool false Store one page pdf’s for process it using PdfToImage. Output Columns Param name Type Default Column Data Description outputCol string text extracted text pageNumCol string pagenum page number or 0 when splitPage = false NOTE: For setting parameters use setParamName method. Example PythonScala import com.johnsnowlabs.ocr.transformers.PdfToText val pdfPath = &quot;path to pdf with text layout&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val transformer = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;pagenum&quot;) .setSplitPage(true) val data = transformer.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() from sparkocr.transformers import * pdfPath = &quot;path to pdf with text layout&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) transformer = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;pagenum&quot;) .setSplitPage(true) data = transformer.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() Output: +-+-+ |pagenum|text | +-+-+ |0 |This is a page. | |1 |This is another page. | |2 |Yet another page. | +-+-+ PdfToImage PdfToImage renders PDF to an image. To be used with scanned PDF documents. Output dataframe contains total_pages field with total number of pages. For process pdf with big number of pages prefer to split pdf by setting splitNumBatch param. Number of partitions should be equal number of cores/executors. Input Columns Param name Type Default Column Data Description inputCol string content binary representation of the PDF document originCol string path path to the original file fallBackCol string text extracted text from previous method for detect if need to run transformer as fallBack Parameters Param name Type Default Description splitPage bool true whether it needed to split document to pages minSizeBeforeFallback int 10 minimal count of characters to extract to decide, that the document is the PDF with text layout imageType ImageType ImageType.TYPE_BYTE_GRAY type of the image resolution int 300 Output image resolution in dpi keepInput boolean false Keep input column in dataframe. By default it is dropping. partitionNum int 0 Number of Spark RDD partitions (0 value - without repartition) binarization boolean false Enable/Disable binarization image after extract image. binarizationParams Array[String] null Array of Binarization params in key=value format. splitNumBatch int 0 Number of partitions or size of partitions, related to the splitting strategy. partitionNumAfterSplit int 0 Number of Spark RDD partitions after splitting pdf document (0 value - without repartition). splittingStategy SplittingStrategy SplittingStrategy.FIXED_SIZE_OF_PARTITION Splitting strategy. Output Columns Param name Type Default Column Data Description outputCol string image extracted image struct (Image schema) pageNumCol string pagenum page number or 0 when splitPage = false Example: PythonScala import com.johnsnowlabs.ocr.transformers.PdfToImage val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToImage = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;pagenum&quot;) .setSplitPage(true) val data = pdfToImage.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() from sparkocr.transformers import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdfToImage = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;pagenum&quot;) .setSplitPage(true) data = pdfToImage.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() ImageToPdf ImageToPdf transform image to Pdf document. If dataframe contains few records for same origin path, it groups image by origin column and create multipage PDF document. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string content binary representation of the PDF document Example: Read images and store it as single page PDF documents. PythonScala import com.johnsnowlabs.ocr.transformers._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(imagePath) // Define transformer for convert to Image struct val binaryToImage = new BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) // Define transformer for store to PDF val imageToPdf = new ImageToPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;content&quot;) // Call transformers val image_df = binaryToImage.transform(df) val pdf_df = pdfToImage.transform(image_df) pdf_df.select(&quot;content&quot;).show() from sparkocr.transformers import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) # Define transformer for convert to Image struct binaryToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for store to PDF imageToPdf = ImageToPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;content&quot;) # Call transformers image_df = binaryToImage.transform(df) pdf_df = pdfToImage.transform(image_df) pdf_df.select(&quot;content&quot;).show() TextToPdf TextToPdf renders ocr results to PDF document as text layout. Each symbol will render to same position with same font size as in original image or PDF. If dataframe contains few records for same origin path, it groups image by origin column and create multipage PDF document. Input Columns Param name Type Default Column Data Description inputCol string positions column with positions struct inputImage string image image struct (Image schema) inputText string text column name with recognized text originCol string path path to the original file inputContent string content column name with binary representation of original PDF file Output Columns Param name Type Default Column Data Description outputCol string pdf binary representation of the PDF document Example: Read PDF document, run OCR and render results to PDF document. PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers._ val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToImage = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image_raw&quot;) .setResolution(400) val binarizer = new ImageBinarizer() .setInputCol(&quot;image_raw&quot;) .setOutputCol(&quot;image&quot;) .setThreshold(130) val ocr = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setIgnoreResolution(false) .setPageSegMode(PageSegmentationMode.SPARSE_TEXT) .setConfidenceThreshold(60) val textToPdf = new TextToPdf() .setInputCol(&quot;positions&quot;) .setInputImage(&quot;image&quot;) .setOutputCol(&quot;pdf&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( pdfToImage, binarizer, ocr, textToPdf )) val modelPipeline = pipeline.fit(df) val pdf = modelPipeline.transform(df) val pdfContent = pdf.select(&quot;pdf&quot;).collect().head.getAs[Array[Byte]](0) // store to file val tmpFile = Files.createTempFile(suffix=&quot;.pdf&quot;).toAbsolutePath.toString val fos = new FileOutputStream(tmpFile) fos.write(pdfContent) fos.close() println(tmpFile) from sparkocr.transformers import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_image = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image_raw&quot;) binarizer = ImageBinarizer() .setInputCol(&quot;image_raw&quot;) .setOutputCol(&quot;image&quot;) .setThreshold(130) ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setIgnoreResolution(False) .setPageSegMode(PageSegmentationMode.SPARSE_TEXT) .setConfidenceThreshold(60) textToPdf = TextToPdf() .setInputCol(&quot;positions&quot;) .setInputImage(&quot;image&quot;) .setOutputCol(&quot;pdf&quot;) pipeline = PipelineModel(stages=[ pdf_to_image, binarizer, ocr, textToPdf ]) result = pipeline.transform(df).collect() # Store to file for debug with open(&quot;test.pdf&quot;, &quot;wb&quot;) as file: file.write(result[0].pdf) PdfDrawRegions PdfDrawRegions transformer for drawing regions to Pdf document. Input Columns Param name Type Default Column Data Description inputCol string content binary representation of the PDF document originCol string path path to the original file inputRegionsCol string region input column which contain regions Parameters Param name Type Default Description lineWidth integer 1 line width for draw regions Output Columns Param name Type Default Column Data Description outputCol string pdf_regions binary representation of the PDF document Example: PythonScala import java.io.FileOutputStream import java.nio.file.Files import com.johnsnowlabs.ocr.transformers._ import com.johnsnowlabs.nlp.{DocumentAssembler, SparkAccessor} import com.johnsnowlabs.nlp.annotators._ import com.johnsnowlabs.nlp.util.io.ReadAs val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) val positionFinder = new PositionFinder() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;coordinates&quot;) .setPageMatrixCol(&quot;positions&quot;) .setMatchingWindow(10) .setPadding(2) val pdfDrawRegions = new PdfDrawRegions() .setInputRegionsCol(&quot;coordinates&quot;) // Create pipeline val pipeline = new Pipeline() .setStages(Array( pdfToText, documentAssembler, sentenceDetector, tokenizer, entityExtractor, positionFinder, pdfDrawRegions )) val pdfWithRegions = pipeline.fit(df).transform(df) val pdfContent = pdfWithRegions.select(&quot;pdf_regions&quot;).collect().head.getAs[Array[Byte]](0) // store to pdf to tmp file val tmpFile = Files.createTempFile(&quot;with_regions_&quot;, s&quot;.pdf&quot;).toAbsolutePath.toString val fos = new FileOutputStream(tmpFile) fos.write(pdfContent) fos.close() println(tmpFile) from pyspark.ml import Pipeline from sparkocr.transformers import * from sparknlp.annotator import * from sparknlp.base import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;page&quot;) .setSplitPage(False) document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) entity_extractor = TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;./sparkocr/resources/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) position_finder = PositionFinder() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;coordinates&quot;) .setPageMatrixCol(&quot;positions&quot;) .setMatchingWindow(10) .setPadding(2) draw = PdfDrawRegions() .setInputRegionsCol(&quot;coordinates&quot;) .setOutputCol(&quot;pdf_with_regions&quot;) .setInputCol(&quot;content&quot;) .setLineWidth(1) pipeline = Pipeline(stages=[ pdf_to_text, document_assembler, sentence_detector, tokenizer, entity_extractor, position_finder, draw ]) pdfWithRegions = pipeline.fit(df).transform(df) pdfContent = pdfWithRegions.select(&quot;pdf_regions&quot;).collect().head.getAs[Array[Byte]](0) # store to pdf to tmp file with open(&quot;test.pdf&quot;, &quot;wb&quot;) as file: file.write(pdfContent[0].pdf_regions) Results: PdfToTextTable Extract tables from Pdf document page. Input is a column with binary representation of PDF document. As output generate column with tables and tables text chunks coordinates (rows/cols). Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PDF document originCol string path path to the original file Parameters Param name Type Default Description pageIndex integer -1 Page index to extract Tables. guess bool false A logical indicating whether to guess the locations of tables on each page. method string decide Identifying the prefered method of table extraction: basic, spreadsheet. Output Columns Param name Type Default Column Data Description outputCol TableContainer tables Extracted tables Example: PythonScala import java.io.FileOutputStream import java.nio.file.Files import com.johnsnowlabs.ocr.transformers._ import com.johnsnowlabs.nlp.{DocumentAssembler, SparkAccessor} import com.johnsnowlabs.nlp.annotators._ import com.johnsnowlabs.nlp.util.io.ReadAs val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToTextTable = new PdfToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;table&quot;) .pdf_to_text_table.setPageIndex(1) .pdf_to_text_table.setMethod(&quot;basic&quot;) table = pdfToTextTable.transform(df) // Show first row table.select(table[&quot;table.chunks&quot;].getItem(1)[&quot;chunkText&quot;]).show(1, False) from pyspark.ml import Pipeline from sparkocr.transformers import * from sparknlp.annotator import * from sparknlp.base import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text_table = PdfToTextTable() pdf_to_text_table.setInputCol(&quot;content&quot;) pdf_to_text_table.setOutputCol(&quot;table&quot;) pdf_to_text_table.setPageIndex(1) pdf_to_text_table.setMethod(&quot;basic&quot;) table = pdf_to_text_table.transform(df) # Show first row table.select(table[&quot;table.chunks&quot;].getItem(1)[&quot;chunkText&quot;]).show(1, False) Output: ++ |table.chunks AS chunks#760[1].chunkText | ++ |[Mazda RX4, 21.0, 6, , 160.0, 110, 3.90, 2.620, 16.46, 0, 1, 4, 4]| ++ DOCX processing Next section describes the transformers that deal with DOCX files with the purpose of extracting text and table data from it. DocToText DocToText extracts text from the DOCX document. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the DOCX document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string text extracted text pageNumCol string pagenum for compatibility with another transformers NOTE: For setting parameters use setParamName method. Example PythonScala import com.johnsnowlabs.ocr.transformers.DocToText val docPath = &quot;path to docx with text layout&quot; // Read DOCX file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new DocToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) val data = transformer.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() from sparkocr.transformers import * docPath = &quot;path to docx with text layout&quot; # Read DOCX file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = DocToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) data = transformer.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() DocToTextTable DocToTextTable extracts table data from the DOCX documents. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PDF document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol TableContainer tables Extracted tables NOTE: For setting parameters use setParamName method. Example PythonScala import com.johnsnowlabs.ocr.transformers.DocToTextTable val docPath = &quot;path to docx with text layout&quot; // Read DOCX file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new DocToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;tables&quot;) val data = transformer.transform(df) data.select(&quot;tables&quot;).show() from sparkocr.transformers import * docPath = &quot;path to docx with text layout&quot; # Read DOCX file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = DocToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;tables&quot;) data = transformer.transform(df) data.select(&quot;tables&quot;).show() DocToPdf DocToPdf convert DOCX document to PDF document. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the DOCX document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string text binary representation of the PDF document NOTE: For setting parameters use setParamName method. Example PythonScala import com.johnsnowlabs.ocr.transformers.DocToPdf val docPath = &quot;path to docx with text layout&quot; // Read DOCX file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new DocToPdf() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;pdf&quot;) val data = transformer.transform(df) data.select(&quot;pdf&quot;).show() from sparkocr.transformers import * docPath = &quot;path to docx with text layout&quot; # Read DOCX file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = DocToPdf() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;pdf&quot;) data = transformer.transform(df) data.select(&quot;pdf&quot;).show() Dicom processing DicomToImage DicomToImage transforms dicom object (loaded as binary file) to image struct. Input Columns Param name Type Default Column Data Description inputCol string content binary dicom object originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string image extracted image struct (Image schema) pageNumCol integer pagenum page (image) number begin from 0 metadataCol string metadata Output column name for dicom metatdata ( json formatted ) Scala example: PythonScala import com.johnsnowlabs.ocr.transformers.DicomToImage val dicomPath = &quot;path to dicom files&quot; // Read dicom file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(dicomPath) val dicomToImage = new DicomToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setMetadataCol(&quot;meta&quot;) val data = dicomToImage.transform(df) data.select(&quot;image&quot;, &quot;pagenum&quot;, &quot;meta&quot;).show() from sparkocr.transformers import * dicomPath = &quot;path to dicom files&quot; # Read dicom file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(dicomPath) dicomToImage = DicomToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setMetadataCol(&quot;meta&quot;) data = dicomToImage.transform(df) data.select(&quot;image&quot;, &quot;pagenum&quot;, &quot;meta&quot;).show() ImageToDicom ImageToDicom transforms image to Dicom document. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) originCol string path path to the original file metadataCol string metadata dicom metatdata ( json formatted ) Output Columns Param name Type Default Column Data Description outputCol string dicom binary dicom object Scala example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageToDicom val imagePath = &quot;path to image file&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToDicom = new ImageToDicom() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;dicom&quot;) val data = imageToDicom.transform(df) data.select(&quot;dicom&quot;).show() from sparkocr.transformers import * imagePath = &quot;path to image file&quot; # Read image file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(imagePath) binaryToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) image_df = binaryToImage.transform(df) imageToDicom = ImageToDicom() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;dicom&quot;) data = imageToDicom.transform(image_df) data.select(&quot;dicom&quot;).show() Image pre-processing Next section describes the transformers for image pre-processing: scaling, binarization, skew correction, etc. BinaryToImage BinaryToImage transforms image (loaded as binary file) to image struct. Input Columns Param name Type Default Column Data Description inputCol string content binary representation of the image originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string image extracted image struct (Image schema) Scala example: PythonScala import com.johnsnowlabs.ocr.transformers.BinaryToImage val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(imagePath) val binaryToImage = new BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) val data = binaryToImage.transform(df) data.select(&quot;image&quot;).show() from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(imagePath) binaryToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) data = binaryToImage.transform(df) data.select(&quot;image&quot;).show() GPUImageTransformer GPUImageTransformer allows to run image pre-processing operations on GPU. It supports following operations: Scaling Otsu thresholding Huang thresholding Erosion Dilation GPUImageTransformer allows to add few operations. For add operations need to call one of the methods with params: Method name Params Description addScalingTransform factor Scale image by scaling factor. addOtsuTransform   The automatic thresholder utilizes the Otsu threshold method. addHuangTransform   The automatic thresholder utilizes the Huang threshold method. addDilateTransform width, height Computes the local maximum of a pixels rectangular neighborhood. The rectangles size is specified by its half-width and half-height. addErodeTransform width, height Computes the local minimum of a pixels rectangular neighborhood. The rectangles size is specified by its half-width and half-height Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description imageType ImageType ImageType.TYPE_BYTE_BINARY Type of the output image gpuName string ”” GPU device name. Output Columns Param name Type Default Column Data Description outputCol string transformed_image image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.GPUImageTransformer import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new GPUImageTransformer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;transformed_image&quot;) .addHuangTransform() .addScalingTransform(3) .addDilateTransform(2, 2) .setImageType(ImageType.TYPE_BYTE_BINARY) val data = transformer.transform(df) data.storeImage(&quot;transformed_image&quot;) from sparkocr.transformers import * from sparkocr.enums import ImageType from sparkocr.utils import display_images imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) transformer = GPUImageTransformer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;transformed_image&quot;) .addHuangTransform() .addScalingTransform(3) .addDilateTransform(2, 2) .setImageType(ImageType.TYPE_BYTE_BINARY) pipeline = PipelineModel(stages=[ binary_to_image, transformer ]) result = pipeline.transform(df) display_images(result, &quot;transformed_image&quot;) ImageBinarizer ImageBinarizer transforms image to binary color schema by threshold. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description threshold int 170   Output Columns Param name Type Default Column Data Description outputCol string binarized_image image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageBinarizer import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val binirizer = new ImageBinarizer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;binary_image&quot;) .setThreshold(100) val data = binirizer.transform(df) data.storeImage(&quot;binary_image&quot;) from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) binirizer = ImageBinarizer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;binary_image&quot;) .setThreshold(100) data = binirizer.transform(df) data.show() Original image: Binarized image with 100 threshold: ImageAdaptiveBinarizer Supported Methods: OTSU Gaussian local thresholding. Thresholds the image using a locally adaptive threshold that is computed using a local square region centered on each pixel. The threshold is equal to the gaussian weighted sum of the surrounding pixels times the scale. Sauvola Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description width float 90 Width of square region. method TresholdingMethod TresholdingMethod.GAUSSIAN Method used to determine adaptive threshold. scale float 1.1f Scale factor used to adjust threshold. imageType ImageType ImageType.TYPE_BYTE_BINARY Type of the output image Output Columns Param name Type Default Column Data Description outputCol string binarized_image image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val binirizer = new ImageAdaptiveBinarizer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;binary_image&quot;) .setWidth(100) .setScale(1.1) val data = binirizer.transform(df) data.storeImage(&quot;binary_image&quot;) from pyspark.ml import PipelineModel from sparkocr.transformers import * from sparkocr.utils import display_image imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) adaptive_thresholding = ImageAdaptiveBinarizer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;binarized_image&quot;) .setWidth(100) .setScale(1.1) pipeline = PipelineModel(stages=[ binary_to_image, adaptive_thresholding ]) result = pipeline.transform(df) for r in result.select(&quot;image&quot;, &quot;corrected_image&quot;).collect(): display_image(r.image) display_image(r.corrected_image) ImageAdaptiveThresholding Compute a threshold mask image based on local pixel neighborhood and apply it to image. Also known as adaptive or dynamic thresholding. The threshold value is the weighted mean for the local neighborhood of a pixel subtracted by a constant. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description blockSize int 170 Odd size of pixel neighborhood which is used to calculate the threshold value (e.g. 3, 5, 7, …, 21, …). method string   Method used to determine adaptive threshold for local neighbourhood in weighted mean image. offset int   Constant subtracted from weighted mean of neighborhood to calculate the local threshold value. Default offset is 0. mode string   The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to ‘constant’ cval int   Value to fill past edges of input if mode is ‘constant’. Output Columns Param name Type Default Column Data Description outputCol string binarized_image image struct (Image schema) Example: PythonScala // Implemented only for Python from pyspark.ml import PipelineModel from sparkocr.transformers import * from sparkocr.utils import display_image imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) adaptive_thresholding = ImageAdaptiveThresholding() .setInputCol(&quot;scaled_image&quot;) .setOutputCol(&quot;binarized_image&quot;) .setBlockSize(21) .setOffset(73) pipeline = PipelineModel(stages=[ binary_to_image, adaptive_thresholding ]) result = pipeline.transform(df) for r in result.select(&quot;image&quot;, &quot;corrected_image&quot;).collect(): display_image(r.image) display_image(r.corrected_image) Original image: Binarized image: ImageScaler ImageScaler scales image by provided scale factor. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description scaleFactor double 1.0 scale factor Output Columns Param name Type Default Column Data Description outputCol string scaled_image scaled image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageScaler import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageScaler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;scaled_image&quot;) .setScaleFactor(0.5) val data = transformer.transform(df) data.storeImage(&quot;scaled_image&quot;) from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) transformer = ImageScaler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;scaled_image&quot;) .setScaleFactor(0.5) data = transformer.transform(df) data.show() ImageAdaptiveScaler ImageAdaptiveScaler detects font size and scales image for have desired font size. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description desiredSize int 34 desired size of font in pixels Output Columns Param name Type Default Column Data Description outputCol string scaled_image scaled image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageAdaptiveScaler import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageAdaptiveScaler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;scaled_image&quot;) .setDesiredSize(34) val data = transformer.transform(df) data.storeImage(&quot;scaled_image&quot;) from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) transformer = ImageAdaptiveScaler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;scaled_image&quot;) .setDesiredSize(34) data = transformer.transform(df) data.show() ImageSkewCorrector ImageSkewCorrector detects skew of the image and rotates it. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description rotationAngle double 0.0 rotation angle automaticSkewCorrection boolean true enables/disables adaptive skew correction halfAngle double 5.0 half the angle(in degrees) that will be considered for correction resolution double 1.0 The step size(in degrees) that will be used for generating correction angle candidates Output Columns Param name Type Default Column Data Description outputCol string corrected_image corrected image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageSkewCorrector import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageSkewCorrector() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;corrected_image&quot;) .setAutomaticSkewCorrection(true) val data = transformer.transform(df) data.storeImage(&quot;corrected_image&quot;) from sparkocr.transformers import * val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageSkewCorrector() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;corrected_image&quot;) .setAutomaticSkewCorrection(true) val data = transformer.transform(df) data.show() Original image: Corrected image: ImageNoiseScorer ImageNoiseScorer computes noise score for each region. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) inputRegionsCol string regions regions Parameters Param name Type Default Description method NoiseMethod string NoiseMethod.RATIO method of computation noise score Output Columns Param name Type Default Column Data Description outputCol string noisescores noise score for each region Example: PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.{ImageNoiseScorer, ImageLayoutAnalyzer} import com.johnsnowlabs.ocr.NoiseMethod import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect regions val layoutAnalyzer = new ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) // Define transformer for compute noise level for each region val noisescorer = new ImageNoiseScorer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;noiselevel&quot;) .setInputRegionsCol(&quot;regions&quot;) .setMethod(NoiseMethod.VARIANCE) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( layoutAnalyzer, noisescorer )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = modelPipeline.transform(df) data.select(&quot;path&quot;, &quot;noiselevel&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * from sparkocr.enums import NoiseMethod imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) # Define transformer for detect regions layoutAnalyzer = ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) # Define transformer for compute noise level for each region noisescorer = ImageNoiseScorer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;noiselevel&quot;) .setInputRegionsCol(&quot;regions&quot;) .setMethod(NoiseMethod.VARIANCE) # Define pipeline pipeline = Pipeline() pipeline.setStages(Array( layoutAnalyzer, noisescorer )) data = pipeline.transform(df) data.select(&quot;path&quot;, &quot;noiselevel&quot;).show() Output: ++--+ |path |noiselevel | ++--+ |file:./noisy.png |[32.01805641767766, 32.312916551193354, 29.99257352247787, 30.62470388308217]| ++--+ ImageRemoveObjects python only ImageRemoveObjects for remove background objects. It support removing: objects less then elements of font with minSizeFont size objects less then minSizeObject holes less then minSizeHole objects more then maxSizeObject Input Columns Param name Type Default Column Data Description inputCol string None image struct (Image schema) Parameters Param name Type Default Description minSizeFont int 10 Min size font in pt. minSizeObject int None Min size of object which will keep on image [*]. connectivityObject int 0 The connectivity defining the neighborhood of a pixel. minSizeHole int None Min size of hole which will keep on image[ *]. connectivityHole int 0 The connectivity defining the neighborhood of a pixel. maxSizeObject int None Max size of object which will keep on image [*]. connectivityMaxObject int 0 The connectivity defining the neighborhood of a pixel. [*] : None value disables removing objects. Output Columns Param name Type Default Column Data Description outputCol string None scaled image struct (Image schema) Example: PythonScala // Implemented only for Python from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) remove_objects = ImageRemoveObjects() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;corrected_image&quot;) .setMinSizeObject(20) pipeline = PipelineModel(stages=[ binary_to_image, remove_objects ]) data = pipeline.transform(df) ImageMorphologyOperation python only ImageMorphologyOperationis a transformer for applying morphological operations to image. It supports following operation: Erosion Dilation Opening Closing Input Columns Param name Type Default Column Data Description inputCol string None image struct (Image schema) Parameters Param name Type Default Description operation MorphologyOperationType MorphologyOperationType.OPENING Operation type kernelShape KernelShape KernelShape.DISK Kernel shape. kernelSize int 1 Kernel size in pixels. Output Columns Param name Type Default Column Data Description outputCol string None scaled image struct (Image schema) Example: PythonScala // Implemented only for Python from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setOperation(MorphologyOperationType.OPENING) adaptive_thresholding = ImageAdaptiveThresholding() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;corrected_image&quot;) .setBlockSize(75) .setOffset(0) opening = ImageMorphologyOperation() .setInputCol(&quot;corrected_image&quot;) .setOutputCol(&quot;opening_image&quot;) .setkernelSize(1) pipeline = PipelineModel(stages=[ binary_to_image, adaptive_thresholding, opening ]) result = pipeline.transform(df) for r in result.select(&quot;image&quot;, &quot;corrected_image&quot;).collect(): display_image(r.image) display_image(r.corrected_image) Original image: Opening image: ImageCropper ImageCropperis a transformer for cropping image. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description cropRectangle Rectangle Rectangle(0,0,0,0) Image rectangle. cropSquareType CropSquareType CropSquareType.TOP_LEFT Type of square. Output Columns Param name Type Default Column Data Description outputCol string cropped_image scaled image struct (Image schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageAdaptiveScaler import com.johnsnowlabs.ocr.OcrContext.implicits._ import java.awt.Rectangle val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val rectangle: Rectangle = new Rectangle(0, 0, 200, 110) val cropper: ImageCropper = new ImageCropper() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cropped_image&quot;) .setCropRectangle(rectangle) val data = transformer.transform(df) data.storeImage(&quot;cropped_image&quot;) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setOperation(MorphologyOperationType.OPENING) cropper = ImageCropper() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cropped_image&quot;) .setCropRectangle((0, 0, 200, 110)) pipeline = PipelineModel(stages=[ binary_to_image, cropper ]) result = pipeline.transform(df) for r in result.select(&quot;image&quot;, &quot;cropped_image&quot;).collect(): display_image(r.image) display_image(r.cropped_image) Splitting image to regions ImageLayoutAnalyzer ImageLayoutAnalyzer analyzes the image and determines regions of text. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description pageSegMode PageSegmentationMode AUTO page segmentation mode pageIteratorLevel PageIteratorLevel BLOCK page iteration level ocrEngineMode EngineMode LSTM_ONLY OCR engine mode Output Columns Param name Type Default Column Data Description outputCol string region array of [Coordinaties]ocr_structures#coordinate-schema) Example: PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.{ImageSplitRegions, ImageLayoutAnalyzer} import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect regions val layoutAnalyzer = new ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) val data = layoutAnalyzer.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect regions layout_analyzer = ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, layout_analyzer ]) data = pipeline.transform(df) data.show() ImageSplitRegions ImageSplitRegions splits image to regions. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) inputRegionsCol string region array of [Coordinaties]ocr_structures#coordinate-schema) Parameters Param name Type Default Description explodeCols Array[string]   Columns which need to explode Output Columns Param name Type Default Column Data Description outputCol string region_image image struct (Image schema) Example: PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.{ImageSplitRegions, ImageLayoutAnalyzer} import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect regions val layoutAnalyzer = new ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) val splitter = new ImageSplitRegions() .setInputCol(&quot;image&quot;) .setRegionCol(&quot;regions&quot;) .setOutputCol(&quot;region_image&quot;) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( layoutAnalyzer, splitter )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = pipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect regions layout_analyzer = ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) splitter = ImageSplitRegions() .setInputCol(&quot;image&quot;) .setRegionCol(&quot;regions&quot;) .setOutputCol(&quot;region_image&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, layout_analyzer, splitter ]) data = pipeline.transform(df) data.show() ImageDrawRegions ImageDrawRegions draw regions to image. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) inputRegionsCol string region array of [Coordinaties]ocr_structures#coordinate-schema) Parameters Param name Type Default Description lineWidth Int 4 Line width for draw rectangles Output Columns Param name Type Default Column Data Description outputCol string image_with_regions image struct (Image schema) Example: PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.{ImageSplitRegions, ImageLayoutAnalyzer} import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect regions val layoutAnalyzer = new ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) val draw = new ImageDrawRegions() .setInputCol(&quot;image&quot;) .setRegionCol(&quot;regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( layoutAnalyzer, draw )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = pipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect regions layout_analyzer = ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) draw = ImageDrawRegions() .setInputCol(&quot;image&quot;) .setRegionCol(&quot;regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, layout_analyzer, draw ]) data = pipeline.transform(df) data.show() Characters recognition Next section describes the estimators for OCR ImageToText ImageToText runs OCR for input image, return recognized text to outputCol and positions with font size to ‘positionsCol’ column. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description pageSegMode PageSegmentationMode AUTO page segmentation mode pageIteratorLevel PageIteratorLevel BLOCK page iteration level ocrEngineMode EngineMode LSTM_ONLY OCR engine mode language Language Language.ENG language confidenceThreshold int 0 Confidence threshold. ignoreResolution bool true Ignore resolution from metadata of image. ocrParams array of strings [] Array of Ocr params in key=value format. pdfCoordinates bool false Transform coordinates in positions to PDF points. modelData string   Path to the local model data. modelType ModelType ModelType.BASE Model type downloadModelData bool false Download model data from JSL S3 withSpaces bool false Include spaces to output positions. Output Columns Param name Type Default Column Data Description outputCol string text Recognized text positionsCol string positions Positions of each block of text (related to pageIteratorLevel) in PageMatrix Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageToText import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setOcrParams(Array(&quot;preserve_interword_spaces=1&quot;)) val data = transformer.transform(df) print(data.select(&quot;text&quot;).collect()[0].text) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setOcrParams([&quot;preserve_interword_spaces=1&quot;, ]) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr ]) data = pipeline.transform(df) data.show() Image: Output: FOREWORD Electronic design engineers are the true idea men of the electronic industries. They create ideas and use them in their designs, they stimu- late ideas in other designers, and they borrow and adapt ideas from others. One could almost say they feed on and grow on ideas. ImageToHocr ImageToHocr runs OCR for input image, return recognized text and bounding boxes to outputCol column in HOCR format. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description pageSegMode PageSegmentationMode AUTO page segmentation mode pageIteratorLevel PageIteratorLevel BLOCK page iteration level ocrEngineMode EngineMode LSTM_ONLY OCR engine mode language string eng language ignoreResolution bool true Ignore resolution from metadata of image. ocrParams array of strings [] Array of Ocr params in key=value format. Output Columns Param name Type Default Column Data Description outputCol string hocr Recognized text Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageToHocr import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val data = transformer.transform(df) print(data.select(&quot;hocr&quot;).collect()[0].hocr) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr ]) data = pipeline.transform(df) data.show() Image: Output: &lt;div class=&#39;ocr_page&#39; id=&#39;page_1&#39; title=&#39;image &quot;&quot;; bbox 0 0 1280 467; ppageno 0&#39;&gt; &lt;div class=&#39;ocr_carea&#39; id=&#39;block_1_1&#39; title=&quot;bbox 516 80 780 114&quot;&gt; &lt;p class=&#39;ocr_par&#39; id=&#39;par_1_1&#39; lang=&#39;eng&#39; title=&quot;bbox 516 80 780 114&quot;&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_1&#39; title=&quot;bbox 516 80 780 114; baseline 0 -1; x_size 44; x_descenders 11; x_ascenders 11&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_1&#39; title=&#39;bbox 516 80 780 114; x_wconf 96&#39;&gt;FOREWORD&lt;/span&gt; &lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;div class=&#39;ocr_carea&#39; id=&#39;block_1_2&#39; title=&quot;bbox 40 237 1249 425&quot;&gt; &lt;p class=&#39;ocr_par&#39; id=&#39;par_1_2&#39; lang=&#39;eng&#39; title=&quot;bbox 40 237 1249 425&quot;&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_2&#39; title=&quot;bbox 122 237 1249 282; baseline 0.001 -12; x_size 45; x_descenders 12; x_ascenders 13&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_2&#39; title=&#39;bbox 122 237 296 270; x_wconf 96&#39;&gt;Electronic&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_3&#39; title=&#39;bbox 308 237 416 281; x_wconf 96&#39;&gt;design&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_4&#39; title=&#39;bbox 428 243 588 282; x_wconf 96&#39;&gt;engineers&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_5&#39; title=&#39;bbox 600 250 653 271; x_wconf 96&#39;&gt;are&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_6&#39; title=&#39;bbox 665 238 718 271; x_wconf 96&#39;&gt;the&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_7&#39; title=&#39;bbox 731 246 798 272; x_wconf 97&#39;&gt;true&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_8&#39; title=&#39;bbox 810 238 880 271; x_wconf 96&#39;&gt;idea&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_9&#39; title=&#39;bbox 892 251 963 271; x_wconf 96&#39;&gt;men&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_10&#39; title=&#39;bbox 977 238 1010 272; x_wconf 96&#39;&gt;of&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_11&#39; title=&#39;bbox 1021 238 1074 271; x_wconf 96&#39;&gt;the&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_12&#39; title=&#39;bbox 1086 239 1249 272; x_wconf 96&#39;&gt;electronic&lt;/span&gt; &lt;/span&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_3&#39; title=&quot;bbox 41 284 1248 330; baseline 0.002 -13; x_size 44; x_descenders 11; x_ascenders 12&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_13&#39; title=&#39;bbox 41 284 214 318; x_wconf 96&#39;&gt;industries.&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_14&#39; title=&#39;bbox 227 284 313 328; x_wconf 96&#39;&gt;They&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_15&#39; title=&#39;bbox 324 292 427 319; x_wconf 96&#39;&gt;create&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_16&#39; title=&#39;bbox 440 285 525 319; x_wconf 96&#39;&gt;ideas&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_17&#39; title=&#39;bbox 537 286 599 318; x_wconf 96&#39;&gt;and&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_18&#39; title=&#39;bbox 611 298 668 319; x_wconf 96&#39;&gt;use&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_19&#39; title=&#39;bbox 680 286 764 319; x_wconf 96&#39;&gt;them&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_20&#39; title=&#39;bbox 777 291 808 319; x_wconf 96&#39;&gt;in&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_21&#39; title=&#39;bbox 821 286 900 319; x_wconf 96&#39;&gt;their&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_22&#39; title=&#39;bbox 912 286 1044 330; x_wconf 96&#39;&gt;designs,&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_23&#39; title=&#39;bbox 1058 286 1132 330; x_wconf 93&#39;&gt;they&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_24&#39; title=&#39;bbox 1144 291 1248 320; x_wconf 92&#39;&gt;stimu-&lt;/span&gt; &lt;/span&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_4&#39; title=&quot;bbox 42 332 1247 378; baseline 0.002 -14; x_size 44; x_descenders 12; x_ascenders 12&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_25&#39; title=&#39;bbox 42 332 103 364; x_wconf 97&#39;&gt;late&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_26&#39; title=&#39;bbox 120 332 204 365; x_wconf 96&#39;&gt;ideas&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_27&#39; title=&#39;bbox 223 337 252 365; x_wconf 96&#39;&gt;in&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_28&#39; title=&#39;bbox 271 333 359 365; x_wconf 96&#39;&gt;other&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_29&#39; title=&#39;bbox 376 333 542 377; x_wconf 96&#39;&gt;designers,&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_30&#39; title=&#39;bbox 561 334 625 366; x_wconf 96&#39;&gt;and&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_31&#39; title=&#39;bbox 643 334 716 377; x_wconf 96&#39;&gt;they&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_32&#39; title=&#39;bbox 734 334 855 366; x_wconf 96&#39;&gt;borrow&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_33&#39; title=&#39;bbox 873 334 934 366; x_wconf 96&#39;&gt;and&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_34&#39; title=&#39;bbox 954 335 1048 378; x_wconf 96&#39;&gt;adapt&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_35&#39; title=&#39;bbox 1067 334 1151 367; x_wconf 96&#39;&gt;ideas&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_36&#39; title=&#39;bbox 1169 334 1247 367; x_wconf 96&#39;&gt;from&lt;/span&gt; &lt;/span&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_5&#39; title=&quot;bbox 40 379 1107 425; baseline 0.002 -13; x_size 45; x_descenders 12; x_ascenders 12&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_37&#39; title=&#39;bbox 40 380 151 412; x_wconf 96&#39;&gt;others.&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_38&#39; title=&#39;bbox 168 383 238 412; x_wconf 96&#39;&gt;One&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_39&#39; title=&#39;bbox 252 379 345 412; x_wconf 96&#39;&gt;could&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_40&#39; title=&#39;bbox 359 380 469 413; x_wconf 96&#39;&gt;almost&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_41&#39; title=&#39;bbox 483 392 537 423; x_wconf 96&#39;&gt;say&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_42&#39; title=&#39;bbox 552 381 626 424; x_wconf 96&#39;&gt;they&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_43&#39; title=&#39;bbox 641 381 712 414; x_wconf 96&#39;&gt;feed&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_44&#39; title=&#39;bbox 727 393 767 414; x_wconf 96&#39;&gt;on&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_45&#39; title=&#39;bbox 783 381 845 414; x_wconf 96&#39;&gt;and&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_46&#39; title=&#39;bbox 860 392 945 425; x_wconf 97&#39;&gt;grow&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_47&#39; title=&#39;bbox 959 393 999 414; x_wconf 96&#39;&gt;on&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_48&#39; title=&#39;bbox 1014 381 1107 414; x_wconf 95&#39;&gt;ideas.&lt;/span&gt; &lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; ImageBrandsToText ImageBrandsToText runs OCR for specified brands of input image, return recognized text to outputCol and positions with font size to ‘positionsCol’ column. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description pageSegMode PageSegmentationMode AUTO page segmentation mode pageIteratorLevel PageIteratorLevel BLOCK page iteration level ocrEngineMode EngineMode LSTM_ONLY OCR engine mode language string eng language confidenceThreshold int 0 Confidence threshold. ignoreResolution bool true Ignore resolution from metadata of image. ocrParams array of strings [] Array of Ocr params in key=value format. brandsCoords string   Json with coordinates of brands. Output Columns Param name Type Default Column Data Description outputCol structure image_brands Structure with recognized text from brands. textCol string text Recognized text positionsCol string positions Positions of each block of text (related to pageIteratorLevel) Example: PythonScala import com.johnsnowlabs.ocr.transformers.ImageToText import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageBrandsToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setBrandsCoordsStr( &quot;&quot;&quot; [ { &quot;name&quot;:&quot;part_one&quot;, &quot;rectangle&quot;:{ &quot;x&quot;:286, &quot;y&quot;:65, &quot;width&quot;:542, &quot;height&quot;:342 } }, { &quot;name&quot;:&quot;part_two&quot;, &quot;rectangle&quot;:{ &quot;x&quot;:828, &quot;y&quot;:65, &quot;width&quot;:1126, &quot;height&quot;:329 } } ] &quot;&quot;&quot;.stripMargin) val data = transformer.transform(df) print(data.select(&quot;text&quot;).collect()[0].text) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageBrandsToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setBrandsCoords(&quot;&quot;&quot;[ { &quot;name&quot;:&quot;part_one&quot;, &quot;rectangle&quot;:{ &quot;x&quot;:286, &quot;y&quot;:65, &quot;width&quot;:542, &quot;height&quot;:342 } }, { &quot;name&quot;:&quot;part_two&quot;, &quot;rectangle&quot;:{ &quot;x&quot;:828, &quot;y&quot;:65, &quot;width&quot;:1126, &quot;height&quot;:329 } } ]&quot;&quot;&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr ]) data = pipeline.transform(df) data.show() Other Next section describes the extra transformers PositionFinder PositionFinder find position of input text entities in original document. Input Columns Param name Type Default Column Data Description inputCols string image Input annotations columns pageMatrixCol string   Column name for Page Matrix schema Parameters Param name Type Default Description matchingWindow int 10 Textual range to match in context, applies in both direction windowPageTolerance boolean true whether or not to increase tolerance as page number grows padding int 5 padding for area Output Columns Param name Type Default Column Data Description outputCol string   Name of output column for store coordinates. Example: PythonScala import com.johnsnowlabs.ocr.transformers._ import com.johnsnowlabs.nlp.{DocumentAssembler, SparkAccessor} import com.johnsnowlabs.nlp.annotators._ import com.johnsnowlabs.nlp.util.io.ReadAs val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) val positionFinder = new PositionFinder() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;coordinates&quot;) .setPageMatrixCol(&quot;positions&quot;) .setMatchingWindow(10) .setPadding(2) // Create pipeline val pipeline = new Pipeline() .setStages(Array( pdfToText, documentAssembler, sentenceDetector, tokenizer, entityExtractor, positionFinder )) val results = pipeline.fit(df).transform(df) results.show() from pyspark.ml import Pipeline from sparkocr.transformers import * from sparknlp.annotator import * from sparknlp.base import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;page&quot;) .setSplitPage(False) document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) entity_extractor = TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;./sparkocr/resources/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) position_finder = PositionFinder() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;coordinates&quot;) .setPageMatrixCol(&quot;positions&quot;) .setMatchingWindow(10) .setPadding(2) pipeline = Pipeline(stages=[ pdf_to_text, document_assembler, sentence_detector, tokenizer, entity_extractor, position_finder ]) results = pipeline.fit(df).transform(df) results.show() UpdateTextPosition UpdateTextPosition update output text and keep old coordinates of original document. Input Columns Param name Type Default Column Data Description inputCol string positions Сolumn name with original positions struct InputText string replace_text Column name for New Text to replace Old one Output Columns Param name Type Default Column Data Description outputCol string output_positions Name of output column for updated positions struct. Example: PythonScala import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.spell.norvig.NorvigSweetingModel import com.johnsnowlabs.nlp.{DocumentAssembler, TokenAssembler} import com.johnsnowlabs.ocr.transformers._ import org.apache.spark.ml.Pipeline val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val token = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;tokens&quot;) val spell = NorvigSweetingModel.pretrained(&quot;spellcheck_norvig&quot;, &quot;en&quot;) .setInputCols(&quot;tokens&quot;) .setOutputCol(&quot;spell&quot;) val tokenAssem = new TokenAssembler() .setInputCols(&quot;spell&quot;) .setOutputCol(&quot;newDocs&quot;) val updatedText = new UpdateTextPosition() .setInputCol(&quot;positions&quot;) .setOutputCol(&quot;output_positions&quot;) .setInputText(&quot;newDocs.result&quot;) val pipeline = new Pipeline() .setStages(Array( pdfToText, documentAssembler, sentence, token, spell, tokenAssem, updatedText )) val results = pipeline.fit(df).transform(df) results.show() from pyspark.ml import Pipeline from sparkocr.transformers import * from sparknlp.annotator import * from sparknlp.base import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;page&quot;) .setSplitPage(False) document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;tokens&quot;) spell = NorvigSweetingModel().pretrained(&quot;spellcheck_norvig&quot;, &quot;en&quot;) .setInputCols(&quot;tokens&quot;) .setOutputCol(&quot;spell&quot;) tokenAssem = TokenAssembler() .setInputCols(&quot;spell&quot;) .setOutputCol(&quot;newDocs&quot;) updatedText = UpdateTextPosition() .setInputCol(&quot;positions&quot;) .setOutputCol(&quot;output_positions&quot;) .setInputText(&quot;newDocs.result&quot;) pipeline = Pipeline(stages=[ document_assembler, sentence_detector, tokenizer, spell, tokenAssem, updatedText ]) results = pipeline.fit(df).transform(df) results.show() FoundationOneReportParser FoundationOneReportParser is a transformer for parsing FoundationOne reports. Current implementation support parsing patient info, genomic, biomarker findings and gene lists from appendix. Output format is json. Input Columns Param name Type Default Column Data Description inputCol string text Сolumn name with text of report originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string report Name of output column with report in json format. Example: PythonScala import com.johnsnowlabs.ocr.transformers._ import org.apache.spark.ml.Pipeline val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) .setTextStripper(TextStripperType.PDF_LAYOUT_TEXT_STRIPPER) val genomicsParser = new FoundationOneReportParser() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;report&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( pdfToText, genomicsParser )) val modelPipeline = pipeline.fit(df) val report = modelPipeline.transform(df) from pyspark.ml import Pipeline from sparkocr.transformers import * from sparkocr.enums import TextStripperType pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text = PdfToText() pdf_to_text.setInputCol(&quot;content&quot;) pdf_to_text.setOutputCol(&quot;text&quot;) pdf_to_text.setSplitPage(False) pdf_to_text.setTextStripper(TextStripperType.PDF_LAYOUT_TEXT_STRIPPER) genomic_parser = FoundationOneReportParser() genomic_parser.setInputCol(&quot;text&quot;) genomic_parser.setOutputCol(&quot;report&quot;) report = genomic_parser.transform(pdf_to_text.transform(df)).collect() Output: { &quot;Patient&quot; : { &quot;disease&quot; : &quot;Unknown primary melanoma&quot;, &quot;name&quot; : &quot;Lekavich Gloria&quot;, &quot;date_of_birth&quot; : &quot;11 November 1926&quot;, &quot;sex&quot; : &quot;Female&quot;, &quot;medical_record&quot; : &quot;11111&quot; }, &quot;Physician&quot; : { &quot;ordering_physician&quot; : &quot;Genes Pinley&quot;, &quot;medical_facility&quot; : &quot;Health Network Cancer Institute&quot;, &quot;additional_recipient&quot; : &quot;Nath&quot;, &quot;medical_facility_id&quot; : &quot;202051&quot;, &quot;pathologist&quot; : &quot;Manqju Nwath&quot; }, &quot;Specimen&quot; : { &quot;specimen_site&quot; : &quot;Rectum&quot;, &quot;specimen_id&quot; : &quot;AVS 1A&quot;, &quot;specimen_type&quot; : &quot;Slide&quot;, &quot;date_of_collection&quot; : &quot;20 March 2015&quot;, &quot;specimen_received&quot; : &quot;30 March 2015 &quot; }, &quot;Biomarker_findings&quot; : [ { &quot;name&quot; : &quot;Tumor Mutation Burden&quot;, &quot;state&quot; : &quot;TMB-Low (3Muts/Mb)&quot;, &quot;actionability&quot; : &quot;No therapies or clinical trials. &quot; } ], &quot;Genomic_findings&quot; : [ { &quot;name&quot; : &quot;FLT3&quot;, &quot;state&quot; : &quot;amplification&quot;, &quot;therapies_with_clinical_benefit_in_patient_tumor_type&quot; : [ &quot;none&quot; ], &quot;therapies_with_clinical_benefit_in_other_tumor_type&quot; : [ &quot;Sorafenib&quot;, &quot;Sunitinib&quot;, &quot;Ponatinib&quot; ] } ], &quot;Appendix&quot; : { &quot;dna_gene_list&quot; : [ &quot;ABL1&quot;, &quot;ACVR1B&quot;, &quot;AKT1&quot;, .... ], &quot;dna_gene_list_rearrangement&quot; : [ &quot;ALK&quot;, &quot;BCL2&quot;, &quot;BCR&quot;, .... ], &quot;additional_assays&quot; : [ &quot;Tumor Mutation Burden (TMB)&quot;, &quot;Microsatellite Status (MS)&quot; ] } }",
    "url": "/docs/en/ocr_pipeline_components",
    "relUrl": "/docs/en/ocr_pipeline_components"
  },
  "47": {
    "id": "47",
    "title": "Spark OCR release notes",
    "content": "3.3.0 Overview Table detection and recognition for scanned documents. For table detection we added ImageTableDetector. It based on CascadeTabNet which used Cascade mask Region-based CNN High-Resolution Network (Cascade mask R-CNN HRNet). Model was pre-trained on the COCO dataset and fine tuned on ICDAR 2019 competitions dataset for table detection. It demonstrates state of the art results for ICDAR 2013 and TableBank. And top results for ICDAR 2019. New Features ImageTableDetector is a DL model for detect tables on the image. ImageTableCellDetector is a transformer for detect regions of cells in the table image. ImageCellsToTextTable is a transformer for extract text from the detected cells. New notebooks Image Table Detection example Image Cell Recognition example Image Table Recognition 3.2.0 Overview Multi-modal visual document understanding, built on the LayoutLM architecture. It achieves new state-of-the-art accuracy in several downstream tasks, including form understanding and receipt understanding. New Features VisualDocumentNER is a DL model for NER problem using text and layout data. Currently available pre-trained model on the SROIE dataset. Enhancements Added support SPARK_OCR_LICENSE env key for read license. Update dependencies and sync Spark versions with Spark NLP. Bugfixes Fixed an issue that some ImageReaderSpi plugins are unavailable in the fat jar. New notebooks Visual Document NER 3.1.0 Overview Image processing on GPU. It is in 3..5 times faster than on CPU. New Features GPUImageTransformer with support: scaling, erosion, delation, Otsu and Huang thresholding. Added display_images util function for display images from Spark DataFrame in Jupyter notebooks. Enhancements Improve display_image util function. Bug fixes Fixed issue with extra dependencies in start function New notebooks GPU image processing 3.0.0 Overview We are very excited to release Spark OCR 3.0.0! Spark OCR 3.0.0 extends the support for Apache Spark 3.0.x and 3.1.x major releases on Scala 2.12 with both Hadoop 2.7. and 3.2. We will support all 4 major Apache Spark and PySpark releases of 2.3.x, 2.4.x, 3.0.x, and 3.1.x. Spark OCR started to support Tensorflow models. First model is VisualDocumentClassifier. New Features Support for Apache Spark and PySpark 3.0.x on Scala 2.12 Support for Apache Spark and PySpark 3.1.x on Scala 2.12 Support 9x new Databricks runtimes: Databricks 7.3 Databricks 7.3 ML GPU Databricks 7.4 Databricks 7.4 ML GPU Databricks 7.5 Databricks 7.5 ML GPU Databricks 7.6 Databricks 7.6 ML GPU Databricks 8.0 Databricks 8.0 ML (there is no GPU in 8.0) Databricks 8.1 Support 2x new EMR 6.x: EMR 6.1.0 (Apache Spark 3.0.0 / Hadoop 3.2.1) EMR 6.2.0 (Apache Spark 3.0.1 / Hadoop 3.2.1) VisualDocumentClassifier model for classification documents using text and layout data. Added support Vietnamese language. New notebooks Visual Document Classifier 1.11.0 Overview Support German, French, Spanish and Russian languages. Improving PositionsFinder and ImageToText for better support de-identification. New Features Loading model data from S3 in ImageToText. Added support German, French, Spanish, Russian languages in ImageToText. Added different OCR model types: Base, Best, Fast in ImageToText. Enhancements Added spaces symbols to the output positions in the ImageToText transformer. Eliminate python-levensthein from dependencies for simplify installation. Bugfixes Fixed issue with extracting coordinates in in ImageToText. Fixed loading model data on cluster in yarn mode. New notebooks Languages Support Image DeIdentification 1.10.0 Overview Support Microsoft Docx documents. New Features Added DocToText transformer for extract text from DOCX documents. Added DocToTextTable transformer for extract table data from DOCX documents. Added DocToPdf transformer for convert DOCX documents to PDF format. Bugfixes Fixed issue with loading model data on some cluster configurations 1.9.0 Release date: 11-12-2020 Overview Extension of FoundationOne report parser and support HOCR output format. New Features Added ImageToHocr transformer for recognize text from image and store it to HOCR format. Added parsing gene lists from ‘Appendix’ in FoundationOneReportParser transformer. 1.8.0 Release date: 20-11-2020 Overview Optimisation performance for processing multipage PDF documents. Support up to 10k pages per document. New Features Added ImageAdaptiveBinarizer Scala transformer with support: Gaussian local thresholding Otsu thresholding Sauvola local thresholding Added possibility to split pdf to small documents for optimize processing in PdfToImage. Enhancements Added applying binarization in PdfToImage for optimize memory usage. Added pdfCoordinates param to the ImageToText transformer. Added ‘total_pages’ field to the PdfToImage transformer. Added different splitting strategies to the PdfToImage transformer. Simplified paging PdfToImage when run it with splitting to small PDF. Added params to the PdfToText for disable extra functionality. Added master_url param to the python start function. 1.7.0 Release date: 22-09-2020 Overview Support Spark 2.3.3. Bugfixes Restored read JPEG2000 image 1.6.0 Release date: 05-09-2020 Overview Support parsing data from tables for selectable PDFs. New Features Added PdfToTextTable transformer for extract tables from Pdf document per each page. Added ImageCropper transformer for crop images. Added ImageBrandsToText transformer for detect text in defined areas. 1.5.0 Release date: 22-07-2020 Overview FoundationOne report parsing support. Enhancements Optimized memory usage during image processing New Features Added FoundationOneReportParser which support parsing patient info, genomic and biomarker findings. 1.4.0 Release date: 23-06-2020 Overview Added support Dicom format and improved support image morphological operations. Enhancements Updated start function. Improved support Spark NLP internal. ImageMorphologyOpening and ImageErosion are removed. Improved existing transformers for support de-identification Dicom documents. Added possibility to draw filled rectangles to ImageDrawRegions. New Features Support reading and writing Dicom documents. Added ImageMorphologyOperation transformer which support: erosion, dilation, opening and closing operations. Bugfixes Fixed issue in ImageToText related to extraction coordinates. 1.3.0 Release date: 22-05-2020 Overview New functionality for de-identification problem. Enhancements Renamed TesseractOCR to ImageToText. Simplified installation. Added check license from SPARK_NLP_LICENSE env varibale. New Features Support storing for binaryFormat. Added support storing Image and PDF files. Support selectable pdf for TextToPdf transformer. Added UpdateTextPosition transformer. 1.2.0 Release date: 08-04-2020 Overview Improved support Databricks and processing selectable pdfs. Enhancements Adapted Spark OCR for run on Databricks. Added rewriting positions in ImageToText when run together with PdfToText. Added ‘positionsCol’ param to ImageToText. Improved support Spark NLP. Changed start function. New Features Added showImage implicit to Dataframe for display images in Scala Databricks notebooks. Added display_images function for display images in Python Databricks notebooks. Added propagation selectable pdf file in TextToPdf. Added ‘inputContent’ param to ‘TextToPdf’. 1.1.2 Release date: 09-03-2020 Overview Minor improvements and fixes Enhancements Improved messages during license validation Bugfixes Fixed dependencies issue 1.1.1 Release date: 06-03-2020 Overview Integration with license server. Enhancements Added license validation. License can be set in following waysq: Environment variable. Set variable ‘JSL_OCR_LICENSE’. System property. Set property ‘jsl.sparkocr.settings.license’. Application.conf file. Set property ‘jsl.sparkocr.settings.license’. Added auto renew license using jsl license server. 1.1.0 Release date: 03-03-2020 Overview This release contains improvements for preprocessing image before run OCR and added possibility to store results to PDF for keep original formatting. New Features Added auto calculation maximum size of objects for removing in ImageRemoveObjects. This improvement avoids to remove . and affect symbols with dots (i, !, ?). Added minSizeFont param to ImageRemoveObjects transformer for activate this functional. Added ocrParams parameter to ImageToText transformer for set any ocr params. Added extraction font size in ImageToText Added TextToPdf transformer for render text with positions to pdf file. Enhancements Added setting resolution in ImageToText. And added ignoreResolution param with default true value to ImageToText transformer for back compatibility. Added parsing resolution from image metadata in BinaryToImage transformer. Added storing resolution in PrfToImage transformer. Added resolution field to Image schema. Updated ‘start’ function for set ‘PYSPARK_PYTHON’ env variable. Improve auto-scaling/skew correction: improved access to images values removing unnecessary copies of images adding more test cases improving auto-correlation in auto-scaling. 1.0.0 Release date: 12-02-2020 Overview Spark NLP OCR functionality was reimplemented as set of Spark ML transformers and moved to separate Spark OCR library. New Features Added extraction coordinates of each symbol in ImageToText Added ImageDrawRegions transformer Added ImageToPdf transformer Added ImageMorphologyOpening transformer Added ImageRemoveObjects transformer Added ImageAdaptiveThresholding transformer Enhancements Reimplement main functionality as Spark ML transformers Moved DrawRectangle functionality to PdfDrawRegions transformer Added ‘start’ function with support SparkMonitor initialization Moved PositionFinder to Spark OCR Bugfixes Fixed bug with transforming complex pdf to image",
    "url": "/docs/en/ocr_release_notes",
    "relUrl": "/docs/en/ocr_release_notes"
  },
  "48": {
    "id": "48",
    "title": "Structures and helpers",
    "content": "Schemas Image Schema Images are loaded as a DataFrame with a single column called “image.” It is a struct-type column, that contains all information about image: image: struct (nullable = true) | |-- origin: string (nullable = true) | |-- height: integer (nullable = false) | |-- width: integer (nullable = false) | |-- nChannels: integer (nullable = false) | |-- mode: integer (nullable = false) | |-- resolution: integer (nullable = true) | |-- data: binary (nullable = true) Fields Field name Type Description origin string source URI height integer image height in pixels width integer image width in pixels nChannels integer number of color channels mode ImageType the data type and channel order the data is stored in resolution integer Resolution of image in dpi data binary image data in a binary format NOTE: Image data stored in a binary format. Image data is represented as a 3-dimensional array with the dimension shape (height, width, nChannels) and array values of type t specified by the mode field. Coordinate Schema element: struct (containsNull = true) | | |-- index: integer (nullable = false) | | |-- page: integer (nullable = false) | | |-- x: float (nullable = false) | | |-- y: float (nullable = false) | | |-- width: float (nullable = false) | | |-- height: float (nullable = false) Field name Type Description index integer Chunk index page integer Page number x float The lower left x coordinate y float The lower left y coordinate width float The width of the rectangle height float The height of the rectangle PageMatrix Schema element: struct (containsNull = true) | | |-- mappings: array[struct] (nullable = false) Field name Type Description mappings Array[Mapping] Array of mappings Mapping Schema element: struct (containsNull = true) | | |-- c: string (nullable = false) | | |-- p: integer (nullable = false) | | |-- x: float (nullable = false) | | |-- y: float (nullable = false) | | |-- width: float (nullable = false) | | |-- height: float (nullable = false) | | |-- fontSize: integer (nullable = false) Field name Type Description c string Character p integer Page number x float The lower left x coordinate y float The lower left y coordinate width float The width of the rectangle height float The height of the rectangle fontSize integer Font size in points Enums PageSegmentationMode OSD_ONLY: Orientation and script detection only. AUTO_OSD: Automatic page segmentation with orientation and script detection. AUTO_ONLY: Automatic page segmentation, but no OSD, or OCR. AUTO: Fully automatic page segmentation, but no OSD. SINGLE_COLUMN: Assume a single column of text of variable sizes. SINGLE_BLOCK_VERT_TEXT: Assume a single uniform block of vertically aligned text. SINGLE_BLOCK: Assume a single uniform block of text. SINGLE_LINE: Treat the image as a single text line. SINGLE_WORD: Treat the image as a single word. CIRCLE_WORD: Treat the image as a single word in a circle. SINGLE_CHAR: Treat the image as a single character. SPARSE_TEXT: Find as much text as possible in no particular order. SPARSE_TEXT_OSD: Sparse text with orientation and script detection. EngineMode TESSERACT_ONLY: Legacy engine only. OEM_LSTM_ONLY: Neural nets LSTM engine only. TESSERACT_LSTM_COMBINED: Legacy + LSTM engines. DEFAULT: Default, based on what is available. PageIteratorLevel BLOCK: Block of text/image/separator line. PARAGRAPH: Paragraph within a block. TEXTLINE: Line within a paragraph. WORD: Word within a text line. SYMBOL: Symbol/character within a word. Language ENG: English FRA: French SPA: Spanish RUS: Russian DEU: German VIE: Vietnamese ModelType BASE: Block of text/image/separator line. BEST: Paragraph within a block. FAST: Line within a paragraph. ImageType TYPE_BYTE_GRAY TYPE_BYTE_BINARY TYPE_3BYTE_BGR TYPE_4BYTE_ABGR NoiseMethod VARIANCE RATIO KernelShape SQUARE DIAMOND DISK OCTAHEDRON OCTAGON STAR MorphologyOperationType OPENING CLOSING EROSION DILATION CropSquareType TOP_LEFT TOP_CENTER TOP_RIGHT CENTER_LEFT CENTER CENTER_RIGHT BOTTOM_LEFT BOTTOM_CENTER BOTTOM_RIGHT SplittingStrategy FIXED_NUMBER_OF_PARTITIONS FIXED_SIZE_OF_PARTITION TresholdingMethod GAUSSIAN OTSU SAUVOLA OCR implicits asImage asImage transforms binary content to Image schema. Parameters Param name Type Default Description outputCol string image output column name contentCol string content input column name with binary content pathCol string path input column name with path to original file Example: import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) df.show() storeImage storeImage stores the image(s) to tmp location and return Dataset with path(s) to stored image files. Parameters Param name Type Default Description inputColumn string   input column name with image struct formatName string png image format name prefix string sparknlp_ocr_ prefix for output file Example: import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) df.storeImage(&quot;image&quot;) showImages Show images on Databrics notebook. Parameters Param name Type Default Description field string image input column name with image struct limit integer 5 count of rows for display width string “800” width of image show_meta boolean true enable/disable displaying methadata of image Jupyter Python helpers display_image Show single image with methadata in Jupyter notebook. Parameters Param name Type Default Description width string “600” width of image show_meta boolean true enable/disable displaying methadata of image Example: from sparkocr.utils import display_image from sparkocr.transformers import BinaryToImage images_path = &quot;/tmp/ocr/images/*.tif&quot; images_example_df = spark.read.format(&quot;binaryFile&quot;).load(images_path).cache() display_image(BinaryToImage().transform(images_example_df).collect()[0].image) display_images Show images from dataframe. Parameters Param name Type Default Description field string image input column name with image struct limit integer 5 count of rows for display width string “600” width of image show_meta boolean true enable/disable displaying methadata of image Example: from sparkocr.utils import display_images from sparkocr.transformers import BinaryToImage images_path = &quot;/tmp/ocr/images/*.tif&quot; images_example_df = spark.read.format(&quot;binaryFile&quot;).load(images_path).cache() display_images(BinaryToImage().transform(images_example_df), limit=3) Databricks Python helpers display_images Show images from dataframe. Parameters Param name Type Default Description field string image input column name with image struct limit integer 5 count of rows for display width string “800” width of image show_meta boolean true enable/disable displaying methadata of image Example: from sparkocr.databricks import display_images from sparkocr.transformers import BinaryToImage images_path = &quot;/tmp/ocr/images/*.tif&quot; images_example_df = spark.read.format(&quot;binaryFile&quot;).load(images_path).cache() display_images(BinaryToImage().transform(images_example_df), limit=3)",
    "url": "/docs/en/ocr_structures",
    "relUrl": "/docs/en/ocr_structures"
  },
  "49": {
    "id": "49",
    "title": "Table recognition",
    "content": "ImageTableDetector ImageTableDetector is a DL model for detect tables on the image. It based on CascadeTabNet which used Cascade mask Region-based CNN High-Resolution Network (Cascade mask R-CNN HRNet). Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description scoreThreshold float 0.9 Score threshold for output regions. Output Columns Param name Type Default Column Data Description outputCol string table_regions array of [Coordinaties]ocr_structures#coordinate-schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect tables val table_detector = ImageTableDetector .pretrained(&quot;general_model_table_detection_v2&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;table_regions&quot;) val draw_regions = new ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;table_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, table_detector, draw_regions ]) val data = pipeline.transform(df) data.storeImage(&quot;image_with_regions&quot;) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect tables table_detector = ImageTableDetector .pretrained(&quot;general_model_table_detection_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;table_regions&quot;) draw_regions = ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;table_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, table_detector, draw_regions ]) data = pipeline.transform(df) display_images(data, &quot;image_with_regions&quot;) Output: ImageTableCellDetector ImageTableCellDetector detect cells on image with table. It based on image processing algorithm by detecting horizontal and vertical lines. Current implementation works for bordered tables without background. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Output Columns Param name Type Default Column Data Description outputCol string cells array of coordinates of cells Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect cells val transformer = new ImageTableCellDetector() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cells&quot;) val data = transformer.transform(df) data.select(&quot;cells&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect cells transformer = ImageTableCellDetector .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cells&quot;) pipeline = PipelineModel(stages=[ binary_to_image, transformer ]) data = pipeline.transform(df) data.select(&quot;cells&quot;).show() Image: Output:* +-+ | cells | +-+ ||[[[[15, 17, 224, 53]], [[241, 17, 179, 53]], [[423, 17, | | 194, 53]], [[619, 17, 164, 53]] .... | +-+ ImageCellsToTextTable ImageCellsToTextTable runs OCR for cells regions on image, return recognized text to outputCol as TableContainer structure. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) cellsCol string celss Array of cells Parameters Param name Type Default Description strip bool true Strip output text. margin bool 1 Margin of cells in pixelx. pageSegMode PageSegmentationMode AUTO page segmentation mode ocrEngineMode EngineMode LSTM_ONLY OCR engine mode language Language Language.ENG language ocrParams array of strings [] Array of Ocr params in key=value format. pdfCoordinates bool false Transform coordinates in positions to PDF points. modelData string   Path to the local model data. modelType ModelType ModelType.BASE Model type downloadModelData bool false Download model data from JSL S3 Output Columns Param name Type Default Column Data Description outputCol string table Recognized text as TableContainer Example: PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect cells val cell_detector = new ImageTableCellDetector() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cells&quot;) val table_recognition = new ImageCellsToTextTable() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;tables&quot;) .setMargin(2) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( cell_detector, table_recognition )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val results = modelPipeline.transform(df) results.select(&quot;tables&quot;) .withColumn(&quot;cells&quot;, explode(col(&quot;tables.chunks&quot;))) .select((0 until 7).map(i =&gt; col(&quot;cells&quot;)(i).getField(&quot;chunkText&quot;).alias(s&quot;col$i&quot;)): _*) .show(false) from pyspark.ml import PipelineModel import pyspark.sql.functions as f from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() binary_to_image.setImageType(ImageType.TYPE_BYTE_GRAY) binary_to_image.setInputCol(&quot;content&quot;) cell_detector = TableCellDetector() cell_detector.setInputCol(&quot;image&quot;) cell_detector.setOutputCol(&quot;cells&quot;) cell_detector.setKeepInput(True) table_recognition = ImageCellsToTextTable() table_recognition.setInputCol(&quot;image&quot;) table_recognition.setCellsCol(&#39;cells&#39;) table_recognition.setMargin(2) table_recognition.setStrip(True) table_recognition.setOutputCol(&#39;table&#39;) pipeline = PipelineModel(stages=[ binary_to_image, cell_detector, table_recognition ]) result = pipeline.transform(df) results.select(&quot;table&quot;) .withColumn(&quot;cells&quot;, f.explode(f.col(&quot;table.chunks&quot;))) .select([f.col(&quot;cells&quot;)[i].getField(&quot;chunkText&quot;).alias(f&quot;col{i}&quot;) for i in range(0, 7)]) .show(20, False) Image: Output: +-+-+--++--++-+ |col0 |col1 |col2 |col3 |col4 |col5 |col6 | +-+-+--++--++-+ |Order Date|Region |Rep |Item |Units|Unit Cost|Total | |1/23/10 |Ontario|Kivell |Binder|50 |$19.99 |$999.50| |2/9/10 |Ontario|Jardine |Pencil|36 |$4.99 |$179.64| |2/26/10 |Ontario|Gill |Pen |27 |$19.99 |$539.73| |3/15/10 |Alberta|Sorvino |Pencil|56 |$2.99 |$167.44| |4/1/10 |Quebec |Jones |Binder|60 |$4.99 |$299.40| |4/18/10 |Ontario|Andrews |Pencil|75 |$1.99 |$149.25| |5/5/10 |Ontario|Jardine |Pencil|90 |$4.99 |$449.10| |5/22/10 |Alberta|Thompson|Pencil|32 |$1.99 |$63.68 | +-+-+--++--++-+",
    "url": "/docs/en/ocr_table_recognition",
    "relUrl": "/docs/en/ocr_table_recognition"
  },
  "50": {
    "id": "50",
    "title": "Visual document understanding",
    "content": "NLP models are great at processing digital text, but many real-word applications use documents with more complex formats. For example, healthcare systems often include visual lab results, sequencing reports, clinical trial forms, and other scanned documents. When we only use an NLP approach for document understanding, we lose layout and style information - which can be vital for document image understanding. New advances in multi-modal learning allow models to learn from both the text in documents (via NLP) and visual layout (via computer vision). We provide multi-modal visual document understanding, built on Spark OCR based on the LayoutLM architecture. It achieves new state-of-the-art accuracy in several downstream tasks, including form understanding (from 70.7 to 79.3), receipt understanding (from 94.0 to 95.2) and document image classification (from 93.1 to 94.4). Please check also webinar: Visual Document Understanding with Multi-Modal Image &amp; Text Mining in Spark OCR 3 VisualDocumentClassifier VisualDocumentClassifier is a DL model for classification documents using text and layout data. Currently available pretrained model on the Tabacco3482 dataset. Input Columns Param name Type Default Column Data Description inputCol string hocr Сolumn name with HOCR of the document Parameters Param name Type Default Description maxSentenceLength int 128 Maximum sentence length. caseSensitive boolean false Determines whether model is case sensitive. confidenceThreshold float 0f Confidence threshold. Output Columns Param name Type Default Column Data Description labelCol string label Name of output column with the predicted label. confidenceCol string confidence Name of output column with confidence. Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToHocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val visualDocumentClassifier = VisualDocumentClassifier .pretrained(&quot;visual_document_classifier_tobacco3482&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setMaxSentenceLength(128) .setInputCol(&quot;hocr&quot;) .setLabelCol(&quot;label&quot;) .setConfidenceCol(&quot;conf&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( imageToHocr, visualDocumentClassifier )) val modelPipeline = pipeline.fit(df) val result = modelPipeline.transform(df) result.select(&quot;label&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) document_classifier = VisualDocumentClassifier() .pretrained(&quot;visual_document_classifier_tobacco3482&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setMaxSentenceLength(128) .setInputCol(&quot;hocr&quot;) .setLabelCol(&quot;label&quot;) .setConfidenceCol(&quot;conf&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr, document_classifier, ]) result = pipeline.transform(df) result.select(&quot;label&quot;).show() Output: ++ | label| ++ |Letter| ++ VisualDocumentNER VisualDocumentNER is a DL model for NER documents using text and layout data. Currently available pre-trained model on the SROIE dataset. Input Columns Param name Type Default Column Data Description inputCol string hocr Сolumn name with HOCR of the document Parameters Param name Type Default Description maxSentenceLength int 512 Maximum sentence length. caseSensitive boolean false Determines whether model is case sensitive. Output Columns Param name Type Default Column Data Description outputCol string entities Name of output column with entities Annotation. Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToHocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val visualDocumentNER = VisualDocumentNER .pretrained(&quot;visual_document_NER_SROIE0526&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) .setMaxSentenceLength(512) .setInputCol(&quot;hocr&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( imageToHocr, visualDocumentNER )) val modelPipeline = pipeline.fit(df) val result = modelPipeline.transform(df) result.select(&quot;entities&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) document_ner = VisualDocumentNer() .pretrained(&quot;visual_document_NER_SROIE0526&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) .setMaxSentenceLength(512) .setInputCol(&quot;hocr&quot;) .setLabelCol(&quot;label&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr, document_ner, ]) result = pipeline.transform(df) result.select(&quot;entities&quot;).show() Output: +-+ |entities | +-+ |[[entity, 0, 0, O, [word -&gt; 0£0, token -&gt; 0£0], []], [entity, 0, 0, | | B-COMPANY, [word -&gt; AEON, token -&gt; aeon], []], [entity, 0, 0, B-COMPANY,| | [word -&gt; CO., token -&gt; co], ... | +-+",
    "url": "/docs/en/ocr_visual_document_understanding",
    "relUrl": "/docs/en/ocr_visual_document_understanding"
  },
  "51": {
    "id": "51",
    "title": "Pipelines",
    "content": "Pretrained Pipelines moved to its own dedicated repository. Please follow this link for updated list: https://github.com/JohnSnowLabs/spark-nlp-models English NOTE: noncontrib pipelines are compatible with Windows operating systems. Pipelines Name Explain Document ML explain_document_ml Explain Document DL explain_document_dl Explain Document DL Win explain_document_dl_noncontrib Explain Document DL Fast explain_document_dl_fast Explain Document DL Fast Win explain_document_dl_fast_noncontrib Recognize Entities DL recognize_entities_dl Recognize Entities DL Win recognize_entities_dl_noncontrib OntoNotes Entities Small onto_recognize_entities_sm OntoNotes Entities Large onto_recognize_entities_lg Match Datetime match_datetime Match Pattern match_pattern Match Chunk match_chunks Match Phrases match_phrases Clean Stop clean_stop Clean Pattern clean_pattern Clean Slang clean_slang Check Spelling check_spelling Analyze Sentiment analyze_sentiment Analyze Sentiment DL analyze_sentimentdl_use_imdb Analyze Sentiment DL analyze_sentimentdl_use_twitter Dependency Parse dependency_parse explain_document_ml import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;The Paris metro will soon enter the 21st century, ditching single-use paper tickets for rechargeable electronic cards.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_ml,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 7 more fields] ++--+--+--+--+--+--+--+--+ | id| text| document| sentence| token| checked| lemmas| stems| pos| ++--+--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...| | 2|The Paris metro w...|[[document, 0, 11...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...| ++--+--+--+--+--+--+--+--+ */ explain_document_dl import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_dl,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 10 more fields] ++--+--+--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| checked| lemma| stem| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[token, 0, 5, Go...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...| | 2|The Paris metro w...|[[document, 0, 11...|[[token, 0, 2, Th...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 4, 8, Pa...| ++--+--+--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Google, TensorFlow] | |[Donald John Trump, United States]| +-+ */ recognize_entities_dl import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;recognize_entities_dl&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(entity_recognizer_dl,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 6 more fields] ++--+--+--+--+--+--+--+ | id| text| document| sentence| token| embeddings| ner| ner_converter| ++--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 5, Go...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...| | 2|Donald John Trump...|[[document, 0, 92...|[[document, 0, 92...|[[token, 0, 5, Do...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 16, D...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Google, TensorFlow] | |[Donald John Trump, United States]| +-+ */ onto_recognize_entities_sm Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the OntoNotes corpus and supports the identification of 18 entities. import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London, from 2008 to 2016, before rejoining Parliament. &quot;), (2, &quot;A little less than a decade later, dozens of self-driving startups have cropped up while automakers around the world clamor, wallet in hand, to secure their place in the fast-moving world of fully automated transportation.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;onto_recognize_entities_sm&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.1.0 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(onto_recognize_entities_sm,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 6 more fields] ++--+--+--+--+--+--+ | id| text| document| token| embeddings| ner| entities| ++--+--+--+--+--+--+ | 1|Johnson first ent...|[[document, 0, 17...|[[token, 0, 6, Jo...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 6, Jo...| | 2|A little less tha...|[[document, 0, 22...|[[token, 0, 0, A,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 32, A...| ++--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* ++ |result | ++ |[Johnson, first, 2001, Parliament, eight years, London, 2008 to 2016, Parliament]| |[A little less than a decade later, dozens] | ++ */ onto_recognize_entities_lg Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the OntoNotes corpus and supports the identification of 18 entities. import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London, from 2008 to 2016, before rejoining Parliament. &quot;), (2, &quot;A little less than a decade later, dozens of self-driving startups have cropped up while automakers around the world clamor, wallet in hand, to secure their place in the fast-moving world of fully automated transportation.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;onto_recognize_entities_lg&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.1.0 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(onto_recognize_entities_lg,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 6 more fields] ++--+--+--+--+--+--+ | id| text| document| token| embeddings| ner| entities| ++--+--+--+--+--+--+ | 1|Johnson first ent...|[[document, 0, 17...|[[token, 0, 6, Jo...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 6, Jo...| | 2|A little less tha...|[[document, 0, 22...|[[token, 0, 0, A,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 32, A...| ++--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Johnson, first, 2001, Parliament, eight years, London, 2008, 2016, Parliament]| |[A little less than a decade later, dozens] | +-+ */ match_datetime DateMatcher yyyy/MM/dd import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;I would like to come over and see you in 01/02/2019.&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;match_datetime&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(match_datetime,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 4 more fields] ++--+--+--+--+--+ | id| text| document| sentence| token| date| ++--+--+--+--+--+ | 1|I would like to c...|[[document, 0, 51...|[[document, 0, 51...|[[token, 0, 0, I,...|[[date, 41, 50, 2...| | 2|Donald John Trump...|[[document, 0, 92...|[[document, 0, 92...|[[token, 0, 5, Do...|[[date, 24, 36, 1...| ++--+--+--+--+--+ */ annotation.select(&quot;date.result&quot;).show(false) /* ++ |result | ++ |[2019/01/02]| |[1946/06/14]| ++ */ match_pattern RegexMatcher (match phone numbers) import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;You should call Mr. Jon Doe at +33 1 79 01 22 89&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;match_pattern&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(match_pattern,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 4 more fields] ++--+--+--+--+--+ | id| text| document| sentence| token| regex| ++--+--+--+--+--+ | 1|You should call M...|[[document, 0, 47...|[[document, 0, 47...|[[token, 0, 2, Yo...|[[chunk, 31, 47, ...| ++--+--+--+--+--+ */ annotation.select(&quot;regex.result&quot;).show(false) /* +-+ |result | +-+ |[+33 1 79 01 22 89]| +-+ */ match_chunks The pipeline uses regex &lt;DT/&gt;?/&lt;JJ/&gt;*&lt;NN&gt;+ import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;The book has many chapters&quot;), (2, &quot;the little yellow dog barked at the cat&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;match_chunks&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(match_chunks,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 5 more fields] ++--+--+--+--+--+--+ | id| text| document| sentence| token| pos| chunk| ++--+--+--+--+--+--+ | 1|The book has many...|[[document, 0, 25...|[[document, 0, 25...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[chunk, 0, 7, Th...| | 2|the little yellow...|[[document, 0, 38...|[[document, 0, 38...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[chunk, 0, 20, t...| ++--+--+--+--+--+--+ */ annotation.select(&quot;chunk.result&quot;).show(false) /* +--+ |result | +--+ |[The book] | |[the little yellow dog, the cat]| +--+ */ French Pipelines Name Explain Document Large explain_document_lg Explain Document Medium explain_document_md Entity Recognizer Large entity_recognizer_lg Entity Recognizer Medium entity_recognizer_md Feature Description NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura POS Trained by PerceptronApproach annotator on the Universal Dependencies Size Model size indicator, md and lg. The large pipeline uses glove_840B_300 and the medium uses glove_6B_300 WordEmbeddings French explain_document_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_lg&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,fr,public/models) testData: org.apache.spark.sql.DataFrame = [id: bigint, text: string] annotation: org.apache.spark.sql.DataFrame = [id: bigint, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[token, 0, 12, C...|[[pos, 0, 12, ADV...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[token, 0, 7, Em...|[[pos, 0, 7, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /*+-+ |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ French explain_document_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_md&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_md,fr,public/models) testData: org.apache.spark.sql.DataFrame = [id: bigint, text: string] annotation: org.apache.spark.sql.DataFrame = [id: bigint, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[token, 0, 12, C...|[[pos, 0, 12, ADV...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[token, 0, 7, Em...|[[pos, 0, 7, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, au CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ French entity_recognizer_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_lg&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ French entity_recognizer_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_md&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /*+-+ |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, au CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ Italian Pipelines Name Explain Document Large explain_document_lg Explain Document Medium explain_document_md Entity Recognizer Large entity_recognizer_lg Entity Recognizer Medium entity_recognizer_md Feature Description NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Lemma Trained by Lemmatizer annotator on DXC Technology dataset POS Trained by PerceptronApproach annotator on the Universal Dependencies Size Model size indicator, md and lg. The large pipeline uses glove_840B_300 and the medium uses glove_6B_300 WordEmbeddings Italian explain_document_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_lg&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[token, 0, 1, La...|[[pos, 0, 1, DET,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 3, 6, FI...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[token, 0, 4, Re...|[[pos, 0, 4, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +--+ |result | +--+ |[FIFA, Zidane, Materazzi] | |[Reims, Domani, Mondiali femminili]| +--+ */ Italian explain_document_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_md&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[token, 0, 1, La...|[[pos, 0, 1, DET,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 9, La...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[token, 0, 4, Re...|[[pos, 0, 4, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[La FIFA, Zidane, Materazzi]| |[Reims, Domani, Mondiali] | +-+ */ Italian entity_recognizer_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_lg&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 3, 6, FI...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +--+ |result | +--+ |[FIFA, Zidane, Materazzi] | |[Reims, Domani, Mondiali femminili]| +--+ */ Italian entity_recognizer_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_md&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 9, La...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[La FIFA, Zidane, Materazzi]| |[Reims, Domani, Mondiali] | +-+ */ Spanish Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.4.0 es   Download Explain Document Medium explain_document_md 2.4.0 es   Download Explain Document Large explain_document_lg 2.4.0 es   Download Entity Recognizer Small entity_recognizer_sm 2.4.0 es   Download Entity Recognizer Medium entity_recognizer_md 2.4.0 es   Download Entity Recognizer Large entity_recognizer_lg 2.4.0 es   Download Feature Description Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Size Model size indicator, sm, md, and lg. The small pipelines use glove_100d, the medium pipelines use glove_6B_300, and large pipelines use glove_840B_300 WordEmbeddings Russian Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.4.4 ru   Download Explain Document Medium explain_document_md 2.4.4 ru   Download Explain Document Large explain_document_lg 2.4.4 ru   Download Entity Recognizer Small entity_recognizer_sm 2.4.4 ru   Download Entity Recognizer Medium entity_recognizer_md 2.4.4 ru   Download Entity Recognizer Large entity_recognizer_lg 2.4.4 ru   Download Feature Description Lemma Trained by Lemmatizer annotator on the Universal Dependencies POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Dutch Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 nl   Download Explain Document Medium explain_document_md 2.5.0 nl   Download Explain Document Large explain_document_lg 2.5.0 nl   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 nl   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 nl   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 nl   Download Norwegian Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 no   Download Explain Document Medium explain_document_md 2.5.0 no   Download Explain Document Large explain_document_lg 2.5.0 no   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 no   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 no   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 no   Download Polish Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 pl   Download Explain Document Medium explain_document_md 2.5.0 pl   Download Explain Document Large explain_document_lg 2.5.0 pl   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 pl   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 pl   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 pl   Download Portuguese Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 pt   Download Explain Document Medium explain_document_md 2.5.0 pt   Download Explain Document Large explain_document_lg 2.5.0 pt   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 pt   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 pt   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 pt   Download Multi-language Pipeline Name Build lang Description Offline LanguageDetectorDL detect_language_7 2.5.2 xx   Download LanguageDetectorDL detect_language_20 2.5.2 xx   Download The model with 7 languages: Czech, German, English, Spanish, French, Italy, and Slovak The model with 20 languages: Bulgarian, Czech, German, Greek, English, Spanish, Finnish, French, Croatian, Hungarian, Italy, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Swedish, Turkish, and Ukrainian How to use Online To use Spark NLP pretrained pipelines, you can call PretrainedPipeline with pipeline’s name and its language (default is en): pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;) Same in Scala val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;, lang=&quot;en&quot;) Offline If you have any trouble using online pipelines or models in your environment (maybe it’s air-gapped), you can directly download them for offline use. After downloading offline models/pipelines and extracting them, here is how you can use them iside your code (the path could be a shared storage like HDFS in a cluster): val advancedPipeline = PipelineModel.load(&quot;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&quot;) // To use the loaded Pipeline for prediction advancedPipeline.transform(predictionDF)",
    "url": "/docs/en/pipelines",
    "relUrl": "/docs/en/pipelines"
  },
  "52": {
    "id": "52",
    "title": "Preannotations with Spark NLP",
    "content": "Annotation Lab offers out-of-the-box support for Named Entity Recognition, Classification and Assertion Status Preannotations. Those are extremely useful for bootstraping any annotation project, as the annotation team does not start the labeling from scratch but can leverage the existing knowledge transfer from domain experts to models. This way, the annotation efforts are significantly reduced. For running preannotation on one or several tasks, the Project Owner or the Manager must select the target tasks and can click on the Preannotate Button from the upper right side of the Tasks List Page. This will display a popup with information regarding the last deployment including the list of models deployed and the labels they predict. This information is very important, especially when multiple users are doing training and deployment in parallel. So before doing preannotations on your tasks, carefully check the list of currently deployed models and their labels. If needed, users can deploy the models defined in the current project (based on the current Labeling Config) by clicking the “Deploy” button. After the deployment is complete, the preannotation can be triggered. Pretrained Models On the project setup screen you can find a Spark NLP pipeline config widget which lists all available models together with the labels those are predicting. By simply selecting the relevant labels for your project and clicking the add button you can add the predefined labels to your project and take advantage of the Spark NLP auto labeling capabilities. In the below example we are reusing the posology model that comes with 7 labels related to drugs.",
    "url": "/docs/en/preannotations",
    "relUrl": "/docs/en/preannotations"
  },
  "53": {
    "id": "53",
    "title": "Project Setup",
    "content": "To create a new project, click on the Create Project button on the Home Page and choose a name for it. The project can include a short description and annotation instructions/guidelines. Share your project with the annotation team When working in teams, projects can be shared with other team members. The user who creates a project is called a Project Owner. He/She has complete visibility and ownership of the project for its entire lifecycle. If the Project Owner is removed from the user database, then all his/her projects are transfered to a new project owner. The Project Owner can edit the project configuration, can import/export tasks, can create a project team that will work on his project and can access project analytics. When defining the project team, a project owner has access to three distinct roles: Annotator, Reviewer, and Manager. These are very useful for most of the workflows that our users follow. An Annotator is able to see the tasks which have been assigned to him or her and can create annotations on the documents. The Reviewer is able to see the work of the annotators and approve it or reject in case he finds issues that need to be solved. The Manager is able to see the work of the Annotators and of the Reviewers and he can assign tasks to team members. This is useful for eliminating work overlap and for a better management of the work load. To add a user to your project team, navigate to the Project Setup page. On the Manage Project Team tab, start typing the name of a user in the available text box. This will populate a list of available users having the username start with the caracters you typed. From the dropdown select the user you want to add to your team. Select a role for the user and click on the “Add to team” button. Supported Project Types We currently support multiple predefined project configurations. The most popular ones are Text Classification and Named Entity Recognition. Create a setup from scratch or customize a predefined one according to your needs. For customizing a predefined configuration, click on the corresponding link in the table above and then navigate to the Labeling config widget and manually edit/update it to contain the labels you need. After you finish editing the labels you want to define for your project click the “Save” button. Text Classification Project The Annotation Lab offers two types of classification widgets: The first one supports single choice labels. You can activate it by choosing Text Classification from the list of predefined projects. The labels can be changed by directly editing them in the Labeling Config XML style widget. The updates will be automatically reflected in the right side preview. The second configuration offers support for multi-class classification. It can be activated by clicking on the Multi classification link in the list of predefined configurations. This option will add to the labeling config widget multiple checkboxes, grouped by headers. The names of the choices and well as the headers are customizable. You can also add new choices if necessary. Named Entity Recognition Project Named Entity Recognition (NER) refers to the identification and classification of entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. The Annotation Lab offers support for two types of labels: Simple labels for NER or assertion models; Binary relations for relation extraction models. Assertion Status Project The syntax for defining an Assertion Status label is the same as for the NER labels, with an additional attribute - assertion which should be set to true (see example below). This is convention defined by Annotation Lab users which we exploited for identifying the labels to include in the training and prediction of Assertion Models. A simple Labeling Config with Assertion Status defined should look like the following: &lt;View&gt; &lt;Labels name=&quot;ner&quot; toName=&quot;text&quot;&gt; &lt;Label value=&quot;Medicine&quot; background=&quot;orange&quot; hotkey=&quot;_&quot;/&gt; &lt;Label value=&quot;Condition&quot; background=&quot;orange&quot; hotkey=&quot;_&quot;/&gt; &lt;Label value=&quot;Procedure&quot; background=&quot;green&quot; hotkey=&quot;8&quot;/&gt; &lt;Label value=&quot;Absent&quot; assertion=&quot;true&quot; background=&quot;red&quot; hotkey=&quot;Z&quot;/&gt; &lt;Label value=&quot;Past&quot; assertion=&quot;true&quot; background=&quot;red&quot; hotkey=&quot;X&quot;/&gt; &lt;/Labels&gt; &lt;View style=&quot;height: 250px; overflow: auto;&quot;&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;/&gt; &lt;/View&gt; &lt;/View&gt; Notice assertion=”true” in Absent and Past labels, which marks each of those labels as Assertion Status Labels. Labels customization Names of the labels must be carefully chosen so they are easy to understand by the annotators. Highlighting colors can be assigned to each labels by either specifying the color name or the color code. Shortcuts keys can be assigned to each label to make the annotation process easier and faster. &lt;Labels name=&quot;ner&quot; toName=&quot;text&quot;&gt; &lt;Label value=&quot;Cancer&quot; background=&quot;red&quot; hotkey=&quot;c&quot;/&gt; &lt;Label value=&quot;TumorSize&quot; background=&quot;blue&quot; hotkey=&quot;t&quot;/&gt; &lt;Label value=&quot;TumorLocation&quot; background=&quot;pink&quot; hotkey=&quot;l&quot;/&gt; &lt;Label value=&quot;Symptom&quot; background=&quot;#dda0dd&quot; hotkey=&quot;z&quot;/&gt; &lt;/Labels&gt; Relations The Annotation Lab also offers support for relation extraction. Relations are introduced by simply specifying their label. &lt;Relations&gt; &lt;Relation value=&quot;CancerSize&quot; /&gt; &lt;Relation value=&quot;CancerLocation&quot;/&gt; &lt;Relation value=&quot;MetastasisLocation&quot;/&gt; &lt;/Relations&gt; No other constraints can currently be enforced on the labels linked by the defined relations so the annotators must be extra careful and follow the annotation guidelines that specify how the defined relations can be used.",
    "url": "/docs/en/project_setup",
    "relUrl": "/docs/en/project_setup"
  },
  "54": {
    "id": "54",
    "title": "Quick Start",
    "content": "Requirements &amp; Setup Spark NLP is built on top of Apache Spark 3.x. For using Spark NLP you need: Java 8 Apache Spark 3.1.x (or 3.0.x, or 2.4.x, or 2.3.x) It is recommended to have basic knowledge of the framework and a working environment before using Spark NLP. Please refer to Spark documentation to get started with Spark. Install Spark NLP in Python Scala and Java Databricks EMR Join our Slack channel Join our channel, to ask for help and share your feedback. Developers and users can help each other getting started here. Spark NLP Slack Spark NLP in Action Make sure to check out our demos built by Streamlit to showcase Spark NLP in action: Spark NLP Demo Spark NLP Workshop If you prefer learning by example, check this repository: Spark NLP Workshop It is full of fresh examples and even a docker container if you want to skip installation. Below, you can follow into a more theoretical and thorough quick start guide. Where to go next If you need more detailed information about how to install Spark NLP you can check the Installation page Detailed information about Spark NLP concepts, annotators and more may be found HERE",
    "url": "/docs/en/quickstart",
    "relUrl": "/docs/en/quickstart"
  },
  "55": {
    "id": "55",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/recognize_biomedical_entities",
    "relUrl": "/recognize_biomedical_entities"
  },
  "56": {
    "id": "56",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/recognize_clinical_entities",
    "relUrl": "/recognize_clinical_entities"
  },
  "57": {
    "id": "57",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/recognize_entitie",
    "relUrl": "/recognize_entitie"
  },
  "58": {
    "id": "58",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/recognize_social_determinants",
    "relUrl": "/recognize_social_determinants"
  },
  "59": {
    "id": "59",
    "title": "Spark NLP release notes",
    "content": "3.1.0 John Snow Labs Spark-NLP 3.1.0: Over 2600+ new models and pipelines in 200+ languages, new DistilBERT, RoBERTa, and XLM-RoBERTa transformers, support for external Transformers, and lots more! Overview We are very excited to release Spark NLP 🚀 3.1.0! This is one of our biggest releases with lots of models, pipelines, and groundworks for future features that we are so proud to share it with our community. Spark NLP 3.1.0 comes with over 2600+ new pretrained models and pipelines in over 200+ languages, new DistilBERT, RoBERTa, and XLM-RoBERTa annotators, support for HuggingFace 🤗 (Autoencoding) models in Spark NLP, and extends support for new Databricks and EMR instances. As always, we would like to thank our community for their feedback, questions, and feature requests. Major features and improvements NEW: Introducing DistiBertEmbeddings annotator. DistilBERT is a small, fast, cheap, and light Transformer model trained by distilling BERT base. It has 40% fewer parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances NEW: Introducing RoBERTaEmbeddings annotator. RoBERTa (Robustly Optimized BERT-Pretraining Approach) models deliver state-of-the-art performance on NLP/NLU tasks and a sizable performance improvement on the GLUE benchmark. With a score of 88.5, RoBERTa reached the top position on the GLUE leaderboard NEW: Introducing XlmRoBERTaEmbeddings annotator. XLM-RoBERTa (Unsupervised Cross-lingual Representation Learning at Scale) is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl data with 100 different languages. It also outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model NEW: Introducing support for HuggingFace exported models in equivalent Spark NLP annotators. Starting this release, you can easily use the saved_model feature in HuggingFace within a few lines of codes and import any BERT, DistilBERT, RoBERTa, and XLM-RoBERTa models to Spark NLP. We will work on the remaining annotators and extend this support to the rest with each release - For more information please visit this discussion NEW: Migrate MarianTransformer to BatchAnnotate to control the throughput when you are on accelerated hardware such as GPU to fully utilize it Upgrade to TensorFlow v2.4.1 with native support for Java to take advantage of many optimizations for CPU/GPU and new features/models introduced in TF v2.x Update to CUDA11 and cuDNN 8.0.2 for GPU support Implement ModelSignatureManager to automatically detect inputs, outputs, save and restore tensors from SavedModel in TF v2. This allows Spark NLP 3.1.x to extend support for external Encoders such as HuggingFace and TF Hub (coming soon!) Implement a new BPE tokenizer for RoBERTa and XLM models. This tokenizer will use the custom tokens from Tokenizer or RegexTokenizer and generates token pieces, encodes, and decodes the results Welcoming new Databricks runtimes to our Spark NLP family: Databricks 8.1 ML &amp; GPU Databricks 8.2 ML &amp; GPU Databricks 8.3 ML &amp; GPU Welcoming a new EMR 6.x series to our Spark NLP family: EMR 6.3.0 (Apache Spark 3.1.1 / Hadoop 3.2.1) Added examples to Spark NLP Scaladoc Models and Pipelines Spark NLP 3.1.0 comes with over 2600+ new pretrained models and pipelines in over 200 languages available for Windows, Linux, and macOS users. Featured Transformers Model Name Build Lang BertEmbeddings bert_base_dutch_cased 3.1.0 nl BertEmbeddings bert_base_german_cased 3.1.0 de BertEmbeddings bert_base_german_uncased 3.1.0 de BertEmbeddings bert_base_italian_cased 3.1.0 it BertEmbeddings bert_base_italian_uncased 3.1.0 it BertEmbeddings bert_base_turkish_cased 3.1.0 tr BertEmbeddings bert_base_turkish_uncased 3.1.0 tr BertEmbeddings chinese_bert_wwm 3.1.0 zh BertEmbeddings bert_base_chinese 3.1.0 zh DistilBertEmbeddings distilbert_base_cased 3.1.0 en DistilBertEmbeddings distilbert_base_uncased 3.1.0 en DistilBertEmbeddings distilbert_base_multilingual_cased 3.1.0 xx RoBertaEmbeddings roberta_base 3.1.0 en RoBertaEmbeddings roberta_large 3.1.0 en RoBertaEmbeddings distilroberta_base 3.1.0 en XlmRoBertaEmbeddings xlm_roberta_base 3.1.0 xx XlmRoBertaEmbeddings twitter_xlm_roberta_base 3.1.0 xx Featured Translation Models Model Name Build Lang MarianTransformer Chinese to Vietnamese 3.1.0 xx MarianTransformer Chinese to Ukrainian 3.1.0 xx MarianTransformer Chinese to Dutch 3.1.0 xx MarianTransformer Chinese to English 3.1.0 xx MarianTransformer Chinese to Finnish 3.1.0 xx MarianTransformer Chinese to Italian 3.1.0 xx MarianTransformer Yoruba to English 3.1.0 xx MarianTransformer Yapese to French 3.1.0 xx MarianTransformer Waray to Spanish 3.1.0 xx MarianTransformer Ukrainian to English 3.1.0 xx MarianTransformer Hindi to Urdu 3.1.0 xx MarianTransformer Italian to Ukrainian 3.1.0 xx MarianTransformer Italian to Icelandic 3.1.0 xx Transformers in Spark NLP Import hundreds of models in different languages to Spark NLP Spark NLP HuggingFace Notebooks BertEmbeddings HuggingFace in Spark NLP - BERT BertSentenceEmbeddings HuggingFace in Spark NLP - BERT Sentence DistilBertEmbeddings HuggingFace in Spark NLP - DistilBERT RoBertaEmbeddings HuggingFace in Spark NLP - RoBERTa XlmRoBertaEmbeddings HuggingFace in Spark NLP - XLM-RoBERTa The complete list of all 3700+ models &amp; pipelines in 200+ languages is available on Models Hub. Backward compatibility We have updated our MarianTransformer annotator to be compatible with TF v2 models. This change is not compatible with previous models/pipelines. However, we have updated and uploaded all the models and pipelines for 3.1.x release. You can either use MarianTransformer.pretrained(MODEL_NAME) and it will automatically download the compatible model or you can visit Models Hub to download the compatible models for offline use via MarianTransformer.load(PATH) Documentation HuggingFace to Spark NLP Models Hub with new models Spark NLP publications Spark NLP in Action Spark NLP documentation Spark NLP Workshop notebooks Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.1.0 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.1.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.1.0 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.1.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.1.0 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.1.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.1.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.1.0 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.1.0.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.1.0.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.1.0.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.1.0.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.1.0.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.1.0.jar 3.0.3 John Snow Labs Spark-NLP 3.0.3: New T5 features for longer and more accurate text generation, new multi-lingual models &amp; pipelines, bug fixes, and other improvements! Overview We are glad to release Spark NLP 3.0.3! We have added some new features to our T5 Transformer annotator to help with longer and more accurate text generation, trained some new multi-lingual models and pipelines in Farsi, Hebrew, Korean, and Turkish, and fixed some bugs in this release. As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add 6 new features to T5Transformer for longer and better text generation doSample: Whether or not to use sampling; use greedy decoding otherwise temperature: The value used to module the next token probabilities topK: The number of highest probability vocabulary tokens to keep for top-k-filtering topP: If set to float &lt; 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation repetitionPenalty: The parameter for repetition penalty. 1.0 means no penalty. See CTRL: A Conditional Transformer Language Model for Controllable Generation paper for more details noRepeatNgramSize: If set to int &gt; 0, all ngrams of that size can only occur once Spark NLP 3.0.3 is compatible with the new Databricks 8.2 (ML) runtime Spark NLP 3.0.3 is compatible with the new EMR 5.33.0 (with Zeppelin 0.9.0) release Bug Fixes Fix ChunkEmbeddings Array out of bounds exception https://github.com/JohnSnowLabs/spark-nlp/pull/2796 Fix pretrained tfhub_use_multi and tfhub_use_multi_lg models in UniversalSentenceEncoder https://github.com/JohnSnowLabs/spark-nlp/pull/2827 Fix anchorDateMonth in Python that resulted in 1 additional month and case sensitivity to some relative dates like next friday or next Friday https://github.com/JohnSnowLabs/spark-nlp/pull/2848 Models and Pipelines New multilingual models and pipelines for Farsi, Hebrew, Korean, and Turkish Model Name Build Lang ClassifierDLModel classifierdl_bert_news 3.0.2 tr UniversalSentenceEncoder tfhub_use_multi 3.0.0 xx UniversalSentenceEncoder tfhub_use_multi_lg 3.0.0 xx Pipeline Name Build Lang PretrainedPipeline recognize_entities_dl 3.0.0 fa PretrainedPipeline explain_document_lg 3.0.2 he PretrainedPipeline explain_document_lg 3.0.2 ko The complete list of all 1100+ models &amp; pipelines in 192+ languages is available on Models Hub. Documentation and Notebooks Add a new Offline section to docs Installing Spark NLP and Spark OCR in air-gapped networks (offline mode) Models Hub with new models Spark NLP publications Spark NLP in Action Spark NLP documentation Spark NLP Workshop notebooks Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.0.3 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.3 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.3 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.0.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.0.3 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.3&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.0.3.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.0.3.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.0.3.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.0.3.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.0.3.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.0.3.jar 3.0.2 John Snow Labs Spark-NLP 3.0.2: New multilingual models, confidence scores for entities and all NER tags, first support for community models, bug fixes, and other improvements! Overview We are glad to release Spark NLP 3.0.2! We have added some new features, improvements, trained some new multi-lingual models, and fixed some bugs in this release. As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Experimental support for community models and pipelines (uploaded by users) https://github.com/JohnSnowLabs/spark-nlp/pull/2743 Provide confidence scores for all available tags in NerDLModel and NerCrfModel https://github.com/JohnSnowLabs/spark-nlp/pull/2760 NerDLModel and NerCrfModel before 3.0.2 [[named_entity, 0, 4, B-LOC, [word -&gt; Japan, confidence -&gt; 0.9998], []] Now in Spark NLP 3.0.2 [[named_entity, 0, 4, B-LOC, [B-LOC -&gt; 0.9998, I-ORG -&gt; 0.0, I-MISC -&gt; 0.0, I-LOC -&gt; 0.0, I-PER -&gt; 0.0, B-MISC -&gt; 0.0, B-ORG -&gt; 1.0E-4, word -&gt; Japan, O -&gt; 0.0, B-PER -&gt; 0.0], []] * Calculate confidence score for entities in NerConverter https://github.com/JohnSnowLabs/spark-nlp/pull/2784 [chunk, 30, 41, Barack Obama, [entity -&gt; PERSON, sentence -&gt; 0, chunk -&gt; 0, confidence -&gt; 0.94035] Enhancements * Add proper conversions for Scala 2.11/2.12 in ContextSpellChecker to use models from Spark 2.x in Spark 3.x https://github.com/JohnSnowLabs/spark-nlp/pull/2758 * Refactoring SentencePiece encoding in AlbertEmbeddings and XlnetEmbeddings https://github.com/JohnSnowLabs/spark-nlp/pull/2777 Bug Fixes * Fix an exception in NerConverter when the documents/sentences don&#39;t carry the used tokens in NerDLModel https://github.com/JohnSnowLabs/spark-nlp/pull/2784 thanks to @rahulraina7 * Fix an exception in AlbertEmbeddings when the original tokens are longer than the piece tokens https://github.com/JohnSnowLabs/spark-nlp/pull/2777 Models and Pipelines New multilingual models for `Afrikaans`, `Welsh`, `Maltese`, `Tamil`, and `Vietnamese` | Model | Name | Build | Lang | |:--|:-|:--|:| | PerceptronModel | [pos_afribooms](https://nlp.johnsnowlabs.com/2021/04/06/pos_afribooms_af.html) | 3.0.0 | `af` | LemmatizerModel | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_cy.html) | 3.0.0 |`cy` | LemmatizerModel | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_mt.html)| 3.0.0 | `mt` | LemmatizerModel | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_af.html)| 3.0.0 | `af` | LemmatizerModel | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_ta.html)| 3.0.0 | `ta` | LemmatizerModel | [lemma](https://nlp.johnsnowlabs.com/2021/04/02/lemma_vi.html)| 3.0.0 | `vi` The complete list of all 1100+ models &amp; pipelines in 192+ languages is available on [Models Hub](https://nlp.johnsnowlabs.com/models). Documentation and Notebooks * Add a new [Offline section](https://github.com/JohnSnowLabs/spark-nlp#offline) to docs * [Models Hub](https://nlp.johnsnowlabs.com/models) with new models * [Spark NLP publications](https://medium.com/spark-nlp) * [Spark NLP in Action](https://nlp.johnsnowlabs.com/demo) * [Spark NLP documentation](https://nlp.johnsnowlabs.com/docs/en/quickstart) * [Spark NLP Workshop](https://github.com/JohnSnowLabs/spark-nlp-workshop) notebooks * [Spark NLP training certification notebooks](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings/Public) for Google Colab and Databricks * [Spark NLP Display](https://github.com/JohnSnowLabs/spark-nlp-display) for visualization of different types of annotations * [Discussions](https://github.com/JohnSnowLabs/spark-nlp/discussions) Engage with other community members, share ideas, and show off how you use Spark NLP! Installation **Python** shell #PyPI pip install spark-nlp==3.0.2 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.2 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.2 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.0.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.0.2 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.2&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.0.2.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.0.2.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.0.2.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.0.2.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.0.2.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.0.2.jar 3.0.1 John Snow Labs Spark-NLP 3.0.1: New parameters in Normalizer, bug fixes and other improvements! Overview We are glad to release Spark NLP 3.0.1! We have made some improvements, added 1 line bash script to set up Google Colab and Kaggle kernel for Spark NLP 3.x, and improved our Models Hub filtering to help our community to have easier access to over 1300 pretrained models and pipelines in over 200+ languages. As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add minLength and maxLength parameters to Normalizer annotator https://github.com/JohnSnowLabs/spark-nlp/pull/2614 1 line to setup Google Colab 1 line to setup Kaggle Kernel Enhancements Adjust shading rule for amazon AWS to support sub-projects from Spark NLP Fat JAR https://github.com/JohnSnowLabs/spark-nlp/pull/2613 Fix the missing variables in BertSentenceEmbeddings https://github.com/JohnSnowLabs/spark-nlp/pull/2615 Restrict loading Sentencepiece ops only to supported models https://github.com/JohnSnowLabs/spark-nlp/pull/2623 improve dependency management and resolvers https://github.com/JohnSnowLabs/spark-nlp/pull/2479 Documentation and Notebooks Models Hub with new models Spark NLP publications Spark NLP in Action Spark NLP documentation Spark NLP Workshop notebooks Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.0.1 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.1 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.1 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.0.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.0.1 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.1&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.0.1.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.0.1.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark24-assembly-3.0.1.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark24-assembly-3.0.1.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.0.1.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-spark23-assembly-3.0.1.jar 3.0.0 John Snow Labs Spark-NLP 3.0.0: Supporting Spark 3.x, Scala 2.12, more Databricks runtimes, more EMR versions, performance improvements &amp; lots more Overview We are very excited to release Spark NLP 3.0.0! This has been one of the biggest releases we have ever done and we are so proud to share this with our community. Spark NLP 3.0.0 extends the support for Apache Spark 3.0.x and 3.1.x major releases on Scala 2.12 with both Hadoop 2.7. and 3.2. We will support all 4 major Apache Spark and PySpark releases of 2.3.x, 2.4.x, 3.0.x, and 3.1.x helping the community to migrate from earlier Apache Spark versions to newer releases without being worried about Spark NLP support. As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Support for Apache Spark and PySpark 3.0.x on Scala 2.12 Support for Apache Spark and PySpark 3.1.x on Scala 2.12 Migrate to TensorFlow v2.3.1 with native support for Java to take advantage of many optimizations for CPU/GPU and new features/models introduced in TF v2.x Welcoming 9x new Databricks runtimes to our Spark NLP family: Databricks 7.3 Databricks 7.3 ML GPU Databricks 7.4 Databricks 7.4 ML GPU Databricks 7.5 Databricks 7.5 ML GPU Databricks 7.6 Databricks 7.6 ML GPU Databricks 8.0 Databricks 8.0 ML (there is no GPU in 8.0) Databricks 8.1 Beta Welcoming 2x new EMR 6.x series to our Spark NLP family: EMR 6.1.0 (Apache Spark 3.0.0 / Hadoop 3.2.1) EMR 6.2.0 (Apache Spark 3.0.1 / Hadoop 3.2.1) Starting Spark NLP 3.0.0 the default packages for CPU and GPU will be based on Apache Spark 3.x and Scala 2.12 (spark-nlp and spark-nlp-gpu will be compatible only with Apache Spark 3.x and Scala 2.12) Starting Spark NLP 3.0.0 we have two new packages to support Apache Spark 2.4.x and Scala 2.11 (spark-nlp-spark24 and spark-nlp-gpu-spark24) Spark NLP 3.0.0 still is and will be compatible with Apache Spark 2.3.x and Scala 2.11 (spark-nlp-spark23 and spark-nlp-gpu-spark23) Adding a new param to sparknlp.start() function in Python for Apache Spark 2.4.x (spark24=True) Adding a new param to adjust Driver memory in sparknlp.start() function (memory=&quot;16G&quot;) Performance Improvements Introducing a new batch annotation technique implemented in Spark NLP 3.0.0 for NerDLModel, BertEmbeddings, and BertSentenceEmbeddings annotators to radically improve prediction/inferencing performance. From now on the batchSize for these annotators means the number of rows that can be fed into the models for prediction instead of sentences per row. You can control the throughput when you are on accelerated hardware such as GPU to fully utilize it. Performance achievements by using Spark NLP 3.0.0 vs. Spark NLP 2.7.x on CPU and GPU: (Performed on a Databricks cluster) Spark NLP 3.0.0 vs. 2.7.x PySpark 3.x on CPU PySpark 3.x on GPU BertEmbeddings (bert-base) +10% +550% (6.6x) BertEmbeddings (bert-large) +12%. +690% (7.9x) NerDLModel +185% +327% (4.2x) Breaking changes There are only 6 annotators that are not compatible to be used with both Scala 2.11 (Apache Spark 2.3 and Apache Spark 2.4) and Scala 2.12 (Apache Spark 3.x) at the same time. You can either train and use them on Apache Spark 2.3.x/2.4.x or train and use them on Apache Spark 3.x. TokenizerModel PerceptronApproach (POS Tagger) WordSegmenter DependencyParser TypedDependencyParser NerCrfModel The rest of our models/pipelines can be used on all Apache Spark and Scala major versions without any issue. We have already retrained and uploaded all the exiting pretrained for Part of Speech and WordSegmenter models in Apache Spark 3.x and Scala 2.12. We will continue doing this as we see existing models which are not compatible with Apache Spark 3.x and Scala 2.12. NOTE: You can always use the .pretrained() function which seamlessly will find the compatible and most recent models to download for you. It will download and extract them in your HOME DIRECTORY ~/cached_pretrained/. More info: https://github.com/JohnSnowLabs/spark-nlp/discussions/2562 Deprecated Starting Spark NLP 3.0.0 release we no longer publish any artifacts on spark-packages and we continue to host all the artifacts only Maven Repository. Documentation Apache Spark Migration Guide PySpark Migration Guide “Spark NLP: Natural language understanding at scale” published paper Spark NLP publications Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Spark NLP Workshop notebooks Models Hub with new models Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==3.0.0 Spark Packages spark-nlp on Apache Spark 3.0.x and 3.1.x (Scala 2.12 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:3.0.0 spark-nlp on Apache Spark 2.4.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark24_2.11:3.0.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark24_2.11:3.0.0 spark-nlp on Apache Spark 2.3.x (Scala 2.11 only): spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:3.0.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:3.0.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu-spark23_2.11:3.0.0 Maven spark-nlp on Apache Spark 3.0.x and 3.1.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark24_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.0.0.jar GPU on Apache Spark 3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.0.0.jar CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-3.0.0.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-3.0.0.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-3.0.0.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-gpu-assembly-3.0.0.jar 2.7.5 John Snow Labs Spark-NLP 2.7.5: Supporting more EMR versions and other improvements! Overview We are glad to release Spark NLP 2.7.5 release! Starting this release we no longer ship Hadoop AWS and AWS Java SDK dependencies. This change allows users to avoid any conflicts in AWS environments and also results in more EMR 5.x versions support. As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Support more EMR 5.x versions emr-5.20.0 emr-5.21.0 emr-5.21.1 emr-5.22.0 emr-5.23.0 emr-5.24.0 emr-5.24.1 emr-5.25.0 emr-5.26.0 emr-5.27.0 emr-5.28.0 emr-5.29.0 emr-5.30.0 emr-5.30.1 emr-5.31.0 emr-5.32.0 Bugfixes Fix BigDecimal error in NerDL when includeConfidence is true Enhancements Shade Hadoop AWS and AWS Java SDK dependencies Documentation and Notebooks “Spark NLP: Natural language understanding at scale” published paper Spark NLP publications Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Spark NLP Workshop notebooks Models Hub with new models New Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==2.7.5 #Conda conda install -c johnsnowlabs spark-nlp==2.7.5 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.5 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.5 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.5 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.5 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.5&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-2.7.5.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-2.7.5.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-2.7.5.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-gpu-assembly-2.7.5.jar 2.7.4 John Snow Labs Spark-NLP 2.7.4: New Bengali NER and Word Embeddings models, new Intent Prediction models, bug fixes, and other improvements! Overview We are glad to release Spark NLP 2.7.4 release! This release comes with a few bug fixes, enhancements, and 4 new pretrained models. As always, we would like to thank our community for their feedback, questions, and feature requests. Bugfixes Fix Tensors with a 0 dimension issue in ClassifierDL and SentimentDL thanks to @pradeepgowda21 https://github.com/JohnSnowLabs/spark-nlp/pull/2288 Fix index error in TokenAssembler https://github.com/JohnSnowLabs/spark-nlp/pull/2289 Fix MatchError in DateMatcher and MultiDateMatcher annotators https://github.com/JohnSnowLabs/spark-nlp/pull/2297 Fix setOutputAsArray and its default value for valueSplitSymbol in Finisher annotator https://github.com/JohnSnowLabs/spark-nlp/pull/2290 Enhancements Implement missing frequencyThreshold and ambiguityThreshold params in WordSegmenterApproach annotator https://github.com/JohnSnowLabs/spark-nlp/pull/2308 Downgrade Hadoop from 3.2 to 2.7 which caused an issue with S3 https://github.com/JohnSnowLabs/spark-nlp/pull/2310 Update Apache HTTP Client https://github.com/JohnSnowLabs/spark-nlp/pull/2312 Models and Pipelines Model Name Build Lang NerDLModel bengali_cc_300d 2.7.3 bn WordEmbeddingsModel bengaliner_cc_300d 2.7.3 bn NerDLModel nerdl_snips_100d 2.7.3 en ClassifierDLModel classifierdl_use_snips 2.7.3 en The complete list of all 1100+ models &amp; pipelines in 192+ languages is available on Models Hub. Compatibility Starting today, we have moved all of the Fat JARs hosted on our S3 to the auxdata.johnsnowlabs.com/public/jars/ location. We have also fixed the links in the previous releases. Documentation and Notebooks “Spark NLP: Natural language understanding at scale” published paper Spark NLP publications Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Spark NLP Workshop notebooks Models Hub with new models New Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==2.7.4 #Conda conda install -c johnsnowlabs spark-nlp==2.7.4 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.4 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.4 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.4 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.4 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.4&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-2.7.4.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-2.7.4.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-2.7.4.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-gpu-assembly-2.7.4.jar 2.7.3 John Snow Labs Spark-NLP 2.7.3: 18 new state-of-the-art transformer-based OntoNotes models and pipelines, new support for Bengali NER and Hindi Word Embeddings, and other improvements! Overview We are glad to release Spark NLP 2.7.3 release! This release comes with a couple of bug fixes, enhancements, and 20+ pretrained models and pipelines including support for Bengali Named Entity Recognition, Hindi Word Embeddings, and state-of-the-art transformer based OntoNotes models and pipelines! As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add anchorDateYear, anchorDateMonth, and anchorDateDay to DateMatcher and MultiDateMatcher to be used for relative dates extraction Bugfixes Fix the default value for action parameter in Python wrapper for DocumentNormalizer annotator Fix Lemmatizer pretrained models published in 2021 Enhancements Improve T5Transformer performance on documents with many sentences Models and Pipelines This release comes with support for Bengali Named Entity Recognition and Hindi Word Embeddings. We are also announcing the release of 18 new state-of-the-art transformer based OntoNotes models and pipelines! These models are trained by using Transformers pretrained models such as BERT Tiny, BERT Mini, BERT Small, BERT Medium, BERT Base, BERT Large, ELECTRA Small, ELECTRA Base, and ELECTRA Large. New Bengali and Hindi Models: Model Name Build Lang NerDLModel ner_jifs_glove_840B_300d 2.7.0 bn WordEmbeddingsModel hindi_cc_300d 2.7.0 hi New Transformer-based OntoNotes Models &amp; Pipelines: Model Name Build Lang NerDLModel onto_small_bert_L2_128 2.7.0 en NerDLModel onto_small_bert_L4_256 2.7.0 en NerDLModel onto_small_bert_L4_512 2.7.0 en NerDLModel onto_small_bert_L8_512 2.7.0 en NerDLModel onto_bert_base_cased 2.7.0 en NerDLModel onto_bert_large_cased 2.7.0 en NerDLModel onto_electra_small_uncased 2.7.0 en NerDLModel onto_electra_base_uncased 2.7.0 en NerDLModel onto_electra_large_uncased 2.7.0 en Pipeline Build Lang onto_recognize_entities_bert_tiny 2.7.0 en onto_recognize_entities_bert_mini 2.7.0 en onto_recognize_entities_bert_small 2.7.0 en onto_recognize_entities_bert_medium 2.7.0 en onto_recognize_entities_bert_base 2.7.0 en onto_recognize_entities_bert_large 2.7.0 en onto_recognize_entities_electra_small 2.7.0 en onto_recognize_entities_electra_base 2.7.0 en onto_recognize_entities_electra_large 2.7.0 en OntoNotes Benchmark: SYSTEM YEAR LANGUAGE ONTONOTES Spark NLP v2.7 2021 Python/Scala/Java/R 90.0 (test F1) 92.5 (dev F1) spaCy 3.0 (RoBERTa) 2020 Python 89.7 (dev F1) Stanza (StanfordNLP) 2020 Python 88.8 (dev F1) Flair 2018 Python 89.7 Documentation and Notebooks “Spark NLP: Natural language understanding at scale” published paper Spark NLP publications Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Spark NLP Workshop notebooks Models Hub with new models New Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==2.7.3 #Conda conda install -c johnsnowlabs spark-nlp==2.7.3 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.3 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.3 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.3&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-2.7.3.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-2.7.3.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-2.7.3.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-gpu-assembly-2.7.3.jar 2.7.2 John Snow Labs Spark-NLP 2.7.2: New multilingual models, GPU support to train a Spell Checker, bug fixes, and other improvements! Overview We are glad to release Spark NLP 2.7.2 release! This release comes with a couple of bug fixes, enhancements, and 25+ pretrained models and pipelines in Amharic, Bengali, Bhojpuri, Japanese, and Korean languages. As always, we would like to thank our community for their feedback, questions, and feature requests. Bugfixes Fix casual mask calculations resulting in bad translation in MarianTransformer https://github.com/JohnSnowLabs/spark-nlp/pull/2149 Fix Serialization issue in the cluster while training ContextSpellChecker https://github.com/JohnSnowLabs/spark-nlp/pull/2167 Fix calculating CHUNK spans based on the sentences’ boundaries in RegexMatcher https://github.com/JohnSnowLabs/spark-nlp/pull/2150 Enhancements Add GPU support for training ContextSpellChecker https://github.com/JohnSnowLabs/spark-nlp/pull/2167 Adding Scalatest ability to control tests by tags https://github.com/JohnSnowLabs/spark-nlp/pull/2156 Models and Pipelines The 2.7.x release comes with over 720+ new pretrained models and pipelines available for Windows, Linux, and macOS users. New Text Classifier models: Model Name Build Lang SentimentDLModel sentimentdl_glove_imdb 2.7.1 en SentimentDLModel sentimentdl_use_imdb 2.7.1 en SentimentDLModel sentimentdl_use_twitter 2.7.1 en ClassifierDLMode classifierdl_use_spam 2.7.1 en ClassifierDLModel classifierdl_use_sarcasm 2.7.1 en ClassifierDLModel classifierdl_use_fakenews 2.7.1 en ClassifierDLModel classifierdl_use_emotion 2.7.1 en ClassifierDLModel classifierdl_use_cyberbullying 2.7.1 en MultiClassifierDLModel multiclassifierdl_use_toxic_sm 2.7.1 en MultiClassifierDLModel multiclassifierdl_use_toxic 2.7.1 en MultiClassifierDLModel multiclassifierdl_use_e2e 2.7.1 en New Multi-lingual models: Some of the new models for Amharic, Bengali, Bhojpuri, Japanese, and Korean languages Model Name Build Lang SentimentDLModel sentiment_jager_use 2.7.1 th SentimentDLModel sentimentdl_urduvec_imdb 2.7.1 ur LemmatizerModel lemma 2.7.0 am LemmatizerModel lemma 2.7.0 bn LemmatizerModel lemma 2.7.0 bh LemmatizerModel lemma 2.7.0 ja LemmatizerModel lemma 2.7.0 ko PerceptronModel pos_ud_att 2.7.0 am PerceptronModel pos_ud_bhtb 2.7.0 bh PerceptronModel pos_msri 2.7.0 bn PerceptronModel pos_lst20 2.7.0 th WordSegmenterModel wordseg_best 2.7.0 th NerDLModel ner_lst20_glove_840B_300d 2.7.0 th The complete list of all 1100+ models &amp; pipelines in 192+ languages is available on Models Hub. Documentation and Notebooks Spark NLP publications Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Spark NLP Workshop notebooks Models Hub with new models New Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==2.7.2 #Conda conda install -c johnsnowlabs spark-nlp==2.7.2 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.2 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.2 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-2.7.2.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-2.7.2.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-2.7.2.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-gpu-assembly-2.7.2.jar 2.7.1 John Snow Labs Spark-NLP 2.7.1: New T5 models, new TREC pipelines, bug fixes, and other improvements! Overview We are glad to release Spark NLP 2.7.1 towards making 2.7 stable release! This release comes with 3 new optimized T5 models, 2 new TREC pipelines, a few bug fixes, and other improvements. We highly recommend all users to upgrade to 2.7.1 for more stability while paying attention to the backward compatibility notice. As always, we would like to thank our community for their feedback, questions, and feature requests. Bugfixes Fix default pretrained model T5Transformer https://github.com/JohnSnowLabs/spark-nlp/pull/2068 Fix default pretrained model WordSegmenter https://github.com/JohnSnowLabs/spark-nlp/pull/2068 Fix missing reference to WordSegmenter in ResourceDwonloader https://github.com/JohnSnowLabs/spark-nlp/pull/2068 Fix T5Transformer models crashing due to unknown task https://github.com/JohnSnowLabs/spark-nlp/pull/2070 Fix the issue of reading and writing ClassifierDL, SentimentDL, and MultiClassifierDL models introduced in the 2.7.0 release https://github.com/JohnSnowLabs/spark-nlp/pull/2081 Enhancements Export new T5 models with optimized Encoder/Decoder https://github.com/JohnSnowLabs/spark-nlp/pull/2074 Add support for alternative tagging with the positional parser in RegexTokenizer https://github.com/JohnSnowLabs/spark-nlp/pull/2077 Refactor AssertAnnotations https://github.com/JohnSnowLabs/spark-nlp/pull/2079 Backward compatibility In order to fix the issue of Classifiers in the clusters, we had to export new TF models and change the read/write functions of these annotators. This caused any model trained prior to the 2.7.0 release not to be compatible with 2.7.1 and require retraining including pre-trained models. (we are re-training all the existing text classification models with 2.7.1) Models and Pipelines The 2.7.x release comes with over 720+ new pretrained models and pipelines available for Windows, Linux, and macOS users. New optimized T5 models: Model Name Build Lang T5Transformer t5_small 2.7.1 en T5Transformer t5_base 2.7.1 en T5Transformer google_t5_small_ssm_nq 2.7.1 en Question classification of open-domain and fact-based questions: Model Name Build Lang ClassifierDL classifierdl_use_trec6 2.7.1 en ClassifierDL classifierdl_use_trec50 2.7.1 en ClassifierDL classifierdl_use_trec6_pipeline 2.7.1 en ClassifierDL classifierdl_use_trec50_pipeline 2.7.1 en The complete list of all 1100+ models &amp; pipelines in 192+ languages is available on Models Hub. Documentation and Notebooks New RegexTokenizer Notebook New T5 Summarization &amp; Question Answering Notebook New T5 Translation Notebook New Marian Translation Notebook Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Update the entire spark-nlp-workshop notebooks Update Models Hub with new models New Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==2.7.1 #Conda conda install -c johnsnowlabs spark-nlp==2.7.1 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.1 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.1 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-2.7.1.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-gpu-assembly-2.7.1.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-assembly-2.7.1.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-spark23-gpu-assembly-2.7.1.jar 2.7.0 John Snow Labs Spark-NLP 2.7.0: New T5 and MarianMT seq2seq transformers, detect up to 375 languages, word segmentation, over 720+ models and pipelines, support for 192+ languages, and many more! Overview We are very excited to release Spark NLP 2.7.0! This has been one of the biggest releases we have ever done that we are so proud to share it with our community! In this release, we are bringing support to state-of-the-art Seq2Seq and Text2Text transformers. We have developed annotators for Google T5 (Text-To-Text Transfer Transformer) and MarianMNT for Neural Machine Translation with over 646 pretrained models and pipelines. This release also comes with a refactored and brand new models for language detection and identification. They are more accurate, faster, and support up to 375 languages. The 2.7.0 release has over 720+ new pretrained models and pipelines while extending our support of multi-lingual models to 192+ languages such as Chinese, Japanese, Korean, Arabic, Persian, Urdu, and Hebrew. As always, we would like to thank our community for their feedback and support. Major features and improvements NEW: Introducing MarianTransformer annotator for machine translation based on MarianNMT models. Marian is an efficient, free Neural Machine Translation framework mainly being developed by the Microsoft Translator team (646+ pretrained models &amp; pipelines in 192+ languages) NEW: Introducing T5Transformer annotator for Text-To-Text Transfer Transformer (Google T5) models to achieve state-of-the-art results on multiple NLP tasks such as Translation, Summarization, Question Answering, Sentence Similarity, and so on NEW: Introducing brand new and refactored language detection and identification models. The new LanguageDetectorDL is faster, more accurate, and supports up to 375 languages NEW: Introducing WordSegmenter annotator, a trainable annotator for word segmentation of languages without any rule-based tokenization such as Chinese, Japanese, or Korean NEW: Introducing DocumentNormalizer annotator cleaning content from HTML or XML documents, applying either data cleansing using an arbitrary number of custom regular expressions either data extraction following the different parameters NEW: Spark NLP Display for visualization of different types of annotations Add support for new multi-lingual models in UniversalSentenceEncoder annotator Add support to Lemmatizer to be trained directly from a DataFrame instead of a text file Add training helper to transform CoNLL-U into Spark NLP annotator type columns Bugfixes and Enhancements Fix all the known issues in ClassifierDL, SentimentDL, and MultiClassifierDL annotators in a Cluster NerDL enhancements for memory optimization and logging during the training with the test dataset SentenceEmbeddings annotator now reuses the storageRef of any embeddings used in prior Fix dropout in SentenceDetectorDL models for more deterministic results. Both English and Multi-lingual models are retrained for the 2.7.0 release Fix Python dataType Annotation Upgrade to Apache Spark 2.4.7 Models and Pipelines The 2.7.0 release comes with over 720+ new pretrained models and pipelines available for Windows, Linux, and macOS users. Selected T5 and Marian models Model Name Build Lang T5Transformer google_t5_small_ssm_nq 2.7.0 en T5Transformer t5_small 2.7.0 en MarianTransformer opus-mt-en-aav 2.7.0 xx MarianTransformer opus-mt-en-af 2.7.0 xx MarianTransformer opus-mt-en-afa 2.7.0 xx MarianTransformer opus-mt-en-alv 2.7.0 xx MarianTransformer opus-mt-en-ar 2.7.0 xx MarianTransformer opus-mt-en-az 2.7.0 xx MarianTransformer opus-mt-en-bat 2.7.0 xx MarianTransformer opus-mt-en-bcl 2.7.0 xx MarianTransformer opus-mt-en-bem 2.7.0 xx MarianTransformer opus-mt-en-ber 2.7.0 xx MarianTransformer opus-mt-en-bg 2.7.0 xx MarianTransformer opus-mt-en-bi 2.7.0 xx MarianTransformer opus-mt-en-bnt 2.7.0 xx MarianTransformer opus-mt-en-bzs 2.7.0 xx MarianTransformer opus-mt-en-ca 2.7.0 xx MarianTransformer opus-mt-en-ceb 2.7.0 xx MarianTransformer opus-mt-en-cel 2.7.0 xx MarianTransformer opus-mt-en-chk 2.7.0 xx MarianTransformer opus-mt-en-cpf 2.7.0 xx MarianTransformer opus-mt-en-cpp 2.7.0 xx MarianTransformer opus-mt-en-crs 2.7.0 xx MarianTransformer opus-mt-en-cs 2.7.0 xx MarianTransformer opus-mt-en-cus 2.7.0 xx MarianTransformer opus-mt-en-cy 2.7.0 xx MarianTransformer opus-mt-en-da 2.7.0 xx MarianTransformer opus-mt-en-de 2.7.0 xx MarianTransformer opus-mt-en-dra 2.7.0 xx MarianTransformer opus-mt-en-ee 2.7.0 xx MarianTransformer opus-mt-en-efi 2.7.0 xx MarianTransformer opus-mt-en-el 2.7.0 xx MarianTransformer opus-mt-en-eo 2.7.0 xx MarianTransformer opus-mt-en-es 2.7.0 xx MarianTransformer opus-mt-en-et 2.7.0 xx MarianTransformer opus-mt-en-eu 2.7.0 xx MarianTransformer opus-mt-en-euq 2.7.0 xx MarianTransformer opus-mt-en-fi 2.7.0 xx MarianTransformer opus-mt-en-fiu 2.7.0 xx MarianTransformer opus-mt-en-fj 2.7.0 xx MarianTransformer opus-mt-en-fr 2.7.0 xx MarianTransformer opus-mt-en-ga 2.7.0 xx MarianTransformer opus-mt-en-gaa 2.7.0 xx MarianTransformer opus-mt-en-gem 2.7.0 xx MarianTransformer opus-mt-en-gil 2.7.0 xx MarianTransformer opus-mt-en-gl 2.7.0 xx MarianTransformer opus-mt-en-gmq 2.7.0 xx MarianTransformer opus-mt-en-gmw 2.7.0 xx MarianTransformer opus-mt-en-grk 2.7.0 xx MarianTransformer opus-mt-en-guw 2.7.0 xx MarianTransformer opus-mt-en-gv 2.7.0 xx MarianTransformer opus-mt-en-ha 2.7.0 xx MarianTransformer opus-mt-en-he 2.7.0 xx MarianTransformer opus-mt-en-hi 2.7.0 xx MarianTransformer opus-mt-en-hil 2.7.0 xx MarianTransformer opus-mt-en-ho 2.7.0 xx MarianTransformer opus-mt-en-ht 2.7.0 xx MarianTransformer opus-mt-en-hu 2.7.0 xx MarianTransformer opus-mt-en-hy 2.7.0 xx MarianTransformer opus-mt-en-id 2.7.0 xx MarianTransformer opus-mt-en-ig 2.7.0 xx MarianTransformer opus-mt-en-iir 2.7.0 xx MarianTransformer opus-mt-en-ilo 2.7.0 xx MarianTransformer opus-mt-en-inc 2.7.0 xx MarianTransformer opus-mt-en-ine 2.7.0 xx MarianTransformer opus-mt-en-is 2.7.0 xx MarianTransformer opus-mt-en-iso 2.7.0 xx MarianTransformer opus-mt-en-it 2.7.0 xx MarianTransformer opus-mt-en-itc 2.7.0 xx MarianTransformer opus-mt-en-jap 2.7.0 xx MarianTransformer opus-mt-en-kg 2.7.0 xx MarianTransformer opus-mt-en-kj 2.7.0 xx MarianTransformer opus-mt-en-kqn 2.7.0 xx MarianTransformer opus-mt-en-kwn 2.7.0 xx MarianTransformer opus-mt-en-kwy 2.7.0 xx MarianTransformer opus-mt-en-lg 2.7.0 xx MarianTransformer opus-mt-en-ln 2.7.0 xx MarianTransformer opus-mt-en-loz 2.7.0 xx MarianTransformer opus-mt-en-lu 2.7.0 xx MarianTransformer opus-mt-en-lua 2.7.0 xx MarianTransformer opus-mt-en-lue 2.7.0 xx MarianTransformer opus-mt-en-lun 2.7.0 xx MarianTransformer opus-mt-en-luo 2.7.0 xx MarianTransformer opus-mt-en-lus 2.7.0 xx MarianTransformer opus-mt-en-map 2.7.0 xx MarianTransformer opus-mt-en-mfe 2.7.0 xx MarianTransformer opus-mt-en-mg 2.7.0 xx MarianTransformer opus-mt-en-mh 2.7.0 xx MarianTransformer opus-mt-en-mk 2.7.0 xx MarianTransformer opus-mt-en-mkh 2.7.0 xx MarianTransformer opus-mt-en-ml 2.7.0 xx MarianTransformer opus-mt-en-mos 2.7.0 xx MarianTransformer opus-mt-en-mr 2.7.0 xx MarianTransformer opus-mt-en-mt 2.7.0 xx MarianTransformer opus-mt-en-mul 2.7.0 xx MarianTransformer opus-mt-en-ng 2.7.0 xx MarianTransformer opus-mt-en-nic 2.7.0 xx MarianTransformer opus-mt-en-niu 2.7.0 xx MarianTransformer opus-mt-en-nl 2.7.0 xx MarianTransformer opus-mt-en-nso 2.7.0 xx MarianTransformer opus-mt-en-ny 2.7.0 xx MarianTransformer opus-mt-en-nyk 2.7.0 xx Chinese models Models Name Build Lang WordSegmenterModel wordseg_weibo 2.7.0 zh WordSegmenterModel wordseg_pku 2.7.0 zh WordSegmenterModel wordseg_msra 2.7.0 zh WordSegmenterModel wordseg_large 2.7.0 zh WordSegmenterModel wordseg_ctb9 2.7.0 zh PerceptronModel pos_ud_gsd 2.7.0 zh PerceptronModel pos_ctb9 2.7.0 zh NerDLModel ner_msra_bert_768d 2.7.0 zh NerDLModel ner_weibo_bert_768d 2.7.0 zh Arabic models Models Name Build Lang StopWordsCleaner stopwords_ar 2.7.0 ar LemmatizerModel lemma 2.7.0 ar PerceptronModel pos_ud_padt 2.7.0 ar WordEmbeddingsModel arabic_w2v_cc_300d 2.7.0 ar NerDLModel aner_cc_300d 2.7.0 ar Persian models Models Name Build Lang StopWordsCleaner stopwords_fa 2.7.0 fa LemmatizerModel lemma 2.7.0 fa PerceptronModel pos_ud_perdt 2.7.0 fa WordEmbeddingsModel persian_w2v_cc_300d 2.7.0 fa NerDLModel personer_cc_300d 2.7.0 fa The complete list of all 1100+ models &amp; pipelines in 192+ languages is available on Models Hub. Documentation and Notebooks Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Update the entire spark-nlp-workshop notebooks Update Models Hub with new models New Spark NLP Display for visualization of different types of annotations Discussions Engage with other community members, share ideas, and show off how you use Spark NLP! Installation Python #PyPI pip install spark-nlp==2.7.0 #Conda conda install -c johnsnowlabs spark-nlp==2.7.0 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.7.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.7.0 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.7.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.7.0 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.7.0.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.7.0.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.7.0.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.7.0.jar 2.6.5 John Snow Labs Spark-NLP 2.6.5: A few bug fixes and other improvements! Overview We are glad to release Spark NLP 2.6.5! This release comes with a few bug fixes before we move to a new major version. As always, we would like to thank our community for their feedback, questions, and feature requests. Bugfixes Fix a bug in batching sentences in BertSentenceEmbeddings Fix AttributeError when trying to load a saved EmbeddingsFinisher in Python Enhancements Improve handling exceptions in DocumentAssmbler when the user uses a corrupted DataFrame Documentation and Notebooks Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Update the entire spark-nlp-workshop notebooks A brand new 1-hour Spark NLP workshop Update Model Hubs with new models Installation Python #PyPI pip install spark-nlp==2.6.5 #Conda conda install -c johnsnowlabs spark-nlp==2.6.5 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.5 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.5 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.5 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.5 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.5&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.6.5.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.6.5.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.6.5.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.6.5.jar 2.6.4 John Snow Labs Spark-NLP 2.6.4: A few bug fixes and other improvements! Overview We are glad to release Spark NLP 2.6.4! This release comes with a few bug fixes before we move to a new major version. As always, we would like to thank our community for their feedback, questions, and feature requests. Bugfixes Fix loading from a local folder with no access to the cache folder https://github.com/JohnSnowLabs/spark-nlp/pull/1141 Fix NullPointerException in DocumentAssembler when there are null in the rows https://github.com/JohnSnowLabs/spark-nlp/pull/1145 Fix dynamic padding in BertSentenceEmbeddings https://github.com/JohnSnowLabs/spark-nlp/pull/1162 Documentation and Notebooks Spark NLP in Action Spark NLP training certification notebooks for Google Colab and Databricks Spark NLP documentation Update the entire spark-nlp-workshop notebooks A brand new 1-hour Spark NLP workshop Update Model Hubs with new models Installation Python #PyPI pip install spark-nlp==2.6.4 #Conda conda install -c johnsnowlabs spark-nlp==2.6.4 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.4 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.4 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.4 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.4 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.4&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.6.4.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.6.4.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.6.4.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.6.4.jar 2.6.3 John Snow Labs Spark-NLP 2.6.3: New refactored NerDL with memory optimization, bug fixes, and other improvements! Overview We are glad to release Spark NLP 2.6.3! This release comes with a refactored NerDLApproach that allows users to train their NerDL on any size of the CoNLL file regardless of the memory limitations. We also have some bug fixes and improvements in the 2.6.3 release. Spark NLP has a new and improved Website for its documentation and models. We have been moving our 330+ pretrained models and pipelines into Models Hubs and we would appreciate your feedback! :) As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add enableMemoryOptimizer to allow training NerDLApproach on a dataset larger than the memory Add option to explode sentences in SentenceDetectorDL Enhancements Improve POS (AveragedPerceptron) performance Improve Norvig Spell Checker performance Bugfixes Fix SentenceDetectorDL unsupported model error in pretrained function Fix a race condition in LRU algorithm that can cause NullPointerException during a LightPipeline operation with embeddings Fix max sequence length calculation in BertEmbeddings and BertSentenceEmbeddings Fix threshold in YakeModel on Python side Documentation and Notebooks Spark NLP training certification notebooks for Google Colab and Databricks A brand new 1-hour Spark NLP workshop Update Model Hubs with new models in Spark NLP 2.6.3 Update documentation for release of Spark NLP 2.6.3 Update the entire spark-nlp-workshop notebooks for Spark NLP 2.6.3 Installation Python #PyPI pip install spark-nlp==2.6.3 #Conda conda install -c johnsnowlabs spark-nlp==2.6.3 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.3 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.3 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.3 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.3 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.3&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.6.3.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.6.3.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.6.3.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.6.3.jar 2.6.2 John Snow Labs Spark-NLP 2.6.2: New SentenceDetectorDL, improved BioBERT models, new Models Hub, and other improvements! Overview We are glad to release Spark NLP 2.6.2! This release comes with a brand new SentenceDetectorDL (SDDL) that is based on a general-purpose neural network model for sentence boundary detection with higher accuracy. In addition, we are releasing 12 new and improved BioBERT models for BertEmbeddings and BertSentenceEembeddings used for sequence and text classifications. Spark NLP has a new and improved Website for its documentation and models. We have been moving our 330+ pretrained models and pipelines into Models Hubs and we would appreciate your feedback! :) As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Introducing a new SentenceDetectorDL (trainable) for sentence boundary detection Dedicated Models Hub for all pretrained models &amp; pipelines Enhancements Improved BioBERT models quality for BertEmbeddings (it achieves higher accuracy in sequence classification) Improved Sentence BioBERT models quality for BertSentenceEmbeddings (it achieves higher accuracy in text classification) Improve loadSavedModel in BertEmbeddings and BertSentenceEmbeddings Add unit test to MultiClassifierDL annotator Better error handling in SentimentDLApproach Bugfixes Fix BERT LaBSE model for BertSentenceEmbeddings Fix loadSavedModel for BertSentenceEmbeddings in Python Deprecations DeepSentenceDetector is deprecated in favor of SentenceDetectorDL Models Model Name Build Lang BertEmbeddings biobert_pubmed_base_cased 2.6.2 en BertEmbeddings biobert_pubmed_large_cased 2.6.2 en BertEmbeddings biobert_pmc_base_cased 2.6.2 en BertEmbeddings biobert_pubmed_pmc_base_cased 2.6.2 en BertEmbeddings biobert_clinical_base_cased 2.6.2 en BertEmbeddings biobert_discharge_base_cased 2.6.2 en BertSentenceEmbeddings sent_biobert_pubmed_base_cased 2.6.2 en BertSentenceEmbeddings sent_biobert_pubmed_large_cased 2.6.2 en BertSentenceEmbeddings sent_biobert_pmc_base_cased 2.6.2 en BertSentenceEmbeddings sent_biobert_pubmed_pmc_base_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_clinical_base_cased 2.6.2 en BertSentenceEmbeddings sent_biobert_discharge_base_cased 2.6.2 en The complete list of all 330+ models &amp; pipelines in 46+ languages is available here. Documentation and Notebooks New notebook to use SentenceDetectorDL Update Model Hubs with new models in Spark NLP 2.6.2 Update documentation for release of Spark NLP 2.6.2 Update the entire spark-nlp-workshop notebooks for Spark NLP 2.6.2 Installation Python #PyPI pip install spark-nlp==2.6.2 #Conda conda install -c johnsnowlabs spark-nlp==2.6.2 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.2 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.2 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.2 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.6.2.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.6.2.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.6.2.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.6.2.jar 2.6.1 John Snow Labs Spark-NLP 2.6.1: New Portuguese BERT models, import any BERT models to Spark NLP, and a bug-fix for ClassifierDL Overview We are glad to release Spark NLP 2.6.1! This release comes with new Portuguese BERT models, a notebook to demonstrate how to import any BERT models to Spark NLP, and a fix for ClassifierDL which was introduced in the 2.6.0 release that resulted in low accuracy during training. As always, we would like to thank our community for their feedback, questions, and feature requests. Bugfixes Fix lower accuracy in ClassifierDL introduced in 2.6.0 release Models and Pipelines Model Name Build Lang BertEmbeddings bert_portuguese_base_cased 2.6.0 pt BertEmbeddings bert_portuguese_large_cased 2.6.0 pt The complete list of all 330+ models &amp; pipelines in 46+ languages is available here. Documentation and Notebooks New notebook to import BERT checkpoints into Spark NLP New notebook to extract keywords Update documentation for release of Spark NLP 2.6.1 Update the entire spark-nlp-models repository with new pre-trained models and pipelines Update the entire spark-nlp-workshop notebooks for Spark NLP 2.6.1 Installation Python #PyPI pip install spark-nlp==2.6.1 #Conda conda install -c johnsnowlabs spark-nlp==2.6.1 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.1 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.1 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.1 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.1 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.6.1.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.6.1.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.6.1.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.6.1.jar 2.6.0 John Snow Labs Spark-NLP 2.6.0: New multi-label classifier, BERT sentence embeddings, unsupervised keyword extractions, over 110 pretrained pipelines, models, Transformers, and more! Overview We are very excited to finally release Spark NLP 2.6.0! This has been one of the biggest releases we have ever made and we are so proud to share it with our community! This release comes with a brand new MultiClassifierDL for multi-label text classification, BertSentenceEmbeddings with 42 models, unsupervised keyword extractions annotator, and adding 28 new pretrained Transformers such as Small BERT, CovidBERT, ELECTRA, and the state-of-the-art language-agnostic BERT Sentence Embedding model(LaBSE). The 2.6.0 release has over 110 new pretrained models, pipelines, and Transformers with extending full support for Danish, Finnish, and Swedish languages. Major features and improvements NEW: A new MultiClassifierDL annotator for multi-label text classification built by using Bidirectional GRU and CNN inside TensorFlow that supports up to 100 classes NEW: A new BertSentenceEmbeddings annotator with 42 available pre-trained models for sentence embeddings used in SentimentDL, ClassifierDL, and MultiClassifierDL annotators NEW: A new YakeModel annotator for an unsupervised, corpus-independent, domain, and language-independent and single-document keyword extraction algorithm NEW: Integrate 24 new Small BERT models where the smallest model is 24x times smaller and 28x times faster compare to BERT base models NEW: Add 3 new ELECTRA small, base, and large models NEW: Add 4 new Finnish BERT models for BertEmbeddings and BertSentenceEmbeddings Improve BertEmbeddings memory consumption by 30% Improve BertEmbeddings performance by more than 70% with a new built-in dynamic shape inputs Remove the poolingLayer parameter in BertEmbeddings in favor of sequence_output that is provided by TF Hub models for new BERT models Add validation loss, validation accuracy, validation F1, and validation True Positive Rate during the training in MultiClassifierDL Add parameter to enable/disable list detection in SentenceDetector Unify the loggings in ClassifierDL and SentimentDL during training Bugfixes Fix Tokenization bug with Bigrams in the exception list Fix the versioning error in second SBT projects causing models not being found via pretrained function Fix logging to file in NerDLApproach, ClassifierDL, SentimentDL, and MultiClassifierDL on HDFS Fix ignored modified tokens in BertEmbeddings, now it will consider modified tokens instead of originals Models and Pipelines This release comes with over 100+ new pretrained models and pipelines available for Windows, Linux, and macOS users. The complete list of all 330+ models &amp; pipelines in 46+ languages is available here. Some selected Transformers: Model Name Build Lang BertEmbeddings electra_small_uncased 2.6.0 en BertEmbeddings electra_base_uncased 2.6.0 en BertEmbeddings electra_large_uncased 2.6.0 en BertEmbeddings covidbert_large_uncased 2.6.0 en BertEmbeddings small_bert_L2_128 2.6.0 en BertEmbeddings small_bert_L4_128 2.6.0 en BertEmbeddings small_bert_L6_128 2.6.0 en BertEmbeddings small_bert_L8_128 2.6.0 en BertEmbeddings small_bert_L10_128 2.6.0 en BertEmbeddings small_bert_L12_128 2.6.0 en BertEmbeddings small_bert_L2_256 2.6.0 en BertEmbeddings small_bert_L4_256 2.6.0 en BertEmbeddings small_bert_L6_256 2.6.0 en BertEmbeddings small_bert_L8_256 2.6.0 en BertEmbeddings small_bert_L10_256 2.6.0 en BertEmbeddings small_bert_L12_256 2.6.0 en BertEmbeddings small_bert_L2_512 2.6.0 en BertEmbeddings small_bert_L4_512 2.6.0 en BertEmbeddings small_bert_L6_512 2.6.0 en BertEmbeddings small_bert_L8_512 2.6.0 en BertEmbeddings small_bert_L10_512 2.6.0 en BertEmbeddings small_bert_L12_512 2.6.0 en BertEmbeddings small_bert_L2_768 2.6.0 en BertEmbeddings small_bert_L4_768 2.6.0 en BertEmbeddings small_bert_L6_768 2.6.0 en BertEmbeddings small_bert_L8_768 2.6.0 en BertEmbeddings small_bert_L10_768 2.6.0 en BertEmbeddings small_bert_L12_768 2.6.0 en BertEmbeddings bert_finnish_cased 2.6.0 fi BertEmbeddings bert_finnish_uncased 2.6.0 fi BertSentenceEmbeddings sent_bert_finnish_cased 2.6.0 fi BertSentenceEmbeddings sent_bert_finnish_uncased 2.6.0 fi BertSentenceEmbeddings sent_electra_small_uncased 2.6.0 en BertSentenceEmbeddings sent_electra_base_uncased 2.6.0 en BertSentenceEmbeddings sent_electra_large_uncased 2.6.0 en BertSentenceEmbeddings sent_bert_base_uncased 2.6.0 en BertSentenceEmbeddings sent_bert_base_cased 2.6.0 en BertSentenceEmbeddings sent_bert_large_uncased 2.6.0 en BertSentenceEmbeddings sent_bert_large_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_pubmed_base_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_pubmed_large_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_pmc_base_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_pubmed_pmc_base_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_clinical_base_cased 2.6.0 en BertSentenceEmbeddings sent_biobert_discharge_base_cased 2.6.0 en BertSentenceEmbeddings sent_covidbert_large_uncased 2.6.0 en BertSentenceEmbeddings sent_small_bert_L2_128 2.6.0 en BertSentenceEmbeddings sent_small_bert_L4_128 2.6.0 en BertSentenceEmbeddings sent_small_bert_L6_128 2.6.0 en BertSentenceEmbeddings sent_small_bert_L8_128 2.6.0 en BertSentenceEmbeddings sent_small_bert_L10_128 2.6.0 en BertSentenceEmbeddings sent_small_bert_L12_128 2.6.0 en BertSentenceEmbeddings sent_small_bert_L2_256 2.6.0 en BertSentenceEmbeddings sent_small_bert_L4_256 2.6.0 en BertSentenceEmbeddings sent_small_bert_L6_256 2.6.0 en BertSentenceEmbeddings sent_small_bert_L8_256 2.6.0 en BertSentenceEmbeddings sent_small_bert_L10_256 2.6.0 en BertSentenceEmbeddings sent_small_bert_L12_256 2.6.0 en BertSentenceEmbeddings sent_small_bert_L2_512 2.6.0 en BertSentenceEmbeddings sent_small_bert_L4_512 2.6.0 en BertSentenceEmbeddings sent_small_bert_L6_512 2.6.0 en BertSentenceEmbeddings sent_small_bert_L8_512 2.6.0 en BertSentenceEmbeddings sent_small_bert_L10_512 2.6.0 en BertSentenceEmbeddings sent_small_bert_L12_512 2.6.0 en BertSentenceEmbeddings sent_small_bert_L2_768 2.6.0 en BertSentenceEmbeddings sent_small_bert_L4_768 2.6.0 en BertSentenceEmbeddings sent_small_bert_L6_768 2.6.0 en BertSentenceEmbeddings sent_small_bert_L8_768 2.6.0 en BertSentenceEmbeddings sent_small_bert_L10_768 2.6.0 en BertSentenceEmbeddings sent_small_bert_L12_768 2.6.0 en BertSentenceEmbeddings sent_bert_multi_cased 2.6.0 xx BertSentenceEmbeddings labse 2.6.0 xx Danish pipelines Pipeline Name Build Lang Explain Document Small explain_document_sm 2.6.0 da Explain Document Medium explain_document_md 2.6.0 da Explain Document Large explain_document_lg 2.6.0 da Entity Recognizer Small entity_recognizer_sm 2.6.0 da Entity Recognizer Medium entity_recognizer_md 2.6.0 da Entity Recognizer Large entity_recognizer_lg 2.6.0 da Finnish pipelines Pipeline Name Build Lang Explain Document Small explain_document_sm 2.6.0 fi Explain Document Medium explain_document_md 2.6.0 fi Explain Document Large explain_document_lg 2.6.0 fi Entity Recognizer Small entity_recognizer_sm 2.6.0 fi Entity Recognizer Medium entity_recognizer_md 2.6.0 fi Entity Recognizer Large entity_recognizer_lg 2.6.0 fi Swedish pipelines Pipeline Name Build Lang Explain Document Small explain_document_sm 2.6.0 sv Explain Document Medium explain_document_md 2.6.0 sv Explain Document Large explain_document_lg 2.6.0 sv Entity Recognizer Small entity_recognizer_sm 2.6.0 sv Entity Recognizer Medium entity_recognizer_md 2.6.0 sv Entity Recognizer Large entity_recognizer_lg 2.6.0 sv Documentation and Notebooks New notebook for training multi-label Toxic comments New notebook for training multi-label E2E Challenge Update documentation for release of Spark NLP 2.6.0 Update the entire spark-nlp-models repository with new pre-trained models and pipelines Update the entire spark-nlp-workshop notebooks for Spark NLP 2.6.0 Installation Python #PyPI pip install spark-nlp==2.6.0 #Conda conda install -c johnsnowlabs spark-nlp==2.6.0 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.6.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.6.0 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.6.0 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.0 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.6.0 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.6.0.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.6.0.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.6.0.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.6.0.jar 2.5.5 John Snow Labs Spark-NLP 2.5.5: 28 new Lemma and POS models in 14 languages, bug fixes, and lots of new notebooks! Overview We are excited to release Spark NLP 2.5.5 with 28 new pretrained models for Lemma and POS in 14 languages, bug fixes, new notebooks, and more! As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add getClasses() function to NerDLModel Add getClasses() function to ClassifierDLModel Add getClasses() function to SentimentDLModel Example: ner_model = NerDLModel.pretrained(&#39;onto_100&#39;) print(ner_model.getClasses()) #[&#39;O&#39;, &#39;B-CARDINAL&#39;, &#39;B-EVENT&#39;, &#39;I-EVENT&#39;, &#39;B-WORK_OF_ART&#39;, &#39;I-WORK_OF_ART&#39;, &#39;B-ORG&#39;, &#39;B-DATE&#39;, &#39;I-DATE&#39;, &#39;I-ORG&#39;, &#39;B-GPE&#39;, &#39;B-PERSON&#39;, &#39;B-PRODUCT&#39;, &#39;B-NORP&#39;, &#39;B-ORDINAL&#39;, &#39;I-PERSON&#39;, &#39;B-MONEY&#39;, &#39;I-MONEY&#39;, &#39;I-GPE&#39;, &#39;B-LOC&#39;, &#39;I-LOC&#39;, &#39;I-CARDINAL&#39;, &#39;B-FAC&#39;, &#39;I-FAC&#39;, &#39;B-LAW&#39;, &#39;I-LAW&#39;, &#39;B-TIME&#39;, &#39;I-TIME&#39;, &#39;B-PERCENT&#39;, &#39;I-PERCENT&#39;, &#39;I-NORP&#39;, &#39;I-PRODUCT&#39;, &#39;B-QUANTITY&#39;, &#39;I-QUANTITY&#39;, &#39;B-LANGUAGE&#39;, &#39;I-ORDINAL&#39;, &#39;I-LANGUAGE&#39;, &#39;X&#39;] Enhancements Improve max sequence length calculation in BertEmbeddings and XlnetEmbeddings Bugfixes Fix a bug in RegexTokenizer in Python Fix StopWordsCleaner exception in Python when pretrained() is used Fix max sequence length issue in AlbertEmbeddings and SentencePiece generation Fix HDFS support for setGaphFolder param in NerDLApproach Models We have added 28 new pretrained models for Lemma and POS in 14 languages: Model Name Build Lang LemmatizerModel (Lemmatizer) lemma 2.5.5 br LemmatizerModel (Lemmatizer) lemma 2.5.5 ca LemmatizerModel (Lemmatizer) lemma 2.5.5 da LemmatizerModel (Lemmatizer) lemma 2.5.5 ga LemmatizerModel (Lemmatizer) lemma 2.5.5 hi LemmatizerModel (Lemmatizer) lemma 2.5.5 hy LemmatizerModel (Lemmatizer) lemma 2.5.5 eu LemmatizerModel (Lemmatizer) lemma 2.5.5 mr LemmatizerModel (Lemmatizer) lemma 2.5.5 yo LemmatizerModel (Lemmatizer) lemma 2.5.5 la LemmatizerModel (Lemmatizer) lemma 2.5.5 lv LemmatizerModel (Lemmatizer) lemma 2.5.5 sl LemmatizerModel (Lemmatizer) lemma 2.5.5 gl LemmatizerModel (Lemmatizer) lemma 2.5.5 id PerceptronModel (POS UD) pos_ud_keb 2.5.5 br PerceptronModel (POS UD) pos_ud_ancora 2.5.5 ca PerceptronModel (POS UD) pos_ud_ddt 2.5.5 da PerceptronModel (POS UD) pos_ud_idt 2.5.5 ga PerceptronModel (POS UD) pos_ud_hdtb 2.5.5 hi PerceptronModel (POS UD) pos_ud_armtdp 2.5.5 hy PerceptronModel (POS UD) pos_ud_bdt 2.5.5 eu PerceptronModel (POS UD) pos_ud_ufal 2.5.5 mr PerceptronModel (POS UD) pos_ud_ytb 2.5.5 yo PerceptronModel (POS UD) pos_ud_llct 2.5.5 la PerceptronModel (POS UD) pos_ud_lvtb 2.5.5 lv PerceptronModel (POS UD) pos_ud_ssj 2.5.5 sl PerceptronModel (POS UD) pos_ud_treegal 2.5.5 gl PerceptronModel (POS UD) pos_ud_gsd 2.5.5 id Languages: Armenian, Basque, Breton, Catalan, Danish, Galician, Hindi, Indonesian, Irish, Latin, Latvian, Marathi, Slovenian, Yoruba Documentation and Notebooks New notebook for pretrained StopWordsCleaner New notebook to Detect entities in German language New notebook to Detect entities in English language New notebook to Detect entities in Spanish language New notebook to Detect entities in French language New notebook to Detect entities in Italian language New notebook to Detect entities in Norwegian language New notebook to Detect entities in Polish language New notebook to Detect entities in Portugese language New notebook to Detect entities in Russian language Update documentation for release of Spark NLP 2.5.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Update the entire spark-nlp-workshop notebooks for Spark NLP 2.5.x Installation Python #PyPI pip install spark-nlp==2.5.5 #Conda conda install -c johnsnowlabs spark-nlp==2.5.5 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.5 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.5 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.5.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.5.5 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.5.5 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.5.5 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.5&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.5.5.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.5.5.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.5.5.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.5.5.jar 2.5.4 John Snow Labs Spark-NLP 2.5.4: Supporting Apache Spark 2.3, 43 new models and 26 new languages, new RegexTokenizer, lots of new notebooks, and more Overview We are excited to release Spark NLP 2.5.4 with the full support of Apache Spark 2.3.x, adding 43 new pre-trained models for stop words cleaning, supporting 26 new languages, a new RegexTokenizer annotator and more! As always, we would like to thank our community for their feedback, questions, and feature requests. New Features Add support for Apache Spark 2.3.x including new Maven artifacts and full support of all pre-trained models/pipelines Add 43 new pre-trained models in 43 languages to StopWordsCleaner annotator Introduce a new RegexTokenizer to split text by regex pattern Enhancements Retrained 6 new BioBERT and ClinicalBERT models Add a new param spark23 to start() function to start the session for Apache Spark 2.3.x Bugfixes Add missing library for SentencePiece used by AlbertEmbeddings and XlnetEmbeddings on Windows Fix ModuleNotFoundError in LanguageDetectorDL pipelines in Python Models We have added 43 new pre-trained models in 43 languages for StopWordsCleaner. Some selected models: Afrikaans - Models Model Name Build Lang Offline StopWordsCleaner stopwords_af 2.5.4 af Download Arabic - Models Model Name Build Lang Offline StopWordsCleaner stopwords_ar 2.5.4 ar Download Armenian - Models Model Name Build Lang Offline StopWordsCleaner stopwords_hy 2.5.4 hy Download Basque - Models Model Name Build Lang Offline StopWordsCleaner stopwords_eu 2.5.4 eu Download Bengali - Models Model Name Build Lang Offline StopWordsCleaner stopwords_bn 2.5.4 bn Download Breton - Models Model Name Build Lang Offline StopWordsCleaner stopwords_br 2.5.4 br Download Documentation and Notebooks New notebook for Language detection and identification New notebook for Classify text according to TREC classes New notebook for Detect Spam messages New notebook for Detect fake news New notebook for Find sentiment in text New notebook for Detect bullying in tweets New notebook for Detect Emotions in text New notebook for Detect Sarcasm in text Update the entire spark-nlp-models repository with new pre-trained models and pipelines Update the entire spark-nlp-workshop notebooks for Spark NLP 2.5.x Update documentation for release of Spark NLP 2.5.x Installation Python #PyPI pip install spark-nlp==2.5.4 #Conda conda install -c johnsnowlabs spark-nlp==2.5.4 Spark spark-nlp on Apache Spark 2.4.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.4 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.4 spark-nlp on Apache Spark 2.3.x: spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.5.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23_2.11:2.5.4 GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.5.4 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-spark23-gpu_2.11:2.5.4 Maven spark-nlp on Apache Spark 2.4.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp on Apache Spark 2.3.x: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu-spark23_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.4&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.5.4.jar GPU on Apache Spark 2.4.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.5.4.jar CPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-assembly-2.5.4.jar GPU on Apache Spark 2.3.x: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-spark23-gpu-assembly-2.5.4.jar 2.5.3 John Snow Labs Spark-NLP 2.5.3: Detect Fake news, emotions, spams, and more classification models, enhancements, and bug fixes Overview We are very happy to release Spark NLP 2.5.3 with 5 new pre-trained ClassifierDL models for multi-class text classification. There are also bug-fixes and other enhancements introduced in this release which were reported and requested by Spark NLP users. As always, we thank our community for their feedback, questions, and feature requests. New Features TextMatcher now can construct the chunks from tokens instead of the original documents via buildFromTokens param CoNLLGenerator now is accessible in Python Bugfixes Fix a bug in ContextSpellChecker resulting in IllegalArgumentException Enhancements Improve RocksDB connection to support different storage capabilities Improve parameters naming convention in ContextSpellChecker Add NerConverter to documentation Fix multi-language tabs in documentation Models We have added 5 new pre-trained ClassifierDL models for multi-class text classification. Model Name Build Lang Description Offline ClassifierDLModel classifierdl_use_spam 2.5.3 en Detect if a message is spam or not Download ClassifierDLModel classifierdl_use_fakenews 2.5.3 en Classify if a news is fake or real Download ClassifierDLModel classifierdl_use_emotion 2.5.3 en Detect Emotions in TweetsDetect Emotions in Tweets Download ClassifierDLModel classifierdl_use_cyberbullying 2.5.3 en Classify if a tweet is bullying Download ClassifierDLModel classifierdl_use_sarcasm 2.5.3 en Identify sarcastic tweets Download Documentation Update documentation for release of Spark NLP 2.5.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.5.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation Python #PyPI pip install spark-nlp==2.5.3 #Conda conda install -c johnsnowlabs spark-nlp==2.5.3 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.3 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.3 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.3&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.5.3.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.5.3.jar 2.5.2 John Snow Labs Spark-NLP 2.5.2: New Language Detection annotator, enhancements, and bug fixes Overview We are very happy to release Spark NLP 2.5.2 with a new state-of-the-art LanguageDetectorDL annotator to detect and identify up to 20 languages. There are also bug-fixes and other enhancements introduced in this release which were reported and requested by Spark NLP users. As always, we thank our community for their feedback, questions, and feature requests. New Features Introducing a new LanguageDetectorDL state-of-the-art annotator to detect and identify languages in documents and sentences Add a new param entityValue to TextMatcher to add custom value inside metadata. Useful in post-processing when there are multiple TextMatcher annotators with multiple dictionaries https://github.com/JohnSnowLabs/spark-nlp/issues/920 Bugfixes Add missing TensorFlow graphs to train ContextSpellChecker annotator https://github.com/JohnSnowLabs/spark-nlp/issues/912 Fix misspelled param in classThreshold param in ContextSpellChecker annotator https://github.com/JohnSnowLabs/spark-nlp/issues/911 Fix a bug where setGraphFolder in NerDLApproach annotator couldn’t find a graph on Databricks (DBFS) https://github.com/JohnSnowLabs/spark-nlp/issues/739 Fix a bug in NerDLApproach when includeConfidence was set to true https://github.com/JohnSnowLabs/spark-nlp/issues/917 Fix a bug in BertEmbeddings https://github.com/JohnSnowLabs/spark-nlp/issues/906 https://github.com/JohnSnowLabs/spark-nlp/issues/918 Enhancements Improve TF backend in ContextSpellChecker annotator Pipelines and Models We have added 4 new LanguageDetectorDL models and pipelines to detect and identify up to 20 languages: The model with 7 languages: Czech, German, English, Spanish, French, Italy, and Slovak The model with 20 languages: Bulgarian, Czech, German, Greek, English, Spanish, Finnish, French, Croatian, Hungarian, Italy, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Swedish, Turkish, and Ukrainian Model Name Build Lang Offline LanguageDetectorDL ld_wiki_7 2.5.2 xx Download LanguageDetectorDL ld_wiki_20 2.5.2 xx Download Pipeline Name Build Lang Offline LanguageDetectorDL detect_language_7 2.5.2 xx Download LanguageDetectorDL detect_language_20 2.5.2 xx Download Documentation Update documentation for release of Spark NLP 2.5.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.5.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation Python #PyPI pip install spark-nlp==2.5.2 #Conda conda install -c johnsnowlabs spark-nlp==2.5.2 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.2 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.2 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.2&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.5.2.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.5.2.jar 2.5.1 John Snow Labs Spark-NLP 2.5.1: Adding support for 6 new BioBERT and ClinicalBERT models Overview We are very excited to extend Spark NLP support to 6 new BERT models for medical and clinical documents. We have also updated our documentation for 2.5.x releases, notebooks in our workshop, and made some enhancements in this release. As always, we thank our community for their feedback and questions in our Slack channel. New Features Add Python support for PubTator reader to convert automatic annotations of the biomedical datasets into DataFrame Add 6 new pre-trained BERT models from BioBERT and ClinicalBERT Models We have added 6 new BERT models for medical and clinical purposes. The 4 BERT pre-trained models are from BioBERT and the other 2 are coming from ClinicalBERT models: Model Name Build Lang Offline BertEmbeddings biobert_pubmed_base_cased 2.5.0 en Download BertEmbeddings biobert_pubmed_large_cased 2.5.0 en Download BertEmbeddings biobert_pmc_base_cased 2.5.0 en Download BertEmbeddings biobert_pubmed_pmc_base_cased 2.5.0 en Download BertEmbeddings biobert_clinical_base_cased 2.5.0 en Download BertEmbeddings biobert_discharge_base_cased 2.5.0 en Download Enhancements Add unit tests for XlnetEmbeddings Add unit tests for AlbertEmbeddings Add unit tests for ContextSpellChecker Documentation Update documentation for release of Spark NLP 2.5.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.5.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation Python #PyPI pip install spark-nlp==2.5.1 #Conda conda install -c johnsnowlabs spark-nlp==2.5.1 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.1 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.1 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.1&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.5.1.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.5.1.jar 2.5.0 John Snow Labs Spark-NLP 2.5.0: ALBERT &amp; XLNet transformers, state-of-the-art spell checker, multi-class sentiment detector, 80+ new models &amp; pipelines in 14 new languages &amp; more Overview When we started planning for Spark NLP 2.5.0 release a few months ago the world was a different place! We have been blown away by the use of Natural Language Processing for early outbreak detections, question-answering chatbot services, text analysis of medical records, monitoring efforts to minimize the virus spread, and many more. In that spirit, we are honored to announce Spark NLP 2.5.0 release! Witnessing the world coming together to fight coronavirus has driven us to deliver perhaps one of the biggest releases we have ever made. As always, we thank our community for their feedback, bug reports, and contributions that made this release possible. Major features and improvements NEW: A new AlbertEmbeddings annotator with 4 available pre-trained models NEW: A new XlnetEmbeddings annotator with 2 available pre-trained models NEW: A new ContextSpellChecker annotator, the state-of-the-art annotator for spell checking NEW: A new SentimentDL annotator for multi-class sentiment analysis. This annotator comes with 2 available pre-trained models trained on IMDB and Twitter datasets NEW: Support for 14 new languages with 80+ pretrained models and pipelines! Add new PubTator reader to convert automatic annotations of the biomedical datasets into DataFrame Introducing a new outputLogsPath param for NerDLApproach, ClassifierDLApproach and SentimentDLApproach annotators Refactored CoNLLGenerator to actually use NER labels from the DataFrame Unified params in NerDLModel in both Scala and Python Extend and complete Scaladoc APIs for all the annotators Bugfixes Fix position of tokens in Normalizer Fix Lemmatizer exception on a bad input Fix annotator logs failing on object storage file systems like DBFS Models and Pipelines Spark NLP 2.5.0 comes with 87 new pretrained models and pipelines in 14 new languages available for all Windows, Linux, and macOS users. We added new languages such as Dutch, Norwegian. Polish, Portuguese, Bulgarian, Czech, Greek, Finnish, Hungarian, Romanian, Slovak, Swedish, Turkish, and Ukrainian. The complete list of 160+ models &amp; pipelines in 22+ languages is available here. Featured Pretrained Pipelines Dutch - Pipelines Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 nl   Download Explain Document Medium explain_document_md 2.5.0 nl   Download Explain Document Large explain_document_lg 2.5.0 nl   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 nl   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 nl   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 nl   Download Norwegian - Pipelines Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 no   Download Explain Document Medium explain_document_md 2.5.0 no   Download Explain Document Large explain_document_lg 2.5.0 no   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 no   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 no   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 no   Download Polish - Pipelines Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 pl   Download Explain Document Medium explain_document_md 2.5.0 pl   Download Explain Document Large explain_document_lg 2.5.0 pl   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 pl   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 pl   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 pl   Download Portuguese - Pipelines Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 pt   Download Explain Document Medium explain_document_md 2.5.0 pt   Download Explain Document Large explain_document_lg 2.5.0 pt   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 pt   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 pt   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 pt   Download Documentation Update documentation for release of Spark NLP 2.5.0 Update the entire spark-nlp-workshop notebooks for Spark NLP 2.5.0 Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation Python #PyPI pip install spark-nlp==2.5.0 #Conda conda install -c johnsnowlabs spark-nlp==2.5.0 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.5.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.5.0.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.5.0.jar 2.4.5 John Snow Labs Spark-NLP 2.4.5: Supporting more Databricks runtimes and YARN in cluster mode Overview We are very excited to extend Spark NLP support to 6 new Databricks runtimes and add support to Cloudera and EMR YARN cluster-mode. As always, we thank our community for their feedback and questions in our Slack channel. New Features Extend Spark NLP support for Databricks runtimes: 6.2 6.2 ML 6.3 6.3 ML 6.4 6.4 ML 6.5 6.5 ML Add support for cluster-mode in Cloudera and EMR YARN clusters New splitPattern param in Tokenizer to split tokens by regex rules Bugfixes Fix ClassifierDLModel save and load in Python Fix ClassifierDL TensorFlow session reuse Fix Normalizer positions of new tokens Documentation Update documentation for release of Spark NLP 2.4.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.4.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation Python #PyPI pip install spark-nlp==2.4.5 #Conda conda install -c johnsnowlabs spark-nlp==2.4.5 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.5&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.4.5.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.4.5.jar 2.4.4 John Snow Labs Spark-NLP 2.4.4: The very first native multi-class text classifier and pre-trained models and pipelines in Russian Overview We are very excited to release the very first multi-class text classifier in Spark NLP v2.4.4! We have built a generic ClassifierDL annotator that uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 50 classes. We are also happy to announce the support of yet another language: Russian! We have trained and prepared 5 pre-trained models and 6 pre-trained pipelines in Russian. NOTE: ClassifierDL is an experimental feature in 2.4.4 before it becomes stable in 2.4.5 release. We have worked hard to aim for simplicity and we are looking forward to your feedback as always. We will add more examples by the upcoming days: Examples: Python and Scala New Features Introducing a generic multi-class text classifier: ClassifierDL. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 50 classes. 5 new pretrained Russian models (Lemma, POS, 3x NER) 6 new pretrained Russian pipelines Models: Model name language LemmatizerModel (Lemmatizer) lemma ru PerceptronModel (POS UD) pos_ud_gsd ru NerDLModel wikiner_6B_100 ru NerDLModel wikiner_6B_300 ru NerDLModel wikiner_840B_300 ru Pipelines: Pipeline name language Explain Document (Small) explain_document_sm ru Explain Document (Medium) explain_document_md ru Explain Document (Large) explain_document_lg ru Entity Recognizer (Small) entity_recognizer_sm ru Entity Recognizer (Medium) entity_recognizer_md ru Entity Recognizer (Large) entity_recognizer_lg ru Evaluation: wikiner_6B_100 with conlleval.pl Accuracy Precision Recall F1-Score 97.76% 88.85% 88.55% 88.70 wikiner_6B_300 with conlleval.pl Accuracy Precision Recall F1-Score 97.78% 89.09% 88.51% 88.80 wikiner_840B_300 with conlleval.pl Accuracy Precision Recall F1-Score 97.85% 89.85% 89.11% 89.48 Example: import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val pipeline = PretrainedPipeline(&quot;explain_document_sm&quot;, lang=&quot;ru&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Пик распространения коронавируса и вызываемой им болезни Covid-19 в Китае прошел, заявил в четверг агентству Синьхуа официальный представитель Госкомитета по гигиене и здравоохранению КНР Ми Фэн.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() Enhancements Add param to NerConverter to override modified tokens instead of original tokens UniversalSentenceEncoder and SentenceEmbeddings are now accepting storageRef Bugfixes Fix TokenAssembler Fix NerConverter exception when NerDL is trained with different tagging style than IOB/IOB2 Normalizer now recomputes the index of tokens when it removes characters from a text Documentation Update documentation for release of Spark NLP 2.4.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.4.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation Python #PyPI pip install spark-nlp==2.4.4 #Conda conda install -c johnsnowlabs spark-nlp==2.4.4 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.4 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.4 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.4&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.4.4.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.4.4.jar 2.4.3 John Snow Labs Spark-NLP 2.4.3: Minor bug fix in Python Overview This minor release fixes a bug on our Python side that was introduced in 2.4.2 release. As always, we thank our community for their feedback and questions in our Slack channel. NOTE: We highly recommend our Python users to update to 2.4.3 release. Bugfixes Fix Python imports which resulted in AttributeError: module ‘sparknlp’ has no attribute Documentation Update documentation for release of Spark NLP 2.4.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.4.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation PyPI pip install spark-nlp==2.4.3 Conda conda install -c johnsnowlabs spark-nlp==2.4.3 spark-shell spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.3 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.3 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.4.3.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.4.3.jar 2.4.2 John Snow Labs Spark-NLP 2.4.2: Minor bug fixes and improvements Overview This minor release fixes a few bugs in some of our annotators reported by our community. As always, we thank our community for their feedback and questions in our Slack channel. Bugfixes Fix UniversalSentenceEncoder.pretrained() that failed in Python Fix ElmoEmbeddings.pretrained() that failed in Python Fix ElmoEmbeddings poolingLayer param to be a string as expected Fix ChunkEmbeddings to preserve chunk’s index Fix NGramGenerator and missing chunk metadata New Features Add GPU support param in Spark NLP start function: sparknlp.start(gpu=true) Improve create_model.py to create custom TF graph for NerDLApproach Documentation Update documentation for release of Spark NLP 2.4.x Update the entire spark-nlp-workshop notebooks for Spark NLP 2.4.x Update the entire spark-nlp-models repository with new pre-trained models and pipelines Installation PyPI pip install spark-nlp==2.4.2 Conda conda install -c johnsnowlabs spark-nlp==2.4.2 spark-shell spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.2 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.2 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.4.2.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.4.2.jar 2.4.1 John Snow Labs Spark-NLP 2.4.1: Bug fixes and the very first Spanish models &amp; pipelines Overview This minor release fixes a few bugs in some of the annotators reported by our community. As always, we thank our community for their feedback on our Slack channel. Models &amp; Pipelines 5 new pretrained Spanish models (Lemma, POS, 3x NER) 6 new pretrained Spanish pipelines Models: Model name language LemmatizerModel (Lemmatizer) lemma es PerceptronModel (POS UD) pos_ud_gsd es NerDLModel wikiner_6B_100 es NerDLModel wikiner_6B_300 es NerDLModel wikiner_840B_300 es Pipelines: Pipeline name language Explain Document (Small) explain_document_sm es Explain Document (Medium) explain_document_md es Explain Document (Large) explain_document_lg es Entity Recognizer (Small) entity_recognizer_sm es Entity Recognizer (Medium) entity_recognizer_md es Entity Recognizer (Large) entity_recognizer_lg es Evaluation: wikiner_6B_100 with conlleval.pl Accuracy Precision Recall F1-Score 98.35% 88.97% 88.64% 88.80 wikiner_6B_300 with conlleval.pl Accuracy Precision Recall F1-Score 98.38% 89.42% 89.03% 89.22 wikiner_840B_300 with conlleval.pl Accuracy Precision Recall F1-Score 98.46% 89.74% 89.43% 89.58 Example import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val pipeline = PretrainedPipeline(&quot;explain_document_sm&quot;, lang=&quot;es&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Ésta se convertiría en una amistad de por vida, y Peleo, conociendo la sabiduría de Quirón , más adelante le confiaría la educación de su hijo Aquiles.&quot;), (2, &quot;Durante algo más de 200 años el territorio de la actual Bolivia constituyó la Real Audiencia de Charcas, uno de los centros más prósperos y densamente poblados de los virreinatos españoles.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() More info on pre-trained models and pipelines Bugfixes Improve ChunkEmbeddings annotator and fix the empty chunk result Fix UniversalSentenceEncoder crashing on empty Tensor Fix NorvigSweetingModel missing sentenceId that results in NGramsGenerator crashing Fix missing storageRef in embeddings’ column for ElmoEmbeddings annotator Documentation Update documentation for release of Spark NLP 2.4.x Add new features such as ElmoEmbeddings and UniversalSentenceEncoder Add multiple programming languages for demos and examples Update the entire spark-nlp-models repository with new pre-trained models and pipelines 2.4.0 John Snow Labs Spark-NLP 2.4.0: New TensorFlow 1.15, Universal Sentence Encoder, Elmo, faster Word Embeddings &amp; more We are very excited to finally release Spark NLP v2.4.0! This has been one of the largest releases we have ever made since the inception of the library! The new release of Spark NLP 2.4.0 has been migrated to TensorFlow 1.15.0 which takes advantage of the latest deep learning technologies and pre-trained models. Major features and improvements NEW: TensorFlow 1.15.0 now works behind Spark NLP. This brings implicit improvements in performance, accuracy, and functionalities NEW: UniversalSentenceEncoder annotator with 2 pre-trained models from TF Hub NEW: ElmoEmbeddings with a pre-trained model from TF Hub NEW: All our pre-trained models are now cross-platform! NEW: For the first time, all the multi-lingual models and pipelines are available for Windows users (French, German and Italian) NEW: MultiDateMatcher capable of matching more than one date per sentence (Extends DateMatcher algorithm) NEW: BigTextMatcher works best with large amounts of input data BertEmbeddings improvements with 5 new models from TF Hub RecursivePipelineModel as an enhanced PipelineModel allows Annotators to access previous annotators in the pipeline for more ML strategies LazyAnnotators: A new Param in Annotators allows them to stand idle in the Pipeline and do nothing. Can be called by other Annotators in a RecursivePipeline RocksDB is now available as a flexible API called Storage. Allows any annotator to have it’s own distributed local index database Now our Tensorflow pre-trained models are cross-platform. Enabling multi-language models and other improvements to Windows users. Improved IO performance in general for handling embeddings Improved cache cleanup and GC by liberating open files utilized in RocksDB (to be improved further) Tokenizer and SentenceDetector Params minLength and MaxLength to filter out annotations outside these bounds Tokenizer improvements in splitChars and simplified rules DateMatcher improvements TextMatcher improvements preload algorithm information within the model for faster prediction Annotators the utilize embeddings have now a strict validation to be using exactly the embeddings they were trained with Improvements in the API allow Annotators with Storage to save and load their RocksDB database independently and let it be shared across Annotators and let it be shared across Annotators Models and Pipelines Spark NLP 2.4.0 comes with new models including Universal Sentence Encoder, BERT, and Elmo models from TF Hub. In addition, our multilingual pipelines are now available for Windows as same as Linux and macOS users. Models Name UniversalSentenceEncoder tf_use UniversalSentenceEncoder tf_use_lg BertEmbeddings bert_large_cased BertEmbeddings bert_large_uncased BertEmbeddings bert_base_cased BertEmbeddings bert_base_uncased BertEmbeddings bert_multi_cased ElmoEmbeddings elmo NerDLModel onto_100 NerDLModel onto_300 Pipelines Name Language Explain Document Large explain_document_lg fr Explain Document Medium explain_document_md fr Entity Recognizer Large entity_recognizer_lg fr Entity Recognizer Medium entity_recognizer_md fr Explain Document Large explain_document_lg de Explain Document Medium explain_document_md de Entity Recognizer Large entity_recognizer_lg de Entity Recognizer Medium entity_recognizer_md de Explain Document Large explain_document_lg it Explain Document Medium explain_document_md it Entity Recognizer Large entity_recognizer_lg it Entity Recognizer Medium entity_recognizer_md it Example: # Import Spark NLP from sparknlp.base import * from sparknlp.annotator import * from sparknlp.pretrained import PretrainedPipeline import sparknlp # Start Spark Session with Spark NLP # If you already have a SparkSession (Zeppelin, Databricks, etc.) # you can skip this spark = sparknlp.start() # Download a pre-trained pipeline pipeline = PretrainedPipeline(&#39;explain_document_md&#39;, lang=&#39;fr&#39;) # Your testing dataset text = &quot;&quot;&quot; Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale. &quot;&quot;&quot; # Annotate your testing dataset result = pipeline.annotate(text) # What&#39;s in the pipeline list(result.keys()) # result: # [&#39;entities&#39;, &#39;lemma&#39;, &#39;document&#39;, &#39;pos&#39;, &#39;token&#39;, &#39;ner&#39;, &#39;embeddings&#39;, &#39;sentence&#39;] # Check the results result[&#39;entities&#39;] # entities: # [&#39;Emmanuel Jean-Michel Frédéric Macron&#39;, &#39;Jean-Michel Macron&#39;, &quot;CHU d&#39;Amiens4&quot;, &#39;Françoise Noguès&#39;, &#39;Sécurité sociale&#39;] Backward incompatibilities Please note that in 2.4.0 we have added storageRef parameter to our WordEmbeddogs. This means every WordEmbeddingsModel will now have storageRef which is also bound to NerDLModel trained by that embeddings. This assures users won’t use a NerDLModel with a wrong WordEmbeddingsModel. Example: val embeddings = new WordEmbeddings() .setStoragePath(&quot;/tmp/glove.6B.100d.txt&quot;, ReadAs.TEXT) .setDimension(100) .setStorageRef(&quot;glove_100d&quot;) // Use or save this WordEmbeddings with storageRef .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) If you save theWordEmbeddings model the storageRef will be glove_100d. If you ever train any NerDLApproach the glove_100d will bind to that NerDLModel. If you have already WordEmbeddingsModels saved from earlier versions, you either need to re-save them with storageRed or you can manually add this param in their metadata/. The same advice works for the NerDLModel from earlier versions. Installation Python #PyPI pip install spark-nlp==2.4.0 #Conda conda install -c johnsnowlabs spark-nlp==2.4.0 Spark spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.0 PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.0 Maven &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.11&lt;/artifactId&gt; &lt;version&gt;2.4.0&lt;/version&gt; &lt;/dependency&gt; FAT JARs CPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-assembly-2.4.0.jar GPU: https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/spark-nlp-gpu-assembly-2.4.0.jar Bugfixes Fixed splitChars in Tokenizer Fixed PretrainedPipeline in Python to allow accessing the inner PipelineModel in the instance Fixes in Chunk and SentenceEmbeddings to better deal with empty cleaned-up Annotations Documentation and examples We have a new Developer section for those who are interested in contributing to Spark NLP Developer We have updated our workshop repository with more notebooks Workshop",
    "url": "/docs/en/release_notes",
    "relUrl": "/docs/en/release_notes"
  },
  "60": {
    "id": "60",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/resolve_entities_codes",
    "relUrl": "/resolve_entities_codes"
  },
  "61": {
    "id": "61",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/split_clean_medical_text",
    "relUrl": "/split_clean_medical_text"
  },
  "62": {
    "id": "62",
    "title": "Start Page",
    "content": "When you login to the Annotation Lab, you will be presented with the list of available projects you have created (My Projects) or that have been shared with you (Shared With Me). Some basic information are shown for each project the total number of tasks and the creation date. The list of projects can be sorted ascending or descending on the project creation date. Project Grouping As the number of projects can grow significantly over time, for an easier management and organization of those, the Annotation Lab allows project grouping. As such, it is possible to assign a project to an existing group or to a new group. Each group can be assigned a color which will be used to highlight projects included in that group. Once a project is assigned to a group, the group name proceeds the name of the project. At any time a project can be remove from one group and added to another group. The list of visible projects can be filtered based on group name, or using the search functionality which applies to both group name and project name.",
    "url": "/docs/en/start_page",
    "relUrl": "/docs/en/start_page"
  },
  "63": {
    "id": "63",
    "title": "Tasks",
    "content": "The Tasks screen shows a list of all documents that have been imported into the current project. Under each task you can see meta data about the task: the time of import, the user who imported the task and the annotators and reviewers assigned to the task. Task Assignment Project Owners/Managers can assign tasks to annotator(s) and reviewer(s) in order to better plan/distribute project work. Annotators and Reviewers can only view tasks that are assigned to them which means there is no chance of accidental work overlap. For assigning a task to an annotator, from the task page select one or more tasks and from the Assign dropdown choose an annotator. You can only assign a task to annotators that have already been added to the project. For adding an annotator to the project, go to the Setup page and share the project with the annotator by giving him/her the update right. Once an annotator is assigned to a task his/her name will be listed below the task name on the tasks screen. When upgrading from an older version of the Annotation Lab, the annotators will no longer have access to the tasks they worked on unless they will be assigned to those explicitely by the admin user who created the project. Once they are assigned, they can resume work and no information will be lost. Task Status At high level, each task can have one of the following statuses: Incomplete, when none of the assigne annotators has started working on the task. In Progress, when at least one of the assigned annotators has submitted at least one completion for this task. Submitted, when all annotators which were assigned to the task have submitted a completion which is set as ground truth (starred). Reviewed, in the case there is a reviewer assigned to the task, and the reviewer has reviewed and accepted the submited completion. To Correct, in the case the assigned reviewer has rejected the completion created by the Annotator. The status of a task varies according to the type of account the logged in user has (his/her visibility over the project) and according to the tasks that have been assigned to him/her. For Project Owner, Manager and Reviewer On the Analytics page and Tasks page, the Project Owner/Manager/Reviewer will see the general overview of the projects which will take into consideration the task level statuses as follows: Incomplete - Annotators have not started working on this task In Progress - At least one annotator still has not starred (marked as ground truth) any submitted completions Submitted - All annotators that are assigned to a task have starred (marked as ground truth) one submitted completion Reviewed - Reviewer has approved all starred submitted completions for the task For Annotators On the Annotator’s Task page, the task status will be shown with regards to the context of the logged-in Annotator’s work. As such, if the same task is assigned to two annotators then: if annotator1 is still working and not submitted the task, then he/she will see task status as In-progress if annotator2 submits the task from his/her side then he/she will see task status as Submitted The following statuses are available on the Annotator’s view. Incomplete – Current logged-in annotator has not started working on this task. In Progress - At least one saved/submitted completions exist, but there is no starred submitted completion. Submitted - Annotator has at least one starred submitted completion. Reviewed - Reviewer has approved the starred submitted completion for the task. To Correct - Reviewer has rejected the submitted work. In this case, the star is removed from the reviewed completion. The annotator should start working on the task and resubmit. Note: The status of a task is maintained/available only for the annotators assigned to the task. The Project Owner completions state are not considered while deciding the status of a task. When multiple Annotators are assigned to a task, the reviewer will see the task as submitted when all annotators submit and star their completions. Otherwise, if one of the assigned Annotators has not submitted or has not starred one completion, then the Reviewer will see the task as In Progress. Task filters As normally annotation projects involve a large number of tasks, the Task page includes filtering and sorting options which will help the user identify the tasks he/she needs faster. Tasks cam be sorted by time of import ascending or descending. Tasks can be filtered by the assigned tags, by the user who imported the task and by the status. There is also a search functionality which will identify the tasks having a given string on their name. The number of tasks visible on the screeen is customizable by selecting the predefined values from the Tasks per page drop-down. Comments Comment can be added to each task by Project Owner or Manager. This is done by clicking the comment icon present on the rightmost side of each Task in the Tasks List page. It is important to notice that these comments are visible to everyone who can view the particular task. *Dark icon = A comment is present on the task. Light icon = No comment is present",
    "url": "/docs/en/tasks",
    "relUrl": "/docs/en/tasks"
  },
  "64": {
    "id": "64",
    "title": "Training",
    "content": "Training Datasets POS Dataset In order to train a Part of Speech Tagger annotator, we need to get corpus data as a spark dataframe. There is a component that does this for us: it reads a plain text file and transforms it to a spark dataset. Input File Format: A|DT few|JJ months|NNS ago|RB you|PRP received|VBD a|DT letter|NN Available parameters are: spark: Spark session path(string): Path to file with corpus data for training POS delimiter(string): Delimiter of token and postag. Defaults to | outputPosCol(string): Name of the column with POS values. Defaults to “tags”. Example: Refer to the POS Scala docs for more details on the API. {% include programmingLanguageSelectScalaPython.html %} from sparknlp.training import POS train_pos = POS().readDataset(spark, &quot;./src/main/resources/anc-pos-corpus&quot;) import com.johnsnowlabs.nlp.training.POS val trainPOS = POS().readDataset(spark, &quot;./src/main/resources/anc-pos-corpus&quot;) CoNLL Dataset In order to train a Named Entity Recognition DL annotator, we need to get CoNLL format data as a spark dataframe. There is a component that does this for us: it reads a plain text file and transforms it to a spark dataset. Constructor parameters: documentCol: String = “document”, sentenceCol: String = “sentence”, tokenCol: String = “token”, posCol: String = “pos”, conllLabelIndex: Int = 3, conllPosIndex: Int = 1, conllTextCol: String = “text”, labelCol: String = “label”, explodeSentences: Boolean = false Available parameters are: spark: Spark session path(string): Path to a CoNLL 2003 IOB NER file. readAs(string): Can be LINE_BY_LINE or SPARK_DATASET, with options if latter is used (default LINE_BY_LINE) Example: Refer to the CoNLL Scala docs for more details on the API. {% include programmingLanguageSelectScalaPython.html %} from sparknlp.training import CoNLL training_conll = CoNLL().readDataset(spark, &quot;./src/main/resources/conll2003/eng.train&quot;) import com.johnsnowlabs.nlp.training.CoNLL val trainingConll = CoNLL().readDataset(spark, &quot;./src/main/resources/conll2003/eng.train&quot;) Spell Checkers Dataset In order to train a Norvig or Symmetric Spell Checkers, we need to get corpus data as a spark dataframe. We can read a plain text file and transforms it to a spark dataset. Example: {% include programmingLanguageSelectScalaPython.html %} train_corpus = spark.read.text(&quot;./sherlockholmes.txt&quot;) .withColumnRenamed(&quot;value&quot;, &quot;text&quot;) val trainCorpus = spark.read.text(&quot;./sherlockholmes.txt&quot;) .select(trainCorpus.col(&quot;value&quot;).as(&quot;text&quot;)) Vivekn Sentiment Analysis Dataset To train ViveknSentimentApproach, it is needed to have input columns DOCUMENT and TOKEN, and a String column which is set with setSentimentCol stating either positive or negative PubTator Dataset The PubTator format includes medical papers’ titles, abstracts, and tagged chunks (see PubTator Docs and MedMentions Docs for more information). We can create a Spark DataFrame from a PubTator text file. Available parameters are: spark: Spark session path(string): Path to a PubTator File Example: import com.johnsnowlabs.nlp.training.PubTator val trainingPubTatorDF = PubTator.readDataset(spark, &quot;./src/test/resources/corpus_pubtator.txt&quot;) TensorFlow Graphs NER DL uses Char CNNs - BiLSTM - CRF Neural Network architecture. Spark NLP defines this architecture through a Tensorflow graph, which requires the following parameters: Tags Embeddings Dimension Number of Chars Spark NLP infers these values from the training dataset used in NerDLApproach annotator and tries to load the graph embedded on spark-nlp package. Currently, Spark NLP has graphs for the most common combination of tags, embeddings, and number of chars values: Tags Embeddings Dimension 10 100 10 200 10 300 10 768 10 1024 25 300 All of these graphs use an LSTM of size 128 and number of chars 100 In case, your train dataset has a different number of tags, embeddings dimension, number of chars and LSTM size combinations shown in the table above, NerDLApproach will raise an IllegalArgumentException exception during runtime with the message below: Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Check https://nlp.johnsnowlabs.com/docs/en/graph for instructions to generate the required graph. To overcome this exception message we have to follow these steps: Clone spark-nlp github repo Run python file create_models with number of tags, embeddings dimension and number of char values mentioned on your exception message error. cd spark-nlp/python/tensorflow export PYTHONPATH=lib/ner python create_models.py [number_of_tags] [embeddings_dimension] [number_of_chars] [output_path] This will generate a graph on the directory defined on `output_path argument. Retry training with NerDLApproach annotator but this time use the parameter setGraphFolder with the path of your graph. Note: Make sure that you have Python 3 and Tensorflow 1.15.0 installed on your system since create_models requires those versions to generate the graph successfully. Note: We also have a notebook in the same directory if you prefer Jupyter notebook to cerate your custom graph.",
    "url": "/docs/en/training",
    "relUrl": "/docs/en/training"
  },
  "65": {
    "id": "65",
    "title": "Transformers",
    "content": "DocumentAssembler: Getting data in In order to get through the NLP process, we need to get raw data annotated. There is a special transformer that does this for us: it creates the first annotation of type Document which may be used by annotators down the road. It can read either a String column or an Array[String] Settable parameters are: setInputCol() setOutputCol() setIdCol() -&gt; OPTIONAL: Sring type column with id information setMetadataCol() -&gt; OPTIONAL: Map type column with metadata information setCleanupMode(disabled) -&gt; Cleaning up options, possible values: disabled: Source kept as original. inplace: removes new lines and tabs. inplace_full: removes new lines and tabs but also those which were converted to strings (i.e. n) shrink: removes new lines and tabs, plus merging multiple spaces and blank lines to a single space. shrink_full: removews new lines and tabs, including stringified values, plus shrinking spaces and blank lines. Example: Refer to the DocumentAssembler Scala docs for more details on the API. {% include programmingLanguageSelectScalaPython.html %} import com.johnsnowlabs.nlp.*; import com.johnsnowlabs.nlp.annotators.*; import org.apache.spark.ml.Pipeline; DocumentAssembler documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) .setCleanupMode(&quot;shrink&quot;) from sparknlp.annotator import * from sparknlp.common import * from sparknlp.base import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) .setCleanupMode(&quot;shrink&quot;) import com.johnsnowlabs.nlp._ import com.johnsnowlabs.nlp.annotators._ import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) .setCleanupMode(&quot;shrink&quot;) TokenAssembler: Getting data reshaped This transformer reconstructs a Document type annotation from tokens, usually after these have been normalized, lemmatized, normalized, spell checked, etc, in order to use this document annotation in further annotators. Settable parameters are: setInputCol(inputs:Array(String)) setOutputCol(output:String) setPreservePosition(preservePosition:bool): Whether to preserve the actual position of the tokens or reduce them to one space Example: Refer to the TokenAssembler Scala docs for more details on the API. {% include programmingLanguageSelectScalaPython.html %} TokenAssembler token_assembler = new TokenAssembler() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;assembled&quot;) token_assembler = TokenAssembler() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;assembled&quot;) val token_assembler = new TokenAssembler() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;assembled&quot;) Doc2Chunk Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol. Chunk text must be contained within input DOCUMENT. May be either StringType or ArrayType[StringType] (using isArray Param) Useful for annotators that require a CHUNK type input. Settable parameters are: setInputCol() setOutputCol() setIsArray(bool) -&gt; Whether the target chunkCol is ArrayType&lt;StringType&gt; setChunkCol(string) -&gt; String or StringArray column with the chunks that belong to the inputCol target setStartCol(string) -&gt; Target INT column pointing to the token index (split by white space) setStartColByTokenIndex(bool) -&gt; Whether to use token index by whitespace or character index in startCol setFailOnMissing(bool) -&gt; Whether to fail when a chunk is not found within inputCol setLowerCase(bool) -&gt; whether to increase matching by lowercasing everything before matching Example: Refer to the Doc2Chunk Scala docs for more details on the API. {% include programmingLanguageSelectScalaPython.html %} chunker = Doc2Chunk() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;chunk&quot;) .setIsArray(False) .setChunkCol(&quot;some_column&quot;) val chunker = new Doc2Chunk() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;chunk&quot;) .setIsArray(false) .setChunkCol(&quot;some_column&quot;) Chunk2Doc Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result. Settable parameters are: setInputCol() setOutputCol() Example: Refer to the Chunk2Doc Scala docs for more details on the API. {% include programmingLanguageSelectScalaPython.html %} chunk_doc = Chunk2Doc() .setInputCols([&quot;chunk_output&quot;]) .setOutputCol(&quot;new_document&quot;) val chunk_doc = new Chunk2Doc() .setInputCols(&quot;chunk_output&quot;) .setOutputCol(&quot;new_document&quot;) Finisher Once we have our NLP pipeline ready to go, we might want to use our annotation results somewhere else where it is easy to use. The Finisher outputs annotation(s) values into string. Settable parameters are: setInputCols() setOutputCols() setCleanAnnotations(True) -&gt; Whether to remove intermediate annotations setValueSplitSymbol(“#”) -&gt; split values within an annotation character setAnnotationSplitSymbol(“@”) -&gt; split values between annotations character setIncludeMetadata(False) -&gt; Whether to include metadata keys. Sometimes useful in some annotations setOutputAsArray(False) -&gt; Whether to output as Array. Useful as input for other Spark transformers. Example: Refer to the Finisher Scala docs for more details on the API. {% include programmingLanguageSelectScalaPython.html %} finisher = Finisher() .setInputCols([&quot;token&quot;]) .setIncludeMetadata(True) # set to False to remove metadata val finisher = new Finisher() .setInputCols(&quot;token&quot;) .setIncludeMetadata(true) // set to False to remove metadata EmbeddingsFinisher This transformer is designed to deal with embedding annotators: WordEmbeddings, BertEmbeddings, SentenceEmbeddingd, and ChunkEmbeddings. By using EmbeddingsFinisher you can easily transform your embeddings into array of floats or Vectors which are compatible with Spark ML functions such as LDA, K-mean, Random Forest classifier or any other functions that require featureCol. Settable parameters are: setInputCols() setOutputCols() setCleanAnnotations(True) -&gt; Whether to remove and cleanup the rest of the annotators (columns) setOutputAsVector(False) -&gt; if enabled, it will output the embeddings as Vectors instead of arrays Example: Refer to the EmbeddingsFinisher Scala docs for more details on the API. {% include programmingLanguageSelectScalaPython.html %} todo embeddings_finisher = EmbeddingsFinisher() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCols(&quot;sentence_embeddings_vectors&quot;) .setOutputAsVector(True) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;sentence_embeddings&quot;, &quot;embeddings&quot;) .setOutputCols(&quot;finished_sentence_embeddings&quot;, &quot;finished_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) Import Transformers into Spark NLP Starting with Spark NLP 3.1.0 the support for HuggingFace exported models in equivalent Spark NLP annotators has been extended. Users can now easily use saved_model feature in HuggingFace within a few lines of codes and import any BERT, DistilBERT, RoBERTa, and XLM-RoBERTa models to Spark NLP. We will work on the remaining annotators and extend this support to the rest with each release. Compatibility Spark NLP: The equivalent annotator in Spark NLP TF Hub: Models from TF Hub HuggingFace: Models from HuggingFace Model Architecture: Which architecture is compatible with that annotator Flags: Fully supported ✅ Partially supported (requires workarounds) ✔️ Under development ❎ Not supported ❌ Spark NLP TF Hub HuggingFace Model Architecture BertEmbeddings ❎ ✅ BERT - Small BERT - ELECTRA BertSentenceEmbeddings ❎ ✅ BERT - Small BERT - ELECTRA DistilBertEmbeddings   ✅ DistilBERT RoBertaEmbeddings   ✅ RoBERTa - DistilRoBERTa XlmRoBertaEmbeddings   ✅ XLM-RoBERTa AlbertEmbeddings ❎ ❎   XlnetEmbeddings   ❎   ElmoEmbeddings ❎ ❎   UniversalSentenceEncoder ❎     T5Transformer   ❌   MarianTransformer   ❌   Notebooks Spark NLP HuggingFace Notebooks BertEmbeddings HuggingFace in Spark NLP - BERT BertSentenceEmbeddings HuggingFace in Spark NLP - BERT Sentence DistilBertEmbeddings HuggingFace in Spark NLP - DistilBERT RoBertaEmbeddings HuggingFace in Spark NLP - RoBERTa XlmRoBertaEmbeddings HuggingFace in Spark NLP - XLM-RoBERTa Limitations If you are importing models from HuggingFace as Embeddings they must be for Fill-Mask task. Meaning you cannot use a model in BertEmbeddings if they were trained or fine-tuned on token/text classification tasks in HuggingFace. They have a different architecture. There is a 2G size limitation with loading a TF SavedModel model in Spark NLP. Your model cannot be larger than 2G size or you will see the following error: `Required array size too large. (We are working on going around this Java limitation, however, for the time being, there are some models which are over 2G and they are not compatible)",
    "url": "/docs/en/transformers",
    "relUrl": "/docs/en/transformers"
  },
  "66": {
    "id": "66",
    "title": "Video Tutorials",
    "content": "Add a new user. Ida Lucente - January, 2021 Update password from User Profile. Ida Lucente - January, 2021 Collect the client secret. Ida Lucente - January, 2021 Setup 2FA. Ida Lucente - January, 2021 API usage example. Ida Lucente - January, 2021",
    "url": "/docs/en/tutorials",
    "relUrl": "/docs/en/tutorials"
  },
  "67": {
    "id": "67",
    "title": "User Management",
    "content": "The Annotation Lab offers user management features. The admin user can add or remove a user from the data base or can edit user information if necessary. This feature is available by navigating to the lower left side menu and selecting User Management feature. All users that have beed added to the current Annotation Lab instance can be seen on the Users screen. First Name, Last Name and e-mail address information should be available for all users. An admin user can edit those information, add a user to a group or change a user’s password. User Details For each user, the Annotation Lab stores basic information such as: the First Name, Last Name, e-mail address. Those can be edited from the User Details page by any Admin User. User Groups Currently the Annotation Lab defines two user groups: Annotators and UserAdmins. By default a new user is added to the Annotators group. This means the user will not have access to any admin features such as: User Management or Settings. For adding a user to the Admin group, a admin user needs to navigate to the Users screen, click on the edit button for the concerned user, then click on the Groups tab and check the Admin checkbox. Reset User Credentials An admin user can change the login credentials for another user, by navigating to the User Credentials tab and by defining a new (temporary) password. For extra protection, the admin user can also enforce the password change on next user login.",
    "url": "/docs/en/user_management",
    "relUrl": "/docs/en/user_management"
  },
  "68": {
    "id": "68",
    "title": "Workflow Setup",
    "content": "When a team of people work together on a large annotation project, the tasks can be organized into a multi-step workflow for an easier management of the team collaboration. This is also necessary when the project has strict requirements on the labels: e.g. the same document must be labeled by multiple annotators; the annotations must be checked by a senior annotator. In this situation, a workflow can be setup using the task tagging functionality provided by the Annotation Lab. Then can be used for splitting work across the team but also for differentiating between first-level annotators and second-level reviewers. A full audit trail is kept on each action performed by all actors. Each saved entry is stored along with an authenticated user and a time stamp, and the user interface includes a visual comparison between versions. To add a tag, select a task and press Tags &gt; Add more. Tasks can be filtered by tags, making it easier to identify, for example, which documents are completed and which ones need to be reviewed.",
    "url": "/docs/en/workflow",
    "relUrl": "/docs/en/workflow"
  }
  
}
