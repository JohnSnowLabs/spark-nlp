{
  "0": {
    "id": "0",
    "title": "404",
    "content": "404 Page not found :( &lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; &lt;script async src=https://www.googletagmanager.com/gtag/js?id=UA-70312582-1&gt;&lt;/script&gt; Multilingual DistilBertForMaskedLM Cased model (from hf-maintainers) HomeDocsLearnModelsDemo Edit on GitHub John Snow Labs Feb 23, 2023 Multilingual DistilBertForMaskedLM Cased model (from hf-maintainers)distilbertopen_sourcedistilbert_embeddingsdistilbertformaskedlmxxtensorflow Description Pretrained DistilBertForMaskedLM model, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. distilbert-base-multilingual-cased is a Multilingual model originally trained by hf-maintainers. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU documentAssembler = DocumentAssembler() .setInputCols([&quot;text&quot;]) .setOutputCols(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_embeddings_base_multilingual_cased&quot;,&quot;xx&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(True) pipeline = Pipeline(stages=[documentAssembler, tokenizer, embeddings]) data = spark.createDataFrame([[&quot;I love Spark NLP&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) val documentAssembler = new DocumentAssembler() .setInputCols(Array(&quot;text&quot;)) .setOutputCols(Array(&quot;document&quot;)) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_embeddings_base_multilingual_cased&quot;,&quot;xx&quot;) .setInputCols(Array(&quot;document&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(True) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer, embeddings)) val data = Seq(&quot;I love Spark NLP&quot;).toDS.toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) Model Information Model Name: distilbert_embeddings_base_multilingual_cased Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [sentence, token] Output Labels: [embeddings] Language: xx Size: 505.8 MB Case sensitive: false References https://huggingface.co/distilbert-base-multilingual-cased PREVIOUSChinese Deberta Embeddings Cased model (from IDEA-CCNL) © John Snow Labs Inc. Terms of Service | Privacy Policy &lt;/html&gt;",
    "url": "/404.html",
    "relUrl": "/404.html"
  },
  "1": {
    "id": "1",
    "title": "CPU NER Benchmarks",
    "content": "CPU NER Benchmarks NER (BiLSTM-CNN-Char Architecture) CPU Benchmark Experiment Dataset : 1000 Clinical Texts from MTSamples Oncology Dataset, approx. 500 tokens per text. Versions : spark-nlp Version: v3.4.4 spark-nlp-jsl Version : v3.5.2 Spark Version : v3.1.2 Spark NLP Pipeline : nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings_clinical, clinical_ner, ner_converter ]) NOTE: Spark NLP Pipeline output data frame (except word_embeddings column) was written as parquet format in transform benchmarks. Plarform Process Repartition Time 2 CPU cores, 13 GB RAM (Google COLAB) LP (fullAnnotate) - 16min 52s Transform (parquet) 10 4min 47s 100 4min 16s 1000 5min 4s 16 CPU cores, 27 GB RAM (AWS EC2 machine) LP (fullAnnotate) - 14min 28s Transform (parquet) 10 1min 5s 100 1min 1s 1000 1min 19s",
    "url": "/docs/en/cpu-ner-benchmark",
    "relUrl": "/docs/en/cpu-ner-benchmark"
  },
  "2": {
    "id": "2",
    "title": "GPU vs CPU benchmark",
    "content": "This section includes benchmarks for different Approach() (training classes), comparing their performance when running in m5.8xlarge CPU vs a Tesla V100 SXM2 GPU, as described in the Machine Specs section below. Different benchmarks, as well as their takeaways and some conclusions of how to get the best of GPU, are included as well, to guide you in the process of getting the best performance out of Spark NLP on GPU. Each major release comes with big improvements, so please, make sure you use at least that version to fully levearge Spark NLP capabilities on GPU. Machine specs CPU An AWS m5.8xlarge machine was used for the CPU benchmarking. This machine consists of 32 vCPUs and 128 GB of RAM, as you can check in the official specification webpage available here GPU A Tesla V100 SXM2 GPU with 32GB of memory was used to calculate the GPU benchmarking. Versions The benchmarking was carried out with the following Spark NLP versions: Spark version: 3.0.2 Hadoop version: 3.2.0 SparkNLP version: 3.3.4 Spark nodes: 1 Benchmark on classifierDLApproach() This experiment consisted of training a Deep Learning Binary Classifier (Question vs Statement classes) at sentence-level, using a fully connected CNN and Bert Sentence Embeddings. Only 1 Spark node was usd for the training. We used the Spark NLP class ClassifierDL and it’s method Approach() as described in the documentation. The pipeline looks as follows: Dataset The size of the dataset was relatively small (200K), consisting of: Training (rows): 162250 Test (rows): 40301 Training params Different batch sizes were tested to demonstrate how GPU performance improves with bigger batches compared to CPU, for a constant number of epochs and learning rate. Epochs: 10 Learning rate: 0.003 Batch sizes: 32, 64, 256, 1024 Results Even for this average-sized dataset, we can observe that GPU is able to beat the CPU machine by a 76% in both training and inference times. Training times depending on batch (in minutes) Batch size CPU GPU 32 66 16.1 64 65 15.3 256 64 14.5 1024 64 14 Inference times (in minutes) The average inference time remained more or less constant regardless the batch size: CPU: 8.7 min GPU: 2 min Performance metrics A weighted F1-score of 0.88 was achieved, with a 0.90 score for question detection and 0.83 for statements. Benchmark on NerDLApproach() This experiment consisted of training a Name Entity Recognition model (token-level), using our class NerDLApproach(), using Bert Word Embeddings and a Char-CNN-BiLSTM Neural Network. Only 1 Spark node was used for the training. We used the Spark NLP class NerDL and it’s method Approach() as described in the documentation. The pipeline looks as follows: Dataset The size of the dataset was small (17K), consisting of: Training (rows): 14041 Test (rows): 3250 Training params Different batch sizes were tested to demonstrate how GPU performance improves with bigger batches compared to CPU, for a constant number of epochs and learning rate. Epochs: 10 Learning rate: 0.003 Batch sizes: 32, 64, 256, 512, 1024, 2048 Results Even for this small dataset, we can observe that GPU is able to beat the CPU machine by a 62% in training time and a 68% in inference times. It’s important to mention that the batch size is very relevant when using GPU, since CPU scales much worse with bigger batch sizes than GPU. Training times depending on batch (in minutes) Batch size CPU GPU 32 9.5 10 64 8.1 6.5 256 6.9 3.5 512 6.7 3 1024 6.5 2.5 2048 6.5 2.5 Inference times (in minutes) Although CPU times in inference remain more or less constant regardless the batch sizes, GPU time experiment good improvements the bigger the batch size is. CPU times: ~29 min Batch size GPU 32 10 64 6.5 256 3.5 512 3 1024 2.5 2048 2.5 Performance metrics A macro F1-score of about 0.92 (0.90 in micro) was achieved, with the following charts extracted from the NERDLApproach() logs: Inference benchmark on BertSentenceEmbeddings() This experiment consisted of benchmarking the improvement obtained in inference by using GPU on BertSentenceEmbeddings(). We used the Spark NLP class BertSentenceEmbeddings() described in the Transformers documentation. The pipeline contains only two components and looks as follows: Dataset The size of the dataset was bigger than the previous ones, with 417735 rows for inference. Results We have observed in previous experiments, using BertSentenceEmbeddings (classifierDL) and also BertEmbeddings (NerDL) how GPU improved both training and inference times. In this case, we observe again big improvements in inference, what is already pointing that one of the main reasons of why GPU improves so much over CPU is the better management of Embeddings (word, sentence level) and bigger batch sizes. Batch sizes: 32, 64, 256, 1024 Inference times depending on batch (in minutes) Batch size CPU GPU 32 80 9.9 64 77 9.8 256 63 9.4 1024 62 9.1 Takeaways: How to get the best of the GPU You will experiment big GPU improvements in the following cases: Embeddings and Transformers are used in your pipeline. Take into consideration that GPU will performance very well in Embeddings / Transformer components, but other components of your pipeline may not leverage as well GPU capabilities; Bigger batch sizes get the best of GPU, while CPU does not scale with bigger batch sizes; Bigger dataset sizes get the best of GPU, while may be a bottleneck while running in CPU and lead to performance drops; MultiGPU training Right now, we don’t support multigpu training (1 model in different GPUs in parallel), but you can train different models in different GPU. Where to look for more information about Training Please, take a look at the Spark NLP and Spark NLP for Healthcare Training sections, and feel free to reach us out in case you want to maximize the performance on your GPU.",
    "url": "/docs/en/CPUvsGPUbenchmark",
    "relUrl": "/docs/en/CPUvsGPUbenchmark"
  },
  "3": {
    "id": "3",
    "title": "GPU vs CPU benchmark",
    "content": "This section includes a benchmark for MedicalNerApproach(), comparing its performance when running in m5.8xlarge CPU vs a Tesla V100 SXM2 GPU, as described in the Machine Specs section below. Big improvements have been carried out from version 3.3.4, so please, make sure you use at least that version to fully levearge Spark NLP capabilities on GPU. Machine specs CPU An AWS m5.8xlarge machine was used for the CPU benchmarking. This machine consists of 32 vCPUs and 128 GB of RAM, as you can check in the official specification webpage available here GPU A Tesla V100 SXM2 GPU with 32GB of memory was used to calculate the GPU benchmarking. Versions The benchmarking was carried out with the following Spark NLP versions: Spark version: 3.0.2 Hadoop version: 3.2.0 SparkNLP version: 3.3.4 SparkNLP for Healthcare version: 3.3.4 Spark nodes: 1 Benchmark on MedicalNerDLApproach() This experiment consisted of training a Name Entity Recognition model (token-level), using our class NerDLApproach(), using Bert Word Embeddings and a Char-CNN-BiLSTM Neural Network. Only 1 Spark node was used for the training. We used the Spark NLP class MedicalNer and it’s method Approach() as described in the documentation. The pipeline looks as follows: Dataset The size of the dataset was small (17K), consisting of: Training (rows): 14041 Test (rows): 3250 Training params Different batch sizes were tested to demonstrate how GPU performance improves with bigger batches compared to CPU, for a constant number of epochs and learning rate. Epochs: 10 Learning rate: 0.003 Batch sizes: 32, 64, 256, 512, 1024, 2048 Results Even for this small dataset, we can observe that GPU is able to beat the CPU machine by a 62% in training time and a 68% in inference times. It’s important to mention that the batch size is very relevant when using GPU, since CPU scales much worse with bigger batch sizes than GPU. Training times depending on batch (in minutes) Batch size CPU GPU 32 9.5 10 64 8.1 6.5 256 6.9 3.5 512 6.7 3 1024 6.5 2.5 2048 6.5 2.5 Inference times (in minutes) Although CPU times in inference remain more or less constant regardless the batch sizes, GPU time experiment good improvements the bigger the batch size is. CPU times: ~29 min Batch size GPU 32 10 64 6.5 256 3.5 512 3 1024 2.5 2048 2.5 Performance metrics A macro F1-score of about 0.92 (0.90 in micro) was achieved, with the following charts extracted from the MedicalNerApproach() logs: Takeaways: How to get the best of the GPU You will experiment big GPU improvements in the following cases: Embeddings and Transformers are used in your pipeline. Take into consideration that GPU will performance very well in Embeddings / Transformer components, but other components of your pipeline may not leverage as well GPU capabilities; Bigger batch sizes get the best of GPU, while CPU does not scale with bigger batch sizes; Bigger dataset sizes get the best of GPU, while may be a bottleneck while running in CPU and lead to performance drops; MultiGPU Inference on Databricks In this part, we will give you an idea on how to choose appropriate hardware specifications for Databricks. Here is a few different hardwares, their prices, as well as their performance: Apparently, GPU hardware is the cheapest among them although it performs the best. Let’s see how overall performance looks like: Figure above clearly shows us that GPU should be the first option of ours. In conclusion, please find the best specifications for your use case since these benchmarks might depend on dataset size, inference batch size, quickness, pricing and so on. Please refer to this video for further info: https://events.johnsnowlabs.com/webinar-speed-optimization-benchmarks-in-spark-nlp-3-making-the-most-of-modern-hardware?hsCtaTracking=a9bb6358-92bd-4cf3-b97c-e76cb1dfb6ef%7C4edba435-1adb-49fc-83fd-891a7506a417 MultiGPU training Currently, we don’t support multiGPU training, meaning training 1 model in different GPUs in parallel. However, you can train different models in different GPUs. MultiGPU inference Spark NLP can carry out MultiGPU inference if GPUs are in different cluster nodes. For example, if you have a cluster with different GPUs, you can repartition your data to match the number of GPU nodes and then coalesce to retrieve the results back to the master node. Currently, inference on multiple GPUs on the same machine is not supported. Where to look for more information about Training Please, take a look at the Spark NLP and Spark NLP for Healthcare Training sections, and feel free to reach us out in case you want to maximize the performance on your GPU.",
    "url": "/docs/en/CPUvsGPUbenchmark_healthcare",
    "relUrl": "/docs/en/CPUvsGPUbenchmark_healthcare"
  },
  "4": {
    "id": "4",
    "title": "Active Learning",
    "content": "Project Owners or Managers can enable the Active Learning feature by clicking on the corresponding Switch available on Model Training tab. If this feature is enabled, the NER training gets triggered automatically on every 50/100/200 new completions. It is possible to change the target completions number by dropdown which is visible only when Active Learning is enabled. While enabling this feature, users are asked whether they want to deploy the newly trained model right after the training process or not. If the user chooses not to automatically deploy the newly trained model, this can be done on demand by navigating to the target project Setup &gt; Configuration &gt; 3. Predefined Labels. Search for the new model by name of the project, select it and add it to your configuration. This will update the Project Configuration (the name of the model is changed in the corresponding label tags). Training date and time of each trained model is also displayed in the predefined labels widget. If the user opts to deploy the model after the training, the Project Configuration is automatically updated for each label that is included in the newly trained model. The value of the model param is updated with the name of the new model. If there is any mistake in the name of models, the validation error is displayed in the Interface Preview Section present on the right side of the Labeling Config area.",
    "url": "/docs/en/alab/active_learning",
    "relUrl": "/docs/en/alab/active_learning"
  },
  "5": {
    "id": "5",
    "title": "Analytics Permission",
    "content": "By default, dashboards in the Analytics page is disabled for a project. Users can request the admin to enable the Analytics page. The request is then listed on the Analytics Request page under the Settings menu. This page is only accessible to the admin user. After the admin user approves the request, the user can access the various dashboards in the Analytics page. Analytics Requests The Analytics Requests page lists all the pending requests for the Analytics page from one or more users. The admin user can grant or deny the permission to the requests as needed. It is accessible from Settings &gt; Analytics Requests. Each request contains information such as the name of project for which the analytics request was made, the user who initiated the request, and the date when the request was made. Granting a request All the requests granted by the admin user is listed under this tab. The table shows information about the granted requests, like the name of the project for which the analytics request was made, the user who initiated the request, the user who granted the request, the date when the request was granted, the latest date when the analytics were updated. The admin user can also revoke an already granted request from this list. Denying/Revoking a request All the requests denied or revoked by the admin user is listed under this tab. The table shows information about the denied/revoked requests, like the name of the project for which the analytics request was made, the user who initiated the request, the user who denied/revoked the request, the date when the request was denied/revoked, the latest date when the analytics were updated.",
    "url": "/docs/en/alab/analytics_permission",
    "relUrl": "/docs/en/alab/analytics_permission"
  },
  "6": {
    "id": "6",
    "title": "Analyze Clinical Notes - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/analyze_clinical_notes",
    "relUrl": "/analyze_clinical_notes"
  },
  "7": {
    "id": "7",
    "title": "Analyze Medical Texts in Spanish - Medical NLP Demos & Notebooks",
    "content": "",
    "url": "/analyze_medical_text_spanish",
    "relUrl": "/analyze_medical_text_spanish"
  },
  "8": {
    "id": "8",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/analyze_non_english_medical_text",
    "relUrl": "/analyze_non_english_medical_text"
  },
  "9": {
    "id": "9",
    "title": "Analyze Non-English Text & Documents - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/analyze_non_english_text_documents",
    "relUrl": "/analyze_non_english_text_documents"
  },
  "10": {
    "id": "10",
    "title": "Analyze Spelling & Grammar - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/analyze_spelling_grammar",
    "relUrl": "/analyze_spelling_grammar"
  },
  "11": {
    "id": "11",
    "title": "Manual Annotation",
    "content": "The Annotation Lab keeps a human expert as productive as possible. It minimizes the number of mouse clicks, keystrokes, and eye movements in the main workflow. The continuous improvement in the UI and the UX is from iterative feedback from the users. Annotation Lab supports keyboard shortcuts for all types of annotations. It enables having one hand on the keyboard, one hand on the mouse, and both eyes on the screen at all times. One-click completion and automatic switching to the next task keep experts in the loop. On the header of the Labeling area, you can find the list of labels defined for the project. In the center, it displays the content of the task. On the right, there are several widgets categorized into different groups. Annotations Versions Progress Labeling Widgets Completions A completion is a list of annotations manually defined by a user for a given task. After completing annotation on a task (e.g., all entities highlighted in the text, or one or more classes is assigned to the task in the case of classification projects) user clicks on the Save button to save their progress or Submit button to submit the completion. A submitted completion is no longer editable, and the user cannot delete it. Creating a new copy of the submitted completion is the only option to edit it. An annotator can modify or delete their completions only if completions are not submitted yet. Dedicated action icons are available on the completions widgets to allow users to quickly run actions like delete, copy, set ground-truth. It is an important to ensure a complete audit trail of all user actions. Annotation Lab tracks the history and details of any deleted completions. It means it is possible to see the name of the completion creator, date of creation, and deletion. Predictions A prediction is a list of annotations created automatically by Spark NLP pre-trained models or from the rules. A project owner/manager can create predictions using the Pre-Annotate button from the Tasks page. Predictions are read-only, which means users can see the predictions but cannot modify those. To reuse a prediction to bootstrap the annotation process, users can copy it to a new completion. This new completion bootstrapped from the prediction is editable. Confidence From version 3.3.0, running pre-annotations on a text project provides one extra piece of information for the automatic annotations - the confidence score. This score shows the confidence the model has for each of the labeled chunks it predicts. It is calculated based on the benchmarking information of the model used to pre-annotate and the score of each prediction. The confidence score is available when working on Named Entity Recognition, Relation Extraction, Assertion, and Classification projects and is also generated when using Rules. On the Labeling page, when selecting the Prediction widget, users can see all preannotation in the Annotations section with a score assigned to them. Using the confidence slider, users can filter out low confidence labels before starting to edit/correct the labels. Both Accept Prediction and Add a new completion based on this prediction operation apply to the filtered annotations from the confidence slider. Annotations The Annotations widget has two sections. Regions Gives a list overview of all annotated chunks. When you click on any annotation, it gets automatically highlighted in the labeling editor. We can edit or remove annotations from here. Relations Lists all the relations that have been created. When the user moves the mouse over any one relation, it is highlighted in the labeling editor. Progress Annotator/Reviewer can see their overall work progress from within the labeling page. The status is calculated for their assigned work. For Annotator View: For Reviewer View: Text Annotation Named Entity Recognition To extract information using NER labels, we first click on the label to select it or press the shortcut key assigned to it, and then, with the mouse, select the relevant part of the text. We can easily edit the incorrect labeling by clicking on the labeled text and then selecting the new label you want to assign to this text. To delete the label from the text, we first click on the text on the labeling editor and then press backspace. Trim leading and ending special characters in annotated chunks When annotating text, it is possible and probable that the annotation is not very precise and the chunks contain leading/trailing spaces and punctuation marks. By default all the leading/trailing spaces and punctuation marks are excluded from the annotated chunk. The labeling editor settings has a new configuration option that can be used to enable/disable this feature if necessary. Assertion Labels To add an assertion label to an extracted entity, select the assertion label and select the labeled entity (from NER) in the labeling editor. After this, the extracted entity will have two labels - one for NER and one for assertion. In the example below, the chunks heart disease, kidney disease, stroke etc., were extracted first using the NER label - Symptom (pink color) and then the assertion label - Absent (green color). Relation Extraction Creating relations with the Annotation Lab is very simple. First, click on any one labeled entity, then press the r key and click on the second labeled entity. You can add a label to the relation, change its direction or delete it using the contextual menu displayed next to the relation arrow or from the relation box. Cross page Annotation From version 2.8.0, Annotation Lab supports cross-page NER annotation for Text projects. It means that Annotators can annotate a chunk starting at the bottom of one page and finishing on the next page. This feature is also available for Relations. Previously, relations were created between chunks located on the same page. But now, relations can be created among tokens located on different pages. The way to do this is to first change the pagination settings to include the tokens to be linked on the same page, then create the relation annotation between the tokens and finally go back to the original pagination settings. The annotation is presented through connectors after updating the pagination. Visual NER Annotation Annotating text included in image documents (e.g., scanned documents) is a common use case in many verticals but comes with several challenges. With the Visual NER labeling config, we aim to ease the work of annotators by allowing them to select text from an image and assign the corresponding label to it. This feature is powered by Visual NLP library; hence a valid Visual NLP license is required to get access to it. Here is how we can use it: Upload a valid [Visual NLP](/docs/en/ocr) license. See how to do this here. Create a new project, specify a name for your project, add team members if necessary, and from the list of predefined templates (Default Project Configs) choose Visual NER Labeling under IMAGE content type. Update the configuration if necessary. This might be useful if you want to use other labels than the default ones. Click the Save Config button. While saving the project, a confirmation dialog is displayed to ask if you want to deploy the OCR pipeline. Select Yes from the confirmation dialog. Import the tasks you want to annotate (images or PDF documents). Start annotating text on top of the image by clicking on the text tokens, or by drawing bounding boxes on top of chunks or image areas. Export annotations in your preferred format. The entire process is illustrated below: Support for multi-page PDF documents When a valid Visual NLP license is available, Annotation Lab offers support for multi-page PDF annotation. We can import, annotate, and export multi-page PDF files easily. Users have two options for importing a new PDF file into the Visual NER project: Import PDF file from local storage. Add a link to the PDF file in the file attribute. After import, the task becomes available on the Tasks Page. The title of the new task is the name of the imported file. On the labeling page, the PDF viewer has pagination support so that annotators can annotate on the PDF document one page at a time. Users can also jump to a specific page in multi-page task, instead of passing through all pages to reach a target section of a PDF document. Support for multiple OCR servers Just like for Preannotation servers, Annotation Lab supports deployment of multiple OCR servers. If a user has uploaded a Visual NLP license, OCR inference is enabled. To work on a Visual NER project, users have to deploy at least one OCR server. Any OCR server can perform preannotation. To select the OCR server, users need to go to the Import page, click on the OCR Server button on the top-right corner and from the popup, choose one of the available OCR servers. If no suitable OCR server is present, you can create a new server by selecting the Create Server option and then clicking on the Deploy button.",
    "url": "/docs/en/alab/annotation",
    "relUrl": "/docs/en/alab/annotation"
  },
  "12": {
    "id": "12",
    "title": "Configurations",
    "content": "Simplified workflow Direct Submit Using the classical annotation workflow, when an annotator works on a task, a series of actions are necessary for creating a new annotation and submitting it as ground truth: Create the completion Save the completion Submit the completion Confirm submission Load next task This process is adapted for more complex workflows and large tasks. For simple projects with smaller tasks, Annotation Lab now offers a simplified workflow. Annotators can submit a completion with just one click. The Project Owner/Manager can activate this option from the Settings dialog (Customize Labels) in the Configuration step of the Setup page. Once enabled, annotators can see the submit button on the labeling page. A second option is available on the same dialog for Project Owner/Manager: Serve next task after completion submission. Once enabled, annotators can see the next task on the labeling page after submitting the completion for the current task. Note: Annotator can Save/Update completion using CTRL+Enter Annotator can Submit completion using ALT+Enter Accept Prediction When predictions are available for a task, Annotator can accept the predictions with just one click and navigate automatically to the next task. When users click on Accept Prediction, a new completion is created based on the prediction, then submitted as ground truth, and the next task in line (assigned to the current annotator/reviewer and with Incomplete or In Progress status) is automatically served. NOTE: Press backspace key (on windows) or delete key (on mac) to delete the selected relation from the labeling editor or use the delete action icon on the Relations widget. Labeling editor Settings The labeling editor offers some configurable features. For example, you can modify the editor’s layout, show or hide predictions, annotations, or the confidence panel, show or hide various controls and information. It is also possible to keep a label selected after creating a region, display labels on bounding boxes, polygons and other regions while labeling, and show line numbers for text labeling. Enable labeling hotkeys This option enables/disable the hotkeys assigned to taxonomy labels to use the hotkeys during the annotation process. Show hotkey tooltips This option shows/hides the hotkey and tooltip on the taxonomy label and the control buttons. Enable labeling hotkeys must be enabled for this option to work. Show labels inside the regions When you enable this option, the labels assigned to each annotated region are displayed on the respective region. Keep label selected after creating a region This option helps users quickly annotate sequences of the same label by keeping the label selected after the annotation of a region. With the option unchecked: With the option checked: Select regions after creating This option keeps the annotated region selected after annotation. In this way, it will be easier for users to quickly change the assigned label for the last selected region if necessary. Show line numbers for Text This option adds line numbers to the text content to annotate in the labeling editor. Label all occurrences of selected text When checked, this option allow users to annotate all occurences of a text in the current task in one step. Labeling editor Customizations The Labeling editor is highly customizable. Project Owners and Managers can change the layout of their projects based on their needs. Search filter for a large number of labels When a project has a large number of NER/Assertion labels in the taxonomy, the display of the taxonomy takes a lot of screen space, and it is difficult for annotators to navigate through all labels. To tackle this challenge, Annotation Lab supports search for labels in NER projects (an autocomplete search option). To add the search bar for NER Labels or Choices, use the Filter tag as shown in the following XML configuration. &lt;Filter /&gt; &lt;View&gt; *** enclose labels tags here *** &lt;/View&gt; &lt;View&gt; *** enclose text tags here *** &lt;/View&gt; Parameters: The following parameters/attributes can be used within the Filter tag. Param Type Default Description placeholder string Quick Filter Placeholder text for filter minlength number 3 Size of the filter style string   CSS style of the string hotkey string   Hotkey to use to focus on the filter text area Usage Example: &lt;Filter placeholder=&quot;Quick Filter&quot;/&gt; For obtaining the above display on a NER project, the config should look as follows: &lt;View&gt; &lt;Filter name=&quot;fl&quot; toName=&quot;label&quot; hotkey=&quot;shift+f&quot; minlength=&quot;1&quot; /&gt; &lt;Labels name=&quot;label&quot; toName=&quot;text&quot;&gt; &lt;Label value=&quot;CARDINAL&quot; model=&quot;ner_onto_100&quot; background=&quot;#af906b&quot;/&gt; &lt;Label value=&quot;EVENT&quot; model=&quot;ner_onto_100&quot; background=&quot;#f384e1&quot;/&gt; ... &lt;Label value=&quot;LANGUAGE&quot; model=&quot;ner_onto_100&quot; background=&quot;#c0dad2&quot;/&gt; &lt;/Labels&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;/&gt; &lt;/View&gt; Notice how users can search for the desired label using the filter bar: Resizable label and text container While annotating longer text documents annotators may need to scroll to the top of the document for selecting the label to use, and then scroll down to create a label. Also, if the text is large, annotators have to scroll to a certain section because the textbox size is fixed. In those cases, the annotation experience can be improved by creating a scrollable labeling area and textbox area. To add the scroll bar, the View tag with a fixed height and overflow-y:scroll style property can be used as shown in the following XML config structure: &lt;View style=&quot;background:white; height: 100px; overflow-y:scroll; resize:vertical; position:sticky; top:0;&quot;&gt; *** enclose labels tags here *** &lt;/View&gt; &lt;View style=&quot;resize:vertical; margin-top:10px; max-height:400px; overflow-y:scroll;&quot;&gt; **** enclose text tags here** &lt;/View&gt; Once it has been added and saved to the Project Configuration, the scroll bar should be visible. Example Using the following Project Configuration &lt;View&gt; &lt;Filter name=&quot;fl&quot; toName=&quot;label&quot; hotkey=&quot;shift+f&quot; minlength=&quot;1&quot; /&gt; &lt;View style=&quot;background:white; height: 100px; overflow-y:scroll; resize:vertical; position:sticky; top:0;&quot;&gt; &lt;Labels name=&quot;label&quot; toName=&quot;text&quot;&gt; &lt;Label value=&quot;CARDINAL&quot; model=&quot;ner_onto_100&quot; background=&quot;#af906b&quot;/&gt; &lt;Label value=&quot;EVENT&quot; model=&quot;ner_onto_100&quot; background=&quot;#f384e1&quot;/&gt; &lt;Label value=&quot;WORK_OF_ART&quot; model=&quot;ner_onto_100&quot; background=&quot;#0fbca4&quot;/&gt; ... &lt;Label value=&quot;LANGUAGE&quot; model=&quot;ner_onto_100&quot; background=&quot;#c0dad2&quot;/&gt; &lt;/Labels&gt; &lt;/View&gt; &lt;View style=&quot;resize:vertical; margin-top:10px; max-height:400px; overflow-y:scroll;&quot;&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;&gt;&lt;/Text&gt; &lt;/View&gt; &lt;/View&gt; we’ll obtain the output illustrated below: Toggle Preview Window Label configuration editor and Preview Window covers 50/50 part of the screen. It can make editing larger XML configurations difficult. For a better editing experience, we can use the Toggle Preview Window button to have the editor use full screen width. Switch Role For users having multiple roles (Annotator/Reviewer/Manager) the labeling page can get confusing. Switch Role filter present on the top-right corner can help address this problem. This filter was introduced in Annotation Lab from version 2.6.0, previously refered to as View As filter. When selecting Annotator option, the view changes to facilitate annotating the task. Similar changes to the view applies when switching to Reviewer or Manager option. The selection persists even when the tab is closed or refreshed.",
    "url": "/docs/en/alab/annotation_configurations",
    "relUrl": "/docs/en/alab/annotation_configurations"
  },
  "13": {
    "id": "13",
    "title": "Annotators",
    "content": "How to read this section All annotators in Spark NLP share a common interface, this is: Annotation: Annotation(annotatorType, begin, end, result, meta-data, embeddings) AnnotatorType: some annotators share a type. This is not only figurative, but also tells about the structure of the metadata map in the Annotation. This is the one referred in the input and output of annotators. Inputs: Represents how many and which annotator types are expected in setInputCols(). These are column names of output of other annotators in the DataFrames. Output Represents the type of the output in the column setOutputCol(). There are two types of Annotators: Approach: AnnotatorApproach extend Estimators, which are meant to be trained through fit() Model: AnnotatorModel extend from Transformers, which are meant to transform DataFrames through transform() Model suffix is explicitly stated when the annotator is the result of a training process. Some annotators, such as Tokenizer are transformers, but do not contain the word Model since they are not trained annotators. Model annotators have a pretrained() on it’s static object, to retrieve the public pre-trained version of a model. pretrained(name, language, extra_location) -&gt; by default, pre-trained will bring a default model, sometimes we offer more than one model, in this case, you may have to use name, language or extra location to download them. Available Annotators Annotator Description Version BigTextMatcher Annotator to match exact phrases (by token) provided in a file against a Document. Opensource Chunk2Doc Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result. Opensource ChunkEmbeddings This annotator utilizes WordEmbeddings, BertEmbeddings etc. to generate chunk embeddings from either Chunker, NGramGenerator, or NerConverter outputs. Opensource ChunkTokenizer Tokenizes and flattens extracted NER chunks. Opensource Chunker This annotator matches a pattern of part-of-speech tags in order to return meaningful phrases from document. Opensource ClassifierDL ClassifierDL for generic Multi-class Text Classification. Opensource ContextSpellChecker Implements a deep-learning based Noisy Channel Model Spell Algorithm. Opensource DateMatcher Matches standard date formats into a provided format. Opensource DependencyParser Unlabeled parser that finds a grammatical relation between two words in a sentence. Opensource Doc2Chunk Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol. Opensource Doc2Vec Word2Vec model that creates vector representations of words in a text corpus. Opensource DocumentAssembler Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. Opensource DocumentNormalizer Annotator which normalizes raw text from tagged text, e.g. scraped web pages or xml documents, from document type columns into Sentence. Opensource EntityRuler Fits an Annotator to match exact strings or regex patterns provided in a file against a Document and assigns them an named entity. Opensource EmbeddingsFinisher Extracts embeddings from Annotations into a more easily usable form. Opensource Finisher Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. Opensource GraphExtraction Extracts a dependency graph between entities. Opensource GraphFinisher Helper class to convert the knowledge graph from GraphExtraction into a generic format, such as RDF. Opensource ImageAssembler Prepares images read by Spark into a format that is processable by Spark NLP. Opensource LanguageDetectorDL Language Identification and Detection by using CNN and RNN architectures in TensorFlow. Opensource Lemmatizer Finds lemmas out of words with the objective of returning a base dictionary word. Opensource MultiClassifierDL Multi-label Text Classification. Opensource MultiDateMatcher Matches standard date formats into a provided format. Opensource MultiDocumentAssembler Prepares data into a format that is processable by Spark NLP. Opensource NGramGenerator A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). Opensource NerConverter Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. Opensource NerCrf Extracts Named Entities based on a CRF Model. Opensource NerDL This Named Entity recognition annotator is a generic NER model based on Neural Networks. Opensource NerOverwriter Overwrites entities of specified strings. Opensource Normalizer Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary. Opensource NorvigSweeting Spellchecker Retrieves tokens and makes corrections automatically if not found in an English dictionary. Opensource POSTagger (Part of speech tagger) Averaged Perceptron model to tag words part-of-speech. Opensource RecursiveTokenizer Tokenizes raw text recursively based on a handful of definable rules. Opensource RegexMatcher Uses rules to match a set of regular expressions and associate them with a provided identifier. Opensource RegexTokenizer A tokenizer that splits text by a regex pattern. Opensource SentenceDetector Annotator that detects sentence boundaries using regular expressions. Opensource SentenceDetectorDL Detects sentence boundaries using a deep learning approach. Opensource SentenceEmbeddings Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols). Opensource SentimentDL Annotator for multi-class sentiment analysis. Opensource SentimentDetector Rule based sentiment detector, which calculates a score based on predefined keywords. Opensource Stemmer Returns hard-stems out of words with the objective of retrieving the meaningful part of the word. Opensource StopWordsCleaner This annotator takes a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences. Opensource SymmetricDelete Spellchecker Symmetric Delete spelling correction algorithm. Opensource TextMatcher Matches exact phrases (by token) provided in a file against a Document. Opensource Token2Chunk Converts TOKEN type Annotations to CHUNK type. Opensource TokenAssembler This transformer reconstructs a DOCUMENT type annotation from tokens, usually after these have been normalized, lemmatized, normalized, spell checked, etc, in order to use this document annotation in further annotators. Opensource Tokenizer Tokenizes raw text into word pieces, tokens. Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. Opensource TypedDependencyParser Labeled parser that finds a grammatical relation between two words in a sentence. Opensource ViveknSentiment Sentiment analyser inspired by the algorithm by Vivek Narayanan. Opensource WordEmbeddings Word Embeddings lookup annotator that maps tokens to vectors. Opensource Word2Vec Word2Vec model that creates vector representations of words in a text corpus. Opensource WordSegmenter Tokenizes non-english or non-whitespace separated texts. Opensource YakeKeywordExtraction Unsupervised, Corpus-Independent, Domain and Language-Independent and Single-Document keyword extraction. Opensource Available Transformers Additionally, these transformers are available to generate embeddings. Transformer Description Version AlbertEmbeddings ALBERT: A Lite BERT for Self-supervised Learning of Language Representations Opensource AlbertForQuestionAnswering AlbertForQuestionAnswering can load ALBERT Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource AlbertForTokenClassification AlbertForTokenClassification can load ALBERT Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource AlbertForSequenceClassification AlbertForSequenceClassification can load ALBERT Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource BertEmbeddings Token-level embeddings using BERT. BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture. Opensource BertForQuestionAnswering BertForQuestionAnswering can load Bert Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource BertForSequenceClassification Bert Models with sequence classification/regression head on top. Opensource BertForTokenClassification BertForTokenClassification can load Bert Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource BertSentenceEmbeddings Sentence-level embeddings using BERT. BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture. Opensource CamemBertEmbeddings CamemBert is based on Facebook’s RoBERTa model released in 2019. Opensource CamemBertForSequenceClassification amemBertForSequenceClassification can load CamemBERT Models with sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for multi-class document classification tasks. Opensource CamemBertForTokenClassification CamemBertForTokenClassification can load CamemBERT Models with a token classification head on top Opensource DeBertaEmbeddings DeBERTa builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in RoBERTa. Opensource DeBertaForQuestionAnswering DeBertaForQuestionAnswering can load DeBERTa Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource DistilBertEmbeddings DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. Opensource DistilBertForQuestionAnswering DistilBertForQuestionAnswering can load DistilBert Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource DistilBertForSequenceClassification DistilBertForSequenceClassification can load DistilBERT Models with sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for multi-class document classification tasks. Opensource DistilBertForTokenClassification DistilBertForTokenClassification can load DistilBERT Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource ElmoEmbeddings Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark. Opensource GPT2Transformer GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. Opensource HubertForCTC Hubert Model with a language modeling head on top for Connectionist Temporal Classification (CTC). Opensource LongformerEmbeddings Longformer is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. Opensource LongformerForQuestionAnswering LongformerForQuestionAnswering can load Longformer Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource LongformerForSequenceClassification LongformerForSequenceClassification can load Longformer Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource LongformerForTokenClassification LongformerForTokenClassification can load Longformer Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource MarianTransformer Marian is an efficient, free Neural Machine Translation framework written in pure C++ with minimal dependencies. Opensource RoBertaEmbeddings RoBERTa: A Robustly Optimized BERT Pretraining Approach Opensource RoBertaForQuestionAnswering RoBertaForQuestionAnswering can load RoBERTa Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource RoBertaForSequenceClassification RoBertaForSequenceClassification can load RoBERTa Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource RoBertaForTokenClassification RoBertaForTokenClassification can load RoBERTa Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource RoBertaSentenceEmbeddings Sentence-level embeddings using RoBERTa. Opensource SpanBertCoref A coreference resolution model based on SpanBert. Opensource SwinForImageClassification SwinImageClassification is an image classifier based on Swin. Opensource T5Transformer T5 reconsiders all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Opensource TapasForQuestionAnswering TapasForQuestionAnswering is an implementation of TaPas - a BERT-based model specifically designed for answering questions about tabular data. Opensource UniversalSentenceEncoder The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks. Opensource ViTForImageClassification Vision Transformer (ViT) for image classification. Opensource Wav2Vec2ForCTC Wav2Vec2 Model with a language modeling head on top for Connectionist Temporal Classification (CTC). Opensource XlmRoBertaEmbeddings XlmRoBerta is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl Opensource XlmRoBertaForQuestionAnswering XlmRoBertaForQuestionAnswering can load XLM-RoBERTa Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource XlmRoBertaForSequenceClassification XlmRoBertaForSequenceClassification can load XLM-RoBERTa Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource XlmRoBertaForTokenClassification XlmRoBertaForTokenClassification can load XLM-RoBERTa Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource XlmRoBertaSentenceEmbeddings Sentence-level embeddings using XLM-RoBERTa. Opensource XlnetEmbeddings XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Opensource XlnetForTokenClassification XlnetForTokenClassification can load XLNet Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource XlnetForSequenceClassification XlnetForSequenceClassification can load XLNet Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource ZeroShotNer ZeroShotNerModel implements zero shot named entity recognition by utilizing RoBERTa transformer models fine tuned on a question answering task. Opensource BigTextMatcher ApproachModel Annotator to match exact phrases (by token) provided in a file against a Document. A text file of predefined phrases must be provided with setStoragePath. In contrast to the normal TextMatcher, the BigTextMatcher is designed for large corpora. For extended examples of usage, see the BigTextMatcherTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: BigTextMatcher Scala API: BigTextMatcher Source: BigTextMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the entities file is of the form # # ... # dolore magna aliqua # lorem ipsum dolor. sit # laborum # ... # # where each line represents an entity phrase to be extracted. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) data = spark.createDataFrame([[&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;]]).toDF(&quot;text&quot;) entityExtractor = BigTextMatcher() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setStoragePath(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(False) pipeline = Pipeline().setStages([documentAssembler, tokenizer, entityExtractor]) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity)&quot;).show(truncate=False) +--+ |col | +--+ |[chunk, 6, 24, dolore magna aliqua, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 53, 59, laborum, [sentence -&gt; 0, chunk -&gt; 1], []] | +--+ // In this example, the entities file is of the form // // ... // dolore magna aliqua // lorem ipsum dolor. sit // laborum // ... // // where each line represents an entity phrase to be extracted. import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.BigTextMatcher import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val data = Seq(&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;).toDF(&quot;text&quot;) val entityExtractor = new BigTextMatcher() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setStoragePath(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(false) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer, entityExtractor)) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity)&quot;).show(false) +--+ |col | +--+ |[chunk, 6, 24, dolore magna aliqua, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 53, 59, laborum, [sentence -&gt; 0, chunk -&gt; 1], []] | +--+ Instantiated model of the BigTextMatcher. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: BigTextMatcherModel Scala API: BigTextMatcherModel Source: BigTextMatcherModel Chunk2Doc Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result. Input Annotator Types: CHUNK Output Annotator Type: DOCUMENT Python API: Chunk2Doc Scala API: Chunk2Doc Source: Chunk2Doc Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline # Location entities are extracted and converted back into `DOCUMENT` type for further processing data = spark.createDataFrame([[1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;]]).toDF(&quot;id&quot;, &quot;text&quot;) # Extracts Named Entities amongst other things pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) chunkToDoc = Chunk2Doc().setInputCols(&quot;entities&quot;).setOutputCol(&quot;chunkConverted&quot;) explainResult = pipeline.transform(data) result = chunkToDoc.transform(explainResult) result.selectExpr(&quot;explode(chunkConverted)&quot;).show(truncate=False) ++ |col | ++ |[document, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []] | |[document, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]| ++ // Location entities are extracted and converted back into `DOCUMENT` type for further processing import spark.implicits._ import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.Chunk2Doc val data = Seq((1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;)).toDF(&quot;id&quot;, &quot;text&quot;) // Extracts Named Entities amongst other things val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) val chunkToDoc = new Chunk2Doc().setInputCols(&quot;entities&quot;).setOutputCol(&quot;chunkConverted&quot;) val explainResult = pipeline.transform(data) val result = chunkToDoc.transform(explainResult) result.selectExpr(&quot;explode(chunkConverted)&quot;).show(false) ++ |col | ++ |[document, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []] | |[document, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]| ++ ChunkEmbeddings This annotator utilizes WordEmbeddings, BertEmbeddings etc. to generate chunk embeddings from either Chunker, NGramGenerator, or NerConverter outputs. For extended examples of usage, see the Examples and the ChunkEmbeddingsTestSpec. Input Annotator Types: CHUNK, WORD_EMBEDDINGS Output Annotator Type: WORD_EMBEDDINGS Python API: ChunkEmbeddings Scala API: ChunkEmbeddings Source: ChunkEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Extract the Embeddings from the NGrams documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) nGrams = NGramGenerator() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;chunk&quot;) .setN(2) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) # Convert the NGram chunks into Word Embeddings chunkEmbeddings = ChunkEmbeddings() .setInputCols([&quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;chunk_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) pipeline = Pipeline() .setStages([ documentAssembler, sentence, tokenizer, nGrams, embeddings, chunkEmbeddings ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk_embeddings) as result&quot;) .select(&quot;result.annotatorType&quot;, &quot;result.result&quot;, &quot;result.embeddings&quot;) .show(5, 80) ++-+--+ | annotatorType| result| embeddings| ++-+--+ |word_embeddings| This is|[-0.55661, 0.42829502, 0.86661, -0.409785, 0.06316501, 0.120775, -0.0732005, ...| |word_embeddings| is a|[-0.40674996, 0.22938299, 0.50597, -0.288195, 0.555655, 0.465145, 0.140118, 0...| |word_embeddings|a sentence|[0.17417, 0.095253006, -0.0530925, -0.218465, 0.714395, 0.79860497, 0.0129999...| |word_embeddings|sentence .|[0.139705, 0.177955, 0.1887775, -0.45545, 0.20030999, 0.461557, -0.07891501, ...| ++-+--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.{NGramGenerator, Tokenizer} import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings import org.apache.spark.ml.Pipeline // Extract the Embeddings from the NGrams val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val nGrams = new NGramGenerator() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;chunk&quot;) .setN(2) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) // Convert the NGram chunks into Word Embeddings val chunkEmbeddings = new ChunkEmbeddings() .setInputCols(&quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;chunk_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentence, tokenizer, nGrams, embeddings, chunkEmbeddings )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk_embeddings) as result&quot;) .select(&quot;result.annotatorType&quot;, &quot;result.result&quot;, &quot;result.embeddings&quot;) .show(5, 80) ++-+--+ | annotatorType| result| embeddings| ++-+--+ |word_embeddings| This is|[-0.55661, 0.42829502, 0.86661, -0.409785, 0.06316501, 0.120775, -0.0732005, ...| |word_embeddings| is a|[-0.40674996, 0.22938299, 0.50597, -0.288195, 0.555655, 0.465145, 0.140118, 0...| |word_embeddings|a sentence|[0.17417, 0.095253006, -0.0530925, -0.218465, 0.714395, 0.79860497, 0.0129999...| |word_embeddings|sentence .|[0.139705, 0.177955, 0.1887775, -0.45545, 0.20030999, 0.461557, -0.07891501, ...| ++-+--+ ChunkTokenizer ApproachModel Tokenizes and flattens extracted NER chunks. The ChunkTokenizer will split the extracted NER CHUNK type Annotations and will create TOKEN type Annotations. The result is then flattened, resulting in a single array. For extended examples of usage, see the ChunkTokenizerTestSpec. Input Annotator Types: CHUNK Output Annotator Type: TOKEN Python API: ChunkTokenizer Scala API: ChunkTokenizer Source: ChunkTokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) entityExtractor = TextMatcher() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setEntities(&quot;src/test/resources/entity-extractor/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) chunkTokenizer = ChunkTokenizer() .setInputCols([&quot;entity&quot;]) .setOutputCol(&quot;chunk_token&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, entityExtractor, chunkTokenizer ]) data = spark.createDataFrame([[ &quot;Hello world, my name is Michael, I am an artist and I work at Benezar&quot;, &quot;Robert, an engineer from Farendell, graduated last year. The other one, Lucas, graduated last week.&quot; ]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;entity.result as entity&quot; , &quot;chunk_token.result as chunk_token&quot;).show(truncate=False) +--++ |entity |chunk_token | +--++ |[world, Michael, work at Benezar] |[world, Michael, work, at, Benezar] | |[engineer from Farendell, last year, last week]|[engineer, from, Farendell, last, year, last, week]| +--++ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotators.{ChunkTokenizer, TextMatcher, Tokenizer} import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;src/test/resources/entity-extractor/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) val chunkTokenizer = new ChunkTokenizer() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;chunk_token&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, entityExtractor, chunkTokenizer )) val data = Seq( &quot;Hello world, my name is Michael, I am an artist and I work at Benezar&quot;, &quot;Robert, an engineer from Farendell, graduated last year. The other one, Lucas, graduated last week.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;entity.result as entity&quot; , &quot;chunk_token.result as chunk_token&quot;).show(false) +--++ |entity |chunk_token | +--++ |[world, Michael, work at Benezar] |[world, Michael, work, at, Benezar] | |[engineer from Farendell, last year, last week]|[engineer, from, Farendell, last, year, last, week]| +--++ Instantiated model of the ChunkTokenizer. For usage and examples see the documentation of the main class. Input Annotator Types: CHUNK Output Annotator Type: TOKEN Python API: ChunkTokenizerModel Scala API: ChunkTokenizerModel Source: ChunkTokenizerModel Chunker This annotator matches a pattern of part-of-speech tags in order to return meaningful phrases from document. Extracted part-of-speech tags are mapped onto the sentence, which can then be parsed by regular expressions. The part-of-speech tags are wrapped by angle brackets &lt;&gt; to be easily distinguishable in the text itself. This example sentence will result in the form: &quot;Peter Pipers employees are picking pecks of pickled peppers.&quot; &quot;&lt;NNP&gt;&lt;NNP&gt;&lt;NNS&gt;&lt;VBP&gt;&lt;VBG&gt;&lt;NNS&gt;&lt;IN&gt;&lt;JJ&gt;&lt;NNS&gt;&lt;.&gt;&quot; To then extract these tags, regexParsers need to be set with e.g.: val chunker = new Chunker() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;)) When defining the regular expressions, tags enclosed in angle brackets are treated as groups, so here specifically &quot;&lt;NNP&gt;+&quot; means 1 or more nouns in succession. Additional patterns can also be set with addRegexParsers. For more extended examples see the Examples) and the ChunkerTestSpec. Input Annotator Types: DOCUMENT, POS Output Annotator Type: CHUNK Python API: Chunker Scala API: Chunker Source: Chunker Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) POSTag = PerceptronModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) chunker = Chunker() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;]) pipeline = Pipeline() .setStages([ documentAssembler, sentence, tokenizer, POSTag, chunker ]) data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(truncate=False) +-+ |result | +-+ |[chunk, 0, 11, Peter Pipers, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 13, 21, employees, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 35, 39, pecks, [sentence -&gt; 0, chunk -&gt; 2], []] | |[chunk, 52, 58, peppers, [sentence -&gt; 0, chunk -&gt; 3], []] | +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotators.{Chunker, Tokenizer} import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val POSTag = PerceptronModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val chunker = new Chunker() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;)) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentence, tokenizer, POSTag, chunker )) val data = Seq(&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(false) +-+ |result | +-+ |[chunk, 0, 11, Peter Pipers, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 13, 21, employees, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 35, 39, pecks, [sentence -&gt; 0, chunk -&gt; 2], []] | |[chunk, 52, 58, peppers, [sentence -&gt; 0, chunk -&gt; 3], []] | +-+ ClassifierDL ApproachModel Trains a ClassifierDL for generic Multi-class Text Classification. ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 100 classes. For instantiated/pretrained models, see ClassifierDLModel. Setting a test dataset to monitor model metrics can be done with .setTestDataset. The method expects a path to a parquet file containing a dataframe that has the same required columns as the training dataframe. The pre-processing steps for the training dataframe should also be applied to the test dataframe. The following example will show how to create the test dataset: val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val preProcessingPipeline = new Pipeline().setStages(Array(documentAssembler, embeddings)) val Array(train, test) = data.randomSplit(Array(0.8, 0.2)) preProcessingPipeline .fit(test) .transform(test) .write .mode(&quot;overwrite&quot;) .parquet(&quot;test_data&quot;) val classifier = new ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setTestDataset(&quot;test_data&quot;) For extended examples of usage, see the Examples [1] [2] and the ClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Note: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. UniversalSentenceEncoder, BertSentenceEmbeddings, or SentenceEmbeddings can be used for the inputCol Python API: ClassifierDLApproach Scala API: ClassifierDLApproach Source: ClassifierDLApproach Show Example PythonScala # In this example, the training data `&quot;sentiment.csv&quot;` has the form of # # text,label # This movie is the best movie I have wached ever! In my opinion this movie can win an award.,0 # This was a terrible movie! The acting was bad really bad!,1 # ... # # Then traning can be done like so: import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline smallCorpus = spark.read.option(&quot;header&quot;,&quot;True&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) docClassifier = ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(5e-3) .setDropout(0.5) pipeline = Pipeline() .setStages( [ documentAssembler, useEmbeddings, docClassifier ] ) pipelineModel = pipeline.fit(smallCorpus) // In this example, the training data `&quot;sentiment.csv&quot;` has the form of // // text,label // This movie is the best movie I have wached ever! In my opinion this movie can win an award.,0 // This was a terrible movie! The acting was bad really bad!,1 // ... // // Then traning can be done like so: import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach import org.apache.spark.ml.Pipeline val smallCorpus = spark.read.option(&quot;header&quot;,&quot;true&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val docClassifier = new ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(5e-3f) .setDropout(0.5f) val pipeline = new Pipeline() .setStages( Array( documentAssembler, useEmbeddings, docClassifier ) ) val pipelineModel = pipeline.fit(smallCorpus) ClassifierDL for generic Multi-class Text Classification. ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 100 classes. This is the instantiated model of the ClassifierDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val classifierDL = ClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;classification&quot;) The default model is &quot;classifierdl_use_trec6&quot;, if no name is provided. It uses embeddings from the UniversalSentenceEncoder and is trained on the TREC-6 dataset. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the ClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: ClassifierDLModel Scala API: ClassifierDLModel Source: ClassifierDLModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) sarcasmDL = ClassifierDLModel.pretrained(&quot;classifierdl_use_sarcasm&quot;) .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sarcasm&quot;) pipeline = Pipeline() .setStages([ documentAssembler, sentence, useEmbeddings, sarcasmDL ]) data = spark.createDataFrame([ [&quot;I&#39;m ready!&quot;], [&quot;If I could put into words how much I love waking up at 6 am on Mondays I would.&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(arrays_zip(sentence, sarcasm)) as out&quot;) .selectExpr(&quot;out.sentence.result as sentence&quot;, &quot;out.sarcasm.result as sarcasm&quot;) .show(truncate=False) +-+-+ |sentence |sarcasm| +-+-+ |I&#39;m ready! |normal | |If I could put into words how much I love waking up at 6 am on Mondays I would.|sarcasm| +-+-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLModel import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val sarcasmDL = ClassifierDLModel.pretrained(&quot;classifierdl_use_sarcasm&quot;) .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sarcasm&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentence, useEmbeddings, sarcasmDL )) val data = Seq( &quot;I&#39;m ready!&quot;, &quot;If I could put into words how much I love waking up at 6 am on Mondays I would.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(arrays_zip(sentence, sarcasm)) as out&quot;) .selectExpr(&quot;out.sentence.result as sentence&quot;, &quot;out.sarcasm.result as sarcasm&quot;) .show(false) +-+-+ |sentence |sarcasm| +-+-+ |I&#39;m ready! |normal | |If I could put into words how much I love waking up at 6 am on Mondays I would.|sarcasm| +-+-+ ContextSpellChecker ApproachModel Trains a deep-learning based Noisy Channel Model Spell Algorithm. Correction candidates are extracted combining context information and word information. For instantiated/pretrained models, see ContextSpellCheckerModel. Spell Checking is a sequence to sequence mapping problem. Given an input sequence, potentially containing a certain number of errors, ContextSpellChecker will rank correction sequences according to three things: Different correction candidates for each word — word level. The surrounding text of each word, i.e. it’s context — sentence level. The relative cost of different correction candidates according to the edit operations at the character level it requires — subword level. For an in-depth explanation of the module see the article Applying Context Aware Spell Checking in Spark NLP. For extended examples of usage, see the article Training a Contextual Spell Checker for Italian Language, the Examples and the ContextSpellCheckerTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: ContextSpellCheckerApproach Scala API: ContextSpellCheckerApproach Source: ContextSpellCheckerApproach Show Example PythonScala # For this example, we use the first Sherlock Holmes book as the training dataset. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) spellChecker = ContextSpellCheckerApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;corrected&quot;) .setWordMaxDistance(3) .setBatchSize(24) .setEpochs(8) .setLanguageModelClasses(1650) # dependant on vocabulary size # .addVocabClass(&quot;_NAME_&quot;, names) # Extra classes for correction could be added like this pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker ]) path = &quot;sherlockholmes.txt&quot; dataset = spark.read.text(path) .toDF(&quot;text&quot;) pipelineModel = pipeline.fit(dataset) // For this example, we use the first Sherlock Holmes book as the training dataset. import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val spellChecker = new ContextSpellCheckerApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;corrected&quot;) .setWordMaxDistance(3) .setBatchSize(24) .setEpochs(8) .setLanguageModelClasses(1650) // dependant on vocabulary size // .addVocabClass(&quot;_NAME_&quot;, names) // Extra classes for correction could be added like this val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker )) val path = &quot;src/test/resources/spell/sherlockholmes.txt&quot; val dataset = spark.sparkContext.textFile(path) .toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(dataset) Implements a deep-learning based Noisy Channel Model Spell Algorithm. Correction candidates are extracted combining context information and word information. Spell Checking is a sequence to sequence mapping problem. Given an input sequence, potentially containing a certain number of errors, ContextSpellChecker will rank correction sequences according to three things: Different correction candidates for each word — word level. The surrounding text of each word, i.e. it’s context — sentence level. The relative cost of different correction candidates according to the edit operations at the character level it requires — subword level. For an in-depth explanation of the module see the article Applying Context Aware Spell Checking in Spark NLP. This is the instantiated model of the ContextSpellCheckerApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val spellChecker = ContextSpellCheckerModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;checked&quot;) The default model is &quot;spellcheck_dl&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the ContextSpellCheckerTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: ContextSpellCheckerModel Scala API: ContextSpellCheckerModel Source: ContextSpellCheckerModel DateMatcher Matches standard date formats into a provided format. Reads from different forms of date and time expressions and converts them to a provided date format. Extracts only one date per document. Use with sentence detector to find matches in each sentence. To extract multiple dates from a document, please use the MultiDateMatcher. Reads the following kind of dates: &quot;1978-01-28&quot;, &quot;1984/04/02,1/02/1980&quot;, &quot;2/28/79&quot;, &quot;The 31st of April in the year 2008&quot;, &quot;Fri, 21 Nov 1997&quot;, &quot;Jan 21, ‘97&quot;, &quot;Sun&quot;, &quot;Nov 21&quot;, &quot;jan 1st&quot;, &quot;next thursday&quot;, &quot;last wednesday&quot;, &quot;today&quot;, &quot;tomorrow&quot;, &quot;yesterday&quot;, &quot;next week&quot;, &quot;next month&quot;, &quot;next year&quot;, &quot;day after&quot;, &quot;the day before&quot;, &quot;0600h&quot;, &quot;06:00 hours&quot;, &quot;6pm&quot;, &quot;5:30 a.m.&quot;, &quot;at 5&quot;, &quot;12:59&quot;, &quot;23:59&quot;, &quot;1988/11/23 6pm&quot;, &quot;next week at 7.30&quot;, &quot;5 am tomorrow&quot; For example &quot;The 31st of April in the year 2008&quot; will be converted into 2008/04/31. Pretrained pipelines are available for this module, see Pipelines. For extended examples of usage, see the Examples and the DateMatcherTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DATE Python API: DateMatcher Scala API: DateMatcher Source: DateMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) date = DateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) pipeline = Pipeline().setStages([ documentAssembler, date ]) data = spark.createDataFrame([[&quot;Fri, 21 Nov 1997&quot;], [&quot;next week at 7.30&quot;], [&quot;see you a day after&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;date&quot;).show(truncate=False) +-+ |date | +-+ |[[date, 5, 15, 1997/11/21, [sentence -&gt; 0], []]] | |[[date, 0, 8, 2020/01/18, [sentence -&gt; 0], []]] | |[[date, 10, 18, 2020/01/12, [sentence -&gt; 0], []]]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.DateMatcher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val date = new DateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, date )) val data = Seq(&quot;Fri, 21 Nov 1997&quot;, &quot;next week at 7.30&quot;, &quot;see you a day after&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;date&quot;).show(false) +-+ |date | +-+ |[[date, 5, 15, 1997/11/21, [sentence -&gt; 0], []]] | |[[date, 0, 8, 2020/01/18, [sentence -&gt; 0], []]] | |[[date, 10, 18, 2020/01/12, [sentence -&gt; 0], []]]| +-+ DependencyParser ApproachModel Trains an unlabeled parser that finds a grammatical relations between two words in a sentence. For instantiated/pretrained models, see DependencyParserModel. Dependency parser provides information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The required training data can be set in two different ways (only one can be chosen for a particular model): Dependency treebank in the Penn Treebank format set with setDependencyTreeBank Dataset in the CoNLL-U format set with setConllU Apart from that, no additional training data is needed. See DependencyParserApproachTestSpec for further reference on how to use this API. Input Annotator Types: DOCUMENT, POS, TOKEN Output Annotator Type: DEPENDENCY Python API: DependencyParserApproach Scala API: DependencyParserApproach Source: DependencyParserApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) dependencyParserApproach = DependencyParserApproach() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) .setDependencyTreeBank(&quot;src/test/resources/parser/unlabeled/dependency_treebank&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, posTagger, dependencyParserApproach ]) # Additional training data is not needed, the dependency parser relies on the dependency tree bank / CoNLL-U only. emptyDataSet = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(emptyDataSet) import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val dependencyParserApproach = new DependencyParserApproach() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) .setDependencyTreeBank(&quot;src/test/resources/parser/unlabeled/dependency_treebank&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, posTagger, dependencyParserApproach )) // Additional training data is not needed, the dependency parser relies on the dependency tree bank / CoNLL-U only. val emptyDataSet = Seq.empty[String].toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(emptyDataSet) Unlabeled parser that finds a grammatical relation between two words in a sentence. Dependency parser provides information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. This is the instantiated model of the DependencyParserApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val dependencyParserApproach = DependencyParserModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) The default model is &quot;dependency_conllu&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the DependencyParserApproachTestSpec. Input Annotator Types: [String]DOCUMENT, POS, TOKEN Output Annotator Type: DEPENDENCY Python API: DependencyParserModel Scala API: DependencyParserModel Source: DependencyParserModel Doc2Chunk Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol. Chunk text must be contained within input DOCUMENT. May be either StringType or ArrayType[StringType] (using setIsArray). Useful for annotators that require a CHUNK type input. Input Annotator Types: DOCUMENT Output Annotator Type: CHUNK Python API: Doc2Chunk Scala API: Doc2Chunk Source: Doc2Chunk Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) chunkAssembler = Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) .setIsArray(True) data = spark.createDataFrame([[ &quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;, [&quot;Spark NLP&quot;, &quot;text processing library&quot;, &quot;natural language processing&quot;] ]]).toDF(&quot;text&quot;, &quot;target&quot;) pipeline = Pipeline().setStages([documentAssembler, chunkAssembler]).fit(data) result = pipeline.transform(data) result.selectExpr(&quot;chunk.result&quot;, &quot;chunk.annotatorType&quot;).show(truncate=False) +--++ |result |annotatorType | +--++ |[Spark NLP, text processing library, natural language processing]|[chunk, chunk, chunk]| +--++ import spark.implicits._ import com.johnsnowlabs.nlp.{Doc2Chunk, DocumentAssembler} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val chunkAssembler = new Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) .setIsArray(true) val data = Seq( (&quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;, Seq(&quot;Spark NLP&quot;, &quot;text processing library&quot;, &quot;natural language processing&quot;)) ).toDF(&quot;text&quot;, &quot;target&quot;) val pipeline = new Pipeline().setStages(Array(documentAssembler, chunkAssembler)).fit(data) val result = pipeline.transform(data) result.selectExpr(&quot;chunk.result&quot;, &quot;chunk.annotatorType&quot;).show(false) +--++ |result |annotatorType | +--++ |[Spark NLP, text processing library, natural language processing]|[chunk, chunk, chunk]| +--++ Doc2Vec ApproachModel Trains a Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. For instantiated/pretrained models, see Doc2VecModel. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: SENTENCE_EMBEDDINGS Python API: Doc2VecApproach Scala API: Doc2VecApproach Source: Doc2VecApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Doc2VecApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings ]) path = &quot;sherlockholmes.txt&quot; dataset = spark.read.text(path).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(dataset) import spark.implicits._ import com.johnsnowlabs.nlp.annotator.{Tokenizer, Doc2VecApproach} import com.johnsnowlabs.nlp.base.DocumentAssembler import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = new Doc2VecApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings )) val path = &quot;src/test/resources/spell/sherlockholmes.txt&quot; val dataset = spark.sparkContext.textFile(path) .toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(dataset) Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. This is the instantiated model of the Doc2VecApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val embeddings = Doc2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;doc2vec_gigaword_300&quot;, if no name is provided. For available pretrained models please see the Models Hub. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: SENTENCE_EMBEDDINGS Python API: Doc2VecModel Scala API: Doc2VecModel Source: Doc2VecModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Doc2VecModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, embeddings, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Tokenizer, Doc2VecModel} import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = Doc2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsFinisher )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ DocumentAssembler Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. The DocumentAssembler can read either a String column or an Array[String]. Additionally, setCleanupMode can be used to pre-process the text (Default: disabled). For possible options please refer the parameters section. For more extended examples on document pre-processing see the Examples. Input Annotator Types: NONE Output Annotator Type: DOCUMENT Python API: DocumentAssembler Scala API: DocumentAssembler Source: DocumentAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library.&quot;]]).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) result = documentAssembler.transform(data) result.select(&quot;document&quot;).show(truncate=False) +-+ |document | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document&quot;).printSchema root |-- document: array (nullable = True) | |-- element: struct (containsNull = True) | | |-- annotatorType: string (nullable = True) | | |-- begin: integer (nullable = False) | | |-- end: integer (nullable = False) | | |-- result: string (nullable = True) | | |-- metadata: map (nullable = True) | | | |-- key: string | | | |-- value: string (valueContainsNull = True) | | |-- embeddings: array (nullable = True) | | | |-- element: float (containsNull = False) import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler val data = Seq(&quot;Spark NLP is an open-source text processing library.&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val result = documentAssembler.transform(data) result.select(&quot;document&quot;).show(false) +-+ |document | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document&quot;).printSchema root |-- document: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- begin: integer (nullable = false) | | |-- end: integer (nullable = false) | | |-- result: string (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | | |-- embeddings: array (nullable = true) | | | |-- element: float (containsNull = false) DocumentNormalizer Annotator which normalizes raw text from tagged text, e.g. scraped web pages or xml documents, from document type columns into Sentence. Removes all dirty characters from text following one or more input regex patterns. Can apply not wanted character removal with a specific policy. Can apply lower case normalization. For extended examples of usage, see the Examples. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: DocumentNormalizer Scala API: DocumentNormalizer Source: DocumentNormalizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) cleanUpPatterns = [&quot;&lt;[^&gt;]&gt;&quot;] documentNormalizer = DocumentNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;normalizedDocument&quot;) .setAction(&quot;clean&quot;) .setPatterns(cleanUpPatterns) .setReplacement(&quot; &quot;) .setPolicy(&quot;pretty_all&quot;) .setLowercase(True) pipeline = Pipeline().setStages([ documentAssembler, documentNormalizer ]) text = &quot;&quot;&quot; &lt;div id=&quot;theworldsgreatest&quot; class=&#39;my-right my-hide-small my-wide toptext&#39; style=&quot;font-family:&#39;Segoe UI&#39;,Arial,sans-serif&quot;&gt; THE WORLD&#39;S LARGEST WEB DEVELOPER SITE &lt;h1 style=&quot;font-size:300%;&quot;&gt;THE WORLD&#39;S LARGEST WEB DEVELOPER SITE&lt;/h1&gt; &lt;p style=&quot;font-size:160%;&quot;&gt;Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&quot;&quot;&quot; data = spark.createDataFrame([[text]]).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(data) result = pipelineModel.transform(data) result.selectExpr(&quot;normalizedDocument.result&quot;).show(truncate=False) +--+ |result | +--+ |[ the world&#39;s largest web developer site the world&#39;s largest web developer site lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum..]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.DocumentNormalizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val cleanUpPatterns = Array(&quot;&lt;[^&gt;]&gt;&quot;) val documentNormalizer = new DocumentNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;normalizedDocument&quot;) .setAction(&quot;clean&quot;) .setPatterns(cleanUpPatterns) .setReplacement(&quot; &quot;) .setPolicy(&quot;pretty_all&quot;) .setLowercase(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, documentNormalizer )) val text = &quot;&quot;&quot; &lt;div id=&quot;theworldsgreatest&quot; class=&#39;my-right my-hide-small my-wide toptext&#39; style=&quot;font-family:&#39;Segoe UI&#39;,Arial,sans-serif&quot;&gt; THE WORLD&#39;S LARGEST WEB DEVELOPER SITE &lt;h1 style=&quot;font-size:300%;&quot;&gt;THE WORLD&#39;S LARGEST WEB DEVELOPER SITE&lt;/h1&gt; &lt;p style=&quot;font-size:160%;&quot;&gt;Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&quot;&quot;&quot; val data = Seq(text).toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(data) val result = pipelineModel.transform(data) result.selectExpr(&quot;normalizedDocument.result&quot;).show(truncate=false) +--+ |result | +--+ |[ the world&#39;s largest web developer site the world&#39;s largest web developer site lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum..]| +--+ EmbeddingsFinisher Extracts embeddings from Annotations into a more easily usable form. This is useful for example: WordEmbeddings, BertEmbeddings, SentenceEmbeddings and ChunkEmbeddings. By using EmbeddingsFinisher you can easily transform your embeddings into array of floats or vectors which are compatible with Spark ML functions such as LDA, K-mean, Random Forest classifier or any other functions that require featureCol. For more extended examples see the Examples. Input Annotator Types: EMBEDDINGS Output Annotator Type: NONE Python API: EmbeddingsFinisher Scala API: EmbeddingsFinisher Source: EmbeddingsFinisher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) stopwordsCleaner = StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) gloveEmbeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;cleanTokens&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) embeddingsFinisher = EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_sentence_embeddings&quot;) .setOutputAsVector(True) .setCleanAnnotations(False) data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library.&quot;]]) .toDF(&quot;text&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, normalizer, stopwordsCleaner, gloveEmbeddings, embeddingsFinisher ]).fit(data) result = pipeline.transform(data) resultWithSize = result.selectExpr(&quot;explode(finished_sentence_embeddings) as embeddings&quot;) resultWithSize.show(5, 80) +--+ | embeddings| +--+ |[0.1619900017976761,0.045552998781204224,-0.03229299932718277,-0.685609996318...| |[-0.42416998744010925,1.1378999948501587,-0.5717899799346924,-0.5078899860382...| |[0.08621499687433243,-0.15772999823093414,-0.06067200005054474,0.395359992980...| |[-0.4970499873161316,0.7164199948310852,0.40119001269340515,-0.05761000141501...| |[-0.08170200139284134,0.7159299850463867,-0.20677000284194946,0.0295659992843...| +--+ import spark.implicits._ import org.apache.spark.ml.Pipeline import com.johnsnowlabs.nlp.{DocumentAssembler, EmbeddingsFinisher} import com.johnsnowlabs.nlp.annotator.{Normalizer, StopWordsCleaner, Tokenizer, WordEmbeddingsModel} val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) val stopwordsCleaner = new StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val gloveEmbeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;cleanTokens&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_sentence_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) val data = Seq(&quot;Spark NLP is an open-source text processing library.&quot;) .toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, normalizer, stopwordsCleaner, gloveEmbeddings, embeddingsFinisher )).fit(data) val result = pipeline.transform(data) val resultWithSize = result.selectExpr(&quot;explode(finished_sentence_embeddings)&quot;) .map { row =&gt; val vector = row.getAs[org.apache.spark.ml.linalg.DenseVector](0) (vector.size, vector) }.toDF(&quot;size&quot;, &quot;vector&quot;) resultWithSize.show(5, 80) +-+--+ |size| vector| +-+--+ | 100|[0.1619900017976761,0.045552998781204224,-0.03229299932718277,-0.685609996318...| | 100|[-0.42416998744010925,1.1378999948501587,-0.5717899799346924,-0.5078899860382...| | 100|[0.08621499687433243,-0.15772999823093414,-0.06067200005054474,0.395359992980...| | 100|[-0.4970499873161316,0.7164199948310852,0.40119001269340515,-0.05761000141501...| | 100|[-0.08170200139284134,0.7159299850463867,-0.20677000284194946,0.0295659992843...| +-+--+ EntityRuler ApproachModel Fits an Annotator to match exact strings or regex patterns provided in a file against a Document and assigns them an named entity. The definitions can contain any number of named entities. There are multiple ways and formats to set the extraction resource. It is possible to set it either as a “JSON”, “JSONL” or “CSV” file. A path to the file needs to be provided to setPatternsResource. The file format needs to be set as the “format” field in the option parameter map and depending on the file type, additional parameters might need to be set. To enable regex extraction, setEnablePatternRegex(true) needs to be called. If the file is in a JSON format, then the rule definitions need to be given in a list with the fields “id”, “label” and “patterns”: [ { &quot;id&quot;: &quot;person-regex&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot; w+ s w+&quot;, &quot; w+- w+&quot;] }, { &quot;id&quot;: &quot;locations-words&quot;, &quot;label&quot;: &quot;LOCATION&quot;, &quot;patterns&quot;: [&quot;Winterfell&quot;] } ] The same fields also apply to a file in the JSONL format: {&quot;id&quot;: &quot;names-with-j&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Jon&quot;, &quot;John&quot;, &quot;John Snow&quot;]} {&quot;id&quot;: &quot;names-with-s&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Stark&quot;, &quot;Snow&quot;]} {&quot;id&quot;: &quot;names-with-e&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Eddard&quot;, &quot;Eddard Stark&quot;]} In order to use a CSV file, an additional parameter “delimiter” needs to be set. In this case, the delimiter might be set by using .setPatternsResource(&quot;patterns.csv&quot;, ReadAs.TEXT, Map(&quot;format&quot;-&gt;&quot;csv&quot;, &quot;delimiter&quot; -&gt; &quot; |&quot;)) PERSON|Jon PERSON|John PERSON|John Snow LOCATION|Winterfell Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: EntityRulerApproach Scala API: EntityRulerApproach Source: EntityRulerApproach Show Example PythonScala # In this example, the entities file as the form of # # PERSON|Jon # PERSON|John # PERSON|John Snow # LOCATION|Winterfell # # where each line represents an entity and the associated string delimited by &quot;|&quot;. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.common import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) entityRuler = EntityRulerApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;entities&quot;) .setPatternsResource( &quot;patterns.csv&quot;, ReadAs.TEXT, {&quot;format&quot;: &quot;csv&quot;, &quot;delimiter&quot;: &quot; |&quot;} ) .setEnablePatternRegex(True) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, entityRuler ]) data = spark.createDataFrame([[&quot;Jon Snow wants to be lord of Winterfell.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(entities)&quot;).show(truncate=False) +--+ |col | +--+ |[chunk, 0, 2, Jon, [entity -&gt; PERSON, sentence -&gt; 0], []] | |[chunk, 29, 38, Winterfell, [entity -&gt; LOCATION, sentence -&gt; 0], []]| +--+ // In this example, the entities file as the form of // // PERSON|Jon // PERSON|John // PERSON|John Snow // LOCATION|Winterfell // // where each line represents an entity and the associated string delimited by &quot;|&quot;. import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.er.EntityRulerApproach import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val entityRuler = new EntityRulerApproach() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;entities&quot;) .setPatternsResource( &quot;src/test/resources/entity-ruler/patterns.csv&quot;, ReadAs.TEXT, {&quot;format&quot;: &quot;csv&quot;, &quot;delimiter&quot;: &quot;|&quot;)} ) .setEnablePatternRegex(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, entityRuler )) val data = Seq(&quot;Jon Snow wants to be lord of Winterfell.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(entities)&quot;).show(false) +--+ |col | +--+ |[chunk, 0, 2, Jon, [entity -&gt; PERSON, sentence -&gt; 0], []] | |[chunk, 29, 38, Winterfell, [entity -&gt; LOCATION, sentence -&gt; 0], []]| +--+ Instantiated model of the EntityRulerApproach. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: EntityRulerModel Scala API: EntityRulerModel Source: EntityRulerModel Finisher Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. The Finisher outputs annotation(s) values into String. For more extended examples on document pre-processing see the Examples. Input Annotator Types: ANY Output Annotator Type: NONE Python API: Finisher Scala API: Finisher Source: Finisher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline data = spark.createDataFrame([[1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;]]).toDF(&quot;id&quot;, &quot;text&quot;) # Extracts Named Entities amongst other things pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) finisher = Finisher().setInputCols(&quot;entities&quot;).setOutputCols(&quot;output&quot;) explainResult = pipeline.transform(data) explainResult.selectExpr(&quot;explode(entities)&quot;).show(truncate=False) ++ |entities | ++ |[[chunk, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []], [chunk, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]]| ++ result = finisher.transform(explainResult) result.select(&quot;output&quot;).show(truncate=False) +-+ |output | +-+ |[New York, New Jersey]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.Finisher val data = Seq((1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;)).toDF(&quot;id&quot;, &quot;text&quot;) // Extracts Named Entities amongst other things val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) val finisher = new Finisher().setInputCols(&quot;entities&quot;).setOutputCols(&quot;output&quot;) val explainResult = pipeline.transform(data) explainResult.selectExpr(&quot;explode(entities)&quot;).show(false) ++ |entities | ++ |[[chunk, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []], [chunk, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]]| ++ val result = finisher.transform(explainResult) result.select(&quot;output&quot;).show(false) +-+ |output | +-+ |[New York, New Jersey]| +-+ GraphExtraction Extracts a dependency graph between entities. The GraphExtraction class takes e.g. extracted entities from a NerDLModel and creates a dependency tree which describes how the entities relate to each other. For that a triple store format is used. Nodes represent the entities and the edges represent the relations between those entities. The graph can then be used to find relevant relationships between words. Both the DependencyParserModel and TypedDependencyParserModel need to be present in the pipeline. There are two ways to set them: Both Annotators are present in the pipeline already. The dependencies are taken implicitly from these two Annotators. Setting setMergeEntities to true will download the default pretrained models for those two Annotators automatically. The specific models can also be set with setDependencyParserModel and setTypedDependencyParserModel: val graph_extraction = new GraphExtraction() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;graph&quot;) .setRelationshipTypes(Array(&quot;prefer-LOC&quot;)) .setMergeEntities(true) //.setDependencyParserModel(Array(&quot;dependency_conllu&quot;, &quot;en&quot;, &quot;public/models&quot;)) //.setTypedDependencyParserModel(Array(&quot;dependency_typed_conllu&quot;, &quot;en&quot;, &quot;public/models&quot;)) To transform the resulting graph into a more generic form such as RDF, see the GraphFinisher. Input Annotator Types: DOCUMENT, TOKEN, NAMED_ENTITY Output Annotator Type: NODE Python API: GraphExtraction Scala API: GraphExtraction Source: GraphExtraction Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) nerTagger = NerDLModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) posTagger = PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) dependencyParser = DependencyParserModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency&quot;) typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols([&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency_type&quot;) graph_extraction = GraphExtraction() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;graph&quot;) .setRelationshipTypes([&quot;prefer-LOC&quot;]) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, embeddings, nerTagger, posTagger, dependencyParser, typedDependencyParser, graph_extraction ]) data = spark.createDataFrame([[&quot;You and John prefer the morning flight through Denver&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;graph&quot;).show(truncate=False) +--+ |graph | +--+ |13, 18, prefer, [relationship -&gt; prefer,LOC, path1 -&gt; prefer,nsubj,morning,flat,flight,flat,Denver], []| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel import com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserModel import org.apache.spark.ml.Pipeline import com.johnsnowlabs.nlp.annotators.GraphExtraction val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val nerTagger = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val dependencyParser = DependencyParserModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) val typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols(&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency_type&quot;) val graph_extraction = new GraphExtraction() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;graph&quot;) .setRelationshipTypes(Array(&quot;prefer-LOC&quot;)) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger, posTagger, dependencyParser, typedDependencyParser, graph_extraction )) val data = Seq(&quot;You and John prefer the morning flight through Denver&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;graph&quot;).show(false) +--+ |graph | +--+ |[[node, 13, 18, prefer, [relationship -&gt; prefer,LOC, path1 -&gt; prefer,nsubj,morning,flat,flight,flat,Denver], []]]| +--+ GraphFinisher Helper class to convert the knowledge graph from GraphExtraction into a generic format, such as RDF. Input Annotator Types: NONE Output Annotator Type: NONE Python API: GraphFinisher Scala API: GraphFinisher Source: GraphFinisher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # This is a continuation of the example of # GraphExtraction. To see how the graph is extracted, see the # documentation of that class. graphFinisher = GraphFinisher() .setInputCol(&quot;graph&quot;) .setOutputCol(&quot;graph_finished&quot;) .setOutputAs[False] finishedResult = graphFinisher.transform(result) finishedResult.select(&quot;text&quot;, &quot;graph_finished&quot;).show(truncate=False) +--+--+ |text |graph_finished | +--+--+ |You and John prefer the morning flight through Denver|(morning,flat,flight), (flight,flat,Denver)| +--+--+ // This is a continuation of the example of // [[com.johnsnowlabs.nlp.annotators.GraphExtraction GraphExtraction]]. To see how the graph is extracted, see the // documentation of that class. import com.johnsnowlabs.nlp.GraphFinisher val graphFinisher = new GraphFinisher() .setInputCol(&quot;graph&quot;) .setOutputCol(&quot;graph_finished&quot;) .setOutputAsArray(false) val finishedResult = graphFinisher.transform(result) finishedResult.select(&quot;text&quot;, &quot;graph_finished&quot;).show(false) +--+--+ |text |graph_finished | +--+--+ |You and John prefer the morning flight through Denver|[[(prefer,nsubj,morning), (morning,flat,flight), (flight,flat,Denver)]]| +--+--+ ImageAssembler Prepares images read by Spark into a format that is processable by Spark NLP. This component is needed to process images. Input Annotator Types: NONE Output Annotator Type: IMAGE Python API: ImageAssembler Scala API: ImageAssembler Source: ImageAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from pyspark.ml import Pipeline data = spark.read.format(&quot;image&quot;).load(&quot;./tmp/images/&quot;).toDF(&quot;image&quot;) imageAssembler = ImageAssembler().setInputCol(&quot;image&quot;).setOutputCol(&quot;image_assembler&quot;) result = imageAssembler.transform(data) result.select(&quot;image_assembler&quot;).show() result.select(&quot;image_assembler&quot;).printSchema() root |-- image_assembler: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- origin: string (nullable = true) | | |-- height: integer (nullable = true) | | |-- width: integer (nullable = true) | | |-- nChannels: integer (nullable = true) | | |-- mode: integer (nullable = true) | | |-- result: binary (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) import com.johnsnowlabs.nlp.ImageAssembler import org.apache.spark.ml.Pipeline val imageDF: DataFrame = spark.read .format(&quot;image&quot;) .option(&quot;dropInvalid&quot;, value = true) .load(&quot;src/test/resources/image/&quot;) val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) val pipeline = new Pipeline().setStages(Array(imageAssembler)) val pipelineDF = pipeline.fit(imageDF).transform(imageDF) pipelineDF.printSchema() root |-- image_assembler: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- origin: string (nullable = true) | | |-- height: integer (nullable = false) | | |-- width: integer (nullable = false) | | |-- nChannels: integer (nullable = false) | | |-- mode: integer (nullable = false) | | |-- result: binary (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) LanguageDetectorDL Language Identification and Detection by using CNN and RNN architectures in TensorFlow. LanguageDetectorDL is an annotator that detects the language of documents or sentences depending on the inputCols. The models are trained on large datasets such as Wikipedia and Tatoeba. Depending on the language (how similar the characters are), the LanguageDetectorDL works best with text longer than 140 characters. The output is a language code in Wiki Code style. Pretrained models can be loaded with pretrained of the companion object: Val languageDetector = LanguageDetectorDL.pretrained() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;language&quot;) The default model is &quot;ld_wiki_tatoeba_cnn_21&quot;, default language is &quot;xx&quot; (meaning multi-lingual), if no values are provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples And the LanguageDetectorDLTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: LANGUAGE Python API: LanguageDetectorDL Scala API: LanguageDetectorDL Source: LanguageDetectorDL Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) languageDetector = LanguageDetectorDL.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;language&quot;) pipeline = Pipeline() .setStages([ documentAssembler, languageDetector ]) data = spark.createDataFrame([ [&quot;Spark NLP is an open-source text processing library for advanced natural language processing for the Python, Java and Scala programming languages.&quot;], [&quot;Spark NLP est une bibliothèque de traitement de texte open source pour le traitement avancé du langage naturel pour les langages de programmation Python, Java et Scala.&quot;], [&quot;Spark NLP ist eine Open-Source-Textverarbeitungsbibliothek für fortgeschrittene natürliche Sprachverarbeitung für die Programmiersprachen Python, Java und Scala.&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;language.result&quot;).show(truncate=False) ++ |result| ++ |[en] | |[fr] | |[de] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.ld.dl.LanguageDetectorDL import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val languageDetector = LanguageDetectorDL.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;language&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, languageDetector )) val data = Seq( &quot;Spark NLP is an open-source text processing library for advanced natural language processing for the Python, Java and Scala programming languages.&quot;, &quot;Spark NLP est une bibliothèque de traitement de texte open source pour le traitement avancé du langage naturel pour les langages de programmation Python, Java et Scala.&quot;, &quot;Spark NLP ist eine Open-Source-Textverarbeitungsbibliothek für fortgeschrittene natürliche Sprachverarbeitung für die Programmiersprachen Python, Java und Scala.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;language.result&quot;).show(false) ++ |result| ++ |[en] | |[fr] | |[de] | ++ Lemmatizer ApproachModel Class to find lemmas out of words with the objective of returning a base dictionary word. Retrieves the significant part of a word. A dictionary of predefined lemmas must be provided with setDictionary. The dictionary can be set as a delimited text file. Pretrained models can be loaded with LemmatizerModel.pretrained. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: Lemmatizer Scala API: Lemmatizer Source: Lemmatizer Show Example PythonScala # In this example, the lemma dictionary `lemmas_small.txt` has the form of # # ... # pick -&gt; pick picks picking picked # peck -&gt; peck pecking pecked pecks # pickle -&gt; pickle pickles pickled pickling # pepper -&gt; pepper peppers peppered peppering # ... # # where each key is delimited by `-&gt;` and values are delimited by ` t` import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = Lemmatizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) pipeline = Pipeline() .setStages([ documentAssembler, sentenceDetector, tokenizer, lemmatizer ]) data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;lemma.result&quot;).show(truncate=False) ++ |result | ++ |[Peter, Pipers, employees, are, pick, peck, of, pickle, pepper, .]| ++ // In this example, the lemma dictionary `lemmas_small.txt` has the form of // // ... // pick -&gt; pick picks picking picked // peck -&gt; peck pecking pecked pecks // pickle -&gt; pickle pickles pickled pickling // pepper -&gt; pepper peppers peppered peppering // ... // // where each key is delimited by `-&gt;` and values are delimited by ` t` import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.Lemmatizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val lemmatizer = new Lemmatizer() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentenceDetector, tokenizer, lemmatizer )) val data = Seq(&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;lemma.result&quot;).show(false) ++ |result | ++ |[Peter, Pipers, employees, are, pick, peck, of, pickle, pepper, .]| ++ Instantiated Model of the Lemmatizer. For usage and examples, please see the documentation of that class. For available pretrained models please see the Models Hub. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: LemmatizerModel Scala API: LemmatizerModel Source: LemmatizerModel MultiClassifierDL ApproachModel Trains a MultiClassifierDL for Multi-label Text Classification. MultiClassifierDL uses a Bidirectional GRU with a convolutional model that we have built inside TensorFlow and supports up to 100 classes. For instantiated/pretrained models, see MultiClassifierDLModel. The input to MultiClassifierDL are Sentence Embeddings such as the state-of-the-art UniversalSentenceEncoder, BertSentenceEmbeddings or SentenceEmbeddings. In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to. Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y). Setting a test dataset to monitor model metrics can be done with .setTestDataset. The method expects a path to a parquet file containing a dataframe that has the same required columns as the training dataframe. The pre-processing steps for the training dataframe should also be applied to the test dataframe. The following example will show how to create the test dataset: val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val preProcessingPipeline = new Pipeline().setStages(Array(documentAssembler, embeddings)) val Array(train, test) = data.randomSplit(Array(0.8, 0.2)) preProcessingPipeline .fit(test) .transform(test) .write .mode(&quot;overwrite&quot;) .parquet(&quot;test_data&quot;) val multiClassifier = new MultiClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setTestDataset(&quot;test_data&quot;) For extended examples of usage, see the Examples and the MultiClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Note: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. UniversalSentenceEncoder, BertSentenceEmbeddings, SentenceEmbeddings or other sentence based embeddings can be used for the inputCol Python API: MultiClassifierDLApproach Scala API: MultiClassifierDLApproach Source: MultiClassifierDLApproach Show Example PythonScala # In this example, the training data has the form # # +-+--+--+ # | id| text| labels| # +-+--+--+ # |ed58abb40640f983|PN NewsYou mean ... | [toxic]| # |a1237f726b5f5d89|Dude. Place the ...| [obscene, insult]| # |24b0d6c8733c2abe|Thanks - thanks ...| [insult]| # |8c4478fb239bcfc0|&quot; Gee, 5 minutes ...|[toxic, obscene, ...| # +-+--+--+ import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Process training data to create text with associated array of labels trainDataset.printSchema() # root # |-- id: string (nullable = true) # |-- text: string (nullable = true) # |-- labels: array (nullable = true) # | |-- element: string (containsNull = true) # Then create pipeline for training documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) .setCleanupMode(&quot;shrink&quot;) embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;embeddings&quot;) docClassifier = MultiClassifierDLApproach() .setInputCols(&quot;embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;labels&quot;) .setBatchSize(128) .setMaxEpochs(10) .setLr(1e-3) .setThreshold(0.5) .setValidationSplit(0.1) pipeline = Pipeline() .setStages( [ documentAssembler, embeddings, docClassifier ] ) pipelineModel = pipeline.fit(trainDataset) // In this example, the training data has the form (Note: labels can be arbitrary) // // mr,ref // &quot;name[Alimentum], area[city centre], familyFriendly[no], near[Burger King]&quot;,Alimentum is an adult establish found in the city centre area near Burger King. // &quot;name[Alimentum], area[city centre], familyFriendly[yes]&quot;,Alimentum is a family-friendly place in the city centre. // ... // // It needs some pre-processing first, so the labels are of type `Array[String]`. This can be done like so: import spark.implicits._ import com.johnsnowlabs.nlp.annotators.classifier.dl.MultiClassifierDLApproach import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import org.apache.spark.ml.Pipeline import org.apache.spark.sql.functions.{col, udf} // Process training data to create text with associated array of labels def splitAndTrim = udf { labels: String =&gt; labels.split(&quot;, &quot;).map(x=&gt;x.trim) } val smallCorpus = spark.read .option(&quot;header&quot;, true) .option(&quot;inferSchema&quot;, true) .option(&quot;mode&quot;, &quot;DROPMALFORMED&quot;) .csv(&quot;src/test/resources/classifier/e2e.csv&quot;) .withColumn(&quot;labels&quot;, splitAndTrim(col(&quot;mr&quot;))) .withColumn(&quot;text&quot;, col(&quot;ref&quot;)) .drop(&quot;mr&quot;) smallCorpus.printSchema() // root // |-- ref: string (nullable = true) // |-- labels: array (nullable = true) // | |-- element: string (containsNull = true) // Then create pipeline for training val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) .setCleanupMode(&quot;shrink&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;embeddings&quot;) val docClassifier = new MultiClassifierDLApproach() .setInputCols(&quot;embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;labels&quot;) .setBatchSize(128) .setMaxEpochs(10) .setLr(1e-3f) .setThreshold(0.5f) .setValidationSplit(0.1f) val pipeline = new Pipeline() .setStages( Array( documentAssembler, embeddings, docClassifier ) ) val pipelineModel = pipeline.fit(smallCorpus) MultiClassifierDL for Multi-label Text Classification. MultiClassifierDL Bidirectional GRU with Convolution model we have built inside TensorFlow and supports up to 100 classes. The input to MultiClassifierDL are Sentence Embeddings such as state-of-the-art UniversalSentenceEncoder, BertSentenceEmbeddings or SentenceEmbeddings. This is the instantiated model of the MultiClassifierDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val multiClassifier = MultiClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;categories&quot;) The default model is &quot;multiclassifierdl_use_toxic&quot;, if no name is provided. It uses embeddings from the UniversalSentenceEncoder and classifies toxic comments. The data is based on the Jigsaw Toxic Comment Classification Challenge. For available pretrained models please see the Models Hub. In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to. Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y). For extended examples of usage, see the Examples and the MultiClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: MultiClassifierDLModel Scala API: MultiClassifierDLModel Source: MultiClassifierDLModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) multiClassifierDl = MultiClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;classifications&quot;) pipeline = Pipeline() .setStages([ documentAssembler, useEmbeddings, multiClassifierDl ]) data = spark.createDataFrame([ [&quot;This is pretty good stuff!&quot;], [&quot;Wtf kind of crap is this&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;text&quot;, &quot;classifications.result&quot;).show(truncate=False) +--+-+ |text |result | +--+-+ |This is pretty good stuff!|[] | |Wtf kind of crap is this |[toxic, obscene]| +--+-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.classifier.dl.MultiClassifierDLModel import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val multiClassifierDl = MultiClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;classifications&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, useEmbeddings, multiClassifierDl )) val data = Seq( &quot;This is pretty good stuff!&quot;, &quot;Wtf kind of crap is this&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;text&quot;, &quot;classifications.result&quot;).show(false) +--+-+ |text |result | +--+-+ |This is pretty good stuff!|[] | |Wtf kind of crap is this |[toxic, obscene]| +--+-+ MultiDateMatcher Matches standard date formats into a provided format. Reads the following kind of dates: &quot;1978-01-28&quot;, &quot;1984/04/02,1/02/1980&quot;, &quot;2/28/79&quot;, &quot;The 31st of April in the year 2008&quot;, &quot;Fri, 21 Nov 1997&quot;, &quot;Jan 21, ‘97&quot;, &quot;Sun&quot;, &quot;Nov 21&quot;, &quot;jan 1st&quot;, &quot;next thursday&quot;, &quot;last wednesday&quot;, &quot;today&quot;, &quot;tomorrow&quot;, &quot;yesterday&quot;, &quot;next week&quot;, &quot;next month&quot;, &quot;next year&quot;, &quot;day after&quot;, &quot;the day before&quot;, &quot;0600h&quot;, &quot;06:00 hours&quot;, &quot;6pm&quot;, &quot;5:30 a.m.&quot;, &quot;at 5&quot;, &quot;12:59&quot;, &quot;23:59&quot;, &quot;1988/11/23 6pm&quot;, &quot;next week at 7.30&quot;, &quot;5 am tomorrow&quot; For example &quot;The 31st of April in the year 2008&quot; will be converted into 2008/04/31. For extended examples of usage, see the Examples and the MultiDateMatcherTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DATE Python API: MultiDateMatcher Scala API: MultiDateMatcher Source: MultiDateMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) date = MultiDateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) pipeline = Pipeline().setStages([ documentAssembler, date ]) data = spark.createDataFrame([[&quot;I saw him yesterday and he told me that he will visit us next week&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(date) as dates&quot;).show(truncate=False) +--+ |dates | +--+ |[date, 57, 65, 2020/01/18, [sentence -&gt; 0], []]| |[date, 10, 18, 2020/01/10, [sentence -&gt; 0], []]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.MultiDateMatcher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val date = new MultiDateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, date )) val data = Seq(&quot;I saw him yesterday and he told me that he will visit us next week&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(date) as dates&quot;).show(false) +--+ |dates | +--+ |[date, 57, 65, 2020/01/18, [sentence -&gt; 0], []]| |[date, 10, 18, 2020/01/10, [sentence -&gt; 0], []]| +--+ MultiDocumentAssembler Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. The MultiDocumentAssembler can read either a String column or an Array[String]. Additionally, MultiDocumentAssembler.setCleanupMode can be used to pre-process the text (Default: disabled). For possible options please refer the parameters section. For more extended examples on document pre-processing see the Examples. Input Annotator Types: NONE Output Annotator Type: DOCUMENT Python API: MultiDocumentAssembler Scala API: MultiDocumentAssembler Source: MultiDocumentAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from pyspark.ml import Pipeline data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library.&quot;], [&quot;Spark NLP is a state-of-the-art Natural Language Processing library built on top of Apache Spark&quot;]]).toDF(&quot;text&quot;, &quot;text2&quot;) documentAssembler = MultiDocumentAssembler().setInputCols([&quot;text&quot;, &quot;text2&quot;]).setOutputCols([&quot;document1&quot;, &quot;document2&quot;]) result = documentAssembler.transform(data) result.select(&quot;document1&quot;).show(truncate=False) +-+ |document1 | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document1&quot;).printSchema() root |-- document: array (nullable = True) | |-- element: struct (containsNull = True) | | |-- annotatorType: string (nullable = True) | | |-- begin: integer (nullable = False) | | |-- end: integer (nullable = False) | | |-- result: string (nullable = True) | | |-- metadata: map (nullable = True) | | | |-- key: string | | | |-- value: string (valueContainsNull = True) | | |-- embeddings: array (nullable = True) | | | |-- element: float (containsNull = False) import spark.implicits._ import com.johnsnowlabs.nlp.MultiDocumentAssembler val data = Seq(&quot;Spark NLP is an open-source text processing library.&quot;).toDF(&quot;text&quot;) val multiDocumentAssembler = new MultiDocumentAssembler().setInputCols(&quot;text&quot;).setOutputCols(&quot;document&quot;) val result = multiDocumentAssembler.transform(data) result.select(&quot;document&quot;).show(false) +-+ |document | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document&quot;).printSchema root |-- document: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- begin: integer (nullable = false) | | |-- end: integer (nullable = false) | | |-- result: string (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | | |-- embeddings: array (nullable = true) | | | |-- element: float (containsNull = false) NGramGenerator A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words. When the input is empty, an empty array is returned. When the input array length is less than n (number of elements per n-gram), no n-grams are returned. For more extended examples see the Examples and the NGramGeneratorTestSpec. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: NGramGenerator Scala API: NGramGenerator Source: NGramGenerator Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) nGrams = NGramGenerator() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;ngrams&quot;) .setN(2) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, nGrams ]) data = spark.createDataFrame([[&quot;This is my sentence.&quot;]]).toDF(&quot;text&quot;) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(ngrams) as result&quot;).show(truncate=False) ++ |result | ++ |[chunk, 0, 6, This is, [sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 5, 9, is my, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 8, 18, my sentence, [sentence -&gt; 0, chunk -&gt; 2], []]| |[chunk, 11, 19, sentence ., [sentence -&gt; 0, chunk -&gt; 3], []]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.NGramGenerator import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val nGrams = new NGramGenerator() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;ngrams&quot;) .setN(2) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, nGrams )) val data = Seq(&quot;This is my sentence.&quot;).toDF(&quot;text&quot;) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(ngrams) as result&quot;).show(false) ++ |result | ++ |[chunk, 0, 6, This is, [sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 5, 9, is my, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 8, 18, my sentence, [sentence -&gt; 0, chunk -&gt; 2], []]| |[chunk, 11, 19, sentence ., [sentence -&gt; 0, chunk -&gt; 3], []]| ++ NerConverter Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. Results in CHUNK Annotation type. NER chunks can then be filtered by setting a whitelist with setWhiteList. Chunks with no associated entity (tagged “O”) are filtered. See also Inside–outside–beginning (tagging) for more information. Input Annotator Types: DOCUMENT, TOKEN, NAMED_ENTITY Output Annotator Type: CHUNK Python API: NerConverter Scala API: NerConverter Source: NerConverter Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # This is a continuation of the example of the NerDLModel. See that class # on how to extract the entities. # The output of the NerDLModel follows the Annotator schema and can be converted like so: # # result.selectExpr(&quot;explode(ner)&quot;).show(truncate=False) # +-+ # |col | # +-+ # |[named_entity, 0, 2, B-ORG, [word -&gt; U.N], []] | # |[named_entity, 3, 3, O, [word -&gt; .], []] | # |[named_entity, 5, 12, O, [word -&gt; official], []] | # |[named_entity, 14, 18, B-PER, [word -&gt; Ekeus], []] | # |[named_entity, 20, 24, O, [word -&gt; heads], []] | # |[named_entity, 26, 28, O, [word -&gt; for], []] | # |[named_entity, 30, 36, B-LOC, [word -&gt; Baghdad], []]| # |[named_entity, 37, 37, O, [word -&gt; .], []] | # +-+ # # After the converter is used: converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;entities&quot;) converter.transform(result).selectExpr(&quot;explode(entities)&quot;).show(truncate=False) ++ |col | ++ |[chunk, 0, 2, U.N, [entity -&gt; ORG, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 14, 18, Ekeus, [entity -&gt; PER, sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 30, 36, Baghdad, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 2], []]| ++ // This is a continuation of the example of the [[com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel NerDLModel]]. See that class // on how to extract the entities. // The output of the NerDLModel follows the Annotator schema and can be converted like so: // // result.selectExpr(&quot;explode(ner)&quot;).show(false) // +-+ // |col | // +-+ // |[named_entity, 0, 2, B-ORG, [word -&gt; U.N], []] | // |[named_entity, 3, 3, O, [word -&gt; .], []] | // |[named_entity, 5, 12, O, [word -&gt; official], []] | // |[named_entity, 14, 18, B-PER, [word -&gt; Ekeus], []] | // |[named_entity, 20, 24, O, [word -&gt; heads], []] | // |[named_entity, 26, 28, O, [word -&gt; for], []] | // |[named_entity, 30, 36, B-LOC, [word -&gt; Baghdad], []]| // |[named_entity, 37, 37, O, [word -&gt; .], []] | // +-+ // // After the converter is used: val converter = new NerConverter() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) .setPreservePosition(false) converter.transform(result).selectExpr(&quot;explode(entities)&quot;).show(false) ++ |col | ++ |[chunk, 0, 2, U.N, [entity -&gt; ORG, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 14, 18, Ekeus, [entity -&gt; PER, sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 30, 36, Baghdad, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 2], []]| ++ NerCrf ApproachModel Algorithm for training a Named Entity Recognition Model For instantiated/pretrained models, see NerCrfModel. This Named Entity recognition annotator allows for a generic model to be trained by utilizing a CRF machine learning algorithm. The training data should be a labeled Spark Dataset, e.g. CoNLL 2003 IOB with Annotation type columns. The data should have columns of type DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS and an additional label column of annotator type NAMED_ENTITY. Excluding the label, this can be done with for example a SentenceDetector, a Tokenizer and a PerceptronModel and a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). Optionally the user can provide an entity dictionary file with setExternalFeatures for better accuracy. For extended examples of usage, see the Examples and the NerCrfApproachTestSpec. Input Annotator Types: DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerCrfApproach Scala API: NerCrfApproach Source: NerCrfApproach Show Example PythonScala # This CoNLL dataset already includes the sentence, token, pos and label column with their respective annotator types. # If a custom dataset is used, these need to be defined. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) nerTagger = NerCrfApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setMinEpochs(1) .setMaxEpochs(3) .setC0(34) .setL2(3.0) .setOutputCol(&quot;ner&quot;) pipeline = Pipeline().setStages([ documentAssembler, embeddings, nerTagger ]) conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) // This CoNLL dataset already includes the sentence, token, pos and label column with their respective annotator types. // If a custom dataset is used, these need to be defined. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotator.NerCrfApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val nerTagger = new NerCrfApproach() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;) .setLabelColumn(&quot;label&quot;) .setMinEpochs(1) .setMaxEpochs(3) .setC0(34) .setL2(3.0) .setOutputCol(&quot;ner&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, embeddings, nerTagger )) val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) Extracts Named Entities based on a CRF Model. This Named Entity recognition annotator allows for a generic model to be trained by utilizing a CRF machine learning algorithm. The data should have columns of type DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS. These can be extracted with for example a SentenceDetector, a Tokenizer and a PerceptronModel This is the instantiated model of the NerCrfApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val nerTagger = NerCrfModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;, &quot;pos&quot;) .setOutputCol(&quot;ner&quot; The default model is &quot;ner_crf&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples. Input Annotator Types: DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerCrfModel Scala API: NerCrfModel Source: NerCrfModel NerDL ApproachModel This Named Entity recognition annotator allows to train generic NER model based on Neural Networks. The architecture of the neural network is a Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. For instantiated/pretrained models, see NerDLModel. The training data should be a labeled Spark Dataset, in the format of CoNLL 2003 IOB with Annotation type columns. The data should have columns of type DOCUMENT, TOKEN, WORD_EMBEDDINGS and an additional label column of annotator type NAMED_ENTITY. Excluding the label, this can be done with for example a SentenceDetector, a Tokenizer and a PerceptronModel and a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). Setting a test dataset to monitor model metrics can be done with .setTestDataset. The method expects a path to a parquet file containing a dataframe that has the same required columns as the training dataframe. The pre-processing steps for the training dataframe should also be applied to the test dataframe. The following example will show how to create the test dataset with a CoNLL dataset: val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = WordEmbeddingsModel .pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val preProcessingPipeline = new Pipeline().setStages(Array(documentAssembler, embeddings)) val conll = CoNLL() val Array(train, test) = conll .readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) .randomSplit(Array(0.8, 0.2)) preProcessingPipeline .fit(test) .transform(test) .write .mode(&quot;overwrite&quot;) .parquet(&quot;test_data&quot;) val nerTagger = new NerDLApproach() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setTestDataset(&quot;test_data&quot;) For extended examples of usage, see the Examples and the NerDLSpec. Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerDLApproach Scala API: NerDLApproach Source: NerDLApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline # First extract the prerequisites for the NerDLApproach documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = BertEmbeddings.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Then the training can start nerTagger = NerDLApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(1) .setRandomSeed(0) .setVerbose(0) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, embeddings, nerTagger ]) # We use the text and labels from the CoNLL dataset conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.BertEmbeddings import com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline // First extract the prerequisites for the NerDLApproach val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger = new NerDLApproach() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(1) .setRandomSeed(0) .setVerbose(0) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) // We use the text and labels from the CoNLL dataset val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) This Named Entity recognition annotator is a generic NER model based on Neural Networks. Neural Network architecture is Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. This is the instantiated model of the NerDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val nerModel = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) The default model is &quot;ner_dl&quot;, if no name is provided. For available pretrained models please see the Models Hub. Additionally, pretrained pipelines are available for this module, see Pipelines. Note that some pretrained models require specific types of embeddings, depending on which they were trained on. For example, the default model &quot;ner_dl&quot; requires the WordEmbeddings &quot;glove_100d&quot;. For extended examples of usage, see the Examples and the NerDLSpec. Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerDLModel Scala API: NerDLModel Source: NerDLModel NerOverwriter Overwrites entities of specified strings. The input for this Annotator have to be entities that are already extracted, Annotator type NAMED_ENTITY. The strings specified with setStopWords will have new entities assigned to, specified with setNewResult. Input Annotator Types: NAMED_ENTITY Output Annotator Type: NAMED_ENTITY Python API: NerOverwriter Scala API: NerOverwriter Source: NerOverwriter Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # First extract the prerequisite Entities documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;bert&quot;) nerTagger = NerDLModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;bert&quot;]) .setOutputCol(&quot;ner&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, embeddings, nerTagger ]) data = spark.createDataFrame([[&quot;Spark NLP Crosses Five Million Downloads, John Snow Labs Announces.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(ner)&quot;).show(truncate=False) # ++ # |col | # ++ # |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | # |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | # |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | # |[named_entity, 18, 21, O, [word -&gt; Five], []] | # |[named_entity, 23, 29, O, [word -&gt; Million], []] | # |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | # |[named_entity, 40, 40, O, [word -&gt; ,], []] | # |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | # |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | # |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | # |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []]| # |[named_entity, 66, 66, O, [word -&gt; .], []] | # ++ # The recognized entities can then be overwritten nerOverwriter = NerOverwriter() .setInputCols([&quot;ner&quot;]) .setOutputCol(&quot;ner_overwritten&quot;) .setStopWords([&quot;Million&quot;]) .setNewResult(&quot;B-CARDINAL&quot;) nerOverwriter.transform(result).selectExpr(&quot;explode(ner_overwritten)&quot;).show(truncate=False) ++ |col | ++ |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | |[named_entity, 18, 21, O, [word -&gt; Five], []] | |[named_entity, 23, 29, B-CARDINAL, [word -&gt; Million], []]| |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | |[named_entity, 40, 40, O, [word -&gt; ,], []] | |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []] | |[named_entity, 66, 66, O, [word -&gt; .], []] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel import com.johnsnowlabs.nlp.annotators.ner.NerOverwriter import org.apache.spark.ml.Pipeline // First extract the prerequisite Entities val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;bert&quot;) val nerTagger = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;bert&quot;) .setOutputCol(&quot;ner&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) val data = Seq(&quot;Spark NLP Crosses Five Million Downloads, John Snow Labs Announces.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(ner)&quot;).show(false) / ++ |col | ++ |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | |[named_entity, 18, 21, O, [word -&gt; Five], []] | |[named_entity, 23, 29, O, [word -&gt; Million], []] | |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | |[named_entity, 40, 40, O, [word -&gt; ,], []] | |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []]| |[named_entity, 66, 66, O, [word -&gt; .], []] | ++ / // The recognized entities can then be overwritten val nerOverwriter = new NerOverwriter() .setInputCols(&quot;ner&quot;) .setOutputCol(&quot;ner_overwritten&quot;) .setStopWords(Array(&quot;Million&quot;)) .setNewResult(&quot;B-CARDINAL&quot;) nerOverwriter.transform(result).selectExpr(&quot;explode(ner_overwritten)&quot;).show(false) ++ |col | ++ |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | |[named_entity, 18, 21, O, [word -&gt; Five], []] | |[named_entity, 23, 29, B-CARDINAL, [word -&gt; Million], []]| |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | |[named_entity, 40, 40, O, [word -&gt; ,], []] | |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []] | |[named_entity, 66, 66, O, [word -&gt; .], []] | ++ Normalizer ApproachModel Annotator that cleans out tokens. Requires stems, hence tokens. Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary For extended examples of usage, see the Examples. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: Normalizer Scala API: Normalizer Source: Normalizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) .setLowercase(True) .setCleanupPatterns([&quot;&quot;&quot;[^ w d s]&quot;&quot;&quot;]) # remove punctuations (keep alphanumeric chars) # if we don&#39;t set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z]) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, normalizer ]) data = spark.createDataFrame([[&quot;John and Peter are brothers. However they don&#39;t support each other that much.&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;normalized.result&quot;).show(truncate = False) +-+ |result | +-+ |[john, and, peter, are, brothers, however, they, dont, support, each, other, that, much]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Normalizer, Tokenizer} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) .setLowercase(true) .setCleanupPatterns(Array(&quot;&quot;&quot;[^ w d s]&quot;&quot;&quot;)) // remove punctuations (keep alphanumeric chars) // if we don&#39;t set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z]) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, normalizer )) val data = Seq(&quot;John and Peter are brothers. However they don&#39;t support each other that much.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;normalized.result&quot;).show(truncate = false) +-+ |result | +-+ |[john, and, peter, are, brothers, however, they, dont, support, each, other, that, much]| +-+ Instantiated Model of the Normalizer. For usage and examples, please see the documentation of that class. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: NormalizerModel Scala API: NormalizerModel Source: NormalizerModel NorvigSweeting Spellchecker ApproachModel Trains annotator, that retrieves tokens and makes corrections automatically if not found in an English dictionary. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster (than the standard approach with deletes + transposes + replaces + inserts) and language independent. A dictionary of correct spellings must be provided with setDictionary as a text file, where each word is parsed by a regex pattern. Inspired by Norvig model and SymSpell. For instantiated/pretrained models, see NorvigSweetingModel. For extended examples of usage, see the NorvigSweetingTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: NorvigSweetingApproach Scala API: NorvigSweetingApproach Source: NorvigSweetingApproach Show Example PythonScala # In this example, the dictionary `&quot;words.txt&quot;` has the form of # # ... # gummy # gummic # gummier # gummiest # gummiferous # ... # # This dictionary is then set to be the basis of the spell checker. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) spellChecker = NorvigSweetingApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker ]) pipelineModel = pipeline.fit(trainingData) // In this example, the dictionary `&quot;words.txt&quot;` has the form of // // ... // gummy // gummic // gummier // gummiest // gummiferous // ... // // This dictionary is then set to be the basis of the spell checker. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.spell.norvig.NorvigSweetingApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val spellChecker = new NorvigSweetingApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker )) val pipelineModel = pipeline.fit(trainingData) This annotator retrieves tokens and makes corrections automatically if not found in an English dictionary. Inspired by Norvig model and SymSpell. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster (than the standard approach with deletes + transposes + replaces + inserts) and language independent. This is the instantiated model of the NorvigSweetingApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val spellChecker = NorvigSweetingModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) .setDoubleVariants(true) The default model is &quot;spellcheck_norvig&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of see the NorvigSweetingTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: NorvigSweetingModel Scala API: NorvigSweetingModel Source: NorvigSweetingModel POSTagger (Part of speech tagger) ApproachModel Trains an averaged Perceptron model to tag words part-of-speech. Sets a POS tag to each word within a sentence. For pretrained models please see the PerceptronModel. The training data needs to be in a Spark DataFrame, where the column needs to consist of Annotations of type POS. The Annotation needs to have member result set to the POS tag and have a &quot;word&quot; mapping to its word inside of member metadata. This DataFrame for training can easily created by the helper class POS. POS().readDataset(spark, datasetPath).selectExpr(&quot;explode(tags) as tags&quot;).show(false) ++ |tags | ++ |[pos, 0, 5, NNP, [word -&gt; Pierre], []] | |[pos, 7, 12, NNP, [word -&gt; Vinken], []] | |[pos, 14, 14, ,, [word -&gt; ,], []] | |[pos, 31, 34, MD, [word -&gt; will], []] | |[pos, 36, 39, VB, [word -&gt; join], []] | |[pos, 41, 43, DT, [word -&gt; the], []] | |[pos, 45, 49, NN, [word -&gt; board], []] | ... For extended examples of usage, see the Examples and PerceptronApproach tests. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: POS Python API: PerceptronApproach Scala API: PerceptronApproach Source: PerceptronApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) datasetPath = &quot;src/test/resources/anc-pos-corpus-small/test-training.txt&quot; trainingPerceptronDF = POS().readDataset(spark, datasetPath) trainedPos = PerceptronApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) .setPosColumn(&quot;tags&quot;) .fit(trainingPerceptronDF) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, trainedPos ]) data = spark.createDataFrame([[&quot;To be or not to be, is this the question?&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;pos.result&quot;).show(truncate=False) +--+ |result | +--+ |[NNP, NNP, CD, JJ, NNP, NNP, ,, MD, VB, DT, CD, .]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.training.POS import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val datasetPath = &quot;src/test/resources/anc-pos-corpus-small/test-training.txt&quot; val trainingPerceptronDF = POS().readDataset(spark, datasetPath) val trainedPos = new PerceptronApproach() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) .setPosColumn(&quot;tags&quot;) .fit(trainingPerceptronDF) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, trainedPos )) val data = Seq(&quot;To be or not to be, is this the question?&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;pos.result&quot;).show(false) +--+ |result | +--+ |[NNP, NNP, CD, JJ, NNP, NNP, ,, MD, VB, DT, CD, .]| +--+ Averaged Perceptron model to tag words part-of-speech. Sets a POS tag to each word within a sentence. This is the instantiated model of the PerceptronApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) The default model is &quot;pos_anc&quot;, if no name is provided. For available pretrained models please see the Models Hub. Additionally, pretrained pipelines are available for this module, see Pipelines. For extended examples of usage, see the Examples. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: POS Python API: PerceptronModel Scala API: PerceptronModel Source: PerceptronModel RecursiveTokenizer ApproachModel Tokenizes raw text recursively based on a handful of definable rules. Unlike the Tokenizer, the RecursiveTokenizer operates based on these array string parameters only: prefixes: Strings that will be split when found at the beginning of token. suffixes: Strings that will be split when found at the end of token. infixes: Strings that will be split when found at the middle of token. whitelist: Whitelist of strings not to split For extended examples of usage, see the Examples and the TokenizerTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: RecursiveTokenizer Scala API: RecursiveTokenizer Source: RecursiveTokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = RecursiveTokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer ]) data = spark.createDataFrame([[&quot;One, after the Other, (and) again. PO, QAM,&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;token.result&quot;).show(truncate=False) ++ |result | ++ |[One, ,, after, the, Other, ,, (, and, ), again, ., PO, ,, QAM, ,]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.RecursiveTokenizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new RecursiveTokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer )) val data = Seq(&quot;One, after the Other, (and) again. PO, QAM,&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;token.result&quot;).show(false) ++ |result | ++ |[One, ,, after, the, Other, ,, (, and, ), again, ., PO, ,, QAM, ,]| ++ Instantiated model of the RecursiveTokenizer. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: RecursiveTokenizerModel Scala API: RecursiveTokenizerModel Source: RecursiveTokenizerModel RegexMatcher ApproachModel Uses rules to match a set of regular expressions and associate them with a provided identifier. A rule consists of a regex pattern and an identifier, delimited by a character of choice. An example could be &quot; d{4} / d d / d d,date&quot; which will match strings like &quot;1970/01/01&quot; to the identifier &quot;date&quot;. Rules must be provided by either setRules (followed by setDelimiter) or an external file. To use an external file, a dictionary of predefined regular expressions must be provided with setExternalRules. The dictionary can be set as a delimited text file. Pretrained pipelines are available for this module, see Pipelines. For extended examples of usage, see the Examples and the RegexMatcherTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: CHUNK Python API: RegexMatcher Scala API: RegexMatcher Source: RegexMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the `rules.txt` has the form of # # the s w+, followed by &#39;the&#39; # ceremonies, ceremony # # where each regex is separated by the identifier by `&quot;,&quot;` documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentence = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) regexMatcher = RegexMatcher() .setExternalRules(&quot;src/test/resources/regex-matcher/rules.txt&quot;, &quot;,&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;regex&quot;) .setStrategy(&quot;MATCH_ALL&quot;) pipeline = Pipeline().setStages([documentAssembler, sentence, regexMatcher]) data = spark.createDataFrame([[ &quot;My first sentence with the first rule. This is my second sentence with ceremonies rule.&quot; ]]).toDF(&quot;text&quot;) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(regex) as result&quot;).show(truncate=False) +--+ |result | +--+ |[chunk, 23, 31, the first, [identifier -&gt; followed by &#39;the&#39;, sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 71, 80, ceremonies, [identifier -&gt; ceremony, sentence -&gt; 1, chunk -&gt; 0], []] | +--+ // In this example, the `rules.txt` has the form of // // the s w+, followed by &#39;the&#39; // ceremonies, ceremony // // where each regex is separated by the identifier by `&quot;,&quot;` import ResourceHelper.spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.RegexMatcher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val regexMatcher = new RegexMatcher() .setExternalRules(&quot;src/test/resources/regex-matcher/rules.txt&quot;, &quot;,&quot;) .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;regex&quot;) .setStrategy(&quot;MATCH_ALL&quot;) val pipeline = new Pipeline().setStages(Array(documentAssembler, sentence, regexMatcher)) val data = Seq( &quot;My first sentence with the first rule. This is my second sentence with ceremonies rule.&quot; ).toDF(&quot;text&quot;) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(regex) as result&quot;).show(false) +--+ |result | +--+ |[chunk, 23, 31, the first, [identifier -&gt; followed by &#39;the&#39;, sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 71, 80, ceremonies, [identifier -&gt; ceremony, sentence -&gt; 1, chunk -&gt; 0], []] | +--+ Instantiated model of the RegexMatcher. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT Output Annotator Type: CHUNK Python API: RegexMatcherModel Scala API: RegexMatcherModel Source: RegexMatcherModel RegexTokenizer A tokenizer that splits text by a regex pattern. The pattern needs to be set with setPattern and this sets the delimiting pattern or how the tokens should be split. By default this pattern is s+ which means that tokens should be split by 1 or more whitespace characters. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: RegexTokenizer Scala API: RegexTokenizer Source: RegexTokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) regexTokenizer = RegexTokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;regexToken&quot;) .setToLowercase(True) .setPattern(&quot; s+&quot;) pipeline = Pipeline().setStages([ documentAssembler, regexTokenizer ]) data = spark.createDataFrame([[&quot;This is my first sentence. nThis is my second.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;regexToken.result&quot;).show(truncate=False) +-+ |result | +-+ |[this, is, my, first, sentence., this, is, my, second.]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.RegexTokenizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val regexTokenizer = new RegexTokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;regexToken&quot;) .setToLowercase(true) .setPattern(&quot; s+&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, regexTokenizer )) val data = Seq(&quot;This is my first sentence. nThis is my second.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;regexToken.result&quot;).show(false) +-+ |result | +-+ |[this, is, my, first, sentence., this, is, my, second.]| +-+ SentenceDetector Annotator that detects sentence boundaries using regular expressions. The following characters are checked as sentence boundaries: Lists (“(i), (ii)”, “(a), (b)”, “1., 2.”) Numbers Abbreviations Punctuations Multiple Periods Geo-Locations/Coordinates (“N°. 1026.253.553.”) Ellipsis (“…”) In-between punctuations Quotation marks Exclamation Points Basic Breakers (“.”, “;”) For the explicit regular expressions used for detection, refer to source of PragmaticContentFormatter. To add additional custom bounds, the parameter customBounds can be set with an array: val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) .setCustomBounds(Array(&quot; n n&quot;)) If only the custom bounds should be used, then the parameter useCustomBoundsOnly should be set to true. Each extracted sentence can be returned in an Array or exploded to separate rows, if explodeSentences is set to true. For extended examples of usage, see the Examples. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: SentenceDetector Scala API: SentenceDetector Source: SentenceDetector Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setCustomBounds([&quot; n n&quot;]) pipeline = Pipeline().setStages([ documentAssembler, sentence ]) data = spark.createDataFrame([[&quot;This is my first sentence. This my second. How about a third?&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(sentence) as sentences&quot;).show(truncate=False) ++ |sentences | ++ |[document, 0, 25, This is my first sentence., [sentence -&gt; 0], []]| |[document, 27, 41, This my second., [sentence -&gt; 1], []] | |[document, 43, 60, How about a third?, [sentence -&gt; 2], []] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) .setCustomBounds(Array(&quot; n n&quot;)) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence )) val data = Seq(&quot;This is my first sentence. This my second. How about a third?&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(sentence) as sentences&quot;).show(false) ++ |sentences | ++ |[document, 0, 25, This is my first sentence., [sentence -&gt; 0], []]| |[document, 27, 41, This my second., [sentence -&gt; 1], []] | |[document, 43, 60, How about a third?, [sentence -&gt; 2], []] | ++ SentenceDetectorDL ApproachModel Trains an annotator that detects sentence boundaries using a deep learning approach. For pretrained models see SentenceDetectorDLModel. Currently, only the CNN model is supported for training, but in the future the architecture of the model can be set with setModelArchitecture. The default model &quot;cnn&quot; is based on the paper Deep-EOS: General-Purpose Neural Networks for Sentence Boundary Detection (2020, Stefan Schweter, Sajawel Ahmed) using a CNN architecture. We also modified the original implementation a little bit to cover broken sentences and some impossible end of line chars. Each extracted sentence can be returned in an Array or exploded to separate rows, if explodeSentences is set to true. For extended examples of usage, see the Examples and the SentenceDetectorDLSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: SentenceDetectorDLApproach Scala API: SentenceDetectorDLApproach Source: SentenceDetectorDLApproach Show Example PythonScala # The training process needs data, where each data point is a sentence. # In this example the `train.txt` file has the form of # # ... # Slightly more moderate language would make our present situation – namely the lack of progress – a little easier. # His political successors now have great responsibilities to history and to the heritage of values bequeathed to them by Nelson Mandela. # ... # # where each line is one sentence. # Training can then be started like so: import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline trainingData = spark.read.text(&quot;train.txt&quot;).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLApproach() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) .setEpochsNumber(100) pipeline = Pipeline().setStages([documentAssembler, sentenceDetector]) model = pipeline.fit(trainingData) // The training process needs data, where each data point is a sentence. // In this example the `train.txt` file has the form of // // ... // Slightly more moderate language would make our present situation – namely the lack of progress – a little easier. // His political successors now have great responsibilities to history and to the heritage of values bequeathed to them by Nelson Mandela. // ... // // where each line is one sentence. // Training can then be started like so: import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLApproach import org.apache.spark.ml.Pipeline val trainingData = spark.read.text(&quot;train.txt&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetectorDLApproach() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentences&quot;) .setEpochsNumber(100) val pipeline = new Pipeline().setStages(Array(documentAssembler, sentenceDetector)) val model = pipeline.fit(trainingData) Annotator that detects sentence boundaries using a deep learning approach. Instantiated Model of the SentenceDetectorDLApproach. Detects sentence boundaries using a deep learning approach. Pretrained models can be loaded with pretrained of the companion object: val sentenceDL = SentenceDetectorDLModel.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentencesDL&quot;) The default model is &quot;sentence_detector_dl&quot;, if no name is provided. For available pretrained models please see the Models Hub. Each extracted sentence can be returned in an Array or exploded to separate rows, if explodeSentences is set to true. For extended examples of usage, see the Examples and the SentenceDetectorDLSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: SentenceDetectorDLModel Scala API: SentenceDetectorDLModel Source: SentenceDetectorDLModel SentenceEmbeddings Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols). This can be configured with setPoolingStrategy, which either be &quot;AVERAGE&quot; or &quot;SUM&quot;. For more extended examples see the Examples. and the SentenceEmbeddingsTestSpec. TIP: Here is how you can explode and convert these embeddings into Vectors or what’s known as Feature column so it can be used in Spark ML regression or clustering functions: PythonScala from org.apache.spark.ml.linal import Vector, Vectors from pyspark.sql.functions import udf # Let&#39;s create a UDF to take array of embeddings and output Vectors @udf(Vector) def convertToVectorUDF(matrix): return Vectors.dense(matrix.toArray.map(_.toDouble)) # Now let&#39;s explode the sentence_embeddings column and have a new feature column for Spark ML pipelineDF.select(explode(&quot;sentence_embeddings.embeddings&quot;).as(&quot;sentence_embedding&quot;)) .withColumn(&quot;features&quot;, convertToVectorUDF(&quot;sentence_embedding&quot;)) import org.apache.spark.ml.linalg.{Vector, Vectors} // Let&#39;s create a UDF to take array of embeddings and output Vectors val convertToVectorUDF = udf((matrix : Seq[Float]) =&gt; { Vectors.dense(matrix.toArray.map(_.toDouble)) }) // Now let&#39;s explode the sentence_embeddings column and have a new feature column for Spark ML pipelineDF.select(explode($&quot;sentence_embeddings.embeddings&quot;).as(&quot;sentence_embedding&quot;)) .withColumn(&quot;features&quot;, convertToVectorUDF($&quot;sentence_embedding&quot;)) Input Annotator Types: DOCUMENT, WORD_EMBEDDINGS Output Annotator Type: SENTENCE_EMBEDDINGS Note: If you choose document as your input for Tokenizer, WordEmbeddings/BertEmbeddings, and SentenceEmbeddings then it averages/sums all the embeddings into one array of embeddings. However, if you choose sentence as inputCols then for each sentence SentenceEmbeddings generates one array of embeddings. Python API: SentenceEmbeddings Scala API: SentenceEmbeddings Source: SentenceEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsSentence = SentenceEmbeddings() .setInputCols([&quot;document&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) .setCleanAnnotations(False) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings, embeddingsSentence, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80) +--+ | result| +--+ |[-0.22093398869037628,0.25130119919776917,0.41810303926467896,-0.380883991718...| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsSentence = new SentenceEmbeddings() .setInputCols(Array(&quot;document&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;sentence_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsSentence, embeddingsFinisher )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80) +--+ | result| +--+ |[-0.22093398869037628,0.25130119919776917,0.41810303926467896,-0.380883991718...| +--+ SentimentDL ApproachModel Trains a SentimentDL, an annotator for multi-class sentiment analysis. In natural language processing, sentiment analysis is the task of classifying the affective state or subjective view of a text. A common example is if either a product review or tweet can be interpreted positively or negatively. For the instantiated/pretrained models, see SentimentDLModel. Notes: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. So positive sentiment can be expressed as either &quot;positive&quot; or 0, negative sentiment as &quot;negative&quot; or 1. UniversalSentenceEncoder, BertSentenceEmbeddings, SentenceEmbeddings or other sentence based embeddings can be used Setting a test dataset to monitor model metrics can be done with .setTestDataset. The method expects a path to a parquet file containing a dataframe that has the same required columns as the training dataframe. The pre-processing steps for the training dataframe should also be applied to the test dataframe. The following example will show how to create the test dataset: val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val preProcessingPipeline = new Pipeline().setStages(Array(documentAssembler, embeddings)) val Array(train, test) = data.randomSplit(Array(0.8, 0.2)) preProcessingPipeline .fit(test) .transform(test) .write .mode(&quot;overwrite&quot;) .parquet(&quot;test_data&quot;) val classifier = new SentimentDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sentiment&quot;) .setLabelColumn(&quot;label&quot;) .setTestDataset(&quot;test_data&quot;) For extended examples of usage, see the Examples and the SentimentDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: SentimentDLApproach Scala API: SentimentDLApproach Source: SentimentDLApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, `sentiment.csv` is in the form # # text,label # This movie is the best movie I have watched ever! In my opinion this movie can win an award.,0 # This was a terrible movie! The acting was bad really bad!,1 # # The model can then be trained with smallCorpus = spark.read.option(&quot;header&quot;, &quot;True&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) docClassifier = SentimentDLApproach() .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCol(&quot;sentiment&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(32) .setMaxEpochs(1) .setLr(5e-3) .setDropout(0.5) pipeline = Pipeline() .setStages( [ documentAssembler, useEmbeddings, docClassifier ] ) pipelineModel = pipeline.fit(smallCorpus) // In this example, `sentiment.csv` is in the form // // text,label // This movie is the best movie I have watched ever! In my opinion this movie can win an award.,0 // This was a terrible movie! The acting was bad really bad!,1 // // The model can then be trained with import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.UniversalSentenceEncoder import com.johnsnowlabs.nlp.annotators.classifier.dl.{SentimentDLApproach, SentimentDLModel} import org.apache.spark.ml.Pipeline val smallCorpus = spark.read.option(&quot;header&quot;, &quot;true&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val docClassifier = new SentimentDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sentiment&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(32) .setMaxEpochs(1) .setLr(5e-3f) .setDropout(0.5f) val pipeline = new Pipeline() .setStages( Array( documentAssembler, useEmbeddings, docClassifier ) ) val pipelineModel = pipeline.fit(smallCorpus) SentimentDL, an annotator for multi-class sentiment analysis. In natural language processing, sentiment analysis is the task of classifying the affective state or subjective view of a text. A common example is if either a product review or tweet can be interpreted positively or negatively. This is the instantiated model of the SentimentDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val sentiment = SentimentDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sentiment&quot;) The default model is &quot;sentimentdl_use_imdb&quot;, if no name is provided. It is english sentiment analysis trained on the IMDB dataset. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the SentimentDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: SentimentDLModel Scala API: SentimentDLModel Source: SentimentDLModel SentimentDetector ApproachModel Trains a rule based sentiment detector, which calculates a score based on predefined keywords. A dictionary of predefined sentiment keywords must be provided with setDictionary, where each line is a word delimited to its class (either positive or negative). The dictionary can be set as a delimited text file. By default, the sentiment score will be assigned labels &quot;positive&quot; if the score is &gt;= 0, else &quot;negative&quot;. To retrieve the raw sentiment scores, enableScore needs to be set to true. For extended examples of usage, see the Examples and the SentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: SentimentDetector Scala API: SentimentDetector Source: SentimentDetector Show Example PythonScala # In this example, the dictionary `default-sentiment-dict.txt` has the form of # # ... # cool,positive # superb,positive # bad,negative # uninspired,negative # ... # # where each sentiment keyword is delimited by `&quot;,&quot;`. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = Lemmatizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) sentimentDetector = SentimentDetector() .setInputCols([&quot;lemma&quot;, &quot;document&quot;]) .setOutputCol(&quot;sentimentScore&quot;) .setDictionary(&quot;default-sentiment-dict.txt&quot;, &quot;,&quot;, ReadAs.TEXT) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, lemmatizer, sentimentDetector, ]) data = spark.createDataFrame([ [&quot;The staff of the restaurant is nice&quot;], [&quot;I recommend others to avoid because it is too expensive&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;sentimentScore.result&quot;).show(truncate=False) +-+ # ++ for enableScore set to True |result | # |result| +-+ # ++ |[positive]| # |[1.0] | |[negative]| # |[-2.0]| +-+ # ++ // In this example, the dictionary `default-sentiment-dict.txt` has the form of // // ... // cool,positive // superb,positive // bad,negative // uninspired,negative // ... // // where each sentiment keyword is delimited by `&quot;,&quot;`. import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotators.Lemmatizer import com.johnsnowlabs.nlp.annotators.sda.pragmatic.SentimentDetector import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val lemmatizer = new Lemmatizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) val sentimentDetector = new SentimentDetector() .setInputCols(&quot;lemma&quot;, &quot;document&quot;) .setOutputCol(&quot;sentimentScore&quot;) .setDictionary(&quot;src/test/resources/sentiment-corpus/default-sentiment-dict.txt&quot;, &quot;,&quot;, ReadAs.TEXT) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, lemmatizer, sentimentDetector, )) val data = Seq( &quot;The staff of the restaurant is nice&quot;, &quot;I recommend others to avoid because it is too expensive&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;sentimentScore.result&quot;).show(false) +-+ // ++ for enableScore set to true |result | // |result| +-+ // ++ |[positive]| // |[1.0] | |[negative]| // |[-2.0]| +-+ // ++ Rule based sentiment detector, which calculates a score based on predefined keywords. This is the instantiated model of the SentimentDetector. For training your own model, please see the documentation of that class. A dictionary of predefined sentiment keywords must be provided with setDictionary, where each line is a word delimited to its class (either positive or negative). The dictionary can be set as a delimited text file. By default, the sentiment score will be assigned labels &quot;positive&quot; if the score is &gt;= 0, else &quot;negative&quot;. To retrieve the raw sentiment scores, enableScore needs to be set to true. For extended examples of usage, see the Examples and the SentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: SentimentDetectorModel Scala API: SentimentDetectorModel Source: SentimentDetectorModel Stemmer Returns hard-stems out of words with the objective of retrieving the meaningful part of the word. For extended examples of usage, see the Examples. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: Stemmer Scala API: Stemmer Source: Stemmer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) stemmer = Stemmer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;stem&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, stemmer ]) data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;stem.result&quot;).show(truncate = False) +-+ |result | +-+ |[peter, piper, employe, ar, pick, peck, of, pickl, pepper, .]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Stemmer, Tokenizer} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val stemmer = new Stemmer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;stem&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, stemmer )) val data = Seq(&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;stem.result&quot;).show(truncate = false) +-+ |result | +-+ |[peter, piper, employe, ar, pick, peck, of, pickl, pepper, .]| +-+ StopWordsCleaner This annotator takes a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences. By default, it uses stop words from MLlibs StopWordsRemover. Stop words can also be defined by explicitly setting them with setStopWords(value: Array[String]) or loaded from pretrained models using pretrained of its companion object. val stopWords = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) // will load the default pretrained model `&quot;stopwords_en&quot;`. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and StopWordsCleanerTestSpec. NOTE: If you need to setStopWords from a text file, you can first read and convert it into an array of string as follows. PythonScala # your stop words text file, each line is one stop word stopwords = sc.textFile(&quot;/tmp/stopwords/english.txt&quot;).collect() # simply use it in StopWordsCleaner stopWordsCleaner = StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setStopWords(stopwords) .setCaseSensitive(False) # or you can use pretrained models for StopWordsCleaner stopWordsCleaner = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) // your stop words text file, each line is one stop word val stopwords = sc.textFile(&quot;/tmp/stopwords/english.txt&quot;).collect() // simply use it in StopWordsCleaner val stopWordsCleaner = new StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setStopWords(stopwords) .setCaseSensitive(false) // or you can use pretrained models for StopWordsCleaner val stopWordsCleaner = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: StopWordsCleaner Scala API: StopWordsCleaner Source: StopWordsCleaner Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) stopWords = StopWordsCleaner() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, stopWords ]) data = spark.createDataFrame([ [&quot;This is my first sentence. This is my second.&quot;], [&quot;This is my third sentence. This is my forth.&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;cleanTokens.result&quot;).show(truncate=False) +-+ |result | +-+ |[first, sentence, ., second, .]| |[third, sentence, ., forth, .] | +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.StopWordsCleaner import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val stopWords = new StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, stopWords )) val data = Seq( &quot;This is my first sentence. This is my second.&quot;, &quot;This is my third sentence. This is my forth.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;cleanTokens.result&quot;).show(false) +-+ |result | +-+ |[first, sentence, ., second, .]| |[third, sentence, ., forth, .] | +-+ SymmetricDelete Spellchecker ApproachModel Trains a Symmetric Delete spelling correction algorithm. Retrieves tokens and utilizes distance metrics to compute possible derived words. Inspired by SymSpell. For instantiated/pretrained models, see SymmetricDeleteModel. See SymmetricDeleteModelTestSpec for further reference. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: SymmetricDeleteApproach Scala API: SymmetricDeleteApproach Source: SymmetricDeleteApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the dictionary `&quot;words.txt&quot;` has the form of # # ... # gummy # gummic # gummier # gummiest # gummiferous # ... # # This dictionary is then set to be the basis of the spell checker. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) spellChecker = SymmetricDeleteApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker ]) pipelineModel = pipeline.fit(trainingData) // In this example, the dictionary `&quot;words.txt&quot;` has the form of // // ... // gummy // gummic // gummier // gummiest // gummiferous // ... // // This dictionary is then set to be the basis of the spell checker. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val spellChecker = new SymmetricDeleteApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker )) val pipelineModel = pipeline.fit(trainingData) Symmetric Delete spelling correction algorithm. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster (than the standard approach with deletes + transposes + replaces + inserts) and language independent. Inspired by SymSpell. Pretrained models can be loaded with pretrained of the companion object: val spell = SymmetricDeleteModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) The default model is &quot;spellcheck_sd&quot;, if no name is provided. For available pretrained models please see the Models Hub. See SymmetricDeleteModelTestSpec for further reference. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: SymmetricDeleteModel Scala API: SymmetricDeleteModel Source: SymmetricDeleteModel TextMatcher ApproachModel Annotator to match exact phrases (by token) provided in a file against a Document. A text file of predefined phrases must be provided with setEntities. For extended examples of usage, see the Examples and the TextMatcherTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: TextMatcher Scala API: TextMatcher Source: TextMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the entities file is of the form # # ... # dolore magna aliqua # lorem ipsum dolor. sit # laborum # ... # # where each line represents an entity phrase to be extracted. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) data = spark.createDataFrame([[&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;]]).toDF(&quot;text&quot;) entityExtractor = TextMatcher() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setEntities(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(False) pipeline = Pipeline().setStages([documentAssembler, tokenizer, entityExtractor]) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity) as result&quot;).show(truncate=False) ++ |result | ++ |[chunk, 6, 24, dolore magna aliqua, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 27, 48, Lorem ipsum dolor. sit, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 1], []]| |[chunk, 53, 59, laborum, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 2], []] | ++ // In this example, the entities file is of the form // // ... // dolore magna aliqua // lorem ipsum dolor. sit // laborum // ... // // where each line represents an entity phrase to be extracted. import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.TextMatcher import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val data = Seq(&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;).toDF(&quot;text&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setEntities(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(false) .setTokenizer(tokenizer.fit(data)) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer, entityExtractor)) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity) as result&quot;).show(false) ++ |result | ++ |[chunk, 6, 24, dolore magna aliqua, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 27, 48, Lorem ipsum dolor. sit, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 1], []]| |[chunk, 53, 59, laborum, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 2], []] | ++ Instantiated model of the TextMatcher. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: TextMatcherModel Scala API: TextMatcherModel Source: TextMatcherModel Token2Chunk Converts TOKEN type Annotations to CHUNK type. This can be useful if a entities have been already extracted as TOKEN and following annotators require CHUNK types. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: Token2Chunk Scala API: Token2Chunk Source: Token2Chunk Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) token2chunk = Token2Chunk() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;chunk&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, token2chunk ]) data = spark.createDataFrame([[&quot;One Two Three Four&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(truncate=False) ++ |result | ++ |[chunk, 0, 2, One, [sentence -&gt; 0], []] | |[chunk, 4, 6, Two, [sentence -&gt; 0], []] | |[chunk, 8, 12, Three, [sentence -&gt; 0], []]| |[chunk, 14, 17, Four, [sentence -&gt; 0], []]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.{Token2Chunk, Tokenizer} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val token2chunk = new Token2Chunk() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;chunk&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, token2chunk )) val data = Seq(&quot;One Two Three Four&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(false) ++ |result | ++ |[chunk, 0, 2, One, [sentence -&gt; 0], []] | |[chunk, 4, 6, Two, [sentence -&gt; 0], []] | |[chunk, 8, 12, Three, [sentence -&gt; 0], []]| |[chunk, 14, 17, Four, [sentence -&gt; 0], []]| ++ TokenAssembler This transformer reconstructs a DOCUMENT type annotation from tokens, usually after these have been normalized, lemmatized, normalized, spell checked, etc, in order to use this document annotation in further annotators. Requires DOCUMENT and TOKEN type annotations as input. For more extended examples on document pre-processing see the Examples. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: DOCUMENT Python API: TokenAssembler Scala API: TokenAssembler Source: TokenAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # First, the text is tokenized and cleaned documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) .setLowercase(False) stopwordsCleaner = StopWordsCleaner() .setInputCols([&quot;normalized&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) # Then the TokenAssembler turns the cleaned tokens into a `DOCUMENT` type structure. tokenAssembler = TokenAssembler() .setInputCols([&quot;sentences&quot;, &quot;cleanTokens&quot;]) .setOutputCol(&quot;cleanText&quot;) data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;]]) .toDF(&quot;text&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, normalizer, stopwordsCleaner, tokenAssembler ]).fit(data) result = pipeline.transform(data) result.select(&quot;cleanText&quot;).show(truncate=False) ++ |cleanText | ++ |0, 80, Spark NLP opensource text processing library advanced natural language processing, [sentence -&gt; 0], []| ++ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.{Normalizer, StopWordsCleaner} import com.johnsnowlabs.nlp.TokenAssembler import org.apache.spark.ml.Pipeline // First, the text is tokenized and cleaned val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) .setLowercase(false) val stopwordsCleaner = new StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) // Then the TokenAssembler turns the cleaned tokens into a `DOCUMENT` type structure. val tokenAssembler = new TokenAssembler() .setInputCols(&quot;sentences&quot;, &quot;cleanTokens&quot;) .setOutputCol(&quot;cleanText&quot;) val data = Seq(&quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;) .toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, normalizer, stopwordsCleaner, tokenAssembler )).fit(data) val result = pipeline.transform(data) result.select(&quot;cleanText&quot;).show(false) ++ |cleanText | ++ |[[document, 0, 80, Spark NLP opensource text processing library advanced natural language processing, [sentence -&gt; 0], []]]| ++ Tokenizer ApproachModel Tokenizes raw text in document type columns into TokenizedSentence . This class represents a non fitted tokenizer. Fitting it will cause the internal RuleFactory to construct the rules for tokenizing from the input configuration. Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. For extended examples of usage see the Examples and Tokenizer test class Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Note: All these APIs receive regular expressions so please make sure that you escape special characters according to Java conventions. Python API: Tokenizer Scala API: Tokenizer Source: Tokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline data = spark.createDataFrame([[&quot;I&#39;d like to say we didn&#39;t expect that. Jane&#39;s boyfriend.&quot;]]).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) tokenizer = Tokenizer().setInputCols([&quot;document&quot;]).setOutputCol(&quot;token&quot;).fit(data) pipeline = Pipeline().setStages([documentAssembler, tokenizer]).fit(data) result = pipeline.transform(data) result.selectExpr(&quot;token.result&quot;).show(truncate=False) +--+ |output | +--+ |[I&#39;d, like, to, say, we, didn&#39;t, expect, that, ., Jane&#39;s, boyfriend, .]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import org.apache.spark.ml.Pipeline val data = Seq(&quot;I&#39;d like to say we didn&#39;t expect that. Jane&#39;s boyfriend.&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer().setInputCols(&quot;document&quot;).setOutputCol(&quot;token&quot;).fit(data) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer)).fit(data) val result = pipeline.transform(data) result.selectExpr(&quot;token.result&quot;).show(false) +--+ |output | +--+ |[I&#39;d, like, to, say, we, didn&#39;t, expect, that, ., Jane&#39;s, boyfriend, .]| +--+ Tokenizes raw text into word pieces, tokens. Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. This class represents an already fitted Tokenizer model. See the main class Tokenizer for more examples of usage. Input Annotator Types: DOCUMENT //A Tokenizer could require only for now a SentenceDetector annotator Output Annotator Type: TOKEN Python API: TokenizerModel Scala API: TokenizerModel Source: TokenizerModel TypedDependencyParser ApproachModel Labeled parser that finds a grammatical relation between two words in a sentence. Its input is either a CoNLL2009 or ConllU dataset. For instantiated/pretrained models, see TypedDependencyParserModel. Dependency parsers provide information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The parser requires the dependant tokens beforehand with e.g. DependencyParser. The required training data can be set in two different ways (only one can be chosen for a particular model): Dataset in the CoNLL 2009 format set with setConll2009 Dataset in the CoNLL-U format set with setConllU Apart from that, no additional training data is needed. See TypedDependencyParserApproachTestSpec for further reference on this API. Input Annotator Types: TOKEN, POS, DEPENDENCY Output Annotator Type: LABELED_DEPENDENCY Python API: TypedDependencyParserApproach Scala API: TypedDependencyParserApproach Source: TypedDependencyParserApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) posTagger = PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) dependencyParser = DependencyParserModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency&quot;) typedDependencyParser = TypedDependencyParserApproach() .setInputCols([&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency_type&quot;) .setConllU(&quot;src/test/resources/parser/labeled/train_small.conllu.txt&quot;) .setNumberOfIterations(1) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, posTagger, dependencyParser, typedDependencyParser ]) # Additional training data is not needed, the dependency parser relies on CoNLL-U only. emptyDataSet = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(emptyDataSet) import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel import com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val dependencyParser = DependencyParserModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) val typedDependencyParser = new TypedDependencyParserApproach() .setInputCols(&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency_type&quot;) .setConllU(&quot;src/test/resources/parser/labeled/train_small.conllu.txt&quot;) .setNumberOfIterations(1) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, posTagger, dependencyParser, typedDependencyParser )) // Additional training data is not needed, the dependency parser relies on CoNLL-U only. val emptyDataSet = Seq.empty[String].toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(emptyDataSet) Labeled parser that finds a grammatical relation between two words in a sentence. Its input is either a CoNLL2009 or ConllU dataset. Dependency parsers provide information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The parser requires the dependant tokens beforehand with e.g. DependencyParser. Pretrained models can be loaded with pretrained of the companion object: val typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols(&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency_type&quot;) The default model is &quot;dependency_typed_conllu&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the TypedDependencyModelTestSpec. Input Annotator Types: TOKEN, POS, DEPENDENCY Output Annotator Type: LABELED_DEPENDENCY Python API: TypedDependencyParserModel Scala API: TypedDependencyParserModel Source: TypedDependencyParserModel ViveknSentiment ApproachModel Trains a sentiment analyser inspired by the algorithm by Vivek Narayanan https://github.com/vivekn/sentiment/. The algorithm is based on the paper “Fast and accurate sentiment classification using an enhanced Naive Bayes model”. The analyzer requires sentence boundaries to give a score in context. Tokenization is needed to make sure tokens are within bounds. Transitivity requirements are also required. The training data needs to consist of a column for normalized text and a label column (either &quot;positive&quot; or &quot;negative&quot;). For extended examples of usage, see the Examples and the ViveknSentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: ViveknSentimentApproach Scala API: ViveknSentimentApproach Source: ViveknSentimentApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) token = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normal&quot;) vivekn = ViveknSentimentApproach() .setInputCols([&quot;document&quot;, &quot;normal&quot;]) .setSentimentCol(&quot;train_sentiment&quot;) .setOutputCol(&quot;result_sentiment&quot;) finisher = Finisher() .setInputCols([&quot;result_sentiment&quot;]) .setOutputCols(&quot;final_sentiment&quot;) pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher]) training = spark.createDataFrame([ (&quot;I really liked this movie!&quot;, &quot;positive&quot;), (&quot;The cast was horrible&quot;, &quot;negative&quot;), (&quot;Never going to watch this again or recommend it to anyone&quot;, &quot;negative&quot;), (&quot;It&#39;s a waste of time&quot;, &quot;negative&quot;), (&quot;I loved the protagonist&quot;, &quot;positive&quot;), (&quot;The music was really really good&quot;, &quot;positive&quot;) ]).toDF(&quot;text&quot;, &quot;train_sentiment&quot;) pipelineModel = pipeline.fit(training) data = spark.createDataFrame([ [&quot;I recommend this movie&quot;], [&quot;Dont waste your time!!!&quot;] ]).toDF(&quot;text&quot;) result = pipelineModel.transform(data) result.select(&quot;final_sentiment&quot;).show(truncate=False) ++ |final_sentiment| ++ |[positive] | |[negative] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.Normalizer import com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentApproach import com.johnsnowlabs.nlp.Finisher import org.apache.spark.ml.Pipeline val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val token = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normal&quot;) val vivekn = new ViveknSentimentApproach() .setInputCols(&quot;document&quot;, &quot;normal&quot;) .setSentimentCol(&quot;train_sentiment&quot;) .setOutputCol(&quot;result_sentiment&quot;) val finisher = new Finisher() .setInputCols(&quot;result_sentiment&quot;) .setOutputCols(&quot;final_sentiment&quot;) val pipeline = new Pipeline().setStages(Array(document, token, normalizer, vivekn, finisher)) val training = Seq( (&quot;I really liked this movie!&quot;, &quot;positive&quot;), (&quot;The cast was horrible&quot;, &quot;negative&quot;), (&quot;Never going to watch this again or recommend it to anyone&quot;, &quot;negative&quot;), (&quot;It&#39;s a waste of time&quot;, &quot;negative&quot;), (&quot;I loved the protagonist&quot;, &quot;positive&quot;), (&quot;The music was really really good&quot;, &quot;positive&quot;) ).toDF(&quot;text&quot;, &quot;train_sentiment&quot;) val pipelineModel = pipeline.fit(training) val data = Seq( &quot;I recommend this movie&quot;, &quot;Dont waste your time!!!&quot; ).toDF(&quot;text&quot;) val result = pipelineModel.transform(data) result.select(&quot;final_sentiment&quot;).show(false) ++ |final_sentiment| ++ |[positive] | |[negative] | ++ Sentiment analyser inspired by the algorithm by Vivek Narayanan https://github.com/vivekn/sentiment/. The algorithm is based on the paper “Fast and accurate sentiment classification using an enhanced Naive Bayes model”. This is the instantiated model of the ViveknSentimentApproach. For training your own model, please see the documentation of that class. The analyzer requires sentence boundaries to give a score in context. Tokenization is needed to make sure tokens are within bounds. Transitivity requirements are also required. For extended examples of usage, see the Examples and the ViveknSentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: ViveknSentimentModel Scala API: ViveknSentimentModel Source: ViveknSentimentModel Word2Vec ApproachModel Trains a Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. For instantiated/pretrained models, see Word2VecModel. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: Word2VecApproach Scala API: Word2VecApproach Source: Word2VecApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Word2VecApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings ]) path = &quot;sherlockholmes.txt&quot; dataset = spark.read.text(path).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(dataset) import spark.implicits._ import com.johnsnowlabs.nlp.annotator.{Tokenizer, Word2VecApproach} import com.johnsnowlabs.nlp.base.DocumentAssembler import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = new Word2VecApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings )) val path = &quot;src/test/resources/spell/sherlockholmes.txt&quot; val dataset = spark.sparkContext.textFile(path) .toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(dataset) Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. This is the instantiated model of the Word2VecApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val embeddings = Word2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;word2vec_gigaword_300&quot;, if no name is provided. For available pretrained models please see the Models Hub. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: Word2VecModel Scala API: Word2VecModel Source: Word2VecModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Word2VecModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, embeddings, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Tokenizer, Word2VecModel} import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = Word2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsFinisher )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ WordEmbeddings ApproachModel Word Embeddings lookup annotator that maps tokens to vectors. For instantiated/pretrained models, see WordEmbeddingsModel. A custom token lookup dictionary for embeddings can be set with setStoragePath. Each line of the provided file needs to have a token, followed by their vector representation, delimited by a spaces. ... are 0.39658191506190343 0.630968081620067 0.5393722253731201 0.8428180123359783 were 0.7535235923631415 0.9699218875629833 0.10397182122983872 0.11833962569383116 stress 0.0492683418305907 0.9415954572751959 0.47624463167525755 0.16790967216778263 induced 0.1535748762292387 0.33498936903209897 0.9235178224122094 0.1158772920395934 ... If a token is not found in the dictionary, then the result will be a zero vector of the same dimension. Statistics about the rate of converted tokens, can be retrieved with[WordEmbeddingsModel.withCoverageColumn and WordEmbeddingsModel.overallCoverage. For extended examples of usage, see the Examples and the WordEmbeddingsTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: WordEmbeddings Scala API: WordEmbeddings Source: WordEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the file `random_embeddings_dim4.txt` has the form of the content above. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddings() .setStoragePath(&quot;src/test/resources/random_embeddings_dim4.txt&quot;, ReadAs.TEXT) .setStorageRef(&quot;glove_4d&quot;) .setDimension(4) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) .setCleanAnnotations(False) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;The patient was diagnosed with diabetes.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(truncate=False) +-+ |result | +-+ |[0.9439099431037903,0.4707513153553009,0.806300163269043,0.16176554560661316] | |[0.7966810464859009,0.5551124811172485,0.8861005902290344,0.28284206986427307] | |[0.025029370561242104,0.35177749395370483,0.052506182342767715,0.1887107789516449]| |[0.08617766946554184,0.8399239182472229,0.5395117998123169,0.7864698767662048] | |[0.6599600911140442,0.16109347343444824,0.6041093468666077,0.8913561105728149] | |[0.5955275893211365,0.01899011991918087,0.4397728443145752,0.8911281824111938] | |[0.9840458631515503,0.7599489092826843,0.9417727589607239,0.8624503016471863] | +-+ // In this example, the file `random_embeddings_dim4.txt` has the form of the content above. import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.embeddings.WordEmbeddings import com.johnsnowlabs.nlp.util.io.ReadAs import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = new WordEmbeddings() .setStoragePath(&quot;src/test/resources/random_embeddings_dim4.txt&quot;, ReadAs.TEXT) .setStorageRef(&quot;glove_4d&quot;) .setDimension(4) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsFinisher )) val data = Seq(&quot;The patient was diagnosed with diabetes.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(false) +-+ |result | +-+ |[0.9439099431037903,0.4707513153553009,0.806300163269043,0.16176554560661316] | |[0.7966810464859009,0.5551124811172485,0.8861005902290344,0.28284206986427307] | |[0.025029370561242104,0.35177749395370483,0.052506182342767715,0.1887107789516449]| |[0.08617766946554184,0.8399239182472229,0.5395117998123169,0.7864698767662048] | |[0.6599600911140442,0.16109347343444824,0.6041093468666077,0.8913561105728149] | |[0.5955275893211365,0.01899011991918087,0.4397728443145752,0.8911281824111938] | |[0.9840458631515503,0.7599489092826843,0.9417727589607239,0.8624503016471863] | +-+ Word Embeddings lookup annotator that maps tokens to vectors This is the instantiated model of WordEmbeddings. Pretrained models can be loaded with pretrained of the companion object: val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;glove_100d&quot;, if no name is provided. For available pretrained models please see the Models Hub. There are also two convenient functions to retrieve the embeddings coverage with respect to the transformed dataset: withCoverageColumn(dataset, embeddingsCol, outputCol): Adds a custom column with word coverage stats for the embedded field: (coveredWords, totalWords, coveragePercentage). This creates a new column with statistics for each row. val wordsCoverage = WordEmbeddingsModel.withCoverageColumn(resultDF, &quot;embeddings&quot;, &quot;cov_embeddings&quot;) wordsCoverage.select(&quot;text&quot;,&quot;cov_embeddings&quot;).show(false) +-+--+ |text |cov_embeddings| +-+--+ |This is a sentence.|[5, 5, 1.0] | +-+--+ overallCoverage(dataset, embeddingsCol): Calculates overall word coverage for the whole data in the embedded field. This returns a single coverage object considering all rows in the field. val wordsOverallCoverage = WordEmbeddingsModel.overallCoverage(wordsCoverage,&quot;embeddings&quot;).percentage 1.0 For extended examples of usage, see the Examples and the WordEmbeddingsTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: WordEmbeddingsModel Scala API: WordEmbeddingsModel Source: WordEmbeddingsModel WordSegmenter ApproachModel Trains a WordSegmenter which tokenizes non-english or non-whitespace separated texts. Many languages are not whitespace separated and their sentences are a concatenation of many symbols, like Korean, Japanese or Chinese. Without understanding the language, splitting the words into their corresponding tokens is impossible. The WordSegmenter is trained to understand these languages and split them into semantically correct parts. This annotator is based on the paper Chinese Word Segmentation as Character Tagging [1]. Word segmentation is treated as a tagging problem. Each character is be tagged as on of four different labels: LL (left boundary), RR (right boundary), MM (middle) and LR (word by itself). The label depends on the position of the word in the sentence. LL tagged words will combine with the word on the right. Likewise, RR tagged words combine with words on the left. MM tagged words are treated as the middle of the word and combine with either side. LR tagged words are words by themselves. Example (from [1], Example 3(a) (raw), 3(b) (tagged), 3(c) (translation)): 上海 计划 到 本 世纪 末 实现 人均 国内 生产 总值 五千 美元 上/LL 海/RR 计/LL 划/RR 到/LR 本/LR 世/LL 纪/RR 末/LR 实/LL 现/RR 人/LL 均/RR 国/LL 内/RR 生/LL 产/RR 总/LL 值/RR 五/LL 千/RR 美/LL 元/RR Shanghai plans to reach the goal of 5,000 dollars in per capita GDP by the end of the century. For instantiated/pretrained models, see WordSegmenterModel. To train your own model, a training dataset consisting of Part-Of-Speech tags is required. The data has to be loaded into a dataframe, where the column is an Annotation of type &quot;POS&quot;. This can be set with setPosColumn. Tip: The helper class POS might be useful to read training data into data frames. For extended examples of usage, see the Examples and the WordSegmenterTest. References: [1] Xue, Nianwen. “Chinese Word Segmentation as Character Tagging.” International Journal of Computational Linguistics &amp; Chinese Language Processing, Volume 8, Number 1, February 2003: Special Issue on Word Formation and Chinese Language Processing, 2003, pp. 29-48. ACLWeb, https://aclanthology.org/O03-4002. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: WordSegmenterApproach Scala API: WordSegmenterApproach Source: WordSegmenterApproach Show Example PythonScala # In this example, `&quot;chinese_train.utf8&quot;` is in the form of # # 十|LL 四|RR 不|LL 是|RR 四|LL 十|RR # # and is loaded with the `POS` class to create a dataframe of `&quot;POS&quot;` type Annotations. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) wordSegmenter = WordSegmenterApproach() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) .setPosColumn(&quot;tags&quot;) .setNIterations(5) pipeline = Pipeline().setStages([ documentAssembler, wordSegmenter ]) trainingDataSet = POS().readDataset( spark, &quot;src/test/resources/word-segmenter/chinese_train.utf8&quot; ) pipelineModel = pipeline.fit(trainingDataSet) // In this example, `&quot;chinese_train.utf8&quot;` is in the form of // // 十|LL 四|RR 不|LL 是|RR 四|LL 十|RR // // and is loaded with the `POS` class to create a dataframe of `&quot;POS&quot;` type Annotations. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.ws.WordSegmenterApproach import com.johnsnowlabs.nlp.training.POS import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val wordSegmenter = new WordSegmenterApproach() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) .setPosColumn(&quot;tags&quot;) .setNIterations(5) val pipeline = new Pipeline().setStages(Array( documentAssembler, wordSegmenter )) val trainingDataSet = POS().readDataset( ResourceHelper.spark, &quot;src/test/resources/word-segmenter/chinese_train.utf8&quot; ) val pipelineModel = pipeline.fit(trainingDataSet) WordSegmenter which tokenizes non-english or non-whitespace separated texts. Many languages are not whitespace separated and their sentences are a concatenation of many symbols, like Korean, Japanese or Chinese. Without understanding the language, splitting the words into their corresponding tokens is impossible. The WordSegmenter is trained to understand these languages and plit them into semantically correct parts. This annotator is based on the paper Chinese Word Segmentation as Character Tagging. Word segmentation is treated as a tagging problem. Each character is be tagged as on of four different labels: LL (left boundary), RR (right boundary), MM (middle) and LR (word by itself). The label depends on the position of the word in the sentence. LL tagged words will combine with the word on the right. Likewise, RR tagged words combine with words on the left. MM tagged words are treated as the middle of the word and combine with either side. LR tagged words are words by themselves. Example (from [1], Example 3(a) (raw), 3(b) (tagged), 3(c) (translation)): 上海 计划 到 本 世纪 末 实现 人均 国内 生产 总值 五千 美元 上/LL 海/RR 计/LL 划/RR 到/LR 本/LR 世/LL 纪/RR 末/LR 实/LL 现/RR 人/LL 均/RR 国/LL 内/RR 生/LL 产/RR 总/LL 值/RR 五/LL 千/RR 美/LL 元/RR Shanghai plans to reach the goal of 5,000 dollars in per capita GDP by the end of the century. This is the instantiated model of the WordSegmenterApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val wordSegmenter = WordSegmenterModel.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;words_segmented&quot;) The default model is &quot;wordseg_pku&quot;, default language is &quot;zh&quot;, if no values are provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the WordSegmenterTest. References: [1] Xue, Nianwen. “Chinese Word Segmentation as Character Tagging.” International Journal of Computational Linguistics &amp; Chinese Language Processing, Volume 8, Number 1, February 2003: Special Issue on Word Formation and Chinese Language Processing, 2003, pp. 29-48. ACLWeb, https://aclanthology.org/O03-4002. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: WordSegmenterModel Scala API: WordSegmenterModel Source: WordSegmenterModel YakeKeywordExtraction Yake is an Unsupervised, Corpus-Independent, Domain and Language-Independent and Single-Document keyword extraction algorithm. Extracting keywords from texts has become a challenge for individuals and organizations as the information grows in complexity and size. The need to automate this task so that text can be processed in a timely and adequate manner has led to the emergence of automatic keyword extraction tools. Yake is a novel feature-based system for multi-lingual keyword extraction, which supports texts of different sizes, domain or languages. Unlike other approaches, Yake does not rely on dictionaries nor thesauri, neither is trained against any corpora. Instead, it follows an unsupervised approach which builds upon features extracted from the text, making it thus applicable to documents written in different languages without the need for further knowledge. This can be beneficial for a large number of tasks and a plethora of situations where access to training corpora is either limited or restricted. The algorithm makes use of the position of a sentence and token. Therefore, to use the annotator, the text should be first sent through a Sentence Boundary Detector and then a tokenizer. Note that each keyword will be given a keyword score greater than 0 (The lower the score better the keyword). Therefore to filter the keywords, an upper bound for the score can be set with setThreshold. For extended examples of usage, see the Examples and the YakeTestSpec. Sources : Campos, R., Mangaravite, V., Pasquali, A., Jatowt, A., Jorge, A., Nunes, C. and Jatowt, A. (2020). YAKE! Keyword Extraction from Single Documents using Multiple Local Features. In Information Sciences Journal. Elsevier, Vol 509, pp 257-289 Paper abstract: As the amount of generated information grows, reading and summarizing texts of large collections turns into a challenging task. Many documents do not come with descriptive terms, thus requiring humans to generate keywords on-the-fly. The need to automate this kind of task demands the development of keyword extraction systems with the ability to automatically identify keywords within the text. One approach is to resort to machine-learning algorithms. These, however, depend on large annotated text corpora, which are not always available. An alternative solution is to consider an unsupervised approach. In this article, we describe YAKE!, a light-weight unsupervised automatic keyword extraction method which rests on statistical text features extracted from single documents to select the most relevant keywords of a text. Our system does not need to be trained on a particular set of documents, nor does it depend on dictionaries, external corpora, text size, language, or domain. To demonstrate the merits and significance of YAKE!, we compare it against ten state-of-the-art unsupervised approaches and one supervised method. Experimental results carried out on top of twenty datasets show that YAKE! significantly outperforms other unsupervised methods on texts of different sizes, languages, and domains. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: YakeKeywordExtraction Scala API: YakeKeywordExtraction Source: YakeKeywordExtraction Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) token = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) .setContextChars([&quot;(&quot;, &quot;]&quot;, &quot;?&quot;, &quot;!&quot;, &quot;.&quot;, &quot;,&quot;]) keywords = YakeKeywordExtraction() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;keywords&quot;) .setThreshold(0.6) .setMinNGrams(2) .setNKeywords(10) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, token, keywords ]) data = spark.createDataFrame([[ &quot;Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud Next conference in San Francisco this week, the official announcement could come as early as tomorrow. Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. Google itself declined &#39;to comment on rumors&#39;. Kaggle, which has about half a million data scientists on its platform, was founded by Goldbloom and Ben Hamner in 2010. The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, it has managed to stay well ahead of them by focusing on its specific niche. The service is basically the de facto home for running data science and machine learning competitions. With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow and other projects). Kaggle has a bit of a history with Google, too, but that&#39;s pretty recent. Earlier this month, Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google will keep the service running - likely under its current name. While the acquisition is probably more about Kaggle&#39;s community than technology, Kaggle did build some interesting tools for hosting its competition and &#39;kernels&#39;, too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can share this code on the platform (the company previously called them &#39;scripts&#39;). Like similar competition-centric sites, Kaggle also runs a job board, too. It&#39;s unclear what Google will do with that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it&#39;s $12.75) since its launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, NaRavikant, Google chie economist Hal Varian, Khosla Ventures and Yuri Milner&quot; ]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) # combine the result and score (contained in keywords.metadata) scores = result .selectExpr(&quot;explode(arrays_zip(keywords.result, keywords.metadata)) as resultTuples&quot;) .selectExpr(&quot;resultTuples[&#39;0&#39;] as keyword&quot;, &quot;resultTuples[&#39;1&#39;].score as score&quot;) # Order ascending, as lower scores means higher importance scores.orderBy(&quot;score&quot;).show(5, truncate = False) ++-+ |keyword |score | ++-+ |google cloud |0.32051516486864573| |google cloud platform|0.37786450577630676| |ceo anthony goldbloom|0.39922830978423146| |san francisco |0.40224744669493756| |anthony goldbloom |0.41584827825302534| ++-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{SentenceDetector, Tokenizer} import com.johnsnowlabs.nlp.annotators.keyword.yake.YakeKeywordExtraction import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val token = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) .setContextChars(Array(&quot;(&quot;, &quot;)&quot;, &quot;?&quot;, &quot;!&quot;, &quot;.&quot;, &quot;,&quot;)) val keywords = new YakeKeywordExtraction() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;keywords&quot;) .setThreshold(0.6f) .setMinNGrams(2) .setNKeywords(10) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, token, keywords )) val data = Seq( &quot;Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud Next conference in San Francisco this week, the official announcement could come as early as tomorrow. Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. Google itself declined &#39;to comment on rumors&#39;. Kaggle, which has about half a million data scientists on its platform, was founded by Goldbloom and Ben Hamner in 2010. The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, it has managed to stay well ahead of them by focusing on its specific niche. The service is basically the de facto home for running data science and machine learning competitions. With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow and other projects). Kaggle has a bit of a history with Google, too, but that&#39;s pretty recent. Earlier this month, Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google will keep the service running - likely under its current name. While the acquisition is probably more about Kaggle&#39;s community than technology, Kaggle did build some interesting tools for hosting its competition and &#39;kernels&#39;, too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can share this code on the platform (the company previously called them &#39;scripts&#39;). Like similar competition-centric sites, Kaggle also runs a job board, too. It&#39;s unclear what Google will do with that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it&#39;s $12.75) since its launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, Naval Ravikant, Google chief economist Hal Varian, Khosla Ventures and Yuri Milner&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) // combine the result and score (contained in keywords.metadata) val scores = result .selectExpr(&quot;explode(arrays_zip(keywords.result, keywords.metadata)) as resultTuples&quot;) .select($&quot;resultTuples.0&quot; as &quot;keyword&quot;, $&quot;resultTuples.1.score&quot;) // Order ascending, as lower scores means higher importance scores.orderBy(&quot;score&quot;).show(5, truncate = false) ++-+ |keyword |score | ++-+ |google cloud |0.32051516486864573| |google cloud platform|0.37786450577630676| |ceo anthony goldbloom|0.39922830978423146| |san francisco |0.40224744669493756| |anthony goldbloom |0.41584827825302534| ++-+",
    "url": "/docs/en/annotators",
    "relUrl": "/docs/en/annotators"
  },
  "14": {
    "id": "14",
    "title": "API Integration",
    "content": "All features provided by the Annotation Lab via UI are also accessible via API. The complete API documentation is available on the SWAGGER page of the Annotation Lab. It is available under Settings &gt; API Integration. Concrete query examples are provided for each available endpoint. Example of creating a new project via API Get Client Secret Get CLIENT_ID and CLIENT_SECRET by following the steps illustrated in the video. Annotation Lab: Collect the Client Secret Call API endpoint For creating a new project via API you can use the following python script. import requests import json # URL to Annotation Lab API_URL = &quot;https://123.45.67.89&quot; # Add user credentials USERNAME = &quot;user&quot; PASSWORD = &quot;password&quot; # The above video shows how to get CLIENT_ID and CLIENT_SECRET CLIENT_ID = &quot;...&quot; CLIENT_SECRET = &quot;...&quot; PROJECT_NAME = &quot;sample_project&quot; IDENTITY_MANAGEMENT_URL = API_URL + &quot;/auth/&quot; IDENTITY_MANAGEMENT_REALM = &quot;master&quot; HEADERS = { &quot;Host&quot;: API_URL.replace(&quot;http://&quot;, &quot;&quot;).replace(&quot;https://&quot;, &quot;&quot;), &quot;Origin&quot;: API_URL, &quot;Content-Type&quot;: &quot;application/json&quot;, } def get_cookies(): url = f&quot;{IDENTITY_MANAGEMENT_URL}realms/{IDENTITY_MANAGEMENT_REALM}/protocol/openid-connect/token&quot; data = { &quot;grant_type&quot;: &quot;password&quot;, &quot;username&quot;: USERNAME, &quot;password&quot;: PASSWORD, &quot;client_id&quot;: CLIENT_ID, &quot;client_secret&quot;: CLIENT_SECRET, } auth_info = requests.post(url, data=data).json() cookies = { &quot;access_token&quot;: f&quot;Bearer {auth_info[&#39;access_token&#39;]}&quot;, &quot;refresh_token&quot;: auth_info[&quot;refresh_token&quot;], } return cookies def create_project(): # GET THIS FROM SWAGGER DOC url = f&quot;{API_URL}/api/projects/create&quot; data = { &quot;project_name&quot;: PROJECT_NAME, &quot;project_description&quot;: &quot;&quot;, &quot;project_sampling&quot;: &quot;uniform&quot;, &quot;project_instruction&quot;: &quot;&quot;, } r = requests.post( url, headers=HEADERS, data=json.dumps(data), cookies=get_cookies() ) return r create_project()",
    "url": "/docs/en/alab/api",
    "relUrl": "/docs/en/alab/api"
  },
  "15": {
    "id": "15",
    "title": "Audit Trail",
    "content": "Annotation Lab is designed to handle Personal Identifying Information (PII) and Protected Health Information (PHI). It keeps a full audit trail for all created completions, where each entry is stored with an authenticated user and a timestamp. It is not possible for Annotators or Reviewers to delete any completions, and only Managers and Project Owners can remove tasks.",
    "url": "/docs/en/alab/audit_trail",
    "relUrl": "/docs/en/alab/audit_trail"
  },
  "16": {
    "id": "16",
    "title": "Helper functions",
    "content": "Spark NLP Annotation functions The functions presented here help users manipulate annotations, by providing both UDFs and dataframe utilities to deal with them more easily Python In python, the functions are straight forward and have both UDF and Dataframe applications map_annotations(f, output_type: DataType) UDF that applies f(). Requires output DataType from pyspark.sql.types map_annotations_strict(f) UDF that apples an f() method that returns a list of Annotations map_annotations_col(dataframe: DataFrame, f, column: str, output_column: str, annotatyon_type: str, output_type: DataType = Annotation.arrayType()) applies f() to column from dataframe map_annotations_cols(dataframe: DataFrame, f, columns: str, output_column: str, annotatyon_type: str, output_type: DataType = Annotation.arrayType()) applies f() to columns from dataframe filter_by_annotations_col(dataframe, f, column) applies a boolean filter f() to column from dataframe explode_annotations_col(dataframe: DataFrame, column, output_column) explodes annotation column from dataframe Scala In Scala, importing inner functions brings implicits that allow these functions to be applied directly on top of the dataframe mapAnnotations(function: Seq[Annotation] =&gt; T, outputType: DataType) mapAnnotationsStrict(function: Seq[Annotation] =&gt; Seq[Annotation]) mapAnnotationsCol[T: TypeTag](column: String, outputCol: String,annotatorType: String, function: Seq[Annotation] =&gt; T) mapAnnotationsCol[T: TypeTag](cols: Seq[String], outputCol: String,annotatorType: String, function: Seq[Annotation] =&gt; T) eachAnnotationsCol[T: TypeTag](column: String, function: Seq[Annotation] =&gt; Unit) def explodeAnnotationsCol[T: TypeTag](column: String, outputCol: String) Imports: PythonScala from sparknlp.functions import * from sparknlp.annotation import Annotation import com.johnsnowlabs.nlp.functions._ import com.johnsnowlabs.nlp.Annotation Examples: Complete usage examples can be seen here: https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/234-release-candidate/jupyter/annotation/english/spark-nlp-basics/spark-nlp-basics-functions.ipynb PythonScala def my_annoation_map_function(annotations): return list(map(lambda a: Annotation( &#39;my_own_type&#39;, a.begin, a.end, a.result, {&#39;my_key&#39;: &#39;custom_annotation_data&#39;}, []), annotations)) result.select( map_annotations(my_annoation_map_function, Annotation.arrayType())(&#39;token&#39;) ).toDF(&quot;my output&quot;).show(truncate=False) val modified = data.mapAnnotationsCol(&quot;pos&quot;, &quot;mod_pos&quot;,&quot;pos&quot; ,(_: Seq[Annotation]) =&gt; { &quot;hello world&quot; })",
    "url": "/docs/en/auxiliary",
    "relUrl": "/docs/en/auxiliary"
  },
  "17": {
    "id": "17",
    "title": "Backup and Restore",
    "content": "Backup You can enable daily backups by adding several variables with --set option to helm command in annotationlab-updater.sh: backup.enable=true backup.files=true backup.s3_access_key=&quot;&lt;ACCESS_KEY&gt;&quot; backup.s3_secret_key=&quot;&lt;SECRET_KEY&gt;&quot; backup.s3_bucket_fullpath=&quot;&lt;FULL_PATH&gt;&quot; &lt;ACCESS_KEY&gt; - your access key for AWS S3 access &lt;SECRET_KEY&gt; - your secret key for AWS S3 access &lt;FULL_PATH&gt; - full path to your backup in s3 bucket (f.e. s3://example.com/path/to/my/backup/dir) Note: File backup is enabled by default. If you don’t need to backup files, you have to change backup.files=true to backup.files=false Configure Backup from the UI In 2.8.0 release, Annotation Lab added support for defining database and files backups via the UI. An admin user can view and edit the backup settings under the Settings menu. Users can select different backup periods and can specify a target S3 bucket for storing the backup files. New backups will be automatically generated and saved to the S3 bucket following the defined schedule. Restore Database To restore Annotation Lab from a backup you need a fresh installation of Annotation Lab. Install it using annotationlab-install.sh. Now, download the latest backup from your S3 bucket and move the archive to restore/database/ directory. Next, go to the restore/database/ directory and execute script restore_all_databases.sh with the name of your backup archive as the argument. For example: cd restore/database/ sudo ./restore_all_databases.sh 2022-04-14-annotationlab-all-databases.tar.xz Note: You need xz and bash installed to execute this script. This script works only with backups created by Annotation Lab backup system. Run this script with sudo command After database restore complete you can check logs in restore_log directory created by restore script. Files Download your files backup and move it to restore/files/ directory. Go to restore/files/ directory and execute script restore_files.sh with the name of your backup archive as the argument. For example: cd restore/files/ sudo ./restore_files.sh 2022-04-14-annotationlab-files.tar Note: You need bash installed to execute this script. This script works only with backups created by Annotation Lab backup system. Run this script with sudo command Reboot After restoring database and files, reboot Annotation Lab: sudo reboot",
    "url": "/docs/en/alab/backup_restore",
    "relUrl": "/docs/en/alab/backup_restore"
  },
  "18": {
    "id": "18",
    "title": "Developers Guideline",
    "content": "Cluster Speed Benchmarks NER (BiLSTM-CNN-Char Architecture) Benchmark Experiment Dataset : 1000 Clinical Texts from MTSamples Oncology Dataset, approx. 500 tokens per text. Driver : Standard_D4s_v3 - 16 GB Memory - 4 Cores Enable Autoscaling : False Cluster Mode : Standart Worker : Standard_D4s_v3 - 16 GB Memory - 4 Cores Standard_D4s_v2 - 28 GB Memory - 8 Cores Versions : Databricks Runtime Version : 8.3(Scala 2.12, Spark 3.1.1) spark-nlp Version: v3.2.3 spark-nlp-jsl Version : v3.2.3 Spark Version : v3.1.1 Spark NLP Pipeline : nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings_clinical, clinical_ner, ner_converter ]) NOTES : The first experiment with 5 different cluster configurations : ner_chunk as a column in Spark NLP Pipeline (ner_converter) output data frame, exploded (lazy evaluation) as ner_chunk and ner_label. Then results were written as parquet and delta formats. A second experiment with 2 different cluster configuration : Spark NLP Pipeline output data frame (except word_embeddings column) was written as parquet and delta formats. In the first experiment with the most basic driver node and worker (1 worker x 4 cores) configuration selection, it took 4.64 mins and 4.53 mins to write 4 partitioned data as parquet and delta formats respectively. With basic driver node and 8 workers (x8 cores) configuration selection, it took 40 seconds and 22 seconds to write 1000 partitioned data as parquet and delta formats respectively. In the second experiment with basic driver node and 4 workers (x 4 cores) configuration selection, it took 1.41 mins as parquet and 1.42 mins as delta format to write 16 partitioned (exploded results) data. Without explode it took 1.08 mins as parquet and 1.12 mins as delta format to write the data frame. Since given computation durations are highly dependent on different parameters including driver node and worker node configurations as well as partitions, results show that explode method increases duration %10-30 on chosen configurations. NER Benchmark Tables driver_name driver_memory driver_cores worker_name worker_memory worker_cores input_data_rows output_data_rows action total_worker_number total_cores partition NER timing NER+RE timing Standard_D4s_v3 16 GB 4 Standard_D4s_v2 28 GB 8 1000 78000 write_parquet 8 64 64 36 sec 1.14 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v2 28 GB 8 1000 78000 write_deltalake 8 64 64 19 sec 1.13 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v2 28 GB 8 1000 78000 write_parquet 8 64 100 21 sec 50 sec Standard_D4s_v3 16 GB 4 Standard_D4s_v2 28 GB 8 1000 78000 write_deltalake 8 64 100 41 sec 51 sec Standard_D4s_v3 16 GB 4 Standard_D4s_v2 28 GB 8 1000 78000 write_parquet 8 64 1000 40 sec 54 sec Standard_D4s_v3 16 GB 4 Standard_D4s_v2 28 GB 8 1000 78000 write_deltalake 8 64 1000 22 sec 46 sec driver_name driver_memory driver_cores worker_name worker_memory worker_cores input_data_rows output_data_rows action total_worker_number total_cores partition duration NER+RE timing Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 8 32 32 1.21 mins 2.05 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 8 32 32 55.8 sec 1.91 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 8 32 100 41 sec 1.64 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 8 32 100 48 sec 1.61 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 8 32 1000 1.36 min 1.83 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 8 32 1000 48 sec 1.70 mins driver_name driver_memory driver_cores worker_name worker_memory worker_cores input_data_rows output_data_rows action total_worker_number total_cores partition NER timing NER+RE timing Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 4 16 10 1.4 mins 3.78 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 4 16 10 1.76 mins 3.93 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 4 16 16 1.41 mins 3.97 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 4 16 16 1.42 mins 3.82 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 4 16 32 1.36 mins 3.70 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 4 16 32 1.35 mins 3.65 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 4 16 100 1.21 mins 3.18 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 4 16 100 1.24 mins 3.15 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 4 16 1000 1.42 mins 3.51 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 4 16 1000 1.46 mins 3.48 mins driver_name driver_memory driver_cores worker_name worker_memory worker_cores input_data_rows output_data_rows action total_worker_number total_cores partition NER timing NER+RE timing Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 2 8 10 2.82 mins 5.91 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 2 8 10 2.82 mins 5.99 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 2 8 100 2.27 mins 5.29 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 2 8 100 2.25 min 5.26 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 2 8 1000 2.65 mins 5.78 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 2 8 1000 2.7 mins 5.81 mins driver_name driver_memory driver_cores worker_name worker_memory worker_cores input_data_rows output_data_rows action total_worker_number total_cores partition NER timing NER+RE timing Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 1 4 4 4.64 mins 13.97 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 1 4 4 4.53 mins 13.88 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 1 4 10 4.42 mins 14.13 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 1 4 10 4.55 mins 14.63 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 1 4 100 4.19 mins 14.68 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 1 4 100 4.18 mins 14.89 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_parquet 1 4 1000 5.01 mins 16.38 mins Standard_D4s_v3 16 GB 4 Standard_D4s_v3 16 GB 4 1000 78000 write_deltalake 1 4 1000 4.99 mins 16.52 mins Clinical Bert For Token Classification Benchmark Experiment Dataset : 7537 Clinical Texts from PubMed Dataset Driver : Standard_DS3_v2 - 14GB Memory - 4 Cores Enable Autoscaling : True Cluster Mode : Standart Worker : Standard_DS3_v2 - 14GB Memory - 4 Cores Versions : Databricks Runtime Version : 10.0 (Apache Spark 3.2.0, Scala 2.12) spark-nlp Version: v3.4.0 spark-nlp-jsl Version : v3.4.0 Spark Version : v3.2.0 Spark NLP Pipeline : nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, ner_jsl_slim_tokenClassifier, ner_converter, finisher]) NOTES : In this experiment, the bert_token_classifier_ner_jsl_slim model was used to measure the inference time of clinical bert for token classification models in the databricks environment. In the first experiment, the data read from the parquet file is saved as parquet after processing. In the second experiment, the data read from the delta table was written to the delta table after it was processed. Bert For Token Classification Benchmark Table Repartition Time Read data from parquet 2 26.03 mins 64 10.84 mins 128 7.53 mins 1000 8.93 mins Read data from delta table 2 40.50 mins 64 11.84 mins 128 6.79 mins 1000 6.92 mins NER speed benchmarks across various Spark NLP and PySpark versions This experiment compares the ClinicalNER runtime for different versions of PySpark and Spark NLP. In this experiment, all reports went through the pipeline 10 times and repeated execution 5 times, so we ran each report 50 times and averaged it, %timeit -r 5 -n 10 run_model(spark, model). Driver : Standard Google Colab environment Spark NLP Pipeline : nlpPipeline = Pipeline( stages=[ documentAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, ner_converter ]) Dataset : File sizes: report_1: ~5.34kb report_2: ~8.51kb report_3: ~11.05kb report_4: ~15.67kb report_5: ~35.23kb   Spark NLP 4.0.0 (PySpark 3.1.2) Spark NLP 4.2.1 (PySpark 3.3.1) Spark NLP 4.2.1 (PySpark 3.1.2) Spark NLP 4.2.2 (PySpark 3.1.2) Spark NLP 4.2.2 (PySpark 3.3.1) Spark NLP 4.2.3 (PySpark 3.3.1) Spark NLP 4.2.3 (PySpark 3.1.2) report_1 2.36066 3.33056 2.23723 2.27243 2.11513 2.19655 2.23915 report_2 2.2179 3.31328 2.15578 2.23432 2.07259 2.07567 2.16776 report_3 2.77923 2.6134 2.69023 2.76358 2.55306 2.4424 2.72496 report_4 4.41064 4.07398 4.66656 4.59879 3.98586 3.92184 4.6145 report_5 9.54389 7.79465 9.25499 9.42764 8.02252 8.11318 9.46555 Results show that the different versions can have some variance in the execution time, but the difference is not too relevant.",
    "url": "/docs/en/benchmark",
    "relUrl": "/docs/en/benchmark"
  },
  "19": {
    "id": "19",
    "title": "Best Practices Using Pretrained Models Together",
    "content": "Entity Resolver Models and Features In the table below, all Entity Resolver models, their features, appropriate embeddings, and AUX info are illustrated. For instance, features of sbertresolve_ner_model_finder are under FEATURES column and it is trained using sbert_jsl_medium_uncased embeddings. Auxiliary info can be found under the AUX column if it is present. NOTE: This table is shared just to give you a rough idea about which pretrained models can be used together. You can get better or worse performance by playing out with different models. NO MODEL NAME FEATURES EMBEDDINGS LANGUAGE AUX 1 sbertresolve_ner_model_finder • Maps clinical entities (NER) to the most appropriate NER model• sbert_jsl_medium_uncased embeddings• Returns a list of pretrained NER models sbert_jsl_medium_uncased EN   2 sbiobertresolve_clinical_abbreviation_acronym • Maps  clinical abbreviations and acronyms to their meanings• sbiobert_base_cased_mli embeddings sbiobert_base_cased_mli EN   3 sbiobertresolve_cpt • CPT Codes• sbiobert_base_cased_mli embeddings sbiobert_base_cased_mli EN   4 sbiobertresolve_cpt_augmented • Augmented version of sbiobertresolve_cpt model• sbiobert_base_cased_mli embeddings sbiobert_base_cased_mli EN   5 sbiobertresolve_cpt_procedures_augmented • Procedures to CPT Codes• sbiobert_base_cased_mli embeddings sbiobert_base_cased_mli EN   6 sbiobertresolve_cpt_procedures_measurements_augmented • Procedure and Measurements to CPT Codes• sbiobert_base_cased_mli embeddings sbiobert_base_cased_mli EN   7 sbiobertresolve_hcc_augmented • HCC Codes• sbiobert_base_cased_mli embeddings sbiobert_base_cased_mli EN   8 sbiobertresolve_icd10cm • ICD-10-CM codes• sbiobert_base_cased_mli embeddings sbiobert_base_cased_mli EN   9 sbertresolve_icd10gm • German ICD10-GM Codes• sent_bert_base_cased (DE) sent_bert_base_cased (DE) DE   10 sbiobertresolve_icd10cm_augmented • ICD-10-CM codes• sbiobert_base_cased_mli embeddings• Augmented version of sbiobertresolve_icd10cm model with synonyms, four times richer. sbiobert_base_cased_mli EN   11 sbiobertresolve_icd10cm_augmented_billable_hcc • ICD-10-CM codes• sbiobert_base_cased_mli embeddings• Provides HCC information of the codes in all_k_aux_labels column• This column can be divided to get further details: billable status - hcc status - hcc score. sbiobert_base_cased_mli EN HCC and Billable Information 12 sbiobertresolve_icd10cm_generalised • ICD-10-CM codes up to 3 Characters (general type of injury or disease)• sbiobert_base_cased_mli embeddings sbiobert_base_cased_mli EN   13 sbiobertresolve_icd10cm_slim_billable_hcc • ICD-10-CM codes• sbiobert_base_cased_mli embeddings• Slim version (synonyms having low cosine similarity to unnormalized terms are dropped)• Provides  the official resolution text within the brackets• Provides  HCC information of the codes in all_k_aux_labels column.• This column can be divided to get further details: billable status - hcc status - hcc score. sbiobert_base_cased_mli EN HCC and Billable Information 14 sbertresolve_icd10cm_slim_billable_hcc_med • ICD-10-CM codes• sbert_jsl_medium_uncased embeddings• Slim version (synonyms having low cosine similarity to unnormalized terms are dropped)• Provides  the official resolution text within the brackets inside the metadata.• Provides  HCC information of the codes in all_k_aux_labels column• This column can be divided to get further details: billable status - hcc status - hcc score. sbert_jsl_medium_uncased EN HCC and Billable Information 15 sbiobertresolve_icd10cm_slim_normalized • ICD-10-CM codes• sbiobert_base_cased_mli embeddings• Slim version (synonyms having low cosine similarity to unnormalized terms are dropped)• Provides  the official resolution text within the brackets inside the metadata. sbiobert_base_cased_mli EN   16 sbiobertresolve_icd10pcs • ICD-10-PCS codes• sbiobert_base_cased_mli embeddings sbiobert_base_cased_mli EN   17 sbiobertresolve_icdo • ICD-O Codes (International Classification of Diseases for Oncology code)• Provides topography codes and morphology codes comprising of Histology and Behavior codes in all_k_aux_labels columns• sbiobert_base_cased_mli embeddings• More granularity with respect to body parts sbiobert_base_cased_mli EN Topography codes, Morphology codes comprising of Histology and Behavior codes 18 sbiobertresolve_icdo_base • ICD-O Codes (International Classification of Diseases for Oncology code)• Provides topography codes and morphology codes comprising of Histology and Behavior codes in all_k_aux_labels columns• sbiobert_base_cased_mli embeddings• More granularity with respect to body parts sbiobert_base_cased_mli EN Topography codes, Morphology codes comprising of Histology and Behavior codes 19 sbiobertresolve_icdo_augmented • ICD-O Codes (International Classification of Diseases for Oncology code)• Provides topography codes and morphology codes comprising of Histology and Behavior codes in all_k_aux_labels columns• sbiobert_base_cased_mli embeddings• More granularity with respect to body parts• Augmented using the site information coming from ICD10 and synonyms coming from SNOMED vocabularies. sbiobert_base_cased_mli EN Topography codes, Morphology codes comprising of Histology and Behavior codes 20 sbiobertresolve_rxnorm • RxNorm Codes for Drugs/ Ingredients• sbiobert_base_cased_mli  embeddings sbiobert_base_cased_mli EN   21 sbiobertresolve_rxnorm_augmented • RxNorm Codes for Drugs/ Ingredients• sbiobert_base_cased_mli  embeddings• Provides concept classes of the drugs in all_k_aux_labels column.• Augmented version of sbiobertresolve_rxnorm model sbiobert_base_cased_mli EN Concept classes of the drugs 22 sbiobertresolve_rxnorm_augmented_cased • RxNorm Codes for Drugs/ Ingredients• sbiobert_base_cased_mli  embeddings• Provides concept classes of the drugs in all_k_aux_labels column.• Cased (unlowered) concept names• Augmented version of sbiobertresolve_rxnorm model sbiobert_base_cased_mli EN Concept classes of the drugs 23 sbiobertresolve_rxnorm_disposition • RxNorm Codes for Drugs/ Ingredients• sbiobert_base_cased_mli  embeddings• Provides dispositions of the RxNorm codes in all_k_aux_labels column. sbiobert_base_cased_mli EN Provides dispositions of the RxNorm codes in all_k_aux_labels column. 24 sbertresolve_rxnorm_disposition • Medication entities (like drugs/ingredients) to RxNorm codes• Provides the dispositions of the codes in all_k_aux_labels column• sbert_jsl_medium_uncased embeddings (light) sbert_jsl_medium_uncased EN Disposition Information 25 sbiobertresolve_jsl_rxnorm_augmented • RxNorm Codes for Drugs/ Ingredients• sbiobert_jsl_rxnorm_cased  embeddings• Provides concept classes of the drugs in all_k_aux_labels column.• Augmented version of sbiobertresolve_rxnorm model sbiobert_jsl_rxnorm_cased EN Concept classes of the drugs 26 sbluebertresolve_rxnorm_augmented_uncased • RxNorm Codes for Drugs/ Ingredients• sbluebert_base_uncased_mli  embeddings• Provides concept classes of the drugs in all_k_aux_labels column.• Augmented version of sbiobertresolve_rxnorm model sbluebert_base_uncased_mli EN Concept classes of the drugs 27 sbertresolve_jsl_rxnorm_augmented_med • RxNorm Codes for Drugs/ Ingredients• sbert_jsl_medium_rxnorm_uncased  embeddings• Provides concept classes of the drugs in all_k_aux_labels column.• Augmented version of sbiobertresolve_rxnorm model sbert_jsl_medium_rxnorm_uncased EN Concept classes of the drugs 28 sbiobertresolve_rxcui • RxCUI Codes• sbiobert_base_cased_mli  embeddings sbiobert_base_cased_mli EN   29 sbluebertresolve_loinc • LOINC Codes• sbluebert_base_uncased_mli  embeddings sbluebert_base_uncased_mli EN   30 sbiobertresolve_loinc • LOINC Codes• sbiobert_base_cased_mli  embeddings sbiobert_base_cased_mli EN   31 sbiobertresolve_loinc_augmented • LOINC Codes• sbiobert_base_cased_mli  embeddings• Augmented version of sbiobertresolve_loinc sbiobert_base_cased_mli EN   32 sbiobertresolve_loinc_cased • LOINC Codes• sbiobert_base_cased_mli  embeddings• Cased (unlowered) concept names• Augmented version of sbiobertresolve_loinc sbiobert_base_cased_mli EN   33 sbluebertresolve_loinc_uncased • LOINC Codes• sbluebert_base_uncased_mli  embeddings• Uncased (lowercased) concept names• Augmented version of sbiobertresolve_loinc sbluebert_base_uncased_mli EN   34 sbiobertresolve_mesh • MeSH Codes• sbiobert_base_cased_mli  embeddings sbiobert_base_cased_mli EN   35 sbiobertresolve_ndc • NDC Codes• sbiobert_base_cased_mli  embeddings• If a drug has more than one NDC code, it returns all other codes in the all_k_aux_label column sbiobert_base_cased_mli EN If drugs have multiple NDC code, it returns all other codes in the all_k_aux_label column 36 sbiobertresolve_hcpcs • HCPCS Codes• sbiobert_base_cased_mli  embeddings sbiobert_base_cased_mli EN Domain Information 37 sbiobertresolve_HPO • Maps phenotypic abnormalities encountered in human diseases to Human Phenotype Ontology (HPO)• Provides associated codes from the following vocabularies for each HPO code: - MeSH (Medical Subject Headings)- SNOMED- UMLS (Unified Medical Language System ) - ORPHA (international reference resource for information on rare diseases and orphan drugs) - OMIM (Online Mendelian Inheritance in Man) in all_k_aux_labels column sbiobert_base_cased_mli EN SNOMED, MeSH, UMLS, ORPHA, OMIM Codes 38 sbiobertresolve_snomed_auxConcepts • SNOMED Codes• sbiobert_base_cased_mli  embeddings• Capable of extracting Morph Abnormality, Procedure, Substance, Physical Object, and Body Structure concepts of Snomed codes sbiobert_base_cased_mli EN   39 sbiobertresolve_snomed_auxConcepts_int • SNOMED Codes (INT Version)• sbiobert_base_cased_mli  embeddings• Capable of extracting Morph Abnormality, Procedure, Substance, Physical Object, and Body Structure concepts of Snomed codes sbiobert_base_cased_mli EN   40 sbiobertresolve_snomed_bodyStructure • SNOMED Codes for body structure• sbiobert_base_cased_mli  embeddings• Anatomical structures to body structure SNOMED codes sbiobert_base_cased_mli EN   41 sbertresolve_snomed_bodyStructure_med • SNOMED Codes for body structure• sbert_jsl_medium_uncased  embeddings• Anatomical structures to body structure SNOMED codes sbert_jsl_medium_uncased EN   42 sbiobertresolve_snomed_drug • SNOMED Codes (drug version)• sbiobert_base_cased_mli  embeddings• Drug entities to SNOMED codes sbiobert_base_cased_mli EN   43 sbiobertresolve_snomed_findings • SNOMED Codes (CT version)• sbiobert_base_cased_mli  embeddings sbiobert_base_cased_mli EN   44 sbiobertresolve_snomed_findings_int • SNOMED Codes (INT version)• sbiobert_base_cased_mli  embeddings sbiobert_base_cased_mli EN   45 sbiobertresolve_snomed_findings_aux_concepts • SNOMED Codes• sbiobert_base_cased_mli  embeddings• Capable of extracting Morph Abnormality, Procedure, Substance, Physical Object, and Body Structure concepts of Snomed codes• Both Aux and CT versions together sbiobert_base_cased_mli EN   46 sbiobertresolve_snomed_procedures_measurements • SNOMED Codes for procedure and measurements• sbiobert_base_cased_mli  embeddings sbiobert_base_cased_mli EN   47 sbiobertresolve_clinical_snomed_procedures_measurements • SNOMED Codes for procedure and measurements• sbiobert_base_cased_mli  embeddings sent_biobert_clinical_base_cased EN   48 sbertresolve_snomed_conditions • Conditions to SNOMED Codes• sbert_jsl_medium_uncased embeddings sbert_jsl_medium_uncased EN   49 robertaresolve_snomed • Spanish SNOMED Codes• Roberta Clinical Word Embeddings (roberta_base_biomedical_es)• averaged with SentenceEmbeddings. roberta_base_biomedical_es ES   50 sbertresolve_snomed • German SNOMED Codes• sent_bert_base_cased (DE) sent_bert_base_cased (DE) DE   51 sbiobertresolve_umls_clinical_drugs • UMLS CUI Codes for Clinical Drugs• sbiobert_base_cased_mli  embeddings sbiobert_base_cased_mli EN   52 sbiobertresolve_umls_disease_syndrome • UMLS CUI Codes for Disease and Syndrom entities• sbiobert_base_cased_mli  embeddings sbiobert_base_cased_mli EN   53 sbiobertresolve_umls_drug_substance • UMLS CUI Codes for Drug and Substance entities• sbiobert_base_cased_mli  embeddings• Clinical Drugs, Pharmacologic Substance, Antibiotic, Hazardous or Poisonous Substance to UMLS CUI Codes sbiobert_base_cased_mli EN   54 sbiobertresolve_umls_findings • UMLS CUI Codes for clinical entities and concepts• sbiobert_base_cased_mli  embeddings• 4 major categories of UMLS CUI codes sbiobert_base_cased_mli EN   55 sbiobertresolve_umls_major_concepts • UMLS CUI Codes for clinical entities and concepts• sbiobert_base_cased_mli  embeddings• 4 major categories (Clinical Findings, Medical Devices, Anatomical Structures, and Injuries &amp; Poisoning terms) of UMLS CUI codes sbiobert_base_cased_mli EN               Entity Resolver Model and NER Model Pairs In the table below, you can find Entity Resolver models as well as its appropriate NER models and labels, that can return optimal results. For instance, sbiobertresolve_hcc_augmented resolver model must be used with sbiobert_base_cased_mli as embeddings, ner_clinical as NER model, PROBLEM set in setWhiteList(). NOTE: This table is shared just to give you a rough idea about which pretrained models can be used together. You can get better or worse performance by playing out with different models. ENTITY RESOLVER MODEL SENTENCE EMBEDDINGS NER MODEL NER MODEL WHITELIST LABEL MERGE CHUNKS (ChunkMergeApproach) sbiobertresolve_HPO sbiobert_base_cased_mli ner_human_phenotype_gene_clinical No need to set whiteList sbiobertresolve_cpt_procedures_measurements_augmented sbiobert_base_cased_mli ner_jsl Procedure Merge ner_jsl and ner_measurements_clinical model chunks ner_measurements_clinical Measurements sbiobertresolve_hcc_augmented sbiobert_base_cased_mli ner_clinical PROBLEM sbiobertresolve_hcpcs sbiobert_base_cased_mli ner_jsl Procedure sbiobertresolve_icd10cm_augmented_billable_hcc sbiobert_base_cased_mli ner_clinical PROBLEM sbiobertresolve_icd10cm_generalised sbiobert_base_cased_mli ner_clinical PROBLEM sbiobertresolve_icd10pcs sbiobert_base_cased_mli ner_jsl Procedure sbiobertresolve_icdo_base sbiobert_base_cased_mli ner_jsl Oncological sbiobertresolve_loinc_augmented sbiobert_base_cased_mli ner_jsl TestBMIHDLLDLMedical_DeviceTemperatureTotal_CholesterolTriglyceridesBlood_Pressure sbiobertresolve_mesh sbiobert_base_cased_mli ner_clinical No need to set whiteList sbiobertresolve_rxcui sbiobert_base_cased_mli ner_posology DRUG sbiobertresolve_rxnorm_augmented sbiobert_base_cased_mli ner_posology DRUG sbiobertresolve_rxnorm_disposition sbiobert_base_cased_mli ner_posology DRUG sbiobertresolve_snomed_bodyStructure sbiobert_base_cased_mli ner_jsl Disease_Syndrome_DisorderExternal_body_part_or_region Merge ner_jsl and ner_anatomy_coarse model chunks ner_anatomy_coarse No need to set whiteList sbiobertresolve_snomed_procedures_measurements sbiobert_base_cased_mli ner_jsl ProcedureTestBMIHDLLDLTemperatureTotal_CholesterolTriglyceridesBlood_Pressure Merge ner_jsl and ner_measurements_clinical model chunks ner_measurements_clinical Measurements sbiobertresolve_snomed_findings sbiobert_base_cased_mli ner_clinical No need to set whiteList sbiobertresolve_umls_disease_syndrome sbiobert_base_cased_mli ner_jsl Cerebrovascular_DiseaseCommunicable_DiseaseDiabetesDisease_Syndrome_DisorderHeart_DiseaseHyperlipidemiaHypertensionInjury_or_PoisoningKidney_DiseaseObesityOncologicalOverweightPsychological_ConditionSymptomVS_FindingImagingFindingsEKG_Findings sbiobertresolve_umls_clinical_drugs sbiobert_base_cased_mli ner_posology DRUG sbiobertresolve_umls_major_concepts sbiobert_base_cased_mli ner_jsl Cerebrovascular_DiseaseCommunicable_DiseaseDiabetesDisease_Syndrome_DisorderHeart_DiseaseHyperlipidemiaHypertensionInjury_or_PoisoningKidney_DiseaseMedical-DeviceObesityOncologicalOverweightPsychological_ConditionSymptomVS_FindingImagingFindingsEKG_Findings sbiobertresolve_ndc sbiobert_base_cased_mli ner_posology_greedy DRUG Relation Extraction Models and Relation Pairs Table In the table below, available Relation Extraction models, its labels, optimal NER model, and meaningful relation pairs are illustrated. For instance, re_bodypart_proceduretest RE model returns (0,1) labels (binary), works optimally with ner_jsl NER model, and outputs relation pairs under the RE PAIRS column. NOTE: This table is shared just to give you a rough idea about which pretrained models can be used together. You can get better or worse performance by playing out with different models. NO RE MODEL RE MODEL LABELS NER MODEL RE PAIRS 1 re_bodypart_proceduretest 0,1 ner_jsl [“external_body_part_or_region-test”, ”test-external_body_part_or_region”,“internal_organ_or_component-test”,“test-internal_organ_or_component”,“external_body_part_or_region-procedure”,“procedure-external_body_part_or_region”,“procedure-internal_organ_or_component”,“internal_organ_or_component-procedure”] 2 re_ade_clinical 0,1 ner_ade_clinical [“ade-drug”, ”drug-ade”] 3 redl_chemprot_biobert CPR:1, CPR:2, CPR:3, CPR:4, CPR:5, CPR:6, CPR:7, CPR:8, CPR:9, CPR:10 ner_chemprot_clinical [“No need to set pairs.”] 4 re_human_phenotype_gene_clinical 0,1 ner_human_phenotype_gene_clinical [“No need to set pairs.”] 5 re_bodypart_directions 0,1 ner_jsl [“direction-external_body_part_or_region”,“external_body_part_or_region-direction”,“direction-internal_organ_or_component”,“internal_organ_or_component-direction”] 6 re_bodypart_problem 0,1 ner_jsl [“internal_organ_or_component-cerebrovascular_disease”, “cerebrovascular_disease-internal_organ_or_component”,“internal_organ_or_component-communicable_disease”, “communicable_disease-internal_organ_or_component”,“internal_organ_or_component-diabetes”, “diabetes-internal_organ_or_component”,“internal_organ_or_component-disease_syndrome_disorder”, “disease_syndrome_disorder-internal_organ_or_component”,“internal_organ_or_component-ekg_findings”, “ekg_findings-internal_organ_or_component”,“internal_organ_or_component-heart_disease”, “heart_disease-internal_organ_or_component”,“internal_organ_or_component-hyperlipidemia”, “hyperlipidemia-internal_organ_or_component”,“internal_organ_or_component-hypertension”, “hypertension-internal_organ_or_component”,“internal_organ_or_component-imagingfindings”, “imagingfindings-internal_organ_or_component”,“internal_organ_or_component-injury_or_poisoning”, “injury_or_poisoning-internal_organ_or_component”,“internal_organ_or_component-kidney_disease”, “kidney_disease-internal_organ_or_component”,“internal_organ_or_component-oncological”, “oncological-internal_organ_or_component”,“internal_organ_or_component-psychological_condition”, “psychological_condition-internal_organ_or_component”,“internal_organ_or_component-symptom”, “symptom-internal_organ_or_component”,“internal_organ_or_component-vs_finding”, “vs_finding-internal_organ_or_component”,“external_body_part_or_region-communicable_disease”, “communicable_disease-external_body_part_or_region”,“external_body_part_or_region-diabetes”, “diabetes-external_body_part_or_region”,“external_body_part_or_region-disease_syndrome_disorder”, “disease_syndrome_disorder-external_body_part_or_region”,“external_body_part_or_region-hypertension”, “hypertension-external_body_part_or_region”,“external_body_part_or_region-imagingfindings”, “imagingfindings-external_body_part_or_region”,“external_body_part_or_region-injury_or_poisoning”, “injury_or_poisoning-external_body_part_or_region”,“external_body_part_or_region-obesity”, “obesity-external_body_part_or_region”,“external_body_part_or_region-oncological”, “oncological-external_body_part_or_region”,“external_body_part_or_region-overweight”, “overweight-external_body_part_or_region”,“external_body_part_or_region-symptom”, “symptom-external_body_part_or_region”,“external_body_part_or_region-vs_finding”, “vs_finding-external_body_part_or_region”] 7 re_drug_drug_interaction_clinical DDI-advise, DDI-effect, DDI-mechanism, DDI-int, DDI-false ner_posology [“drug-drug”] 8 re_clinical TrIP, TrWP, TrCP, TrAP, TrAP, TeRP, TeCP, PIP ner_clinical [“No need to set pairs.”] 9 re_temporal_events_clinical AFTER, BEFORE, OVERLAP ner_events_clinical [“No need to set pairs.”] 10 re_temporal_events_enriched_clinical BEFORE, AFTER, SIMULTANEOUS, BEGUN_BY, ENDED_BY, DURING, BEFORE_OVERLAP ner_events_clinical [“No need to set pairs.”] 11 re_test_problem_finding 0,1 ner_jsl [“test-cerebrovascular_disease”, “cerebrovascular_disease-test”,“test-communicable_disease”, “communicable_disease-test”,“test-diabetes”, “diabetes-test”,“test-disease_syndrome_disorder”, “disease_syndrome_disorder-test”,“test-heart_disease”, “heart_disease-test”,“test-hyperlipidemia”, “hyperlipidemia-test”,“test-hypertension”, “hypertension-test”,“test-injury_or_poisoning”, “injury_or_poisoning-test”,“test-kidney_disease”, “kidney_disease-test”,“test-obesity”, “obesity-test”,“test-oncological”, “oncological-test”,“test-psychological_condition”, “psychological_condition-test”,“test-symptom”, “symptom-test”,“ekg_findings-disease_syndrome_disorder”, “disease_syndrome_disorder-ekg_findings”,“ekg_findings-heart_disease”, “heart_disease-ekg_findings”,“ekg_findings-symptom”, “symptom-ekg_findings”,“imagingfindings-cerebrovascular_disease”, “cerebrovascular_disease-imagingfindings”,“imagingfindings-communicable_disease”, “communicable_disease-imagingfindings”,“imagingfindings-disease_syndrome_disorder”, “disease_syndrome_disorder-imagingfindings”,“imagingfindings-heart_disease”, “heart_disease-imagingfindings”,“imagingfindings-hyperlipidemia”, “hyperlipidemia-imagingfindings”,“imagingfindings-hypertension”, “hypertension-imagingfindings”,“imagingfindings-injury_or_poisoning”, “injury_or_poisoning-imagingfindings”,“imagingfindings-kidney_disease”, “kidney_disease-imagingfindings”,“imagingfindings-oncological”, “oncological-imagingfindings”,“imagingfindings-psychological_condition”, “psychological_condition-imagingfindings”,“imagingfindings-symptom”, “symptom-imagingfindings”,“vs_finding-cerebrovascular_disease”, “cerebrovascular_disease-vs_finding”,“vs_finding-communicable_disease”, “communicable_disease-vs_finding”,“vs_finding-diabetes”, “diabetes-vs_finding”,“vs_finding-disease_syndrome_disorder”, “disease_syndrome_disorder-vs_finding”,“vs_finding-heart_disease”, “heart_disease-vs_finding”,“vs_finding-hyperlipidemia”, “hyperlipidemia-vs_finding”,“vs_finding-hypertension”, “hypertension-vs_finding”,“vs_finding-injury_or_poisoning”, “injury_or_poisoning-vs_finding”,“vs_finding-kidney_disease”, “kidney_disease-vs_finding”,“vs_finding-obesity”, “obesity-vs_finding”,“vs_finding-oncological”, “oncological-vs_finding”,“vs_finding-overweight”, “overweight-vs_finding”,“vs_finding-psychological_condition”, “psychological_condition-vs_finding”,“vs_finding-symptom”, “symptom-vs_finding”] 12 re_test_result_date is_finding_of, is_result_of, is_date_of, O ner_jsl [“test-test_result”, “test_result-test”,“test-date”, “date-test”,“test-imagingfindings”, “imagingfindings-test”,“test-ekg_findings”, “ekg_findings-test”,“date-test_result”, “test_result-date”,“date-imagingfindings”, “imagingfindings-date”,“date-ekg_findings”, “ekg_findings-date”] 13 re_date_clinical 0,1 ner_jsl [“date-admission_discharge”, “admission_discharge-date”,“date-alcohol”, “alcohol-date”,“date-allergen”, “allergen-date”,“date-bmi”, “bmi-date”,“date-birth_entity”, “birth_entity-date”,“date-blood_pressure”, “blood_pressure-date”,“date-cerebrovascular_disease”, “cerebrovascular_disease-date”,“date-clinical_dept”, “clinical_dept-date”,“date-communicable_disease”, “communicable_disease-date”,“date-death_entity”, “death_entity-date”,“date-diabetes”, “diabetes-date”,“date-diet”, “diet-date”,“date-disease_syndrome_disorder”, “disease_syndrome_disorder-date”,“date-drug_brandname”, “drug_brandname-date”,“date-drug_ingredient”, “drug_ingredient-date”,“date-ekg_findings”, “ekg_findings-date”,“date-external_body_part_or_region”, “external_body_part_or_region-date”,“date-fetus_newborn”, “fetus_newborn-date”,“date-hdl”, “hdl-date”,“date-heart_disease”, “heart_disease-date”,“date-height”, “height-date”,“date-hyperlipidemia”, “hyperlipidemia-date”,“date-hypertension”, “hypertension-date”,“date-imagingfindings”, “imagingfindings-date”,“date-imaging_technique”, “imaging_technique-date”,“date-injury_or_poisoning”, “injury_or_poisoning-date”,“date-internal_organ_or_component”, “internal_organ_or_component-date”,“date-kidney_disease”, “kidney_disease-date”,“date-ldl”, “ldl-date”,“date-modifier”, “modifier-date”,“date-o2_saturation”, “o2_saturation-date”,“date-obesity”, “obesity-date”,“date-oncological”, “oncological-date”,“date-overweight”, “overweight-date”,“date-oxygen_therapy”, “oxygen_therapy-date”,“date-pregnancy”, “pregnancy-date”,“date-procedure”, “procedure-date”,“date-psychological_condition”, “psychological_condition-date”,“date-pulse”, “pulse-date”,“date-respiration”, “respiration-date”,“date-smoking”, “smoking-date”,“date-substance”, “substance-date”,“date-substance_quantity”, “substance_quantity-date”,“date-symptom”, “symptom-date”,“date-temperature”, “temperature-date”,“date-test”, “test-date”,“date-test_result”, “test_result-date”,“date-total_cholesterol”, “total_cholesterol-date”,“date-treatment”, “treatment-date”,“date-triglycerides”, “triglycerides-date”,“date-vs_finding”, “vs_finding-date”,“date-vaccine”, “vaccine-date”,“date-vital_signs_header”, “vital_signs_header-date”,“date-weight”, “weight-date”,“time-admission_discharge”, “admission_discharge-time”,“time-alcohol”, “alcohol-time”,“time-allergen”, “allergen-time”,“time-bmi”, “bmi-time”,“time-birth_entity”, “birth_entity-time”,“time-blood_pressure”, “blood_pressure-time”,“time-cerebrovascular_disease”, “cerebrovascular_disease-time”,“time-clinical_dept”, “clinical_dept-time”,“time-communicable_disease”, “communicable_disease-time”,“time-death_entity”, “death_entity-time”,“time-diabetes”, “diabetes-time”,“time-diet”, “diet-time”,“time-disease_syndrome_disorder”, “disease_syndrome_disorder-time”,“time-drug_brandname”, “drug_brandname-time”,“time-drug_ingredient”, “drug_ingredient-time”,“time-ekg_findings”, “ekg_findings-time”,“time-external_body_part_or_region”, “external_body_part_or_region-time”,“time-fetus_newborn”, “fetus_newborn-time”,“time-hdl”, “hdl-time”,“time-heart_disease”, “heart_disease-time”,“time-height”, “height-time”,“time-hyperlipidemia”, “hyperlipidemia-time”,“time-hypertension”, “hypertension-time”,“time-imagingfindings”, “imagingfindings-time”,“time-imaging_technique”, “imaging_technique-time”,“time-injury_or_poisoning”, “injury_or_poisoning-time”,“time-internal_organ_or_component”, “internal_organ_or_component-time”,“time-kidney_disease”, “kidney_disease-time”,“time-ldl”, “ldl-time”,“time-modifier”, “modifier-time”,“time-o2_saturation”, “o2_saturation-time”,“time-obesity”, “obesity-time”,“time-oncological”, “oncological-time”,“time-overweight”, “overweight-time”,“time-oxygen_therapy”, “oxygen_therapy-time”,“time-pregnancy”, “pregnancy-time”,“time-procedure”, “procedure-time”,“time-psychological_condition”, “psychological_condition-time”,“time-pulse”, “pulse-time”,“time-respiration”, “respiration-time”,“time-smoking”, “smoking-time”,“time-substance”, “substance-time”,“time-substance_quantity”, “substance_quantity-time”,“time-symptom”, “symptom-time”,“time-temperature”, “temperature-time”,“time-test”, “test-time”,“time-test_result”, “test_result-time”,“time-total_cholesterol”, “total_cholesterol-time”,“time-treatment”, “treatment-time”,“time-triglycerides”, “triglycerides-time”,“time-vs_finding”, “vs_finding-time”,“time-vaccine”, “vaccine-time”,“time-vital_signs_header”, “vital_signs_header-time”,“time-weight”, “weight-time”,“relativedate-admission_discharge”, “admission_discharge-relativedate”,“relativedate-alcohol”, “alcohol-relativedate”,“relativedate-allergen”, “allergen-relativedate”,“relativedate-bmi”, “bmi-relativedate”,“relativedate-birth_entity”, “birth_entity-relativedate”,“relativedate-blood_pressure”, “blood_pressure-relativedate”,“relativedate-cerebrovascular_disease”, “cerebrovascular_disease-relativedate”,“relativedate-clinical_dept”, “clinical_dept-relativedate”,“relativedate-communicable_disease”, “communicable_disease-relativedate”,“relativedate-death_entity”, “death_entity-relativedate”,“relativedate-diabetes”, “diabetes-relativedate”,“relativedate-diet”, “diet-relativedate”,“relativedate-disease_syndrome_disorder”, “disease_syndrome_disorder-relativedate”,“relativedate-drug_brandname”, “drug_brandname-relativedate”,“relativedate-drug_ingredient”, “drug_ingredient-relativedate”,“relativedate-ekg_findings”, “ekg_findings-relativedate”,“relativedate-external_body_part_or_region”, “external_body_part_or_region-relativedate”,“relativedate-fetus_newborn”, “fetus_newborn-relativedate”,“relativedate-hdl”, “hdl-relativedate”,“relativedate-heart_disease”, “heart_disease-relativedate”,“relativedate-height”, “height-relativedate”,“relativedate-hyperlipidemia”, “hyperlipidemia-relativedate”,“relativedate-hypertension”, “hypertension-relativedate”,“relativedate-imagingfindings”, “imagingfindings-relativedate”,“relativedate-imaging_technique”, “imaging_technique-relativedate”,“relativedate-injury_or_poisoning”, “injury_or_poisoning-relativedate”,“relativedate-internal_organ_or_component”, “internal_organ_or_component-relativedate”,“relativedate-kidney_disease”, “kidney_disease-relativedate”,“relativedate-ldl”, “ldl-relativedate”,“relativedate-modifier”, “modifier-relativedate”,“relativedate-o2_saturation”, “o2_saturation-relativedate”,“relativedate-obesity”, “obesity-relativedate”,“relativedate-oncological”, “oncological-relativedate”,“relativedate-overweight”, “overweight-relativedate”,“relativedate-oxygen_therapy”, “oxygen_therapy-relativedate”,“relativedate-pregnancy”, “pregnancy-relativedate”,“relativedate-procedure”, “procedure-relativedate”,“relativedate-psychological_condition”, “psychological_condition-relativedate”,“relativedate-pulse”, “pulse-relativedate”,“relativedate-respiration”, “respiration-relativedate”,“relativedate-smoking”, “smoking-relativedate”,“relativedate-substance”, “substance-relativedate”,“relativedate-substance_quantity”, “substance_quantity-relativedate”,“relativedate-symptom”, “symptom-relativedate”,“relativedate-temperature”, “temperature-relativedate”,“relativedate-test”, “test-relativedate”,“relativedate-test_result”, “test_result-relativedate”,“relativedate-total_cholesterol”, “total_cholesterol-relativedate”,“relativedate-treatment”, “treatment-relativedate”,“relativedate-triglycerides”, “triglycerides-relativedate”,“relativedate-vs_finding”, “vs_finding-relativedate”,“relativedate-vaccine”, “vaccine-relativedate”,“relativedate-vital_signs_header”, “vital_signs_header-relativedate”,“relativedate-weight”, “weight-relativedate”,“relativetime-admission_discharge”, “admission_discharge-relativetime”,“relativetime-alcohol”, “alcohol-relativetime”,“relativetime-allergen”, “allergen-relativetime”,“relativetime-bmi”, “bmi-relativetime”,“relativetime-birth_entity”, “birth_entity-relativetime”,“relativetime-blood_pressure”, “blood_pressure-relativetime”,“relativetime-cerebrovascular_disease”, “cerebrovascular_disease-relativetime”,“relativetime-clinical_dept”, “clinical_dept-relativetime”,“relativetime-communicable_disease”, “communicable_disease-relativetime”,“relativetime-death_entity”, “death_entity-relativetime”,“relativetime-diabetes”, “diabetes-relativetime”,“relativetime-diet”, “diet-relativetime”,“relativetime-disease_syndrome_disorder”, “disease_syndrome_disorder-relativetime”,“relativetime-drug_brandname”, “drug_brandname-relativetime”,“relativetime-drug_ingredient”, “drug_ingredient-relativetime”,“relativetime-ekg_findings”, “ekg_findings-relativetime”,“relativetime-external_body_part_or_region”, “external_body_part_or_region-relativetime”,“relativetime-fetus_newborn”, “fetus_newborn-relativetime”,“relativetime-hdl”, “hdl-relativetime”,“relativetime-heart_disease”, “heart_disease-relativetime”,“relativetime-height”, “height-relativetime”,“relativetime-hyperlipidemia”, “hyperlipidemia-relativetime”,“relativetime-hypertension”, “hypertension-relativetime”,“relativetime-imagingfindings”, “imagingfindings-relativetime”,“relativetime-imaging_technique”, “imaging_technique-relativetime”,“relativetime-injury_or_poisoning”, “injury_or_poisoning-relativetime”,“relativetime-internal_organ_or_component”, “internal_organ_or_component-relativetime”,“relativetime-kidney_disease”, “kidney_disease-relativetime”,“relativetime-ldl”, “ldl-relativetime”,“relativetime-modifier”, “modifier-relativetime”,“relativetime-o2_saturation”, “o2_saturation-relativetime”,“relativetime-obesity”, “obesity-relativetime”,“relativetime-oncological”, “oncological-relativetime”,“relativetime-overweight”, “overweight-relativetime”,“relativetime-oxygen_therapy”, “oxygen_therapy-relativetime”,“relativetime-pregnancy”, “pregnancy-relativetime”,“relativetime-procedure”, “procedure-relativetime”,“relativetime-psychological_condition”, “psychological_condition-relativetime”,“relativetime-pulse”, “pulse-relativetime”,“relativetime-respiration”, “respiration-relativetime”,“relativetime-smoking”, “smoking-relativetime”,“relativetime-substance”, “substance-relativetime”,“relativetime-substance_quantity”, “substance_quantity-relativetime”,“relativetime-symptom”, “symptom-relativetime”,“relativetime-temperature”, “temperature-relativetime”,“relativetime-test”, “test-relativetime”,“relativetime-test_result”, “test_result-relativetime”,“relativetime-total_cholesterol”, “total_cholesterol-relativetime”,“relativetime-treatment”, “treatment-relativetime”,“relativetime-triglycerides”, “triglycerides-relativetime”,“relativetime-vs_finding”, “vs_finding-relativetime”,“relativetime-vaccine”, “vaccine-relativetime”,“relativetime-vital_signs_header”, “vital_signs_header-relativetime”,“relativetime-weight”, “weight-relativetime”] 14 redl_drugprot_biobert  INHIBITOR, DIRECT-REGULATOR, SUBSTRATE, ACTIVATOR, INDIRECT-UPREGULATOR, INDIRECT-DOWNREGULATOR, ANTAGONIST, PRODUCT-OF, PART-OF, AGONIST ner_drugprot_clinical [“checmical-gene”, “chemical-gene_and_chemical”, “gene_and_chemical-gene”] 15 re_drugprot_clinical  INHIBITOR, DIRECT-REGULATOR, SUBSTRATE, ACTIVATOR, INDIRECT-UPREGULATOR, INDIRECT-DOWNREGULATOR, ANTAGONIST, PRODUCT-OF, PART-OF, AGONIST ner_drugprot_clinical [“checmical-gene”, “chemical-gene_and_chemical”, “gene_and_chemical-gene”] 16 redl_nihss_biobert Has_Value, 0 ner_nihss [“No need to set pairs.”]",
    "url": "/docs/en/best_practices_pretrained_models",
    "relUrl": "/docs/en/best_practices_pretrained_models"
  },
  "20": {
    "id": "20",
    "title": "Biomedical Research  - Biomedical NLP Demos & Notebooks",
    "content": "",
    "url": "/biomedical_research",
    "relUrl": "/biomedical_research"
  },
  "21": {
    "id": "21",
    "title": "License Management",
    "content": "By default, the Annotation Lab allows access to community pre-trained models and embeddings. Those are available on the Models Hub page. To gain access to licensed resources (e.g. pre-trained models and embeddings) admin user can import a license (Healthcare, Finance, Legal, or Visual NLP) which will activate additional features: Access to licensed models for pre-annotation Access to healthcare, finance, and legal embeddings Access to rules Access to optimized annotators Access to training custom models using licensed embeddings The admin user can upload a Spark NLP license JSON file by visiting the License page. The license is generated by the John Snow Labs license server and is available on my.JohnSnowLabs.com. Once a valid license is uploaded, all the licensed (Healthcare, Finance, Legal, and Visual NLP) models and embeddings become available for download. The License page shows the history of license uploads with detailed information like License Info, Status, Renewal Date, and License Secrets. Support for Floating Licenses Annotation Lab supports floating licenses with different scopes (ocr: training, ocr: inference, healthcare: inference, healthcare: training, finance: inference, finance: training, legal: inference, legal: training). Depending on the scope of the available license, users can perform model training and/or deploy pre-annotation servers. Licenses are a must for training Healthcare, Finance, and Legal models and deploying these models as pre-annotation servers. Floating licenses can be acquired on self-service via my.JohnSnowLabs.com. One floating license is bound to only one server (pre-annotation server, OCR server, training job) at a time. To run multiple model training jobs and/or pre-annotations servers, users must provide multiple floating licenses. Annotation Lab supports either floating licenses or air-gapped licenses. Mixing floating and air-gapped licenses on the same Annotation Lab instance is not allowed. Usage of NLP Licenses The number of available floating licenses can influence the creation of multiple training and pre-annotation servers. For example, to deploy 5 pre-annotation servers using Spark NLP for Healthcare models or embeddings, across 5 different projects, you will need 5 floating licenses. Since one floating license can only be used for one server, it is not possible to deploy a pre-annotation server and then trigger training from the same project when only one license is available. In this case, the pre-annotation server has to be deleted first, and then the training can be started. Those restrictions do not apply when using Spark NLP models and embeddings.",
    "url": "/docs/en/alab/byol",
    "relUrl": "/docs/en/alab/byol"
  },
  "22": {
    "id": "22",
    "title": "Classify Documents - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/classify_documents",
    "relUrl": "/classify_documents"
  },
  "23": {
    "id": "23",
    "title": "Classify Financial Documents - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/classify_financial_documents",
    "relUrl": "/classify_financial_documents"
  },
  "24": {
    "id": "24",
    "title": "Clinical Trials - Healthcare NLP Demos & Notebooks",
    "content": "",
    "url": "/clinical_trials",
    "relUrl": "/clinical_trials"
  },
  "25": {
    "id": "25",
    "title": "Cluster Management",
    "content": "Management of Preannotation and Training Servers Annotation Lab gives users the ability to view the list of all active servers. Any user can access the Clusters page by navigating to Settings &gt; Clusters. This page provides the following details. A summary of the status/limitations of the current infrastructure to run Spark NLP for Healthcare training jobs and/or pre-annotation servers. Ability to delete a server and free up resources when required, so that another training job and/or pre-annotation server can be started. Shows details of the server Server Name: The name of server that can help identify it while running pre-annotation or importing files. License Used/Scope: The license that is being used in the server and its scope. Usage: Let the user know the usage of the server. A server can be used for pre-annotation, training, or OCR. Status: Status of training and pre-annotation servers. Deployed By: The user who deployed the server. This information might be useful for contacting the user who deployed a server before deleting it. Deployed At: Shows when the server was deployed. By default, only 1 server can be initialized for either pre-annotation or training even if there are multiple licenses present. To enable more than 1 servers to be initialized update the below configuration parameter in annotationlab-updater.sh script inside the artifacts folder and then re-run it. model_server.count=&lt;NUMBER_OF_SERVER_TO_INITIALIZE&gt; airflow.model_server.count=&lt;NUMBER_OF_SERVER_TO_INITIALIZE&gt; To run the script: sudo ./annotationlab-updater.sh Status of Training and Preannotation Server A new column, status, is added to the Clusters page that gives the status of training and pre-annotation servers. The available pre-annotation server statuses are: Idle Busy Stopped Users can visualize which servers are busy and which are idle. It is very useful information when the user intends to deploy a new server in replacement of an idle one. In this situation, the user can delete an idle server and deploy another pre-annotation/ training server. This information is also available on the pre-annotation popup when the user selects the deployed server to use for pre-annotation. Also, if any issues are encountered during server initialization, those are displayed on the tooltip accessible via mouse-over. Depending on the issue, changes might be required in the infrastructure settings, and the user will have to manually redeploy the training/pre-annotation server.",
    "url": "/docs/en/alab/cluster_management",
    "relUrl": "/docs/en/alab/cluster_management"
  },
  "26": {
    "id": "26",
    "title": "General Concepts",
    "content": "Concepts Spark ML provides a set of Machine Learning applications that can be build using two main components: Estimators and Transformers. The Estimators have a method called fit() which secures and trains a piece of data to such application. The Transformer is generally the result of a fitting process and applies changes to the the target dataset. These components have been embedded to be applicable to Spark NLP. Pipelines are a mechanism for combining multiple estimators and transformers in a single workflow. They allow multiple chained transformations along a Machine Learning task. For more information please refer to Spark ML library. Annotation The basic result of a Spark NLP operation is an annotation. It’s structure includes: annotatorType: the type of annotator that generated the current annotation begin: the begin of the matched content relative to raw-text end: the end of the matched content relative to raw-text result: the main output of the annotation metadata: content of matched result and additional information embeddings: (new in 2.0) contains vector mappings if required This object is automatically generated by annotators after a transform process. No manual work is required. However, it is important to clearly understand the structure of an annotation to be able too efficiently use it. Annotators Annotators are the spearhead of NLP functions in Spark NLP. There are two forms of annotators: Annotator Approaches: are those who represent a Spark ML Estimator and require a training stage. They have a function called fit(data) which trains a model based on some data. They produce the second type of annotator which is an annotator model or transformer. Annotator Models: are spark models or transformers, meaning they have a transform(data) function. This function takes as input a dataframe to which it adds a new column containing the result of the current annotation. All transformers are additive, meaning they append to current data, never replace or delete previous information. Both forms of annotators can be included in a Pipeline. All annotators included in a Pipeline will be automatically executed in the defined order and will transform the data accordingly. A Pipeline is turned into a PipelineModel after the fit() stage. The Pipeline can be saved to disk and re-loaded at any time. Common Functions setInputCols(column_names): Takes a list of column names of annotations required by this annotator. Those are generated by the annotators which precede the current annotator in the pipeline. setOutputCol(column_name): Defines the name of the column containing the result of the current annotator. Use this name as an input for other annotators down the pipeline requiring the outputs generated by the current annotator. Quickly annotate some text You can run these examples using Python or Scala. The easiest way to run the python examples is by starting a pyspark jupyter notebook including the spark-nlp package: $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.7 -y $ conda activate sparknlp # spark-nlp by default is based on pyspark 3.x $ pip install spark-nlp==4.3.1 pyspark==3.3.1 jupyter $ jupyter notebook Explain Document ML Spark NLP offers a variety of pretrained pipelines that will help you get started, and get a sense of how the library works. We are constantly working on improving the available content. You can checkout a demo application of the Explain Document ML pipeline here: View Demo Downloading and using a pretrained pipeline Explain Document ML (explain_document_ml) is a pretrained pipeline that does a little bit of everything NLP related. Let’s try it out in scala. Note that the first time you run the below code it might take longer since it downloads the pretrained pipeline from our servers! PythonScala import sparknlp sparknlp.start() from sparknlp.pretrained import PretrainedPipeline explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) annotations = explain_document_pipeline.annotate(&quot;We are very happy about SparkNLP&quot;) print(annotations) OUTPUT: { &#39;stem&#39;: [&#39;we&#39;, &#39;ar&#39;, &#39;veri&#39;, &#39;happi&#39;, &#39;about&#39;, &#39;sparknlp&#39;], &#39;checked&#39;: [&#39;We&#39;, &#39;are&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], &#39;lemma&#39;: [&#39;We&#39;, &#39;be&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], &#39;document&#39;: [&#39;We are very happy about SparkNLP&#39;], &#39;pos&#39;: [&#39;PRP&#39;, &#39;VBP&#39;, &#39;RB&#39;, &#39;JJ&#39;, &#39;IN&#39;, &#39;NNP&#39;], &#39;token&#39;: [&#39;We&#39;, &#39;are&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], &#39;sentence&#39;: [&#39;We are very happy about SparkNLP&#39;] } import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) OUTPUT: explain_document_ml download started this may take some time. Approximate size to download 9.4 MB Download done! Loading the resource. explain_document_pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_ml,en,public/models) val annotations = explainDocumentPipeline.annotate(&quot;We are very happy about SparkNLP&quot;) println(annotations) OUTPUT: Map( stem -&gt; List(we, ar, veri, happi, about, sparknlp), checked -&gt; List(We, are, very, happy, about, SparkNLP), lemma -&gt; List(We, be, very, happy, about, SparkNLP), document -&gt; List(We are very happy about SparkNLP), pos -&gt; ArrayBuffer(PRP, VBP, RB, JJ, IN, NNP), token -&gt; List(We, are, very, happy, about, SparkNLP), sentence -&gt; List(We are very happy about SparkNLP) ) As you can see the explain_document_ml is able to annotate any “document” providing as output a list of stems, check-spelling, lemmas, part of speech tags, tokens and sentence boundary detection and all this “out-of-the-box”!. Using a pretrained pipeline with spark dataframes You can also use the pipeline with a spark dataframe. You just need to create first a spark dataframe with a column named “text” that will work as the input for the pipeline and then use the .transform() method to run the pipeline over that dataframe and store the outputs of the different components in a spark dataframe. Remember than when starting jupyter notebook from pyspark or when running the spark-shell for scala, a Spark Session is started in the background by default within the namespace ‘scala’. PythonScala import sparknlp sparknlp.start() sentences = [ [&#39;Hello, this is an example sentence&#39;], [&#39;And this is a second sentence.&#39;] ] # spark is the Spark Session automatically started by pyspark. data = spark.createDataFrame(sentences).toDF(&quot;text&quot;) # Download the pretrained pipeline from Johnsnowlab&#39;s servers explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) OUTPUT: explain_document_ml download started this may take some time. Approx size to download 9.4 MB [OK!] # Transform &#39;data&#39; and store output in a new &#39;annotations_df&#39; dataframe annotations_df = explain_document_pipeline.transform(data) # Show the results annotations_df.show() OUTPUT: +--+--+--+--+--+--+--+--+ | text| document| sentence| token| checked| lemma| stem| pos| +--+--+--+--+--+--+--+--+ |Hello, this is an...|[[document, 0, 33...|[[document, 0, 33...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, he...|[[pos, 0, 4, UH, ...| |And this is a sec...|[[document, 0, 29...|[[document, 0, 29...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, an...|[[pos, 0, 2, CC, ...| +--+--+--+--+--+--+--+--+ val data = Seq( &quot;Hello, this is an example sentence&quot;, &quot;And this is a second sentence&quot;) .toDF(&quot;text&quot;) data.show(truncate=false) OUTPUT: ++ |text | ++ |Hello, this is an example set | |And this is a second sentence.| ++ val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) val annotations_df = explainDocumentPipeline.transform(data) annotations_df.show() OUTPUT: +--+--+--+--+--+--+--+--+ | text| document| sentence| token| checked| lemma| stem| pos| +--+--+--+--+--+--+--+--+ |Hello, this is an...|[[document, 0, 33...|[[document, 0, 33...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, he...|[[pos, 0, 4, UH, ...| |And this is a sec...|[[document, 0, 29...|[[document, 0, 29...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, an...|[[pos, 0, 2, CC, ...| +--+--+--+--+--+--+--+--+ Manipulating pipelines The output of the previous DataFrame was in terms of Annotation objects. This output is not really comfortable to deal with, as you can see by running the code: PythonScala annotations_df.select(&quot;token&quot;).show(truncate=False) OUTPUT: +--+ |token | +--+ |[[token, 0, 4, Hello, [sentence -&gt; 0], [], []], [token, 5, 5, ,, [sentence -&gt; 0], [], []], [token, 7, 10, this, [sentence -&gt; 0], [], []], [token, 12, 13, is, [sentence -&gt; 0], [], []], [token, 15, 16, an, [sentence -&gt; 0], [], []], [token, 18, 24, example, [sentence -&gt; 0], [], []], [token, 26, 33, sentence, [sentence -&gt; 0], [], []]]| |[[token, 0, 2, And, [sentence -&gt; 0], [], []], [token, 4, 7, this, [sentence -&gt; 0], [], []], [token, 9, 10, is, [sentence -&gt; 0], [], []], [token, 12, 12, a, [sentence -&gt; 0], [], []], [token, 14, 19, second, [sentence -&gt; 0], [], []], [token, 21, 28, sentence, [sentence -&gt; 0], [], []], [token, 29, 29, ., [sentence -&gt; 0], [], []]] | +--+ annotations_df.select(&quot;token&quot;).show(truncate=false) OUTPUT: +--+ |token | +--+ |[[token, 0, 4, Hello, [sentence -&gt; 0], [], []], [token, 5, 5, ,, [sentence -&gt; 0], [], []], [token, 7, 10, this, [sentence -&gt; 0], [], []], [token, 12, 13, is, [sentence -&gt; 0], [], []], [token, 15, 16, an, [sentence -&gt; 0], [], []], [token, 18, 24, example, [sentence -&gt; 0], [], []], [token, 26, 33, sentence, [sentence -&gt; 0], [], []]]| |[[token, 0, 2, And, [sentence -&gt; 0], [], []], [token, 4, 7, this, [sentence -&gt; 0], [], []], [token, 9, 10, is, [sentence -&gt; 0], [], []], [token, 12, 12, a, [sentence -&gt; 0], [], []], [token, 14, 19, second, [sentence -&gt; 0], [], []], [token, 21, 28, sentence, [sentence -&gt; 0], [], []], [token, 29, 29, ., [sentence -&gt; 0], [], []]] | +--+ What if we want to deal with just the resulting annotations? We can use the Finisher annotator, retrieve the Explain Document ML pipeline, and add them together in a Spark ML Pipeline. Remember that pretrained pipelines expect the input column to be named “text”. PythonScala from sparknlp import Finisher from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline finisher = Finisher().setInputCols([&quot;token&quot;, &quot;lemmas&quot;, &quot;pos&quot;]) explain_pipeline_model = PretrainedPipeline(&quot;explain_document_ml&quot;).model pipeline = Pipeline() .setStages([ explain_pipeline_model, finisher ]) sentences = [ [&#39;Hello, this is an example sentence&#39;], [&#39;And this is a second sentence.&#39;] ] data = spark.createDataFrame(sentences).toDF(&quot;text&quot;) model = pipeline.fit(data) annotations_finished_df = model.transform(data) annotations_finished_df.select(&#39;finished_token&#39;).show(truncate=False) OUTPUT: +-+ |finished_token | +-+ |[Hello, ,, this, is, an, example, sentence]| |[And, this, is, a, second, sentence, .] | +-+ scala&gt; import com.johnsnowlabs.nlp.Finisher scala&gt; import org.apache.spark.ml.Pipeline scala&gt; val finisher = new Finisher().setInputCols(&quot;token&quot;, &quot;lemma&quot;, &quot;pos&quot;) scala&gt; val explainPipelineModel = PretrainedPipeline(&quot;explain_document_ml&quot;).model scala&gt; val pipeline = new Pipeline(). setStages(Array( explainPipelineModel, finisher )) scala&gt; val data = Seq( &quot;Hello, this is an example sentence&quot;, &quot;And this is a second sentence&quot;) .toDF(&quot;text&quot;) scala&gt; val model = pipeline.fit(data) scala&gt; val annotations_df = model.transform(data) scala&gt; annotations_df.select(&quot;finished_token&quot;).show(truncate=false) OUTPUT: +-+ |finished_token | +-+ |[Hello, ,, this, is, an, example, sentence]| |[And, this, is, a, second, sentence, .] | +-+ Setup your own pipeline Annotator types Every annotator has a type. Those annotators that share a type, can be used interchangeably, meaning you could use any of them when needed. For example, when a token type annotator is required by another annotator, such as a sentiment analysis annotator, you can either provide a normalized token or a lemma, as both are of type token. Necessary imports Since version 1.5.0 we are making necessary imports easy to reach, base._ will include general Spark NLP transformers and concepts, while annotator._ will include all annotators that we currently provide. We also need Spark ML pipelines. PythonScala from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline DocumentAssembler: Getting data in In order to get through the NLP process, we need to get raw data annotated. There is a special transformer that does this for us: the DocumentAssembler, it creates the first annotation of type Document which may be used by annotators down the road. PythonScala documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val documentAssembler = new DocumentAssembler(). setInputCol(&quot;text&quot;). setOutputCol(&quot;document&quot;) Sentence detection and tokenization In this quick example, we now proceed to identify the sentences in the input document. SentenceDetector requires a Document annotation, which is provided by the DocumentAssembler output, and it’s itself a Document type token. The Tokenizer requires a Document annotation type. That means it works both with DocumentAssembler or SentenceDetector output. In the following example we use the sentence output. PythonScala sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;Sentence&quot;) regexTokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) val sentenceDetector = new SentenceDetector(). setInputCols(Array(&quot;document&quot;)). setOutputCol(&quot;sentence&quot;) val regexTokenizer = new Tokenizer(). setInputCols(Array(&quot;sentence&quot;)). setOutputCol(&quot;token&quot;) Spark NLP also includes another special transformer, called Finisher to show tokens in a human language. finisher = Finisher() .setInputCols([&quot;token&quot;]) .setCleanAnnotations(False) val finisher = new Finisher(). setInputCols(&quot;token&quot;). setCleanAnnotations(false) Finisher: Getting data out At the end of each pipeline or any stage that was done by Spark NLP, you may want to get results out whether onto another pipeline or simply write them on disk. The Finisher annotator helps you to clean the metadata (if it’s set to true) and output the results into an array: PythonScala finisher = Finisher() .setInputCols([&quot;token&quot;]) .setIncludeMetadata(True) val finisher = new Finisher() .setInputCols(&quot;token&quot;) .setIncludeMetadata(true) If you need to have a flattened DataFrame (each sub-array in a new column) from any annotations other than struct type columns, you can use explode function from Spark SQL. You can also use Apache Spark functions (SQL) to manipulate the output DataFrame in any way you need. Here we combine the tokens and NER results together: import pyspark.sql.functions as F df.withColumn(&quot;tmp&quot;, F.explode(&quot;chunk&quot;)).select(&quot;tmp.*&quot;) finisher.withColumn(&quot;newCol&quot;, explode(arrays_zip($&quot;finished_token&quot;, $&quot;finished_ner&quot;))) import org.apache.spark.sql.functions._ df.withColumn(&quot;tmp&quot;, explode(col(&quot;chunk&quot;))).select(&quot;tmp.*&quot;) Using Spark ML Pipeline Now we want to put all this together and retrieve the results, we use a Pipeline for this. We use the same data in fit() that we will use in transform since none of the pipeline stages have a training stage. PythonScala pipeline = Pipeline() .setStages([ documentAssembler, sentenceDetector, regexTokenizer, finisher ]) OUTPUT: +-+ |finished_token | +-+ |[hello, ,, this, is, an, example, sentence]| +-+ val pipeline = new Pipeline(). setStages(Array( documentAssembler, sentenceDetector, regexTokenizer, finisher )) val data = Seq(&quot;hello, this is an example sentence&quot;).toDF(&quot;text&quot;) val annotations = pipeline. fit(data). transform(data).toDF(&quot;text&quot;)) annotations.select(&quot;finished_token&quot;).show(truncate=false) OUTPUT: +-+ |finished_token | +-+ |[hello, ,, this, is, an, example, sentence]| +-+ Using Spark NLP’s LightPipeline LightPipeline is a Spark NLP specific Pipeline class equivalent to Spark ML Pipeline. The difference is that it’s execution does not hold to Spark principles, instead it computes everything locally (but in parallel) in order to achieve fast results when dealing with small amounts of data. This means, we do not input a Spark Dataframe, but a string or an Array of strings instead, to be annotated. To create Light Pipelines, you need to input an already trained (fit) Spark ML Pipeline. It’s transform() stage is converted into annotate() instead. PythonScala from sparknlp.base import LightPipeline explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) lightPipeline = LightPipeline(explain_document_pipeline.model) OUTPUT: explain_document_ml download started this may take some time. Approx size to download 9.4 MB [OK!] lightPipeline.annotate(&quot;Hello world, please annotate my text&quot;) OUTPUT: {&#39;stem&#39;: [&#39;hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;pleas&#39;, &#39;annot&#39;, &#39;my&#39;, &#39;text&#39;], &#39;checked&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;my&#39;, &#39;text&#39;], &#39;lemma&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;i&#39;, &#39;text&#39;], &#39;document&#39;: [&#39;Hello world, please annotate my text&#39;], &#39;pos&#39;: [&#39;UH&#39;, &#39;NN&#39;, &#39;,&#39;, &#39;VB&#39;, &#39;NN&#39;, &#39;PRP$&#39;, &#39;NN&#39;], &#39;token&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;my&#39;, &#39;text&#39;], &#39;sentence&#39;: [&#39;Hello world, please annotate my text&#39;]} import com.johnsnowlabs.nlp.base._ val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) val lightPipeline = new LightPipeline(explainDocumentPipeline.model) lightPipeline.annotate(&quot;Hello world, please annotate my text&quot;) OUTPUT: Map[String,Seq[String]] = Map( stem -&gt; List(hello, world, ,, pleas, annot, my, text), checked -&gt; List(Hello, world, ,, please, annotate, my, tex), lemma -&gt; List(Hello, world, ,, please, annotate, i, text), document -&gt; List(Hello world, please annotate my text), pos -&gt; ArrayBuffer(UH, NN, ,, VB, NN, PRP$, NN), token -&gt; List(Hello, world, ,, please, annotate, my, text), sentence -&gt; List(Hello world, please annotate my text) ) Training annotators Training methodology Training your own annotators is a key concept when dealing with real life scenarios. Any of the annotators provided above, such as pretrained pipelines and models, can be applied out-of-the-box to a specific use case, but better results are obtained when they are fine-tuned to your specific use-case. Dealing with real life problems ofter requires training your own models. In Spark NLP, we support three ways of training a custom annotator: Train from a dataset. Most annotators are capable of training from a dataset passed to fit() method just as Spark ML does. Annotators that use the suffix Approach are such trainable annotators. Training from fit() is the standard behavior in Spark ML. Annotators have different schema requirements for training. Check the reference to see what are the requirements of each annotators. Training from an external source: Some of our annotators train from an external file or folder passed to the annotator as a param. You will see such ones as setCorpus() or setDictionary() param setter methods, allowing you to configure the input to use. You can set Spark NLP to read them as Spark datasets or LINE_BY_LINE which is usually faster for small files. Last but not least, some of our annotators are Deep Learning based. These models may be trained with the standard AnnotatorApproach API just like any other annotator. For more advanced users, we also allow importing your own graphs or even training from Python and converting them into an AnnotatorModel. Spark NLP Imports base includes general Spark NLP transformers and concepts, annotator includes all annotators that we currently provide, embeddings includes word embedding annotators. Example: PythonScala from sparknlp.base import * from sparknlp.annotator import * from sparknlp.embeddings import * import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ Spark ML Pipelines SparkML Pipelines are a uniform structure that helps creating and tuning practical machine learning pipelines. Spark NLP integrates with them seamlessly so it is important to have this concept handy. Once a Pipeline is trained with fit(), it becomes a PipelineModel Example: PythonScala from pyspark.ml import Pipeline pipeline = Pipeline().setStages([...]) import org.apache.spark.ml.Pipeline new Pipeline().setStages(Array(...)) LightPipeline LightPipelines are Spark ML pipelines converted into a single machine but multithreaded task, becoming more than 10x times faster for smaller amounts of data (small is relative, but 50k sentences is roughly a good maximum). To use them, simply plug in a trained (fitted) pipeline. Example: PythonScala from sparknlp.base import LightPipeline LightPipeline(someTrainedPipeline).annotate(someStringOrArray) import com.johnsnowlabs.nlp.LightPipeline new LightPipeline(somePipelineModel).annotate(someStringOrArray)) Functions: annotate(string or string[]): returns dictionary list of annotation results fullAnnotate(string or string[]): returns dictionary list of entire annotations content For more details please refer to Using Spark NLP’s LightPipelines. RecursivePipeline Recursive pipelines are SparkNLP specific pipelines that allow a Spark ML Pipeline to know about itself on every Pipeline Stage task, allowing annotators to utilize this same pipeline against external resources to process them in the same way the user decides. Only some of our annotators take advantage of this. RecursivePipeline behaves exactly the same as normal Spark ML pipelines, so they can be used with the same intention. Example: PythonScala from sparknlp.annotator import * recursivePipeline = RecursivePipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, lemmatizer, finisher ]) import com.johnsnowlabs.nlp.RecursivePipeline val recursivePipeline = new RecursivePipeline() .setStages(Array( documentAssembler, sentenceDetector, tokenizer, lemmatizer, finisher )) Params and Features Annotator parameters SparkML uses ML Params to store pipeline parameter maps. In SparkNLP, we also use Features, which are a way to store parameter maps that are larger than just a string or a boolean. These features are serialized as either Parquet or RDD objects, allowing much faster and scalable annotator information. Features are also broadcasted among executors for better performance.",
    "url": "/docs/en/concepts",
    "relUrl": "/docs/en/concepts"
  },
  "27": {
    "id": "27",
    "title": "Contribute",
    "content": "Refer to our GitHub page to take a look at the GH Issues, as the project is yet small. You can create in there your own issues to either work on them yourself or simply propose them. Feel free to clone the repository locally and submit pull requests so we can review them and work together. feedback, ideas and bug reports testing and development training and testing nlp corpora documentation and research Help is always welcome, for any further questions, contact nlp@johnsnowlabs.com. Your own annotator model Creating your first annotator transformer should not be hard, here are a few guidelines to get you started. Lets assume we want a wrapper annotator, which puts a character surrounding tokens provided by a Tokenizer WordWrapper uid is utilized for transformer serialization, AnnotatorModel[MyAnnotator] will contain the common annotator logic We need to use standard constructor for java and python compatibility class WordWrapper(override val uid: String) extends AnnotatorModel[WordWrapper] { def this() = this(Identifiable.randomUID(&quot;WORD_WRAPPER&quot;)) } Annotator attributes This annotator is not flexible if we don’t provide parameters import com.johnsnowlabs.nlp.AnnotatorType._ override val annotatorType: AnnotatorType = TOKEN override val requiredAnnotatorTypes: Array[AnnotatorType] = Array[AnnotatorType](TOKEN) Annotator parameters This annotator is not flexible if we don’t provide parameters protected val character: Param[String] = new Param(this, &quot;character&quot;, &quot;this is the character used to wrap a token&quot;) def setCharacter(value: String): this.type = set(pattern, value) def getCharacter: String = $(pattern) setDefault(character, &quot;@&quot;) Annotator logic Here is how we act, annotations will automatically provide our required annotations We generally use annotatorType for metadata keys override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = { annotations.map(annotation =&gt; { Annotation( annotatorType, annotation.begin, annotation.end, Map(annotatorType -&gt; $(character) + annotation.result + $(character)) }) }",
    "url": "/contribute",
    "relUrl": "/contribute"
  },
  "28": {
    "id": "28",
    "title": "Databricks Solution Accelerators",
    "content": "",
    "url": "/databricks_solution_accelerators",
    "relUrl": "/databricks_solution_accelerators"
  },
  "29": {
    "id": "29",
    "title": "De-Identification - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/deidentification",
    "relUrl": "/deidentification"
  },
  "30": {
    "id": "30",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/demo",
    "relUrl": "/demo"
  },
  "31": {
    "id": "31",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/demos",
    "relUrl": "/demos"
  },
  "32": {
    "id": "32",
    "title": "Detect Sentiment & Emotion - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/detect_sentiment_emotion",
    "relUrl": "/detect_sentiment_emotion"
  },
  "33": {
    "id": "33",
    "title": "Developers Guideline",
    "content": "Spark NLP is an open-source library and everyone’s contribution is welcome! In this section we provide a guide on how to setup your environment using IntelliJ IDEA for a smoother start. You can also check our video tutorials available on our YouTube channel: https://www.youtube.com/johnsnowlabs Setting up the Environment Import to IntelliJ IDEA Setup Spark NLP development environment. This section will cover library set up for IntelliJ IDEA. Before you begin, make sure what you have Java and Spark installed in your system. We suggest that you have installed jdk 8 and Apache Spark 2.4.x. To check installation run: java -version and spark-submit --version Next step is to open IntelliJ IDEA. On the Welcome to IntelliJ IDEA screen you will see ability to Check out from Version Controle. Log in into your github account in pop up. After select from a list Spark NLP repo url: https://github.com/JohnSnowLabs/spark-nlp and press clone button. If you don’t see url in the list, clone or fork repo first to your Github account and try again. When the repo cloned IDE will detect SBT file with dependencies. Click Yes to start import from sbt. In the Import from sbt pop up make sure you have JDK 8 detected. Click Ok to proceed and download required resources. If you already had dependences installed you may see the pop up Not empty folder, click Ok to ignore it and reload resources. IntelliJ IDEA will be open and it will start syncing SBT project. It make take some time, you will see the progress in the build output panel in the bottom of the screen. To see the project panel in the left press Alt+1. Next step is to install Python plugin to the IntelliJ IDEA. To do this, open File -&gt; Settings -&gt; Plugins, type Python in the search and select Python plugin by JetBrains. Install this plugin by clicking Install button. After this steps you can check project structure in the File -&gt; Project Structure -&gt; Modules. Make sure what you have spark-nlp and spark-nlp-build folders and no errors in the exported dependencies. In the Project settings check what project SDK is set to 1.8 and in Platform Settings -&gt; SDK&#39;s you have Java installation as well as Python installation. If you don’t see Python installed in the SDK&#39;s tab click + button, add Python SDK with new virtual environment in the project folder with Python 3.x. Compiling, assembly and unit testing Run tests in Scala Click Add configuration in the Top right corner. In the pop up click on the + and look for sbt task. In the Name field put Test. In the Tasks field write down test. After you can disable checkbox in Use sbt shell to have more custom configurations. In the VM parameters increase the memory by changing -Xmx1024M to -Xmx10G and click Ok. If everything was set up correctly you suhould see unabled green button Run ‘Test’ in the top right. Click on it to start running the tests. This algorithm will Run all tests under spark-nlp/src/test/scala/com.johnsnowlabs/ Copy tasks After you created task, click Edit configuration. Select target task and instead of + button you can click copy in the same menu. It will recreate all settings from parent task and create a new task. You can do it for Scala or for Python tasks. Run individual tests Open test file you want to run. For example, spark-nlp/src/test/scala/com.johnsnowlabs/nlp/FinisherTestSpec.scala. Right click on the class name and select Copy reference. It will copy to you buffer classpath - com.johnsnowlabs.nlp.FinisherTestSpec. Copy existing Scala task and Name it as FinisherTest. In the Tasks field write down &quot;testOnly *classpath*&quot; -&gt; &quot;testOnly com.johnsnowlabs.nlp.FinisherTestSpec&quot; and click Ok to save individual scala test run configuration. Press play button to run individual test. Debugging tests To run tests in debug mode click Debug button (next to play button). In this mode task will stop at the given break points. Run tests in Python To run Python test, first you need to configure project structure. Go to File -&gt; Project Settings -&gt; Modules, click on the + button and select New Module. In the pop up choose Python on left menu, select Python SDK from created virtual environment and click Next. Enter python in the Module name and click Finish. After you need to add Spark dependencies. Select created Python module and click on the + button in the Dependencies part. Choose Jars or directories… and find the find installation path of spark (usually the folder name is spark-2.4.5-bin-hadoop2.7). In the Spark folder go to the python/libs and select pyspark.zip to the project. Do the same for another file in the same folder - py4j-0.10.7-src.zip. All available tests are in spark-nlp/python/run-tests.py. Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and look for Python. In the Script path locate file spark-nlp/python/run-tests.py. Also you need to add SPARK_HOME environment variable to the project. Choose Environment variables and add new variable SPARK_HOME. Insert installation path of spark to the Value field. Click Ok to save and close pop up and click Ok to confirm new task creation. Before running the tests we need to install requered python dependencies in the new virtual environment. Select in the bottom menu Terminal and activate your environment with command source venv/bin/activate after install packages by running pip install pyspark==3.3.1 numpy Compiling jar Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and select sbt task. In the Name field put AssemblyCopy. In the Tasks field write down assemblyAndCopy. After you can disable checkbox in Use sbt shell to have more custom configurations. In the VM parameters increase the memory by changing -Xmx1024M to -Xmx6G and click Ok. You can find created jar in the folder spark-nlp/python/lib/sparknlp.jar Note: Assembly command creates a fat jars, that includes all dependencies within Compiling pypi, whl Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and select sbt task. In the Name field put AssemblyAndCopyForPyPi. In the Tasks field write down assemblyAndCopyForPyPi. Then you go to spark-nlp/python/ directory and run: python setup.py sdist bdist_wheel You can find created whl and tar.gz in the folder spark-nlp/python/dist/. Use this files to install spark-nlp locally: pip install spark_nlp-2.x.x-py3-none-any.whl",
    "url": "/docs/en/developers",
    "relUrl": "/docs/en/developers"
  },
  "34": {
    "id": "34",
    "title": "Diagnoses & Procedures - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/diagnoses_procedures",
    "relUrl": "/diagnoses_procedures"
  },
  "35": {
    "id": "35",
    "title": "Spark NLP Display",
    "content": "Getting started Spark NLP Display is an open-source python library for visualizing the annotations generated with Spark NLP. It currently offers out-of-the-box suport for the following types of annotations: Dependency Parser Named Entity Recognition Entity Resolution Relation Extraction Assertion Status The ability to quickly visualize the entities/relations/assertion statuses, etc. generated using Spark NLP is a very useful feature for speeding up the development process as well as for understanding the obtained results. Getting all of this in a one liner is extremelly convenient especially when running Jupyter notebooks which offers full support for html visualizations. The visualisation classes work with the outputs returned by both Pipeline.transform() function and LightPipeline.fullAnnotate(). Install Spark NLP Display You can install the Spark NLP Display library via pip by using: pip install spark-nlp-display A complete guideline on how to use the Spark NLP Display library is available here. Visualize a dependency tree For visualizing a dependency trees generated with DependencyParserApproach you can use the following code. from sparknlp_display import DependencyParserVisualizer dependency_vis = DependencyParserVisualizer() dependency_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe. pos_col = &#39;pos&#39;, #specify the pos column dependency_col = &#39;dependency&#39;, #specify the dependency column dependency_type_col = &#39;dependency_type&#39; #specify the dependency type column ) The following image gives an example of html output that is obtained for a test sentence: Visualize extracted named entities The NerVisualizer highlights the named entities that are identified by Spark NLP and also displays their labels as decorations on top of the analyzed text. The colors assigned to the predicted labels can be configured to fit the particular needs of the application. from sparknlp_display import NerVisualizer ner_vis = NerVisualizer() ner_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe label_col=&#39;entities&#39;, #specify the entity column document_col=&#39;document&#39; #specify the document column (default: &#39;document&#39;) labels=[&#39;PER&#39;] #only allow these labels to be displayed. (default: [] - all labels will be displayed) ) ## To set custom label colors: ner_vis.set_label_colors({&#39;LOC&#39;:&#39;#800080&#39;, &#39;PER&#39;:&#39;#77b5fe&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences: Visualize relations The RelationExtractionVisualizer can be used to visualize the relations predicted by Spark NLP. The two entities involved in a relation will be highlighted and their label will be displayed. Also a directed and labeled arc(line) will be used to connect the two entities. from sparknlp_display import RelationExtractionVisualizer re_vis = RelationExtractionVisualizer() re_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe relation_col = &#39;relations&#39;, #specify relations column document_col = &#39;document&#39;, #specify document column show_relations=True #display relation names on arrows (default: True) ) The following image gives an example of html output that is obtained for a couple of test sentences: Visualize assertion status The AssertionVisualizer is a special type of NerVisualizer that also displays on top of the labeled entities the assertion status that was infered by a Spark NLP model. from sparknlp_display import AssertionVisualizer assertion_vis = AssertionVisualizer() assertion_vis.display(pipeline_result[0], label_col = &#39;entities&#39;, #specify the ner result column assertion_col = &#39;assertion&#39; #specify assertion column document_col = &#39;document&#39; #specify the document column (default: &#39;document&#39;) ) ## To set custom label colors: assertion_vis.set_label_colors({&#39;TREATMENT&#39;:&#39;#008080&#39;, &#39;problem&#39;:&#39;#800080&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences: Visualize entity resolution Entity resolution refers to the normalization of named entities predicted by Spark NLP with respect to standard terminologies such as ICD-10, SNOMED, RxNorm etc. You can read more about the available entity resolvers here. The EntityResolverVisualizer will automatically display on top of the NER label the standard code (ICD10 CM, PCS, ICDO; CPT) that corresponds to that entity as well as the short description of the code. If no resolution code could be identified a regular NER-type of visualization will be displayed. from sparknlp_display import EntityResolverVisualizer er_vis = EntityResolverVisualizer() er_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe label_col=&#39;entities&#39;, #specify the ner result column resolution_col = &#39;resolution&#39; document_col=&#39;document&#39; #specify the document column (default: &#39;document&#39;) ) ## To set custom label colors: er_vis.set_label_colors({&#39;TREATMENT&#39;:&#39;#800080&#39;, &#39;PROBLEM&#39;:&#39;#77b5fe&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences:",
    "url": "/docs/en/display",
    "relUrl": "/docs/en/display"
  },
  "36": {
    "id": "36",
    "title": "John Snow Labs - NLP Documentation",
    "content": "",
    "url": "/docs",
    "relUrl": "/docs"
  },
  "37": {
    "id": "37",
    "title": "Drugs & Adverse Events - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/drug_adverse_events",
    "relUrl": "/drug_adverse_events"
  },
  "38": {
    "id": "38",
    "title": "East Asian Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/east_asian_languages",
    "relUrl": "/east_asian_languages"
  },
  "39": {
    "id": "39",
    "title": "Embeddings",
    "content": "All the embeddings available in the Annotation Lab are listed on this page. General information about the embeddings like the name, version, source, and date of upload/download is available. Like models, any compatible embeddings can be downloaded from NLP Models Hub. By default, glove_100d, bert_base_cased, tfhub_use embeddings are included in every fresh installation of Annotation Lab. Custom Embeddings Upload Custom embeddings can be uploaded using the Upload button present in the top right corner of the page. Note: The embeddings to upload need to be Spark NLP compatible.",
    "url": "/docs/en/alab/embeddings",
    "relUrl": "/docs/en/alab/embeddings"
  },
  "40": {
    "id": "40",
    "title": "Enhance Low-Quality Images - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/enhance_low_quality_images",
    "relUrl": "/enhance_low_quality_images"
  },
  "41": {
    "id": "41",
    "title": "European Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/european_languages",
    "relUrl": "/european_languages"
  },
  "42": {
    "id": "42",
    "title": "Evaluation",
    "content": "Spark NLP Evaluation This module includes tools to evaluate the accuracy of annotators and visualize the parameters used on training. It includes specific metrics for each annotator and its training time. The results will display on the console or to an MLflow tracking UI. Just with a simple import you can start using eval module. Check how to setup MLflow UI See here on eval folder if you want to check specific running examples. Example: PythonScala from sparknlp_jsl.eval import * import com.johnsnowlabs.nlp.eval._ Evaluating Norvig Spell Checker You can evaluate this spell checker either by training an annotator or by using a pretrained model. spark: Spark session. trainFile: A corpus of documents with correctly spell words. testFile: A corpus of documents with misspells words. groundTruthFile: The same corpus used on testFile but with correctly spell words. Train File Example: Any document that you prefer with correctly spell words. Test File Example: My siter go to Munich. Ground Truth File Example: My sister goes to Munich. Example for annotator: PythonScala spell = NorvigSweetingApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;checked&quot;) .setDictionary(dictionary_file) norvigSpellEvaluation = NorvigSpellEvaluation(spark, test_file, ground_truth_file) norvigSpellEvaluation.computeAccuracyAnnotator(train_file, spell) val spell = new NorvigSweetingApproach() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;checked&quot;) .setDictionary(dictionary_file) val norvigSpellEvaluation = new NorvigSpellEvaluation(spark, testFile, groundTruthFile) norvigSpellEvaluation.computeAccuracyAnnotator(trainFile, spell) Example for pretrained model: PythonScala spell = NorvigSweetingModel.pretrained() norvigSpellEvaluation = NorvigSpellEvaluation(spark, test_file, ground_truth_file) norvigSpellEvaluation.computeAccuracyModel(spell) val spell = NorvigSweetingModel.pretrained() val norvigSpellEvaluation = new NorvigSpellEvaluation(spark, testFile, groundTruthFile) norvigSpellEvaluation.computeAccuracyModel(spell) Evaluating Symmetric Spell Checker You can evaluate this spell checker either by training an annotator or by using a pretrained model. spark: Spark session trainFile: A corpus of documents with correctly spell words. testFile: A corpus of documents with misspells words. groundTruthFile: The same corpus used on testFile but with correctly spell words. Train File Example: Any document that you prefer with correctly spell words. Test File Example: My siter go to Munich. Ground Truth File Example: My sister goes to Munich. Example for annotator: PythonScala spell = SymmetricDeleteApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;checked&quot;) .setDictionary(dictionary_file) symSpellEvaluation = SymSpellEvaluation(spark, test_file, ground_truth_file) symSpellEvaluation.computeAccuracyAnnotator(train_file, spell) val spell = new SymmetricDeleteApproach() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;checked&quot;) val symSpellEvaluation = new SymSpellEvaluation(spark, testFile, groundTruthFile) symSpellEvaluation.computeAccuracyAnnotator(trainFile, spell) Example for pretrained model: PythonScala spell = SymmetricDeleteModel.pretrained() symSpellEvaluation = NorvigSpellEvaluation(spark, test_file, ground_truth_file) symSpellEvaluation.computeAccuracyModel(spell) val spell = SymmetricDeleteModel.pretrained() val symSpellEvaluation = new SymSpellEvaluation(spark, testFile, groundTruthFile) symSpellEvaluation.computeAccuracyModel(spell) Evaluating NER DL You can evaluate NER DL when training an annotator. spark: Spark session. trainFile: Files with labeled NER entities for training. testFile: Files with labeled NER entities for testing. These files are used to evaluate the model. So, it’s used for prediction and the labels as ground truth. tagLevel: The granularity of tagging when measuring accuracy on entities. Set “IOB” to include inside and beginning, empty to ignore it. For example to display accuracy for entity I-PER and B-PER set “IOB” whereas just for entity PER set it as an empty string. Example: PythonScala embeddings = WordEmbeddings() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setEmbeddingsSource(&quot;glove.6B.100d.txt&quot;, 100, &quot;TEXT&quot;) ner_approach = NerDLApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) .setRandomSeed(0) nerDLEvaluation = NerDLEvaluation(spark, test_File, tag_level) nerDLEvaluation.computeAccuracyAnnotator(train_file, ner_approach, embeddings) val embeddings = new WordEmbeddings() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setEmbeddingsSource(&quot;glove.6B.100d.txt&quot;, 100, WordEmbeddingsFormat.TEXT) val nerApproach = new NerDLApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) .setRandomSeed(0) val nerDLEvaluation = new NerDLEvaluation(spark, testFile, tagLevel) nerDLEvaluation.computeAccuracyAnnotator(trainFile, nerApproach, embeddings) Example for pretrained model: PythonScala ner_dl = NerDLModel.pretrained() nerDlEvaluation = NerDLEvaluation(spark, test_File, tag_level) nerDlEvaluation.computeAccuracyModel(ner_dl) val nerDl = NerDLModel.pretrained() val nerDlEvaluation = NerDLEvaluation(spark, testFile, tagLevel) nerDlEvaluation.computeAccuracyModel(nerDl) Evaluating NER CRF You can evaluate NER CRF when training an annotator. spark: Spark session. trainFile: Files with labeled NER entities for training. testFile: Files with labeled NER entities for testing. These files are used to evaluate the model. So, it’s used for prediction and the labels as ground truth. format: The granularity of tagging when measuring accuracy on entities. Set “IOB” to include inside and beginning, empty to ignore it. For example to display accuracy for entity I-PER and B-PER set “IOB” whereas just for entity PER set it as an empty string. Example: PythonScala embeddings = WordEmbeddings() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setEmbeddingsSource(&quot;glove.6B.100d.txt&quot;, 100, &quot;TEXT&quot;) ner_approach = NerCrfApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) .setRandomSeed(0) nerCrfEvaluation = NerCrfEvaluation(spark, test_File, tag_level) nerCrfEvaluation.computeAccuracyAnnotator(train_file, ner_approach, embeddings) val embeddings = new WordEmbeddings() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setEmbeddingsSource(&quot;./glove.6B.100d.txt &quot;, 100, WordEmbeddingsFormat.TEXT) .setCaseSensitive(true) val nerTagger = new NerCrfApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;pos&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(10) val nerCrfEvaluation = new NerCrfEvaluation(testFile, format) nerCrfEvaluation.computeAccuracyAnnotator(trainFile, nerTagger, embeddings) Example for pretrained model: PythonScala ner_crf = NerCrfModel.pretrained() nerCrfEvaluation = NerCrfEvaluation(spark, test_File, tag_level) nerCrfEvaluation.computeAccuracyModel(ner_crf) nerCrf = NerCrfModel.pretrained() nerCrfEvaluation = NerCrfEvaluation(spark, testFile, tagLevel) nerCrfEvaluation.computeAccuracyModel(nerCrf) Evaluating POS Tagger You can evaluate POS either by training an annotator or by using a pretrained model. spark: Spark session. trainFile: A labeled POS file see and example here. testFile: A CoNLL-U format file. Example for annotator: PythonScala pos_tagger = PerceptronApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) .setNIterations(2) posEvaluation = POSEvaluation(spark, test_file) posEvaluation.computeAccuracyAnnotator(train_file, pos_tagger) val posTagger = new PerceptronApproach() .setInputCols(Array(&quot;document&quot;, &quot;token&quot;)) .setOutputCol(&quot;pos&quot;) .setNIterations(2) val posEvaluation = new POSEvaluation(spark, testFile) posEvaluation.computeAccuracyAnnotator(trainFile, posTagger)",
    "url": "/docs/en/evaluation",
    "relUrl": "/docs/en/evaluation"
  },
  "43": {
    "id": "43",
    "title": "Examples",
    "content": "Showcasing notebooks and codes of how to use Spark NLP in Python and Scala. Python Setup $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.7 -y $ conda activate sparknlp $ pip install spark-nlp==4.3.1 pyspark==3.3.1 Google Colab Notebook Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account. Run the following code in Google Colab notebook and start using spark-nlp right away. # This is only to setup PySpark and Spark NLP on Colab !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash This script comes with the two options to define pyspark and spark-nlp versions via options: # -p is for pyspark # -s is for spark-nlp # by default they are set to the latest !bash colab.sh -p 3.2.3 -s 4.3.1 Spark NLP quick start on Google Colab is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines. Kaggle Kernel Run the following code in Kaggle Kernel and start using spark-nlp right away. # Let&#39;s setup Kaggle for Spark NLP and PySpark !wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash Notebooks Tutorials and articles Jupyter Notebooks",
    "url": "/docs/en/examples",
    "relUrl": "/docs/en/examples"
  },
  "44": {
    "id": "44",
    "title": "Annotations Export",
    "content": "Annotations can be exported in various format for storage and later use. You can export the annotations applied to the tasks of any project by going to the Tasks page and clicking on the Export button on the top-right corner of this page. You will be navigated to the Export page and from there you can select the format and configure the export options to export the annotations to a file/s. Supported Formats for Text Projects The completions and predictions are stored in a database for fast search and access. Completions and predictions can be exported into the formats described below. JSON You can export the manual annotations (completions) and automatic annotations (predictions) to JSON format using the JSON option on the Export page. An example of JSON export file is shown below: [ { &quot;completions&quot;: [ { &quot;created_username&quot;: &quot;eric&quot;, &quot;created_ago&quot;: &quot;2022-10-29T14:42:50.867Z&quot;, &quot;lead_time&quot;: 82, &quot;result&quot;: [ { &quot;value&quot;: { &quot;start&quot;: 175, &quot;end&quot;: 187, &quot;text&quot;: &quot;tuberculosis&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.9524 }, &quot;id&quot;: &quot;zgam2AbdmY&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 213, &quot;end&quot;: 239, &quot;text&quot;: &quot;Mycobacterium tuberculosis&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 0.904775 }, &quot;id&quot;: &quot;1v76SqlWtj&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 385, &quot;end&quot;: 394, &quot;text&quot;: &quot;pneumonia&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.91655 }, &quot;id&quot;: &quot;CURKae4Eca&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 436, &quot;end&quot;: 449, &quot;text&quot;: &quot;Streptococcus&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 0.9157500000000001 }, &quot;id&quot;: &quot;cM5BvAsZL4&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 454, &quot;end&quot;: 465, &quot;text&quot;: &quot;Pseudomonas&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 0.91495 }, &quot;id&quot;: &quot;KGOLhb8OPV&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 532, &quot;end&quot;: 540, &quot;text&quot;: &quot;Shigella&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 0.91655 }, &quot;id&quot;: &quot;JCIhVQTDZl&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 542, &quot;end&quot;: 555, &quot;text&quot;: &quot;Campylobacter&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 0.9163 }, &quot;id&quot;: &quot;CkxrbwvFzb&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 561, &quot;end&quot;: 571, &quot;text&quot;: &quot;Salmonella&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 0.9164000000000001 }, &quot;id&quot;: &quot;c6ev6McH4Z&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 623, &quot;end&quot;: 630, &quot;text&quot;: &quot;tetanus&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.97 }, &quot;id&quot;: &quot;9ZmEaJnqKG&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 632, &quot;end&quot;: 645, &quot;text&quot;: &quot;typhoid fever&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.976675 }, &quot;id&quot;: &quot;Uo5CWzdd1S&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 647, &quot;end&quot;: 657, &quot;text&quot;: &quot;diphtheria&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.9737 }, &quot;id&quot;: &quot;7nc71jXT3P&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 659, &quot;end&quot;: 667, &quot;text&quot;: &quot;syphilis&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.97355 }, &quot;id&quot;: &quot;nIKfsOWNyE&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 673, &quot;end&quot;: 689, &quot;text&quot;: &quot;Hansen&#39;s disease&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.899025 }, &quot;id&quot;: &quot;SyuVYMn7ax&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 30, &quot;end&quot;: 38, &quot;text&quot;: &quot;bacteria&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 1 }, &quot;id&quot;: &quot;lq7qtJj1yX&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 98, &quot;end&quot;: 106, &quot;text&quot;: &quot;bacteria&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 1 }, &quot;id&quot;: &quot;kxaB_gMstN&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; } ], &quot;honeypot&quot;: true, &quot;copied_from&quot;: &quot;prediction: 11001&quot;, &quot;id&quot;: 11001, &quot;confidence_range&quot;: [ 0, 1 ], &quot;copy&quot;: true, &quot;cid&quot;: &quot;11001&quot;, &quot;data_type&quot;: &quot;prediction&quot;, &quot;updated_at&quot;: &quot;2022-10-29T15:13:03.445569Z&quot;, &quot;updated_by&quot;: &quot;eric&quot;, &quot;submitted_at&quot;: &quot;2022-10-30T20:57:54.303&quot; }, { &quot;created_username&quot;: &quot;jenny&quot;, &quot;created_ago&quot;: &quot;2022-10-29T15:03:51.669Z&quot;, &quot;lead_time&quot;: 0, &quot;result&quot;: [ { &quot;value&quot;: { &quot;start&quot;: 175, &quot;end&quot;: 187, &quot;text&quot;: &quot;tuberculosis&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.9524 }, &quot;id&quot;: &quot;zgam2AbdmY&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 213, &quot;end&quot;: 239, &quot;text&quot;: &quot;Mycobacterium tuberculosis&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 0.904775 }, &quot;id&quot;: &quot;1v76SqlWtj&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 385, &quot;end&quot;: 394, &quot;text&quot;: &quot;pneumonia&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.91655 }, &quot;id&quot;: &quot;CURKae4Eca&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 436, &quot;end&quot;: 449, &quot;text&quot;: &quot;Streptococcus&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 0.9157500000000001 }, &quot;id&quot;: &quot;cM5BvAsZL4&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 454, &quot;end&quot;: 465, &quot;text&quot;: &quot;Pseudomonas&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 0.91495 }, &quot;id&quot;: &quot;KGOLhb8OPV&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 532, &quot;end&quot;: 540, &quot;text&quot;: &quot;Shigella&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 0.91655 }, &quot;id&quot;: &quot;JCIhVQTDZl&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 542, &quot;end&quot;: 555, &quot;text&quot;: &quot;Campylobacter&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 0.9163 }, &quot;id&quot;: &quot;CkxrbwvFzb&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 561, &quot;end&quot;: 571, &quot;text&quot;: &quot;Salmonella&quot;, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;confidence&quot;: 0.9164000000000001 }, &quot;id&quot;: &quot;c6ev6McH4Z&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 623, &quot;end&quot;: 630, &quot;text&quot;: &quot;tetanus&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.97 }, &quot;id&quot;: &quot;9ZmEaJnqKG&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 632, &quot;end&quot;: 645, &quot;text&quot;: &quot;typhoid fever&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.976675 }, &quot;id&quot;: &quot;Uo5CWzdd1S&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 647, &quot;end&quot;: 657, &quot;text&quot;: &quot;diphtheria&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.9737 }, &quot;id&quot;: &quot;7nc71jXT3P&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 659, &quot;end&quot;: 667, &quot;text&quot;: &quot;syphilis&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.97355 }, &quot;id&quot;: &quot;nIKfsOWNyE&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; }, { &quot;value&quot;: { &quot;start&quot;: 673, &quot;end&quot;: 689, &quot;text&quot;: &quot;Hansen&#39;s disease&quot;, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;confidence&quot;: 0.899025 }, &quot;id&quot;: &quot;SyuVYMn7ax&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; } ], &quot;honeypot&quot;: true, &quot;confidence_range&quot;: [ 0, 1 ], &quot;submitted_at&quot;: &quot;2022-10-29T20:48:51.669&quot;, &quot;id&quot;: 11002 } ], &quot;predictions&quot;: [ { &quot;created_username&quot;: &quot;SparkNLP Pre-annotation&quot;, &quot;result&quot;: [ { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;zgam2AbdmY&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 187, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;start&quot;: 175, &quot;text&quot;: &quot;tuberculosis&quot;, &quot;confidence&quot;: &quot;0.9524&quot; } }, { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;1v76SqlWtj&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 239, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;start&quot;: 213, &quot;text&quot;: &quot;Mycobacterium tuberculosis&quot;, &quot;confidence&quot;: &quot;0.904775&quot; } }, { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;CURKae4Eca&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 394, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;start&quot;: 385, &quot;text&quot;: &quot;pneumonia&quot;, &quot;confidence&quot;: &quot;0.91655&quot; } }, { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;cM5BvAsZL4&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 449, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;start&quot;: 436, &quot;text&quot;: &quot;Streptococcus&quot;, &quot;confidence&quot;: &quot;0.9157500000000001&quot; } }, { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;KGOLhb8OPV&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 465, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;start&quot;: 454, &quot;text&quot;: &quot;Pseudomonas&quot;, &quot;confidence&quot;: &quot;0.91495&quot; } }, { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;JCIhVQTDZl&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 540, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;start&quot;: 532, &quot;text&quot;: &quot;Shigella&quot;, &quot;confidence&quot;: &quot;0.91655&quot; } }, { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;CkxrbwvFzb&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 555, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;start&quot;: 542, &quot;text&quot;: &quot;Campylobacter&quot;, &quot;confidence&quot;: &quot;0.9163&quot; } }, { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;c6ev6McH4Z&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 571, &quot;labels&quot;: [ &quot;Pathogen&quot; ], &quot;start&quot;: 561, &quot;text&quot;: &quot;Salmonella&quot;, &quot;confidence&quot;: &quot;0.9164000000000001&quot; } }, { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;9ZmEaJnqKG&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 630, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;start&quot;: 623, &quot;text&quot;: &quot;tetanus&quot;, &quot;confidence&quot;: &quot;0.97&quot; } }, { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;Uo5CWzdd1S&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 645, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;start&quot;: 632, &quot;text&quot;: &quot;typhoid fever&quot;, &quot;confidence&quot;: &quot;0.976675&quot; } }, { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;7nc71jXT3P&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 657, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;start&quot;: 647, &quot;text&quot;: &quot;diphtheria&quot;, &quot;confidence&quot;: &quot;0.9737&quot; } }, { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;nIKfsOWNyE&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 667, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;start&quot;: 659, &quot;text&quot;: &quot;syphilis&quot;, &quot;confidence&quot;: &quot;0.97355&quot; } }, { &quot;from_name&quot;: &quot;label&quot;, &quot;id&quot;: &quot;SyuVYMn7ax&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 689, &quot;labels&quot;: [ &quot;MedicalCondition&quot; ], &quot;start&quot;: 673, &quot;text&quot;: &quot;Hansen&#39;s disease&quot;, &quot;confidence&quot;: &quot;0.899025&quot; } } ], &quot;created_ago&quot;: &quot;2022-10-29T14:07:58.553246Z&quot;, &quot;id&quot;: 11001 } ], &quot;created_at&quot;: &quot;2022-10-29 14:07:12&quot;, &quot;created_by&quot;: &quot;admin&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;Although the vast majority of bacteria are harmless or beneficial to one&#39;s body, a few pathogenic bacteria can cause infectious diseases. The most common bacterial disease is tuberculosis, caused by the bacterium Mycobacterium tuberculosis, which affects about 2 million people mostly in sub-Saharan Africa. Pathogenic bacteria contribute to other globally important diseases, such as pneumonia, which can be caused by bacteria such as Streptococcus and Pseudomonas, and foodborne illnesses, which can be caused by bacteria such as Shigella, Campylobacter, and Salmonella. Pathogenic bacteria also cause infections such as tetanus, typhoid fever, diphtheria, syphilis, and Hansen&#39;s disease. They typically range between 1 and 5 micrometers in length.&quot;, &quot;title&quot;: &quot;cord19-11.txt&quot; }, &quot;id&quot;: 11 } ] Below are some explanations related to the structure of the JSON export file. The export represents a list/array of task, containing completions and/or predictions. Each task in the list has the following main elements: TASK: A task can have 0 or several completions and 0 or several predictions. { &quot;completions&quot;: [COMPLETION1, COMPLETION2, ...], &quot;predictions&quot;: [PREDICTION1, PREDICTION2, ...], &quot;created_at&quot;: &quot;2022-07-04 06:17:26&quot;, &quot;created_by&quot;: &quot;admin&quot;, &quot;data&quot;: { &quot;text&quot;: &lt;sample_text&gt;&quot; }, &quot;id&quot;: 1 } completions: list of completions (manual annotations) predictions: list of predictions (annotations generated by spark-nlp model(s)) created_by: time stamp representing the creation time for a task created_by: the user who created the task data: input data on which the annotations/preannotation are defined (text/image/audio etc) id: task ID COMPLETION/PREDICTION: Completions and predictions have the following structure: { &quot;created_username&quot;: &quot;collaborate&quot;, &quot;created_ago&quot;: &quot;2022-07-04T06:18:39.720155Z&quot;, &quot;lead_time&quot;: 11, &quot;result&quot;: [RESULT1, RESULT2, RESULT3, ....], &quot;honeypot&quot;: true, &quot;id&quot;: 1001, &quot;updated_at&quot;: &quot;2022-07-04T06:18:49.037150Z&quot;, &quot;updated_by&quot;: &quot;collaborate&quot;, } created_username: user who created the annotation created_ago: timestamp of when the annotation was created lead_time: time taken (in seconds) to create this annotation (valid for manual completion only) result: list of annotated labels honeypot: boolean value to set/unset ground truth id: completion/prediction ID updated_at: timestamp of when the annotation was last updated updated_by: user who updated the annotation Each completion/prediction contains one or several results which can be seen as individual annotations. RESULT: The structure of the RESULT dictionary differs according to the project configuration: 1. NER: { &quot;value&quot;: { &quot;start&quot;: 17, &quot;end&quot;: 25, &quot;text&quot;: &quot;pleasant&quot;, &quot;labels&quot;: [ &quot;FAC&quot; ], &quot;confidence&quot;: 1 }, &quot;id&quot;: &quot;iJOo_XgIao&quot;, &quot;from_name&quot;: &quot;label&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot; } value: start: start index of the annotated chunk end: end index of the annotated chunk text: annotated chunk labels: associated label confidence: confidence score (1 for manual annotation and a value between 0 and 1 for predicted annotations) id: id of annotation (used while creating relations between entities) from_name/to_name: this attribute is set according to the project config: from_name -&gt; name attribute of the Labels tag to_name -&gt; toName attribute of the Labels tag &lt;Labels name=&quot;label&quot; toName=&quot;text&quot;&gt; type: type of the annotation (labels, choices, relations etc.) 2. Classification: { &quot;value&quot;: { &quot;choices&quot;: [ &quot;sadness&quot; ], &quot;confidence&quot;: 1 }, &quot;id&quot;: &quot;VyY-OHe_lf&quot;, &quot;from_name&quot;: &quot;surprise&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;choices&quot; } value: choices: the options/choises selected by users confidence: confidence score (1 for manual annotation and between 0 and 1 for predicted annotation) id: id of annotation from_name/to_name: this field is set according to the project config: `from_name` -&gt; name attribute of the Choice tag `to_name` -&gt; toName attribute of the Choice tag &lt;Choices name=&quot;surprise&quot; toName=&quot;text&quot; choice=&quot;single&quot;&gt; type: type of the annotation (labels, choices, relations etc) 3. Relations: The information below is added in the RESULT section: { &quot;from_id&quot;: &quot;ucfP3c4xWg&quot;, &quot;to_id&quot;: &quot;IlWac4TdFx&quot;, &quot;type&quot;: &quot;relation&quot;, &quot;direction&quot;: &quot;right&quot;, &quot;confidence&quot;: 1 } from_id/to_id: IDs of the related annotations type: type of the annotation (labels, choices, relations etc) direction: direction for the relation. The accepted values are right and left. Submitted annotation: When a completion is submitted, it has a submitted timestamp on the COMPLETION dictionary (refer to the above json example): &quot;submitted_at&quot;: &quot;2022-07-04T12:03:48.824&quot; Reviewed annotation: When a completion is reviewed, the following information is added to the COMPLETION dictionary: &quot;review_status&quot;: { &quot;approved&quot;: true, &quot;comment&quot;: &quot;Looks good!&quot;, &quot;reviewer&quot;: &quot;Mauro&quot;, &quot;reviewed_at&quot;: &quot;2022-07-04T06:19:31.897Z&quot; } approved: boolean value (true -&gt; approved and false -&gt; rejected) comment: text comment manually defined by the reviewer reviewer: user who reviewed the completion reviewed_at: review timestamp Copied Annotation: When an annotation is copied/cloned from a specific completion/prediction, the COMPLETION dictionary contains the copied_from filed: &quot;copied_from&quot;: &quot;prediction: 4001&quot; CSV Results are stored in a comma-separated tabular file with column names specified by “from_name” and “to_name” values. TSV Results are stored in a tab-separated tabular file with column names specified by “from_name” and “to_name” values. CoNLL2003 The CoNLL export feature generates a single output file, containing all available completions for all the tasks in the project. The resulting file has the following format: -DOCSTART- -X- O Sample -X- _ O Type -X- _ O Medical -X- _ O Specialty: -X- _ O Endocrinology -X- _ O Sample -X- _ O Name: -X- _ O Diabetes -X- _ B-Diagnosis Mellitus -X- _ I-Diagnosis Followup -X- _ O Description: -X- _ O Return -X- _ O visit -X- _ O to -X- _ O the -X- _ O endocrine -X- _ O clinic -X- _ O for -X- _ O followup -X- _ O management -X- _ O of -X- _ O type -X- _ O 1 -X- _ O diabetes -X- _ O mellitus -X- _ O Plan -X- _ O today -X- _ O is -X- _ O to -X- _ O make -X- _ O adjustments -X- _ O to -X- _ O her -X- _ O pump -X- _ O based -X- _ O on -X- _ O a -X- _ O total -X- _ O daily -X- _ B-FREQUENCY dose -X- _ O of -X- _ O 90 -X- _ O units -X- _ O of -X- _ O insulin -X- _ O … Users can specify if only starred completions should be included in the output file by checking the Only ground truth option before generating the export. Supported Formats for Visual NER Projects The process of annotations export from Visual NER projects is similar to that of text projects. When exporting the Visual NER annotations users have two additional formats available: COCO and VOC. For Visual NER projects, the image documents annotated as part of each task are included in the project export archive under the images folder. COCO The COCO format is a specific JSON structure dictating how labels and metadata are saved for an image dataset. It is a large-scale object detection, segmentation, and captioning dataset. Exporting in COCO format is available for Visual NER projects only. Below is a sample format: { &quot;images&quot;: [ { &quot;width&quot;: 6.588235294117647, &quot;height&quot;: 0.9396786905122766, &quot;id&quot;: 0, &quot;file_name&quot;: [ &quot;/images/19/0160023239a-1655481445_0.png&quot;, &quot;/images/19/0160023239a-1655481445_1.png&quot; ] } ], &quot;categories&quot;: [ { &quot;id&quot;: 0, &quot;name&quot;: &quot;OGSContractNumber&quot;, &quot;supercategory&quot;: &quot;OGSContractNumber&quot; }, { &quot;id&quot;: 1, &quot;name&quot;: &quot;Contractor&quot;, &quot;supercategory&quot;: &quot;Contractor&quot; }, { &quot;id&quot;: 2, &quot;name&quot;: &quot;FederalID&quot;, &quot;supercategory&quot;: &quot;FederalID&quot; }, { &quot;id&quot;: 3, &quot;name&quot;: &quot;VendorID&quot;, &quot;supercategory&quot;: &quot;VendorID&quot; }, { &quot;id&quot;: 4, &quot;name&quot;: &quot;Title&quot;, &quot;supercategory&quot;: &quot;Title&quot; }, { &quot;id&quot;: 5, &quot;name&quot;: &quot;AwardNumber&quot;, &quot;supercategory&quot;: &quot;AwardNumber&quot; }, { &quot;id&quot;: 6, &quot;name&quot;: &quot;ContractPeriod&quot;, &quot;supercategory&quot;: &quot;ContractPeriod&quot; }, { &quot;id&quot;: 7, &quot;name&quot;: &quot;BidOpeningDate&quot;, &quot;supercategory&quot;: &quot;BidOpeningDate&quot; }, { &quot;id&quot;: 8, &quot;name&quot;: &quot;DateOfIssue&quot;, &quot;supercategory&quot;: &quot;DateOfIssue&quot; }, { &quot;id&quot;: 9, &quot;name&quot;: &quot;SpecificationReference&quot;, &quot;supercategory&quot;: &quot;SpecificationReference&quot; }, { &quot;id&quot;: 10, &quot;name&quot;: &quot;GroupNumber&quot;, &quot;supercategory&quot;: &quot;GroupNumber&quot; } ], &quot;annotations&quot;: [ { &quot;id&quot;: 0, &quot;image_id&quot;: 0, &quot;category_id&quot;: 0, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 0, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;PC69434&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 1, &quot;image_id&quot;: 0, &quot;category_id&quot;: 0, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 0, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;PC69435&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 2, &quot;image_id&quot;: 0, &quot;category_id&quot;: 0, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 0, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;PC69436&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 3, &quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 1, 0, 1, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;Cream-O-Land Dairies, LLC&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 4, &quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 1, 0, 1, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;Hudson Valley Fresh Dairy, LLC&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 5, &quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 1, 0, 1, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;Upstate Niagara Inc.&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 6, &quot;image_id&quot;: 0, &quot;category_id&quot;: 2, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 3, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;223629742&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 7, &quot;image_id&quot;: 0, &quot;category_id&quot;: 2, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 3, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;461053272&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 8, &quot;image_id&quot;: 0, &quot;category_id&quot;: 2, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 3, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;160845625&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 9, &quot;image_id&quot;: 0, &quot;category_id&quot;: 3, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 5, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;1100070111&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 10, &quot;image_id&quot;: 0, &quot;category_id&quot;: 3, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 5, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;1100212977&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 11, &quot;image_id&quot;: 0, &quot;category_id&quot;: 3, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 5, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;1000014941&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 12, &quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 4, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;Cream-O-Land - LLC&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 13, &quot;image_id&quot;: 0, &quot;category_id&quot;: 0, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 5, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;PC69434&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 14, &quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 4, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;Hudson Valley Fresh Dairy, LLC&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 15, &quot;image_id&quot;: 0, &quot;category_id&quot;: 0, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 5, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;PC69435&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 16, &quot;image_id&quot;: 0, &quot;category_id&quot;: 0, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 5, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;PC69436&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 17, &quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 4, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;Upstate Niagara Cooperative, Inc.&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 18, &quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 4, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;; Upstate Niagara Cooperative,&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 19, &quot;image_id&quot;: 0, &quot;category_id&quot;: 0, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 5, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;PC69436&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 20, &quot;image_id&quot;: 0, &quot;category_id&quot;: 4, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 3, 0, 1, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;Milk, Fluid (Statewide)&quot;, &quot;pageNumber&quot;: 1 }, { &quot;id&quot;: 21, &quot;image_id&quot;: 0, &quot;category_id&quot;: 5, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 2, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;23239&quot;, &quot;pageNumber&quot;: 1 }, { &quot;id&quot;: 22, &quot;image_id&quot;: 0, &quot;category_id&quot;: 6, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 2, 0, 2, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;September 21, 2021 Through September 20, 2026&quot;, &quot;pageNumber&quot;: 1 }, { &quot;id&quot;: 23, &quot;image_id&quot;: 0, &quot;category_id&quot;: 7, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 2, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;June 10, 2021&quot;, &quot;pageNumber&quot;: 1 }, { &quot;id&quot;: 24, &quot;image_id&quot;: 0, &quot;category_id&quot;: 8, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 2, 0, 1, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;September 14, 2021&quot;, &quot;pageNumber&quot;: 1 }, { &quot;id&quot;: 27, &quot;image_id&quot;: 0, &quot;category_id&quot;: 10, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 2, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;Group&quot;, &quot;pageNumber&quot;: 1 }, { &quot;id&quot;: 29, &quot;image_id&quot;: 0, &quot;category_id&quot;: 4, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 3, 0, 1, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;Milk, Fluid (Statewide)&quot;, &quot;pageNumber&quot;: 1 }, { &quot;id&quot;: 30, &quot;image_id&quot;: 0, &quot;category_id&quot;: 5, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 2, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;23239&quot;, &quot;pageNumber&quot;: 1 }, { &quot;id&quot;: 31, &quot;image_id&quot;: 0, &quot;category_id&quot;: 6, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 2, 0, 2, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;September 21, 2021 Through September 20, 2026&quot;, &quot;pageNumber&quot;: 1 }, { &quot;id&quot;: 32, &quot;image_id&quot;: 0, &quot;category_id&quot;: 6, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 2, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;June 10, 2021&quot;, &quot;pageNumber&quot;: 1 }, { &quot;id&quot;: 33, &quot;image_id&quot;: 0, &quot;category_id&quot;: 6, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 2, 0, 1, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;September 14, 2021&quot;, &quot;pageNumber&quot;: 1 }, { &quot;id&quot;: 34, &quot;image_id&quot;: 0, &quot;category_id&quot;: 5, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 2, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;23239&quot;, &quot;pageNumber&quot;: 1 }, { &quot;id&quot;: 35, &quot;image_id&quot;: 0, &quot;category_id&quot;: 0, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 0, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;PC69434&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 36, &quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 1, 0, 1, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;Cream-O-Land Dairies, LLC&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 38, &quot;image_id&quot;: 0, &quot;category_id&quot;: 3, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 5, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;1100070111&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 39, &quot;image_id&quot;: 0, &quot;category_id&quot;: 0, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 0, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;PC69435&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 41, &quot;image_id&quot;: 0, &quot;category_id&quot;: 2, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 3, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;461053272&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 43, &quot;image_id&quot;: 0, &quot;category_id&quot;: 0, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 0, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;PC69436&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 45, &quot;image_id&quot;: 0, &quot;category_id&quot;: 2, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 3, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;160845625&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 47, &quot;image_id&quot;: 0, &quot;category_id&quot;: 1, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 4, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;Cream-O-Land&quot;, &quot;pageNumber&quot;: 2 }, { &quot;id&quot;: 49, &quot;image_id&quot;: 0, &quot;category_id&quot;: 0, &quot;segmentation&quot;: [], &quot;bbox&quot;: [ 5, 0, 0, 0 ], &quot;ignore&quot;: 0, &quot;iscrowd&quot;: 0, &quot;area&quot;: 0, &quot;text&quot;: &quot;PC69434&quot;, &quot;pageNumber&quot;: 2 } ], &quot;info&quot;: { &quot;year&quot;: 2022, &quot;version&quot;: &quot;1.0&quot;, &quot;contributor&quot;: &quot;Annotation Lab Converter&quot; } } Pascal VOC XML Pascal Visual Object Classes(VOC) is an XML file that contains the image details, bounding box details, classes, pose, truncated, and other data. For each image of the task there will be an XML annotation file. Exporting in VOC format is available for Visual NER projects only. Below is a sample format: &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt; &lt;annotation&gt; &lt;folder&gt;images&lt;/folder&gt; &lt;filename&gt;0160023239a-1655481445_0.png&lt;/filename&gt; &lt;source&gt; &lt;database&gt;ALABDB&lt;/database&gt; &lt;/source&gt; &lt;owner&gt; &lt;name&gt;AnnotationLab&lt;/name&gt; &lt;/owner&gt; &lt;size&gt; &lt;width&gt;2550&lt;/width&gt; &lt;height&gt;3299&lt;/height&gt; &lt;depth&gt;1&lt;/depth&gt; &lt;/size&gt; &lt;segmented&gt;0&lt;/segmented&gt; &lt;object&gt; &lt;name&gt;Title&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;1305&lt;/xmin&gt; &lt;ymin&gt;660&lt;/ymin&gt; &lt;xmax&gt;1780&lt;/xmax&gt; &lt;ymax&gt;703&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;AwardNumber&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;973&lt;/xmin&gt; &lt;ymin&gt;791&lt;/ymin&gt; &lt;xmax&gt;1099&lt;/xmax&gt; &lt;ymax&gt;834&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;ContractPeriod&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;974&lt;/xmin&gt; &lt;ymin&gt;903&lt;/ymin&gt; &lt;xmax&gt;2038&lt;/xmax&gt; &lt;ymax&gt;946&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;BidOpeningDate&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;974&lt;/xmin&gt; &lt;ymin&gt;1013&lt;/ymin&gt; &lt;xmax&gt;1263&lt;/xmax&gt; &lt;ymax&gt;1054&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;DateOfIssue&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;974&lt;/xmin&gt; &lt;ymin&gt;1124&lt;/ymin&gt; &lt;xmax&gt;1393&lt;/xmax&gt; &lt;ymax&gt;1166&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;SpecificationReference&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;973&lt;/xmin&gt; &lt;ymin&gt;1235&lt;/ymin&gt; &lt;xmax&gt;1729&lt;/xmax&gt; &lt;ymax&gt;1277&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;GroupNumber&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;974&lt;/xmin&gt; &lt;ymin&gt;660&lt;/ymin&gt; &lt;xmax&gt;1248&lt;/xmax&gt; &lt;ymax&gt;702&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;GroupNumber&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;974&lt;/xmin&gt; &lt;ymin&gt;660&lt;/ymin&gt; &lt;xmax&gt;1108&lt;/xmax&gt; &lt;ymax&gt;702&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;AwardNumber&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;1125&lt;/xmin&gt; &lt;ymin&gt;660&lt;/ymin&gt; &lt;xmax&gt;1248&lt;/xmax&gt; &lt;ymax&gt;694&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;Title&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;1304&lt;/xmin&gt; &lt;ymin&gt;660&lt;/ymin&gt; &lt;xmax&gt;1780&lt;/xmax&gt; &lt;ymax&gt;703&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;AwardNumber&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;974&lt;/xmin&gt; &lt;ymin&gt;791&lt;/ymin&gt; &lt;xmax&gt;1097&lt;/xmax&gt; &lt;ymax&gt;825&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;ContractPeriod&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;974&lt;/xmin&gt; &lt;ymin&gt;902&lt;/ymin&gt; &lt;xmax&gt;2038&lt;/xmax&gt; &lt;ymax&gt;945&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;ContractPeriod&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;974&lt;/xmin&gt; &lt;ymin&gt;1013&lt;/ymin&gt; &lt;xmax&gt;1264&lt;/xmax&gt; &lt;ymax&gt;1054&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;ContractPeriod&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;974&lt;/xmin&gt; &lt;ymin&gt;1124&lt;/ymin&gt; &lt;xmax&gt;1393&lt;/xmax&gt; &lt;ymax&gt;1166&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;object&gt; &lt;name&gt;AwardNumber&lt;/name&gt; &lt;pose&gt;Unspecified&lt;/pose&gt; &lt;truncated&gt;0&lt;/truncated&gt; &lt;difficult&gt;0&lt;/difficult&gt; &lt;bndbox&gt; &lt;xmin&gt;791&lt;/xmin&gt; &lt;ymin&gt;2246&lt;/ymin&gt; &lt;xmax&gt;903&lt;/xmax&gt; &lt;ymax&gt;2276&lt;/ymax&gt; &lt;/bndbox&gt; &lt;/object&gt; &lt;/annotation&gt; Export Options Tags Only allow export of tasks having the specified tags. Only Ground Truth If this option is enabled then only the tasks having ground truth in the completion will be exported. Exclude tasks without Completions Previous versions of the Annotation Lab only allowed the export of tasks that contained completions. From version 2.8.0 on, the tasks without any completions can be exported as this can be necessary for cloning projects. In the case where only tasks with completions are required in the export, users can enable the Exclude tasks without Completions option on the Export page.",
    "url": "/docs/en/alab/export",
    "relUrl": "/docs/en/alab/export"
  },
  "45": {
    "id": "45",
    "title": "Extract handwritten texts - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/extract_handwritten_texts",
    "relUrl": "/extract_handwritten_texts"
  },
  "46": {
    "id": "46",
    "title": "Extract Tables - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/extract_tables",
    "relUrl": "/extract_tables"
  },
  "47": {
    "id": "47",
    "title": "Extract Text from Documents - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/extract_text_from_documents",
    "relUrl": "/extract_text_from_documents"
  },
  "48": {
    "id": "48",
    "title": "Normalization & Data Augmentation - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/financial_company_normalization",
    "relUrl": "/financial_company_normalization"
  },
  "49": {
    "id": "49",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/financial_deidentification",
    "relUrl": "/financial_deidentification"
  },
  "50": {
    "id": "50",
    "title": "Financial Document Splitting - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/financial_document_splitting",
    "relUrl": "/financial_document_splitting"
  },
  "51": {
    "id": "51",
    "title": "Financial Document Understanding - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/financial_document_understanding",
    "relUrl": "/financial_document_understanding"
  },
  "52": {
    "id": "52",
    "title": "Recognize Financial Entities - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/financial_entity_recognition",
    "relUrl": "/financial_entity_recognition"
  },
  "53": {
    "id": "53",
    "title": "Extract Financial Relationships - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/financial_relation_extraction",
    "relUrl": "/financial_relation_extraction"
  },
  "54": {
    "id": "54",
    "title": "Finance NLP Release Notes",
    "content": "Releases log Finance NLP 1.0.0 on Medium Finance NLP 1.1.0 on Medium Finance NLP 1.2.0 on Medium Finance NLP 1.3.0 on Medium Finance NLP 1.4.0 on Medium Finance NLP 1.5.0 on Medium Finance NLP 1.6.0 on Medium Finance NLP 1.7.0 on Medium Finance NLP 1.8.0 on Medium Slack - Join #finance channel",
    "url": "/docs/en/financial_release_notes",
    "relUrl": "/docs/en/financial_release_notes"
  },
  "55": {
    "id": "55",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/financial_table_extraction",
    "relUrl": "/financial_table_extraction"
  },
  "56": {
    "id": "56",
    "title": "Version Compatibility",
    "content": "Legal NLP runs on top of johnsnowlabs library (former nlu). Please find technical documentation about how to install it here. Finance NLP johnsnowlabs 1.X.X 4.X.X Check which version of Spark NLP, Visual NLP or even Clinical NLP are included in johnsnowlabs versions here Finance NLP is also supported in Annotation Lab from Alab 4.2.3 version on!",
    "url": "/docs/en/financial_version_compatibility",
    "relUrl": "/docs/en/financial_version_compatibility"
  },
  "57": {
    "id": "57",
    "title": "Financial Visual Document Classification - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/financial_visual_document_classification",
    "relUrl": "/financial_visual_document_classification"
  },
  "58": {
    "id": "58",
    "title": "Genes, Variants, Phenotypes - Biomedical NLP Demos & Notebooks",
    "content": "",
    "url": "/genes_variants_phenotypes",
    "relUrl": "/genes_variants_phenotypes"
  },
  "59": {
    "id": "59",
    "title": "German - Medical NLP Demos & Notebooks",
    "content": "",
    "url": "/german",
    "relUrl": "/german"
  },
  "60": {
    "id": "60",
    "title": "Tensorflow Graph",
    "content": "NER DL uses Char CNNs - BiLSTM - CRF Neural Network architecture. Spark NLP defines this architecture through a Tensorflow graph, which requires the following parameters: Tags Embeddings Dimension Number of Chars Spark NLP infers these values from the training dataset used in NerDLApproach annotator and tries to load the graph embedded on spark-nlp package. Currently, Spark NLP has graphs for the most common combination of tags, embeddings, and number of chars values: Tags Embeddings Dimension 10 100 10 200 10 300 10 768 10 1024 25 300 All of these graphs use an LSTM of size 128 and number of chars 100 In case, your train dataset has a different number of tags, embeddings dimension, number of chars and LSTM size combinations shown in the table above, NerDLApproach will raise an IllegalArgumentException exception during runtime with the message below: Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Check https://nlp.johnsnowlabs.com/docs/en/graph for instructions to generate the required graph. To overcome this exception message we have to follow these steps: Clone spark-nlp github repo Run python file create_models with number of tags, embeddings dimension and number of char values mentioned on your exception message error. cd spark-nlp/python/tensorflow export PYTHONPATH=lib/ner python ner/create_models.py [number_of_tags] [embeddings_dimension] [number_of_chars] [output_path] This will generate a graph on the directory defined on `output_path argument. Retry training with NerDLApproach annotator but this time use the parameter setGraphFolder with the path of your graph. Note: Make sure that you have Python 3 and Tensorflow 1.15.0 installed on your system since create_models requires those versions to generate the graph successfully. Note: We also have a notebook in the same directory if you prefer Jupyter notebook to cerate your custom graph (create_models.ipynb).",
    "url": "/docs/en/graph",
    "relUrl": "/docs/en/graph"
  },
  "61": {
    "id": "61",
    "title": "Hardware Acceleration",
    "content": "Spark NLP is a production-ready and fully-featured NLP library that runs natively on Apache Spark. It is already faster on a single machine than other popular NLP libraries let alone in a cluster with multiple machines. In addition, we are constantly optimizing our codes to make them even faster while using fewer resources (memory/CPU). For incense, the Spark NLP 4.0 comes with massive optimizations for GPU and modern CPUs for most of our Transformer-based annotators. That said, some downstream tasks such as Language Models (Transformer models like BERT) or text and token classifiers use Deep Learning via the TensorFlow engine. Therefore, there are ways to optimize them even more by using newer hardware, especially those with accelerations. The following benchmarks have been done by using a single Dell Server with the following specs: GPU: Tesla P100 PCIe 12GB - CUDA Version: 11.3 - Driver Version: 465.19.01 CPU: Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz - 40 Cores Memory: 80G GPU Perhaps the best and the easiest way in Spark NLP to massively improve a DL-based task(s) is to use GPU. Spark NLP comes with a zero-code change feature to run seamlessly on both CPU and GPU by simply enabling GPU via sparknlp.start(gpu=True) or using directly the Maven package that is for GPU spark-nlp-gpu. (more details) Since the new Transformer models such as BERT for Word and Sentence embeddings are the most computationally available downstream tasks in Spark NLP, we will show a benchmark for inference (prediction) to compare CPU (without any accelerations) to GPU: Model on GPU Spark NLP 3.4.3 vs. 4.0.0 RoBERTa base +560%(6.6x) RoBERTa Large +332%(4.3x) Albert Base +587%(6.9x) Albert Large +332%(4.3x) DistilBERT +659%(7.6x) XLM-RoBERTa Base +638%(7.4x) XLM-RoBERTa Large +365%(4.7x) XLNet Base +449%(5.5x) XLNet Large +267%(3.7x) DeBERTa Base +713%(8.1x) DeBERTa Large +477%(5.8x) Longformer Base +52%(1.5x) Spark NLP 4.3.1 is built with TensorFlow 2.7.1 and the following NVIDIA® software are only required for GPU support: NVIDIA® GPU drivers version 450.80.02 or higher CUDA® Toolkit 11.2 cuDNN SDK 8.1.0 CPU The oneAPI Deep Neural Network Library (oneDNN) optimizations are now available in Spark NLP 4.0.0 which uses TensorFlow 2.7.1. You can enable those CPU optimizations by setting the environment variable TF_ENABLE_ONEDNN_OPTS=1. Intel has been collaborating with Google to optimize its performance on Intel Xeon processor-based platforms using Intel oneAPI Deep Neural Network (oneDNN), an open-source, cross-platform performance library for DL applications. TensorFlow optimizations are enabled via oneDNN to accelerate key performance-intensive operations such as convolution, matrix multiplication, and batch normalization. This feature is experimental as it has to be enabled manually and benchmarked manually to see whether or not your pipeline can benefit from oneDNN accelerations. That being said, it does not always result in accelerating your annotators as it highly depends on the hardware and the NLP tasks. Similar to GPU, if the task is not computational it won’t change the result and it may even slow down the inferences. NOTE: Always have a baseline benchmark without having oneDNN enabled so you can compare it with oneDNN. In addition, always make sure you repeat the same steps if you are moving to another hardware (CPU). Here we compare the last release of Spark NLP 3.4.3 on CPU (normal) with Spark NLP 4.0.0 on CPU with oneDNN enabled. We chose some of the most computational downstream tasks in Spark NLP as they are usually required in the pipeline for other tasks such as NER or text classification): Model on CPU 3.4.x vs. 4.0.0 with oneDNN BERT Base +47% BERT Large +42% RoBERTa Base +51% RoBERTa Large +61% Albert Base +83% Albert Large +58% DistilBERT +80% XLM-RoBERTa Base +82% XLM-RoBERTa Large +72% XLNet Base +50% XLNet Large +27% DeBERTa Base +59% DeBERTa Large +56% CamemBERT Base +97% CamemBERT Large +65% Longformer Base +63% In future TensorFlow releases, the oneDNN will be enabled by default (starting TF 2.9) as this feature becomes more stable and more generic for almost all TF ops. Maximize TensorFlow* Performance on CPU: Considerations and Recommendations for Inference Workloads GPU vs. CPU Webinar: Speed Optimization &amp; Benchmarks in Spark NLP 3: Making the Most of Modern Hardware",
    "url": "/docs/en/hardware_acceleration",
    "relUrl": "/docs/en/hardware_acceleration"
  },
  "62": {
    "id": "62",
    "title": "NLP Libraries Integration",
    "content": "Healthcare NLP provides an easy to use module for interacting with Annotation Lab with minimal code. In this section, you can find the instructions for performing specific operations using the annotation lab module of the Healthcare NLP library. You can execute these instructions in a python notebook (Jupyter, Colab, Kaggle, etc.). Before running the instructions described in the following sub-sections, some initial environment setup needs to be performed in order to configure the Healthcare NLP library and start a Spark session. NOTE: For using this integration a Healthcare, Finance and/or Legal NLP License key is requirend. If you do not have one, you can get it here. import json import os from google.colab import files license_keys = files.upload() with open(list(license_keys.keys())[0]) as f: license_keys = json.load(f) # Defining license key-value pairs as local variables locals().update(license_keys) # Adding license key-value pairs to environment variables os.environ.update(license_keys) NOTE: The license upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving jsl_keys.json to jsl_keys (2).json # Installing pyspark and spark-nlp ! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION # Installing Spark NLP Healthcare ! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION --extra-index-url https://pypi.johnsnowlabs.com/$SECRET # Installing Spark NLP Display Library for visualization ! pip install -q spark-nlp-display |████████████████████████████████| 212.4 MB 51 kB/s |████████████████████████████████| 616 kB 56.5 MB/s |████████████████████████████████| 198 kB 52.8 MB/s Building wheel for pyspark (setup.py) ... done |████████████████████████████████| 206 kB 2.9 MB/s |████████████████████████████████| 95 kB 2.4 MB/s |████████████████████████████████| 66 kB 4.9 MB/s |████████████████████████████████| 1.6 MB 44.7 MB/s import pandas as pd import requests import json from zipfile import ZipFile from io import BytesIO import os from pyspark.ml import Pipeline,PipelineModel from pyspark.sql import SparkSession from pyspark.sql import functions as F from sparknlp.annotator import * from sparknlp_jsl.annotator import * from sparknlp.base import * import sparknlp_jsl import sparknlp import warnings warnings.filterwarnings(&#39;ignore&#39;) params = {&quot;spark.driver.memory&quot;:&quot;16G&quot;, &quot;spark.kryoserializer.buffer.max&quot;:&quot;2000M&quot;, &quot;spark.driver.maxResultSize&quot;:&quot;2000M&quot;} print(&quot;Spark NLP Version :&quot;, sparknlp.version()) print(&quot;Spark NLP_JSL Version :&quot;, sparknlp_jsl.version()) spark = sparknlp_jsl.start(license_keys[&#39;SECRET&#39;],params=params) spark Spark NLP Version : 4.1.0 Spark NLP_JSL Version : 4.1.0 SparkSession - in-memory SparkContext Spark UI Version v3.1.2 Master local[*] AppName Spark NLP Licensed Using already exported JSON to generate training data - No Annotation Lab Credentials Required # import the module from sparknlp_jsl.alab import AnnotationLab alab = AnnotationLab() # downloading demo json !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Annotation_Lab/data/alab_demo.json --2022-09-29 18:47:21-- https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Annotation_Lab/data/alab_demo.json Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 66538 (65K) [text/plain] Saving to: ‘alab_demo.json’ alab_demo.json 100%[===================&gt;] 64.98K --.-KB/s in 0.01s 2022-09-29 18:47:21 (5.43 MB/s) - ‘alab_demo.json’ saved [66538/66538] Generating training data for different models No Annotation Lab Credentials Required. Only the exported JSON is used. Classification Model The following snippet shows how to generate data for training a classification model. alab.get_classification_data( # required: path to Annotation Lab JSON export input_json_path=&#39;alab_demo.json&#39;, # optional: set to True to select ground truth completions, False to select latest completions, # defaults to False ground_truth=True ) Processing 14 annotation(s). Output:   task_id task_title text class 0 2 Note 2 The patient is a 5-month-old infant who presen… [Female] 1 3 Note 3 The patient is a 21-day-old male here for 2 da… [Male] 2 1 Note 1 On 18/08 patient declares she has a headache s… [Female] NER Model The JSON export must be converted into a CoNLL format suitable for training an NER model. alab.get_conll_data( # required: Spark session with spark-nlp-jsl jar spark=spark, # required: path to Annotation Lab JSON export input_json_path=&quot;alab_demo.json&quot;, # required: name of the CoNLL file to save output_name=&quot;conll_demo&quot;, # optional: path for CoNLL file saving directory, defaults to &#39;exported_conll&#39; save_dir=&quot;exported_conll&quot;, # optional: set to True to select ground truth completions, False to select latest completions, # defaults to False ground_truth=True, # optional: labels to exclude from CoNLL; these are all assertion labels and irrelevant NER labels, # defaults to empty list excluded_labels=[&#39;ABSENT&#39;], # optional: set a pattern to use regex tokenizer, defaults to regular tokenizer if pattern not defined regex_pattern=&quot; s+|(?=[-.:;*+,$&amp;% [ ]])|(?&lt;=[-.:;*+,$&amp;% [ ]])&quot; # optional: list of Annotation Lab task IDs to exclude from CoNLL, defaults to empty list # excluded_task_ids = [2, 3] # optional: list of Annotation Lab task titles to exclude from CoNLL, defaults to None # excluded_task_titles = [&#39;Note 1&#39;] ) sentence_detector_dl_healthcare download started this may take some time. Approximate size to download 367.3 KB [OK!] pos_clinical download started this may take some time. Approximate size to download 1.5 MB [OK!] Spark NLP LightPipeline is created sentence_detector_dl_healthcare download started this may take some time. Approximate size to download 367.3 KB [OK!] Spark NLP LightPipeline is created Attempting to process: Task ID# 1 Task ID# 1 is included Attempting to process: Task ID# 2 Task ID# 2 is included Attempting to process: Task ID# 3 Task ID# 3 is included Saved in location: exported_conll/conll_demo.conll Printing first 30 lines of CoNLL for inspection: [&#39;-DOCSTART- -X- -1- O n n&#39;, &#39;On II II O n&#39;, &#39;18/08 MC MC B-DATE n&#39;, &#39;patient NN NN O n&#39;, &#39;declares NNS NNS O n&#39;, &#39;she PN PN O n&#39;, &#39;has VHZ VHZ O n&#39;, &#39;a DD DD O n&#39;, &#39;headache NN NN B-PROBLEM n&#39;, &#39;since CS CS O n&#39;, &#39;06/08 MC MC B-DATE n&#39;, &#39;, NN NN O n&#39;, &#39;needs VVZ VVZ O n&#39;, &#39;to TO TO O n&#39;, &#39;get VVI VVI O n&#39;, &#39;a DD DD O n&#39;, &#39;head NN NN B-TEST n&#39;, &#39;CT NN NN I-TEST n&#39;, &#39;, NN NN O n&#39;, &#39;and CC CC O n&#39;, &#39;appears VVZ VVZ O n&#39;, &#39;anxious JJ JJ B-PROBLEM n&#39;, &#39;when CS CS O n&#39;, &#39;she PN PN O n&#39;, &#39;walks RR RR O n&#39;, &#39;fast JJ JJ O n&#39;, &#39;. NN NN O n&#39;, &#39;No NN NN O n&#39;, &#39;alopecia NN NN B-PROBLEM n&#39;, &#39;noted VVNJ VVNJ O n&#39;] Assertion Model The JSON export is converted into a dataframe, suitable for training an assertion model. alab.get_assertion_data( # required: SparkSession with spark-nlp-jsl jar spark=spark, # required: path to Annotation Lab JSON export input_json_path = &#39;alab_demo.json&#39;, # required: annotated assertion labels to train on assertion_labels = [&#39;ABSENT&#39;], # required: relevant NER labels that are assigned assertion labels relevant_ner_labels = [&#39;PROBLEM&#39;, &#39;TREATMENT&#39;], # optional: set to True to select ground truth completions, False to select latest completions, # defaults to False ground_truth = True, # optional: assertion label to assign to entities that have no assertion labels, defaults to None unannotated_label = &#39;PRESENT&#39;, # optional: set a pattern to use regex tokenizer, defaults to regular tokenizer if pattern not defined regex_pattern = &quot; s+|(?=[-.:;*+,$&amp;% [ ]])|(?&lt;=[-.:;*+,$&amp;% [ ]])&quot;, # optional: set the strategy to control the number of occurrences of the unannotated assertion label # in the output dataframe, options are &#39;weighted&#39; or &#39;counts&#39;, &#39;weighted&#39; allows to sample using a # fraction, &#39;counts&#39; allows to sample using absolute counts, defaults to None unannotated_label_strategy = &#39;weighted&#39;, # optional: dictionary in the format {&#39;ENTITY_LABEL&#39;: sample_weight_or_counts} to control the number of # occurrences of the unannotated assertion label in the output dataframe, where &#39;ENTITY_LABEL&#39; are the # NER labels that are assigned the unannotated assertion label, and sample_weight_or_counts should be # between 0 and 1 if `unannotated_label_strategy` is &#39;weighted&#39; or between 0 and the max number of # occurrences of that NER label if `unannotated_label_strategy` is &#39;counts&#39; unannotated_label_strategy_dict = {&#39;PROBLEM&#39;: 0.5, &#39;TREATMENT&#39;: 0.5}, # optional: list of Annotation Lab task IDs to exclude from output dataframe, defaults to None # excluded_task_ids = [2, 3] # optional: list of Annotation Lab task titles to exclude from output dataframe, defaults to None # excluded_task_titles = [&#39;Note 1&#39;] ) sentence_detector_dl_healthcare download started this may take some time. Approximate size to download 367.3 KB [OK!] Spark NLP LightPipeline is created Processing Task ID# 2 Processing Task ID# 3 Processing Task ID# 1 Output:   task_id title text target ner_label label start end 0 1 Note 1 On 18/08 patient declares she has a headache s… headache PROBLEM PRESENT 7 7 1 1 Note 1 On 18/08 patient declares she has a headache s… alopecia PROBLEM ABSENT 27 27 2 1 Note 1 On 18/08 patient declares she has a headache s… pain PROBLEM ABSENT 32 32 3 2 Note 2 Mom states she had no fever. fever PROBLEM ABSENT 5 5 4 2 Note 2 She had no difficulty breathing and her cough … difficulty breathing PROBLEM ABSENT 3 4 5 2 Note 2 She had no difficulty breathing and her cough … cough PROBLEM PRESENT 7 7 6 2 Note 2 She had no difficulty breathing and her cough … dry PROBLEM PRESENT 11 11 7 2 Note 2 She had no difficulty breathing and her cough … hacky PROBLEM PRESENT 13 13 8 2 Note 2 At that time, physical exam showed no signs of… flu PROBLEM ABSENT 10 10 9 3 Note 3 The patient is a 21-day-old male here for 2 da… congestion PROBLEM PRESENT 15 15 10 3 Note 3 The patient is a 21-day-old male here for 2 da… suctioning yellow discharge TREATMENT PRESENT 23 25 11 3 Note 3 The patient is a 21-day-old male here for 2 da… perioral cyanosis PROBLEM ABSENT 47 48 12 3 Note 3 One day ago, mom also noticed a tactile temper… tactile temperature PROBLEM PRESENT 8 9 Relation Extraction Model The JSON export is converted into a dataframe suitable for training a relation extraction model. alab.get_relation_extraction_data( # required: Spark session with spark-nlp-jsl jar spark=spark, # required: path to Annotation Lab JSON export input_json_path=&#39;alab_demo.json&#39;, # optional: set to True to select ground truth completions, False to select latest completions, # defaults to False ground_truth=True, # optional: set to True to assign a relation label between entities where no relation was annotated, # defaults to False negative_relations=True, # optional: all assertion labels that were annotated in the Annotation Lab, defaults to None assertion_labels=[&#39;ABSENT&#39;], # optional: plausible pairs of entities for relations, separated by a &#39;-&#39;, use the same casing as the # annotations, include only one relation direction, defaults to all possible pairs of annotated entities relation_pairs=[&#39;DATE-PROBLEM&#39;,&#39;TREATMENT-PROBLEM&#39;,&#39;TEST-PROBLEM&#39;], # optional: set the strategy to control the number of occurrences of the negative relation label # in the output dataframe, options are &#39;weighted&#39; or &#39;counts&#39;, &#39;weighted&#39; allows to sample using a # fraction, &#39;counts&#39; allows to sample using absolute counts, defaults to None negative_relation_strategy=&#39;weighted&#39;, # optional: dictionary in the format {&#39;ENTITY1-ENTITY2&#39;: sample_weight_or_counts} to control the number of # occurrences of negative relations in the output dataframe for each entity pair, where &#39;ENTITY1-ENTITY2&#39; # represent the pairs of entities for relations separated by a `-` (include only one relation direction), # and sample_weight_or_counts should be between 0 and 1 if `negative_relation_strategy` is &#39;weighted&#39; or # between 0 and the max number of occurrences of negative relations if `negative_relation_strategy` is # &#39;counts&#39;, defaults to None negative_relation_strategy_dict = {&#39;DATE-PROBLEM&#39;: 0.1, &#39;TREATMENT-PROBLEM&#39;: 0.5, &#39;TEST-PROBLEM&#39;: 0.2}, # optional: list of Annotation Lab task IDs to exclude from output dataframe, defaults to None # excluded_task_ids = [2, 3] # optional: list of Annotation Lab task titles to exclude from output dataframe, defaults to None # excluded_task_titles = [&#39;Note 1&#39;] ) Successfully processed relations for task: Task ID# 2 Successfully processed relations for task: Task ID# 3 Successfully processed relations for task: Task ID# 1 Total tasks processed: 3 Total annotated relations processed: 10 sentence_detector_dl_healthcare download started this may take some time. Approximate size to download 367.3 KB [OK!] Successfully processed NER labels for: Task ID# 2 Successfully processed NER labels for: Task ID# 3 Successfully processed NER labels for: Task ID# 1 Total tasks processed: 3 Total annotated NER labels processed: 28 Output:   task_id title sentence firstCharEnt1 firstCharEnt2 lastCharEnt1 lastCharEnt2 chunk1 chunk2 label1 label2 rel 0 1 Note 1 On 18/08 patient declares she has a headache s… 36 51 44 56 headache 06/08 PROBLEM DATE is_date_of 1 1 Note 1 On 18/08 patient declares she has a headache s… 36 73 44 80 headache head CT PROBLEM TEST is_test_of 2 1 Note 1 On 18/08 patient declares she has a headache s… 51 156 56 160 06/08 pain DATE PROBLEM O 3 1 Note 1 On 18/08 patient declares she has a headache s… 73 126 80 134 head CT alopecia TEST PROBLEM O 4 2 Note 2 At that time, physical exam showed no signs of… 14 47 27 50 physical exam flu TEST PROBLEM is_test_of 5 2 Note 2 The patient is a 5-month-old infant who presen… 63 76 68 80 Feb 8 cold DATE PROBLEM is_date_of 6 2 Note 2 The patient is a 5-month-old infant who presen… 63 82 68 87 Feb 8 cough DATE PROBLEM is_date_of 7 2 Note 2 The patient is a 5-month-old infant who presen… 63 93 68 103 Feb 8 runny nose DATE PROBLEM is_date_of 8 2 Note 2 The patient is a 5-month-old infant who presen… 82 110 87 115 cough Feb 2 PROBLEM DATE O 9 3 Note 3 One day ago, mom also noticed a tactile temper… 32 73 51 80 tactile temperature Tylenol PROBLEM TREATMENT is_treatment_of 10 3 Note 3 The patient is a 21-day-old male here for 2 da… 52 69 62 77 congestion Nov 8/15 PROBLEM DATE is_date_of 11 3 Note 3 The patient is a 21-day-old male here for 2 da… 52 93 62 120 congestion suctioning yellow discharge PROBLEM TREATMENT is_treatment_of 12 3 Note 3 The patient is a 21-day-old male here for 2 da… 93 244 120 261 suctioning yellow discharge perioral cyanosis TREATMENT PROBLEM O 13 3 Note 3 The patient is a 21-day-old male here for 2 da… 93 265 120 276 suctioning yellow discharge retractions TREATMENT PROBLEM O 14 3 Note 3 The patient is a 21-day-old male here for 2 da… 173 217 196 225 mild breathing problems Nov 9/15 PROBLEM DATE is_date_of Generate Pre-annotations using Spark NLP pipelines No Annotation Lab credentials are required. The first step is to define the Healthcare NLP pipeline. The same procedure can be followed for Legal and Finance NLP pipelines. document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&#39;document&#39;]) .setOutputCol(&#39;sentence&#39;) .setCustomBounds([&#39; n&#39;]) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = WordEmbeddingsModel().pretrained(&#39;embeddings_clinical&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;sentence&quot;, &#39;token&#39;]) .setOutputCol(&quot;embeddings&quot;) ner_model = MedicalNerModel.pretrained(&#39;ner_jsl&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) assertion_model = AssertionDLModel().pretrained(&#39;assertion_dl&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &#39;embeddings&#39;]) .setOutputCol(&quot;assertion_res&quot;) pos_tagger = PerceptronModel() .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos_tags&quot;) dependency_parser = DependencyParserModel() .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;pos_tags&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependencies&quot;) relation_clinical = RelationExtractionModel.pretrained(&#39;re_clinical&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunk&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations_clinical&quot;) .setRelationPairs([&#39;procedure-disease_syndrome_disorder&#39;, &#39;test-oncological&#39;, &#39;test-disease_syndrome_disorder&#39;, &#39;external_body_part_or_region-procedure&#39;, &#39;oncological-external_body_part_or_region&#39;, &#39;oncological-procedure&#39;]) .setMaxSyntacticDistance(0) relation_pos = RelationExtractionModel.pretrained(&#39;posology_re&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunk&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations_pos&quot;) .setRelationPairs([&#39;drug_ingredient-drug_brandname&#39;, &#39;drug_ingredient-dosage&#39;, &#39;drug_ingredient-strength&#39;, &#39;drug_ingredient-route&#39;]) .setMaxSyntacticDistance(0) ner_pipeline = Pipeline( stages = [ document, sentence, tokenizer, word_embeddings, ner_model, converter, assertion_model, pos_tagger, dependency_parser, relation_clinical, relation_pos ]) empty_data = spark.createDataFrame([[&#39;&#39;]]).toDF(&quot;text&quot;) pipeline_model = ner_pipeline.fit(empty_data) lmodel = LightPipeline(pipeline_model) embeddings_clinical download started this may take some time. Approximate size to download 1.6 GB [OK!] ner_jsl download started this may take some time. [OK!] assertion_dl download started this may take some time. [OK!] pos_clinical download started this may take some time. Approximate size to download 1.5 MB [OK!] dependency_conllu download started this may take some time. Approximate size to download 16.7 MB [OK!] re_clinical download started this may take some time. Approximate size to download 6 MB [OK!] Run on sample tasks txt1 = &quot;The patient is a 21-day-old male here for 2 days of congestion since Nov 8/15 - mom has been suctioning yellow discharge from the patient&#39;s nares, plus she has noticed some mild breathing problems while feeding since Nov 9/15 (without signs of perioral cyanosis or retractions). One day ago, mom also noticed a tactile temperature and gave the patient Tylenol.&quot; txt2 = &quot;The patient is a 5-month-old infant who presented initially on Feb 8 with a cold, cough, and runny nose since Feb 2. Mom states she had no fever. She had no difficulty breathing and her cough was described as dry and hacky. At that time, physical exam showed no signs of flu.&quot; task_list = [txt1, txt2] results = lmodel.fullAnnotate(task_list) # full pipeline: # results = pipeline_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: task_list}))).collect() Generate pre-annotation JSON using pipeline results pre_annotations, summary = alab.generate_preannotations( # required: list of results. all_results = results, # requied: output column name of &#39;DocumentAssembler&#39; stage - to get original document string. document_column = &#39;document&#39;, # required: column name(s) of ner model(s). Note: multiple NER models can be used, but make sure their results don&#39;t overrlap. # Or use &#39;ChunkMergeApproach&#39; to combine results from multiple NER models. ner_columns = [&#39;ner_chunk&#39;], # optional: column name(s) of assertion model(s). Note: multiple assertion models can be used, but make sure their results don&#39;t overrlap. assertion_columns = [&#39;assertion_res&#39;], # optional: column name(s) of relation extraction model(s). Note: multiple relation extraction models can be used, but make sure their results don&#39;t overrlap. relations_columns = [&#39;relations_clinical&#39;, &#39;relations_pos&#39;], # optional: This can be defined to identify which pipeline/user/model was used to get predictions. # Default: &#39;model&#39; user_name = &#39;model&#39;, # optional: Option to assign custom titles to tasks. By default, tasks will be titled as &#39;task_#&#39; titles_list = [], # optional: If there are already tasks in project, then this id offset can be used to make sure default titles &#39;task_#&#39; do not overlap. # While upload a batch after the first one, this can be set to number of tasks currently present in the project # This number would be added to each tasks&#39;s ID and title. id_offset=0 ) Processing 2 Annotations. The Generated JSON can be uploaded to Annotation Lab to particular project directly via UI or via API. pre_annotations [{&#39;predictions&#39;: [{&#39;created_username&#39;: &#39;model&#39;, &#39;result&#39;: [{&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;YCtU7EDvme&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 27, &#39;labels&#39;: [&#39;Age&#39;], &#39;start&#39;: 17, &#39;text&#39;: &#39;21-day-old&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;xqbYIUPhhB&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 32, &#39;labels&#39;: [&#39;Gender&#39;], &#39;start&#39;: 28, &#39;text&#39;: &#39;male&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;7GYr3DFbAs&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 48, &#39;labels&#39;: [&#39;Duration&#39;], &#39;start&#39;: 38, &#39;text&#39;: &#39;for 2 days&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;akBx3N0Gy2&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 62, &#39;labels&#39;: [&#39;Symptom&#39;], &#39;start&#39;: 52, &#39;text&#39;: &#39;congestion&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;TJKowx9hR2&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 77, &#39;labels&#39;: [&#39;Date&#39;], &#39;start&#39;: 69, &#39;text&#39;: &#39;Nov 8/15&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;UuWHo6pGz8&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 83, &#39;labels&#39;: [&#39;Gender&#39;], &#39;start&#39;: 80, &#39;text&#39;: &#39;mom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;qIgnDgSJw6&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 110, &#39;labels&#39;: [&#39;Modifier&#39;], &#39;start&#39;: 104, &#39;text&#39;: &#39;yellow&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;DkE8rIoKVg&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 120, &#39;labels&#39;: [&#39;Symptom&#39;], &#39;start&#39;: 111, &#39;text&#39;: &#39;discharge&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;RBjrHSa1sj&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 145, &#39;labels&#39;: [&#39;External_body_part_or_region&#39;], &#39;start&#39;: 140, &#39;text&#39;: &#39;nares&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;yHEPWvrk9s&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 155, &#39;labels&#39;: [&#39;Gender&#39;], &#39;start&#39;: 152, &#39;text&#39;: &#39;she&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;dbeel0WXqw&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 177, &#39;labels&#39;: [&#39;Modifier&#39;], &#39;start&#39;: 173, &#39;text&#39;: &#39;mild&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;cFJwsYMe2k&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 210, &#39;labels&#39;: [&#39;Symptom&#39;], &#39;start&#39;: 178, &#39;text&#39;: &#39;breathing problems while feeding&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;PhiSDTDXlV&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 225, &#39;labels&#39;: [&#39;Date&#39;], &#39;start&#39;: 217, &#39;text&#39;: &#39;Nov 9/15&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;lsGep4SLRn&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 261, &#39;labels&#39;: [&#39;Symptom&#39;], &#39;start&#39;: 244, &#39;text&#39;: &#39;perioral cyanosis&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;WiTJIGOZ9Z&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 276, &#39;labels&#39;: [&#39;Symptom&#39;], &#39;start&#39;: 265, &#39;text&#39;: &#39;retractions&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;omIdHl5z74&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 290, &#39;labels&#39;: [&#39;RelativeDate&#39;], &#39;start&#39;: 279, &#39;text&#39;: &#39;One day ago&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;CqDclquhmD&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 295, &#39;labels&#39;: [&#39;Gender&#39;], &#39;start&#39;: 292, &#39;text&#39;: &#39;mom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;u8Q3GTVzZh&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 359, &#39;labels&#39;: [&#39;Drug_BrandName&#39;], &#39;start&#39;: 352, &#39;text&#39;: &#39;Tylenol&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;i2JLPQOUxv&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 27, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 17, &#39;text&#39;: &#39;Age&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;QShr4s6bpg&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 32, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 28, &#39;text&#39;: &#39;Gender&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;800DYq0quS&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 48, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 38, &#39;text&#39;: &#39;Duration&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;ns3P70kktN&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 62, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 52, &#39;text&#39;: &#39;Symptom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;tQOdI1ANUO&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 77, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 69, &#39;text&#39;: &#39;Date&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;oEwYVnyi2A&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 83, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 80, &#39;text&#39;: &#39;Gender&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;PkCkXQEIFN&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 110, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 104, &#39;text&#39;: &#39;Modifier&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;m9Bz8CzaXd&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 120, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 111, &#39;text&#39;: &#39;Symptom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;GBIhXo8nks&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 145, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 140, &#39;text&#39;: &#39;External_body_part_or_region&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;CDVDDIwVrl&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 155, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 152, &#39;text&#39;: &#39;Gender&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;CHPuFoT9iK&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 177, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 173, &#39;text&#39;: &#39;Modifier&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;eS4vXGth7v&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 210, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 178, &#39;text&#39;: &#39;Symptom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;2nDgfsoGZ7&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 225, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 217, &#39;text&#39;: &#39;Date&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;dhYC0U4sKG&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 261, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 244, &#39;text&#39;: &#39;Symptom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;TKN6DIP2ua&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 276, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 265, &#39;text&#39;: &#39;Symptom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;7X9EwULTA1&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 290, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 279, &#39;text&#39;: &#39;RelativeDate&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;UIFKYTDKcm&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 295, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 292, &#39;text&#39;: &#39;Gender&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;U6TqOYf3Ez&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 359, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 352, &#39;text&#39;: &#39;Drug_BrandName&#39;}}]}], &#39;data&#39;: {&#39;title&#39;: &#39;task_0&#39;, &#39;text&#39;: &quot;The patient is a 21-day-old male here for 2 days of congestion since Nov 8/15 - mom has been suctioning yellow discharge from the patient&#39;s nares, plus she has noticed some mild breathing problems while feeding since Nov 9/15 (without signs of perioral cyanosis or retractions). One day ago, mom also noticed a tactile temperature and gave the patient Tylenol.&quot;}, &#39;id&#39;: 0}, {&#39;predictions&#39;: [{&#39;created_username&#39;: &#39;model&#39;, &#39;result&#39;: [{&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;qd28OkdmDO&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 28, &#39;labels&#39;: [&#39;Age&#39;], &#39;start&#39;: 17, &#39;text&#39;: &#39;5-month-old&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;UIZm8wCy3c&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 35, &#39;labels&#39;: [&#39;Age&#39;], &#39;start&#39;: 29, &#39;text&#39;: &#39;infant&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;KpMv4PIy21&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 68, &#39;labels&#39;: [&#39;Date&#39;], &#39;start&#39;: 63, &#39;text&#39;: &#39;Feb 8&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;Uyj3awC8jp&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 80, &#39;labels&#39;: [&#39;Symptom&#39;], &#39;start&#39;: 76, &#39;text&#39;: &#39;cold&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;Dt3xtm1l5A&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 87, &#39;labels&#39;: [&#39;Symptom&#39;], &#39;start&#39;: 82, &#39;text&#39;: &#39;cough&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;bp9yUFAUaE&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 103, &#39;labels&#39;: [&#39;Symptom&#39;], &#39;start&#39;: 93, &#39;text&#39;: &#39;runny nose&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;QhuFKxwFVk&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 113, &#39;labels&#39;: [&#39;Date&#39;], &#39;start&#39;: 110, &#39;text&#39;: &#39;Feb&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;m9ikgaeJMY&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 120, &#39;labels&#39;: [&#39;Gender&#39;], &#39;start&#39;: 117, &#39;text&#39;: &#39;Mom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;QXhhDJ6CXn&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 131, &#39;labels&#39;: [&#39;Gender&#39;], &#39;start&#39;: 128, &#39;text&#39;: &#39;she&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;YUCHE7GcHB&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 144, &#39;labels&#39;: [&#39;VS_Finding&#39;], &#39;start&#39;: 139, &#39;text&#39;: &#39;fever&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;xbphfajGY1&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 149, &#39;labels&#39;: [&#39;Gender&#39;], &#39;start&#39;: 146, &#39;text&#39;: &#39;She&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;xN5GuZpeUw&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 177, &#39;labels&#39;: [&#39;Symptom&#39;], &#39;start&#39;: 157, &#39;text&#39;: &#39;difficulty breathing&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;VK9lAjcVNy&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 185, &#39;labels&#39;: [&#39;Gender&#39;], &#39;start&#39;: 182, &#39;text&#39;: &#39;her&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;dqiohcfX4G&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 191, &#39;labels&#39;: [&#39;Symptom&#39;], &#39;start&#39;: 186, &#39;text&#39;: &#39;cough&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;18bjvjxuDL&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 212, &#39;labels&#39;: [&#39;Modifier&#39;], &#39;start&#39;: 209, &#39;text&#39;: &#39;dry&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;BB90sIXIYZ&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 274, &#39;labels&#39;: [&#39;Disease_Syndrome_Disorder&#39;], &#39;start&#39;: 271, &#39;text&#39;: &#39;flu&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;sAL9AHWvOa&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 28, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 17, &#39;text&#39;: &#39;Age&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;vyio7vnpmS&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 35, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 29, &#39;text&#39;: &#39;Age&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;r6T6e8WmO9&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 68, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 63, &#39;text&#39;: &#39;Date&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;3SdFeft6ya&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 80, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 76, &#39;text&#39;: &#39;Symptom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;0iyfhRx1nl&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 87, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 82, &#39;text&#39;: &#39;Symptom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;pqJFZRu8Zu&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 103, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 93, &#39;text&#39;: &#39;Symptom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;zRa9noedl5&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 113, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 110, &#39;text&#39;: &#39;Date&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;RJ8MHb5Css&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 120, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 117, &#39;text&#39;: &#39;Gender&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;sbtQpMnkxH&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 131, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 128, &#39;text&#39;: &#39;Gender&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;K0yEKeG7GR&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 144, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 139, &#39;text&#39;: &#39;VS_Finding&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;V4fTVAh4Ro&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 149, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 146, &#39;text&#39;: &#39;Gender&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;1K1NUt9mcU&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 177, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 157, &#39;text&#39;: &#39;Symptom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;kXl3bnMSqM&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 185, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 182, &#39;text&#39;: &#39;Gender&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;spqjsrISZg&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 191, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 186, &#39;text&#39;: &#39;Symptom&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;EcrKDs2yyH&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 212, &#39;labels&#39;: [&#39;present&#39;], &#39;start&#39;: 209, &#39;text&#39;: &#39;Modifier&#39;}}, {&#39;from_name&#39;: &#39;label&#39;, &#39;id&#39;: &#39;FHYcyz14aj&#39;, &#39;source&#39;: &#39;$text&#39;, &#39;to_name&#39;: &#39;text&#39;, &#39;type&#39;: &#39;labels&#39;, &#39;value&#39;: {&#39;end&#39;: 274, &#39;labels&#39;: [&#39;absent&#39;], &#39;start&#39;: 271, &#39;text&#39;: &#39;Disease_Syndrome_Disorder&#39;}}]}], &#39;data&#39;: {&#39;title&#39;: &#39;task_1&#39;, &#39;text&#39;: &#39;The patient is a 5-month-old infant who presented initially on Feb 8 with a cold, cough, and runny nose since Feb 2. Mom states she had no fever. She had no difficulty breathing and her cough was described as dry and hacky. At that time, physical exam showed no signs of flu.&#39;}, &#39;id&#39;: 1}] An annotation summary is also generated that can be used to setup and configure a new project. { &#39;ner_labels&#39;: [ &#39;Age&#39;, &#39;VS_Finding&#39;, &#39;Gender&#39;, &#39;Modifier&#39;, &#39;Duration&#39;, &#39;RelativeDate&#39;, &#39;Symptom&#39;, &#39;Date&#39;, &#39;External_body_part_or_region&#39;, &#39;Disease_Syndrome_Disorder&#39;, &#39;Drug_BrandName&#39; ], &#39;assertion_labels&#39;: [&#39;present&#39;, &#39;absent&#39;], &#39;re_labels&#39;: [] } Interacting with Annotation Lab Credentials are required for the following actions. Set Credentials alab = AnnotationLab() username=&#39;&#39; password=&#39;&#39; client_secret=&quot;&quot; annotationlab_url=&quot;&quot; alab.set_credentials( # required: username username=username, # required: password password=password, # required: secret for you Annotation Lab instance (every Annotation Lab installation has a different secret) client_secret=client_secret, # required: http(s) url for you annotation lab annotationlab_url=annotationlab_url ) Get All visible projects alab.get_all_projects() Operation completed successfully. Response code: 200 {&#39;has_next&#39;: False, &#39;has_prev&#39;: False, &#39;items&#39;: [{&#39;creation_date&#39;: &#39;Thu, 29 Sep 2022 18:01:07 GMT&#39;, &#39;group_color&#39;: None, &#39;group_id&#39;: None, &#39;group_name&#39;: None, &#39;owner&#39;: &#39;hasham&#39;, &#39;owner_id&#39;: &#39;ba60df4b-7192-47ca-aa92-759fa577a617&#39;, &#39;project_description&#39;: &#39;&#39;, &#39;project_id&#39;: 1129, &#39;project_members&#39;: [&#39;hasham&#39;], &#39;project_name&#39;: &#39;alab_demo&#39;, &#39;resource_id&#39;: &#39;1dabaac8-54a0-4c52-a876-8c01f42b44e7&#39;, &#39;total_tasks&#39;: 2}, {&#39;creation_date&#39;: &#39;Tue, 27 Sep 2022 03:12:18 GMT&#39;, &#39;group_color&#39;: None, &#39;group_id&#39;: None, &#39;group_name&#39;: None, &#39;owner&#39;: &#39;hasham&#39;, &#39;owner_id&#39;: &#39;ba60df4b-7192-47ca-aa92-759fa577a617&#39;, &#39;project_description&#39;: &#39;&#39;, &#39;project_id&#39;: 1117, &#39;project_members&#39;: [&#39;hasham&#39;], &#39;project_name&#39;: &#39;testing101&#39;, &#39;resource_id&#39;: &#39;b1388775-9a3b-436e-b1cc-ea36bab44699&#39;, &#39;total_tasks&#39;: 9}, {&#39;creation_date&#39;: &#39;Fri, 06 Nov 2020 12:08:02 GMT&#39;, &#39;group_color&#39;: &#39;#dbdf2e&#39;, &#39;group_id&#39;: 10, &#39;group_name&#39;: &#39;MT_Samples&#39;, &#39;owner&#39;: &#39;mauro&#39;, &#39;owner_id&#39;: &#39;7b6048c8-f923-46e4-9011-2c749e3c2c93&#39;, &#39;project_description&#39;: &#39;&#39;, &#39;project_id&#39;: 126, &#39;project_members&#39;: [], &#39;project_name&#39;: &#39;PathologyReports&#39;, &#39;resource_id&#39;: &#39;7ed36c55-db19-48e0-bc56-4b2114f9a251&#39;, &#39;total_tasks&#39;: 97}], &#39;iter_pages&#39;: [1], &#39;next_num&#39;: None, &#39;prev_num&#39;: None, &#39;total_count&#39;: 3} Create a new project alab.create_project( # required: unique name of project project_name = &#39;alab_demo&#39;, # optional: other details about project. Default: Empty string project_description=&#39;&#39;, # optional: Sampling option of tasks. Default: random project_sampling=&#39;&#39;, # optional: Annotation Guidelines of project project_instruction=&#39;&#39; ) Operation completed successfully. Response code: 201 {&#39;project_name&#39;: &#39;alab_demo&#39;} Delete a project alab.delete_project( # required: unique name of project project_name = &#39;alab_demo&#39;, # optional: confirmation for deletion. Default: False - will ask for confirmation. If set to true, will delete directly. confirm=False ) Deleting Project. Press &quot;Y&quot; to confirm.y Operation completed successfully. Response code: 200 {&#39;message&#39;: &#39;Project successfully Deleted!&#39;} Set / Edit configuration of a project ## First, recreate a project alab.create_project( # required: unique name of project project_name = &#39;alab_demo&#39;, # optional: other details about project. Default: Empty string project_description=&#39;&#39;, # optional: Sampling option of tasks. Default: random project_sampling=&#39;&#39;, # optional: Annotation Guidelines of project project_instruction=&#39;&#39; ) Operation completed successfully. Response code: 201 {&#39;project_name&#39;: &#39;alab_demo&#39;} Set Configuration - First Time ## set configuration - first time alab.set_project_config( # required: name of project project_name = &#39;alab_demo&#39;, # optional: labels of classes for classification tasks classification_labels=[&#39;Male&#39;, &#39;Female&#39;], # optional: labels of classes for classification tasks ner_labels=[&#39;Age&#39;, &#39;Symptom&#39;, &#39;Procedure&#39;, &#39;BodyPart&#39;], # optional: labels of classes for classification tasks assertion_labels=[&#39;absent&#39;, &#39;family_history&#39;, &#39;someone_else&#39;], # optional: labels of classes for classification tasks relations_labels=[&#39;is_related&#39;] ) Operation completed successfully. Response code: 201 {&#39;messages&#39;: [{&#39;message&#39;: &#39;Project config saved.&#39;, &#39;success&#39;: True}]} Edit Configuration - add classes and labels ## Note: At least one type of labels should be provided. ## Note: to define relation labels, NER labels should be provided. alab.set_project_config( # required: name of project project_name = &#39;alab_demo&#39;, # optional: labels of classes for classification tasks classification_labels=[&#39;Male&#39;, &#39;Female&#39;, &#39;Unknown&#39;], # optional: labels of classes for classification tasks ner_labels=[&#39;Age&#39;, &#39;Symptom&#39;, &#39;Procedure&#39;, &#39;BodyPart&#39;, &#39;Test&#39;, &#39;Drug&#39;], # optional: labels of classes for classification tasks assertion_labels=[&#39;absent&#39;, &#39;family_history&#39;, &#39;someone_else&#39;], # optional: labels of classes for classification tasks relations_labels=[&#39;is_related&#39;, &#39;is_reactioni_of&#39;] ) Operation completed successfully. Response code: 201 {&#39;messages&#39;: [{&#39;message&#39;: &#39;Project config saved.&#39;, &#39;success&#39;: True}]} Set Configuration using summary generated at the pre-annotation step alab.set_project_config( # required: name of project project_name = &#39;alab_demo&#39;, # optional: labels of classes for classification tasks classification_labels=[&#39;Male&#39;, &#39;Female&#39;, &#39;Unknown&#39;], # optional: labels of classes for classification tasks ner_labels=summary[&#39;ner_labels&#39;], # optional: labels of classes for classification tasks assertion_labels=summary[&#39;assertion_labels&#39;], # optional: labels of classes for classification tasks relations_labels=[&#39;is_related&#39;, &#39;is_reactioni_of&#39;] ) Operation completed successfully. Response code: 201 {&#39;messages&#39;: [{&#39;message&#39;: &#39;Project config saved.&#39;, &#39;success&#39;: True}]} Get configuration of any project alab.get_project_config( # required: name of project project_name = &#39;alab_demo&#39; ) Operation completed successfully. Response code: 200 {&#39;analytics_permission&#39;: {}, &#39;annotators&#39;: [&#39;hasham&#39;], &#39;config&#39;: {&#39;allow_delete_completions&#39;: True, &#39;debug&#39;: False, &#39;editor&#39;: {&#39;debug&#39;: False}, &#39;enable_predictions_button&#39;: True, &#39;input_path&#39;: None, &#39;instruction&#39;: &#39;&#39;, &#39;ml_backends&#39;: [], &#39;output_dir&#39;: &#39;completions&#39;, &#39;port&#39;: 8200, &#39;sampling&#39;: &#39;uniform&#39;, &#39;templates_dir&#39;: &#39;examples&#39;, &#39;title&#39;: &#39;alab_demo&#39;}, &#39;created_version&#39;: &#39;4.0.1&#39;, &#39;creation_date&#39;: &#39;Thu, 29 Sep 2022 18:46:02 GMT&#39;, &#39;evaluation_info&#39;: None, &#39;group_id&#39;: None, &#39;isVisualNER&#39;: None, &#39;label_config&#39;: &#39;&#39;, &#39;labels&#39;: [&#39;Age&#39;, &#39;VS_Finding&#39;, &#39;Gender&#39;, &#39;Modifier&#39;, &#39;Duration&#39;, &#39;RelativeDate&#39;, &#39;Symptom&#39;, &#39;Date&#39;, &#39;External_body_part_or_region&#39;, &#39;Disease_Syndrome_Disorder&#39;, &#39;Drug_BrandName&#39;, &#39;present&#39;, &#39;absent&#39;], &#39;model_types&#39;: [{&#39;choices&#39;: [&#39;sentiment&#39;], &#39;name&#39;: &#39;classification&#39;}, {&#39;name&#39;: &#39;ner&#39;}, {&#39;name&#39;: &#39;assertion&#39;}], &#39;ocr_info&#39;: None, &#39;owner&#39;: {&#39;id&#39;: &#39;ba60df4b-7192-47ca-aa92-759fa577a617&#39;, &#39;username&#39;: &#39;hasham&#39;}, &#39;project_description&#39;: &#39;&#39;, &#39;project_id&#39;: 1130, &#39;project_name&#39;: &#39;alab_demo&#39;, &#39;resource_id&#39;: &#39;e8e17001-a25b-4a92-b419-88948d917647&#39;, &#39;tasks_count&#39;: 0, &#39;team_members_order&#39;: [&#39;hasham&#39;]} Upload tasks to a project # Define a list of tasks/string to upload txt1 = &quot;The patient is a 21-day-old male here for 2 days of congestion since Nov 8/15 - mom has been suctioning yellow discharge from the patient&#39;s nares, plus she has noticed some mild breathing problems while feeding since Nov 9/15 (without signs of perioral cyanosis or retractions). One day ago, mom also noticed a tactile temperature and gave the patient Tylenol.&quot; txt2 = &quot;The patient is a 5-month-old infant who presented initially on Feb 8 with a cold, cough, and runny nose since Feb 2. Mom states she had no fever. She had no difficulty breathing and her cough was described as dry and hacky. At that time, physical exam showed no signs of flu.&quot; task_list = [txt1, txt2] alab.upload_tasks( # required: name of project to upload tasks to project_name=&#39;alab_demo&#39;, # required: list of examples / tasks as string (One string is one task). task_list=task_list, # optional: Option to assign custom titles to tasks. By default, tasks will be titled as &#39;task_#&#39; title_list = [], # optional: If there are already tasks in project, then this id offset can be used to make sure default titles &#39;task_#&#39; do not overlap. # While upload a batch after the first one, this can be set to number of tasks currently present in the project # This number would be added to each tasks&#39;s ID and title. id_offset=0 ) Uploading 2 task(s). Operation completed successfully. Response code: 201 {&#39;completion_count&#39;: 0, &#39;duration&#39;: 0.11868953704833984, &#39;failed_count&#39;: 0, &#39;ignored_count&#39;: 0, &#39;prediction_count&#39;: 0, &#39;task_count&#39;: 2, &#39;task_ids&#39;: [1, 2], &#39;task_title_warning&#39;: 0, &#39;updated_count&#39;: 0} Delete tasks of a project alab.delete_tasks( # required: name of project to upload tasks to project_name=&#39;alab_demo&#39;, # required: list of ids of tasks. # note: you can get task ids from the above step. Look for &#39;task_ids&#39; key. task_ids=[1, 2], # optional: confirmation for deletion. Default: False - will ask for confirmation. If set to true, will delete directly. confirm=False ) Deleting 2 task(s). Press &quot;Y&quot; to confirm.y Operation completed successfully. Response code: 200 {&#39;message&#39;: &#39;Task(s) successfully deleted!&#39;} Upload pre-annotations to Annotation Lab You can get the data for pre_annotations from this section. alab.upload_preannotations( # required: name of project to upload annotations to project_name = &#39;alab_demo&#39;, # required: preannotation JSON preannotations = pre_annotations ) Uploading 2 preannotation(s). Operation completed successfully. Response code: 201 {&#39;completion_count&#39;: 0, &#39;duration&#39;: 0.14992427825927734, &#39;failed_count&#39;: 0, &#39;ignored_count&#39;: 0, &#39;prediction_count&#39;: 2, &#39;task_count&#39;: 2, &#39;task_ids&#39;: [1, 2], &#39;task_title_warning&#39;: 0, &#39;updated_count&#39;: 0}",
    "url": "/docs/en/alab/healthcare",
    "relUrl": "/docs/en/alab/healthcare"
  },
  "63": {
    "id": "63",
    "title": "Risk Adjustments Score Calculation",
    "content": "Our Risk Adjustment Score implementation uses the Hierarchical Condition Category (HCC) Risk Adjustment model from the Centers for Medicare &amp; Medicaid Service (CMS). HCC groups similar conditions in terms of healthcare costs and similarities in the diagnosis, and the model uses any ICD code that has a corresponging HCC category in the computation, discarding other ICD codes. This module supports versions 22, 23, and 24 of the CMS-HCC risk adjustment model and needs the following parameters in order to calculate the risk score: ICD Codes (Obtained by, e.g., our pretrained model sbiobertresolve_icd10cm_augmented_billable_hcc from theSentenceEntityResolverModel annotator) Age (Obtained by, e.g., our pretrained model ner_jsl from theNerModel annotator) Gender (Obtained by, e.g., our pretrained model classifierdl_gender_biobert from theClassifierDLModel annotator) The eligibility segment of the patient (information from the health plan provider) The original reason for entitlement (information from the health plan provider) If the patient is in Medicaid or not (information from the health plan provider) Available softwares and profiles As mentioned, we implemented versions 22, 23, and 24 of the CMS-HCC software, and have the following profiles: Version 22 Year 2017 Year 2018 Year 2019 Year 2020 Year 2021 Year 2022 Version 23 Year 2018 Year 2019 Version 24 Year 2017 Year 2018 Year 2019 Year 2020 Year 2021 Year 2022 Usage The module can perform the computations given a data frame containing the required information (Age, Gender, ICD codes, eligibility segment, the original reason for entitlement, and if the patient is in Medicaid or not). For example, given the dataset df: +--++--+-++--+-+--+-+ | filename|Age| icd10_code|Extracted_Entities_vs_ICD_Codes|Gender|eligibility|orec|medicaid| DOB| +--++--+-++--+-+--+-+ |mt_note_03.txt| 66|[C499, C499, D618...| [{leiomyosarcoma,...| F| CND| 1| false|1956-05-30| |mt_note_01.txt| 59| [C801]| [{cancer, C801}]| F| CFA| 0| true|1961-10-12| |mt_note_10.txt| 16| [C6960, C6960]| [{Rhabdomyosarcom...| M| CFA| 2| false|2006-02-14| |mt_note_08.txt| 66| [C459, C800]| [{malignant mesot...| F| CND| 1| true|1956-03-17| |mt_note_09.txt| 19| [D5702, K5505]| [{Sickle cell cri...| M| CPA| 3| true|2003-06-11| |mt_note_05.txt| 57|[C5092, C5091, C5...| [{Breast Cancer, ...| F| CPA| 3| true|1963-08-12| |mt_note_06.txt| 63| [F319, F319]| [{type 1 bipolar ...| F| CFA| 0| false|1959-07-24| +--++--+-++--+-+--+-+ Where column orec means original reason for entitlement and DOB means date of birth (can also be used to compute age). You can use any of the available profiles to compute the scores (in the example, we use version 24, year 2020): from johnsnowlabs import medical # Creates the risk profile df = df.withColumn( &quot;hcc_profile&quot;, medical.profileV24Y20( df.icd10_code, df.Age, df.Gender, df.eligibility, df.orec, df.medicaid ), ) # Extract relevant information df = ( df.withColumn(&quot;risk_score&quot;, df.hcc_profile.getItem(&quot;risk_score&quot;)) .withColumn(&quot;hcc_lst&quot;, df.hcc_profile.getItem(&quot;hcc_lst&quot;)) .withColumn(&quot;parameters&quot;, df.hcc_profile.getItem(&quot;parameters&quot;)) .withColumn(&quot;details&quot;, df.hcc_profile.getItem(&quot;details&quot;)) ) df.select( &quot;filename&quot;, &quot;risk_score&quot;, &quot;icd10_code&quot;, &quot;Age&quot;, &quot;Gender&quot;, &quot;eligibility&quot;, &quot;orec&quot;, &quot;medicaid&quot;, ).show(truncate=False) +--+-++++--+-+--+ filename |risk_score|icd10_code |Age|Gender|eligibility|orec|medicaid| +--+-++++--+-+--+ mt_note_01.txt|0.158 |[C801] |59 |F |CFA |0 |true | mt_note_03.txt|1.03 |[C499, C499, D6181, M069, C801] |66 |F |CND |1 |false | mt_note_05.txt|2.991 |[C5092, C5091, C779, C5092, C800, G20, C5092]|57 |F |CPA |3 |true | mt_note_06.txt|0.299 |[F319] |63 |F |CFA |0 |true | mt_note_08.txt|2.714 |[C459, C800] |66 |F |CND |1 |false | mt_note_09.txt|1.234 |[D5702, K5505] |19 |F |CPA |3 |true | +--+-++++--+-+--+ For more details and usage examples, check the notebook Medicare Risk Adjustment notebook from our Spark NLP Workshop repository.",
    "url": "/docs/en/healthcare_risk_adjustments_score_calculation",
    "relUrl": "/docs/en/healthcare_risk_adjustments_score_calculation"
  },
  "64": {
    "id": "64",
    "title": "Identify & Translate Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/identify_translate_languages",
    "relUrl": "/identify_translate_languages"
  },
  "65": {
    "id": "65",
    "title": "Import Documents",
    "content": "Once a new project is created and its configuration is saved, the user is redirected to the Import page. Here the user has multiple options for importing tasks. Users can import the accepted file formats in multiple ways. They can drag and drop the file(s) to the upload box, select the file from the file explorer, provide the URL of the file in JSON format, or import it directly from the S3 bucket. To import from Amazon S3 bucket the user needs to provide the necessary connection details (credentials, access keys, and S3 bucket path). All documents present in the specified path, are then imported as tasks in the current project. Plain text file When you upload a plain text file, only one task will be created which will contain the entire data in the input file. This is an update from earlier versions of Annotation Lab when the input text file was split by the new line character and one task was created for each line. Json file For bulk importing a list of documents you can use the json import option. The expected format is illustrated in the image below. It consists of a list of dictionaries, each with 2 keys-values pairs (“text” and “title”). [{&quot;text&quot;: &quot;Task text content.&quot;, &quot;title&quot;:&quot;Task title&quot;}] CSV, TSV file When CSV / TSV formatted text file is used, column names are interpreted as task data keys: Task text content, Task title this is a first task, Colon Cancer.txt this is a second task, Breast radiation therapy.txt Import annotated tasks When importing tasks that already contain annotations (e.g. exported from another project, with predictions generated by pre-trained models) the user has the option to overwrite completions/predictions or to skip the tasks that are already imported into the project. NOTE: When importing tasks from different projects with the purpose of combining them in one project, users should take care of the overlaps existing between tasks IDs. Annotation Lab will simply overwrite tasks with the same ID. Dynamic Task Pagination The support for pagination offered by earlier versions of the Annotation Lab involved the use of the &lt;pagebreak&gt; tag. A document pre-processing step was necessary for adding/changing the page breaks and those involved extra effort from the part of the users. Annotation Lab 2.8.0 introduces a paradigm change for pagination. Going forward, pagination is dynamic and can be configured according to the user’s needs and preferences from the Labeling page. Annotators or reviewers can now choose the number of words to include on a single page from a predefined list of values or can add the desired counts. A new settings option has been added to prevent splitting a sentence into two different pages.",
    "url": "/docs/en/alab/import",
    "relUrl": "/docs/en/alab/import"
  },
  "66": {
    "id": "66",
    "title": "Spark NLP: <span>State of the Art Natural Language Processing</span>",
    "content": "",
    "url": "/",
    "relUrl": "/"
  },
  "67": {
    "id": "67",
    "title": "Infer Meaning & Intent - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/infer_meaning_intent",
    "relUrl": "/infer_meaning_intent"
  },
  "68": {
    "id": "68",
    "title": "Infrastructure Configuration",
    "content": "The admin user can now define the infrastructure configurations for the prediction and training tasks. Resource allocation for Training and Preannotation Annotation Lab gives users the ability to change the configuration for the training and pre-annotation processes. This is done from the Settings &gt; Infrastructure page. The settings can be edited by admin user and they are read-only for the other users. The Infrastructure page consists of three sections namely Resource For Training, Resource For Preannotation Server, Resources for Prenotation Pipeline. Resources Inclusion: Memory Limit – Represents the maximum memory size to allocate for the training/pre-annotation processes. CPU Limit – Specifies this maximum number of CPUs to use by the training/pre-annotation server. Note: If the specified configurations exceed the available resources, the server will not start.",
    "url": "/docs/en/alab/infrastructure",
    "relUrl": "/docs/en/alab/infrastructure"
  },
  "69": {
    "id": "69",
    "title": "Installation",
    "content": "Type of installation Dedicated Server AWS Marketplace Azure Marketplace EKS deployment AKS deployment AirGap Environment OpenShift Dedicated Server Install NLP Lab (Annotation Lab) on a dedicated server to reduce the likelihood of conflicts or unexpected behavior. Fresh install To install NLP Lab run the following command: wget https://setup.johnsnowlabs.com/annotationlab/install.sh -O - | sudo bash -s $VERSION Replace $VERSION in the above one liners with the version you want to install. For installing the latest available version of the NLP Lab use: wget https://setup.johnsnowlabs.com/annotationlab/install.sh -O - | sudo bash -s -- Upgrade To upgrade your NLP Lab installation to a newer version, run the following command on a terminal: wget https://setup.johnsnowlabs.com/annotationlab/upgrade.sh -O - | sudo bash -s $VERSION Replace $VERSION in the above one liners with the version you want to upgrade to. For upgrading to the latest version of the NLP Lab, use: wget https://setup.johnsnowlabs.com/annotationlab/upgrade.sh -O - | sudo bash -s -- NOTE: The install/upgrade script displays the login credentials for the admin user on the terminal. After running the install/upgrade script, the NLP Lab is available at http://INSTANCE_IP or https://INSTANCE_IP We have an aesthetically pleasing Sign-In Page with a section highlighting the key features of NLP Lab using animated GIFs. AWS Marketplace Visit the product page on AWS Marketplace and follow the instructions on the video below to subscribe and deploy. Deploy NLP Lab via AWS Marketplace Azure Marketplace Visit the product page on Azure Marketplace and follow the instructions on the video below to subscribe and deploy. Deploy NLP Lab via Azure Marketplace EKS deployment Create NodeGroup for a given cluster eksctl create nodegroup --config-file eks-nodegroup.yaml kind: ClusterConfig apiVersion: eksctl.io/v1alpha5 metadata: name: &lt;cluster-name&gt; region: &lt;region&gt; version: &quot;1.21&quot; availabilityZones: - &lt;zone-1&gt; - &lt;zone-2&gt; vpc: id: &quot;&lt;vpc-id&gt;&quot; subnets: private: us-east-1d: id: &quot;&lt;subnet-id&quot; us-east-1f: id: &quot;&lt;subent-id&gt;&quot; securityGroup: &quot;&lt;security-group&gt;&quot; iam: withOIDC: true managedNodeGroups: - name: alab-workers instanceType: m5.large desiredCapacity: 3 VolumeSize: 50 VolumeType: gp2 privateNetworking: true ssh: publicKeyPath: &lt;path/to/id_rsa_pub&gt; eksctl utils associate-iam-oidc-provider --region=us-east-1 --cluster=&lt;cluster-name&gt; --approve Create an EFS as shared storage. EFS stands for Elastic File System and is a scalable storage solution that can be used for general purpose workloads. curl -S https://raw.githubusercontent.com/kubernetes-sigs/aws-efs-csi-driver/v1.2.0/docs/iam-policy-example.json -o iam-policy.json aws iam create-policy --policy-name EFSCSIControllerIAMPolicy --policy-document file://iam-policy.json eksctl create iamserviceaccount --cluster=&lt;cluster&gt; --region &lt;AWS Region&gt; --namespace=kube-system --name=efs-csi-controller-sa --override-existing-serviceaccounts --attach-policy-arn=arn:aws:iam::&lt;AWS account ID&gt;:policy/EFSCSIControllerIAMPolicy --approve helm repo add aws-efs-csi-driver https://kubernetes-sigs.github.io/aws-efs-csi-driver helm repo update helm upgrade -i aws-efs-csi-driver aws-efs-csi-driver/aws-efs-csi-driver --namespace kube-system --set image.repository=602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/aws-efs-csi-driver --set controller.serviceAccount.create=false --set controller.serviceAccount.name=efs-csi-controller-sa Create storageClass.yaml cat &lt;&lt;EOF &gt; storageClass.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: efs-sc provisioner: efs.csi.aws.com parameters: provisioningMode: efs-ap fileSystemId: &lt;EFS file system ID&gt; directoryPerms: &quot;700&quot; EOF kubectl apply -f storageClass.yaml Edit annotationlab-installer.sh inside artifact folder as follows: helm install annotationlab annotationlab-${ANNOTATIONLAB_VERSION}.tgz --set image.tag=${ANNOTATIONLAB_VERSION} --set model_server.count=1 --set ingress.enabled=true --set networkPolicy.enabled=true --set networkPolicy.enabled=true --set extraNetworkPolicies=&#39;- namespaceSelector: matchLabels: kubernetes.io/metadata.name: kube-system podSelector: matchLabels: app.kubernetes.io/name: traefik app.kubernetes.io/instance: traefik&#39; --set keycloak.postgresql.networkPolicy.enabled=true --set sharedData.storageClass=efs-sc --set airflow.postgresql.networkPolicy.enabled=true --set postgresql.networkPolicy.enabled=true --set airflow.networkPolicies.enabled=true --set ingress.defaultBackend=true --set ingress.uploadLimitInMegabytes=16 --set &#39;ingress.hosts[0].host=domain.tld&#39; --set airflow.model_server.count=1 --set airflow.redis.password=$(bash -c &quot;echo ${password_gen_string}&quot;) --set configuration.FLASK_SECRET_KEY=$(bash -c &quot;echo ${password_gen_string}&quot;) --set configuration.KEYCLOAK_CLIENT_SECRET_KEY=$(bash -c &quot;echo ${uuid_gen_string}&quot;) --set postgresql.postgresqlPassword=$(bash -c &quot;echo ${password_gen_string}&quot;) --set keycloak.postgresql.postgresqlPassword=$(bash -c &quot;echo ${password_gen_string}&quot;) --set keycloak.secrets.admincreds.stringData.user=admin --set keycloak.secrets.admincreds.stringData.password=$(bash -c &quot;echo ${password_gen_string}&quot;) Run annotationlab-installer.sh script ./artifacts/annotationlab-installer.sh Install ingress Controller helm repo add nginx-stable https://helm.nginx.com/stable helm repo update helm install my-release nginx-stable/nginx-ingress Apply ingress.yaml cat &lt;&lt;EOF &gt; ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: annotationlab meta.helm.sh/release-namespace: default name: annotationlab spec: defaultBackend: service: name: annotationlab port: name: http rules: - host: domain.tld http: paths: - backend: service: name: annotationlab port: name: http path: / pathType: ImplementationSpecific - backend: service: name: annotationlab-keyclo-http port: name: http path: /auth pathType: ImplementationSpecific EOF kubectl apply -f ingress.yaml AKS deployment To deploy NLP Lab on Azure Kubernetes Service (AKS) a Kubernetes cluster needs to be created in Microsoft Azure. Login to your Azure Portal and search for Kubernetes services. On the Kubernetes services page click on the Create dropdown and select Create a Kubernetes cluster. On the Create Kubernetes cluster page, select the resource group and provide the name you want to give to the cluster. You can keep the rest of the fields to default values and click on Review + create. Click on Create button to start the deployment process. Once the deployment is completed, click on Go to resource button. On the newly created resource page, click on Connect button. You will be shown a list of commands to run on the Cloud Shell or Azure CLI to connect to this resource. We will execute them successively in the following steps. Run the following commands to connect to Azure Kubernetes Service. az account set --subscription &lt;subscription-id&gt; NOTE: Replace with your account&#39;s subscription id. az aks get-credentials --resource-group &lt;resource-group-name&gt; --name &lt;cluster-name&gt; NOTE: Replace and with what you selected in Step 3. Check to see if azurefile or azuredisk storage class is present by running the following command: kubectl get storageclass Later in the helm script we need to update the value of sharedData.storageClass with the respective storage class. Go to the artifact directory and from there edit the annotationlab-installer.sh script. helm install annotationlab annotationlab-${ANNOTATIONLAB_VERSION}.tgz --set image.tag=${ANNOTATIONLAB_VERSION} --set model_server.count=1 --set ingress.enabled=true --set networkPolicy.enabled=true --set networkPolicy.enabled=true --set extraNetworkPolicies=&#39;- namespaceSelector: matchLabels: kubernetes.io/metadata.name: kube-system podSelector: matchLabels: app.kubernetes.io/name: traefik app.kubernetes.io/instance: traefik&#39; --set keycloak.postgresql.networkPolicy.enabled=true --set sharedData.storageClass=azurefile --set airflow.postgresql.networkPolicy.enabled=true --set postgresql.networkPolicy.enabled=true --set airflow.networkPolicies.enabled=true --set ingress.defaultBackend=true --set ingress.uploadLimitInMegabytes=16 --set &#39;ingress.hosts[0].host=domain.tld&#39; --set airflow.model_server.count=1 --set airflow.redis.password=$(bash -c &quot;echo ${password_gen_string}&quot;) --set configuration.FLASK_SECRET_KEY=$(bash -c &quot;echo ${password_gen_string}&quot;) --set configuration.KEYCLOAK_CLIENT_SECRET_KEY=$(bash -c &quot;echo ${uuid_gen_string}&quot;) --set postgresql.postgresqlPassword=$(bash -c &quot;echo ${password_gen_string}&quot;) --set keycloak.postgresql.postgresqlPassword=$(bash -c &quot;echo ${password_gen_string}&quot;) --set keycloak.secrets.admincreds.stringData.user=admin --set keycloak.secrets.admincreds.stringData.password=$(bash -c &quot;echo ${password_gen_string}&quot;) Execute the annotationlab-installer.sh script to run the NLP Lab installation. ./annotationlab-installer.sh Verify if the installation was successful. kubectl get pods Install ingress controller. This will be required for load-balancing purpose. helm repo add nginx-stable https://helm.nginx.com/stable helm repo update helm install my-release nginx-stable/nginx-ingress Create a YAML configuration file named ingress.yaml with the following configuration apiVersion: networking.k8s.io/v1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx meta.helm.sh/release-name: annotationlab meta.helm.sh/release-namespace: default name: annotationlab spec: defaultBackend: service: name: annotationlab port: name: http rules: - host: domain.tld http: paths: - backend: service: name: annotationlab port: name: http path: / pathType: ImplementationSpecific - backend: service: name: annotationlab-keyclo-http port: name: http path: /auth pathType: ImplementationSpecific Apply the ingress.yaml by running the following command kubectl apply -f ingress.yaml AirGap Environment Get Artifact Run the following command on a terminal to fetch the compressed artifact (tarball) of the NLP Lab. wget https://s3.amazonaws.com/auxdata.johnsnowlabs.com/annotationlab/annotationlab-$VERSION.tar.gz Extract the tarball and the change directory to the extracted folder (artifacts): tar -xzf annotationlab-$VERSION.tar.gz cd artifacts Replace $VERSION with the version you want to download and install. Fresh Install Run the installer script annotationlab-installer.sh with sudo privileges. $ sudo su $ ./annotationlab-installer.sh Upgrade Run the upgrade script annotationlab-updater.sh with sudo privileges. $ sudo su $ ./annotationlab-updater.sh OpenShift Annotation Lab can also be installed using the operator framework on an OpenShift cluster. The Annotation Lab operator can be found under the OperatorHub. Find and select The OperatorHub has a large list of operators that can be installed into your cluster. Search for Annotation Lab operator under AI/Machine Learning category and select it. Install Some basic information about this operator is provided on the navigation panel that opens after selecting Annotation Lab on the previous step. NOTE: Make sure you have defined shared storage such as efs/nfs/cephfs prior to installing the Annotation Lab Operator. Click on the Install button located on the top-left corner of this panel to start the installation process. After successful installation of the Annotation Lab operator, you can access it by navigating to the Installed Operators page. Create Instance Next step is to create a cluster instance of the Annotation Lab. For this, select the Annotation Lab operator under the Installed Operators page and then switch to Annotationlab tab. On this section, click on Create Annotationlab button to spawn a new instance of Annotation Lab. Define shared Storage Class Update the storageClass property in the YAML configuration to define the storage class to one of efs, nfs, or cephfs depending upon what storage you set up before Annotation Lab operator installation. Define domain name Update the host property in the YAML configuration to define the required domain name to use instead of the default hostname annotationlab as shown in the image below. Click on Create button once you have made all the necessary changes. This will also set up all the necessary resources to run the instance in addition to standing up the services themselves. View Resources After the instance is successfully created we can visit its page to view all the resources as well as supporting resources like the secrets, configuration maps, etc that were created. Now, we can access the Annotation Lab from the provided domain name or also from the location defined for this service under the Networking &gt; Routes page Work over proxy Custom CA certificate You can provide a custom CA certificate chain to be included into the deployment. To do it add --set-file custom_cacert=./cachain.pem options to helm install/upgrade command inside annotationlab-installer.sh and annotationlab-updater.sh files. cachain.pem must include a certificate in the following format: --BEGIN CERTIFICATE-- .... --END CERTIFICATE-- Proxy env variables You can provide a proxy to use for external communications. To do that add `--set proxy.http=[protocol://]&lt;host&gt;[:port]`, `--set proxy.https=[protocol://]&lt;host&gt;[:port]`, `--set proxy.no=&lt;comma-separated list of hosts/domains&gt;` commands inside annotationlab-installer.sh and annotationlab-updater.sh files. Recommended Configurations System requirements You can install Annotation Lab on a Ubuntu 20+ machine. Port requirements Annotation Lab expects ports 443 and 80 to be open by default. Server requirements The minimal required configuration is 32GB RAM, 8 Core CPU, 512 SSD. The ideal configuration in case model training and preannotations are required on a large number of tasks is 64 GiB, 16 Core CPU, 2TB HDD, 512 SSD. Web browser support Annotation Lab is tested with the latest version of Google Chrome and is expected to work in the latest versions of: Google Chrome Apple Safari Mozilla Firefox",
    "url": "/docs/en/alab/install",
    "relUrl": "/docs/en/alab/install"
  },
  "70": {
    "id": "70",
    "title": "Installation",
    "content": "Spark NLP Cheatsheet # Install Spark NLP from PyPI pip install spark-nlp==4.3.1 # Install Spark NLP from Anacodna/Conda conda install -c johnsnowlabs spark-nlp # Load Spark NLP with Spark Shell spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.1 # Load Spark NLP with PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.1 # Load Spark NLP with Spark Submit spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.1 # Load Spark NLP as external JAR after compiling and building Spark NLP by `sbt assembly` spark-shell --jars spark-nlp-assembly-4.3.1.jar Python Spark NLP supports Python 3.6.x and above depending on your major PySpark version. Quick Install Let’s create a new Conda environment to manage all the dependencies there. You can use Python Virtual Environment if you prefer or not have any environment. $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.8 -y $ conda activate sparknlp $ pip install spark-nlp==4.3.1 pyspark==3.3.1 Of course you will need to have jupyter installed in your system: pip install jupyter Now you should be ready to create a jupyter notebook running from terminal: jupyter notebook Start Spark NLP Session from python If you need to manually start SparkSession because you have other configurations and sparknlp.start() is not including them, you can manually start the SparkSession: spark = SparkSession.builder .appName(&quot;Spark NLP&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;,&quot;16G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.1&quot;) .getOrCreate() Scala and Java Maven spark-nlp on Apache Spark 3.0.x, 3.1.x, 3.2.x, and 3.3.x: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-m1: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-m1 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-m1_2.12&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;/dependency&gt; spark-nlp-aarch64: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-aarch64 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-aarch64_2.12&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;/dependency&gt; SBT spark-nlp on Apache Spark 3.0.x, 3.1.x, 3.2.x, and 3.3.x: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp&quot; % &quot;4.3.1&quot; spark-nlp-gpu: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-gpu&quot; % &quot;4.3.1&quot; spark-nlp-m1: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-m1 libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-m1&quot; % &quot;4.3.1&quot; spark-nlp-aarch64: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-aarch64 libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-aarch64&quot; % &quot;4.3.1&quot; Maven Central: https://mvnrepository.com/artifact/com.johnsnowlabs.nlp If you are interested, there is a simple SBT project for Spark NLP to guide you on how to use it in your projects Spark NLP SBT Starter Installation for M1 Macs Starting from version 4.0.0, Spark NLP has experimental support for M1 macs. Note that at the moment, only the standard variant of the M1 is supported. Other variants (e.g. M1 Pro/Max/Ultra, M2) will most likely not work. Make sure the following prerequisites are met: An M1 compiled java version needs to be installed. For example to install the Zulu Java 11 JDK head to Download Azul JDKs and install that java version. To check if the installed java environment is running natively on arm64 and not rosetta, you can run the following commands in your shell: johnsnow@m1mac ~ % cat $(which java) | file - /dev/stdin: Mach-O 64-bit executable arm64 The environment variable JAVA_HOME should also be set to this java version. You can check this by running echo $JAVA_HOME in your terminal. If it is not set, you can set it by adding export JAVA_HOME=$(/usr/libexec/java_home) to your ~/.zshrc file. If you are planning to use Annotators or Pipelines that use the RocksDB library (for example WordEmbeddings, TextMatcher or explain_document_dl_en Pipeline respectively) with spark-submit, then a workaround is required to get it working. See M1 RocksDB workaround for spark-submit with Spark version &gt;= 3.2.0. M1 RocksDB workaround for spark-submit with Spark version &gt;= 3.2.0 Starting from Spark version 3.2.0, Spark includes their own version of the RocksDB dependency. Unfortunately, this is an older version of RocksDB does not include the necessary binaries of M1. To work around this issue, the default packaged RocksDB jar has to be removed from the Spark distribution. For example, if you downloaded Spark version 3.2.0 from the official archives, you will find the following folders in the directory of Spark: $ ls bin conf data examples jars kubernetes LICENSE licenses NOTICE python R README.md RELEASE sbin yarn To check for the RocksDB jar, you can run $ ls jars | grep rocksdb rocksdbjni-6.20.3.jar to find the jar you have to remove. After removing the jar, the pipelines should work as expected. Scala and Java for M1 Adding Spark NLP to your Scala or Java project is easy: Simply change to dependency coordinates to spark-nlp-m1 and add the dependency to your project. How to do this is mentioned above: Scala And Java So for example for Spark NLP with Apache Spark 3.0.x and 3.1.x you will end up with maven coordinates like these: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-m1 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-m1_2.12&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;/dependency&gt; or in case of sbt: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-m1&quot; % &quot;4.3.1&quot; If everything went well, you can now start Spark NLP with the m1 flag set to true: import com.johnsnowlabs.nlp.SparkNLP val spark = SparkNLP.start(m1 = true) Python for M1 First, make sure you have a recent Python 3 installation. johnsnow@m1mac ~ % python3 --version Python 3.9.13 Then we can install the dependency as described in the Python section. It is also recommended to use a virtual environment for this. If everything went well, you can now start Spark NLP with the m1 flag set to True: import sparknlp spark = sparknlp.start(m1=True) Installation for Linux Aarch64 Systems Starting from version 4.3.1, Spark NLP supports Linux systems running on an aarch64 processor architecture. The necessary dependencies have been built on Ubuntu 16.04, so a recent system with an environment of at least that will be needed. Check the Python section and the Scala And Java section on to install Spark NLP for your system. Starting Spark NLP Spark NLP needs to be started with the aarch64 flag set to true: For Scala: import com.johnsnowlabs.nlp.SparkNLP val spark = SparkNLP.start(aarch64 = true) For Python: import sparknlp spark = sparknlp.start(aarch64=True) Google Colab Notebook Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account. Run the following code in Google Colab notebook and start using spark-nlp right away. # This is only to setup PySpark and Spark NLP on Colab !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash This script comes with the two options to define pyspark and spark-nlp versions via options: # -p is for pyspark # -s is for spark-nlp # by default they are set to the latest !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.2.3 -s 4.3.1 Spark NLP quick start on Google Colab is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines. Kaggle Kernel Run the following code in Kaggle Kernel and start using spark-nlp right away. # Let&#39;s setup Kaggle for Spark NLP and PySpark !wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash Spark NLP quick start on Kaggle Kernel is a live demo on Kaggle Kernel that performs named entity recognitions by using Spark NLP pretrained pipeline. Databricks Support Spark NLP 4.3.1 has been tested and is compatible with the following runtimes: CPU: 7.3 7.3 ML 9.1 9.1 ML 10.1 10.1 ML 10.2 10.2 ML 10.3 10.3 ML 10.4 10.4 ML 10.5 10.5 ML 11.0 11.0 ML 11.1 11.1 ML GPU: 9.1 ML &amp; GPU 10.1 ML &amp; GPU 10.2 ML &amp; GPU 10.3 ML &amp; GPU 10.4 ML &amp; GPU 10.5 ML &amp; GPU 11.0 ML &amp; GPU 11.1 ML &amp; GPU NOTE: Spark NLP 4.0.x is based on TensorFlow 2.7.x which is compatible with CUDA11 and cuDNN 8.0.2. The only Databricks runtimes supporting CUDA 11 are 9.x and above as listed under GPU. Install Spark NLP on Databricks Create a cluster if you don’t have one already On a new cluster or existing one you need to add the following to the Advanced Options -&gt; Spark tab: spark.kryoserializer.buffer.max 2000M spark.serializer org.apache.spark.serializer.KryoSerializer In Libraries tab inside your cluster you need to follow these steps: 3.1. Install New -&gt; PyPI -&gt; spark-nlp -&gt; Install 3.2. Install New -&gt; Maven -&gt; Coordinates -&gt; com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.1 -&gt; Install Now you can attach your notebook to the cluster and use Spark NLP! NOTE: Databrick’s runtimes support different Apache Spark major releases. Please make sure you choose the correct Spark NLP Maven pacakge name (Maven Coordinate) for your runtime from our Packages Cheatsheet Databricks Notebooks You can view all the Databricks notebooks from this address: https://johnsnowlabs.github.io/spark-nlp-workshop/databricks/index.html Note: You can import these notebooks by using their URLs. EMR Support Spark NLP 4.3.1 has been tested and is compatible with the following EMR releases: emr-6.2.0 emr-6.3.0 emr-6.3.1 emr-6.4.0 emr-6.5.0 emr-6.6.0 Full list of Amazon EMR 6.x releases NOTE: The EMR 6.1.0 and 6.1.1 are not supported. How to create EMR cluster via CLI To lanuch EMR cluster with Apache Spark/PySpark and Spark NLP correctly you need to have bootstrap and software configuration. A sample of your bootstrap script #!/bin/bash set -x -e echo -e &#39;export PYSPARK_PYTHON=/usr/bin/python3 export HADOOP_CONF_DIR=/etc/hadoop/conf export SPARK_JARS_DIR=/usr/lib/spark/jars export SPARK_HOME=/usr/lib/spark&#39; &gt;&gt; $HOME/.bashrc &amp;&amp; source $HOME/.bashrc sudo python3 -m pip install awscli boto spark-nlp set +x exit 0 A sample of your software configuration in JSON on S3 (must be public access): [{ &quot;Classification&quot;: &quot;spark-env&quot;, &quot;Configurations&quot;: [{ &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;PYSPARK_PYTHON&quot;: &quot;/usr/bin/python3&quot; } }] }, { &quot;Classification&quot;: &quot;spark-defaults&quot;, &quot;Properties&quot;: { &quot;spark.yarn.stagingDir&quot;: &quot;hdfs:///tmp&quot;, &quot;spark.yarn.preserve.staging.files&quot;: &quot;true&quot;, &quot;spark.kryoserializer.buffer.max&quot;: &quot;2000M&quot;, &quot;spark.serializer&quot;: &quot;org.apache.spark.serializer.KryoSerializer&quot;, &quot;spark.driver.maxResultSize&quot;: &quot;0&quot;, &quot;spark.jars.packages&quot;: &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.1&quot; } } ] A sample of AWS CLI to launch EMR cluster: aws emr create-cluster --name &quot;Spark NLP 4.3.1&quot; --release-label emr-6.2.0 --applications Name=Hadoop Name=Spark Name=Hive --instance-type m4.4xlarge --instance-count 3 --use-default-roles --log-uri &quot;s3://&lt;S3_BUCKET&gt;/&quot; --bootstrap-actions Path=s3://&lt;S3_BUCKET&gt;/emr-bootstrap.sh,Name=custome --configurations &quot;https://&lt;public_access&gt;/sparknlp-config.json&quot; --ec2-attributes KeyName=&lt;your_ssh_key&gt;,EmrManagedMasterSecurityGroup=&lt;security_group_with_ssh&gt;,EmrManagedSlaveSecurityGroup=&lt;security_group_with_ssh&gt; --profile &lt;aws_profile_credentials&gt; GCP Dataproc Support Create a cluster if you don’t have one already as follows. At gcloud shell: gcloud services enable dataproc.googleapis.com compute.googleapis.com storage-component.googleapis.com bigquery.googleapis.com bigquerystorage.googleapis.com REGION=&lt;region&gt; BUCKET_NAME=&lt;bucket_name&gt; gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME} REGION=&lt;region&gt; ZONE=&lt;zone&gt; CLUSTER_NAME=&lt;cluster_name&gt; BUCKET_NAME=&lt;bucket_name&gt; You can set image-version, master-machine-type, worker-machine-type, master-boot-disk-size, worker-boot-disk-size, num-workers as your needs. If you use the previous image-version from 2.0, you should also add ANACONDA to optional-components. And, you should enable gateway. gcloud dataproc clusters create ${CLUSTER_NAME} --region=${REGION} --zone=${ZONE} --image-version=2.0 --master-machine-type=n1-standard-4 --worker-machine-type=n1-standard-2 --master-boot-disk-size=128GB --worker-boot-disk-size=128GB --num-workers=2 --bucket=${BUCKET_NAME} --optional-components=JUPYTER --enable-component-gateway --metadata &#39;PIP_PACKAGES=spark-nlp spark-nlp-display google-cloud-bigquery google-cloud-storage&#39; --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh On an existing one, you need to install spark-nlp and spark-nlp-display packages from PyPI. Now, you can attach your notebook to the cluster and use the Spark NLP! Amazon Linux 2 Support # Update Package List &amp; Install Required Packages sudo yum update sudo yum install -y amazon-linux-extras sudo yum -y install python3-pip # Create Python virtual environment and activate it: python3 -m venv .sparknlp-env source .sparknlp-env/bin/activate Check JAVA version: For Sparknlp versions above 3.x, please use JAVA-11 Checking Java versions installed on your machine: sudo alternatives --config java You can pick the index number (I am using java-8 as default - index 2): If you dont have java-11 or java-8 in you system, you can easily install via: sudo yum install java-1.8.0-openjdk Now, we can start installing the required libraries: pip install pyspark==3.3.1 pip install spark-nlp Docker Support For having Spark NLP, PySpark, Jupyter, and other ML/DL dependencies as a Docker image you can use the following template: #Download base image ubuntu 18.04 FROM ubuntu:18.04 ENV NB_USER jovyan ENV NB_UID 1000 ENV HOME /home/${NB_USER} ENV PYSPARK_PYTHON=python3 ENV PYSPARK_DRIVER_PYTHON=python3 RUN apt-get update &amp;&amp; apt-get install -y tar wget bash rsync gcc libfreetype6-dev libhdf5-serial-dev libpng-dev libzmq3-dev python3 python3-dev python3-pip unzip pkg-config software-properties-common graphviz RUN adduser --disabled-password --gecos &quot;Default user&quot; --uid ${NB_UID} ${NB_USER} # Install OpenJDK-8 RUN apt-get update &amp;&amp; apt-get install -y openjdk-8-jdk &amp;&amp; apt-get install -y ant &amp;&amp; apt-get clean; # Fix certificate issues RUN apt-get update &amp;&amp; apt-get install ca-certificates-java &amp;&amp; apt-get clean &amp;&amp; update-ca-certificates -f; # Setup JAVA_HOME -- useful for docker commandline ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/ RUN export JAVA_HOME RUN echo &quot;export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/&quot; &gt;&gt; ~/.bashrc RUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* RUN pip3 install --upgrade pip # You only need pyspark and spark-nlp paclages to use Spark NLP # The rest of the PyPI packages are here as examples RUN pip3 install --no-cache-dir pyspark spark-nlp==3.2.3 notebook==5.* numpy pandas mlflow Keras scikit-spark scikit-learn scipy matplotlib pydot tensorflow==2.4.1 graphviz # Make sure the contents of our repo are in ${HOME} RUN mkdir -p /home/jovyan/tutorials RUN mkdir -p /home/jovyan/jupyter COPY data ${HOME}/data COPY jupyter ${HOME}/jupyter COPY tutorials ${HOME}/tutorials RUN jupyter notebook --generate-config COPY jupyter_notebook_config.json /home/jovyan/.jupyter/jupyter_notebook_config.json USER root RUN chown -R ${NB_UID} ${HOME} USER ${NB_USER} WORKDIR ${HOME} # Specify the default command to run CMD [&quot;jupyter&quot;, &quot;notebook&quot;, &quot;--ip&quot;, &quot;0.0.0.0&quot;] Finally, use jupyter_notebook_config.json for the password: { &quot;NotebookApp&quot;: { &quot;password&quot;: &quot;sha1:65adaa6ffb9c:36df1c2086ef294276da703667d1b8ff38f92614&quot; } } Windows Support In order to fully take advantage of Spark NLP on Windows (8 or 10), you need to setup/install Apache Spark, Apache Hadoop, Java and a Pyton environment correctly by following the following instructions: https://github.com/JohnSnowLabs/spark-nlp/discussions/1022 How to correctly install Spark NLP on Windows Follow the below steps to set up Spark NLP with Spark 3.2.3: Download Adopt OpenJDK 1.8 Make sure it is 64-bit Make sure you install it in the root of your main drive C: java. During installation after changing the path, select setting Path Download the pre-compiled Hadoop binaries winutils.exe, hadoop.dll and put it in a folder called C: hadoop bin from https://github.com/cdarlint/winutils/tree/master/hadoop-3.2.0/bin Note: The version above is for Spark 3.2.3, which was built for Hadoop 3.2.0. You might have to change the hadoop version in the link, depending on which Spark version you are using. Download Apache Spark 3.2.3 and extract it to C: spark. Set/add environment variables for HADOOP_HOME to C: hadoop and SPARK_HOME to C: spark. Add %HADOOP_HOME% bin and %SPARK_HOME% bin to the PATH environment variable. Install Microsoft Visual C++ 2010 Redistributed Package (x64). Create folders C: tmp and C: tmp hive If you encounter issues with permissions to these folders, you might need to change the permissions by running the following commands: %HADOOP_HOME% bin winutils.exe chmod 777 /tmp/hive %HADOOP_HOME% bin winutils.exe chmod 777 /tmp/ Requisites for PySpark We recommend using conda to manage your Python environment on Windows. Download Miniconda for Python 3.8 See Quick Install on how to set up a conda environment with Spark NLP. The following environment variables need to be set: PYSPARK_PYTHON=python Optionally, if you want to use the Jupyter Notebook runtime of Spark: first install it in the environment with conda install notebook then set PYSPARK_DRIVER_PYTHON=jupyter, PYSPARK_DRIVER_PYTHON_OPTS=notebook The environment variables can either be directly set in windows, or if only the conda env will be used, with conda env config vars set PYSPARK_PYTHON=python. After setting the variable with conda, you need to deactivate and re-activate the environment. Now you can use the downloaded binary by navigating to %SPARK_HOME% bin and running Either create a conda env for python 3.6, install pyspark==3.3.1 spark-nlp numpy and use Jupyter/python console, or in the same conda env you can go to spark bin for pyspark –packages com.johnsnowlabs.nlp:spark-nlp_2.12:4.3.1. Offline Spark NLP library and all the pre-trained models/pipelines can be used entirely offline with no access to the Internet. If you are behind a proxy or a firewall with no access to the Maven repository (to download packages) or/and no access to S3 (to automatically download models and pipelines), you can simply follow the instructions to have Spark NLP without any limitations offline: Instead of using the Maven package, you need to load our Fat JAR Instead of using PretrainedPipeline for pretrained pipelines or the .pretrained() function to download pretrained models, you will need to manually download your pipeline/model from Models Hub, extract it, and load it. Example of SparkSession with Fat JAR to have Spark NLP offline: spark = SparkSession.builder .appName(&quot;Spark NLP&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;,&quot;16G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars&quot;, &quot;/tmp/spark-nlp-assembly-4.3.1.jar&quot;) .getOrCreate() You can download provided Fat JARs from each release notes, please pay attention to pick the one that suits your environment depending on the device (CPU/GPU) and Apache Spark version (3.x) If you are local, you can load the Fat JAR from your local FileSystem, however, if you are in a cluster setup you need to put the Fat JAR on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., hdfs:///tmp/spark-nlp-assembly-4.3.1.jar) Example of using pretrained Models and Pipelines in offline: # instead of using pretrained() for online: # french_pos = PerceptronModel.pretrained(&quot;pos_ud_gsd&quot;, lang=&quot;fr&quot;) # you download this model, extract it, and use .load french_pos = PerceptronModel.load(&quot;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) # example for pipelines # instead of using PretrainedPipeline # pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;) # you download this pipeline, extract it, and use PipelineModel PipelineModel.load(&quot;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&quot;) Since you are downloading and loading models/pipelines manually, this means Spark NLP is not downloading the most recent and compatible models/pipelines for you. Choosing the right model/pipeline is on you If you are local, you can load the model/pipeline from your local FileSystem, however, if you are in a cluster setup you need to put the model/pipeline on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., hdfs:///tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/)",
    "url": "/docs/en/install",
    "relUrl": "/docs/en/install"
  },
  "71": {
    "id": "71",
    "title": "Install NLP Libraries",
    "content": "Spark NLP For installing Spark NLP on your infrastructure please follow the instructions detailed here. Spark NLP for Healthcare For installing Spark NLP for Healthcare please follow the instructions detailed here. Spark OCR For installing Spark OCR please follow the instructions detailed here.",
    "url": "/docs/en/install_NLP_libraries",
    "relUrl": "/docs/en/install_NLP_libraries"
  },
  "72": {
    "id": "72",
    "title": "Installation",
    "content": "Deploy using Docker For deploying NLP Server on your instance run the following command. docker run --pull=always -p 5000:5000 johnsnowlabs/nlp-server:latest This will check if the latest docker image is available on your local machine and if not it will automatically download and run it. If you want to keep downloaded models between restarts of the docker image, you can mount a volume. mkdir /var/cache_pretrained chown 1000:1000 /var/cache_pretrained docker run --pull=always -v /var/cache_pretrained:/home/johnsnowlabs/cache_pretrained -p 5000:5000 johnsnowlabs/nlp-server:latest Deploy using AWS Marketplace NLP Server on AWS Marketplace provides one of the fastest and easiest ways to get up and running on Amazon Web Services (AWS). NLP Server is available through AWS Marketplace free of charge. However, to use licensed spells in NLP Server, you need to buy our license from here. You can get NLP Server on AWS Marketplace from this URL. Follow the seven steps instructions or the video tutorial given below to learn how to deploy NLP Server using AWS Marketplace. Make sure you have a valid AWS account and log in to the AWS Marketplace using your credentials. Deploy NLP Server via AWS Marketplace 1.Click on Continue to subscribe button for creating a subscription to the NLP Server product. The software is free of charge. 2.Read the subscription EULA and click on Accept terms button if you want to continue. 3.In a couple of seconds the subscription becomes active. Once it is active you see this screen. 4.Go to AWS Marketplace &gt; Manage subscriptions and click on the Launch new instance button corresponding to the NLP Server subscription. This will redirect you to the following screen. Click on Continue to launch through EC2 button. 5.From the available options select the instance type you want to use for the deployment. Then click on Review and Lauch button. 6.Select an existing key pair or create a new one. This ensures a secured connection to the instance. If you create a new key make sure that you download and safely store it for future usage. Click on the Launch button. 7.While the instance is starting you will see this screen. Then the instance will appear on your EC2 Instances list. The NLP Server can now be accessed via a web browser at http://PUBLIC_EC2_IP . API documentation is also available at http://PUBLIC_EC2_IP/docs Deploy using Azure Marketplace NLP Server on Azure Marketplace provides one of the fastest and easiest ways to get up and running on Microsoft Azure. NLP Server is available through Azure Marketplace free of charge. However, to use licensed spells in NLP Server, you need to buy our license from here. You can get NLP Server on Azure Marketplace from this URL. Follow the video tutorial given below to learn how to deploy NLP Server using Azure Marketplace. Deploy NLP Server using Azure Marketplace",
    "url": "/docs/en/nlp_server/installation",
    "relUrl": "/docs/en/nlp_server/installation"
  },
  "73": {
    "id": "73",
    "title": "Labs, Tests, and Vitals - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/labs_tests_and_vitals",
    "relUrl": "/labs_tests_and_vitals"
  },
  "74": {
    "id": "74",
    "title": "African Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/languages_africa",
    "relUrl": "/languages_africa"
  },
  "75": {
    "id": "75",
    "title": "Languages of India - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/languages_india",
    "relUrl": "/languages_india"
  },
  "76": {
    "id": "76",
    "title": "",
    "content": "",
    "url": "/latest.html",
    "relUrl": "/latest.html"
  },
  "77": {
    "id": "77",
    "title": "Learn",
    "content": "Introductions to Spark NLP Videos State of the Art Natural Language Processing at Scale. David Talby - April 13, 2020 Spark NLP: State of the art natural language processing at scale. David Talby - 4 Jun 2020 What is Spark NLP. John Snow Labs - 30 Jul 2019 Apache Spark NLP Extending Spark ML to Deliver Fast, Scalable, and Unified Natural Language Process. David Talby - 6 May 2019 Natural Language Understanding at Scale with Spark Native NLP, Spark ML &amp;TensorFlow with Alex Thomas. Alex Thomas - 26 Oct 2017 Articles Introducing the Natural Language Processing Library for Apache SparkDavid Talby - October 19, 2017 Improving Clinical Document Understanding on COVID-19 Research with Spark NLPVeysel Kocaman, David Talby - 7 December, 2020 Topic Modelling with PySpark and Spark NLPMaria Obedkova - May 29, 2020 Installing Spark NLP and Spark OCR in air-gapped networks (offline mode)Veysel Kocaman - May 04, 2020 Cleaning and extracting text from HTML/XML documents by using Spark NLPStefano Lori - Jan 13, 2020 A Google Colab Notebook Introducing Spark NLPVeysel Kocaman - September, 2020 State-of-the-art Natural Language Processing at ScaleDavid Talby - April 13, 2020 How to Wrap Your Head Around Spark NLPMustafa Aytuğ Kaya - August 25, 2020 5 Reasons Why Spark NLP Is The Most Widely Used Library In EnterprisesAmbika Choudhury - May 28, 2019 My Experience with SparkNLP Workshop &amp; CertificationAngelina Maria Leigh - August 17, 2020 Out of the box Spark NLP models in actionDia Trambitas - August 14, 2020 Get started with Machine Learning in Java using Spark NLPWill Price - August 27, 2020 SPARK NLP 3: MASSIVE SPEEDUPS &amp; THE LATEST COMPUTE PLATFORMSMaziyar Panahi - March 25, 2021 SPARK NLP 2.7: 720+ NEW MODELS &amp; PIPELINES FOR 192 LANGUAGES!David Talby - January 05, 2021 Python’s NLU Library Videos &quot;Python&#39;s NLU library: 1,000+ Models, 200+ Languages, 1 Line of Code&quot; by: Christian Kasim Loan - 18 June 2021 John Snow Labs NLU: Become a Data Science Superhero with One Line of Python code. Christian Kasim Loan - November, 2020 Articles 1 line to GLOVE Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to XLNET Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to ALBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to COVIDBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to ELECTRA Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to BioBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 Line of Code, 350 + NLP Models with John Snow Labs’ NLU in PythonChristian Kasim Loan - September 21, 2020 Easy sentence similarity with BERT Sentence Embeddings using John Snow Labs NLUChristian Kasim Loan - November 20, 2020 Training Deep Learning NLP Classifier TutorialChristian Kasim Loan - November 20, 2020 1 Python Line for ELMo Word Embeddings and t-SNE plots with John Snow Labs’ NLUChristian Kasim Loan - October 24, 2020 1 line of Python code for BERT, ALBERT, ELMO, ELECTRA, XLNET, GLOVE, Part of Speech with NLU and t-SNEChristian Kasim Loan - September 21, 2020 1 line to BERT Word Embeddings with NLU in PythonChristian Kasim Loan - September 21, 2020 Question answering, intent classification, aspect based ner, and new multilingual models in python’s NLU libraryChristian Kasim Loan - February 12, 2021 Intent and action classification, analyze chinese news and crypto market, 200+ languages &amp; answer questions with NLU 1.1.3Christian Kasim Loan - March 02, 2021 Hindi wordembeddings, bengali named entity recognition, 30+ new models, analyze crypto news with NLU 1.1.2Christian Kasim Loan - February 18, 2021 Named Entity Recognition Videos State-of-the-art Clinical Named Entity Recognition in Spark NLP Workshop - Veysel Kocaman Train your own NerDL. John Snow Labs - 7 Oct 2019 Articles State-of-the-art named entity recognition with BERTVeysel Kocaman - February 26th, 2020 State-of-the-art Named Entity Recognition in Spark NLPVeysel Kocaman Spark NLP in action: intelligent, high-accuracy fact extraction from long financial documentsSaif Addin Ellafi - May 5, 2020 Named Entity Recognition (NER) with BERT in Spark NLPVeysel Kocaman - Mar 4, 2020 Document Classification Videos Spark NLP in Action: Learning to read Life Science research - Saif Addin Ellafi. Saif Addin Ellafi - 1 Aug 2018 State of the art emotion and sentiment analysis with Spark NLP (Data science Salon). Dia Trambitas - December 1, 2020 Articles GloVe, ELMo &amp; BERT. A guide to state-of-the-art text classification using Spark NLP Ryan Burke - March 16, 2021 Distributed Topic Modelling using Spark NLP and Spark MLLib(LDA)Satish Silveri - June 11, 2020 Text Classification in Spark NLP with Bert and Universal Sentence EncodersVeysel Kocaman - April 12, 2020 Classification of Unstructured Documents into the Environmental, Social &amp; Governance (ESG) TaxonomyAlina Petukhova - May, 2020 Using Spark NLP to build a drug discovery knowledge graph for COVID-19Vishnu Vettrivel, Alexander Thomas - October 8, 2020 Build Text Categorization Model with Spark NLPSatish Silveri - Jul 8 2020 Topic Modelling with PySpark and Spark NLPMaria Obedkova - May 29 2020 Spark NLP Tasks &amp; Pipelines Videos Spark NLP Annotators, Annotations and Pipelines. John Snow Labs - 23 Oct 2019 Your first Spark NLP Pipeline. John Snow Labs - 23 Oct 2019 Natural Language Understanding at Scale with Spark NLP | DSS 2020. Veysel Kocaman - December 12, 2020 Articles Cleaning and extracting text from HTML/XML documents by using Spark NLPStefano Lori - January 13 2021 NER model with ELMo Embeddings in 5 minutes in Spark-NLPChristian Kasim Loan - Jule 2020 Applying Context Aware Spell Checking in Spark NLPAlberto Andreotti - May 2020 Spark nlp 2.5 delivers state-of-the-art accuracy for spell checking and sentiment analysisIda Lucente - May 12, 2020 Spark NLP 2.4: More Accurate NER, OCR, and Entity ResolutionIda Lucente - February 14, 2020 Introduction to Spark NLP: Foundations and Basic Components (Part-I)Veysel Kocaman - Sep 29, 2019 Introducing Spark NLP: Why would we need another NLP library (Part-I)Veysel Kocaman - October 22, 2019 Introducing Spark NLP: basic components and underlying technologies (Part-III)Veysel Kocaman - December 2, 2019 Explain document DL – Spark NLP pretrained pipelineVeysel Kocaman - January 15, 2020 Spark NLP Walkthrough, powered by TensorFlowSaif Addin Ellafi - Nov 19, 2018 Natural Language Processing with PySpark and Spark-NLPAllison Honold - Feb 5, 2020 Spark NLP for Healthcare Videos Advancing the State of the Art in Applied Natural Language Processing | Healthcare NLP Summit 2021. David Talby - 21 Apr 2021 How to Apply State-of-the-Art Natural Language Processing in Healthcare. David Talby - 15 Sep 2020 Advanced Natural Language Processing with Apache Spark NLP. David Talby - 20 Aug 2020 Applying State-of-the-art Natural Language Processing for Personalized Healthcare. David Talby - April 13, 2020 State-of-the-art Natural Language Processing at Scale. David Talby - April 13, 2020 Apache SPARK NLP: Extending SPARK ML to Deliver Fast, Scalable &amp; Unified Natural Language Processing. David Talby - June 04, 2018 State of the Art Natural Language Processing at Scale. David Talby - June 04, 2018 Spark NLP in Action: Learning to read Life Science research. Saif Addin Ellafi - May 28, 2018 Natural Language Understanding at Scale with Spark-Native NLP, Spark ML, and TensorFlow. Alexander Thomas - October 14, 2018 Apache Spark NLP for Healthcare: Lessons Learned Building Real-World Healthcare AI Systems. Veysel Kocaman - 9 Jul 2020 SNOMED entity resolver. John Snow Labs - 31 Jul 2020 NLP and its applications in Healthcare. Veysel Kocaman - 17 May 2020 Lessons Learned Building Real-World Healthcare AI Systems. Veysel Kocaman - April 13, 2020 Application of Spark NLP for Development of Multi-Modal Prediction Model from EHR | Healthcare NLP. Sutanay Choudhury - 14 Apr 2021 Best Practices in Improving NLP Accuracy for Clinical Use Cases I Healthcare NLP Summit 2021. Rajesh Chamarthi, Veysel Kocaman - 15 Apr 2021 Articles Contextual Parser: Increased Flexibility Extracting Entities in Spark NLPLuca Martial - Feb 09 2022 Named Entity Recognition for Healthcare with SparkNLP NerDL and NerCRFMaggie Yilmaz - Jul 20 2020 Roche automates knowledge extraction from pathology reports with Spark NLPCase Study Spark NLP in action: Improving patient flow forecastingCase Study Using Spark NLP to Enable Real-World Evidence (RWE) and Clinical Decision Support in OncologyVeysel Kocaman - April 13, 2020 Applying State-of-the-art Natural Language Processing for Personalized HealthcareDavid Talby - April 13, 2020 Automated Mapping of Clinical Entities from Natural Language Text to Medical TerminologiesAndrés Fernández - April 29 2020 Contextual Parser in Spark NLP: Extracting Medical Entities ContextuallyAlina Petukhova - May 28 2020 Deep6 accelerates clinical trial recruitment with Spark NLPCase Study SelectData uses AI to better understand home health patientsCase Study Explain Clinical Document Spark NLP Pretrained PipelineVeysel Kocaman - January 20, 2020 Introducing Spark NLP: State of the art NLP Package (Part-II)Veysel Kocaman - January 20, 2020 Automated Adverse Drug Event (ADE) Detection from Text in Spark NLP with BioBertVeysel Kocaman - Octover 4, 2020 Normalize drug names and dosage units with spark NLPDavid Cecchini - February 23, 2021 Spark NLP for healthcare 2.7.3 with biobert extraction models, higher accuracy, de-identification, new radiology ner model &amp; moreVeysel Kocaman - February 09, 2021 Spark OCR &amp; De-Identification Videos Maximizing Text Recognition Accuracy with Image Transformers in Spark OCR. Mykola Melnyk - June 24, 2020 Accurate de-identification, obfuscation, and editing of scanned medical documents and images. Alina Petukhova - August 19, 2020 Accurate De-Identification of Structured &amp; Unstructured Medical Data at Scale. Julio Bonis - March 18, 2020 Articles A Unified CV, OCR &amp; NLP Model Pipeline for Document Understanding at DocuSignPatrick Beukema, Michael Chertushkin - October 6, 2020 Scaling High-Accuracy Text Extraction from Images using Spark OCR on DatabricksMikola Melnyk - July 2, 2020 Spark NLP at Scale Videos Turbocharging State-of-the-art Natural Language Processing on Ray. David Talby - October 3, 2020 Articles Big Data Analysis of Meetup Events using Spark NLP, Kafka and Vegas VisualizationAndrei Deuşteanu - August 25, 2020 Setup Spark NLP on Databricks in 2 Minutes and get the taste of scalable NLPChristian Kasim Loan - May 25, 2020 Real-time trending topic detection using Spark NLP, Kafka and Vegas VisualizationValentina Crisan - Oct 15, 2020 Mueller Report for Nerds! Spark meets NLP with TensorFlow and BERTMaziyar Panahi - May 1, 2019 Spark in Docker in Kubernetes: A Practical Approach for Scalable NLPJürgen Schmidl - Jan 18 2020 Running Spark NLP in Docker Container for Named Entity Recognition and Other NLP FeaturesYuefeng Zhang - Jun 5 2020 Annotation Lab Videos Accelerating Clinical Data Abstraction and Real-World Data Curation with Active Learning, Dia Trambitas - Apr 15, 2021 MLOPS Veysel &amp; Dia. Dia Trambitas, Veysel Kocaman - July 16, 2020 Best Practices &amp; Tools for Accurate Document Annotation and Data Abstraction. Dia Trambitas - May 27, 2020 Articles John Snow Labs’ data annotator &amp; active learning for human-in-the-loop AI is now included with all subscriptionsIda Lucente - May 26, 2020 Auto NLP: Pretrain, Tune &amp; Deploy State-of-the-art Models Without CodingDia Trambitas - October 6, 2020 Lesson Learned annotating training data for healthcare NLP projectsRebecca Leung, Marianne Mak - October 8, 2020 Task review workflows in the annotation labDia Trambitas - March 08, 2021 The annotation lab 1.1 is here with improvements to speed, accuracy, and productivityIda Lucente - January 20, 2021 Tips and tricks on how to annotate assertion in clinical textsMauro Nievas Offidani - November 24, 2020 Spark NLP Benchmarks Articles Biomedical Named Entity Recognition at ScaleVeysel Kocaman, David Talby - November 12, 2020 NLP Industry Survey Analysis: the industry landscape of natural language use cases in 2020Paco Nathan - October 6, 2020 Comparing the Functionality of Open Source Natural Language Processing LibrariesMaziyar Panahi and David Talby - April 7, 2019 SpaCy or Spark NLP — A Benchmarking ComparisonMustafa Aytuğ Kaya - Aug 27, 2020 Comparing production-grade NLP libraries: Training Spark-NLP and spaCy pipelinesSaif Addin Ellafi - February 28, 2018 Comparing production-grade NLP libraries: Running Spark-NLP and spaCy pipelinesSaif Addin Ellafi - February 28, 2018 Comparing production-grade NLP libraries: Accuracy, performance, and scalabilitySaif Addin Ellafi - February 28, 2018 Spark NLP Awards Articles John Snow Labs is healthcare tech outlook’s 2020 healthcare analytics provider of the yearIda Lucente - July 14, 2020 John Snow Labs wins the 2020 artificial intelligence excellence awardIda Lucente - April 27, 2020 John Snow Labs is named ‘2019 ai platform of the yearIda Lucente - August 14, 2019 Spark NLP is the world’s most widely used nlp library by enterprise practitionersIda Lucente - May 6, 2019 John Snow Labs’ spark nlp wins “most significant open source project” at the strata data awardsIda Lucente April 1 - 2019 John Snow Labs named “artificial intelligence solution provider of the year” by cio reviewIda Lucente - February 7, 2019",
    "url": "/learnold",
    "relUrl": "/learnold"
  },
  "78": {
    "id": "78",
    "title": "The NLP Learning Hub",
    "content": "The Technology Spark NLP Healthcare NLP Spark OCR Annotation Lab Auto NLP Multimodal AI The Technology in Action Medical AI Applications Finance AI Applications De-Identification Multilingual NLP NLP on Databricks Industry Trends AI in Healthcare No-Code AI Responsible NL Data Philanthropy Announcements Awards",
    "url": "/learn",
    "relUrl": "/learn"
  },
  "79": {
    "id": "79",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/legal_assertion_status",
    "relUrl": "/legal_assertion_status"
  },
  "80": {
    "id": "80",
    "title": "Normalization & Data Augmentation - Legal NLP Demos & Notebooks",
    "content": "",
    "url": "/legal_company_normalization",
    "relUrl": "/legal_company_normalization"
  },
  "81": {
    "id": "81",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/legal_deidentification",
    "relUrl": "/legal_deidentification"
  },
  "82": {
    "id": "82",
    "title": "Recognize Legal Entities - Legal NLP Demos & Notebooks",
    "content": "",
    "url": "/legal_entity_recognition",
    "relUrl": "/legal_entity_recognition"
  },
  "83": {
    "id": "83",
    "title": "Extract Legal Relationships - Legal NLP Demos & Notebooks",
    "content": "",
    "url": "/legal_relation_extraction",
    "relUrl": "/legal_relation_extraction"
  },
  "84": {
    "id": "84",
    "title": "Legal NLP Release Notes",
    "content": "Releases log Legal NLP 1.0.0 on Medium Legal NLP 1.1.0 on Medium Legal NLP 1.2.0 on Medium Legal NLP 1.3.0 on Medium Legal NLP 1.4.0 on Medium Legal NLP 1.5.0 on Medium Legal NLP 1.6.0 on Medium Legal NLP 1.7.0 on Medium Legal NLP 1.8.0 on Medium Slack - Join #legal channel",
    "url": "/docs/en/legal_release_notes",
    "relUrl": "/docs/en/legal_release_notes"
  },
  "85": {
    "id": "85",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/legal_table_extraction",
    "relUrl": "/legal_table_extraction"
  },
  "86": {
    "id": "86",
    "title": "Classify Legal Texts - Legal NLP Demos & Notebooks",
    "content": "",
    "url": "/legal_text_classification",
    "relUrl": "/legal_text_classification"
  },
  "87": {
    "id": "87",
    "title": "Version Compatibility",
    "content": "Legal NLP runs on top of johnsnowlabs library (former nlu). Please find technical documentation about how to install it here. Legal NLP johnsnowlabs 1.X.X 4.X.X Check which version of Spark NLP, Visual NLP or even Clinical NLP are included in johnsnowlabs versions here Legal NLP is also supported in Annotation Lab from Alab 4.2.3 version on!",
    "url": "/docs/en/legal_version_compatibility",
    "relUrl": "/docs/en/legal_version_compatibility"
  },
  "88": {
    "id": "88",
    "title": "Enterprise Spark NLP",
    "content": "PythonScalaNLU spark-nlp-jsljohnsnowlabs ... pos = PerceptronModel.pretrained(&quot;pos_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;token&quot;,&quot;sentence&quot;]) .setOutputCol(&quot;pos&quot;) pos_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, pos]) light_pipeline = LightPipeline(pos_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;))) result = light_pipeline.fullAnnotate(&quot;&quot;&quot;He was given boluses of MS04 with some effect, he has since been placed on a PCA - he take 80mg of oxycontin at home, his PCA dose is ~ 2 the morphine dose of the oxycontin, he has also received ativan for anxiety.&quot;&quot;&quot;) ... pos = PerceptronModel.pretrained(&quot;pos_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;token&quot;,&quot;sentence&quot;]) .setOutputCol(&quot;pos&quot;) pos_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, pos]) light_pipeline = LightPipeline(pos_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;))) result = light_pipeline.fullAnnotate(&quot;&quot;&quot;He was given boluses of MS04 with some effect, he has since been placed on a PCA - he take 80mg of oxycontin at home, his PCA dose is ~ 2 the morphine dose of the oxycontin, he has also received ativan for anxiety.&quot;&quot;&quot;) val pos = PerceptronModel.pretrained(&quot;pos_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;token&quot;,&quot;sentence&quot;) .setOutputCol(&quot;pos&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, pos)) val data = Seq(&quot;He was given boluses of MS04 with some effect, he has since been placed on a PCA - he take 80mg of oxycontin at home, his PCA dose is ~ 2 the morphine dose of the oxycontin, he has also received ativan for anxiety.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) import nlu nlu.load(&quot;en.pos.clinical&quot;).predict(&quot;&quot;&quot;He was given boluses of MS04 with some effect, he has since been placed on a PCA - he take 80mg of oxycontin at home, his PCA dose is ~ 2 the morphine dose of the oxycontin, he has also received ativan for anxiety.&quot;&quot;&quot;) Getting started We call Enterprise Spark NLP libraries to all the commercial NLP libraries, including Healthcare NLP (former Spark NLP for Healthcare), Finance, Legal NLP, among others. This excludes Visual NLP (former Spark OCR), which has its own documentation page, available here. If you don’t have an Enterprise Spark NLP subscription yet, you can ask for a free trial by clicking on the Try Free button and following the instructions provides in the video below. Try Free 30-day free trials for the John Snow Labs NLP libraries can be obtained via AWS and Azure markeplaces. To get a free trial please subscribe to one of the pay-as-you-go products: John Snow Labs NLP Libraries - AWS Marketplace John Snow Labs NLP Libraries - Azure Marketplace Note: It is important to note that every AWS/Azure account is limited to one 30-day free trial period for John Snow Labs NLP Libraries, and users are responsible for verifying the status of any past trials before subscribing and being charged for usage. Enterprise Spark NLP libraries provides healthcare-specific annotators, pipelines, models, and embeddings for: Entity recognition Entity Linking Entity normalization Assertion Status Detection De-identification Relation Extraction Spell checking &amp; correction and much more!",
    "url": "/docs/en/license_getting_started",
    "relUrl": "/docs/en/license_getting_started"
  },
  "89": {
    "id": "89",
    "title": "Enterprise NLP Annotators",
    "content": "A Spark NLP Enterprise license includes access to unique annotators. At the Spark NLP Workshop you can see different types of annotators in action. By clicking on any annotator, you will see different sections: The Approach, or class to train models. The Model, to infer using pretrained models. Also, for most of the annotators, you will find examples for the different enterprise libraries: Healthcare NLP Finance NLP Legal NLP Check out the Spark NLP Annotators page for more information on how to read this page. Available Annotators Annotators Description AssertionDL AssertionDL is a deep Learning based approach used to extract Assertion Status from extracted entities and text. AssertionFilterer Filters entities coming from ASSERTION type annotations and returns the CHUNKS. AssertionLogReg Logistic Regression is used to extract Assertion Status from extracted entities and text. Chunk2Token A feature transformer that converts the input array of strings (annotatorType CHUNK) into an array of chunk-based tokens (annotatorType TOKEN). ChunkEntityResolver Returns a normalized entity for a particular trained ontology / curated dataset (e.g. clinical ICD-10, RxNorm, SNOMED; financial SEC’s EDGAR database, etc). ChunkFilterer Filters entities coming from CHUNK annotations. ChunkKeyPhraseExtraction Uses Bert Sentence Embeddings to determine the most relevant key phrases describing a text. ChunkMerge Merges entities coming from different CHUNK annotations. ContextualParser Extracts entity from a document based on user defined rules. DeIdentification Deidentifies Input Annotations of types DOCUMENT, TOKEN and CHUNK, by either masking or obfuscating the given CHUNKS. DocumentLogRegClassifier Classifies documents with a Logarithmic Regression algorithm. DrugNormalizer Annotator which normalizes raw text from documents, e.g. scraped web pages or xml documents FeaturesAssembler Collects features from different columns. GenericClassifier Creates a generic single-label classifier which uses pre-generated Tensorflow graphs. IOBTagger Merges token tags and NER labels from chunks in the specified format. NerChunker Extracts phrases that fits into a known pattern using the NER tags. NerConverterInternal Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. NerDisambiguator Links words of interest, such as names of persons, locations and companies, from an input text document to a corresponding unique entity in a target Knowledge Base (KB). MedicalNer This Named Entity recognition annotator is a generic NER model based on Neural Networks.. RENerChunksFilter Filters and outputs combinations of relations between extracted entities, for further processing. ReIdentification Reidentifies obfuscated entities by DeIdentification. RelationExtraction Extracts and classifies instances of relations between named entities. RelationExtractionDL Extracts and classifies instances of relations between named entities. SentenceEntityResolver Returns the normalized entity for a particular trained ontology / curated dataset (e.g. clinical ICD-10, RxNorm, SNOMED; financial SEC’s EDGAR database, etc) based on sentence embeddings. AnnotationMerger Merge annotations from different pipeline steps that have the same annotation type into a unified annotation. Possible annotations that can be merged include: document (e.g., output of DocumentAssembler annotator) token (e.g., output of Tokenizer annotator) word_embeddings (e.g., output of WordEmbeddingsModel annotator) sentence_embeddings (e.g., output of BertSentenceEmbeddings annotator) category (e.g., output of RelationExtractionModel annotator) date (e.g., output of DateMatcher annotator) sentiment (e.g., output of SentimentDLModel annotator) pos (e.g., output of PerceptronModel annotator) chunk (e.g., output of NerConverter annotator) named_entity (e.g., output of NerDLModel annotator) regex (e.g., output of RegexTokenizer annotator) dependency (e.g., output of DependencyParserModel annotator) language (e.g., output of LanguageDetectorDL annotator) keyword (e.g., output of YakeModel annotator) Input Annotator Types: ANY Output Annotator Type: ANY Python API: AnnotationMerger Scala API: AnnotationMerger Show Example PythonScala Medical # Create the pipeline with two RE models documenter = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencer = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;tokens&quot;) words_embedder = WordEmbeddingsModel() .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) pos_tagger = PerceptronModel() .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;pos_tags&quot;) pos_ner_tagger = MedicalNerModel() .pretrained(&quot;ner_posology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner_pos&quot;) pos_ner_chunker = NerConverterInternal() .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_pos&quot;]) .setOutputCol(&quot;pos_ner_chunks&quot;) dependency_parser = DependencyParserModel() .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) pos_reModel = RelationExtractionModel() .pretrained(&quot;posology_re&quot;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;pos_ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;pos_relations&quot;) .setMaxSyntacticDistance(4) ade_ner_tagger = MedicalNerModel.pretrained(&quot;ner_ade_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ade_ner_tags&quot;) ade_ner_chunker = NerConverterInternal() .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;ade_ner_tags&quot;]) .setOutputCol(&quot;ade_ner_chunks&quot;) ade_reModel = RelationExtractionModel() .pretrained(&quot;re_ade_clinical&quot;, &quot;en&quot;, &#39;clinical/models&#39;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ade_ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;ade_relations&quot;) .setMaxSyntacticDistance(10) .setRelationPairs([&quot;drug-ade, ade-drug&quot;]) annotation_merger = AnnotationMerger() .setInputCols(&quot;ade_relations&quot;, &quot;pos_relations&quot;) .setInputType(&quot;category&quot;) .setOutputCol(&quot;all_relations&quot;) merger_pipeline = Pipeline(stages=[ documenter, sentencer, tokenizer, words_embedder, pos_tagger, pos_ner_tagger, pos_ner_chunker, dependency_parser, pos_reModel, ade_ner_tagger, ade_ner_chunker, ade_reModel, annotation_merger ]) empty_df= spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) merger_model= merger_pipeline.fit(empty_df) # Show example result text = &quot;&quot;&quot; The patient was prescribed 1 unit of naproxen for 5 days after meals for chronic low back pain. The patient was also given 1 unit of oxaprozin daily for rheumatoid arthritis presented with tense bullae and cutaneous fragility on the face and the back of the hands.. &quot;&quot;&quot; data = spark.createDataFrame([[text]]).toDF(&quot;text&quot;) result = merger_model.transform(data) result.show() +--+--+--+--+--+--+--+--+--+--+--+--+--+--+ text| document| sentences| tokens| embeddings| pos_tags| ner_pos| pos_ner_chunks| dependencies| pos_relations| ade_ner_tags| ade_ner_chunks| ade_relations| all_relations| +--+--+--+--+--+--+--+--+--+--+--+--+--+--+ The patient was ...|[{document, 0, 26...|[{document, 1, 95...|[{token, 1, 3, Th...|[{word_embeddings...|[{pos, 1, 3, DD, ...|[{named_entity, 1...|[{chunk, 28, 33, ...|[{dependency, 1, ...|[{category, 28, 4...|[{named_entity, 1...|[{chunk, 38, 45, ...|[{category, 134, ...|[{category, 134, ...| +--+--+--+--+--+--+--+--+--+--+--+--+--+--+ AssertionChunkConverter This annotator creates a CHUNK column with metadata useful for training an Assertion Status Detection model (see AssertionDL). In some cases, there may be issues while creating the chunk column when using token indices that can lead to loss of data to train assertion status models. The AssertionChunkConverter annotator uses both begin and end indices of the tokens as input to add a more robust metadata to the chunk column in a way that improves the reliability of the indices and avoid loss of data. NOTE: Chunk begin and end indices in the assertion status model training dataframe can be populated using the new version of ALAB module. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: AssertionChunkConverter Scala API: AssertionChunkConverter Show Example PythonScala Medical data = spark.createDataFrame( [ [ &quot;An angiography showed bleeding in two vessels off of the Minnie supplying the sigmoid that were succesfully embolized.&quot;, &quot;Minnie&quot;, 57, 64, ], [ &quot;After discussing this with his PCP, Leon was clear that the patient had had recurrent DVTs and ultimately a PE and his PCP felt strongly that he required long-term anticoagulation &quot;, &quot;PCP&quot;, 31, 34, ], ] ).toDF(&quot;text&quot;, &quot;target&quot;, &quot;char_begin&quot;, &quot;char_end&quot;) document_assembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = ( SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) ) tokenizer = Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;tokens&quot;) converter = ( AssertionChunkConverter() .setInputCols(&quot;tokens&quot;) .setChunkTextCol(&quot;target&quot;) .setChunkBeginCol(&quot;char_begin&quot;) .setChunkEndCol(&quot;char_end&quot;) .setOutputTokenBeginCol(&quot;token_begin&quot;) .setOutputTokenEndCol(&quot;token_end&quot;) .setOutputCol(&quot;chunk&quot;) ) pipeline = Pipeline().setStages( [document_assembler, sentenceDetector, tokenizer, converter] ) results = pipeline.fit(data).transform(data) results.selectExpr( &quot;target&quot;, &quot;char_begin&quot;, &quot;char_end&quot;, &quot;token_begin&quot;, &quot;token_end&quot;, &quot;tokens[token_begin].result&quot;, &quot;tokens[token_end].result&quot;, &quot;target&quot;, &quot;chunk&quot;, ).show(truncate=False) ++-+--+--++--+++-+ |target|char_begin|char_end|token_begin|token_end|tokens[token_begin].result|tokens[token_end].result|target|chunk | ++-+--+--++--+++-+ |Minnie|57 |64 |10 |10 |Minnie |Minnie |Minnie|[{chunk, 57, 62, Minnie, {sentence -&gt; 0}, []}]| |PCP |31 |34 |5 |5 |PCP |PCP |PCP |[{chunk, 31, 33, PCP, {sentence -&gt; 0}, []}] | ++-+--+--++--+++-+ AssertionDL ApproachModel Trains AssertionDL, a deep Learning based approach used to extract Assertion Status from extracted entities and text. Contains all the methods for training an AssertionDLModel. For pretrained models please use AssertionDLModel and see the Models Hub for available models. Input Annotator Types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output Annotator Type: ASSERTION Python API: AssertionDLApproach Scala API: AssertionDLApproach Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # First, pipeline stages for pre-processing the dataset (containing columns for text and label) are defined. document = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) chunk = nlp.Doc2Chunk() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;chunk&quot;) token = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Define AssertionDLApproach with parameters and start training assertionStatus = medical.AssertionDLApproach() .setLabelCol(&quot;label&quot;) .setInputCols([&quot;document&quot;, &quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) .setBatchSize(128) .setDropout(0.012) .setLearningRate(0.015) .setEpochs(1) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) .setMaxSentLen(250) trainingPipeline = Pipeline().setStages([ document, chunk, token, embeddings, assertionStatus ]) assertionModel = trainingPipeline.fit(data) assertionResults = assertionModel.transform(data).cache() from johnsnowlabs import * # First, pipeline stages for pre-processing the dataset (containing columns for text and label) are defined. document = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) chunk = nlp.Doc2Chunk() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;chunk&quot;) token = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Define AssertionDLApproach with parameters and start training assertionStatus = finance.AssertionDLApproach() .setLabelCol(&quot;label&quot;) .setInputCols([&quot;document&quot;, &quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) .setBatchSize(128) .setDropout(0.012) .setLearningRate(0.015) .setEpochs(1) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) .setMaxSentLen(250) trainingPipeline = Pipeline().setStages([ document, chunk, token, embeddings, assertionStatus ]) assertionModel = trainingPipeline.fit(data) assertionResults = assertionModel.transform(data).cache() from johnsnowlabs import * # First, pipeline stages for pre-processing the dataset (containing columns for text and label) are defined. document = nlp.DocumentAssembler() .setInputCol(&quot;sentence&quot;) .setOutputCol(&quot;document&quot;) chunk = nlp.Doc2Chunk() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;doc_chunk&quot;) token = nlp.Tokenizer() .setInputCols([&#39;document&#39;]) .setOutputCol(&#39;token&#39;) roberta_embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setMaxSentenceLength(512) # Define AssertionDLApproach with parameters and start training assertionStatus = legal.AssertionDLApproach() .setLabelCol(&quot;assertion_label&quot;) .setInputCols(&quot;document&quot;, &quot;doc_chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setBatchSize(128) .setLearningRate(0.001) .setEpochs(2) .setStartCol(&quot;tkn_start&quot;) .setEndCol(&quot;tkn_end&quot;) .setMaxSentLen(1200) .setEnableOutputLogs(True) .setOutputLogsPath(&#39;training_logs/&#39;) .setGraphFolder(graph_folder) .setGraphFile(f&quot;{graph_folder}/assertion_graph.pb&quot;) .setTestDataset(path=&quot;test_data.parquet&quot;, read_as=&#39;SPARK&#39;, options={&#39;format&#39;: &#39;parquet&#39;}) .setScopeWindow(scope_window) #.setValidationSplit(0.2) #.setDropout(0.1) trainingPipeline = Pipeline().setStages([ document, chunk, token, roberta_embeddings, assertionStatus ]) assertionModel = trainingPipeline.fit(data) assertionResults = assertionModel.transform(data).cache() MedicalFinanceLegal from johnsnowlabs import * // First, pipeline stages for pre-processing the dataset (containing columns for text and label) are defined. val document = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val chunk = new nlp.Doc2Chunk() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;chunk&quot;) val token = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Define AssertionDLApproach with parameters and start training val assertionStatus = new medical.AssertionDLApproach() .setLabelCol(&quot;label&quot;) .setInputCols(&quot;document&quot;, &quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setBatchSize(128) .setDropout(0.012f) .setLearningRate(0.015f) .setEpochs(1) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) .setMaxSentLen(250) val trainingPipeline = new Pipeline().setStages(Array( document, chunk, token, embeddings, assertionStatus )) val assertionModel = trainingPipeline.fit(data) val assertionResults = assertionModel.transform(data).cache() from johnsnowlabs import * // First, pipeline stages for pre-processing the dataset (containing columns for text and label) are defined. val document = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val chunk = new nlp.Doc2Chunk() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;chunk&quot;) val token = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Define AssertionDLApproach with parameters and start training val assertionStatus = new finance.AssertionDLApproach() .setLabelCol(&quot;label&quot;) .setInputCols(&quot;document&quot;, &quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setBatchSize(128) .setDropout(0.012f) .setLearningRate(0.015f) .setEpochs(1) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) .setMaxSentLen(250) val trainingPipeline = new Pipeline().setStages(Array( document, chunk, token, embeddings, assertionStatus )) val assertionModel = trainingPipeline.fit(data) val assertionResults = assertionModel.transform(data).cache() from johnsnowlabs import * val document = new nlp.DocumentAssembler() .setInputCol(&quot;sentence&quot;) .setOutputCol(&quot;document&quot;) val chunk = new nlp.Doc2Chunk() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;doc_chunk&quot;) .setChunkCol(&quot;chunk&quot;) .setStartCol(&quot;tkn_start&quot;) .setStartColByTokenIndex(True) .setFailOnMissing(False) .setLowerCase(False) val token = new nlp.Tokenizer() .setInputCols([&#39;document&#39;]) .setOutputCol(&#39;token&#39;) val roberta_embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setMaxSentenceLength(512) # Define AssertionDLApproach with parameters and start training val assertionStatus = new legal.AssertionDLApproach() .setLabelCol(&quot;assertion_label&quot;) .setInputCols(&quot;document&quot;, &quot;doc_chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setBatchSize(128) .setLearningRate(0.001) .setEpochs(2) .setStartCol(&quot;tkn_start&quot;) .setEndCol(&quot;tkn_end&quot;) .setMaxSentLen(1200) .setEnableOutputLogs(True) .setOutputLogsPath(&#39;training_logs/&#39;) .setGraphFolder(graph_folder) .setGraphFile(f&quot;{graph_folder}/assertion_graph.pb&quot;) .setTestDataset(path=&quot;test_data.parquet&quot;, read_as=&#39;SPARK&#39;, options={&#39;format&#39;: &#39;parquet&#39;}) .setScopeWindow(scope_window) #.setValidationSplit(0.2) #.setDropout(0.1) val trainingPipeline = new Pipeline().setStages(Array( document, chunk, token, roberta_embeddings, assertionStatus )) val assertionModel = trainingPipeline.fit(data) val assertionResults = assertionModel.transform(data).cache() AssertionDL is a deep Learning based approach used to extract Assertion Status from extracted entities and text. AssertionDLModel requires DOCUMENT, CHUNK and WORD_EMBEDDINGS type annotator inputs, which can be obtained by e.g a DocumentAssembler, NerConverter and WordEmbeddingsModel. The result is an assertion status annotation for each recognized entity. Possible values include “present”, “absent”, “hypothetical”, “conditional”, “associated_with_other_person” etc. For pretrained models please see the Models Hub for available models. Input Annotator Types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output Annotator Type: ASSERTION Python API: AssertionDLModel Scala API: AssertionDLModel Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Define pipeline stages to extract NER chunks first data = spark.createDataFrame([ [&quot;Patient with severe fever and sore throat&quot;], [&quot;Patient shows no stomach pain&quot;], [&quot;She was maintained on an epidural and PCA for pain control.&quot;]]).toDF(&quot;text&quot;) documentAssembler = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setOutputCol(&quot;embeddings&quot;) nerModel = medical.NerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]).setOutputCol(&quot;ner&quot;) nerConverter = nlp.NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]).setOutputCol(&quot;ner_chunk&quot;) # Then a pretrained AssertionDLModel is used to extract the assertion status clinicalAssertion = medical.AssertionDLModel.pretrained(&quot;assertion_dl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion ]) assertionModel = assertionPipeline.fit(data) # Show results result = assertionModel.transform(data) result.selectExpr(&quot;ner_chunk.result&quot;, &quot;assertion.result&quot;).show(3, truncate=False) +--+--+ |result |result | +--+--+ |[severe fever, sore throat] |[present, present] | |[stomach pain] |[absent] | |[an epidural, PCA, pain control]|[present, present, hypothetical]| +--+--+ from johnsnowlabs import * data = spark.createDataFrame([[&quot;Our competitors include the following by general category: legacy antivirus product providers, such as McAfee LLC and Broadcom Inc.&quot;]]).toDF(&quot;text&quot;) document_assembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = finance.NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) assertion = finance.AssertionDLModel.pretrained(&quot;finassertion_competitors&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) pipeline = Pipeline(stages=[ document_assembler, sentence_detector, tokenizer, embeddings, ner_model, ner_converter, assertion ]) assertionModel = pipeline.fit(data) # Show results result = assertionModel.transform(data) result.select(F.explode(F.arrays_zip(result.ner_chunk.result, result.ner_chunk.metadata, result.assertion.result)).alias(&quot;cols&quot;)) .select(F.expr(&quot;cols[&#39;1&#39;][&#39;sentence&#39;]&quot;).alias(&quot;sent_id&quot;), F.expr(&quot;cols[&#39;0&#39;]&quot;).alias(&quot;chunk&quot;), F.expr(&quot;cols[&#39;1&#39;][&#39;entity&#39;]&quot;).alias(&quot;ner_label&quot;), F.expr(&quot;cols[&#39;2&#39;]&quot;).alias(&quot;assertion&quot;)).show(truncate=False) +-+++-+ |sent_id|chunk |ner_label|assertion | +-+++-+ |0 |McAfee LLC |ORG |COMPETITOR| |0 |Broadcom Inc|ORG |COMPETITOR| +-+++-+ from johnsnowlabs import * data = spark.createDataFrame([[&quot;This is an Intellectual Property Agreement between Amazon Inc. and Atlantic Inc.&quot;]]).toDF(&quot;text&quot;) document_assembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings_ner = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings_ner&quot;) ner_model = legal.NerModel.pretrained(&#39;legner_contract_doc_parties&#39;, &#39;en&#39;, &#39;legal/models&#39;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings_ner&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&quot;DOC&quot;, &quot;EFFDATE&quot;, &quot;PARTY&quot;]) embeddings_ass = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings_ass&quot;) assertion = legal.AssertionDLModel.pretrained(&quot;legassertion_time&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings_ass&quot;]) .setOutputCol(&quot;assertion&quot;) nlpPipeline = Pipeline(stages=[ document_assembler, sentence_detector, tokenizer, embeddings_ner, ner_model, ner_converter, embeddings_ass, assertion ]) assertionModel = nlpPipeline.fit(data) # Show results result = assertionModel.transform(data) result.select(F.explode(F.arrays_zip(result.ner_chunk.result, result.ner_chunk.begin, result.ner_chunk.end, result.ner_chunk.metadata, result.assertion.result)).alias(&quot;cols&quot;)) .select(F.expr(&quot;cols[&#39;0&#39;]&quot;).alias(&quot;chunk&quot;), F.expr(&quot;cols[&#39;1&#39;]&quot;).alias(&quot;begin&quot;), F.expr(&quot;cols[&#39;2&#39;]&quot;).alias(&quot;end&quot;), F.expr(&quot;cols[&#39;3&#39;][&#39;entity&#39;]&quot;).alias(&quot;ner_label&quot;), F.expr(&quot;cols[&#39;4&#39;]&quot;).alias(&quot;assertion&quot;)).show(truncate=False) +-+--++++ |chunk |begin|end|ner_label|assertion| +-+--++++ |Intellectual Property Agreement|11 |41 |DOC |PRESENT | |Amazon Inc |51 |60 |PARTY |PRESENT | |Atlantic Inc |67 |78 |PARTY |PRESENT | +-+--++++ MedicalFinanceLegal from johnsnowlabs import * // Define pipeline stages to extract NER chunks first val data = Seq( &quot;Patient with severe fever and sore throat&quot;, &quot;Patient shows no stomach pain&quot;, &quot;She was maintained on an epidural and PCA for pain control.&quot;).toDF(&quot;text&quot;) val documentAssembler = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)).setOutputCol(&quot;embeddings&quot;) val nerModel = medical.NerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)).setOutputCol(&quot;ner&quot;) val nerConverter = new nlp.NerConverter().setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)).setOutputCol(&quot;ner_chunk&quot;) // Then a pretrained AssertionDLModel is used to extract the assertion status val clinicalAssertion = medical.AssertionDLModel.pretrained(&quot;assertion_dl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;assertion&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion )) val assertionModel = assertionPipeline.fit(data) // Show results val result = assertionModel.transform(data) result.selectExpr(&quot;ner_chunk.result&quot;, &quot;assertion.result&quot;).show(3, truncate=false) +--+--+ |result |result | +--+--+ |[severe fever, sore throat] |[present, present] | |[stomach pain] |[absent] | |[an epidural, PCA, pain control]|[present, present, hypothetical]| +--+--+ from johnsnowlabs import * val data = Seq(&quot;Our competitors include the following by general category: legacy antivirus product providers, such as McAfee LLC and Broadcom Inc.&quot;).toDF(&quot;text&quot;) val document_assembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence_detector = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new finance.NerConverterInternal() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) val assertion = finance.AssertionDLModel.pretrained(&quot;finassertion_competitors&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;assertion&quot;) val pipeline = new Pipeline().setStages(Array( document_assembler, sentence_detector, tokenizer, embeddings, ner_model, ner_converter, assertion ) val assertionModel = pipeline.fit(data) from johnsnowlabs import * val data = Seq(&quot;This is an Intellectual Property Agreement between Amazon Inc. and Atlantic Inc.&quot;).toDF(&quot;text&quot;) val document_assembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence_detector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings_ner = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings_ner&quot;) val ner_model = legal.NerModel.pretrained(&#39;legner_contract_doc_parties&#39;, &#39;en&#39;, &#39;legal/models&#39;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings_ner&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList(Array(&quot;DOC&quot;, &quot;EFFDATE&quot;, &quot;PARTY&quot;)) val embeddings_ass = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings_ass&quot;) val assertion = legal.AssertionDLModel.pretrained(&quot;legassertion_time&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings_ass&quot;)) .setOutputCol(&quot;assertion&quot;) val pipeline = new Pipeline().setStages(Array( document_assembler, sentence_detector, tokenizer, embeddings_ner, ner_model, ner_converter, embeddings_ass, assertion ) val assertionModel = pipeline.fit(data) AssertionFilterer Filters entities coming from ASSERTION type annotations and returns the CHUNKS. Filters can be set via a white list on the extracted chunk, the assertion or a regular expression. White list for assertion is enabled by default. To use chunk white list, criteria has to be set to &quot;isin&quot;. For regex, criteria has to be set to &quot;regex&quot;. Input Annotator Types: DOCUMENT, CHUNK, ASSERTION Output Annotator Type: CHUNK Python API: AssertionFilterer Scala API: AssertionFilterer Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # To see how the assertions are extracted, see the example for AssertionDLModel. # Define an extra step where the assertions are filtered assertionFilterer = medical.AssertionFilterer() .setInputCols([&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;]) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;assertion&quot;) .setWhiteList([&quot;present&quot;]) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion, assertionFilterer ]) assertionModel = assertionPipeline.fit(data) result = assertionModel.transform(data) # Show results: result.selectExpr(&quot;ner_chunk.result&quot;, &quot;assertion.result&quot;).show(3, truncate=False) +--+--+ |result |result | +--+--+ |[severe fever, sore throat] |[present, present] | |[stomach pain] |[absent] | |[an epidural, PCA, pain control]|[present, present, hypothetical]| +--+--+ result.select(&quot;filtered.result&quot;).show(3, truncate=False) ++ |result | ++ |[severe fever, sore throat]| |[] | |[an epidural, PCA] | ++ from johnsnowlabs import * # To see how the assertions are extracted, see the example for AssertionDLModel. # Define an extra step where the assertions are filtered assertionFilterer = finance.AssertionFilterer() .setInputCols([&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;]) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;assertion&quot;) .setWhiteList([&quot;present&quot;]) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion, assertionFilterer ]) assertionModel = assertionPipeline.fit(data) result = assertionModel.transform(data) from johnsnowlabs import * # To see how the assertions are extracted, see the example for AssertionDLModel. # Define an extra step where the assertions are filtered assertionFilterer = legal.AssertionFilterer() .setInputCols([&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;]) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;assertion&quot;) .setWhiteList([&quot;present&quot;]) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion, assertionFilterer ]) assertionModel = assertionPipeline.fit(data) result = assertionModel.transform(data) MedicalFinanceLegal from johnsnowlabs import * // To see how the assertions are extracted, see the example for // [[com.johnsnowlabs.nlp.annotators.assertion.dl.AssertionDLModel AssertionDLModel]]. // Define an extra step where the assertions are filtered val assertionFilterer = new medical.AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;assertion&quot;) .setWhiteList(&quot;present&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion, assertionFilterer )) val assertionModel = assertionPipeline.fit(data) val result = assertionModel.transform(data) // Show results: // // result.selectExpr(&quot;ner_chunk.result&quot;, &quot;assertion.result&quot;).show(3, truncate=false) // +--+--+ // |result |result | // +--+--+ // |[severe fever, sore throat] |[present, present] | // |[stomach pain] |[absent] | // |[an epidural, PCA, pain control]|[present, present, hypothetical]| // +--+--+ // result.select(&quot;filtered.result&quot;).show(3, truncate=false) // ++ // |result | // ++ // |[severe fever, sore throat]| // |[] | // |[an epidural, PCA] | // ++ // from johnsnowlabs import * // To see how the assertions are extracted, see the example for // [[com.johnsnowlabs.nlp.annotators.assertion.dl.AssertionDLModel AssertionDLModel]]. // Define an extra step where the assertions are filtered val assertionFilterer = new legal.AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;assertion&quot;) .setWhiteList(&quot;present&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion, assertionFilterer )) val assertionModel = assertionPipeline.fit(data) val result = assertionModel.transform(data) from johnsnowlabs import * // To see how the assertions are extracted, see the example for // [[com.johnsnowlabs.nlp.annotators.assertion.dl.AssertionDLModel AssertionDLModel]]. // Define an extra step where the assertions are filtered val assertionFilterer = new legal.AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;assertion&quot;) .setWhiteList(&quot;present&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, clinicalAssertion, assertionFilterer )) val assertionModel = assertionPipeline.fit(data) val result = assertionModel.transform(data) AssertionLogReg ApproachModel Trains a classification method, which uses the Logarithmic Regression Algorithm. It is used to extract Assertion Status from extracted entities and text. Contains all the methods for training a AssertionLogRegModel, together with trainWithChunk, trainWithStartEnd. Input Annotator Types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output Annotator Type: ASSERTION Python API: AssertionLogRegApproach Scala API: AssertionLogRegApproach Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Training with Glove Embeddings # First define pipeline stages to extract embeddings and text chunks documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) glove = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) .setCaseSensitive(False) chunkAssembler = nlp.Doc2Chunk() .setInputCols([&quot;document&quot;]) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) # Then the AssertionLogRegApproach model is defined. Label column is needed in the dataset for training. assertion = medical.AssertionLogRegApproach() .setLabelCol(&quot;label&quot;) .setInputCols([&quot;document&quot;, &quot;chunk&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, assertion ]) assertionModel = assertionPipeline.fit(dataset) from johnsnowlabs import * # Training with Glove Embeddings # First define pipeline stages to extract embeddings and text chunks documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) glove = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) .setCaseSensitive(False) chunkAssembler = nlp.Doc2Chunk() .setInputCols([&quot;document&quot;]) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) # Then the AssertionLogRegApproach model is defined. Label column is needed in the dataset for training. assertion = finance.AssertionLogRegApproach() .setLabelCol(&quot;label&quot;) .setInputCols([&quot;document&quot;, &quot;chunk&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, assertion ]) assertionModel = assertionPipeline.fit(dataset) from johnsnowlabs import * # Training with Glove Embeddings # First define pipeline stages to extract embeddings and text chunks documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) glove = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) .setCaseSensitive(False) chunkAssembler = nlp.Doc2Chunk() .setInputCols([&quot;document&quot;]) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) # Then the AssertionLogRegApproach model is defined. Label column is needed in the dataset for training. assertion = legal.AssertionLogRegApproach() .setLabelCol(&quot;label&quot;) .setInputCols([&quot;document&quot;, &quot;chunk&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) assertionPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, assertion ]) assertionModel = assertionPipeline.fit(dataset) MedicalFinanceLegal from johnsnowlabs import * // Training with Glove Embeddings // First define pipeline stages to extract embeddings and text chunks val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val glove = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;token&quot;)) .setOutputCol(&quot;word_embeddings&quot;) .setCaseSensitive(false) val chunkAssembler = new nlp.Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) // Then the AssertionLogRegApproach model is defined. Label column is needed in the dataset for training. val assertion = new medical.AssertionLogRegApproach() .setLabelCol(&quot;label&quot;) .setInputCols(Array(&quot;document&quot;, &quot;chunk&quot;, &quot;word_embeddings&quot;)) .setOutputCol(&quot;assertion&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, assertion )) val assertionModel = assertionPipeline.fit(dataset) from johnsnowlabs import * // Training with Glove Embeddings // First define pipeline stages to extract embeddings and text chunks val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val glove = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;token&quot;)) .setOutputCol(&quot;word_embeddings&quot;) .setCaseSensitive(false) val chunkAssembler = new nlp.Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) // Then the AssertionLogRegApproach model is defined. Label column is needed in the dataset for training. val assertion = new finance.AssertionLogRegApproach() .setLabelCol(&quot;label&quot;) .setInputCols(Array(&quot;document&quot;, &quot;chunk&quot;, &quot;word_embeddings&quot;)) .setOutputCol(&quot;assertion&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, assertion )) val assertionModel = assertionPipeline.fit(dataset) from johnsnowlabs import * // Training with Glove Embeddings // First define pipeline stages to extract embeddings and text chunks val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val glove = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;token&quot;)) .setOutputCol(&quot;word_embeddings&quot;) .setCaseSensitive(false) val chunkAssembler = new nlp.Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) // Then the AssertionLogRegApproach model is defined. Label column is needed in the dataset for training. val assertion = new legal.AssertionLogRegApproach() .setLabelCol(&quot;label&quot;) .setInputCols(Array(&quot;document&quot;, &quot;chunk&quot;, &quot;word_embeddings&quot;)) .setOutputCol(&quot;assertion&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) val assertionPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, assertion )) val assertionModel = assertionPipeline.fit(dataset) This is a main class in AssertionLogReg family. Logarithmic Regression is used to extract Assertion Status from extracted entities and text. AssertionLogRegModel requires DOCUMENT, CHUNK and WORD_EMBEDDINGS type annotator inputs, which can be obtained by e.g a DocumentAssembler, NerConverter and WordEmbeddingsModel. The result is an assertion status annotation for each recognized entity. Possible values are &quot;Negated&quot;, &quot;Affirmed&quot; and &quot;Historical&quot;. Unlike the DL Model, this class does not extend AnnotatorModel. Instead it extends the RawAnnotator, that’s why the main point of interest is method transform(). At the moment there are no pretrained models available for this class. Please refer to AssertionLogRegApproach to train your own model. Input Annotator Types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output Annotator Type: ASSERTION Python API: AssertionLogRegModel Scala API: AssertionLogRegModel BertSentenceChunkEmbeddings This annotator allows aggregating sentence embeddings with ner chunk embeddings to get specific and more accurate resolution codes. It works by averaging sentence and chunk embeddings add contextual information in the embedding value. Input to this annotator is the context (sentence) and ner chunks, while the output is embedding for each chunk that can be fed to the resolver model. The setChunkWeight parameter can be used to control the influence of surrounding context. For more information and examples of BertSentenceChunkEmbeddings annotator, you can check the Spark NLP Workshop, and in special, the notebook 24.1.Improved_Entity_Resolution_with_SentenceChunkEmbeddings.ipynb. Input Annotator Types: DOCUMENT, CHUNK Output Annotator Type: SENTENCE_EMBEDDINGS Python API: BertSentenceChunkEmbeddings Scala API: BertSentenceChunkEmbeddings Show Example PythonScala Medical # Define the pipeline document_assembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) clinical_ner = medical.NerModel.pretrained(&quot;ner_abbreviation_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = medical.NerConverterInternal() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&#39;ABBR&#39;]) sentence_chunk_embeddings = medical.BertSentenceChunkEmbeddings.pretrained(&quot;sbiobert_base_cased_mli&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;ner_chunk&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) .setChunkWeight(0.5) .setCaseSensitive(True) abbr_resolver = medical.SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_clinical_abbreviation_acronym&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;abbr_meaning&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) resolver_pipeline = Pipeline( stages = [ document_assembler, tokenizer, word_embeddings, clinical_ner, ner_converter, sentence_chunk_embeddings, abbr_resolver ]) # Example results sample_text = [ &quot;&quot;&quot;The patient admitted from the IR for aggressive irrigation of the Miami pouch. DISCHARGE DIAGNOSES: 1. A 58-year-old female with a history of stage 2 squamous cell carcinoma of the cervix status post total pelvic exenteration in 1991.&quot;&quot;&quot;, &quot;&quot;&quot;Gravid with estimated fetal weight of 6-6/12 pounds. LOWER EXTREMITIES: No edema. LABORATORY DATA: Laboratory tests include a CBC which is normal. Blood Type: AB positive. Rubella: Immune. VDRL: Nonreactive. Hepatitis C surface antigen: Negative. HIV: Negative. One-Hour Glucose: 117. Group B strep has not been done as yet.&quot;&quot;&quot;] from pyspark.sql.types import StringType, IntegerType df = spark.createDataFrame(sample_text, StringType()).toDF(&#39;text&#39;) df.show(truncate = 100) +-+ | text| +-+ |The patient admitted from the IR for aggressive irrigation of the Miami pouch. DISCHARGE DIAGNOSE...| |Gravid with estimated fetal weight of 6-6/12 pounds. LOWER EXTREMITIES: No edema. LABORATORY DATA...| +-+ Medical val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;tokens&quot;) val wordEmbeddings = BertEmbeddings .pretrained(&quot;biobert_pubmed_base_cased&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;word_embeddings&quot;) val nerModel = MedicalNerModel .pretrained(&quot;ner_clinical_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;tokens&quot;, &quot;word_embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val nerConverter = new NerConverter() .setInputCols(&quot;sentence&quot;, &quot;tokens&quot;, &quot;ner&quot;) .setOutputCol(&quot;ner_chunk&quot;) val sentenceChunkEmbeddings = BertSentenceChunkEmbeddings .pretrained(&quot;sbluebert_base_uncased_mli&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;ner_chunk&quot;)) .setOutputCol(&quot;sentence_chunk_embeddings&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentenceDetector, tokenizer, wordEmbeddings, nerModel, nerConverter, sentenceChunkEmbeddings)) val sampleText = &quot;Her Diabetes has become type 2 in the last year with her Diabetes.&quot; + &quot; He complains of swelling in his right forearm.&quot; val testDataset = Seq(&quot;&quot;).toDS.toDF(&quot;text&quot;) val result = pipeline.fit(emptyDataset).transform(testDataset) result .selectExpr(&quot;explode(sentence_chunk_embeddings) AS s&quot;) .selectExpr(&quot;s.result&quot;, &quot;slice(s.embeddings, 1, 5) AS averageEmbedding&quot;) .show(truncate=false) +--+--+ | result| averageEmbedding| +--+--+ |Her Diabetes |[-0.31995273, -0.04710883, -0.28973156, -0.1294758, 0.12481072] | |type 2 |[-0.027161136, -0.24613449, -0.0949309, 0.1825444, -0.2252143] | |her Diabetes |[-0.31995273, -0.04710883, -0.28973156, -0.1294758, 0.12481072] | |swelling in his right forearm|[-0.45139068, 0.12400375, -0.0075617577, -0.90806055, 0.12871636]| +--+--+ Chunk2Token A feature transformer that converts the input array of strings (annotatorType CHUNK) into an array of chunk-based tokens (annotatorType TOKEN). When the input is empty, an empty array is returned. This Annotator is specially convenient when using NGramGenerator annotations as inputs to WordEmbeddingsModels Input Annotator Types: CHUNK Output Annotator Type: TOKEN Scala API: Chunk2Token Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Define a pipeline for generating n-grams data = spark.createDataFrame([[&quot;A 63-year-old man presents to the hospital ...&quot;]]).toDF(&quot;text&quot;) document = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) token = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) ngrammer = nlp.NGramGenerator() .setN(2) .setEnableCumulative(False) .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;ngrams&quot;) .setDelimiter(&quot;_&quot;) # Stage to convert n-gram CHUNKS to TOKEN type chunk2Token = medical.Chunk2Token().setInputCols([&quot;ngrams&quot;]).setOutputCol(&quot;ngram_tokens&quot;) trainingPipeline = Pipeline(stages=[document, sentenceDetector, token, ngrammer, chunk2Token]).fit(data) result = trainingPipeline.transform(data).cache() result.selectExpr(&quot;explode(ngram_tokens)&quot;).show(5, False) +-+ |col | +-+ |{token, 3, 15, A_63-year-old, {sentence -&gt; 0, chunk -&gt; 0}, []} | |{token, 5, 19, 63-year-old_man, {sentence -&gt; 0, chunk -&gt; 1}, []}| |{token, 17, 28, man_presents, {sentence -&gt; 0, chunk -&gt; 2}, []} | |{token, 21, 31, presents_to, {sentence -&gt; 0, chunk -&gt; 3}, []} | |{token, 30, 35, to_the, {sentence -&gt; 0, chunk -&gt; 4}, []} | +-+ from johnsnowlabs import * # Define a pipeline for generating n-grams document = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) token = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) ngrammer = nlp.NGramGenerator() .setN(2) .setEnableCumulative(False) .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;ngrams&quot;) .setDelimiter(&quot;_&quot;) # Stage to convert n-gram CHUNKS to TOKEN type chunk2Token = finance.Chunk2Token().setInputCols([&quot;ngrams&quot;]).setOutputCol(&quot;ngram_tokens&quot;) trainingPipeline = Pipeline(stages=[document, sentenceDetector, token, ngrammer, chunk2Token]) from johnsnowlabs import * # Define a pipeline for generating n-grams document = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) token = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) ngrammer = nlp.NGramGenerator() .setN(2) .setEnableCumulative(False) .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;ngrams&quot;) .setDelimiter(&quot;_&quot;) # Stage to convert n-gram CHUNKS to TOKEN type chunk2Token = legal.Chunk2Token().setInputCols([&quot;ngrams&quot;]).setOutputCol(&quot;ngram_tokens&quot;) trainingPipeline = Pipeline(stages=[document, sentenceDetector, token, ngrammer, chunk2Token]) MedicalFinanceLegal from johnsnowlabs import * // Define a pipeline for generating n-grams val data = Seq((&quot;A 63-year-old man presents to the hospital ...&quot;)).toDF(&quot;text&quot;) val document = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val token = new nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val ngrammer = new nlp.NGramGenerator() .setN(2) .setEnableCumulative(false) .setInputCols(&quot;token&quot;) .setOutputCol(&quot;ngrams&quot;) .setDelimiter(&quot;_&quot;) // Stage to convert n-gram CHUNKS to TOKEN type val chunk2Token = new medical.Chunk2Token().setInputCols(&quot;ngrams&quot;).setOutputCol(&quot;ngram_tokens&quot;) val trainingPipeline = new Pipeline().setStages(Array(document, sentenceDetector, token, ngrammer, chunk2Token)).fit(data) val result = trainingPipeline.transform(data).cache() result.selectExpr(&quot;explode(ngram_tokens)&quot;).show(5, false) +-+ |col | +-+ |{token, 3, 15, A_63-year-old, {sentence -&gt; 0, chunk -&gt; 0}, []} | |{token, 5, 19, 63-year-old_man, {sentence -&gt; 0, chunk -&gt; 1}, []}| |{token, 17, 28, man_presents, {sentence -&gt; 0, chunk -&gt; 2}, []} | |{token, 21, 31, presents_to, {sentence -&gt; 0, chunk -&gt; 3}, []} | |{token, 30, 35, to_the, {sentence -&gt; 0, chunk -&gt; 4}, []} | +-+ from johnsnowlabs import * // Define a pipeline for generating n-grams val document = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val token = new nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val ngrammer = new nlp.NGramGenerator() .setN(2) .setEnableCumulative(false) .setInputCols(&quot;token&quot;) .setOutputCol(&quot;ngrams&quot;) .setDelimiter(&quot;_&quot;) // Stage to convert n-gram CHUNKS to TOKEN type val chunk2Token = new finance.Chunk2Token().setInputCols(&quot;ngrams&quot;).setOutputCol(&quot;ngram_tokens&quot;) val trainingPipeline = new Pipeline().setStages(Array(document, sentenceDetector, token, ngrammer, chunk2Token)) from johnsnowlabs import * // Define a pipeline for generating n-grams val document = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val token = new nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val ngrammer = new nlp.NGramGenerator() .setN(2) .setEnableCumulative(false) .setInputCols(&quot;token&quot;) .setOutputCol(&quot;ngrams&quot;) .setDelimiter(&quot;_&quot;) // Stage to convert n-gram CHUNKS to TOKEN type val chunk2Token = new legal.Chunk2Token().setInputCols(&quot;ngrams&quot;).setOutputCol(&quot;ngram_tokens&quot;) val trainingPipeline = new Pipeline().setStages(Array(document, sentenceDetector, token, ngrammer, chunk2Token)) ChunkConverter Convert chunks from RegexMatcher to chunks with a entity in the metadata. This annotator is important when the user wants to merge entities identified by NER models together with rules-based matching used by the RegexMathcer annotator. In the following steps of the pipeline, all the identified entities can be treated in a unified field. Input Annotator Types: DOCUMENT, CHUNK Output Annotator Type: CHUNK Python API: ChunkConverter Scala API: ChunkConverter Show Example PythonScala Medical # Creating the pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_model = MedicalNerModel.pretrained(&quot;ner_clinical_large&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) ner_converter= NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) regex_matcher = RegexMatcher() .setInputCols(&#39;document&#39;) .setStrategy(&quot;MATCH_ALL&quot;) .setOutputCol(&quot;regex_matches&quot;) .setExternalRules(path=&#39;file:/dbfs/regex_rules.txt&#39;, delimiter=&#39;,&#39;) chunkConverter = ChunkConverter() .setInputCols(&quot;regex_matches&quot;) .setOutputCol(&quot;regex_chunk&quot;) merger= ChunkMergeApproach() .setInputCols([&quot;regex_chunk&quot;, &quot;ner_chunk&quot;]) .setOutputCol(&quot;merged_chunks&quot;) .setMergeOverlapping(True) .setChunkPrecedence(&quot;field&quot;) pipeline= Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, word_embeddings, ner_model, ner_converter, regex_matcher, chunkConverter, merger ]) empty_df= spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model= pipeline.fit(empty_df) lp_model = LightPipeline(model) results = lp_model.fullAnnotate(sample_text)[0] # Displaying the results chunk= [] merge= [] for result in list(results[&quot;merged_chunks&quot;]): merge.append(result.metadata[&quot;entity&quot;]) chunk.append(result.result) df_merge = pd.DataFrame({&quot;chunk&quot;: chunk, &quot;merged_entity&quot;: merge}) df_merge | chunk | merged_entity | |--:|:| | POSTOPERATIVE DIAGNOSIS: | SECTION_HEADER | | Cervical lymphadenopathy | PROBLEM | | PROCEDURE: | SECTION_HEADER | | Excisional biopsy of right cervical lymph node | TEST | | ANESTHESIA: | SECTION_HEADER | | General endotracheal anesthesia | TREATMENT | | Right cervical lymph node | PROBLEM | | EBL: | SECTION_HEADER | | COMPLICATIONS: | SECTION_HEADER | | FINDINGS: | SECTION_HEADER | | Enlarged level 2 lymph node | PROBLEM | | ... | | Medical val sampleDataset = ResourceHelper.spark.createDataFrame(Seq( (1, &quot;My first sentence with the first rule. This is my second sentence with ceremonies rule.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val regexMatcher = new RegexMatcher() .setExternalRules(ExternalResource(&quot;src/test/resources/regex-matcher/rules.txt&quot;, ReadAs.TEXT, Map(&quot;delimiter&quot; -&gt; &quot;,&quot;))) .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;regex&quot;) .setStrategy(strategy) val chunkConverter = new ChunkConverter().setInputCols(&quot;regex&quot;).setOutputCol(&quot;chunk&quot;) val pipeline = new Pipeline().setStages(Array(documentAssembler, sentence, regexMatcher,chunkConverter)) val results = pipeline.fit(sampleDataset).transform(sampleDataset) results.select(&quot;chunk&quot;).show(truncate = false) ++ |col | ++ |[chunk, 23, 31, the first, [identifier -&gt; NAME, sentence -&gt; 0, chunk -&gt; 0, entity -&gt; NAME], []] | |[chunk, 71, 80, ceremonies, [identifier -&gt; NAME, sentence -&gt; 1, chunk -&gt; 0, entity -&gt; NAME], []]| ++ ChunkEntityResolver ApproachModel Contains all the parameters and methods to train a ChunkEntityResolverModel. It transform a dataset with two Input Annotations of types TOKEN and WORD_EMBEDDINGS, coming from e.g. ChunkTokenizer and ChunkEmbeddings Annotators and returns the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.) To use pretrained models please use ChunkEntityResolverModel and see the Models Hub for available models. Input Annotator Types: TOKEN, WORD_EMBEDDINGS Output Annotator Type: ENTITY Scala API: ChunkEntityResolverApproach Show Example PythonScala Medical from johnsnowlabs import * # Training a SNOMED model # Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data # and their labels. document = nlp.DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) chunk = nlp.Doc2Chunk() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;chunk&quot;) token = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_healthcare_100d&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) chunkEmb = nlp.ChunkEmbeddings() .setInputCols([&quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;chunk_embeddings&quot;) snomedTrainingPipeline = Pipeline().setStages([ document, chunk, token, embeddings, chunkEmb ]) snomedTrainingModel = snomedTrainingPipeline.fit(data) snomedData = snomedTrainingModel.transform(data).cache() # Then the Resolver can be trained with snomedExtractor = medical.ChunkEntityResolverApproach() .setInputCols([&quot;token&quot;, &quot;chunk_embeddings&quot;]) .setOutputCol(&quot;recognized&quot;) .setNeighbours(1000) .setAlternatives(25) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setEnableWmd(True).setEnableTfidf(True).setEnableJaccard(True) .setEnableSorensenDice(True).setEnableJaroWinkler(True).setEnableLevenshtein(True) .setDistanceWeights([1, 2, 2, 1, 1, 1]) .setAllDistancesMetadata(True) .setPoolingStrategy(&quot;MAX&quot;) .setThreshold(1e32) model = snomedExtractor.fit(snomedData) Medical from johnsnowlabs import * // Training a SNOMED model // Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data // and their labels. val document = new nlp.DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) val chunk = new nlp.Doc2Chunk() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;chunk&quot;) val token = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_healthcare_100d&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val chunkEmb = new nlp.ChunkEmbeddings() .setInputCols(Array(&quot;chunk&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;chunk_embeddings&quot;) val snomedTrainingPipeline = new Pipeline().setStages(Array( document, chunk, token, embeddings, chunkEmb )) val snomedTrainingModel = snomedTrainingPipeline.fit(data) val snomedData = snomedTrainingModel.transform(data).cache() // Then the Resolver can be trained with val snomedExtractor = new medical.ChunkEntityResolverApproach() .setInputCols(Array(&quot;token&quot;, &quot;chunk_embeddings&quot;)) .setOutputCol(&quot;recognized&quot;) .setNeighbours(1000) .setAlternatives(25) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setEnableWmd(true).setEnableTfidf(true).setEnableJaccard(true) .setEnableSorensenDice(true).setEnableJaroWinkler(true).setEnableLevenshtein(true) .setDistanceWeights(Array(1, 2, 2, 1, 1, 1)) .setAllDistancesMetadata(true) .setPoolingStrategy(&quot;MAX&quot;) .setThreshold(1e32) val model = snomedExtractor.fit(snomedData) Returns a normalized entity for a particular trained ontology / curated dataset (e.g. ICD-10, RxNorm, SNOMED etc). For available pretrained models please see the Models Hub. Input Annotator Types: TOKEN, WORD_EMBEDDINGS Output Annotator Type: ENTITY Scala API: ChunkEntityResolverModel Show Example PythonScala Medical from johnsnowlabs import * # Using pretrained models for SNOMED # First the prior steps of the pipeline are defined. # Output of types TOKEN and WORD_EMBEDDINGS are needed. data = spark.createDataFrame([[&quot;A 63-year-old man presents to the hospital ...&quot;]]).toDF(&quot;text&quot;) docAssembler = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) word_embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) icdo_ner = medical.NerModel.pretrained(&quot;ner_bionlp&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;icdo_ner&quot;) icdo_chunk = nlp.NerConverter().setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;icdo_ner&quot;]).setOutputCol(&quot;icdo_chunk&quot;).setWhiteList([&quot;Cancer&quot;]) icdo_chunk_embeddings = nlp.ChunkEmbeddings() .setInputCols([&quot;icdo_chunk&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;icdo_chunk_embeddings&quot;) icdo_chunk_resolver = medical.ChunkEntityResolverModel.pretrained(&quot;chunkresolve_icdo_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;token&quot;,&quot;icdo_chunk_embeddings&quot;]) .setOutputCol(&quot;tm_icdo_code&quot;) clinical_ner = medical.NerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) ner_chunk_tokenizer = nlp.ChunkTokenizer() .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;ner_token&quot;) ner_chunk_embeddings = nlp.ChunkEmbeddings() .setInputCols([&quot;ner_chunk&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;ner_chunk_embeddings&quot;) # Definition of the SNOMED Resolution ner_snomed_resolver = medical.ChunkEntityResolverModel.pretrained(&quot;chunkresolve_snomed_findings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;ner_token&quot;,&quot;ner_chunk_embeddings&quot;]).setOutputCol(&quot;snomed_result&quot;) pipelineFull = Pipeline().setStages([ docAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, ner_converter, ner_chunk_embeddings, ner_chunk_tokenizer, ner_snomed_resolver, icdo_ner, icdo_chunk, icdo_chunk_embeddings, icdo_chunk_resolver ]) pipelineModelFull = pipelineFull.fit(data) result = pipelineModelFull.transform(data).cache() # Show results result.selectExpr(&quot;explode(snomed_result)&quot;) .selectExpr( &quot;col.metadata.target_text&quot;, &quot;col.metadata.resolved_text&quot;, &quot;col.metadata.confidence&quot;, &quot;col.metadata.all_k_results&quot;, &quot;col.metadata.all_k_resolutions&quot;) .filter($&quot;confidence&quot; &gt; 0.2).show(5) +--+--+-+--+--+ | target_text| resolved_text|confidence| all_k_results| all_k_resolutions| +--+--+-+--+--+ |hypercholesterolemia|Hypercholesterolemia| 0.2524|13644009:::267432...|Hypercholesterole...| | CBC| Neocyte| 0.4980|259680000:::11573...|Neocyte:::Blood g...| | CD38| Hypoviscosity| 0.2560|47872005:::370970...|Hypoviscosity:::E...| | platelets| Increased platelets| 0.5267|6631009:::2596800...|Increased platele...| | CD38| Hypoviscosity| 0.2560|47872005:::370970...|Hypoviscosity:::E...| +--+--+-+--+--+ Medical from johnsnowlabs import * // Using pretrained models for SNOMED // First the prior steps of the pipeline are defined. // Output of types TOKEN and WORD_EMBEDDINGS are needed. val data = Seq((&quot;A 63-year-old man presents to the hospital ...&quot;)).toDF(&quot;text&quot;) val docAssembler = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val word_embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;word_embeddings&quot;) val icdo_ner = medical.NerModel.pretrained(&quot;ner_bionlp&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;)) .setOutputCol(&quot;icdo_ner&quot;) val icdo_chunk = new nlp.NerConverter().setInputCols(Array(&quot;sentence&quot;,&quot;token&quot;,&quot;icdo_ner&quot;)).setOutputCol(&quot;icdo_chunk&quot;).setWhiteList(&quot;Cancer&quot;) val icdo_chunk_embeddings = new nlp.ChunkEmbeddings() .setInputCols(Array(&quot;icdo_chunk&quot;, &quot;word_embeddings&quot;)) .setOutputCol(&quot;icdo_chunk_embeddings&quot;) val icdo_chunk_resolver = medical.ChunkEntityResolverModel.pretrained(&quot;chunkresolve_icdo_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;token&quot;,&quot;icdo_chunk_embeddings&quot;)) .setOutputCol(&quot;tm_icdo_code&quot;) val clinical_ner = medical.NerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) val ner_chunk_tokenizer = new nlp.ChunkTokenizer() .setInputCols(&quot;ner_chunk&quot;) .setOutputCol(&quot;ner_token&quot;) val ner_chunk_embeddings = new nlp.ChunkEmbeddings() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;word_embeddings&quot;)) .setOutputCol(&quot;ner_chunk_embeddings&quot;) // Definition of the SNOMED Resolution val ner_snomed_resolver = medical.ChunkEntityResolverModel.pretrained(&quot;chunkresolve_snomed_findings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(Array(&quot;ner_token&quot;,&quot;ner_chunk_embeddings&quot;)).setOutputCol(&quot;snomed_result&quot;) val pipelineFull = new Pipeline().setStages(Array( docAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, ner_converter, ner_chunk_embeddings, ner_chunk_tokenizer, ner_snomed_resolver, icdo_ner, icdo_chunk, icdo_chunk_embeddings, icdo_chunk_resolver )) val pipelineModelFull = pipelineFull.fit(data) val result = pipelineModelFull.transform(data).cache() // Show results // // result.selectExpr(&quot;explode(snomed_result)&quot;) // .selectExpr( // &quot;col.metadata.target_text&quot;, // &quot;col.metadata.resolved_text&quot;, // &quot;col.metadata.confidence&quot;, // &quot;col.metadata.all_k_results&quot;, // &quot;col.metadata.all_k_resolutions&quot;) // .filter($&quot;confidence&quot; &gt; 0.2).show(5) // +--+--+-+--+--+ // | target_text| resolved_text|confidence| all_k_results| all_k_resolutions| // +--+--+-+--+--+ // |hypercholesterolemia|Hypercholesterolemia| 0.2524|13644009:::267432...|Hypercholesterole...| // | CBC| Neocyte| 0.4980|259680000:::11573...|Neocyte:::Blood g...| // | CD38| Hypoviscosity| 0.2560|47872005:::370970...|Hypoviscosity:::E...| // | platelets| Increased platelets| 0.5267|6631009:::2596800...|Increased platele...| // | CD38| Hypoviscosity| 0.2560|47872005:::370970...|Hypoviscosity:::E...| // +--+--+-+--+--+ // ChunkFilterer Filters entities coming from CHUNK annotations. Filters can be set via a white list of terms or a regular expression. White list criteria is enabled by default. To use regex, criteria has to be set to regex. Input Annotator Types: DOCUMENT,CHUNK Output Annotator Type: CHUNK Python API: ChunkFilterer Scala API: ChunkFilterer Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Filtering POS tags # First pipeline stages to extract the POS tags are defined data = spark.createDataFrame([[&quot;Has a past history of gastroenteritis and stomach pain, however patient ...&quot;]]).toDF(&quot;text&quot;) docAssembler = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) posTagger = nlp.PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) chunker = nlp.Chunker() .setInputCols([&quot;pos&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;(&lt;NN&gt;)+&quot;]) # Then the chunks can be filtered via a white list. Here only terms with &quot;gastroenteritis&quot; remain. chunkerFilter = medical.ChunkFilterer() .setInputCols([&quot;sentence&quot;,&quot;chunk&quot;]) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList([&quot;gastroenteritis&quot;]) pipeline = Pipeline(stages=[ docAssembler, sentenceDetector, tokenizer, posTagger, chunker, chunkerFilter]) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk)&quot;).show(truncate=False) ++ |col | ++ |{chunk, 11, 17, history, {sentence -&gt; 0, chunk -&gt; 0}, []} | |{chunk, 22, 36, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 42, 53, stomach pain, {sentence -&gt; 0, chunk -&gt; 2}, []} | |{chunk, 64, 70, patient, {sentence -&gt; 0, chunk -&gt; 3}, []} | |{chunk, 81, 110, stomach pain now.We don&#39;t care, {sentence -&gt; 0, chunk -&gt; 4}, []}| |{chunk, 118, 132, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 5}, []} | ++ result.selectExpr(&quot;explode(filtered)&quot;).show(truncate=False) +-+ |col | +-+ |{chunk, 22, 36, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 118, 132, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 5}, []}| +-+ from johnsnowlabs import * # Filtering POS tags # First pipeline stages to extract the POS tags are defined docAssembler = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) posTagger = nlp.PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) chunker = nlp.Chunker() .setInputCols([&quot;pos&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;(&lt;NN&gt;)+&quot;]) # Then the chunks can be filtered via a white list. Here only terms with &quot;gastroenteritis&quot; remain. chunkerFilter = finance.ChunkFilterer() .setInputCols([&quot;sentence&quot;,&quot;chunk&quot;]) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList([&quot;gastroenteritis&quot;]) pipeline = Pipeline(stages=[ docAssembler, sentenceDetector, tokenizer, posTagger, chunker, chunkerFilter]) result = pipeline.fit(data).transform(data) from johnsnowlabs import * # Filtering POS tags # First pipeline stages to extract the POS tags are defined docAssembler = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) posTagger = nlp.PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) chunker = nlp.Chunker() .setInputCols([&quot;pos&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;(&lt;NN&gt;)+&quot;]) # Then the chunks can be filtered via a white list. Here only terms with &quot;gastroenteritis&quot; remain. chunkerFilter = legal.ChunkFilterer() .setInputCols([&quot;sentence&quot;,&quot;chunk&quot;]) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList([&quot;gastroenteritis&quot;]) pipeline = Pipeline(stages=[ docAssembler, sentenceDetector, tokenizer, posTagger, chunker, chunkerFilter]) result = pipeline.fit(data).transform(data) MedicalFinanceLegal from johnsnowlabs import * // Filtering POS tags // First pipeline stages to extract the POS tags are defined val data = Seq(&quot;Has a past history of gastroenteritis and stomach pain, however patient ...&quot;).toDF(&quot;text&quot;) val docAssembler = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val posTagger = nlp.PerceptronModel.pretrained() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;pos&quot;) val chunker = new nlp.Chunker() .setInputCols(Array(&quot;pos&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;(&lt;NN&gt;)+&quot;)) // Then the chunks can be filtered via a white list. Here only terms with &quot;gastroenteritis&quot; remain. val chunkerFilter = new medical.ChunkFilterer() .setInputCols(Array(&quot;sentence&quot;,&quot;chunk&quot;)) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList(&quot;gastroenteritis&quot;) val pipeline = new Pipeline().setStages(Array( docAssembler, sentenceDetector, tokenizer, posTagger, chunker, chunkerFilter)) result.selectExpr(&quot;explode(chunk)&quot;).show(truncate=false) ++ |col | ++ |{chunk, 11, 17, history, {sentence -&gt; 0, chunk -&gt; 0}, []} | |{chunk, 22, 36, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 42, 53, stomach pain, {sentence -&gt; 0, chunk -&gt; 2}, []} | |{chunk, 64, 70, patient, {sentence -&gt; 0, chunk -&gt; 3}, []} | |{chunk, 81, 110, stomach pain now.We don&#39;t care, {sentence -&gt; 0, chunk -&gt; 4}, []}| |{chunk, 118, 132, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 5}, []} | ++ result.selectExpr(&quot;explode(filtered)&quot;).show(truncate=false) +-+ |col | +-+ |{chunk, 22, 36, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 1}, []} | |{chunk, 118, 132, gastroenteritis, {sentence -&gt; 0, chunk -&gt; 5}, []}| +-+ from johnsnowlabs import * val docAssembler = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val posTagger = nlp.PerceptronModel.pretrained() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;pos&quot;) val chunker = new nlp.Chunker() .setInputCols(Array(&quot;pos&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;(&lt;NN&gt;)+&quot;)) // Then the chunks can be filtered via a white list. Here only terms with &quot;gastroenteritis&quot; remain. val chunkerFilter = new finance.ChunkFilterer() .setInputCols(Array(&quot;sentence&quot;,&quot;chunk&quot;)) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList(&quot;gastroenteritis&quot;) val pipeline = new Pipeline().setStages(Array( docAssembler, sentenceDetector, tokenizer, posTagger, chunker, chunkerFilter)) from johnsnowlabs import * val docAssembler = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val posTagger = nlp.PerceptronModel.pretrained() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;pos&quot;) val chunker = new nlp.Chunker() .setInputCols(Array(&quot;pos&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;(&lt;NN&gt;)+&quot;)) // Then the chunks can be filtered via a white list. Here only terms with &quot;gastroenteritis&quot; remain. val chunkerFilter = new legal.ChunkFilterer() .setInputCols(Array(&quot;sentence&quot;,&quot;chunk&quot;)) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList(&quot;gastroenteritis&quot;) val pipeline = new Pipeline().setStages(Array( docAssembler, sentenceDetector, tokenizer, posTagger, chunker, chunkerFilter)) ChunkKeyPhraseExtraction Chunk KeyPhrase Extraction uses Bert Sentence Embeddings to determine the most relevant key phrases describing a text. The input to the model consists of chunk annotations and sentence or document annotation. The model compares the chunks against the corresponding sentences/documents and selects the chunks which are most representative of the broader text context (i.e. the document or the sentence they belong to). The key phrases candidates (i.e. the input chunks) can be generated in various ways, e.g. by NGramGenerator, TextMatcher or NerConverter. The model operates either at sentence (selecting the most descriptive chunks from the sentence they belong to) or at document level. In the latter case, the key phrases are selected to represent all the input document annotations. This model is a subclass of [[BertSentenceEmbeddings]] and shares all parameters with it. It can load any pretrained BertSentenceEmbeddings model. Available models can be found at the Models Hub. Input Annotator Types: DOCUMENT, CHUNK Output Annotator Type: CHUNK Python API: ChunkKeyPhraseExtraction Scala API: ChunkKeyPhraseExtraction Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * documenter = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencer = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;tokens&quot;) embeddings = nlp.WordEmbeddingsModel() .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_tagger = medical.NerModel() .pretrained(&quot;ner_jsl_slim&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) ner_converter = nlp.NerConverter() .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;) .setOutputCol(&quot;ner_chunks&quot;) key_phrase_extractor = medical.ChunkKeyPhraseExtraction .pretrained() .setTopN(1) .setDocumentLevelProcessing(False) .setDivergence(0.4) .setInputCols([&quot;sentences&quot;, &quot;ner_chunks&quot;]) .setOutputCol(&quot;ner_chunk_key_phrases&quot;) pipeline = sparknlp.base.Pipeline() .setStages([documenter, sentencer, tokenizer, embeddings, ner_tagger, ner_converter, key_phrase_extractor]) data = spark.createDataFrame([[&quot;Her Diabetes has become type 2 in the last year with her Diabetes.He complains of swelling in his right forearm.&quot;]]).toDF(&quot;text&quot;) results = pipeline.fit(data).transform(data) results .selectExpr(&quot;explode(ner_chunk_key_phrases) AS key_phrase&quot;) .selectExpr( &quot;key_phrase.result&quot;, &quot;key_phrase.metadata.entity&quot;, &quot;key_phrase.metadata.DocumentSimilarity&quot;, &quot;key_phrase.metadata.MMRScore&quot;) .show(truncate=False) +--++-+ |result |DocumentSimilarity|MMRScore | +--++-+ |gestational diabetes mellitus|0.7391447825527298|0.44348688715422274| |28-year-old |0.4366776288430703|0.13577881610104517| |type two diabetes mellitus |0.7323921930094919|0.085800103824974 | +--++-+ from johnsnowlabs import * documenter = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencer = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;tokens&quot;) embeddings = nlp.WordEmbeddingsModel() .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner&quot;) .setOutputCol(&quot;ner_chunks&quot;) key_phrase_extractor = finance.ChunkKeyPhraseExtraction .pretrained() .setTopN(1) .setDocumentLevelProcessing(False) .setDivergence(0.4) .setInputCols([&quot;sentences&quot;, &quot;ner_chunks&quot;]) .setOutputCol(&quot;ner_chunk_key_phrases&quot;) pipeline = sparknlp.base.Pipeline() .setStages([documenter, sentencer, tokenizer, embeddings, ner_model, ner_converter, key_phrase_extractor]) from johnsnowlabs import * documenter = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencer = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;tokens&quot;) embeddings = nlp.WordEmbeddingsModel() .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_model = legal.NerModel.pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner&quot;) .setOutputCol(&quot;ner_chunks&quot;) key_phrase_extractor = legal.ChunkKeyPhraseExtraction .pretrained() .setTopN(1) .setDocumentLevelProcessing(False) .setDivergence(0.4) .setInputCols([&quot;sentences&quot;, &quot;ner_chunks&quot;]) .setOutputCol(&quot;ner_chunk_key_phrases&quot;) pipeline = sparknlp.base.Pipeline() .setStages([documenter, sentencer, tokenizer, embeddings, ner_model, ner_converter, key_phrase_extractor]) MedicalFinanceLegal from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;tokens&quot;) val stopWordsCleaner = nlp.StopWordsCleaner.pretrained() .setInputCols(&quot;tokens&quot;) .setOutputCol(&quot;clean_tokens&quot;) .setCaseSensitive(false) val nGrams = new nlp.NGramGenerator() .setInputCols(Array(&quot;clean_tokens&quot;)) .setOutputCol(&quot;ngrams&quot;) .setN(3) val chunkKeyPhraseExtractor = medical.ChunkKeyPhraseExtraction .pretrained() .setTopN(2) .setDivergence(0.7f) .setInputCols(Array(&quot;document&quot;, &quot;ngrams&quot;)) .setOutputCol(&quot;key_phrases&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, stopWordsCleaner, nGrams, chunkKeyPhraseExtractor)) val sampleText = &quot;Her Diabetes has become type 2 in the last year with her Diabetes.&quot; + &quot; He complains of swelling in his right forearm.&quot; val testDataset = Seq(&quot;&quot;).toDS.toDF(&quot;text&quot;) val result = pipeline.fit(emptyDataset).transform(testDataset) result .selectExpr(&quot;explode(key_phrases) AS key_phrase&quot;) .selectExpr( &quot;key_phrase.result&quot;, &quot;key_phrase.metadata.DocumentSimilarity&quot;, &quot;key_phrase.metadata.MMRScore&quot;) .show(truncate=false) +--+-++ |result |DocumentSimilarity |MMRScore | +--+-++ |complains swelling forearm|0.6325718954229369 |0.1897715761677257| |type 2 year |0.40181028931546364|-0.189501077108947| +--+-++ from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;tokens&quot;) val stopWordsCleaner = nlp.StopWordsCleaner.pretrained() .setInputCols(&quot;tokens&quot;) .setOutputCol(&quot;clean_tokens&quot;) .setCaseSensitive(false) val nGrams = new nlp.NGramGenerator() .setInputCols(Array(&quot;clean_tokens&quot;)) .setOutputCol(&quot;ngrams&quot;) .setN(3) val chunkKeyPhraseExtractor = finance.ChunkKeyPhraseExtraction .pretrained() .setTopN(2) .setDivergence(0.7f) .setInputCols(Array(&quot;document&quot;, &quot;ngrams&quot;)) .setOutputCol(&quot;key_phrases&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, stopWordsCleaner, nGrams, chunkKeyPhraseExtractor)) from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;tokens&quot;) val stopWordsCleaner = nlp.StopWordsCleaner.pretrained() .setInputCols(&quot;tokens&quot;) .setOutputCol(&quot;clean_tokens&quot;) .setCaseSensitive(false) val nGrams = new nlp.NGramGenerator() .setInputCols(Array(&quot;clean_tokens&quot;)) .setOutputCol(&quot;ngrams&quot;) .setN(3) val chunkKeyPhraseExtractor = legal.ChunkKeyPhraseExtraction .pretrained() .setTopN(2) .setDivergence(0.7f) .setInputCols(Array(&quot;document&quot;, &quot;ngrams&quot;)) .setOutputCol(&quot;key_phrases&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, stopWordsCleaner, nGrams, chunkKeyPhraseExtractor)) ChunkMapper ApproachModel We can use ChunkMapper to map entities with their associated code/reference based on pre-defined dictionaries. This is the AnnotatorApproach of the ChunkMapper, which can be used to train ChunkMapper models by giving a custom mapping dictionary. To use pretriained models, check the documentation of the ChunkMapperModel annotator. The annotator also allows using fuzzy matching, which can take into consideration parts of the tokens tha can map even when word order is different, char ngrams that can map even when thre are typos, and using fuzzy distance metric (Jaccard, Levenshtein, etc.). Example usage and more details can be found on Spark NLP Workshop repository accessible in GitHub, for example the notebook Healthcare Chunk Mapping. Input Annotator Types: CHUNK Output Annotator Type: LABEL_DEPENDENCY Python API: ChunkMapperApproach Scala API: ChunkMapperApproach Show Example PythonScala Medical # First, create a dictionay in JSON format following this schema: import json data_set= { &quot;mappings&quot;: [ { &quot;key&quot;: &quot;metformin&quot;, &quot;relations&quot;: [ { &quot;key&quot;: &quot;action&quot;, &quot;values&quot; : [&quot;hypoglycemic&quot;, &quot;Drugs Used In Diabetes&quot;] }, { &quot;key&quot;: &quot;treatment&quot;, &quot;values&quot; : [&quot;diabetes&quot;, &quot;t2dm&quot;] }] }] } with open(&#39;sample_drug.json&#39;, &#39;w&#39;, encoding=&#39;utf-8&#39;) as f: json.dump(data_set, f, ensure_ascii=False, indent=4) # Create a pipeline document_assembler = DocumentAssembler() .setInputCol(&#39;text&#39;) .setOutputCol(&#39;document&#39;) sentence_detector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) word_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) #NER model to detect drug in the text clinical_ner = MedicalNerModel.pretrained(&quot;ner_posology_small&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) .setLabelCasing(&quot;upper&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&quot;DRUG&quot;]) chunkerMapper = ChunkMapperApproach() .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setDictionary(&quot;sample_drug.json&quot;) .setRels([&quot;action&quot;]) #or treatment pipeline = Pipeline( stages=[ document_assembler, sentence_detector, tokenizer, word_embeddings, clinical_ner, ner_converter, chunkerMapper, ] ) # Train the model text = [&quot;The patient was given 1 unit of metformin daily.&quot;] test_data = spark.createDataFrame([text]).toDF(&quot;text&quot;) model = pipeline.fit(test_data) We can use ChunkMapper to map entities with their associated code/reference based on pre-defined dictionaries. This is the AnnotatorModel of the ChunkMapper, which can be used to access pretrained models with the .pretrained() or .load() methods. To train a new model, check the documentation of the ChunkMapperApproach annotator. The annotator also allows using fuzzy matching, which can take into consideration parts of the tokens tha can map even when word order is different, char ngrams that can map even when thre are typos, and using fuzzy distance metric (Jaccard, Levenshtein, etc.). Example usage and more details can be found on Spark NLP Workshop repository accessible in GitHub, for example the notebook Healthcare Chunk Mapping. Input Annotator Types: CHUNK Output Annotator Type: LABEL_DEPENDENCY Python API: ChunkMapperModel Scala API: ChunkMapperModel Show Example PythonScala Medical # Use `rxnorm_mapper` pretrained model to map entities with their corresponding RxNorm codes. document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) chunkerMapper = ChunkMapperModel.pretrained(&quot;rxnorm_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;rxnorm&quot;) .setRels([&quot;rxnorm_code&quot;]) mapper_pipeline = Pipeline().setStages([document_assembler, chunkerMapper]) empty_df = spark.createDataFrame([[&#39;&#39;]]).toDF(&#39;text&#39;) mapper_model = mapper_pipeline.fit(empty_df) mapper_lp = LightPipeline(mapper_model) mapper_lp.fullAnnotate(&quot;metformin&quot;) [{&#39;ner_chunk&#39;: [Annotation(document, 0, 8, metformin, {})], &#39;rxnorm&#39;: [Annotation(labeled_dependency, 0, 8, 6809, {&#39;entity&#39;: &#39;metformin&#39;, &#39;relation&#39;: &#39;rxnorm_code&#39;, &#39;all_relations&#39;: &#39;&#39;})]}] Medical val document_assembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) val chunkerMapper = ChunkMapperModel.pretrained(&quot;rxnorm_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;rxnorm&quot;) .setRels([&quot;rxnorm_code&quot;]) mapper_pipeline = Pipeline().setStages([document_assembler, chunkerMapper]) empty_df = spark.createDataFrame([[&#39;&#39;]]).toDF(&#39;text&#39;) mapper_model = mapper_pipeline.fit(empty_df) mapper_lp = LightPipeline(mapper_model) mapper_lp.fullAnnotate(&quot;metformin&quot;) [{&#39;ner_chunk&#39;: [Annotation(document, 0, 8, metformin, {})], &#39;rxnorm&#39;: [Annotation(labeled_dependency, 0, 8, 6809, {&#39;entity&#39;: &#39;metformin&#39;, &#39;relation&#39;: &#39;rxnorm_code&#39;, &#39;all_relations&#39;: &#39;&#39;})]}] ChunkMapperFilterer ApproachModel Input Annotator Types: `` Output Annotator Type: `` ChunkMapperFilterer is an annotator to be used after ChunkMapper that allows to filter chunks based on the results of the mapping, whether it was successful or failed. Example usage and more details can be found on Spark NLP Workshop repository accessible in GitHub, for example the notebook Healthcare Chunk Mapping. Input Annotator Types: CHUNK, LABEL_DEPENDENCY Output Annotator Type: CHUNK Python API: ChunkMapperFilterer Scala API: ChunkMapperFilterer Show Example PythonScala Medical document_assembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentence_detector = ( SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) ) tokenizer = Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) word_embeddings = ( WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ) ner_model = ( MedicalNerModel.pretrained(&quot;ner_posology_greedy&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ) ner_converter = ( NerConverter().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;).setOutputCol(&quot;chunk&quot;) ) chunkerMapper = ( ChunkMapperModel.pretrained(&quot;rxnorm_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;chunk&quot;]) .setOutputCol(&quot;RxNorm_Mapper&quot;) .setRel(&quot;rxnorm_code&quot;) ) cfModel = ( ChunkMapperFilterer() .setInputCols([&quot;chunk&quot;, &quot;RxNorm_Mapper&quot;]) .setOutputCol(&quot;chunks_fail&quot;) .setReturnCriteria(&quot;fail&quot;) ) chunk2doc = Chunk2Doc().setInputCols(&quot;chunks_fail&quot;).setOutputCol(&quot;doc_chunk&quot;) sbert_embedder = ( BertSentenceEmbeddings.pretrained( &quot;sbiobert_base_cased_mli&quot;, &quot;en&quot;, &quot;clinical/models&quot; ) .setInputCols([&quot;doc_chunk&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) .setCaseSensitive(False) ) resolver = ( SentenceEntityResolverModel.pretrained( &quot;sbiobertresolve_rxnorm_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot; ) .setInputCols([&quot;chunks_fail&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;resolver_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) ) resolverMerger = ( ResolverMerger() .setInputCols([&quot;resolver_code&quot;, &quot;RxNorm_Mapper&quot;]) .setOutputCol(&quot;RxNorm&quot;) ) mapper_pipeline = Pipeline( stages=[ document_assembler, sentence_detector, tokenizer, word_embeddings, ner_model, ner_converter, chunkerMapper, chunkerMapper, cfModel, chunk2doc, sbert_embedder, resolver, resolverMerger, ] ) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = mapper_pipeline.fit(empty_data) samples = [ [&quot;The patient was given Adapin 10 MG, coumadn 5 mg&quot;], [&quot;The patient was given Avandia 4 mg, Tegretol, zitiga&quot;], ] result = model.transform(spark.createDataFrame(samples).toDF(&quot;text&quot;)) result.selectExpr( &quot;chunk.result as chunk&quot;, &quot;RxNorm_Mapper.result as RxNorm_Mapper&quot;, &quot;chunks_fail.result as chunks_fail&quot;, &quot;resolver_code.result as resolver_code&quot;, &quot;RxNorm.result as RxNorm&quot;, ).show(truncate=False) +--+-+--+-++ chunk |RxNorm_Mapper |chunks_fail |resolver_code|RxNorm | +--+-+--+-++ [Adapin 10 MG, coumadn 5 mg] |[1000049, NONE] |[coumadn 5 mg]|[200883] |[1000049, 200883] | [Avandia 4 mg, Tegretol, zitiga]|[261242, 203029, NONE]|[zitiga] |[220989] |[261242, 203029, 220989]| +--+-+--+-++ ChunkMerge ApproachModel Merges two chunk columns coming from two annotators(NER, ContextualParser or any other annotator producing chunks). The merger of the two chunk columns is made by selecting one chunk from one of the columns according to certain criteria. The decision on which chunk to select is made according to the chunk indices in the source document. (chunks with longer lengths and highest information will be kept from each source) Labels can be changed by setReplaceDictResource. Input Annotator Types: CHUNK, CHUNK Output Annotator Type: CHUNK Python API: ChunkMergeApproach Scala API: ChunkMergeApproach Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Define a pipeline with 2 different NER models with a ChunkMergeApproach at the end data = spark.createDataFrame([[&quot;A 63-year-old man presents to the hospital ...&quot;]]).toDF(&quot;text&quot;) pipeline = Pipeline(stages=[ nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;), nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;), nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;), nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setOutputCol(&quot;embs&quot;), medical.NerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;]).setOutputCol(&quot;jsl_ner&quot;), nlp.NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;jsl_ner&quot;]).setOutputCol(&quot;jsl_ner_chunk&quot;), medical.NerModel.pretrained(&quot;ner_bionlp&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;]).setOutputCol(&quot;bionlp_ner&quot;), nlp.NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;bionlp_ner&quot;]) .setOutputCol(&quot;bionlp_ner_chunk&quot;), medical.ChunkMergeApproach().setInputCols([&quot;jsl_ner_chunk&quot;, &quot;bionlp_ner_chunk&quot;]).setOutputCol(&quot;merged_chunk&quot;) ]) # Show results result = pipeline.fit(data).transform(data).cache() result.selectExpr(&quot;explode(merged_chunk) as a&quot;) .selectExpr(&quot;a.begin&quot;,&quot;a.end&quot;,&quot;a.result as chunk&quot;,&quot;a.metadata.entity as entity&quot;) .show(5, False) +--++--++ |begin|end|chunk |entity | +--++--++ |5 |15 |63-year-old|Age | |17 |19 |man |Gender | |64 |72 |recurrent |Modifier | |98 |107|cellulitis |Diagnosis| |110 |119|pneumonias |Diagnosis| +--++--++ from johnsnowlabs import * data = spark.createDataFrame([[&quot;Jeffrey Preston Bezos is an American entrepreneur, founder and CEO of Amazon&quot;]]).toDF(&quot;text&quot;) documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) bert_embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;bert_embeddings&quot;) fin_ner = finance.NerModel.pretrained(&#39;finner_deid&#39;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) #.setLabelCasing(&quot;upper&quot;) ner_converter = finance.NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setReplaceLabels({&quot;ORG&quot;: &quot;PARTY&quot;}) # Replace &quot;ORG&quot; entity as &quot;PARTY&quot; ner_finner = finance.NerModel.pretrained(&quot;finner_org_per_role_date&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;bert_embeddings&quot;]) .setOutputCol(&quot;ner_finner&quot;) #.setLabelCasing(&quot;upper&quot;) ner_converter_finner = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_finner&quot;]) .setOutputCol(&quot;ner_finner_chunk&quot;) .setWhiteList([&#39;ROLE&#39;]) # Just use &quot;ROLE&quot; entity from this NER chunk_merge = finance.ChunkMergeApproach() .setInputCols(&quot;ner_finner_chunk&quot;, &quot;ner_chunk&quot;) .setOutputCol(&quot;deid_merged_chunk&quot;) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, bert_embeddings, fin_ner, ner_converter, ner_finner, ner_converter_finner, chunk_merge]) # Show results result = nlpPipeline.fit(data).transform(data).cache() result.select(F.explode(F.arrays_zip(result.deid_merged_chunk.result, result.deid_merged_chunk.metadata)).alias(&quot;cols&quot;)) .select(F.expr(&quot;cols[&#39;0&#39;]&quot;).alias(&quot;chunk&quot;), F.expr(&quot;cols[&#39;1&#39;][&#39;entity&#39;]&quot;).alias(&quot;ner_label&quot;)).show(truncate=False) +++ |chunk |ner_label| +++ |Jeffrey Preston Bezos|PERSON | |founder |ROLE | |CEO |ROLE | |Amazon |PARTY | +++ from johnsnowlabs import * data = spark.createDataFrame([[&quot;ENTIRE AGREEMENT. This Agreement contains the entire understanding of the parties hereto with respect to the transactions and matters contemplated hereby, supersedes all previous Agreements between i-Escrow and 2TheMart concerning the subject matter. 2THEMART.COM, INC.: I-ESCROW, INC.: By:Dominic J. Magliarditi By:Sanjay Bajaj Name: Dominic J. Magliarditi Name: Sanjay Bajaj Title: President Title: VP Business Development Date: 6/21/99 Date: 6/11/99 &quot;]]).toDF(&quot;text&quot;) documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) legal_ner = legal.NerModel.pretrained(&quot;legner_contract_doc_parties&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) #.setLabelCasing(&quot;upper&quot;) ner_converter = legal.NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setReplaceLabels({&quot;ALIAS&quot;: &quot;PARTY&quot;}) ner_signers = legal.NerModel.pretrained(&quot;legner_signers&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_signers&quot;) #.setLabelCasing(&quot;upper&quot;) ner_converter_signers = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_signers&quot;]) .setOutputCol(&quot;ner_signer_chunk&quot;) chunk_merge = legal.ChunkMergeApproach() .setInputCols(&quot;ner_signer_chunk&quot;, &quot;ner_chunk&quot;) .setOutputCol(&quot;deid_merged_chunk&quot;) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, legal_ner, ner_converter, ner_signers, ner_converter_signers, chunk_merge]) # Show results result = nlpPipeline.fit(data).transform(data).cache() result.select(F.explode(F.arrays_zip(result.deid_merged_chunk.result, result.deid_merged_chunk.metadata)).alias(&quot;cols&quot;)) .select(F.expr(&quot;cols[&#39;0&#39;]&quot;).alias(&quot;chunk&quot;), F.expr(&quot;cols[&#39;1&#39;][&#39;entity&#39;]&quot;).alias(&quot;ner_label&quot;)).show(truncate=False) +--+--+ |chunk |ner_label | +--+--+ |ENTIRE AGREEMENT |DOC | |INC |PARTY | |J. Magliarditi |SIGNING_PERSON| |Bajaj |SIGNING_PERSON| |Dominic J. Magliarditi |SIGNING_PERSON| |Sanjay Bajaj |SIGNING_PERSON| |President |SIGNING_TITLE | |VP Business Development|SIGNING_TITLE | +--+--+ MedicalFinanceLegal from johnsnowlabs import * // Define a pipeline with 2 different NER models with a ChunkMergeApproach at the end val data = Seq((&quot;A 63-year-old man presents to the hospital ...&quot;)).toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;), new nlp.SentenceDetector().setInputCol(&quot;document&quot;).setOutputCol(&quot;sentence&quot;), new nlp.Tokenizer().setInputCol(&quot;sentence&quot;).setOutputCol(&quot;token&quot;), nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols(Array(&quot;sentence&quot;,&quot;token&quot;)).setOutputCol(&quot;embs&quot;), medical.NerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;)).setOutputCol(&quot;jsl_ner&quot;), new nlp.NerConverter().setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;jsl_ner&quot;)).setOutputCol(&quot;jsl_ner_chunk&quot;), medical.NerModel.pretrained(&quot;ner_bionlp&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;)).setOutputCol(&quot;bionlp_ner&quot;), new nlp.NerConverter().setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;bionlp_ner&quot;)) .setOutputCol(&quot;bionlp_ner_chunk&quot;), new medical.ChunkMergeApproach().setInputCols(Array(&quot;jsl_ner_chunk&quot;, &quot;bionlp_ner_chunk&quot;)).setOutputCol(&quot;merged_chunk&quot;) )) // Show results val result = pipeline.fit(data).transform(data).cache() result.selectExpr(&quot;explode(merged_chunk) as a&quot;) .selectExpr(&quot;a.begin&quot;,&quot;a.end&quot;,&quot;a.result as chunk&quot;,&quot;a.metadata.entity as entity&quot;) .show(5, false) +--++--++ |begin|end|chunk |entity | +--++--++ |5 |15 |63-year-old|Age | |17 |19 |man |Gender | |64 |72 |recurrent |Modifier | |98 |107|cellulitis |Diagnosis| |110 |119|pneumonias |Diagnosis| +--++--++ from johnsnowlabs import * val data = Seq((&quot;Jeffrey Preston Bezos is an American entrepreneur, founder and CEO of Amazon&quot;)).toDF(&quot;text&quot;) val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCol(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCol(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val bert_embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;bert_embeddings&quot;) val fin_ner = finance.NerModel.pretrained(&#39;finner_deid&#39;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) #.setLabelCasing(&quot;upper&quot;) val ner_converter = finance.NerConverterInternal() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setReplaceLabels({&quot;ORG&quot;: &quot;PARTY&quot;}) # Replace &quot;ORG&quot; entity as &quot;PARTY&quot; val ner_finner = finance.NerModel.pretrained(&quot;finner_org_per_role_date&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;bert_embeddings&quot;)) .setOutputCol(&quot;ner_finner&quot;) #.setLabelCasing(&quot;upper&quot;) val ner_converter_finner = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner_finner&quot;)) .setOutputCol(&quot;ner_finner_chunk&quot;) .setWhiteList([&#39;ROLE&#39;]) # Just use &quot;ROLE&quot; entity from this NER val chunk_merge = new finance.ChunkMergeApproach() .setInputCols(Array(&quot;ner_finner_chunk&quot;, &quot;ner_chunk&quot;)) .setOutputCol(&quot;deid_merged_chunk&quot;) val nlpPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, bert_embeddings, fin_ner, ner_converter, ner_finner, ner_converter_finner, chunk_merge)) val model = nlpPipeline.fit(data) from johnsnowlabs import * val data = Seq((&quot;ENTIRE AGREEMENT. This Agreement contains the entire understanding of the parties hereto with respect to the transactions and matters contemplated hereby, supersedes all previous Agreements between i-Escrow and 2TheMart concerning the subject matter. 2THEMART.COM, INC.: I-ESCROW, INC.: By:Dominic J. Magliarditi By:Sanjay Bajaj Name: Dominic J. Magliarditi Name: Sanjay Bajaj Title: President Title: VP Business Development Date: 6/21/99 Date: 6/11/99 &quot;)).toDF(&quot;text&quot;) val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCol(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCol(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val legal_ner = legal.NerModel.pretrained(&quot;legner_contract_doc_parties&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) #.setLabelCasing(&quot;upper&quot;) val ner_converter = new legal.NerConverterInternal() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setReplaceLabels({&quot;ALIAS&quot;: &quot;PARTY&quot;}) val ner_signers = legal.NerModel.pretrained(&quot;legner_signers&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner_signers&quot;) #.setLabelCasing(&quot;upper&quot;) val ner_converter_signers = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner_signers&quot;)) .setOutputCol(&quot;ner_signer_chunk&quot;) val chunk_merge = new legal.ChunkMergeApproach() .setInputCols(Array(&quot;ner_signer_chunk&quot;, &quot;ner_chunk&quot;)) .setOutputCol(&quot;deid_merged_chunk&quot;) val nlpPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, legal_ner, ner_converter, ner_signers, ner_converter_signers, chunk_merge)) val model = nlpPipeline.fit(data) Merges entities coming from different CHUNK annotations Input Annotator Types: CHUNK, CHUNK Output Annotator Type: CHUNK Python API: ChunkMergeModel Scala API: ChunkMergeModel ChunkSentenceSplitter ApproachModel Input Annotator Types: `` Output Annotator Type: `` ChunkSentenceSplitter annotator can split the documents into chunks according to separators given as CHUNK columns. It is useful when you need to perform different models or analysis in different sections of your document (for example, for different headers, clauses, items, etc.). The given separator chunk can be the output from, for example, RegexMatcher or NerModel. For detailed usage of this annotator, visit this notebook from our Spark NLP Workshop. Input Annotator Types: DOCUMENT, CHUNK Output Annotator Type: DOCUMENT Python API: ChunkSentenceSplitter Scala API: ChunkSentenceSplitter Show Example PythonScala Medical # Defining the pipeline documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) tokenizer = Tokenizer().setInputCols([&quot;document&quot;]).setOutputCol(&quot;token&quot;) tokenClassifier = ( MedicalBertForTokenClassifier.pretrained( &quot;bert_token_classifier_ner_jsl_slim&quot;, &quot;en&quot;, &quot;clinical/models&quot; ) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ) ner_converter = ( NerConverter() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&quot;Header&quot;]) ) chunkSentenceSplitter = ( ChunkSentenceSplitter() .setInputCols(&quot;document&quot;, &quot;ner_chunk&quot;) .setOutputCol(&quot;paragraphs&quot;) .setGroupBySentences(False) ) pipeline = Pipeline( stages=[ documentAssembler, tokenizer, tokenClassifier, ner_converter, chunkSentenceSplitter, ] ) empty_df = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) pipeline_model = pipeline.fit(empty_df) sentences = [ [ &quot;&quot;&quot;ADMISSION DIAGNOSIS Right pleural effusion and suspected malignant mesothelioma. PRINCIPAL DIAGNOSIS Right pleural effusion, suspected malignant mesothelioma. REVIEW OF SYSTEMS Right pleural effusion, firm nodules, diffuse scattered throughout the right pleura and diaphragmatic surface. &quot;&quot;&quot; ] ] df = spark.createDataFrame(sentences).toDF(&quot;text&quot;) paragraphs = pipeline_model.transform(df) paragraphs.selectExpr(&quot;explode(paragraphs) as result&quot;).selectExpr(&quot;result.result&quot;,&quot;result.metadata.entity&quot;, &quot;result.metadata.splitter_chunk&quot;).show(truncate=80) +--++-+ | result|entity| splitter_chunk| +--++-+ |ADMISSION DIAGNOSIS Right pleural effusion and suspected malignant mesothelio...|Header|ADMISSION DIAGNOSIS| |PRINCIPAL DIAGNOSIS Right pleural effusion, suspected malignant mesothelioma....|Header|PRINCIPAL DIAGNOSIS| |REVIEW OF SYSTEMS Right pleural effusion, firm nodules, diffuse scattered thr...|Header| REVIEW OF SYSTEMS| +--++-+ Medical val data = Seq(text,text).toDS.toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;doc&quot;) val regexMatcher = new RegexMatcher().setInputCols(&quot;doc&quot;).setOutputCol(&quot;chunks&quot;).setExternalRules(&quot;src/test/resources/chunker/title_regex.txt&quot;,&quot;,&quot;) val chunkSentenceSplitter = new ChunkSentenceSplitter().setInputCols(&quot;chunks&quot;,&quot;doc&quot;).setOutputCol(&quot;paragraphs&quot;) val pipeline = new Pipeline().setStages(Array(documentAssembler,regexMatcher,chunkSentenceSplitter)) val result = pipeline.fit(data).transform(data).select(&quot;paragraphs&quot;) result.show(truncate = false) ContextualParser ApproachModel Creates a model, that extracts entity from a document based on user defined rules. Rule matching is based on a RegexMatcher defined in a JSON file. It is set through the parameter setJsonPath() In this JSON file, regex is defined that you want to match along with the information that will output on metadata field. Additionally, a dictionary can be provided with setDictionary to map extracted entities to a unified representation. The first column of the dictionary file should be the representation with following columns the possible matches. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: ContextualParserApproach Scala API: ContextualParserApproach Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # An example JSON file `regex_token.json` can look like this: # # { # &quot;entity&quot;: &quot;Stage&quot;, # &quot;ruleScope&quot;: &quot;sentence&quot;, # &quot;regex&quot;: &quot;[cpyrau]?[T][0-9X?][a-z^cpyrau]&quot;, # &quot;matchScope&quot;: &quot;token&quot; # } # # Which means to extract the stage code on a sentence level. # An example pipeline could then be defined like this # Pipeline could then be defined like this documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) # Define the parser (json file needs to be provided) data = spark.createDataFrame([[&quot;A patient has liver metastases pT1bN0M0 and the T5 primary site may be colon or... &quot;]]).toDF(&quot;text&quot;) contextualParser = medical.ContextualParserApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;entity&quot;) .setJsonPath(&quot;/path/to/regex_token.json&quot;) .setCaseSensitive(True) .setContextMatch(False) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, contextualParser ]) result = pipeline.fit(data).transform(data) # Show Results result.selectExpr(&quot;explode(entity)&quot;).show(5, truncate=False) +-+ |col | +-+ |{chunk, 32, 39, pT1bN0M0, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 0}, []} | |{chunk, 49, 50, T5, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 0}, []} | |{chunk, 148, 156, cT4bcN2M1, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 1}, []}| |{chunk, 189, 194, T?N3M1, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 2}, []} | |{chunk, 316, 323, pT1bN0M0, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 3}, []} | +-+ from johnsnowlabs import * # An example JSON file `regex_token.json` can look like this: # # { # &quot;entity&quot;: &quot;Stage&quot;, # &quot;ruleScope&quot;: &quot;sentence&quot;, # &quot;regex&quot;: &quot;[cpyrau]?[T][0-9X?][a-z^cpyrau]&quot;, # &quot;matchScope&quot;: &quot;token&quot; # } # # Which means to extract the stage code on a sentence level. # An example pipeline could then be defined like this # Pipeline could then be defined like this documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) # Define the parser (json file needs to be provided) contextualParser = finance.ContextualParserApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;entity&quot;) .setJsonPath(&quot;/path/to/regex_token.json&quot;) .setCaseSensitive(True) .setContextMatch(False) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, contextualParser ]) from johnsnowlabs import * # An example JSON file `regex_token.json` can look like this: # # { # &quot;entity&quot;: &quot;Stage&quot;, # &quot;ruleScope&quot;: &quot;sentence&quot;, # &quot;regex&quot;: &quot;[cpyrau]?[T][0-9X?][a-z^cpyrau]&quot;, # &quot;matchScope&quot;: &quot;token&quot; # } # # Which means to extract the stage code on a sentence level. # An example pipeline could then be defined like this # Pipeline could then be defined like this documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) # Define the parser (json file needs to be provided) contextualParser = legal.ContextualParserApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;entity&quot;) .setJsonPath(&quot;/path/to/regex_token.json&quot;) .setCaseSensitive(True) .setContextMatch(False) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, contextualParser ]) MedicalFinanceLegal from johnsnowlabs import * // An example JSON file `regex_token.json` can look like this: // // { // &quot;entity&quot;: &quot;Stage&quot;, // &quot;ruleScope&quot;: &quot;sentence&quot;, // &quot;regex&quot;: &quot;[cpyrau]?[T][0-9X?][a-z^cpyrau]&quot;, // &quot;matchScope&quot;: &quot;token&quot; // } // // Which means to extract the stage code on a sentence level. // An example pipeline could then be defined like this val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) // Define the parser (json file needs to be provided) val data = Seq(&quot;A patient has liver metastases pT1bN0M0 and the T5 primary site may be colon or... &quot;).toDF(&quot;text&quot;) val contextualParser = new medical.ContextualParserApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;entity&quot;) .setJsonPath(&quot;/path/to/regex_token.json&quot;) .setCaseSensitive(true) .setContextMatch(false) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, contextualParser )) val result = pipeline.fit(data).transform(data) // Show Results // // result.selectExpr(&quot;explode(entity)&quot;).show(5, truncate=false) // +-+ // |col | // +-+ // |{chunk, 32, 39, pT1bN0M0, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 0}, []} | // |{chunk, 49, 50, T5, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 0}, []} | // |{chunk, 148, 156, cT4bcN2M1, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 1}, []}| // |{chunk, 189, 194, T?N3M1, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 2}, []} | // |{chunk, 316, 323, pT1bN0M0, {field -&gt; Stage, normalized -&gt; , confidenceValue -&gt; 0.13, hits -&gt; regex, sentence -&gt; 3}, []} | // +-+ // from johnsnowlabs import * // An example JSON file `regex_token.json` can look like this: // // { // &quot;entity&quot;: &quot;Stage&quot;, // &quot;ruleScope&quot;: &quot;sentence&quot;, // &quot;regex&quot;: &quot;[cpyrau]?[T][0-9X?][a-z^cpyrau]&quot;, // &quot;matchScope&quot;: &quot;token&quot; // } // // Which means to extract the stage code on a sentence level. // An example pipeline could then be defined like this val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) // Define the parser (json file needs to be provided) val data = Seq(&quot;A patient has liver metastases pT1bN0M0 and the T5 primary site may be colon or... &quot;).toDF(&quot;text&quot;) val contextualParser = new finance.ContextualParserApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;entity&quot;) .setJsonPath(&quot;/path/to/regex_token.json&quot;) .setCaseSensitive(true) .setContextMatch(false) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, contextualParser )) from johnsnowlabs import * // An example JSON file `regex_token.json` can look like this: // // { // &quot;entity&quot;: &quot;Stage&quot;, // &quot;ruleScope&quot;: &quot;sentence&quot;, // &quot;regex&quot;: &quot;[cpyrau]?[T][0-9X?][a-z^cpyrau]&quot;, // &quot;matchScope&quot;: &quot;token&quot; // } // // Which means to extract the stage code on a sentence level. // An example pipeline could then be defined like this val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) // Define the parser (json file needs to be provided) val data = Seq(&quot;A patient has liver metastases pT1bN0M0 and the T5 primary site may be colon or... &quot;).toDF(&quot;text&quot;) val contextualParser = new legal.ContextualParserApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;entity&quot;) .setJsonPath(&quot;/path/to/regex_token.json&quot;) .setCaseSensitive(true) .setContextMatch(false) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, contextualParser )) Extracts entity from a document based on user defined rules. Rule matching is based on a RegexMatcher defined in a JSON file. In this file, regex is defined that you want to match along with the information that will output on metadata field. To instantiate a model, see ContextualParserApproach and its accompanied example. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: ContextualParserModel Scala API: ContextualParserModel DateNormalizer This annotator transforms date mentions to a common standard format: YYYY/MM/DD. It is useful when using data from different sources, some times from different countries that has different formats to represent dates. For the relative dates (next year, past month, etc.), you can define an achor date to create the normalized date by setting the parameters anchorDateYear, anchorDateMonth, and anchorDateDay. The resultant chunk date will contain a metada indicating whether the normalization was successful or not (True / False). Input Annotator Types: CHUNK Output Annotator Type: CHUNK Python API: DateNormalizer Scala API: DateNormalizer Show Example PythonScala Medical from pyspark.sql.types import StringType dates = [ &quot;08/02/2018&quot;, &quot;11/2018&quot;, &quot;11/01/2018&quot;, &quot;12Mar2021&quot;, &quot;Jan 30, 2018&quot;, &quot;13.04.1999&quot;, &quot;3April 2020&quot;, &quot;next monday&quot;, &quot;today&quot;, &quot;next week&quot;, ] df = spark.createDataFrame(dates, StringType()).toDF(&quot;original_date&quot;) document_assembler = ( DocumentAssembler().setInputCol(&quot;original_date&quot;).setOutputCol(&quot;document&quot;) ) doc2chunk = Doc2Chunk().setInputCols(&quot;document&quot;).setOutputCol(&quot;date_chunk&quot;) date_normalizer = ( DateNormalizer() .setInputCols(&quot;date_chunk&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2000) .setAnchorDateMonth(3) .setAnchorDateDay(15) ) pipeline = Pipeline(stages=[document_assembler, doc2chunk, date_normalizer]) result = pipeline.fit(df).transform(df) result.selectExpr( &quot;date.result as normalized_date&quot;, &quot;original_date&quot;, &quot;date.metadata[0].normalized as metadata&quot;, ).show() ++-+--+ |normalized_date|original_date|metadata| ++-+--+ | [2018/08/02]| 08/02/2018| true| | [2018/11/DD]| 11/2018| true| | [2018/11/01]| 11/01/2018| true| | [2021/03/12]| 12Mar2021| true| | [2018/01/30]| Jan 30, 2018| true| | [1999/04/13]| 13.04.1999| true| | [2020/04/03]| 3April 2020| true| | [2000/03/20]| next monday| true| | [2000/03/15]| today| true| | [2000/03/22]| next week| true| ++-+--+ Medical val df = Seq((&quot;08/02/2018&quot;),(&quot;11/2018&quot;),(&quot;11/01/2018&quot;),(&quot;next monday&quot;),(&quot;today&quot;),(&quot;next week&quot;)).toDF(&quot;original_date&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;original_date&quot;).setOutputCol(&quot;document&quot;) val chunksDF = documentAssembler .transform(df) .mapAnnotationsCol[Seq[Annotation]](&quot;document&quot;, &quot;chunk_date&quot;, CHUNK, (aa:Seq[Annotation]) =&gt; aa.map( ann =&gt; ann.copy(annotatorType = CHUNK))) val dateNormalizerModel = new DateNormalizer() .setInputCols(&quot;chunk_date&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateDay(15) .setAnchorDateMonth(3) .setAnchorDateYear(2000) val dateDf = dateNormalizerModel.transform(chunksDF) dateDf.select(&quot;chunk_date.result&quot;,&quot;text&quot;).show() +-+-+ | result|original_date| +-+-+ | [08/02/2018]| 08/02/2018| | [11/2018]| 11/2018| | [11/01/2018]| 11/01/2018| |[next monday]| next monday| | [today]| today| | [next week]| next week| +-+-+ DeIdentification ApproachModel Contains all the methods for training a DeIdentificationModel model. This module can obfuscate or mask the entities that contains personal information. These can be set with a file of regex patterns with setRegexPatternsDictionary, where each line is a mapping of entity to regex. DATE d{4} AID d{6,7} Additionally, obfuscation strings can be defined with setObfuscateRefFile, where each line is a mapping of string to entity. The format and seperator can be speficied with setRefFileFormat and setRefSep. Dr. Gregory House#DOCTOR 01010101#MEDICALRECORD Ideally this annotator works in conjunction with Demographic Named EntityRecognizers that can be trained either using TextMatchers, RegexMatchers, DateMatchers, NerCRFs or NerDLs Input Annotator Types: DOCUMENT, TOKEN, CHUNK Output Annotator Type: DOCUMENT Python API: DeIdentification Scala API: DeIdentification Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(True) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Ner entities clinical_sensitive_entities = medical.NerModel .pretrained(&quot;ner_deid_enriched&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]).setOutputCol(&quot;ner&quot;) nerConverter = medical.NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) # Deidentification deIdentification = medical.DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) # file with custom regex pattern for custom entities .setRegexPatternsDictionary(&quot;path/to/dic_regex_patterns_main_categories.txt&quot;) # file with custom obfuscator names for the entities .setObfuscateRefFile(&quot;path/to/obfuscate_fixed_entities.txt&quot;) .setRefFileFormat(&quot;csv&quot;) .setRefSep(&quot;#&quot;) .setMode(&quot;obfuscate&quot;) .setDateFormats(Array(&quot;MM/dd/yy&quot;,&quot;yyyy-MM-dd&quot;)) .setObfuscateDate(True) .setDateTag(&quot;DATE&quot;) .setDays(5) .setObfuscateRefSource(&quot;file&quot;) # Pipeline data = spark.createDataFrame([ [&quot;# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09.&quot;] ]).toDF(&quot;text&quot;) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, clinical_sensitive_entities, nerConverter, deIdentification ]) result = pipeline.fit(data).transform(data) # Show Results result.select(&quot;dei.result&quot;).show(truncate = False) +--+ |result | +--+ |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , &lt;AGE&gt; years-old , Record date : 2079-11-14.]| +--+ from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(True) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Ner entities ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) nerConverter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_con&quot;) # Deidentification deIdentification = finance.DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) # file with custom regex pattern for custom entities .setRegexPatternsDictionary(&quot;path/to/dic_regex_patterns_main_categories.txt&quot;) # file with custom obfuscator names for the entities .setObfuscateRefFile(&quot;path/to/obfuscate_fixed_entities.txt&quot;) .setRefFileFormat(&quot;csv&quot;) .setRefSep(&quot;#&quot;) .setMode(&quot;obfuscate&quot;) .setDateFormats(Array(&quot;MM/dd/yy&quot;,&quot;yyyy-MM-dd&quot;)) .setObfuscateDate(True) .setDateTag(&quot;DATE&quot;) .setDays(5) .setObfuscateRefSource(&quot;file&quot;) # Pipeline pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, nerConverter, deIdentification ]) from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(True) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Ner entities ner_model = legal.NerModel.pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) nerConverter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_con&quot;) # Deidentification deIdentification = legal.DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) # file with custom regex pattern for custom entities .setRegexPatternsDictionary(&quot;path/to/dic_regex_patterns_main_categories.txt&quot;) # file with custom obfuscator names for the entities .setObfuscateRefFile(&quot;path/to/obfuscate_fixed_entities.txt&quot;) .setRefFileFormat(&quot;csv&quot;) .setRefSep(&quot;#&quot;) .setMode(&quot;obfuscate&quot;) .setDateFormats(Array(&quot;MM/dd/yy&quot;,&quot;yyyy-MM-dd&quot;)) .setObfuscateDate(True) .setDateTag(&quot;DATE&quot;) .setDays(5) .setObfuscateRefSource(&quot;file&quot;) # Pipeline pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, nerConverter, deIdentification ]) MedicalFinanceLegal from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(true) val tokenizer = new nlp.Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) // Ner entities val clinical_sensitive_entities = medical.NerModel.pretrained(&quot;ner_deid_enriched&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)).setOutputCol(&quot;ner&quot;) val nerConverter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_con&quot;) // Deidentification val deIdentification = new medical.DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) // file with custom regex patterns for custom entities .setRegexPatternsDictionary(&quot;path/to/dic_regex_patterns_main_categories.txt&quot;) // file with custom obfuscator names for the entities .setObfuscateRefFile(&quot;path/to/obfuscate_fixed_entities.txt&quot;) .setRefFileFormat(&quot;csv&quot;) .setRefSep(&quot;#&quot;) .setMode(&quot;obfuscate&quot;) .setDateFormats(Array(&quot;MM/dd/yy&quot;,&quot;yyyy-MM-dd&quot;)) .setObfuscateDate(true) .setDateTag(&quot;DATE&quot;) .setDays(5) .setObfuscateRefSource(&quot;file&quot;) // Pipeline val data = Seq( &quot;# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09.&quot; ).toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, clinical_sensitive_entities, nerConverter, deIdentification )) val result = pipeline.fit(data).transform(data) result.select(&quot;dei.result&quot;).show(truncate = false) // Show Results // // result.select(&quot;dei.result&quot;).show(truncate = false) // +--+ // |result | // +--+ // |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , &lt;AGE&gt; years-old , Record date : 2079-11-14.]| // +--+ // from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(true) val tokenizer = new nlp.Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) // Ner entities val ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val nerConverter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_con&quot;) // Deidentification val deIdentification = new finance.DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) // file with custom regex patterns for custom entities .setRegexPatternsDictionary(&quot;path/to/dic_regex_patterns_main_categories.txt&quot;) // file with custom obfuscator names for the entities .setObfuscateRefFile(&quot;path/to/obfuscate_fixed_entities.txt&quot;) .setRefFileFormat(&quot;csv&quot;) .setRefSep(&quot;#&quot;) .setMode(&quot;obfuscate&quot;) .setDateFormats(Array(&quot;MM/dd/yy&quot;,&quot;yyyy-MM-dd&quot;)) .setObfuscateDate(true) .setDateTag(&quot;DATE&quot;) .setDays(5) .setObfuscateRefSource(&quot;file&quot;) // Pipeline val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, nerConverter, deIdentification )) from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(true) val tokenizer = new nlp.Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) // Ner entities val ner_model = legal.NerModel.pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val nerConverter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_con&quot;) // Deidentification val deIdentification = new legal.DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) // file with custom regex patterns for custom entities .setRegexPatternsDictionary(&quot;path/to/dic_regex_patterns_main_categories.txt&quot;) // file with custom obfuscator names for the entities .setObfuscateRefFile(&quot;path/to/obfuscate_fixed_entities.txt&quot;) .setRefFileFormat(&quot;csv&quot;) .setRefSep(&quot;#&quot;) .setMode(&quot;obfuscate&quot;) .setDateFormats(Array(&quot;MM/dd/yy&quot;,&quot;yyyy-MM-dd&quot;)) .setObfuscateDate(true) .setDateTag(&quot;DATE&quot;) .setDays(5) .setObfuscateRefSource(&quot;file&quot;) // Pipeline val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, nerConverter, deIdentification )) Deidentifies Input Annotations of types DOCUMENT, TOKEN and CHUNK, by either masking or obfuscating the given CHUNKS. To create a configured DeIdentificationModel, please see the example of DeIdentification. Input Annotator Types: DOCUMENT, TOKEN, CHUNK Output Annotator Type: DOCUMENT Python API: DeIdentificationModel Scala API: DeIdentificationModel Show Example PythonScala FinanceLegal from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) bert_embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;bert_embeddings&quot;) fin_ner = finance.NerModel.pretrained(&#39;finner_deid&#39;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) #.setLabelCasing(&quot;upper&quot;) ner_converter = finance.NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setReplaceLabels({&quot;ORG&quot;: &quot;PARTY&quot;}) # Replace &quot;ORG&quot; entity as &quot;PARTY&quot; ner_finner = finance.NerModel.pretrained(&quot;finner_org_per_role_date&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;bert_embeddings&quot;]) .setOutputCol(&quot;ner_finner&quot;) #.setLabelCasing(&quot;upper&quot;) ner_converter_finner = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_finner&quot;]) .setOutputCol(&quot;ner_finner_chunk&quot;) .setWhiteList([&#39;ROLE&#39;]) # Just use &quot;ROLE&quot; entity from this NER chunk_merge = finance.ChunkMergeApproach() .setInputCols(&quot;ner_finner_chunk&quot;, &quot;ner_chunk&quot;) .setOutputCol(&quot;deid_merged_chunk&quot;) deidentification = finance.DeIdentification() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;deid_merged_chunk&quot;]) .setOutputCol(&quot;deidentified&quot;) .setMode(&quot;mask&quot;) .setIgnoreRegex(True) # Pipeline data = spark.createDataFrame([ [&quot;Jeffrey Preston Bezos is an American entrepreneur, founder and CEO of Amazon&quot;] ]).toDF(&quot;text&quot;) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, bert_embeddings, fin_ner, ner_converter, ner_finner, ner_converter_finner, chunk_merge, deidentification]) result = nlpPipeline.fit(data).transform(data) from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) legal_ner = legal.NerModel.pretrained(&quot;legner_contract_doc_parties&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) #.setLabelCasing(&quot;upper&quot;) ner_converter = legal.NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setReplaceLabels({&quot;ALIAS&quot;: &quot;PARTY&quot;}) ner_signers = legal.NerModel.pretrained(&quot;legner_signers&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_signers&quot;) #.setLabelCasing(&quot;upper&quot;) ner_converter_signers = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_signers&quot;]) .setOutputCol(&quot;ner_signer_chunk&quot;) chunk_merge = legal.ChunkMergeApproach() .setInputCols(&quot;ner_signer_chunk&quot;, &quot;ner_chunk&quot;) .setOutputCol(&quot;deid_merged_chunk&quot;) deidentification = legal.DeIdentification() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;deid_merged_chunk&quot;]) .setOutputCol(&quot;deidentified&quot;) .setMode(&quot;mask&quot;) .setIgnoreRegex(True) # Pipeline data = spark.createDataFrame([ [&quot;ENTIRE AGREEMENT. This Agreement contains the entire understanding of the parties hereto with respect to the transactions and matters contemplated hereby, supersedes all previous Agreements between i-Escrow and 2TheMart concerning the subject matter. 2THEMART.COM, INC.: I-ESCROW, INC.: By:Dominic J. Magliarditi By:Sanjay Bajaj Name: Dominic J. Magliarditi Name: Sanjay Bajaj Title: President Title: VP Business Development Date: 6/21/99 Date: 6/11/99 &quot;] ]).toDF(&quot;text&quot;) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, legal_ner, ner_converter, ner_signers, ner_converter_signers, chunk_merge, deidentification]) result = nlpPipeline.fit(data).transform(data) FinanceLegal from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) val embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val bert_embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;bert_embeddings&quot;) val fin_ner = finance.NerModel.pretrained(&#39;finner_deid&#39;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) #.setLabelCasing(&quot;upper&quot;) val ner_converter = finance.NerConverterInternal() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setReplaceLabels({&quot;ORG&quot;: &quot;PARTY&quot;}) # Replace &quot;ORG&quot; entity as &quot;PARTY&quot; val ner_finner = finance.NerModel.pretrained(&quot;finner_org_per_role_date&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;bert_embeddings&quot;)) .setOutputCol(&quot;ner_finner&quot;) #.setLabelCasing(&quot;upper&quot;) val ner_converter_finner = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner_finner&quot;)) .setOutputCol(&quot;ner_finner_chunk&quot;) .setWhiteList([&#39;ROLE&#39;]) # Just use &quot;ROLE&quot; entity from this NER val chunk_merge = new finance.ChunkMergeApproach() .setInputCols(Array(&quot;ner_finner_chunk&quot;, &quot;ner_chunk&quot;)) .setOutputCol(&quot;deid_merged_chunk&quot;) val deidentification = new finance.DeIdentification() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;deid_merged_chunk&quot;)) .setOutputCol(&quot;deidentified&quot;) .setMode(&quot;mask&quot;) .setIgnoreRegex(True) # Pipeline val data = Seq(&quot;Jeffrey Preston Bezos is an American entrepreneur, founder and CEO of Amazon&quot;).toDF(&quot;text&quot;) val nlpPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, bert_embeddings, fin_ner, ner_converter, ner_finner, ner_converter_finner, chunk_merge, deidentification)) val result = nlpPipeline.fit(data).transform(data) from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) val embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val legal_ner = legal.NerModel.pretrained(&quot;legner_contract_doc_parties&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) #.setLabelCasing(&quot;upper&quot;) val ner_converter = new legal.NerConverterInternal() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setReplaceLabels({&quot;ALIAS&quot;: &quot;PARTY&quot;}) val ner_signers = legal.NerModel.pretrained(&quot;legner_signers&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner_signers&quot;) #.setLabelCasing(&quot;upper&quot;) val ner_converter_signers = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner_signers&quot;)) .setOutputCol(&quot;ner_signer_chunk&quot;) val chunk_merge = new legal.ChunkMergeApproach() .setInputCols(Array(&quot;ner_signer_chunk&quot;, &quot;ner_chunk&quot;)) .setOutputCol(&quot;deid_merged_chunk&quot;) val deidentification = new legal.DeIdentification() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;deid_merged_chunk&quot;)) .setOutputCol(&quot;deidentified&quot;) .setMode(&quot;mask&quot;) .setIgnoreRegex(True) # Pipeline val data = Seq(&quot;ENTIRE AGREEMENT. This Agreement contains the entire understanding of the parties hereto with respect to the transactions and matters contemplated hereby, supersedes all previous Agreements between i-Escrow and 2TheMart concerning the subject matter. 2THEMART.COM, INC.: I-ESCROW, INC.: By:Dominic J. Magliarditi By:Sanjay Bajaj Name: Dominic J. Magliarditi Name: Sanjay Bajaj Title: President Title: VP Business Development Date: 6/21/99 Date: 6/11/99 &quot;).toDF(&quot;text&quot;) val nlpPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, legal_ner, ner_converter, ner_signers, ner_converter_signers, chunk_merge, deidentification)) val result = nlpPipeline.fit(data).transform(data) Doc2ChunkInternal Converts DOCUMENT, TOKEN typed annotations into CHUNK type with the contents of a chunkCol. Chunk text must be contained within input DOCUMENT. May be either StringType or ArrayType[StringType] (using setIsArray). Useful for annotators that require a CHUNK type input. For more extended examples on document pre-processing see the Spark NLP Workshop. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: Doc2ChunkInternal Scala API: Doc2ChunkInternal Show Example PythonScala Medical import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) tokenizer = Tokenizer().setInputCol(&quot;document&quot;).setOutputCol(&quot;token&quot;) chunkAssembler = ( Doc2ChunkInternal() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) .setIsArray(True) ) data = spark.createDataFrame( [ [ &quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;, [&quot;Spark NLP&quot;, &quot;text processing library&quot;, &quot;natural language processing&quot;], ] ] ).toDF(&quot;text&quot;, &quot;target&quot;) pipeline = ( Pipeline().setStages([documentAssembler, tokenizer, chunkAssembler]).fit(data) ) result = pipeline.transform(data) result.selectExpr(&quot;chunk.result&quot;, &quot;chunk.annotatorType&quot;).show(truncate=False) +--++ |result |annotatorType | +--++ |[Spark NLP, text processing library, natural language processing]|[chunk, chunk, chunk]| +--++ DocumentHashCoder This annotator can replace dates in a column of DOCUMENT type according with the hash code of any other column. It uses the hash of the specified column and creates a new document column containing the day shift information. In sequence, the DeIdentification annotator deidentifies the document with the shifted date information. If the specified column contains strings that can be parsed to integers, use those numbers to make the shift in the data accordingly. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: DocumentHashCoder Scala API: DocumentHashCoder Show Example PythonScala Medical import pandas as pd data = pd.DataFrame( {&#39;patientID&#39; : [&#39;A001&#39;, &#39;A001&#39;, &#39;A003&#39;, &#39;A003&#39;], &#39;text&#39; : [&#39;Chris Brown was discharged on 10/02/2022&#39;, &#39;Mark White was discharged on 10/04/2022&#39;, &#39;John was discharged on 15/03/2022&#39;, &#39;John Moore was discharged on 15/12/2022&#39; ], &#39;dateshift&#39; : [&#39;10&#39;, &#39;10&#39;, &#39;30&#39;, &#39;30&#39;] } ) my_input_df = spark.createDataFrame(data) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) documentHasher = DocumentHashCoder() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;document2&quot;) .setDateShiftColumn(&quot;dateshift&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document2&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document2&quot;, &quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) clinical_ner = MedicalNerModel .pretrained(&quot;ner_deid_subentity_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document2&quot;,&quot;token&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;document2&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;document2&quot;]) .setOutputCol(&quot;deid_text&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateDate(True) .setDateTag(&quot;DATE&quot;) .setLanguage(&quot;en&quot;) .setObfuscateRefSource(&#39;faker&#39;) .setUseShifDays(True) pipeline_col = Pipeline().setStages([ documentAssembler, documentHasher, tokenizer, embeddings, clinical_ner, ner_converter, de_identification ]) empty_data = spark.createDataFrame([[&quot;&quot;, &quot;&quot;, &quot;&quot;]]).toDF(&quot;patientID&quot;,&quot;text&quot;, &quot;dateshift&quot;) pipeline_col_model = pipeline_col.fit(empty_data) output = pipeline_col_model.transform(my_input_df) output.select(&#39;text&#39;, &#39;dateshift&#39;, &#39;deid_text.result&#39;).show(truncate = False) +-++-+ text |dateshift|result | +-++-+ Chris Brown was discharged on 10/02/2022|10 |[Ellender Manual was discharged on 20/02/2022]| Mark White was discharged on 10/04/2022 |10 |[Errol Bang was discharged on 20/04/2022] | John was discharged on 15/03/2022 |30 |[Ariel Null was discharged on 14/04/2022] | John Moore was discharged on 15/12/2022 |30 |[Jean Cotton was discharged on 14/01/2023] | +-++-+ DocumentLogRegClassifier ApproachModel Trains a model to classify documents with a Logarithmic Regression algorithm. Training data requires columns for text and their label. The result is a trained DocumentLogRegClassifierModel. Input Annotator Types: TOKEN Output Annotator Type: CATEGORY Python API: DocumentLogRegClassifierApproach Scala API: DocumentLogRegClassifierApproach Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Define pipeline stages to prepare the data document_assembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = nlp.Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) stopwords_cleaner = nlp.StopWordsCleaner() .setInputCols([&quot;normalized&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) stemmer = nlp.Stemmer() .setInputCols([&quot;cleanTokens&quot;]) .setOutputCol(&quot;stem&quot;) # Define the document classifier and fit training data to it logreg = medical.DocumentLogRegClassifierApproach() .setInputCols([&quot;stem&quot;]) .setLabelCol(&quot;category&quot;) .setOutputCol(&quot;prediction&quot;) pipeline = Pipeline(stages=[ document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, logreg ]) model = pipeline.fit(trainingData) from johnsnowlabs import * # Define pipeline stages to prepare the data document_assembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = nlp.Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) stopwords_cleaner = nlp.StopWordsCleaner() .setInputCols([&quot;normalized&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) stemmer = nlp.Stemmer() .setInputCols([&quot;cleanTokens&quot;]) .setOutputCol(&quot;stem&quot;) # Define the document classifier and fit training data to it logreg = finance.DocumentLogRegClassifierApproach() .setInputCols([&quot;stem&quot;]) .setLabelCol(&quot;category&quot;) .setOutputCol(&quot;prediction&quot;) pipeline = Pipeline(stages=[ document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, logreg ]) model = pipeline.fit(trainingData) from johnsnowlabs import * # Define pipeline stages to prepare the data document_assembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = nlp.Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) stopwords_cleaner = nlp.StopWordsCleaner() .setInputCols([&quot;normalized&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) stemmer = nlp.Stemmer() .setInputCols([&quot;cleanTokens&quot;]) .setOutputCol(&quot;stem&quot;) # Define the document classifier and fit training data to it logreg = legal.DocumentLogRegClassifierApproach() .setInputCols([&quot;stem&quot;]) .setLabelCol(&quot;category&quot;) .setOutputCol(&quot;prediction&quot;) pipeline = Pipeline(stages=[ document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, logreg ]) model = pipeline.fit(trainingData) MedicalFinanceLegal from johnsnowlabs import * // Define pipeline stages to prepare the data val document_assembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new nlp.Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) val stopwords_cleaner = new nlp.StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val stemmer = new nlp.Stemmer() .setInputCols(&quot;cleanTokens&quot;) .setOutputCol(&quot;stem&quot;) // Define the document classifier and fit training data to it val logreg = new medical.DocumentLogRegClassifierApproach() .setInputCols(&quot;stem&quot;) .setLabelCol(&quot;category&quot;) .setOutputCol(&quot;prediction&quot;) val pipeline = new Pipeline().setStages(Array( document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, logreg )) val model = pipeline.fit(trainingData) from johnsnowlabs import * // Define pipeline stages to prepare the data val document_assembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new nlp.Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) val stopwords_cleaner = new nlp.StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val stemmer = new nlp.Stemmer() .setInputCols(&quot;cleanTokens&quot;) .setOutputCol(&quot;stem&quot;) // Define the document classifier and fit training data to it val logreg = new finance.DocumentLogRegClassifierApproach() .setInputCols(&quot;stem&quot;) .setLabelCol(&quot;category&quot;) .setOutputCol(&quot;prediction&quot;) val pipeline = new Pipeline().setStages(Array( document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, logreg )) val model = pipeline.fit(trainingData) from johnsnowlabs import * // Define pipeline stages to prepare the data val document_assembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new nlp.Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) val stopwords_cleaner = new nlp.StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val stemmer = new nlp.Stemmer() .setInputCols(&quot;cleanTokens&quot;) .setOutputCol(&quot;stem&quot;) // Define the document classifier and fit training data to it val logreg = new legal.DocumentLogRegClassifierApproach() .setInputCols(&quot;stem&quot;) .setLabelCol(&quot;category&quot;) .setOutputCol(&quot;prediction&quot;) val pipeline = new Pipeline().setStages(Array( document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, logreg )) val model = pipeline.fit(trainingData) Classifies documents with a Logarithmic Regression algorithm. Currently there are no pretrained models available. Please see DocumentLogRegClassifierApproach to train your own model. Please check out the Models Hub for available models in the future. Input Annotator Types: TOKEN Output Annotator Type: CATEGORY Python API: DocumentLogRegClassifierModel Scala API: DocumentLogRegClassifierModel DrugNormalizer Annotator which normalizes raw text from clinical documents, e.g. scraped web pages or xml documents, from document type columns into Sentence. Removes all dirty characters from text following one or more input regex patterns. Can apply non wanted character removal which a specific policy. Can apply lower case normalization. See Spark NLP Workshop for more examples of usage. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: DrugNormalizer Scala API: DrugNormalizer Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * data = spark.createDataFrame([ [&quot;Sodium Chloride/Potassium Chloride 13bag&quot;], [&quot;interferon alfa-2b 10 million unit ( 1 ml ) injec&quot;], [&quot;aspirin 10 meq/ 5 ml oral sol&quot;] ]).toDF(&quot;text&quot;) document = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) drugNormalizer = medical.DrugNormalizer().setInputCols([&quot;document&quot;]).setOutputCol(&quot;document_normalized&quot;) trainingPipeline = Pipeline(stages=[document, drugNormalizer]) result = trainingPipeline.fit(data).transform(data) result.selectExpr(&quot;explode(document_normalized.result) as normalized_text&quot;).show(truncate=False) +-+ |normalized_text | +-+ |Sodium Chloride / Potassium Chloride 13 bag | |interferon alfa - 2b 10000000 unt ( 1 ml ) injection| |aspirin 2 meq/ml oral solution | +-+ from johnsnowlabs import * document = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) drugNormalizer = finance.DrugNormalizer().setInputCols([&quot;document&quot;]).setOutputCol(&quot;document_normalized&quot;) trainingPipeline = Pipeline(stages=[document, drugNormalizer]) from johnsnowlabs import * document = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) drugNormalizer = legal.DrugNormalizer().setInputCols([&quot;document&quot;]).setOutputCol(&quot;document_normalized&quot;) trainingPipeline = Pipeline(stages=[document, drugNormalizer]) MedicalFinanceLegal from johnsnowlabs import * val data = Seq( (&quot;Sodium Chloride/Potassium Chloride 13bag&quot;), (&quot;interferon alfa-2b 10 million unit ( 1 ml ) injec&quot;), (&quot;aspirin 10 meq/ 5 ml oral sol&quot;) ).toDF(&quot;text&quot;) val document = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val drugNormalizer = new medical.DrugNormalizer().setInputCols(&quot;document&quot;).setOutputCol(&quot;document_normalized&quot;) val trainingPipeline = new Pipeline().setStages(Array(document, drugNormalizer)) val result = trainingPipeline.fit(data).transform(data) result.selectExpr(&quot;explode(document_normalized.result) as normalized_text&quot;).show(false) +-+ |normalized_text | +-+ |Sodium Chloride / Potassium Chloride 13 bag | |interferon alfa - 2b 10000000 unt ( 1 ml ) injection| |aspirin 2 meq/ml oral solution | +-+ from johnsnowlabs import * val document = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val drugNormalizer = new finance.DrugNormalizer().setInputCols(&quot;document&quot;).setOutputCol(&quot;document_normalized&quot;) val trainingPipeline = new Pipeline().setStages(Array(document, drugNormalizer)) from johnsnowlabs import * val document = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val drugNormalizer = new legal.DrugNormalizer().setInputCols(&quot;document&quot;).setOutputCol(&quot;document_normalized&quot;) val trainingPipeline = new Pipeline().setStages(Array(document, drugNormalizer)) EntityChunkEmbeddings Weighted average embeddings of multiple named entities chunk annotations. Entity Chunk Embeddings uses BERT Sentence embeddings to compute a weighted average vector represention of related entity chunks. The input the model consists of chunks of recognized named entities. One or more entities are selected as target entities and for each of them a list of related entities is specified (if empty, all other entities are assumed to be related). The model looks for chunks of the target entities and then tries to pair each target entity (e.g. DRUG) with other related entities (e.g. DOSAGE, STRENGTH, FORM, etc). The criterion for pairing a target entity with another related entity is that they appear in the same sentence and the maximal syntactic distance is below a predefined threshold. The relationship between target and related entities is one-to-many, meaning that if there multiple instances of the same target entity (e.g.) within a sentence, the model will map a related entity (e.g. DOSAGE) to at most one of the instances of the target entity. For example, if there is a sentence “The patient was given 125 mg of paracetamol and metformin”, the model will pair “125 mg” to “paracetamol”, but not to “metformin”. The output of the model is an average embeddings of the chunks of each of the target entities and their related entities. It is possible to specify a particular weight for each entity type. An entity can be defined both as target a entity and as a related entity for some other target entity. For example, we may want to compute the embeddings of SYMPTOMs and their related entities, as well as the embeddings of DRUGs and their related entities, one of each is also SYMPTOM. In such cases, it is possible to use the TARGET_ENTITY:RELATED_ENTITY notation to specify the weight of an related entity (e.g. “DRUG:SYMPTOM” to set the weight of SYMPTOM when it appears as an related entity to target entity DRUG). The relative weights of entities for particular entity chunk embeddings are available in the annotations metadata. This model is a subclass of BertSentenceEmbeddings and shares all parameters with it. It can load any pretrained BertSentenceEmbeddings model. The default model is &quot;sbiobert_base_cased_mli&quot; from clinical/models. Other available models can be found at Models Hub. Input Annotator Types: DEPENDENCY, CHUNK Output Annotator Type: SENTENCE_EMBEDDINGS Python API: EntityChunkEmbeddingsModel Scala API: EntityChunkEmbeddingsModel Show Example PythonScala Medical import sparknlp from sparknlp.base import * from sparknlp_jsl.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline documenter = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;documents&quot;) sentence_detector = SentenceDetector() .setInputCols(&quot;documents&quot;) .setOutputCol(&quot;sentences&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;tokens&quot;) embeddings = WordEmbeddingsModel() .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_model = MedicalNerModel() .pretrained(&quot;ner_posology_large&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverterInternal() .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner&quot;) .setOutputCol(&quot;ner_chunks&quot;) pos_tager = PerceptronModel() .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;) .setOutputCol(&quot;pos_tags&quot;) dependency_parser = DependencyParserModel() .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) drug_chunk_embeddings = EntityChunkEmbeddings() .pretrained(&quot;sbiobert_base_cased_mli&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;drug_chunk_embeddings&quot;) .setMaxSyntacticDistance(3) .setTargetEntities({&quot;DRUG&quot;: []}) .setEntityWeights({&quot;DRUG&quot;: 0.8, &quot;STRENGTH&quot;: 0.2, &quot;DOSAGE&quot;: 0.2, &quot;FORM&quot;: 0.5}) sampleData = &quot;The parient was given metformin 125 mg, 250 mg of coumadin and then one pill paracetamol&quot; data = SparkContextForTest.spark.createDataFrame([[sampleData]]).toDF(&quot;text&quot;) pipeline = Pipeline().setStages([ documenter, sentence_detector, tokenizer, embeddings, ner_model, ner_converter, pos_tager, dependency_parser, drug_chunk_embeddings]) results = pipeline.fit(data).transform(data) results = results .selectExpr(&quot;explode(drug_chunk_embeddings) AS drug_chunk&quot;) .selectExpr(&quot;drug_chunk.result&quot;, &quot;slice(drug_chunk.embeddings, 1, 5) AS drug_embedding&quot;) .cache() results.show(truncate=False) +--+--+ | result| drug_embedding&quot;| +--+--+ |metformin 125 mg |[-0.267413, 0.07614058, -0.5620966, 0.83838946, 0.8911504] | |250 mg coumadin |[0.22319649, -0.07094894, -0.6885556, 0.79176235, 0.82672405] | |one pill paracetamol |[-0.10939768, -0.29242, -0.3574444, 0.3981813, 0.79609615] | +--+--+ Medical import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.ner.{MedicalNerModel, NerConverterInternal} import com.johnsnowlabs.nlp.annotators.embeddings.EntityChunkEmbeddings import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;tokens&quot;) val wordEmbeddings = WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;word_embeddings&quot;) val nerModel = MedicalNerModel .pretrained(&quot;ner_posology_large&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;tokens&quot;, &quot;word_embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val nerConverter = new NerConverterInternal() .setInputCols(&quot;sentence&quot;, &quot;tokens&quot;, &quot;ner&quot;) .setOutputCol(&quot;ner_chunk&quot;) val posTager = PerceptronModel .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;) .setOutputCol(&quot;pos_tags&quot;) val dependencyParser = DependencyParserModel .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;dependencies&quot;) val drugChunkEmbeddings = EntityChunkEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(Array(&quot;ner_chunks&quot;, &quot;dependencies&quot;)) .setOutputCol(&quot;drug_chunk_embeddings&quot;) .setMaxSyntacticDistance(3) .setTargetEntities(Map(&quot;DRUG&quot; -&gt; List())) .setEntityWeights(Map[String, Float](&quot;DRUG&quot; -&gt; 0.8f, &quot;STRENGTH&quot; -&gt; 0.2f, &quot;DOSAGE&quot; -&gt; 0.2f, &quot;FORM&quot; -&gt; 0.5f)) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentenceDetector, tokenizer, wordEmbeddings, nerModel, nerConverter, posTager, dependencyParser, drugChunkEmbeddings)) val sampleText = &quot;The patient was given metformin 125 mg, 250 mg of coumadin and then one pill paracetamol.&quot; val testDataset = Seq(&quot;&quot;).toDS.toDF(&quot;text&quot;) val result = pipeline.fit(emptyDataset).transform(testDataset) result .selectExpr(&quot;explode(drug_chunk_embeddings) AS drug_chunk&quot;) .selectExpr(&quot;drug_chunk.result&quot;, &quot;slice(drug_chunk.embeddings, 1, 5) AS drugEmbedding&quot;) .show(truncate=false) +--+--+ | result| drugEmbedding| +--+--+ |metformin 125 mg |[-0.267413, 0.07614058, -0.5620966, 0.83838946, 0.8911504] | |250 mg coumadin |[0.22319649, -0.07094894, -0.6885556, 0.79176235, 0.82672405] | |one pill paracetamol |[-0.10939768, -0.29242, -0.3574444, 0.3981813, 0.79609615] | +--+-+ FeaturesAssembler The FeaturesAssembler is used to collect features from different columns. It can collect features from single value columns (anything which can be cast to a float, if casts fails then the value is set to 0), array columns or SparkNLP annotations (if the annotation is an embedding, it takes the embedding, otherwise tries to cast the result field). The output of the transformer is a FEATURE_VECTOR annotation (the numeric vector is in the embeddings field). Input Annotator Types: NONE Output Annotator Type: &quot;feature_vector&quot; Python API: FeaturesAssembler Scala API: FeaturesAssembler Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * features_asm = medical.FeaturesAssembler() .setInputCols([&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;]) .setOutputCol(&quot;features&quot;) gen_clf = medical.GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setLearningRate(0.001) .setFixImbalance(True) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2) # keep 20% of the data for validation purposes pipeline = Pipeline(stages=[ features_asm, gen_clf ]) clf_model = pipeline.fit(data) from johnsnowlabs import * features_asm = finance.FeaturesAssembler() .setInputCols([&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;]) .setOutputCol(&quot;features&quot;) gen_clf = finance.GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setLearningRate(0.001) .setFixImbalance(True) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2) # keep 20% of the data for validation purposes pipeline = Pipeline(stages=[ features_asm, gen_clf ]) clf_model = pipeline.fit(data) from johnsnowlabs import * features_asm = legal.FeaturesAssembler() .setInputCols([&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;]) .setOutputCol(&quot;features&quot;) gen_clf = legal.GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setLearningRate(0.001) .setFixImbalance(True) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2) # keep 20% of the data for validation purposes pipeline = Pipeline(stages=[ features_asm, gen_clf ]) clf_model = pipeline.fit(data) MedicalFinanceLegal from johnsnowlabs import * val features_asm = new medical.FeaturesAssembler() .setInputCols(Array(&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;)) .setOutputCol(&quot;features&quot;) val gen_clf = new medical.GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols(&quot;features&quot;) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001f) .setFixImbalance(true) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2f) // keep 20% of the data for validation purposes val pipeline = new Pipeline().setStages(Array( features_asm, gen_clf )) val clf_model = pipeline.fit(data) from johnsnowlabs import * val features_asm = new finance.FeaturesAssembler() .setInputCols(Array(&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;)) .setOutputCol(&quot;features&quot;) val gen_clf = new finance.GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols(&quot;features&quot;) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001f) .setFixImbalance(true) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2f) // keep 20% of the data for validation purposes val pipeline = new Pipeline().setStages(Array( features_asm, gen_clf )) val clf_model = pipeline.fit(data) from johnsnowlabs import * val features_asm = new legal.FeaturesAssembler() .setInputCols(Array(&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;)) .setOutputCol(&quot;features&quot;) val gen_clf = new legal.GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols(&quot;features&quot;) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001f) .setFixImbalance(true) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2f) // keep 20% of the data for validation purposes val pipeline = new Pipeline().setStages(Array( features_asm, gen_clf )) val clf_model = pipeline.fit(data) GenericClassifier ApproachModel Trains a TensorFlow model for generic classification of feature vectors. It takes FEATURE_VECTOR annotations from FeaturesAssembler as input, classifies them and outputs CATEGORY annotations. Please see the Parameters section for required training parameters. For a more extensive example please see the Spark NLP Workshop. Input Annotator Types: FEATURE_VECTOR Output Annotator Type: CATEGORY Python API: GenericClassifierApproach Scala API: GenericClassifierApproach Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * features_asm = medical.FeaturesAssembler() .setInputCols([&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;]) .setOutputCol(&quot;features&quot;) gen_clf = medical.GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001) .setFixImbalance(True) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2) # keep 20% of the data for validation purposes pipeline = Pipeline().setStages([ features_asm, gen_clf ]) clf_model = pipeline.fit(data) from johnsnowlabs import * features_asm = finance.FeaturesAssembler() .setInputCols([&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;]) .setOutputCol(&quot;features&quot;) gen_clf = finance.GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001) .setFixImbalance(True) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2) # keep 20% of the data for validation purposes pipeline = Pipeline().setStages([ features_asm, gen_clf ]) clf_model = pipeline.fit(data) from johnsnowlabs import * features_asm = legal.FeaturesAssembler() .setInputCols([&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;]) .setOutputCol(&quot;features&quot;) gen_clf = legal.GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001) .setFixImbalance(True) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2) # keep 20% of the data for validation purposes pipeline = Pipeline().setStages([ features_asm, gen_clf ]) clf_model = pipeline.fit(data) MedicalFinanceLegal from johnsnowlabs import * val features_asm = new medical.FeaturesAssembler() .setInputCols(Array(&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;)) .setOutputCol(&quot;features&quot;) val gen_clf = new medical.GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols(&quot;features&quot;) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001f) .setFixImbalance(true) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2f) // keep 20% of the data for validation purposes val pipeline = new Pipeline().setStages(Array( features_asm, gen_clf )) val clf_model = pipeline.fit(data) from johnsnowlabs import * val features_asm = new finance.FeaturesAssembler() .setInputCols(Array(&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;)) .setOutputCol(&quot;features&quot;) val gen_clf = new finance.GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols(&quot;features&quot;) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001f) .setFixImbalance(true) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2f) // keep 20% of the data for validation purposes val pipeline = new Pipeline().setStages(Array( features_asm, gen_clf )) val clf_model = pipeline.fit(data) from johnsnowlabs import * val features_asm = new legal.FeaturesAssembler() .setInputCols(Array(&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;)) .setOutputCol(&quot;features&quot;) val gen_clf = new legal.GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols(&quot;features&quot;) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001f) .setFixImbalance(true) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2f) // keep 20% of the data for validation purposes val pipeline = new Pipeline().setStages(Array( features_asm, gen_clf )) val clf_model = pipeline.fit(data) Creates a generic single-label classifier which uses pre-generated Tensorflow graphs. The model operates on FEATURE_VECTOR annotations which can be produced using FeatureAssembler. Requires the FeaturesAssembler to create the input. Input Annotator Types: FEATURE_VECTOR Output Annotator Type: CATEGORY Python API: GenericClassifierModel Scala API: GenericClassifierModel IOBTagger Merges token tags and NER labels from chunks in the specified format. For example output columns as inputs from NerConverter and Tokenizer can be used to merge. Input Annotator Types: TOKEN, CHUNK Output Annotator Type: NAMED_ENTITY Python API: IOBTagger Scala API: IOBTagger Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Pipeline stages are defined where NER is done. NER is converted to chunks. data = spark.createDataFrame([[&quot;A 63-year-old man presents to the hospital ...&quot;]]).toDF(&quot;text&quot;) docAssembler = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols([&quot;sentence&quot;, &quot;token&quot;]).setOutputCol(&quot;embs&quot;) nerModel = medical.NerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;]).setOutputCol(&quot;ner&quot;) nerConverter = nlp.NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]).setOutputCol(&quot;ner_chunk&quot;) # Define the IOB tagger, which needs tokens and chunks as input. Show results. iobTagger = medical.IOBTagger().setInputCols([&quot;token&quot;, &quot;ner_chunk&quot;]).setOutputCol(&quot;ner_label&quot;) pipeline = Pipeline(stages=[docAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, iobTagger]) result.selectExpr(&quot;explode(ner_label) as a&quot;) .selectExpr(&quot;a.begin&quot;,&quot;a.end&quot;,&quot;a.result as chunk&quot;,&quot;a.metadata.word as word&quot;) .where(&quot;chunk!=&#39;O&#39;&quot;).show(5, False) +--++--+--+ |begin|end|chunk |word | +--++--+--+ |5 |15 |B-Age |63-year-old| |17 |19 |B-Gender |man | |64 |72 |B-Modifier |recurrent | |98 |107|B-Diagnosis|cellulitis | |110 |119|B-Diagnosis|pneumonias | +--++--+--+ from johnsnowlabs import * # Pipeline stages are defined where NER is done. NER is converted to chunks. docAssembler = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols([&quot;sentence&quot;, &quot;token&quot;]).setOutputCol(&quot;embs&quot;) ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;).setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;]).setOutputCol(&quot;ner&quot;) nerConverter = nlp.NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]).setOutputCol(&quot;ner_chunk&quot;) # Define the IOB tagger, which needs tokens and chunks as input. Show results. iobTagger = finance.IOBTagger().setInputCols([&quot;token&quot;, &quot;ner_chunk&quot;]).setOutputCol(&quot;ner_label&quot;) pipeline = Pipeline(stages=[docAssembler, sentenceDetector, tokenizer, embeddings, ner_model, nerConverter, iobTagger]) from johnsnowlabs import * # Pipeline stages are defined where NER is done. NER is converted to chunks. docAssembler = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols([&quot;sentence&quot;, &quot;token&quot;]).setOutputCol(&quot;embs&quot;) ner_model = legal.NerModel.pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;).setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;]).setOutputCol(&quot;ner&quot;) nerConverter = nlp.NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]).setOutputCol(&quot;ner_chunk&quot;) # Define the IOB tagger, which needs tokens and chunks as input. Show results. iobTagger = legal.IOBTagger().setInputCols([&quot;token&quot;, &quot;ner_chunk&quot;]).setOutputCol(&quot;ner_label&quot;) pipeline = Pipeline(stages=[docAssembler, sentenceDetector, tokenizer, embeddings, ner_model, nerConverter, iobTagger]) MedicalFinanceLegal from johnsnowlabs import * // Pipeline stages are defined where NER is done. NER is converted to chunks. val data = Seq((&quot;A 63-year-old man presents to the hospital ...&quot;)).toDF(&quot;text&quot;) val docAssembler = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)).setOutputCol(&quot;embs&quot;) val nerModel = medical.NerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;)).setOutputCol(&quot;ner&quot;) val nerConverter = new nlp.NerConverter().setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)).setOutputCol(&quot;ner_chunk&quot;) // Define the IOB tagger, which needs tokens and chunks as input. Show results. val iobTagger = new medical.IOBTagger().setInputCols(Array(&quot;token&quot;, &quot;ner_chunk&quot;)).setOutputCol(&quot;ner_label&quot;) val pipeline = new Pipeline().setStages(Array(docAssembler, sentenceDetector, tokenizer, embeddings, nerModel, nerConverter, iobTagger)) result.selectExpr(&quot;explode(ner_label) as a&quot;) .selectExpr(&quot;a.begin&quot;,&quot;a.end&quot;,&quot;a.result as chunk&quot;,&quot;a.metadata.word as word&quot;) .where(&quot;chunk!=&#39;O&#39;&quot;).show(5, false) +--++--+--+ |begin|end|chunk |word | +--++--+--+ |5 |15 |B-Age |63-year-old| |17 |19 |B-Gender |man | |64 |72 |B-Modifier |recurrent | |98 |107|B-Diagnosis|cellulitis | |110 |119|B-Diagnosis|pneumonias | +--++--+--+ from johnsnowlabs import * // Pipeline stages are defined where NER is done. NER is converted to chunks. val docAssembler = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)).setOutputCol(&quot;embs&quot;) val ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;).setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;)).setOutputCol(&quot;ner&quot;) val nerConverter = new nlp.NerConverter().setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)).setOutputCol(&quot;ner_chunk&quot;) // Define the IOB tagger, which needs tokens and chunks as input. Show results. val iobTagger = new legal.IOBTagger().setInputCols(Array(&quot;token&quot;, &quot;ner_chunk&quot;)).setOutputCol(&quot;ner_label&quot;) val pipeline = new Pipeline().setStages(Array(docAssembler, sentenceDetector, tokenizer, embeddings, ner_model, nerConverter, iobTagger)) from johnsnowlabs import * // Pipeline stages are defined where NER is done. NER is converted to chunks. val docAssembler = new nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;).setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)).setOutputCol(&quot;embs&quot;) val ner_model = legal.NerModel.pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;).setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;)).setOutputCol(&quot;ner&quot;) val nerConverter = new nlp.NerConverter().setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)).setOutputCol(&quot;ner_chunk&quot;) // Define the IOB tagger, which needs tokens and chunks as input. Show results. val iobTagger = new legal.IOBTagger().setInputCols(Array(&quot;token&quot;, &quot;ner_chunk&quot;)).setOutputCol(&quot;ner_label&quot;) val pipeline = new Pipeline().setStages(Array(docAssembler, sentenceDetector, tokenizer, embeddings, ner_model, nerConverter, iobTagger)) NerChunker Extracts phrases that fits into a known pattern using the NER tags. Useful for entity groups with neighboring tokens when there is no pretrained NER model to address certain issues. A Regex needs to be provided to extract the tokens between entities. Input Annotator Types: DOCUMENT, NAMED_ENTITY Output Annotator Type: CHUNK Python API: NerChunker Scala API: NerChunker Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Defining pipeline stages for NER data= spark.createDataFrame([[&quot;She has cystic cyst on her kidney.&quot;]]).toDF(&quot;text&quot;) documentAssembler= nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector= nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(False) tokenizer= nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) ner = medical.NerModel.pretrained(&quot;ner_radiology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) .setIncludeConfidence(True) # Define the NerChunker to combine to chunks chunker = medical.NerChunker() .setInputCols([&quot;sentence&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers([&quot;&lt;ImagingFindings&gt;.*&lt;BodyPart&gt;&quot;]) pipeline= Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner, chunker ]) result = pipeline.fit(data).transform(data) # Show results: result.selectExpr(&quot;explode(arrays_zip(ner.metadata , ner.result))&quot;) .selectExpr(&quot;col[&#39;0&#39;].word as word&quot; , &quot;col[&#39;1&#39;] as ner&quot;).show(truncate=False) ++--+ |word |ner | ++--+ |She |O | |has |O | |cystic|B-ImagingFindings| |cyst |I-ImagingFindings| |on |O | |her |O | |kidney|B-BodyPart | |. |O | ++--+ result.select(&quot;ner_chunk.result&quot;).show(truncate=False) ++ |result | ++ |[cystic cyst on her kidney]| ++ from johnsnowlabs import * # Defining pipeline stages for NER documentAssembler= nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector= nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(False) tokenizer= nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) # Define the NerChunker to combine to chunks chunker = finance.NerChunker() .setInputCols([&quot;sentence&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers([&quot;&lt;ImagingFindings&gt;.*&lt;BodyPart&gt;&quot;]) pipeline= Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, chunker ]) from johnsnowlabs import * # Defining pipeline stages for NER documentAssembler= nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector= nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(False) tokenizer= nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) ner_model = legal.NerModel.pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) # Define the NerChunker to combine to chunks chunker = legal.NerChunker() .setInputCols([&quot;sentence&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers([&quot;&lt;ImagingFindings&gt;.*&lt;BodyPart&gt;&quot;]) pipeline= Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, chunker ]) MedicalFinanceLegal from johnsnowlabs import * // Defining pipeline stages for NER val data= Seq(&quot;She has cystic cyst on her kidney.&quot;).toDF(&quot;text&quot;) val documentAssembler=new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector=new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(False) val tokenizer=new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;,&quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) val ner = medical.NerModel.pretrained(&quot;ner_radiology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) .setIncludeConfidence(True) // Define the NerChunker to combine to chunks val chunker = new medical.NerChunker() .setInputCols(Array(&quot;sentence&quot;,&quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers(Array(&quot;&lt;ImagingFindings&gt;.&lt;BodyPart&gt;&quot;)) val pipeline=new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner, chunker )) val result = pipeline.fit(data).transform(data) // Show results: // // result.selectExpr(&quot;explode(arrays_zip(ner.metadata , ner.result))&quot;) // .selectExpr(&quot;col[&#39;0&#39;].word as word&quot; , &quot;col[&#39;1&#39;] as ner&quot;).show(truncate=false) // ++--+ // |word |ner | // ++--+ // |She |O | // |has |O | // |cystic|B-ImagingFindings| // |cyst |I-ImagingFindings| // |on |O | // |her |O | // |kidney|B-BodyPart | // |. |O | // ++--+ // result.select(&quot;ner_chunk.result&quot;).show(truncate=false) // ++ // |result | // ++ // |[cystic cyst on her kidney]| // ++ // from johnsnowlabs import * // Defining pipeline stages for NER val documentAssembler=new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector=new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(False) val tokenizer=new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;,&quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) val ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) // Define the NerChunker to combine to chunks val chunker = new finance.NerChunker() .setInputCols(Array(&quot;sentence&quot;,&quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers(Array(&quot;&lt;ImagingFindings&gt;.&lt;BodyPart&gt;&quot;)) val pipeline=new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, chunker )) from johnsnowlabs import * // Defining pipeline stages for NER val documentAssembler=new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector=new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) .setUseAbbreviations(False) val tokenizer=new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;,&quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) val ner_model = legal.NerModel.pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) // Define the NerChunker to combine to chunks val chunker = new legal.NerChunker() .setInputCols(Array(&quot;sentence&quot;,&quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers(Array(&quot;&lt;ImagingFindings&gt;.&lt;BodyPart&gt;&quot;)) val pipeline=new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, chunker )) NerConverterInternal Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. Chunks with no associated entity (tagged “O”) are filtered out. This licensed annotator adds extra functionality to the open-source version by adding the following parameters: blackList, greedyMode, threshold, and ignoreStopWords that are not available in the NerConverter annotator. See also Inside–outside–beginning (tagging) for more information. Input Annotator Types: DOCUMENT, TOKEN, NAMED_ENTITY Output Annotator Type: CHUNK Python API: NerConverterInternal Scala API: NerConverterInternal Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl_healthcare&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) jsl_ner = medical.NerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;jsl_ner&quot;) jsl_ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;jsl_ner&quot;]) .setOutputCol(&quot;jsl_ner_chunk&quot;) jsl_ner_converter_internal = medical.NerConverterInternal() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;jsl_ner&quot;]) .setOutputCol(&quot;replaced_ner_chunk&quot;) .setReplaceDictResource(&quot;replace_dict.csv&quot;,&quot;text&quot;, {&quot;delimiter&quot;:&quot;,&quot;}) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, word_embeddings, jsl_ner, jsl_ner_converter, jsl_ner_converter_internal ]) result = nlpPipeline.fit(data).transform(data) from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) #.setCustomBounds([&quot; n n&quot;]) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) fin_ner = finance.NerModel.pretrained(&quot;finner_deid&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) #.setLabelCasing(&quot;upper&quot;) ner_converter = finance.NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setReplaceLabels({&quot;ORG&quot;: &quot;PARTY&quot;}) # Replace &quot;ORG&quot; entity as &quot;PARTY&quot; nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, fin_ner, ner_converter]) result = nlpPipeline.fit(data).transform(data) from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) #.setCustomBounds([&quot; n n&quot;]) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) legal_ner = legal.NerModel.pretrained(&quot;legner_contract_doc_parties&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) #.setLabelCasing(&quot;upper&quot;) ner_converter = legal.NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setReplaceLabels({&quot;ALIAS&quot;: &quot;PARTY&quot;}) # &quot;ALIAS&quot; are secondary names of companies, so let&#39;s extract them also as PARTY nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, legal_ner, ner_converter]) result = nlpPipeline.fit(data).transform(data) MedicalFinanceLegal from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetectorDLModel .pretrained(&quot;sentence_detector_dl_healthcare&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val word_embeddings = nlp.WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val jsl_ner = medical.NerModel .pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;jsl_ner&quot;) val jsl_ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;jsl_ner&quot;)) .setOutputCol(&quot;jsl_ner_chunk&quot;) val jsl_ner_converter_internal = new medical.NerConverterInternal() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;jsl_ner&quot;)) .setOutputCol(&quot;replaced_ner_chunk&quot;) .setReplaceDictResource(&quot;replace_dict.csv&quot;,&quot;text&quot;, {&quot;delimiter&quot;:&quot;,&quot;}) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, word_embeddings, jsl_ner, jsl_ner_converter, jsl_ner_converter_internal )) val result = pipeline.fit(data).transform(data) from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetectorDLModel .pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.RoBertaEmbeddings .pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val fin_ner = finance.NerModel .pretrained(&quot;finner_deid&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new finance.NerConverterInternal() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setReplaceLabels({&quot;ORG&quot;: &quot;PARTY&quot;}) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, fin_ner, ner_converter )) val result = pipeline.fit(data).transform(data) from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetectorDLModel .pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.RoBertaEmbeddings .pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val legal_ner = legal.NerModel .pretrained(&quot;legner_contract_doc_parties&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new legal.NerConverterInternal() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setReplaceLabels({&quot;ALIAS&quot;: &quot;PARTY&quot;}) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, legal_ner, ner_converter )) val result = pipeline.fit(data).transform(data) NerDisambiguator ApproachModel Links words of interest, such as names of persons, locations and companies, from an input text document to a corresponding unique entity in a target Knowledge Base (KB). Words of interest are called Named Entities (NEs), mentions, or surface forms. The model needs extracted CHUNKS and SENTENCE_EMBEDDINGS type input from e.g. SentenceEmbeddings and NerConverter. Input Annotator Types: CHUNK, SENTENCE_EMBEDDINGS Output Annotator Type: DISAMBIGUATION Python API: NerDisambiguator Scala API: NerDisambiguator Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Extracting Person identities # First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. # Extracting Person identities # First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. data = spark.createDataFrame([[&quot;The show also had a contestant named Donald Trump who later defeated Christina Aguilera ...&quot;]]) .toDF(&quot;text&quot;) documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = nlp.WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) sentence_embeddings = nlp.SentenceEmbeddings() .setInputCols([&quot;sentence&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) ner_model = nlp.NerDLModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&quot;PER&quot;]) # Then the extracted entities can be disambiguated. disambiguator = medical.NerDisambiguator() .setS3KnowledgeBaseName(&quot;i-per&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;disambiguation&quot;) .setNumFirstChars(5) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, word_embeddings, sentence_embeddings, ner_model, ner_converter, disambiguator]) model = nlpPipeline.fit(data) result = model.transform(data) # Show results result.selectExpr(&quot;explode(disambiguation)&quot;) .selectExpr(&quot;col.metadata.chunk as chunk&quot;, &quot;col.result as result&quot;).show(5, False) +++ |chunk |result | +++ |Donald Trump |http:#en.wikipedia.org/?curid=4848272, http:#en.wikipedia.org/?curid=31698421, http:#en.wikipedia.org/?curid=55907961 | |Christina Aguilera|http:#en.wikipedia.org/?curid=144171, http:#en.wikipedia.org/?curid=6636454 | +++ from johnsnowlabs import * # Extracting Person identities # First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. # Extracting Person identities # First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = nlp.WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) sentence_embeddings = nlp.SentenceEmbeddings() .setInputCols([&quot;sentence&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&quot;PER&quot;]) # Then the extracted entities can be disambiguated. disambiguator = finance.NerDisambiguator() #.setS3KnowledgeBaseName(&quot;i-per&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;disambiguation&quot;) .setNumFirstChars(5) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, word_embeddings, sentence_embeddings, ner_model, ner_converter, disambiguator]) from johnsnowlabs import * # Extracting Person identities # First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. # Extracting Person identities # First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = nlp.WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) sentence_embeddings = nlp.SentenceEmbeddings() .setInputCols([&quot;sentence&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) ner_model = legal.NerModel.pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&quot;PER&quot;]) # Then the extracted entities can be disambiguated. disambiguator = legal.NerDisambiguator() #.setS3KnowledgeBaseName(&quot;i-per&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;disambiguation&quot;) .setNumFirstChars(5) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, word_embeddings, sentence_embeddings, ner_model, ner_converter, disambiguator]) MedicalFinanceLegal from johnsnowlabs import * // Extracting Person identities // First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. val data = Seq(&quot;The show also had a contestant named Donald Trump who later defeated Christina Aguilera ...&quot;) .toDF(&quot;text&quot;) val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val word_embeddings = nlp.WordEmbeddingsModel.pretrained() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val sentence_embeddings = new nlp.SentenceEmbeddings() .setInputCols(Array(&quot;sentence&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;sentence_embeddings&quot;) val ner_model = nlp.NerDLModel.pretrained() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList(&quot;PER&quot;) // Then the extracted entities can be disambiguated. val disambiguator = new medical.NerDisambiguator() .setS3KnowledgeBaseName(&quot;i-per&quot;) .setInputCols(Array(&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;)) .setOutputCol(&quot;disambiguation&quot;) .setNumFirstChars(5) val nlpPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, word_embeddings, sentence_embeddings, ner_model, ner_converter, disambiguator)) val model = nlpPipeline.fit(data) val result = model.transform(data) // Show results // // result.selectExpr(&quot;explode(disambiguation)&quot;) // .selectExpr(&quot;col.metadata.chunk as chunk&quot;, &quot;col.result as result&quot;).show(5, false) // +++ // |chunk |result | // +++ // |Donald Trump |http://en.wikipedia.org/?curid=4848272, http://en.wikipedia.org/?curid=31698421, http://en.wikipedia.org/?curid=55907961| // |Christina Aguilera|http://en.wikipedia.org/?curid=144171, http://en.wikipedia.org/?curid=6636454 | // +++ // from johnsnowlabs import * // Extracting Person identities // First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. val data = Seq(&quot;The show also had a contestant named Donald Trump who later defeated Christina Aguilera ...&quot;) .toDF(&quot;text&quot;) val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val word_embeddings = nlp.WordEmbeddingsModel.pretrained() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val sentence_embeddings = new nlp.SentenceEmbeddings() .setInputCols(Array(&quot;sentence&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;sentence_embeddings&quot;) val ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList(&quot;PER&quot;) // Then the extracted entities can be disambiguated. val disambiguator = new finance.NerDisambiguator() #.setS3KnowledgeBaseName(&quot;i-per&quot;) .setInputCols(Array(&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;)) .setOutputCol(&quot;disambiguation&quot;) .setNumFirstChars(5) val nlpPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, word_embeddings, sentence_embeddings, ner_model, ner_converter, disambiguator)) from johnsnowlabs import * // Extracting Person identities // First define pipeline stages that extract entities and embeddings. Entities are filtered for PER type entities. val data = Seq(&quot;The show also had a contestant named Donald Trump who later defeated Christina Aguilera ...&quot;) .toDF(&quot;text&quot;) val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val word_embeddings = nlp.WordEmbeddingsModel.pretrained() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val sentence_embeddings = new nlp.SentenceEmbeddings() .setInputCols(Array(&quot;sentence&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;sentence_embeddings&quot;) val ner_model = legal.NerModel.pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList(&quot;PER&quot;) // Then the extracted entities can be disambiguated. val disambiguator = new legal.NerDisambiguator() #.setS3KnowledgeBaseName(&quot;i-per&quot;) .setInputCols(Array(&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;)) .setOutputCol(&quot;disambiguation&quot;) .setNumFirstChars(5) val nlpPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, word_embeddings, sentence_embeddings, ner_model, ner_converter, disambiguator)) Links words of interest, such as names of persons, locations and companies, from an input text document to a corresponding unique entity in a target Knowledge Base (KB). Words of interest are called Named Entities (NEs), mentions, or surface forms. Instantiated / pretrained model of the NerDisambiguator. Links words of interest, such as names of persons, locations and companies, from an input text document to a corresponding unique entity in a target Knowledge Base (KB). Words of interest are called Named Entities (NEs), mentions, or surface forms. Input Annotator Types: CHUNK, SENTENCE_EMBEDDINGS Output Annotator Type: DISAMBIGUATION Python API: NerDisambiguatorModel Scala API: NerDisambiguatorModel NerModel ApproachModel This Named Entity recognition annotator allows to train generic NER model based on Neural Networks. The architecture of the neural network is a Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. For instantiated/pretrained models, see NerDLModel. The training data should be a labeled Spark Dataset, in the format of CoNLL 2003 IOB with Annotation type columns. The data should have columns of type DOCUMENT, TOKEN, WORD_EMBEDDINGS and an additional label column of annotator type NAMED_ENTITY. Excluding the label, this can be done with for example a SentenceDetector, a Tokenizer and a WordEmbeddingsModel with clinical embeddings (any clinical word embeddings can be chosen). For extended examples of usage, see the Spark NLP Workshop (sections starting with Training a Clinical NER) Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: MedicalNerApproach Scala API: MedicalNerApproach Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # First extract the prerequisites for the NerDLApproach documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) clinical_embeddings = nlp.WordEmbeddingsModel.pretrained(&#39;embeddings_clinical&#39;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Then the training can start nerTagger = medical.NerApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(2) .setBatchSize(64) .setRandomSeed(0) .setVerbose(1) .setValidationSplit(0.2) .setEvaluationLogExtended(True) .setEnableOutputLogs(True) .setIncludeConfidence(True) .setOutputLogsPath(&#39;ner_logs&#39;) .setGraphFolder(&#39;medical_ner_graphs&#39;) .setEnableMemoryOptimizer(True) #&gt;&gt; if you have a limited memory and a large conll file, you can set this True to train batch by batch pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, clinical_embeddings, nerTagger ]) # We use the text and labels from the CoNLL dataset conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) from johnsnowlabs import * # First extract the prerequisites for the NerDLApproach documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) clinical_embeddings = nlp.WordEmbeddingsModel.pretrained(&#39;embeddings_clinical&#39;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Then the training can start nerTagger = finance.NerApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(2) .setBatchSize(64) .setRandomSeed(0) .setVerbose(1) .setValidationSplit(0.2) .setEvaluationLogExtended(True) .setEnableOutputLogs(True) .setIncludeConfidence(True) .setOutputLogsPath(&#39;ner_logs&#39;) .setGraphFolder(&#39;medical_ner_graphs&#39;) .setEnableMemoryOptimizer(True) #&gt;&gt; if you have a limited memory and a large conll file, you can set this True to train batch by batch pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, clinical_embeddings, nerTagger ]) from johnsnowlabs import * # First extract the prerequisites for the NerDLApproach documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) clinical_embeddings = nlp.WordEmbeddingsModel.pretrained(&#39;embeddings_clinical&#39;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Then the training can start nerTagger = legal.NerApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(2) .setBatchSize(64) .setRandomSeed(0) .setVerbose(1) .setValidationSplit(0.2) .setEvaluationLogExtended(True) .setEnableOutputLogs(True) .setIncludeConfidence(True) .setOutputLogsPath(&#39;ner_logs&#39;) .setGraphFolder(&#39;medical_ner_graphs&#39;) .setEnableMemoryOptimizer(True) #&gt;&gt; if you have a limited memory and a large conll file, you can set this True to train batch by batch pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, clinical_embeddings, nerTagger ]) MedicalFinanceLegal from johnsnowlabs import * // First extract the prerequisites for the NerDLApproach val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel .pretrained(&#39;embeddings_clinical&#39;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger =new medical.NerApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(5) .setLr(0.003f) .setBatchSize(8) .setRandomSeed(0) .setVerbose(1) .setEvaluationLogExtended(false) .setEnableOutputLogs(false) .setIncludeConfidence(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) // We use the text and labels from the CoNLL dataset val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) from johnsnowlabs import * // First extract the prerequisites for the NerDLApproach val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel .pretrained(&#39;embeddings_clinical&#39;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger =new finance.NerApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(5) .setLr(0.003f) .setBatchSize(8) .setRandomSeed(0) .setVerbose(1) .setEvaluationLogExtended(false) .setEnableOutputLogs(false) .setIncludeConfidence(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) from johnsnowlabs import * // First extract the prerequisites for the NerDLApproach val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.WordEmbeddingsModel .pretrained(&#39;embeddings_clinical&#39;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger =new legal.NerApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(5) .setLr(0.003f) .setBatchSize(8) .setRandomSeed(0) .setVerbose(1) .setEvaluationLogExtended(false) .setEnableOutputLogs(false) .setIncludeConfidence(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) This Named Entity recognition annotator is a generic NER model based on Neural Networks. Pretrained models can be loaded with pretrained of the companion object: val nerModel = nlp.NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) The default model is &quot;ner_clinical&quot;, if no name is provided. For available pretrained models please see the Models Hub. Additionally, pretrained pipelines are available for this module, see Pipelines. Note that some pretrained models require specific types of embeddings, depending on which they were trained on. For example, the default model &quot;ner_dl&quot; requires the WordEmbeddings &quot;ner_clinical&quot;. For extended examples of usage, see the Spark NLP Workshop (sections starting with Training a Clinical NER) Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: MedicalNerModel Scala API: MedicalNerModel Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl_healthcare&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) jsl_ner = medical.NerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;jsl_ner&quot;) jsl_ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;jsl_ner&quot;]) .setOutputCol(&quot;jsl_ner_chunk&quot;) jsl_ner_pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, word_embeddings, jsl_ner, jsl_ner_converter]) result = jsl_ner_pipeline.fit(data).transform(data) from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_model = finance.NerModel.pretrained(&quot;finner_headers&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, ner_converter]) result = nlpPipeline.fit(data).transform(data) from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_model = legal.NerModel.pretrained(&quot;legner_headers&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, ner_converter]) result = nlpPipeline.fit(data).transform(data) MedicalFinanceLegal from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetectorDLModel .pretrained(&quot;sentence_detector_dl_healthcare&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val word_embeddings = nlp.WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val jsl_ner = medical.NerModel .pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;jsl_ner&quot;) val jsl_ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;jsl_ner&quot;)) .setOutputCol(&quot;jsl_ner_chunk&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, word_embeddings, jsl_ner, jsl_ner_converter )) val result = pipeline.fit(data).transform(data) from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetectorDLModel .pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.BertEmbeddings .pretrained(&quot;bert_embeddings_sec_bert_base&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val ner_model = finance.NerModel .pretrained(&quot;finner_headers&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, ner_converter )) val result = pipeline.fit(data).transform(data) from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetectorDLModel .pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.RoBertaEmbeddings .pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val ner_model = legal.NerModel .pretrained(&quot;legner_headers&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, ner_converter )) val result = pipeline.fit(data).transform(data) RENerChunksFilter Filters entities’ dependency relations. The annotator filters desired relation pairs (defined by the parameter realtionPairs), and store those on the output column. Filtering the possible relations can be useful to perform additional analysis for a specific use case (e.g., checking adverse drug reactions and drug realations), which can be the input for further analysis using a pretrained RelationExtractionDLModel. For example, the ner_clinical NER model can identify PROBLEM, TEST, and TREATMENT entities. By using the RENerChunksFilter, one can filter only the relations between PROBLEM and TREATMENT entities only, removing any relation between the other entities, to further analyze the associations between clinical problems and treatments. Input Annotator Types: CHUNK, DEPENDENCY Output Annotator Type: CHUNK Python API: RENerChunksFilter Scala API: RENerChunksFilter Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Define pipeline stages to extract entities documenter = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencer = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;tokens&quot;) words_embedder = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) pos_tagger = nlp.PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;pos_tags&quot;) dependency_parser = nlp.DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) clinical_ner_tagger = medical.NerModel.pretrained(&quot;jsl_ner_wip_greedy_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) ner_chunker = nlp.NerConverter() .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;]) .setOutputCol(&quot;ner_chunks&quot;) # Define the relation pairs and the filter relationPairs = [ &quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot; ] re_ner_chunk_filter = medical.RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs([&quot;internal_organ_or_component-direction&quot;]) trained_pipeline = Pipeline(stages=[ documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_ner_chunk_filter ]) data = spark.createDataFrame([[&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;]]).toDF(&quot;text&quot;) result = trained_pipeline.fit(data).transform(data) # Show results result.selectExpr(&quot;explode(re_ner_chunks) as re_chunks&quot;) .selectExpr(&quot;re_chunks.begin&quot;, &quot;re_chunks.result&quot;, &quot;re_chunks.metadata.entity&quot;, &quot;re_chunks.metadata.paired_to&quot;) .show(6, truncate=False) +--+-+++ |begin|result |entity |paired_to| +--+-+++ |35 |upper |Direction |41 | |41 |brain stem |Internal_organ_or_component|35 | |35 |upper |Direction |59 | |59 |cerebellum |Internal_organ_or_component|35 | |35 |upper |Direction |81 | |81 |basil ganglia|Internal_organ_or_component|35 | +--+-+++ from johnsnowlabs import * # Define pipeline stages to extract entities documenter = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencer = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;tokens&quot;) words_embedder = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) pos_tagger = nlp.PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;pos_tags&quot;) dependency_parser = nlp.DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_chunker = nlp.NerConverter() .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunks&quot;) # Define the relation pairs and the filter relationPairs = [ &quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot; ] re_ner_chunk_filter = finance.RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs([&quot;internal_organ_or_component-direction&quot;]) trained_pipeline = Pipeline(stages=[ documenter, sentencer, tokenizer, words_embedder, pos_tagger, dependency_parser, ner_model, ner_chunker, re_ner_chunk_filter ]) from johnsnowlabs import * # Define pipeline stages to extract entities documenter = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencer = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;tokens&quot;) words_embedder = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) pos_tagger = nlp.PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;pos_tags&quot;) dependency_parser = nlp.DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) ner_model = legal.NerModel.pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embedding&quot;]) .setOutputCol(&quot;ner&quot;) ner_chunker = nlp.NerConverter() .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunks&quot;) # Define the relation pairs and the filter relationPairs = [ &quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot; ] re_ner_chunk_filter = legal.RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs([&quot;internal_organ_or_component-direction&quot;]) trained_pipeline = Pipeline(stages=[ documenter, sentencer, tokenizer, words_embedder, pos_tagger, dependency_parser, ner_model, ner_chunker, re_ner_chunk_filter ]) MedicalFinanceLegal from johnsnowlabs import * // Define pipeline stages to extract entities val documenter = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentencer = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;tokens&quot;) val words_embedder = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;embeddings&quot;) val pos_tagger = nlp.PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;pos_tags&quot;) val dependency_parser = nlp.DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;dependencies&quot;) val clinical_ner_tagger = medical.NerModel.pretrained(&quot;jsl_ner_wip_greedy_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner_tags&quot;) val ner_chunker = new nlp.NerConverter() .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;)) .setOutputCol(&quot;ner_chunks&quot;) // Define the relation pairs and the filter val relationPairs = Array(&quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot;) val re_ner_chunk_filter = new medical.RENerChunksFilter() .setInputCols(Array(&quot;ner_chunks&quot;, &quot;dependencies&quot;)) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs(Array(&quot;internal_organ_or_component-direction&quot;)) val trained_pipeline = new Pipeline().setStages(Array( documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_ner_chunk_filter )) val data = Seq(&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;).toDF(&quot;text&quot;) val result = trained_pipeline.fit(data).transform(data) // Show results // // result.selectExpr(&quot;explode(re_ner_chunks) as re_chunks&quot;) // .selectExpr(&quot;re_chunks.begin&quot;, &quot;re_chunks.result&quot;, &quot;re_chunks.metadata.entity&quot;, &quot;re_chunks.metadata.paired_to&quot;) // .show(6, truncate=false) // +--+-+++ // |begin|result |entity |paired_to| // +--+-+++ // |35 |upper |Direction |41 | // |41 |brain stem |Internal_organ_or_component|35 | // |35 |upper |Direction |59 | // |59 |cerebellum |Internal_organ_or_component|35 | // |35 |upper |Direction |81 | // |81 |basil ganglia|Internal_organ_or_component|35 | // +--+-+++ // from johnsnowlabs import * // Define pipeline stages to extract entities val documenter = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentencer = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;tokens&quot;) val words_embedder = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;embeddings&quot;) val pos_tagger = nlp.PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;pos_tags&quot;) val dependency_parser = nlp.DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;dependencies&quot;) val ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_chunker = new nlp.NerConverter() .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunks&quot;) // Define the relation pairs and the filter val relationPairs = Array(&quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot;) val re_ner_chunk_filter = new finance.RENerChunksFilter() .setInputCols(Array(&quot;ner_chunks&quot;, &quot;dependencies&quot;)) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs(Array(&quot;internal_organ_or_component-direction&quot;)) val trained_pipeline = new Pipeline().setStages(Array( documenter, sentencer, tokenizer, words_embedder, pos_tagger, dependency_parser, ner_model, ner_chunker, re_ner_chunk_filter )) from johnsnowlabs import * // Define pipeline stages to extract entities val documenter = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentencer = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;tokens&quot;) val words_embedder = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;embeddings&quot;) val pos_tagger = nlp.PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;pos_tags&quot;) val dependency_parser = nlp.DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;dependencies&quot;) val ner_model = legal.NerModel.pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embedding&quot;)) .setOutputCol(&quot;ner&quot;) val ner_chunker = new nlp.NerConverter() .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunks&quot;) // Define the relation pairs and the filter val relationPairs = Array(&quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot;) val re_ner_chunk_filter = new legal.RENerChunksFilter() .setInputCols(Array(&quot;ner_chunks&quot;, &quot;dependencies&quot;)) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs(Array(&quot;internal_organ_or_component-direction&quot;)) val trained_pipeline = new Pipeline().setStages(Array( documenter, sentencer, tokenizer, words_embedder, pos_tagger, dependency_parser, ner_model, ner_chunker, re_ner_chunk_filter )) ReIdentification Reidentifies obfuscated entities by DeIdentification. This annotator requires the outputs from the deidentification as input. Input columns need to be the deidentified document and the deidentification mappings set with DeIdentification.setMappingsColumn. To see how the entities are deidentified, please refer to the example of that class. Input Annotator Types: DOCUMENT,CHUNK Output Annotator Type: DOCUMENT Python API: ReIdentification Scala API: ReIdentification Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Define the reidentification stage and transform the deidentified documents reideintification = medical.ReIdentification() .setInputCols([&quot;dei&quot;, &quot;protectedEntities&quot;]) .setOutputCol(&quot;reid&quot;) .transform(result) # Show results result.select(&quot;dei.result&quot;).show(truncate = False) +--+ |result | +--+ |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , &lt;AGE&gt; years-old , Record date : 2079-11-14.]| +--+ reideintification.selectExpr(&quot;explode(reid.result)&quot;).show(truncate=False) +--+ |col | +--+ |# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09.| +--+ from johnsnowlabs import * # Define the reidentification stage and transform the deidentified documents reideintification = finance.ReIdentification() .setInputCols([&quot;aux&quot;, &quot;deidentified&quot;]) .setOutputCol(&quot;original&quot;) .transform(result) from johnsnowlabs import * # Define the reidentification stage and transform the deidentified documents reideintification = legal.ReIdentification() .setInputCols([&quot;aux&quot;, &quot;deidentified&quot;]) .setOutputCol(&quot;original&quot;) .transform(result) MedicalFinanceLegal from johnsnowlabs import * // Define the reidentification stage and transform the deidentified documents val reideintification = new medical.ReIdentification() .setInputCols(Array(&quot;dei&quot;, &quot;protectedEntities&quot;)) .setOutputCol(&quot;reid&quot;) .transform(result) // Show results // // result.select(&quot;dei.result&quot;).show(truncate = false) // +--+ // |result | // +--+ // |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , &lt;AGE&gt; years-old , Record date : 2079-11-14.]| // +--+ // reideintification.selectExpr(&quot;explode(reid.result)&quot;).show(false) // +--+ // |col | // +--+ // |# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09.| // +--+ // from johnsnowlabs import * // Define the reidentification stage and transform the deidentified documents val reideintification = new finance.ReIdentification() .setInputCols(Array(&quot;aux&quot;, &quot;deidentified&quot;)) .setOutputCol(&quot;original&quot;) .transform(result) from johnsnowlabs import * // Define the reidentification stage and transform the deidentified documents val reideintification = new legal.ReIdentification() .setInputCols(Array(&quot;aux&quot;, &quot;deidentified&quot;)) .setOutputCol(&quot;original&quot;) .transform(result) RelationExtraction ApproachModel Trains a TensorFlow model for relation extraction. To train a custom relation extraction model, you need to first creat a Tensorflow graph using either the TfGraphBuilder annotator or the tf_graph module. Then, set the path to the Tensorflow graph using the method .setModelFile(&quot;path/to/tensorflow_graph.pb&quot;). If the parameter relationDirectionCol is set, the model will be trained using the direction information (see the parameter decription for details). Otherwise, the model won’t have direction between the relation of the entities. After training a model (using the .fit() method), the resulting object is of class RelationExtractionModel. Input Annotator Types: WORD_EMBEDDINGS, POS, CHUNK, DEPENDENCY Output Annotator Type: NONE Python API: RelationExtractionApproach Scala API: RelationExtractionApproach Show Example PythonScala Medical from johnsnowlabs import * # Defining pipeline stages to extract entities first documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;tokens&quot;) embedder = nlp.WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) posTagger = nlp.PerceptronModel .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;posTags&quot;) nerTagger = nlp.MedicalNerModel .pretrained(&quot;ner_events_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) nerConverter = nlp.NerConverter() .setInputCols([&quot;document&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;]) .setOutputCol(&quot;nerChunks&quot;) depencyParser = nlp.DependencyParserModel .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;document&quot;, &quot;posTags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) # Then define `RelationExtractionApproach` and training parameters re = medical.RelationExtractionApproach() .setInputCols([&quot;embeddings&quot;, &quot;posTags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations_t&quot;) .setLabelColumn(&quot;target_rel&quot;) .setEpochsNumber(300) .setBatchSize(200) .setLearningRate(0.001) .setModelFile(&quot;path/to/graph_file.pb&quot;) .setFixImbalance(True) .setValidationSplit(0.05) .setFromEntity(&quot;from_begin&quot;, &quot;from_end&quot;, &quot;from_label&quot;) .setToEntity(&quot;to_begin&quot;, &quot;to_end&quot;, &quot;to_label&quot;) finisher = nlp.Finisher() .setInputCols([&quot;relations_t&quot;]) .setOutputCols([&quot;relations&quot;]) .setCleanAnnotations(False) .setValueSplitSymbol(&quot;,&quot;) .setAnnotationSplitSymbol(&quot;,&quot;) .setOutputAsArray(False) # Define complete pipeline and start training pipeline = Pipeline(stages=[ documentAssembler, tokenizer, embedder, posTagger, nerTagger, nerConverter, depencyParser, re, finisher]) model = pipeline.fit(trainData) Medical from johnsnowlabs import * // Defining pipeline stages to extract entities first val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;tokens&quot;) val embedder = nlp.WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;embeddings&quot;) val posTagger = nlp.PerceptronModel .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;posTags&quot;) val nerTagger = medical.NerModel .pretrained(&quot;ner_events_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner_tags&quot;) val nerConverter = new nlp.NerConverter() .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;)) .setOutputCol(&quot;nerChunks&quot;) val depencyParser = nlp.DependencyParserModel .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(Array(&quot;document&quot;, &quot;posTags&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;dependencies&quot;) // Then define `RelationExtractionApproach` and training parameters val re = new medical.RelationExtractionApproach() .setInputCols(Array(&quot;embeddings&quot;, &quot;posTags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;)) .setOutputCol(&quot;relations_t&quot;) .setLabelColumn(&quot;target_rel&quot;) .setEpochsNumber(300) .setBatchSize(200) .setlearningRate(0.001f) .setModelFile(&quot;path/to/graph_file.pb&quot;) .setFixImbalance(true) .setValidationSplit(0.05f) .setFromEntity(&quot;from_begin&quot;, &quot;from_end&quot;, &quot;from_label&quot;) .setToEntity(&quot;to_begin&quot;, &quot;to_end&quot;, &quot;to_label&quot;) val finisher = new nlp.Finisher() .setInputCols(Array(&quot;relations_t&quot;)) .setOutputCols(Array(&quot;relations&quot;)) .setCleanAnnotations(false) .setValueSplitSymbol(&quot;,&quot;) .setAnnotationSplitSymbol(&quot;,&quot;) .setOutputAsArray(false) // Define complete pipeline and start training val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embedder, posTagger, nerTagger, nerConverter, depencyParser, re, finisher)) val model = pipeline.fit(trainData) Extracts and classifies instances of relations between named entities. For pretrained models please see the Models Hub for available models. Input Annotator Types: WORD_EMBEDDINGS, POS, CHUNK, DEPENDENCY Output Annotator Type: CATEGORY Python API: RelationExtractionModel Scala API: RelationExtractionModel Show Example PythonScala Medical from johnsnowlabs import * # Relation Extraction between body parts # Define pipeline stages to extract entities documenter = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencer = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;tokens&quot;) words_embedder = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) pos_tagger = nlp.PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;pos_tags&quot;) dependency_parser = nlp.DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) clinical_ner_tagger = medical.NerModel.pretrained(&quot;jsl_ner_wip_greedy_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) ner_chunker = nlp.NerConverter() .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;]) .setOutputCol(&quot;ner_chunks&quot;) # Define the relations that are to be extracted relationPairs = [ &quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot; ] re_model = medical.RelationExtractionModel.pretrained(&quot;re_bodypart_directions&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setRelationPairs(relationPairs) .setMaxSyntacticDistance(4) .setPredictionThreshold(0.9) pipeline = Pipeline().setStages([ documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_model ]) data = spark.createDataFrame([[&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) # Show results # result.selectExpr(&quot;explode(relations) as relations&quot;) .select( &quot;relations.metadata.chunk1&quot;, &quot;relations.metadata.entity1&quot;, &quot;relations.metadata.chunk2&quot;, &quot;relations.metadata.entity2&quot;, &quot;relations.result&quot; ) .where(&quot;result != 0&quot;) .show(truncate=False) # Show results result.selectExpr(&quot;explode(relations) as relations&quot;) .select( &quot;relations.metadata.chunk1&quot;, &quot;relations.metadata.entity1&quot;, &quot;relations.metadata.chunk2&quot;, &quot;relations.metadata.entity2&quot;, &quot;relations.result&quot; ).where(&quot;result != 0&quot;) .show(truncate=False) +++-+++ |chunk1|entity1 |chunk2 |entity2 |result| +++-+++ |upper |Direction|brain stem |Internal_organ_or_component|1 | |left |Direction|cerebellum |Internal_organ_or_component|1 | |right |Direction|basil ganglia|Internal_organ_or_component|1 | +++-+++ Medical from johnsnowlabs import * // Relation Extraction between body parts // Define pipeline stages to extract entities val documenter = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentencer = new nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;tokens&quot;) val words_embedder = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;embeddings&quot;) val pos_tagger = nlp.PerceptronModel.pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;pos_tags&quot;) val dependency_parser = nlp.DependencyParserModel.pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;dependencies&quot;) val clinical_ner_tagger = medical.NerModel.pretrained(&quot;jsl_ner_wip_greedy_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner_tags&quot;) val ner_chunker = new nlp.NerConverter() .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;)) .setOutputCol(&quot;ner_chunks&quot;) // Define the relations that are to be extracted val relationPairs = Array(&quot;direction-external_body_part_or_region&quot;, &quot;external_body_part_or_region-direction&quot;, &quot;direction-internal_organ_or_component&quot;, &quot;internal_organ_or_component-direction&quot;) val re_model = medical.RelationExtractionModel.pretrained(&quot;re_bodypart_directions&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;)) .setOutputCol(&quot;relations&quot;) .setRelationPairs(relationPairs) .setMaxSyntacticDistance(4) .setPredictionThreshold(0.9f) val pipeline = new Pipeline().setStages(Array( documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_model )) val data = Seq(&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) // Show results // // result.selectExpr(&quot;explode(relations) as relations&quot;) // .select( // &quot;relations.metadata.chunk1&quot;, // &quot;relations.metadata.entity1&quot;, // &quot;relations.metadata.chunk2&quot;, // &quot;relations.metadata.entity2&quot;, // &quot;relations.result&quot; // ) // .where(&quot;result != 0&quot;) // .show(truncate=false) // +++-+++ // |chunk1|entity1 |chunk2 |entity2 |result| // +++-+++ // |upper |Direction|brain stem |Internal_organ_or_component|1 | // |left |Direction|cerebellum |Internal_organ_or_component|1 | // |right |Direction|basil ganglia|Internal_organ_or_component|1 | // +++-+++ // RelationExtractionDL Extracts and classifies instances of relations between named entities. In contrast with RelationExtractionModel, RelationExtractionDLModel is based on BERT. For pretrained models please see the Models Hub for available models. Input Annotator Types: CHUNK, DOCUMENT Output Annotator Type: CATEGORY Python API: RelationExtractionDLModel Scala API: RelationExtractionDLModel Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Relation Extraction between body parts # This is a continuation of the RENerChunksFilter example. See that class on how to extract the relation chunks. # Define the extraction model re_ner_chunk_filter = medical.RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs([&quot;internal_organ_or_component-direction&quot;]) re_model = medical.RelationExtractionDLModel.pretrained(&quot;redl_bodypart_direction_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setPredictionThreshold(0.5) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) trained_pipeline = Pipeline(stages=[ documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_ner_chunk_filter, re_model ]) data = spark.createDataFrame([[&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;]]).toDF(&quot;text&quot;) result = trained_pipeline.fit(data).transform(data) # Show results result.selectExpr(&quot;explode(relations) as relations&quot;) .select( &quot;relations.metadata.chunk1&quot;, &quot;relations.metadata.entity1&quot;, &quot;relations.metadata.chunk2&quot;, &quot;relations.metadata.entity2&quot;, &quot;relations.result&quot; ) .where(&quot;result != 0&quot;) .show(truncate=False) +++-+++ |chunk1|entity1 |chunk2 |entity2 |result| +++-+++ |upper |Direction|brain stem |Internal_organ_or_component|1 | |left |Direction|cerebellum |Internal_organ_or_component|1 | |right |Direction|basil ganglia|Internal_organ_or_component|1 | +++-+++ from johnsnowlabs import * document_assembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;xx&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;,&quot;en&quot;,&quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_org&quot;) ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner_org&quot;]) .setOutputCol(&quot;ner_chunk_org&quot;) token_classifier = nlp.DeBertaForTokenClassification.pretrained(&quot;deberta_v3_base_token_classifier_ontonotes&quot;, &quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;ner_date&quot;) .setCaseSensitive(True) .setMaxSentenceLength(512) ner_converter_date = nlp.NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner_date&quot;]) .setOutputCol(&quot;ner_chunk_date&quot;) .setWhiteList([&quot;DATE&quot;]) chunk_merger = finance.ChunkMergeApproach() .setInputCols(&quot;ner_chunk_org&quot;, &quot;ner_chunk_date&quot;) .setOutputCol(&#39;ner_chunk&#39;) re_model = finance.RelationExtractionDLModel.pretrained(&quot;finre_acquisitions_subsidiaries&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setPredictionThreshold(0.3) .setInputCols([&quot;ner_chunk&quot;, &quot;document&quot;]) .setOutputCol(&quot;relations&quot;) pipeline = Pipeline(stages=[ document_assembler, sentence_detector, tokenizer, embeddings, ner_model, ner_converter, token_classifier, ner_converter_date, chunk_merger, re_model ]) result = pipeline.fit(data).transform(data) from johnsnowlabs import * document_assembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;xx&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.RoBertaEmbeddings.pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setMaxSentenceLength(512) ner_model = legal.NerModel.pretrained(&quot;legner_contract_doc_parties&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) re_model = legal.RelationExtractionDLModel.pretrained(&quot;legre_contract_doc_parties&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setPredictionThreshold(0.5) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;relations&quot;) pipeline = Pipeline(stages=[ document_assembler, sentence_detector, tokenizer, embeddings, ner_model, ner_converter, re_model ]) result = pipeline.fit(data).transform(data) MedicalFinanceLegal from johnsnowlabs import * // Relation Extraction between body parts // This is a continuation of the [[RENerChunksFilter]] example. See that class on how to extract the relation chunks. // Define the extraction model val re_ner_chunk_filter = new medical.RENerChunksFilter() .setInputCols(&quot;ner_chunks&quot;, &quot;dependencies&quot;) .setOutputCol(&quot;re_ner_chunks&quot;) .setMaxSyntacticDistance(4) .setRelationPairs(Array(&quot;internal_organ_or_component-direction&quot;)) val re_model = medical.RelationExtractionDLModel.pretrained(&quot;redl_bodypart_direction_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setPredictionThreshold(0.5f) .setInputCols(&quot;re_ner_chunks&quot;, &quot;sentences&quot;) .setOutputCol(&quot;relations&quot;) val trained_pipeline = new Pipeline().setStages(Array( documenter, sentencer, tokenizer, words_embedder, pos_tagger, clinical_ner_tagger, ner_chunker, dependency_parser, re_ner_chunk_filter, re_model )) val data = Seq(&quot;MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia&quot;).toDF(&quot;text&quot;) val result = trained_pipeline.fit(data).transform(data) // Show results // // result.selectExpr(&quot;explode(relations) as relations&quot;) // .select( // &quot;relations.metadata.chunk1&quot;, // &quot;relations.metadata.entity1&quot;, // &quot;relations.metadata.chunk2&quot;, // &quot;relations.metadata.entity2&quot;, // &quot;relations.result&quot; // ) // .where(&quot;result != 0&quot;) // .show(truncate=false) // +++-+++ // |chunk1|entity1 |chunk2 |entity2 |result| // +++-+++ // |upper |Direction|brain stem |Internal_organ_or_component|1 | // |left |Direction|cerebellum |Internal_organ_or_component|1 | // |right |Direction|basil ganglia|Internal_organ_or_component|1 | // +++-+++ // from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetectorDLModel .pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.BertEmbeddings .pretrained(&quot;bert_embeddings_sec_bert_base&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val ner_model = finance.NerModel .pretrained(&quot;finner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;ner_org&quot;) val ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner_org&quot;)) .setOutputCol(&quot;ner_chunk_org&quot;) val token_classifier = nlp.DeBertaForTokenClassification .pretrained(&quot;deberta_v3_base_token_classifier_ontonotes&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;ner_date&quot;) .setCaseSensitive(True) .setMaxSentenceLength(512) val ner_converter_date = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner_date&quot;)) .setOutputCol(&quot;ner_chunk_date&quot;) .setWhiteList(Array(&quot;DATE&quot;)) val chunk_merger = new finance.ChunkMergeApproach() .setInputCols(&quot;ner_chunk_org&quot;, &quot;ner_chunk_date&quot;) .setOutputCol(&#39;ner_chunk&#39;) val re_model = finance.RelationExtractionDLModel .pretrained(&quot;finre_acquisitions_subsidiaries&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setPredictionThreshold(0.3) .setInputCols(Array(&quot;ner_chunk&quot;, &quot;document&quot;)) .setOutputCol(&quot;relations&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, ner_converter, token_classifier, ner_converter_date, chunk_merger, re_model )) val result = pipeline.fit(data).transform(data) from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetectorDLModel .pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.RoBertaEmbeddings .pretrained(&quot;roberta_embeddings_legal_roberta_base&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) .setMaxSentenceLength(512) val ner_model = legal.NerModel .pretrained(&quot;legner_contract_doc_parties&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) val re_model = legal.RelationExtractionDLModel .pretrained(&quot;legre_contract_doc_parties&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setPredictionThreshold(0.5) .setInputCols(Array(&quot;ner_chunk&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;relations&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, ner_converter, re_model )) val result = pipeline.fit(data).transform(data) SentenceEntityResolver ApproachModel Contains all the parameters and methods to train a SentenceEntityResolverModel. The model transforms a dataset with Input Annotation type SENTENCE_EMBEDDINGS, coming from e.g. BertSentenceEmbeddings and returns the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.) To use pretrained models please use SentenceEntityResolverModel and see the Models Hub for available models. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: ENTITY Python API: SentenceEntityResolverApproach Scala API: SentenceEntityResolverApproach Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Training a SNOMED resolution model using BERT sentence embeddings # Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) bertEmbeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;bert_embeddings&quot;) snomedTrainingPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, bertEmbeddings ]) snomedTrainingModel = snomedTrainingPipeline.fit(data) snomedData = snomedTrainingModel.transform(data).cache() # Then the Resolver can be trained with bertExtractor = medical.SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols([&quot;bert_embeddings&quot;]) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(False) snomedModel = bertExtractor.fit(snomedData) from johnsnowlabs import * # Training a SNOMED resolution model using BERT sentence embeddings # Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) bertEmbeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;bert_embeddings&quot;) snomedTrainingPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, bertEmbeddings ]) snomedTrainingModel = snomedTrainingPipeline.fit(data) snomedData = snomedTrainingModel.transform(data).cache() # Then the Resolver can be trained with bertExtractor = finance.SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols([&quot;bert_embeddings&quot;]) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(False) snomedModel = bertExtractor.fit(snomedData) from johnsnowlabs import * # Training a SNOMED resolution model using BERT sentence embeddings # Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) bertEmbeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;bert_embeddings&quot;) snomedTrainingPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, bertEmbeddings ]) snomedTrainingModel = snomedTrainingPipeline.fit(data) snomedData = snomedTrainingModel.transform(data).cache() # Then the Resolver can be trained with bertExtractor = legal.SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols([&quot;bert_embeddings&quot;]) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(False) snomedModel = bertExtractor.fit(snomedData) MedicalFinanceLegal from johnsnowlabs import * // Training a SNOMED resolution model using BERT sentence embeddings // Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val bertEmbeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;bert_embeddings&quot;) val snomedTrainingPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, bertEmbeddings )) val snomedTrainingModel = snomedTrainingPipeline.fit(data) val snomedData = snomedTrainingModel.transform(data).cache() // Then the Resolver can be trained with val bertExtractor = new medical.SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols(&quot;bert_embeddings&quot;) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(false) val snomedModel = bertExtractor.fit(snomedData) from johnsnowlabs import * // Training a SNOMED resolution model using BERT sentence embeddings // Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val bertEmbeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;bert_embeddings&quot;) val snomedTrainingPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, bertEmbeddings )) val snomedTrainingModel = snomedTrainingPipeline.fit(data) val snomedData = snomedTrainingModel.transform(data).cache() // Then the Resolver can be trained with val bertExtractor = new finance.SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols(&quot;bert_embeddings&quot;) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(false) val snomedModel = bertExtractor.fit(snomedData) from johnsnowlabs import * // Training a SNOMED resolution model using BERT sentence embeddings // Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val bertEmbeddings = nlp.BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;bert_embeddings&quot;) val snomedTrainingPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, bertEmbeddings )) val snomedTrainingModel = snomedTrainingPipeline.fit(data) val snomedData = snomedTrainingModel.transform(data).cache() // Then the Resolver can be trained with val bertExtractor = new legal.SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols(&quot;bert_embeddings&quot;) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(false) val snomedModel = bertExtractor.fit(snomedData) The model transforms a dataset with Input Annotation type SENTENCE_EMBEDDINGS, coming from e.g. BertSentenceEmbeddings and returns the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.) To use pretrained models please see the Models Hub for available models. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: ENTITY Python API: SentenceEntityResolverModel Scala API: SentenceEntityResolverModel Show Example PythonScala MedicalFinanceLegal from johnsnowlabs import * # Resolving CPT # First define pipeline stages to extract entities documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetectorDLModel.pretrained() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) clinical_ner = medical.NerModel.pretrained(&quot;jsl_ner_wip_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&quot;Test&quot;,&quot;Procedure&quot;]) c2doc = nlp.Chunk2Doc() .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;ner_chunk_doc&quot;) sbert_embedder = nlp.BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) # Then the resolver is defined on the extracted entities and sentence embeddings cpt_resolver = medical.SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_cpt_procedures_augmented&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;cpt_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) sbert_pipeline_cpt = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, ner_converter, c2doc, sbert_embedder, cpt_resolver]) sbert_outputs = sbert_pipeline_cpt.fit(data_ner).transform(data) # Show results # # sbert_outputs # .select(&quot;explode(arrays_zip(ner_chunk.result ,ner_chunk.metadata, cpt_code.result, cpt_code.metadata, ner_chunk.begin, ner_chunk.end)) as cpt_code&quot;) # .selectExpr( # &quot;cpt_code[&#39;0&#39;] as chunk&quot;, # &quot;cpt_code[&#39;1&#39;].entity as entity&quot;, # &quot;cpt_code[&#39;2&#39;] as code&quot;, # &quot;cpt_code[&#39;3&#39;].confidence as confidence&quot;, # &quot;cpt_code[&#39;3&#39;].all_k_resolutions as all_k_resolutions&quot;, # &quot;cpt_code[&#39;3&#39;].all_k_results as all_k_results&quot; # ).show(5) # +--++--+-+--+--+ # | chunk| entity| code|confidence| all_k_resolutions| all_k_codes| # +--++--+-+--+--+ # | heart cath|Procedure|93566| 0.1180|CCA - Cardiac cat...|93566:::62319:::9...| # |selective coronar...| Test|93460| 0.1000|Coronary angiogra...|93460:::93458:::9...| # |common femoral an...| Test|35884| 0.1808|Femoral artery by...|35884:::35883:::3...| # | StarClose closure|Procedure|33305| 0.1197|Heart closure:::H...|33305:::33300:::3...| # | stress test| Test|93351| 0.2795|Cardiovascular st...|93351:::94621:::9...| # +--++--+-+--+--+ # from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_model = finance.NerModel.pretrained(&quot;finner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) chunk2doc = nlp.Chunk2Doc() .setInputCols(&quot;ner_chunk&quot;) .setOutputCol(&quot;ner_chunk_doc&quot;) sentence_embeddings = nlp.UniversalSentenceEncoder.pretrained(&quot;tfhub_use&quot;, &quot;en&quot;) .setInputCols(&quot;ner_chunk_doc&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) resolver = finance.SentenceEntityResolverModel.pretrained(&quot;finel_edgar_company_name&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols([&quot;text&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, ner_converter, chunk2doc, sentence_embeddings, resolver ]) result = pipeline.fit(data).transform(data) from johnsnowlabs import * documentAssembler = nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;,&quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_model = legal.NerModel.pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) chunk2doc = nlp.Chunk2Doc() .setInputCols(&quot;ner_chunk&quot;) .setOutputCol(&quot;ner_chunk_doc&quot;) sentence_embeddings = nlp.UniversalSentenceEncoder.pretrained(&quot;tfhub_use&quot;, &quot;en&quot;) .setInputCols(&quot;ner_chunk_doc&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) resolver = legal.SentenceEntityResolverModel.pretrained(&quot;legel_edgar_company_name&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;text&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, ner_converter, chunk2doc, sentence_embeddings, resolver ]) result = pipeline.fit(data).transform(data) MedicalFinanceLegal from johnsnowlabs import * // Resolving CPT // First define pipeline stages to extract entities val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetectorDLModel.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val word_embeddings = nlp.WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val clinical_ner = medical.NerModel.pretrained(&quot;jsl_ner_wip_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new nlp.NerConverter() .setInputCols(array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList(&quot;Test&quot;,&quot;Procedure&quot;) val c2doc = new nlp.Chunk2Doc() .setInputCols(&quot;ner_chunk&quot;) .setOutputCol(&quot;ner_chunk_doc&quot;) val sbert_embedder = nlp.BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;ner_chunk_doc&quot;) .setOutputCol(&quot;sbert_embeddings&quot;) // Then the resolver is defined on the extracted entities and sentence embeddings val cpt_resolver = medical.SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_cpt_procedures_augmented&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;)) .setOutputCol(&quot;cpt_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) val sbert_pipeline_cpt = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, ner_converter, c2doc, sbert_embedder, cpt_resolver)) // Show results // // sbert_outputs // .select(&quot;explode(arrays_zip(ner_chunk.result ,ner_chunk.metadata, cpt_code.result, cpt_code.metadata, ner_chunk.begin, ner_chunk.end)) as cpt_code&quot;) // .selectExpr( // &quot;cpt_code[&#39;0&#39;] as chunk&quot;, // &quot;cpt_code[&#39;1&#39;].entity as entity&quot;, // &quot;cpt_code[&#39;2&#39;] as code&quot;, // &quot;cpt_code[&#39;3&#39;].confidence as confidence&quot;, // &quot;cpt_code[&#39;3&#39;].all_k_resolutions as all_k_resolutions&quot;, // &quot;cpt_code[&#39;3&#39;].all_k_results as all_k_results&quot; // ).show(5) // +--++--+-+--+--+ // | chunk| entity| code|confidence| all_k_resolutions| all_k_codes| // +--++--+-+--+--+ // | heart cath|Procedure|93566| 0.1180|CCA - Cardiac cat...|93566:::62319:::9...| // |selective coronar...| Test|93460| 0.1000|Coronary angiogra...|93460:::93458:::9...| // |common femoral an...| Test|35884| 0.1808|Femoral artery by...|35884:::35883:::3...| // | StarClose closure|Procedure|33305| 0.1197|Heart closure:::H...|33305:::33300:::3...| // | stress test| Test|93351| 0.2795|Cardiovascular st...|93351:::94621:::9...| // +--++--+-+--+--+ // from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetectorDLModel .pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.BertEmbeddings .pretrained(&quot;bert_embeddings_sec_bert_base&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val ner_model = finance.NerModel .pretrained(&quot;finner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) val chunk2doc = new nlp.Chunk2Doc() .setInputCols(&quot;ner_chunk&quot;) .setOutputCol(&quot;ner_chunk_doc&quot;) val sentence_embeddings = nlp.UniversalSentenceEncoder .pretrained(&quot;tfhub_use&quot;, &quot;en&quot;) .setInputCols(&quot;ner_chunk_doc&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val resolver = finance.SentenceEntityResolverModel .pretrained(&quot;finel_edgar_company_name&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols(Array(&quot;text&quot;, &quot;sentence_embeddings&quot;)) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, ner_converter, chunk2doc, sentence_embeddings, resolver )) val result = pipeline.fit(data).transform(data) from johnsnowlabs import * val documentAssembler = new nlp.DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = nlp.SentenceDetectorDLModel .pretrained(&quot;sentence_detector_dl&quot;,&quot;xx&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new nlp.Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = nlp.BertEmbeddings .pretrained(&quot;bert_embeddings_sec_bert_base&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;embeddings&quot;) val ner_model = legal.NerModel .pretrained(&quot;legner_orgs_prods_alias&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) val ner_converter = new nlp.NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) val chunk2doc = new nlp.Chunk2Doc() .setInputCols(&quot;ner_chunk&quot;) .setOutputCol(&quot;ner_chunk_doc&quot;) val sentence_embeddings = nlp.UniversalSentenceEncoder .pretrained(&quot;tfhub_use&quot;, &quot;en&quot;) .setInputCols(&quot;ner_chunk_doc&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val resolver = legal.SentenceEntityResolverModel .pretrained(&quot;legel_edgar_company_name&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols(Array(&quot;text&quot;, &quot;sentence_embeddings&quot;)) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, ner_converter, chunk2doc, sentence_embeddings, resolver )) val result = pipeline.fit(data).transform(data) ZeroShotNerModel This is a zero shot named entity recognition based on RoBertaForQuestionAnswering. Zero shot models excel at generalization, meaning that the model can accurately predict entities in very different data sets without the need to fine tune the model or train from scratch for each different domain. Even though a model trained to solve a specific problem can achieve better accuracy than a zero-shot model in this specific task, it probably won’t be be useful in a different task. That is where zero-shot models shows its usefulness by being able to achieve good results in many different scenarions. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: NAMED_ENTITY Python API: ZeroShotNerModel Scala API: ZeroShotNerModel Show Example PythonScala MedicalFinanceLegal documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentenceDetector = ( SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) ) tokenizer = Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) zero_shot_ner = ( ZeroShotNerModel.pretrained(&quot;zero_shot_ner_roberta&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setEntityDefinitions( { &quot;PROBLEM&quot;: [ &quot;What is the disease?&quot;, &quot;What is his symptom?&quot;, &quot;What is her disease?&quot;, &quot;What is his disease?&quot;, &quot;What is the problem?&quot;, &quot;What does a patient suffer&quot;, &quot;What was the reason that the patient is admitted to the clinic?&quot;, ], &quot;DRUG&quot;: [ &quot;Which drug?&quot;, &quot;Which is the drug?&quot;, &quot;What is the drug?&quot;, &quot;Which drug does he use?&quot;, &quot;Which drug does she use?&quot;, &quot;Which drug do I use?&quot;, &quot;Which drug is prescribed for a symptom?&quot;, ], &quot;ADMISSION_DATE&quot;: [&quot;When did patient admitted to a clinic?&quot;], &quot;PATIENT_AGE&quot;: [ &quot;How old is the patient?&quot;, &quot;What is the gae of the patient?&quot;, ], } ) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;zero_shot_ner&quot;) .setPredictionThreshold(0.1) ) # default 0.01 ner_converter = ( sparknlp.annotators.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;zero_shot_ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) ) pipeline = Pipeline( stages=[ documentAssembler, sentenceDetector, tokenizer, zero_shot_ner, ner_converter, ] ) zero_shot_ner_model = pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) text_list = [ &quot;The doctor pescribed Majezik for my severe headache.&quot;, &quot;The patient was admitted to the hospital for his colon cancer.&quot;, &quot;27 years old patient was admitted to clinic on Sep 1st by Dr. X for a right-sided pleural effusion for thoracentesis.&quot;, ] data = spark.createDataFrame(text_list, StringType()).toDF(&quot;text&quot;) results = zero_shot_ner_model.transform(data) results.select( F.explode( F.arrays_zip( results.token.result, results.zero_shot_ner.result, results.zero_shot_ner.metadata, results.zero_shot_ner.begin, results.zero_shot_ner.end, ) ).alias(&quot;cols&quot;) ).select( F.expr(&quot;cols[&#39;0&#39;]&quot;).alias(&quot;token&quot;), F.expr(&quot;cols[&#39;1&#39;]&quot;).alias(&quot;ner_label&quot;), F.expr(&quot;cols[&#39;2&#39;][&#39;sentence&#39;]&quot;).alias(&quot;sentence&quot;), F.expr(&quot;cols[&#39;3&#39;]&quot;).alias(&quot;begin&quot;), F.expr(&quot;cols[&#39;4&#39;]&quot;).alias(&quot;end&quot;), F.expr(&quot;cols[&#39;2&#39;][&#39;confidence&#39;]&quot;).alias(&quot;confidence&quot;), ).show( 50, truncate=100 ) +-+-+--+--++-+ token| ner_label|sentence|begin|end|confidence| +-+-+--+--++-+ The| O| 0| 0| 2| null| doctor| O| 0| 4| 9| null| pescribed| O| 0| 11| 19| null| Majezik| B-DRUG| 0| 21| 27| 0.6467137| for| O| 0| 29| 31| null| my| O| 0| 33| 34| null| severe| B-PROBLEM| 0| 36| 41|0.55263567| headache| I-PROBLEM| 0| 43| 50|0.55263567| .| O| 0| 51| 51| null| The| O| 0| 0| 2| null| patient| O| 0| 4| 10| null| was| O| 0| 12| 14| null| admitted| O| 0| 16| 23| null| to| O| 0| 25| 26| null| the| O| 0| 28| 30| null| hospital| O| 0| 32| 39| null| for| O| 0| 41| 43| null| his| O| 0| 45| 47| null| colon| B-PROBLEM| 0| 49| 53| 0.8898501| cancer| I-PROBLEM| 0| 55| 60| 0.8898501| .| O| 0| 61| 61| null| 27| B-PATIENT_AGE| 0| 0| 1| 0.6943086| years| I-PATIENT_AGE| 0| 3| 7| 0.6943086| old| I-PATIENT_AGE| 0| 9| 11| 0.6943086| patient| O| 0| 13| 19| null| was| O| 0| 21| 23| null| admitted| O| 0| 25| 32| null| to| O| 0| 34| 35| null| clinic| O| 0| 37| 42| null| on| O| 0| 44| 45| null| Sep|B-ADMISSION_DATE| 0| 47| 49|0.95646083| 1st|I-ADMISSION_DATE| 0| 51| 53|0.95646083| by| O| 0| 55| 56| null| Dr| O| 0| 58| 59| null| .| O| 0| 60| 60| null| X| O| 0| 62| 62| null| for| O| 0| 64| 66| null| a| B-PROBLEM| 0| 68| 68|0.50026655| right-sided| I-PROBLEM| 0| 70| 80|0.50026655| pleural| I-PROBLEM| 0| 82| 88|0.50026655| effusion| I-PROBLEM| 0| 90| 97|0.50026655| for| I-PROBLEM| 0| 99|101|0.50026655| thoracentesis| I-PROBLEM| 0| 103|115|0.50026655| .| O| 0| 116|116| null| +-+-+--+--++-+ document_assembler = ( nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) ) sentence_detector = ( nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) ) tokenizer = nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) zero_shot_ner = ( finance.ZeroShotNerModel.pretrained( &quot;finner_roberta_zeroshot&quot;, &quot;en&quot;, &quot;finance/models&quot; ) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;zero_shot_ner&quot;) .setEntityDefinitions( { &quot;DATE&quot;: [ &quot;When was the company acquisition?&quot;, &quot;When was the company purchase agreement?&quot;, ], &quot;ORG&quot;: [&quot;Which company was acquired?&quot;], &quot;PRODUCT&quot;: [&quot;Which product?&quot;], &quot;PROFIT_INCREASE&quot;: [&quot;How much has the gross profit increased?&quot;], &quot;REVENUES_DECLINED&quot;: [&quot;How much has the revenues declined?&quot;], &quot;OPERATING_LOSS_2020&quot;: [&quot;Which was the operating loss in 2020&quot;], &quot;OPERATING_LOSS_2019&quot;: [&quot;Which was the operating loss in 2019&quot;], } ) ) ner_converter = ( nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;zero_shot_ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) ) pipeline = nlp.Pipeline( stages=[ document_assembler, sentence_detector, tokenizer, zero_shot_ner, ner_converter, ] ) from pyspark.sql.types import StringType sample_text = [ &quot;In March 2012, as part of a longer-term strategy, the Company acquired Vertro, Inc., which owned and operated the ALOT product portfolio.&quot;, &quot;In February 2017, the Company entered into an asset purchase agreement with NetSeer, Inc.&quot;, &quot;While our gross profit margin increased to 81.4% in 2020 from 63.1% in 2019, our revenues declined approximately 27% in 2020 as compared to 2019.&quot;, &quot;We reported an operating loss of approximately $8,048,581 million in 2020 as compared to an operating loss of $7,738,193 in 2019.&quot;, ] p_model = pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) res = p_model.transform(spark.createDataFrame(sample_text, StringType()).toDF(&quot;text&quot;)) res.select( F.explode( F.arrays_zip( res.ner_chunk.result, res.ner_chunk.begin, res.ner_chunk.end, res.ner_chunk.metadata, ) ).alias(&quot;cols&quot;) ).select( F.expr(&quot;cols[&#39;0&#39;]&quot;).alias(&quot;chunk&quot;), F.expr(&quot;cols[&#39;3&#39;][&#39;entity&#39;]&quot;).alias(&quot;ner_label&quot;) ).filter( &quot;ner_label!=&#39;O&#39;&quot; ).show( truncate=False ) ++-+ |chunk |ner_label | ++-+ |March 2012 |DATE | |Vertro |ORG | |ALOT |PRODUCT | |February 2017 |DATE | |NetSeer |ORG | |81.4% |PROFIT_INCREASE | |27% |REVENUES_DECLINED | |$8,048,581 million|OPERATING_LOSS_2020| |$7,738,193 |OPERATING_LOSS_2019| |2019 |DATE | ++-+ documentAssembler = nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentence = nlp.SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = nlp.Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) zero_shot_ner = ( legal.ZeroShotNerModel.pretrained(&quot;legner_roberta_zeroshot&quot;, &quot;en&quot;, &quot;legal/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;zero_shot_ner&quot;) .setEntityDefinitions( { &quot;DATE&quot;: [ &quot;When was the company acquisition?&quot;, &quot;When was the company purchase agreement?&quot;, &quot;When was the agreement?&quot;, ], &quot;ORG&quot;: [&quot;Which company?&quot;], &quot;STATE&quot;: [&quot;Which state?&quot;], &quot;AGREEMENT&quot;: [&quot;What kind of agreement?&quot;], &quot;LICENSE&quot;: [&quot;What kind of license?&quot;], &quot;LICENSE_RECIPIENT&quot;: [&quot;To whom the license is granted?&quot;], } ) ) nerconverter = ( nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;zero_shot_ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) ) pipeline = nlp.Pipeline( stages=[ documentAssembler, sentence, tokenizer, zero_shot_ner, nerconverter, ] ) from pyspark.sql.types import StructType, StructField, StringType sample_text = [ &quot;In March 2012, as part of a longer-term strategy, the Company acquired Vertro, Inc., which owned and operated the ALOT product portfolio.&quot;, &quot;In February 2017, the Company entered into an asset purchase agreement with NetSeer, Inc.&quot;, &quot;This INTELLECTUAL PROPERTY AGREEMENT, dated as of December 31, 2018 (the &#39;Effective Date&#39;) is entered into by and between Armstrong Flooring, Inc., a Delaware corporation (&#39;Seller&#39;) and AFI Licensing LLC, a Delaware company (the &#39;Licensee&#39;)&quot;, &quot;The Company hereby grants to Seller a perpetual, non- exclusive, royalty-free license&quot;, ] p_model = pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) res = p_model.transform(spark.createDataFrame(sample_text, StringType()).toDF(&quot;text&quot;)) res.select( F.explode( F.arrays_zip( res.ner_chunk.result, res.ner_chunk.begin, res.ner_chunk.end, res.ner_chunk.metadata, ) ).alias(&quot;cols&quot;) ).select( F.expr(&quot;cols[&#39;0&#39;]&quot;).alias(&quot;chunk&quot;), F.expr(&quot;cols[&#39;3&#39;][&#39;entity&#39;]&quot;).alias(&quot;ner_label&quot;) ).filter( &quot;ner_label!=&#39;O&#39;&quot; ).show( truncate=False ) +-+--+ |chunk |ner_label | +-+--+ |March 2012 |DATE | |Vertro, Inc |ORG | |February 2017 |DATE | |asset purchase agreement |AGREEMENT | |NetSeer |ORG | |INTELLECTUAL PROPERTY |AGREEMENT | |December 31, 2018 |DATE | |Armstrong Flooring |LICENSE_RECIPIENT| |Delaware |STATE | |AFI Licensing LLC, a Delaware company|LICENSE_RECIPIENT| |Seller |LICENSE_RECIPIENT| |perpetual |LICENSE | |non- exclusive |LICENSE | |royalty-free |LICENSE | +-+--+ FinanceLegal val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentences&quot;) val zeroShotNer = ZeroShotNerModel .pretrained() .setEntityDefinitions( Map( &quot;NAME&quot; -&gt; Array(&quot;What is his name?&quot;, &quot;What is her name?&quot;), &quot;CITY&quot; -&gt; Array(&quot;Which city?&quot;))) .setPredictionThreshold(0.01f) .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;zero_shot_ner&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentenceDetector, zeroShotNer)) val model = pipeline.fit(Seq(&quot;&quot;).toDS.toDF(&quot;text&quot;)) val results = model.transform( Seq(&quot;Clara often travels between New York and Paris.&quot;).toDS.toDF(&quot;text&quot;)) results .selectExpr(&quot;document&quot;, &quot;explode(zero_shot_ner) AS entity&quot;) .select( col(&quot;entity.result&quot;), col(&quot;entity.metadata.word&quot;), col(&quot;entity.metadata.sentence&quot;), col(&quot;entity.begin&quot;), col(&quot;entity.end&quot;), col(&quot;entity.metadata.confidence&quot;), col(&quot;entity.metadata.question&quot;)) .show(truncate=false) ++--+--+--++-++ |result|word |sentence|begin|end|confidence|question | ++--+--+--++-++ |B-CITY|Paris|0 |41 |45 |0.78655756|Which is the city?| |B-CITY|New |0 |28 |30 |0.29346612|Which city? | |I-CITY|York |0 |32 |35 |0.29346612|Which city? | ++--+--+--++-++ val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentences&quot;) val zeroShotNer = ZeroShotNerModel .pretrained() .setEntityDefinitions( Map( &quot;NAME&quot; -&gt; Array(&quot;What is his name?&quot;, &quot;What is her name?&quot;), &quot;CITY&quot; -&gt; Array(&quot;Which city?&quot;))) .setPredictionThreshold(0.01f) .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;zero_shot_ner&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentenceDetector, zeroShotNer)) val model = pipeline.fit(Seq(&quot;&quot;).toDS.toDF(&quot;text&quot;)) val results = model.transform( Seq(&quot;Clara often travels between New York and Paris.&quot;).toDS.toDF(&quot;text&quot;)) results .selectExpr(&quot;document&quot;, &quot;explode(zero_shot_ner) AS entity&quot;) .select( col(&quot;entity.result&quot;), col(&quot;entity.metadata.word&quot;), col(&quot;entity.metadata.sentence&quot;), col(&quot;entity.begin&quot;), col(&quot;entity.end&quot;), col(&quot;entity.metadata.confidence&quot;), col(&quot;entity.metadata.question&quot;)) .show(truncate=false) ++--+--+--++-++ |result|word |sentence|begin|end|confidence|question | ++--+--+--++-++ |B-CITY|Paris|0 |41 |45 |0.78655756|Which is the city?| |B-CITY|New |0 |28 |30 |0.29346612|Which city? | |I-CITY|York |0 |32 |35 |0.29346612|Which city? | ++--+--+--++-++ ZeroShotRelationExtractionModel ZeroShotRelationExtractionModel implements zero-shot binary relations extraction by utilizing BERT transformer models trained on the NLI (Natural Language Inference) task. The model inputs consists of documents/sentences and paired NER chunks, usually obtained by RENerChunksFilter. The definitions of relations which are extracted is given by a dictionary structures, specifying a set of statements regarding the relationship of named entities. These statements are automatically appended to each document in the dataset and the NLI model is used to determine whether a particular relationship between entities. For available pretrained models please see the NLP Models Hub. Input Annotator Types: CHUNK, DOCUMENT Output Annotator Type: CATEGORY Python API: ZeroShotRelationExtractionModel Scala API: ZeroShotRelationExtractionModel Show Example PythonScala MedicalFinanceLegal documenter = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentencer = ( SentenceDetectorDLModel.pretrained( &quot;sentence_detector_dl_healthcare&quot;, &quot;en&quot;, &quot;clinical/models&quot; ) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) ) tokenizer = Tokenizer().setInputCols([&quot;sentences&quot;]).setOutputCol(&quot;tokens&quot;) words_embedder = ( WordEmbeddingsModel() .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) ) ner_clinical = ( MedicalNerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_clinical&quot;) ) ner_clinical_converter = ( NerConverter() .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_clinical&quot;]) .setOutputCol(&quot;ner_clinical_chunks&quot;) .setWhiteList([&quot;PROBLEM&quot;, &quot;TEST&quot;]) ) # PROBLEM-TEST-TREATMENT ner_posology = ( MedicalNerModel.pretrained(&quot;ner_posology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_posology&quot;) ) ner_posology_converter = ( NerConverter() .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_posology&quot;]) .setOutputCol(&quot;ner_posology_chunks&quot;) .setWhiteList([&quot;DRUG&quot;]) ) # DRUG-FREQUENCY-DOSAGE-DURATION-FORM-ROUTE-STRENGTH chunk_merger = ( ChunkMergeApproach() .setInputCols(&quot;ner_clinical_chunks&quot;, &quot;ner_posology_chunks&quot;) .setOutputCol(&quot;merged_ner_chunks&quot;) ) ## ZERO-SHOT RE Starting... pos_tagger = ( PerceptronModel() .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;pos_tags&quot;) ) dependency_parser = ( DependencyParserModel() .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;document&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) ) re_ner_chunk_filter = ( RENerChunksFilter() .setRelationPairs([&quot;problem-test&quot;, &quot;problem-drug&quot;]) .setMaxSyntacticDistance(4) .setDocLevelRelations(False) .setInputCols([&quot;merged_ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) ) re_model = ( ZeroShotRelationExtractionModel.pretrained( &quot;re_zeroshot_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot; ) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) .setRelationalCategories( { &quot;ADE&quot;: [&quot;{DRUG} causes {PROBLEM}.&quot;], &quot;IMPROVE&quot;: [&quot;{DRUG} improves {PROBLEM}.&quot;, &quot;{DRUG} cures {PROBLEM}.&quot;], &quot;REVEAL&quot;: [&quot;{TEST} reveals {PROBLEM}.&quot;], } ) .setMultiLabel(True) ) pipeline = sparknlp.base.Pipeline().setStages( [ documenter, sentencer, tokenizer, words_embedder, ner_clinical, ner_clinical_converter, ner_posology, ner_posology_converter, chunk_merger, pos_tagger, dependency_parser, re_ner_chunk_filter, re_model, ] ) sample_text = &quot;Paracetamol can alleviate headache or sickness. An MRI test can be used to find cancer.&quot; data = spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;) model = pipeline.fit(data) results = model.transform(data) from pyspark.sql import functions as F results.select( F.explode(F.arrays_zip(results.relations.metadata, results.relations.result)).alias( &quot;cols&quot; ) ).select( F.expr(&quot;cols[&#39;0&#39;][&#39;sentence&#39;]&quot;).alias(&quot;sentence&quot;), F.expr(&quot;cols[&#39;0&#39;][&#39;entity1_begin&#39;]&quot;).alias(&quot;entity1_begin&quot;), F.expr(&quot;cols[&#39;0&#39;][&#39;entity1_end&#39;]&quot;).alias(&quot;entity1_end&quot;), F.expr(&quot;cols[&#39;0&#39;][&#39;chunk1&#39;]&quot;).alias(&quot;chunk1&quot;), F.expr(&quot;cols[&#39;0&#39;][&#39;entity1&#39;]&quot;).alias(&quot;entity1&quot;), F.expr(&quot;cols[&#39;0&#39;][&#39;entity2_begin&#39;]&quot;).alias(&quot;entity2_begin&quot;), F.expr(&quot;cols[&#39;0&#39;][&#39;entity2_end&#39;]&quot;).alias(&quot;entity2_end&quot;), F.expr(&quot;cols[&#39;0&#39;][&#39;chunk2&#39;]&quot;).alias(&quot;chunk2&quot;), F.expr(&quot;cols[&#39;0&#39;][&#39;entity2&#39;]&quot;).alias(&quot;entity2&quot;), F.expr(&quot;cols[&#39;0&#39;][&#39;hypothesis&#39;]&quot;).alias(&quot;hypothesis&quot;), F.expr(&quot;cols[&#39;0&#39;][&#39;nli_prediction&#39;]&quot;).alias(&quot;nli_prediction&quot;), F.expr(&quot;cols[&#39;1&#39;]&quot;).alias(&quot;relation&quot;), F.expr(&quot;cols[&#39;0&#39;][&#39;confidence&#39;]&quot;).alias(&quot;confidence&quot;), ).show( truncate=70 ) +--+-+--+--+-+-+--+--+-++--+--+-+ sentence|entity1_begin|entity1_end| chunk1|entity1|entity2_begin|entity2_end| chunk2|entity2| hypothesis|nli_prediction|relation|confidence| +--+-+--+--+-+-+--+--+-++--+--+-+ 0| 0| 10|Paracetamol| DRUG| 38| 45|sickness|PROBLEM|Paracetamol improves sickness.| entail| IMPROVE|0.98819494| 0| 0| 10|Paracetamol| DRUG| 26| 33|headache|PROBLEM|Paracetamol improves headache.| entail| IMPROVE| 0.9929625| 1| 48| 58|An MRI test| TEST| 80| 85| cancer|PROBLEM| An MRI test reveals cancer.| entail| REVEAL| 0.9760039| +--+-+--+--+-+-+--+--+-++--+--+-+ document_assembler = ( nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) ) sentence_detector = ( nlp.SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl&quot;, &quot;xx&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) ) tokenizer = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) embeddings = ( nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_sec_bert_base&quot;, &quot;en&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ) ner_model = ( finance.NerModel.pretrained(&quot;finner_financial_small&quot;, &quot;en&quot;, &quot;finance/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ) ner_converter = ( nlp.NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) ) re_model = ( finance.ZeroShotRelationExtractionModel.pretrained( &quot;finre_zero_shot&quot;, &quot;en&quot;, &quot;finance/models&quot; ) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;relations&quot;) .setMultiLabel(False) ) re_model.setRelationalCategories( { &quot;profit_decline_by&quot;: [ &quot;{PROFIT_DECLINE} decreased by {AMOUNT} from&quot;, &quot;{PROFIT_DECLINE} decreased by {AMOUNT} to&quot;, ], &quot;profit_decline_by_per&quot;: [ &quot;{PROFIT_DECLINE} decreased by a {PERCENTAGE} from&quot;, &quot;{PROFIT_DECLINE} decreased by a {PERCENTAGE} to&quot;, ], &quot;profit_decline_from&quot;: [ &quot;{PROFIT_DECLINE} decreased from {AMOUNT}&quot;, &quot;{PROFIT_DECLINE} decreased from {AMOUNT} for the year&quot;, ], &quot;profit_decline_from_per&quot;: [ &quot;{PROFIT_DECLINE} decreased from {PERCENTAGE} to&quot;, &quot;{PROFIT_DECLINE} decreased from {PERCENTAGE} to a total of&quot;, ], &quot;profit_decline_to&quot;: [&quot;{PROFIT_DECLINE} to {AMOUNT}&quot;], &quot;profit_increase_from&quot;: [&quot;{PROFIT_INCREASE} from {AMOUNT}&quot;], &quot;profit_increase_to&quot;: [&quot;{PROFIT_INCREASE} to {AMOUNT}&quot;], &quot;expense_decrease_by&quot;: [&quot;{EXPENSE_DECREASE} decreased by {AMOUNT}&quot;], &quot;expense_decrease_by_per&quot;: [&quot;{EXPENSE_DECREASE} decreased by a {PERCENTAGE}&quot;], &quot;expense_decrease_from&quot;: [&quot;{EXPENSE_DECREASE} decreased from {AMOUNT}&quot;], &quot;expense_decrease_to&quot;: [ &quot;{EXPENSE_DECREASE} for a total of {AMOUNT} for the fiscal year&quot; ], &quot;has_date&quot;: [ &quot;{AMOUNT} for the fiscal year ended {FISCAL_YEAR}&quot;, &quot;{PERCENTAGE} for the fiscal year ended {FISCAL_YEAR}&quot;, ], } ) pipeline = nlp.Pipeline( stages=[ document_assembler, sentence_detector, tokenizer, embeddings, ner_model, ner_converter, re_model, ] ) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = pipeline.fit(empty_data) light_model = nlp.LightPipeline(model) sample_text = &quot;&quot;&quot;License fees revenue decreased 40 %, or $ 0.5 million to $ 0.7 million for the year ended December 31, 2020 compared to $ 1.2 million for the year ended December 31, 2019. Services revenue increased 4 %, or $ 1.1 million, to $ 25.6 million for the year ended December 31, 2020 from $ 24.5 million for the year ended December 31, 2019. Costs of revenue, excluding depreciation and amortization increased by $ 0.1 million, or 2 %, to $ 8.8 million for the year ended December 31, 2020 from $ 8.7 million for the year ended December 31, 2019. Also, a decrease in travel costs of $ 0.4 million due to travel restrictions caused by the global pandemic. As a percentage of revenue, cost of revenue, excluding depreciation and amortization was 34 % for each of the years ended December 31, 2020 and 2019. Sales and marketing expenses decreased 20 %, or $ 1.5 million, to $ 6.0 million for the year ended December 31, 2020 from $ 7.5 million for the year ended December 31, 2019&quot;&quot;&quot; data = spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;) result = model.transform(data) result.selectExpr(&quot;explode(relations) as relation&quot;).show(truncate=False) ++ |relation | ++ |{category, 8462, 8693, has_date, {entity1_begin -&gt; 227, relation -&gt; has_date, hypothesis -&gt; 25.6 million for the fiscal year ended December 31, 2019, confidence -&gt; 0.8744761, nli_prediction -&gt; entail, entity1 -&gt; AMOUNT, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2019, entity2_end -&gt; 332, entity1_end -&gt; 238, entity2_begin -&gt; 316, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 25.6 million, sentence -&gt; 1}, []} | |{category, 4643, 4873, has_date, {entity1_begin -&gt; 31, relation -&gt; has_date, hypothesis -&gt; 40 for the fiscal year ended December 31, 2019, confidence -&gt; 0.7889031, nli_prediction -&gt; entail, entity1 -&gt; PERCENTAGE, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2019, entity2_end -&gt; 169, entity1_end -&gt; 32, entity2_begin -&gt; 153, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 40, sentence -&gt; 0}, []} | |{category, 13507, 13748, expense_decrease_from, {entity1_begin -&gt; 799, relation -&gt; expense_decrease_from, hypothesis -&gt; Sales and marketing expenses decreased from 7.5 million, confidence -&gt; 0.9770538, nli_prediction -&gt; entail, entity1 -&gt; EXPENSE_DECREASE, syntactic_distance -&gt; undefined, chunk2 -&gt; 7.5 million, entity2_end -&gt; 933, entity1_end -&gt; 826, entity2_begin -&gt; 923, entity2 -&gt; AMOUNT, chunk1 -&gt; Sales and marketing expenses, sentence -&gt; 5}, []}| |{category, 5354, 5593, has_date, {entity1_begin -&gt; 59, relation -&gt; has_date, hypothesis -&gt; 0.7 million for the fiscal year ended December 31, 2020, confidence -&gt; 0.6718765, nli_prediction -&gt; entail, entity1 -&gt; AMOUNT, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2020, entity2_end -&gt; 106, entity1_end -&gt; 69, entity2_begin -&gt; 90, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 0.7 million, sentence -&gt; 0}, []} | |{category, 6490, 6697, profit_increase_to, {entity1_begin -&gt; 172, relation -&gt; profit_increase_to, hypothesis -&gt; Services revenue to 25.6 million, confidence -&gt; 0.9674029, nli_prediction -&gt; entail, entity1 -&gt; PROFIT_INCREASE, syntactic_distance -&gt; undefined, chunk2 -&gt; 25.6 million, entity2_end -&gt; 238, entity1_end -&gt; 187, entity2_begin -&gt; 227, entity2 -&gt; AMOUNT, chunk1 -&gt; Services revenue, sentence -&gt; 1}, []} | |{category, 4412, 4642, has_date, {entity1_begin -&gt; 31, relation -&gt; has_date, hypothesis -&gt; 40 for the fiscal year ended December 31, 2020, confidence -&gt; 0.778003, nli_prediction -&gt; entail, entity1 -&gt; PERCENTAGE, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2020, entity2_end -&gt; 106, entity1_end -&gt; 32, entity2_begin -&gt; 90, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 40, sentence -&gt; 0}, []} | |{category, 13989, 14221, has_date, {entity1_begin -&gt; 838, relation -&gt; has_date, hypothesis -&gt; 20 for the fiscal year ended December 31, 2020, confidence -&gt; 0.8545547, nli_prediction -&gt; entail, entity1 -&gt; PERCENTAGE, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2020, entity2_end -&gt; 914, entity1_end -&gt; 839, entity2_begin -&gt; 898, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 20, sentence -&gt; 5}, []} | |{category, 11157, 11314, expense_decrease_by, {entity1_begin -&gt; 561, relation -&gt; expense_decrease_by, hypothesis -&gt; travel costs decreased by 0.4 million, confidence -&gt; 0.9946776, nli_prediction -&gt; entail, entity1 -&gt; EXPENSE_DECREASE, syntactic_distance -&gt; undefined, chunk2 -&gt; 0.4 million, entity2_end -&gt; 589, entity1_end -&gt; 572, entity2_begin -&gt; 579, entity2 -&gt; AMOUNT, chunk1 -&gt; travel costs, sentence -&gt; 3}, []} | |{category, 5114, 5353, has_date, {entity1_begin -&gt; 42, relation -&gt; has_date, hypothesis -&gt; 0.5 million for the fiscal year ended December 31, 2019, confidence -&gt; 0.77566886, nli_prediction -&gt; entail, entity1 -&gt; AMOUNT, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2019, entity2_end -&gt; 169, entity1_end -&gt; 52, entity2_begin -&gt; 153, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 0.5 million, sentence -&gt; 0}, []} | |{category, 6281, 6489, profit_increase_from, {entity1_begin -&gt; 172, relation -&gt; profit_increase_from, hypothesis -&gt; Services revenue from 1.1 million, confidence -&gt; 0.96610945, nli_prediction -&gt; entail, entity1 -&gt; PROFIT_INCREASE, syntactic_distance -&gt; undefined, chunk2 -&gt; 1.1 million, entity2_end -&gt; 219, entity1_end -&gt; 187, entity2_begin -&gt; 209, entity2 -&gt; AMOUNT, chunk1 -&gt; Services revenue, sentence -&gt; 1}, []} | |{category, 9199, 9471, has_date, {entity1_begin -&gt; 408, relation -&gt; has_date, hypothesis -&gt; 0.1 million for the fiscal year ended December 31, 2019, confidence -&gt; 0.9083246, nli_prediction -&gt; entail, entity1 -&gt; AMOUNT, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2019, entity2_end -&gt; 537, entity1_end -&gt; 418, entity2_begin -&gt; 521, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 0.1 million, sentence -&gt; 2}, []} | |{category, 14455, 14696, has_date, {entity1_begin -&gt; 849, relation -&gt; has_date, hypothesis -&gt; 1.5 million for the fiscal year ended December 31, 2020, confidence -&gt; 0.75281376, nli_prediction -&gt; entail, entity1 -&gt; AMOUNT, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2020, entity2_end -&gt; 914, entity1_end -&gt; 859, entity2_begin -&gt; 898, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 1.5 million, sentence -&gt; 5}, []} | |{category, 14697, 14938, has_date, {entity1_begin -&gt; 849, relation -&gt; has_date, hypothesis -&gt; 1.5 million for the fiscal year ended December 31, 2019, confidence -&gt; 0.8073463, nli_prediction -&gt; entail, entity1 -&gt; AMOUNT, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2019, entity2_end -&gt; 970, entity1_end -&gt; 859, entity2_begin -&gt; 954, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 1.5 million, sentence -&gt; 5}, []} | |{category, 4874, 5113, has_date, {entity1_begin -&gt; 42, relation -&gt; has_date, hypothesis -&gt; 0.5 million for the fiscal year ended December 31, 2020, confidence -&gt; 0.71575713, nli_prediction -&gt; entail, entity1 -&gt; AMOUNT, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2020, entity2_end -&gt; 106, entity1_end -&gt; 52, entity2_begin -&gt; 90, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 0.5 million, sentence -&gt; 0}, []} | |{category, 6908, 7115, profit_increase_to, {entity1_begin -&gt; 172, relation -&gt; profit_increase_to, hypothesis -&gt; Services revenue to 24.5 million, confidence -&gt; 0.85972106, nli_prediction -&gt; entail, entity1 -&gt; PROFIT_INCREASE, syntactic_distance -&gt; undefined, chunk2 -&gt; 24.5 million, entity2_end -&gt; 295, entity1_end -&gt; 187, entity2_begin -&gt; 284, entity2 -&gt; AMOUNT, chunk1 -&gt; Services revenue, sentence -&gt; 1}, []} | |{category, 5594, 5833, has_date, {entity1_begin -&gt; 59, relation -&gt; has_date, hypothesis -&gt; 0.7 million for the fiscal year ended December 31, 2019, confidence -&gt; 0.7484568, nli_prediction -&gt; entail, entity1 -&gt; AMOUNT, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2019, entity2_end -&gt; 169, entity1_end -&gt; 69, entity2_begin -&gt; 153, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 0.7 million, sentence -&gt; 0}, []} | |{category, 7326, 7546, has_date, {entity1_begin -&gt; 199, relation -&gt; has_date, hypothesis -&gt; 4 for the fiscal year ended December 31, 2020, confidence -&gt; 0.8412763, nli_prediction -&gt; entail, entity1 -&gt; PERCENTAGE, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2020, entity2_end -&gt; 275, entity1_end -&gt; 199, entity2_begin -&gt; 259, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 4, sentence -&gt; 1}, []} | |{category, 9472, 9734, has_date, {entity1_begin -&gt; 424, relation -&gt; has_date, hypothesis -&gt; 2 for the fiscal year ended December 31, 2020, confidence -&gt; 0.8046481, nli_prediction -&gt; entail, entity1 -&gt; PERCENTAGE, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2020, entity2_end -&gt; 481, entity1_end -&gt; 424, entity2_begin -&gt; 465, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 2, sentence -&gt; 2}, []} | |{category, 9735, 9997, has_date, {entity1_begin -&gt; 424, relation -&gt; has_date, hypothesis -&gt; 2 for the fiscal year ended December 31, 2019, confidence -&gt; 0.8485106, nli_prediction -&gt; entail, entity1 -&gt; PERCENTAGE, syntactic_distance -&gt; undefined, chunk2 -&gt; December 31, 2019, entity2_end -&gt; 537, entity1_end -&gt; 424, entity2_begin -&gt; 521, entity2 -&gt; FISCAL_YEAR, chunk1 -&gt; 2, sentence -&gt; 2}, []} | |{category, 691, 916, profit_decline_by_per, {entity1_begin -&gt; 0, relation -&gt; profit_decline_by_per, hypothesis -&gt; License fees revenue decreased by a 40 to, confidence -&gt; 0.9948003, nli_prediction -&gt; entail, entity1 -&gt; PROFIT_DECLINE, syntactic_distance -&gt; undefined, chunk2 -&gt; 40, entity2_end -&gt; 32, entity1_end -&gt; 19, entity2_begin -&gt; 31, entity2 -&gt; PERCENTAGE, chunk1 -&gt; License fees revenue, sentence -&gt; 0}, []} | ++ only showing top 20 rows document_assembler = ( nlp.DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) ) tokenizer = nlp.Tokenizer().setInputCols([&quot;sentence&quot;]).setOutputCol(&quot;token&quot;) tokenClassifier = ( legal.BertForTokenClassification.pretrained( &quot;legner_obligations&quot;, &quot;en&quot;, &quot;legal/models&quot; ) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ) ner_converter = ( nlp.NerConverter() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) ) re_model = ( legal.ZeroShotRelationExtractionModel.pretrained( &quot;legre_zero_shot&quot;, &quot;en&quot;, &quot;legal/models&quot; ) .setInputCols([&quot;ner_chunk&quot;, &quot;document&quot;]) .setOutputCol(&quot;relations&quot;) ) re_model.setRelationalCategories( { &quot;should_provide&quot;: [ &quot;{OBLIGATION_SUBJECT} will provide {OBLIGATION}&quot;, &quot;{OBLIGATION_SUBJECT} should provide {OBLIGATION}&quot;, ], &quot;commits_with&quot;: [ &quot;{OBLIGATION_SUBJECT} to {OBLIGATION_INDIRECT_OBJECT}&quot;, &quot;{OBLIGATION_SUBJECT} with {OBLIGATION_INDIRECT_OBJECT}&quot;, ], &quot;commits_to&quot;: [&quot;{OBLIGATION_SUBJECT} commits to {OBLIGATION}&quot;], &quot;agree_to&quot;: [&quot;{OBLIGATION_SUBJECT} agrees to {OBLIGATION}&quot;], } ) pipeline = nlp.Pipeline( stages=[document_assembler, tokenizer, tokenClassifier, ner_converter, re_model] ) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = pipeline.fit(empty_data) light_model = nlp.LightPipeline(model) import pandas as pd def get_relations_df(results, col=&quot;relations&quot;): rel_pairs = [] for i in range(len(results)): for rel in results[i][col]: rel_pairs.append( ( rel.result, rel.metadata[&quot;entity1&quot;], rel.metadata[&quot;entity1_begin&quot;], rel.metadata[&quot;entity1_end&quot;], rel.metadata[&quot;chunk1&quot;], rel.metadata[&quot;entity2&quot;], rel.metadata[&quot;entity2_begin&quot;], rel.metadata[&quot;entity2_end&quot;], rel.metadata[&quot;chunk2&quot;], rel.metadata[&quot;confidence&quot;], ) ) rel_df = pd.DataFrame( rel_pairs, columns=[ &quot;relation&quot;, &quot;entity1&quot;, &quot;entity1_begin&quot;, &quot;entity1_end&quot;, &quot;chunk1&quot;, &quot;entity2&quot;, &quot;entity2_begin&quot;, &quot;entity2_end&quot;, &quot;chunk2&quot;, &quot;confidence&quot;, ], ) return rel_df sample_text = &quot;&quot;&quot;This INTELLECTUAL PROPERTY AGREEMENT (this &quot;Agreement&quot;), dated as of December 31, 2018 (the &quot;Effective Date&quot;) is entered into by and between Armstrong Flooring, Inc., a Delaware corporation (&quot;Seller&quot;) and AFI Licensing LLC, a Delaware limited liability company (&quot;Licensing&quot; and together with Seller, &quot;Arizona&quot;) and AHF Holding, Inc. (formerly known as Tarzan HoldCo, Inc.), a Delaware corporation (&quot;Buyer&quot;) and Armstrong Hardwood Flooring Company, a Tennessee corporation (the &quot;Company&quot; and together with Buyer the &quot;Buyer Entities&quot;) (each of Arizona on the one hand and the Buyer Entities on the other hand, a &quot;Party&quot; and collectively, the &quot;Parties&quot;).&quot;&quot;&quot; result = light_model.fullAnnotate(sample_text) rel_df = get_relations_df(result) rel_df[rel_df[&quot;relation&quot;] != &quot;no_rel&quot;] | relation | entity1 | entity1_begin | entity1_end | chunk1 | entity2 | entity2_begin | entity2_end | chunk2 | confidence | |:|--:|--:|:|:|--:|--:|:|:|--:| | dated_as | DOC | 5 | 35 | INTELLECTUAL PROPERTY AGREEMENT | EFFDATE | 69 | 85 | December 31, 2018 | 0.98433626 | | signed_by | DOC | 5 | 35 | INTELLECTUAL PROPERTY AGREEMENT | PARTY | 141 | 163 | Armstrong Flooring, Inc | 0.60404813 | | has_alias | PARTY | 141 | 163 | Armstrong Flooring, Inc | ALIAS | 192 | 197 | Seller | 0.96357507 | | has_alias | PARTY | 205 | 221 | AFI Licensing LLC | ALIAS | 263 | 271 | Licensing | 0.9546678 | | has_alias | PARTY | 315 | 330 | AHF Holding, Inc | ALIAS | 611 | 615 | Party | 0.5387175 | | has_alias | PARTY | 315 | 330 | AHF Holding, Inc | ALIAS | 641 | 647 | Parties | 0.5387175 | | has_collective_alias | ALIAS | 399 | 403 | Buyer | ALIAS | 611 | 615 | Party | 0.5539445 | | has_collective_alias | ALIAS | 399 | 403 | Buyer | ALIAS | 641 | 647 | Parties | 0.5539445 | | has_alias | PARTY | 411 | 445 | Armstrong Hardwood Flooring Company | ALIAS | 478 | 484 | Company | 0.92106056 | | has_alias | PARTY | 411 | 445 | Armstrong Hardwood Flooring Company | ALIAS | 611 | 615 | Party | 0.58123946 | | has_alias | PARTY | 411 | 445 | Armstrong Hardwood Flooring Company | ALIAS | 641 | 647 | Parties | 0.58123946 | | has_collective_alias | ALIAS | 505 | 509 | Buyer | ALIAS | 516 | 529 | Buyer Entities | 0.63492435 | | has_collective_alias | ALIAS | 505 | 509 | Buyer | ALIAS | 611 | 615 | Party | 0.6483803 | | has_collective_alias | ALIAS | 505 | 509 | Buyer | ALIAS | 641 | 647 | Parties | 0.6483803 | | has_collective_alias | ALIAS | 516 | 529 | Buyer Entities | ALIAS | 611 | 615 | Party | 0.6970743 | | has_collective_alias | ALIAS | 516 | 529 | Buyer Entities | ALIAS | 641 | 647 | Parties | 0.6970743 | Medical val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;tokens&quot;) val sentencer = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentences&quot;) val embeddings = WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;embeddings&quot;) val posTagger = PerceptronModel .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;posTags&quot;) val nerTagger = MedicalNerModel .pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;nerTags&quot;) val nerConverter = new NerConverter() .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;, &quot;nerTags&quot;)) .setOutputCol(&quot;nerChunks&quot;) val dependencyParser = DependencyParserModel .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(Array(&quot;document&quot;, &quot;posTags&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;dependencies&quot;) val reNerFilter = new RENerChunksFilter() .setRelationPairs(Array(&quot;problem-test&quot;,&quot;problem-treatment&quot;)) .setMaxSyntacticDistance(4) .setDocLevelRelations(false) .setInputCols(Array(&quot;nerChunks&quot;, &quot;dependencies&quot;)) .setOutputCol(&quot;RENerChunks&quot;) val re = ZeroShotRelationExtractionModel .load(&quot;/tmp/spark_sbert_zero_shot&quot;) .setRelationalCategories( Map( &quot;CURE&quot; -&gt; Array(&quot;{TREATMENT} cures {PROBLEM}.&quot;), &quot;IMPROVE&quot; -&gt; Array(&quot;{TREATMENT} improves {PROBLEM}.&quot;, &quot;{TREATMENT} cures {PROBLEM}.&quot;), &quot;REVEAL&quot; -&gt; Array(&quot;{TEST} reveals {PROBLEM}.&quot;) )) .setPredictionThreshold(0.9f) .setMultiLabel(false) .setInputCols(Array(&quot;sentences&quot;, &quot;RENerChunks&quot;)) .setOutputCol(&quot;relations) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentencer, tokenizer, embeddings, posTagger, nerTagger, nerConverter, dependencyParser, reNerFilter, re)) val model = pipeline.fit(Seq(&quot;&quot;).toDS.toDF(&quot;text&quot;)) val results = model.transform( Seq(&quot;Paracetamol can alleviate headache or sickness. An MRI test can be used to find cancer.&quot;).toDS.toDF(&quot;text&quot;)) results .selectExpr(&quot;EXPLODE(relations) as relation&quot;) .selectExpr(&quot;relation.result&quot;, &quot;relation.metadata.confidence&quot;) .show(truncate = false) +-+-+ |result |confidence| +-+-+ |REVEAL |0.9760039 | |IMPROVE|0.98819494| |IMPROVE|0.9929625 | +-+-+",
    "url": "/docs/en/licensed_annotators",
    "relUrl": "/docs/en/licensed_annotators"
  },
  "90": {
    "id": "90",
    "title": "Enterprise Spark NLP Installation",
    "content": "AWS Marketplace The entire suite of John Snow Labs NLP and Visual NLP libraries are offered as a pay-as-you-go product on AWS Marketplace, pre-installed and ready to use. 30+ Notebooks are included in the AWS product to allow you to start experimenting on your own data right away. To subscribe to the pay-as-you-go product on AWS Marketplace navigate to the product page and follow the instructions in the video below. Subscribe to John Snow Labs NLP Libraries via AWS Marketplace Note: 30-day free trial are available for AWS and Azure subscriptions. Installation with johnsnowlabs On Oct 4th, 2022 we released johnsnowlabs library, which eases the installation and session starting processes in an almost transparent way for the user. Finance NLP and Legal NLP are built on the top of a new John Snow Labs library, called johnsnowlabs. If you are a former user of Spark NLP or Spark NLP for Healthcare, you will find this new way of deploying your Spark NLP clusters much more user-friendly! Clinical NLP (former Spark NLP for Healthcare) still can be run without johnsnowlabs library, although we highly recommend to install it with this new method. For advanced installation options, please check johnsnowlabs webpage. 1. Installing johnsnowlabs The first step you need to carry out is installing johnsnowlabs library. This is as easy as doing: !pip install johnsnowlabs 2. Installing Enterprise NLP (Finance, Legal, Clinical) Import johnsnowlabs and use our one-liner nlp.install() to install all the dependencies, downloading the jars (yes, Spark NLP runs on top of the Java Virtual Machine!), preparing the cluster environment variables, licenses, etc! from johnsnowlabs import * nlp.install(force_browser=True) The force_browser=True command gets rid of you uploading a license. It will open a popup to connect to our license server at my.johnsnowlabs.com retrieve the license for you, and install everything your license allows you to use! If you are a user of Financial NLP, you will get that installed. If you are a Legal user, then Legal NLP will be installed, or Clinical! Everything will be taken care on your behalf! Optional: Uploading the license manually We still have the way of downloading manually the license, in case the connection with my.johnsnowlabs.com is not an option for you. Just put your license json in the same folder of the notebook, and run: nlp.install() In colab, you can use this fancy widget to upload a file to your environment: from google.colab import files print(&#39;Please Upload your John Snow Labs License using the button below&#39;) license_keys = files.upload() And then do: nlp.install() 3. Starting an Enterprise NLP cluster Another one-liner can be used to start your Enterprise Spark NLP cluster: spark = nlp.start() It will take into account the previous steps and your license and return a Spark Session. 4. Ready to go! And you are done! Simple, isn’t it? Find hundreds of notebooks using johnsnowlabs library here: Finance NLP notebooks Legal NLP notebooks Clinical NLP notebooks Finance, Legal, Clinical NLP on Databricks List of tested runtimes. Recommended instance type Standard_F8s_v2 (16 GB Memory, 8 Cores) or higher. The installation takes around 15 minutes. Connection via Databricks Partner connect Databricks has an integration of Spark NLP libraries via Partner Connect. If you are eligible, you can connect your Databricks workspace to John Snow Labs. The Partner Connect wizard will redirect you to John Snow Labs portal. After you fill-in/validate your information a 30-day trial license will be automatically generated for you. A new Databricks cluster will also be created, and all necessary resources to run the library on your account will be installed on your new cluster. Furthermore, a set of ready to use notebooks will be copied to your workspace, so you can start experimenting on your data right away. The trial license file will also be deployed to your environment and made available to your cluster. The trial period is 30 days. You can use the trial period only once. After the trial period, we will contact you with a licensing offer. Start exploring preloaded notebooks Workspace -&gt; Shared -&gt; John Snow Labs Automatic deployment of John Snow Labs NLP libraries from www.johnsnowlabs.com/databricks Alternatively, you can automatically deploy John Snow Labs libraries on Databricks by filling in the form available here. This will allow you to start a 30-day free trial with no limit on the amount of processed data. You just need to provide a Databricks Access Token that is used by our deployment script to connect to your Databricks instance and install John Snow Labs NLP libraries on a cluster of your choice. Start exploring preloaded notebooks Workspace -&gt; Shared -&gt; John Snow Labs Automatic deployment via my.JohnSnowLabs.com Login to your account on my.JohnSnowLabs.com, navigate to ‘My Subscriptions’ page, and identify your license for Databricks. Click on the three dots as illustrated in the image below, then select the Install On Cluster option. On the install form, provide an access token for this account and then select the cluster where you want to install the libraries. Once it is done, you will get an email with information on the status of your deployment and on how to get started with the libraries. Automatic deployment or upgrade from the Databricks workspace If you have already deployed the libraries in the past, you have a script Workspace -&gt; Shared -&gt; John Snow Labs -&gt; Install JohnSnowLabs NLP. If you attach it to any cluster and run it, it will reinstall the libraries on the respective cluster. This is also the recommended way to upgrade to the latest versions of the libraries. Manual deployment of Enterprise Spark NLP Automatic deployment is the preferred option. Create a cluster with one of the supported runtimes if you don’t have one already. On a new cluster or existing one you need to add the following to the Advanced Options -&gt; Spark tab, in Spark.Config box: spark.kryoserializer.buffer.max 1000M spark.serializer org.apache.spark.serializer.KryoSerializer Please add the following to the Advanced Options -&gt; Spark tab, in Environment Variables box: AWS_ACCESS_KEY_ID=xxx AWS_SECRET_ACCESS_KEY=yyy SPARK_NLP_LICENSE=zzz Note: Enterprise Spark NLP also support reading the license from the Databricks DFS, on the fixed location, dbfs:/FileStore/johnsnowlabs/license.key. The precedence for that location is the highest, so make sure that file is not containing any outdated license key. (OPTIONAL) If the environment variables used to setup the AWS Access/Secret keys are conflicting with the credential provider chain in Databricks, you may not be able to access to other s3 buckets. To access both JSL repos with JSL AWS keys as well as your own s3 bucket with your own AWS keys), you need to use the following script, copy that to dbfs folder, then go to the Databricks console (init scripts menu) to add the init script for your cluster as follows: %scala val script = &quot;&quot;&quot; #!/bin/bash echo &quot;******** Inject Spark NLP AWS Profile Credentials ******** &quot; mkdir ~/.aws/ cat &lt;&lt; EOF &gt; ~/.aws/credentials [spark_nlp] aws_access_key_id=&lt;YOUR_AWS_ACCESS_KEY&gt; aws_secret_access_key=&lt;YOUR_AWS_SECRET_KEY&gt; EOF echo &quot;******** End Inject Spark NLP AWS Profile Credentials ******** &quot; &quot;&quot;&quot; In Libraries tab inside your cluster you need to follow these steps: Lookup the version of Healhcare NLP vs. Spark NLP you will install. Install Spark NLP (Public): New -&gt; PyPI -&gt; spark-nlp==${x.y.z_public_version} -&gt; Install Install: New -&gt; Maven -&gt; Coordinates -&gt; com.johnsnowlabs.nlp:spark-nlp_2.12:${x.y.z_public_version} -&gt; Install Please add following jars: Install: New -&gt; Python Whl -&gt; upload https://pypi.johnsnowlabs.com/${secret.code}/spark-nlp-jsl/spark_nlp_jsl-${x.y.z_healthcare_version}-py3-none-any.whl Install: New -&gt; Jar -&gt; upload https://pypi.johnsnowlabs.com/${secret.code}/spark-nlp-jsl-${x.y.z_healthcare_version}.jar (For Legal and Finance NLP) Install: New -&gt; PyPI -&gt; johnsnowlabs-for-databricks==${x.y.z_healthcare_version} -&gt; Install Now you can attach your notebook to the cluster and use Spark NLP! Windows Support In order to fully take advantage of Spark NLP on Windows (8 or 10), you need to setup/install Apache Spark, Apache Hadoop, Java and a Pyton environment correctly by following the following instructions: https://github.com/JohnSnowLabs/spark-nlp/discussions/1022 How to correctly install Spark NLP on Windows Follow the below steps to set up Spark NLP with Spark 3.1.2: Download Adopt OpenJDK 1.8 Make sure it is 64-bit Make sure you install it in the root of your main drive C: java. During installation after changing the path, select setting Path Download the pre-compiled Hadoop binaries winutils.exe, hadoop.dll and put it in a folder called C: hadoop bin from https://github.com/cdarlint/winutils/tree/master/hadoop-3.2.0/bin Note: The version above is for Spark 3.1.2, which was built for Hadoop 3.2.0. You might have to change the hadoop version in the link, depending on which Spark version you are using. Download Apache Spark 3.1.2 and extract it to C: spark. Set/add environment variables for HADOOP_HOME to C: hadoop and SPARK_HOME to C: spark. Add %HADOOP_HOME% bin and %SPARK_HOME% bin to the PATH environment variable. Install Microsoft Visual C++ 2010 Redistributed Package (x64). Create folders C: tmp and C: tmp hive If you encounter issues with permissions to these folders, you might need to change the permissions by running the following commands: %HADOOP_HOME% bin winutils.exe chmod 777 /tmp/hive %HADOOP_HOME% bin winutils.exe chmod 777 /tmp/ Requisites for PySpark We recommend using conda to manage your python environment on Windows. Download Miniconda for python 3.8 See Quick Install on how to set up a conda environment with Spark NLP. The following environment variables need to be set: PYSPARK_python=python Optionally, if you want to use the Jupyter Notebook runtime of Spark: first install it in the environment with conda install notebook then set PYSPARK_DRIVER_python=jupyter, PYSPARK_DRIVER_python_OPTS=notebook The environment variables can either be directly set in windows, or if only the conda env will be used, with conda env config vars set PYSPARK_python=python. After setting the variable with conda, you need to deactivate and re-activate the environment. Now you can use the downloaded binary by navigating to %SPARK_HOME% bin and running Either create a conda env for python 3.6, install pyspark==3.1.2 spark-nlp numpy and use Jupyter/python console, or in the same conda env you can go to spark bin for pyspark –packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4. Non-johnsnowlabs Clinical NLP on Ubuntu These instructions use non-johnsnowlabs installation syntax. For simplified installation with johnsnowlabs library, check first section. For installing John Snow Labs NLP libraries on an Ubuntu machine/VM please run the following command: wget https://setup.johnsnowlabs.com/nlp/install.sh -O - | sudo bash -s -- -a PATH_TO_LICENSE_JSON_FILE -i -r This script will install Spark NLP, Enterprise Spark NLP, Spark OCR, NLU and Spark NLP Display on the specified virtual environment. It will also create a special folder, ./JohnSnowLabs, dedicated to all resources necessary for using the libraries. Under ./JohnSnowLabs/example_notebooks you will find some ready to use example notebooks that you can use to test the libraries on your data. For a complete step-by-step guide on how to install NLP Libraries check the video below: Install John Snow Labs NLP Libraries on Ubuntu The install script offers several options: -h show brief help -i install mode: create a virtual environment and install the library -r run mode: start jupyter after installation of the library -v path to virtual environment (default: ./sparknlp_env) -j path to license json file for Enterprise Spark NLP -o path to license json file for Spark OCR -a path to a single license json file for both Spark OCR and Spark NLP -s specify pyspark version -p specify port of jupyter notebook Use the -i flag for installing the libraries in a new virtual environment. You can provide the desired path for virtual env using -v flag, otherwise a default location of ./sparknlp_env will be selected. The PATH_TO_LICENSE_JSON_FILE parameter must be replaced with the path where the license file is available on the local machine. According to the libraries you want to use different flags are available: -j, -o or -a. The license files can be easily downloaded from My Subscription section in your my.JohnSnowLabs.com account. To start using Jupyter Notebook after the installation of the libraries use the -r flag. The install script downloads a couple of example notebooks that you can use to start experimenting with the libraries. Those will be availabe under ./JohnSnowLabs/example_notebooks folder. Non-johnsnowlabs Clinical NLP via Docker These instructions use non-johnsnowlabs installation syntax. For simplified installation with johnsnowlabs library, check first section. A docker image that contains all the required libraries for installing and running Enterprise Spark NLP libraries is also available. However, it does not contain the library itself, as it is licensed, and requires installation credentials. Make sure you have a valid license for Enterprise Spark NLP libraries (in case you do not have one, you can ask for a trial here ), and follow the instructions below: Instructions Run the following commands to download the docker-compose.yml and the sparknlp_keys.txt files on your local machine: curl -o docker-compose.yaml https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/blob/513a4d682f11abc33b2e26ef8a9d72ad52a7b4f0/jupyter/docker_image_nlp_hc/docker-compose.yaml curl -o sparknlp_keys.txt https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jupyter/docker_image_nlp_hc/sparknlp_keys.txt Download your license key in json format from my.JohnSnowLabs.com Populate License keys in sparknlp_keys.txt file. Run the following command to run the container in detached mode: docker-compose up -d By default, the jupyter notebook runs on port 8888 - you can access it by typing localhost:8888 in your browser. Troubleshooting Make sure docker is installed on your system. If you face any error while importing the lib inside jupyter, make sure all the credentials are correct in the key files and restart the service again. If the default port 8888 is already occupied by another process, please change the mapping. You can change/adjust volume and port mapping in the docker-compose.yml file. You don’t have a license key? Ask for a trial license here. Non-johnsnowlabs Clinical NLP on python These instructions use non-johnsnowlabs installation syntax. For simplified installation with johnsnowlabs library, check first section. You can install the Clinical NLP by using: pip install -q spark-nlp-jsl==${version} --extra-index-url https://pypi.johnsnowlabs.com/${secret.code} --upgrade {version} is the version part of the {secret.code} ({secret.code}.split(&#39;-&#39;)[0]) (i.e. 2.6.0) The {secret.code} is a secret code that is only available to users with valid/trial license. You can ask for a free trial for Enterprise Spark NLP libraries here. Then, you can obtain the secret code by visiting your account on my.JohnSnowLabs.com. Read more on how to get a license here. Setup AWS-CLI Credentials for licensed pretrained models You need to first set up your AWS credentials to be able to access the private repository for John Snow Labs Pretrained Models. You can do this setup via Amazon AWS Command Line Interface (AWSCLI). Instructions about how to install AWSCLI are available at: Installing the AWS CLI Make sure you configure your credentials with AWS configure following the instructions at: Configuring the AWS CLI Please substitute the ACCESS_KEY and SECRET_KEY with the credentials available on your license json file. This is available on your account from my.JohnSnowLabs.com. Read this for more information. Start Spark NLP Session from python The following will initialize the spark session in case you have run the Jupyter Notebook directly. If you have started the notebook using pyspark this cell is just ignored. Initializing the spark session takes some seconds (usually less than 1 minute) as the jar from the server needs to be loaded. The {secret.code} is a secret code that is only available to users with valid/trial license. You can ask for a free trial for Enterprise Spark NLP here. Then, you can obtain the secret code by visiting your account on my.JohnSnowLabs.com. Read more on how to get a license here. You can either use our convenience function to start your Spark Session that will use standard configuration arguments: import sparknlp_jsl spark = sparknlp_jsl.start(SECRET) Or use the SparkSession module for more flexibility: from pyspark.sql import SparkSession def start(SECRET): builder = SparkSession.builder .appName(&quot;Spark NLP Licensed&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;16G&quot;) .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:&quot;+PUBLIC_VERSION) .config(&quot;spark.jars&quot;, &quot;https://pypi.johnsnowlabs.com/&quot;+SECRET+&quot;/spark-nlp-jsl-&quot;+JSL_VERSION+&quot;.jar&quot;) return builder.getOrCreate() spark = start(SECRET) If you want to download the source files (jar and whl files) locally, you can follow the instructions here. Cheatsheet # Install Spark NLP from PyPI pip install spark-nlp==3.2.3 #install Spark NLP helathcare pip install spark-nlp-jsl==${version} --extra-index-url https://pypi.johnsnowlabs.com/${secret.code} --upgrade # Load Spark NLP with Spark Shell spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.3 --jars spark-nlp-jsl-${version}.jar # Load Spark NLP with PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.3 --jars spark-nlp-jsl-${version}.jar # Load Spark NLP with Spark Submit spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:3.2.3 --jars spark-nlp-jsl-${version}.jar Non-johnsnowlabs Clinical NLP for Scala These instructions use non-johnsnowlabs installation syntax, since johnsnowlabs is a Python library. Use Spark NLP in Spark shell 1.Download the fat jar for Enterprise Spark NLP aws s3 cp --region us-east-2 s3://pypi.johnsnowlabs.com/$jsl_secret/spark-nlp-jsl-$jsl_version.jar spark-nlp-jsl-$jsl_version.jar 2.Set up the Environment Variables box: AWS_ACCESS_KEY_ID=xxx AWS_SECRET_ACCESS_KEY=yyy SPARK_NLP_LICENSE=zzz 3.The preferred way to use the library when running Spark programs is using the --packagesand --jar option as specified in the spark-packages section. spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:${public-version} --jars /spark-nlp-jsl-${version}.jar Non-johnsnowlabs Clinical NLP in Sbt project These instructions use non-johnsnowlabs installation syntax. For simplified installation with johnsnowlabs library, check first section. 1.Download the fat jar for Enterprise Spark NLP. aws s3 cp --region us-east-2 s3://pypi.johnsnowlabs.com/$jsl_secret/spark-nlp-jsl-$jsl_version.jar spark-nlp-jsl-$jsl_version.jar 2.Set up the Environment Variables box: AWS_ACCESS_KEY_ID=xxx AWS_SECRET_ACCESS_KEY=yyy SPARK_NLP_LICENSE=zzz 3.Add the spark-nlp jar in your build.sbt project libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp&quot; % &quot;{public-version}&quot; 4.You need to create the /lib folder and paste the spark-nlp-jsl-${version}.jar file. 5.Add the fat spark-nlp-healthcare in your classpath. You can do it by adding this line in your build.sbt unmanagedJars in Compile += file(&quot;lib/sparknlp-jsl.jar&quot;) Non-johnsnowlabs Clinical NLP on Colab This is the way to run Clinical NLP in Google Colab if you don’t use johnsnowlabs library. Run the following code in Google Colab notebook and start using Spark NLP right away. The first thing that you need is to create the json file with the credentials and the configuration in your local system. { &quot;PUBLIC_VERSION&quot;: &quot;3.2.3&quot;, &quot;JSL_VERSION&quot;: &quot;{version}&quot;, &quot;SECRET&quot;: &quot;{version}-{secret.code}&quot;, &quot;SPARK_NLP_LICENSE&quot;: &quot;xxxxx&quot;, &quot;AWS_ACCESS_KEY_ID&quot;: &quot;yyyy&quot;, &quot;AWS_SECRET_ACCESS_KEY&quot;: &quot;zzzz&quot; } If you have a valid floating license, the license json file can be downloaded from your account on my.JohnSnowLabs.com on My Subscriptions section. To get a trial license please visit Then you need to write that piece of code to load the credentials that you created before. import json import os from google.colab import files license_keys = files.upload() with open(list(license_keys.keys())[0]) as f: license_keys = json.load(f) # Defining license key-value pairs as local variables locals().update(license_keys) # Adding license key-value pairs to environment variables os.environ.update(license_keys) # This is only to setup PySpark and Spark NLP on Colab !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jsl_colab_setup.sh # -p is for pyspark (by default 3.1.1) !bash jsl_colab_setup.sh Spark NLP quick start on Google Colab is a live demo on Google Colab that performs named entity recognitions for HealthCare. Non-johnsnowlabs Clinical NLP on GCP Dataproc These instructions use non-johnsnowlabs installation syntax. For simplified installation with johnsnowlabs library, check first section. You can follow the steps here for installation via IU Create a cluster if you don’t have one already as follows. At gcloud shell: gcloud services enable dataproc.googleapis.com compute.googleapis.com storage-component.googleapis.com bigquery.googleapis.com bigquerystorage.googleapis.com REGION=&lt;region&gt; BUCKET_NAME=&lt;bucket_name&gt; gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME} REGION=&lt;region&gt; ZONE=&lt;zone&gt; CLUSTER_NAME=&lt;cluster_name&gt; BUCKET_NAME=&lt;bucket_name&gt; You can set image-version, master-machine-type, worker-machine-type, master-boot-disk-size, worker-boot-disk-size, num-workers as your needs. If you use the previous image-version from 2.0, you should also add ANACONDA to optional-components. And, you should enable gateway. As noticed below, you should explicitly write JSL_SECRET and JSL_VERSION at metadata param inside the quotes. This will start the pip installation using the wheel file of Licensed SparkNLP! gcloud dataproc clusters create ${CLUSTER_NAME} --region=${REGION} --network=${NETWORK} --zone=${ZONE} --image-version=2.0 --master-machine-type=n1-standard-4 --worker-machine-type=n1-standard-2 --master-boot-disk-size=128GB --worker-boot-disk-size=128GB --num-workers=2 --bucket=${BUCKET_NAME} --optional-components=JUPYTER --enable-component-gateway --metadata &#39;PIP_PACKAGES=google-cloud-bigquery google-cloud-storage spark-nlp-display https://s3.eu-west-1.amazonaws.com/pypi.johnsnowlabs.com/JSL_SECRET/spark-nlp-jsl/spark_nlp_jsl-JSL_VERSION-py3-none-any.whl&#39; --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh On an existing one, you need to install spark-nlp and spark-nlp-display packages from PyPI. Now, you can attach your notebook to the cluster and use Spark NLP via following the instructions. The key part of this usage is how to start SparkNLP sessions using Apache Hadoop YARN cluster manager. 3.1. Read license file from the notebook using GCS. 3.2. Set the right path of the Java Home Path. 3.3. Use the start function to start the SparkNLP JSL version such as follows: def start(secret): builder = SparkSession.builder .appName(&quot;Spark NLP Licensed&quot;) .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:&quot;+PUBLIC_VERSION) .config(&quot;spark.jars&quot;, &quot;https://pypi.johnsnowlabs.com/&quot;+SECRET+&quot;/spark-nlp-jsl-&quot;+JSL_VERSION+&quot;.jar&quot;) return builder.getOrCreate() spark = start(SECRET) As you see, we did not set .master(&#39;local[*]&#39;) explicitly to let YARN manage the cluster. Or you can set .master(&#39;yarn&#39;). Non-johnsnowlabs Clinical NLP on AWS SageMaker These instructions use non-johnsnowlabs installation syntax. For simplified installation with johnsnowlabs library, check first section. Access AWS Sagemaker in AWS. Go to Notebook -&gt; Notebook Instances. Create a new Notebook Instance, follow this Instructions Steps Minimum requirement 16G RAM and 50G Volume. This is the configuration we have used, although most of the interesting models will require a ml.t3.xlarge instance or more. Reserve at least 50GB of memory Once created, open JupyterLab and use Conda python 3 kernel. Upload license key and set Environment Variables. import json import os with open(&#39;spark_nlp_for_healthcare.json&#39;, &#39;r&#39;) as f: for k, v in json.load(f).items(): %set_env $k=$v %set_env PYSPARK=3.2.2 %set_env SPARK_HOME=/home/ec2-user/SageMaker/spark-3.2.2-bin-hadoop2.7 Download and install libraries !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jsl_sagemaker_setup.sh !bash jsl_sagemaker_setup.sh Import libraries and start session import sparknlp import sparknlp_jsl from pyspark.sql import SparkSession spark = sparknlp_jsl.start(license_keys[&#39;SECRET&#39;]) Non-johnsnowlabs Clinical NLP with Poetry These instructions use non-johnsnowlabs installation syntax. For simplified installation with johnsnowlabs library, check first section. This is a sample project.toml file which you can use with poetry install to setup spark NLP + the Healthcare python library spark-nlp-jsl You need to point it to either the tar.gz or .whl file which are hosted at https://pypi.johnsnowlabs.com/&lt;SECRET&gt;/spark-nlp-jsl/ NOTE You must update the url whenever you are upgrading your spark-nlp-jsl version [tool.poetry] name = &quot;poertry_demo&quot; version = &quot;0.1.0&quot; description = &quot;&quot; authors = [&quot;person &lt;person@gmail.com&gt;&quot;] [tool.poetry.dependencies] python = &quot;^3.7&quot; [tool.poetry.dev-dependencies] spark-nlp = &quot;3.4.4&quot; spark-nlp-jsl = { url = &quot;https://pypi.johnsnowlabs.com/SECRET/spark-nlp-jsl/spark_nlp_jsl-tar.gz_OR_.whl&quot; } [build-system] requires = [&quot;poetry-core&gt;=1.0.0&quot;] build-backend = &quot;poetry.core.masonry.api&quot; Non-johnsnowlabs Clinical NLP on AWS EMR These instructions use non-johnsnowlabs installation syntax. For simplified installation with johnsnowlabs library, check first section. In this page we explain how to setup Spark-NLP + Spark-NLP Healthcare in AWS EMR, using the AWS console. Steps You must go to the blue button “Create Cluster” on the UI. By doing that you will get directed to the “Create Cluster - Quick Options” page. Don’t use the quick options, click on “Go to advanced options” instead. Now in Advanced Options, on Step 1, “Software and Steps”, please pick the following selection in the checkboxes, Also in the “Edit Software Settings” page, enter the following, [{ &quot;Classification&quot;: &quot;spark-env&quot;, &quot;Configurations&quot;: [{ &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;PYSPARK_python&quot;: &quot;/usr/bin/python3&quot;, &quot;AWS_ACCESS_KEY_ID&quot;: &quot;XYXYXYXYXYXYXYXYXYXY&quot;, &quot;AWS_SECRET_ACCESS_KEY&quot;: &quot;XYXYXYXYXYXYXYXYXYXY&quot;, &quot;SPARK_NLP_LICENSE&quot;: &quot;XYXYXYXYXYXYXYXYXYXYXYXYXYXY&quot; } }] }, { &quot;Classification&quot;: &quot;spark-defaults&quot;, &quot;Properties&quot;: { &quot;spark.yarn.stagingDir&quot;: &quot;hdfs:///tmp&quot;, &quot;spark.yarn.preserve.staging.files&quot;: &quot;true&quot;, &quot;spark.kryoserializer.buffer.max&quot;: &quot;2000M&quot;, &quot;spark.serializer&quot;: &quot;org.apache.spark.serializer.KryoSerializer&quot;, &quot;spark.driver.maxResultSize&quot;: &quot;0&quot;, &quot;spark.driver.memory&quot;: &quot;32G&quot; } }] Make sure that you replace all the secret information(marked here as XYXYXYXYXY) by the appropriate values that you received with your license. In “Step 2” choose the hardware and networking configuration you prefer, or just pick the defaults. Move to next step by clocking the “Next” blue button. Now you are in “Step 3”, in which you assign a name to your cluster, and you can change the location of the cluster logs. If the location of the logs is OK for you, take note of the path so you can debug potential problems by using the logs. Still on “Step 3”, go to the bottom of the page, and expand the “Bootstrap Actions” tab. We’re gonna add an action to execute during bootstrap of the cluster. Select “Custom Action”, then press on “Configure and add”. You need to provide a path to a script on S3. The path needs to be public. Keep this in mind, no secret information can be contained there. The script we’ll used for this setup is emr_bootstrap.sh . This script will install Spark-NLP 3.1.0, and Spark-NLP Healthcare 3.1.1. You’ll have to edit the script if you need different versions. After you entered the route to S3 in which you place the emr_bootstrap.sh file, and before clicking “add” in the dialog box, you must pass an additional parameter containing the SECRET value you received with your license. Just paste the secret on the “Optional arguments” field in that dialog box. There’s not much additional setup you need to perform. So just start a notebook server, connect it to the cluster you just created(be patient, it takes a while), and test with the NLP_EMR_Setup.ipynb test notebook. Non-johnsnowlabs Clinical NLP on Amazon Linux 2 These instructions use non-johnsnowlabs installation syntax. For simplified installation with johnsnowlabs library, check first section. # Update Package List &amp; Install Required Packages sudo yum update sudo yum install -y amazon-linux-extras sudo yum -y install python3-pip # Create python virtual environment and activate it: python3 -m venv .sparknlp-env source .sparknlp-env/bin/activate Check JAVA version: For Sparknlp versions above 3.x, please use JAVA-11 For Sparknlp versions below 3.x and SparkOCR, please use JAVA-8 Checking Java versions installed on your machine: sudo alternatives --config java You can pick the index number (I am using java-8 as default - index 2): If you dont have java-11 or java-8 in you system, you can easily install via: sudo yum install java-1.8.0-openjdk Now, we can start installing the required libraries: pip install jupyter We can start jupyter notebook via: jupyter notebook ### Now we are in the jupyter notebook cell: import json import os with open(&#39;sparknlp_for_healthcare.json) as f: license_keys = json.load(f) # Defining license key-value pairs as local variables locals().update(license_keys) # Adding license key-value pairs to environment variables os.environ.update(license_keys) # Installing pyspark and spark-nlp ! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION # Installing Spark NLP Healthcare ! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION --extra-index-url https://pypi.johnsnowlabs.com/$SECRET Fancy trying? You can ask for a free trial for Enterprise Spark NLP here. This will automatically create a new account for you on my.JohnSnowLabs.com. Login in to your new account and from My Subscriptions section, you can download your license key as a json file. The license json file contains: the secrets for installing the Enterprise Spark NLP and Spark OCR libraries, the license key as well as AWS credentials that you need to access the s3 bucket where the healthcare models and pipelines are published. If you have asked for a trial license, but you cannot access your account on my.JohnSnowLabs.com and you did not receive the license information via email, please contact us at support@johnsnowlabs.com.",
    "url": "/docs/en/licensed_install",
    "relUrl": "/docs/en/licensed_install"
  },
  "91": {
    "id": "91",
    "title": "Licensed Models",
    "content": "Pretrained Models We are currently in the process of moving the pretrained models and pipelines to a Model Hub that you can explore here: Models Hub",
    "url": "/docs/en/licensed_models",
    "relUrl": "/docs/en/licensed_models"
  },
  "92": {
    "id": "92",
    "title": "Spark NLP for Healthcare Release Notes",
    "content": "4.3.0 Highlights 12 new clinical models and pipelines added &amp; updated (8 new clinical named entity recognition models including 4 social determinants of health models) New Chunk Mapper model for mapping RxNorm codes to drug brand names New text classification annotators (architectures) for training text classification models using SVM and Logistic Regression with sentence embeddings One-liner clinical deidentification module Certification_Training notebooks (written in johnsnowlabs library) moved to parent workshop folder Different validation split per epoch in MedicalNerApproach Core improvements and bug fixes New read_conll method for reading conll files as Conll.readDataset does but it returns pandas dataframe with document(task) ids. Updated documentation Allow using FeatureAssembler in pretrained pipelines. Fixed RelationExtractionModel running in LightPipeline Fixed get_conll_data method issue New and updated notebooks New Clinical Deidentification Utility Module Notebook. Updated Clinical_Named_Entity_Recognition_Model with Conll.readDataset examples. Updated Clinical Text Classification with Spark NLP with new GenericLogRegClassifierApproach and GenericSVMClassifierApproach examples. New and updated demos SOCIAL DETERMINANT NER demo SOCIAL DETERMINANT CLASSIFICATION demo SOCIAL DETERMINANT GENERIC CLASSIFICATION demo 13 new clinical models and pipelines added &amp; updated in total 12 New Clinical Models And Pipelines Added &amp; Updated (8 New Clinical Named Entity Recognition Models Including 4 Social Determinants of Health Models) We are releasing 4 new SDOH NER models that were trained by using embeddings_clinical embeddings model. model name description predicted entities ner_sdoh_wip Extracts terminology related to Social Determinants of Health from various kinds of biomedical documents. Other_SDoH_Keywords Education Population_Group Quality_Of_Life Housing Substance_Frequency Smoking Eating_Disorder Obesity Healthcare_Institution Financial_Status Age Chidhood_Event Exercise Communicable_Disease Hypertension Other_Disease Violence_Or_Abuse Spiritual_Beliefs Employment Social_Exclusion Access_To_Care Marital_Status Diet Social_Support Disability Mental_Health Alcohol Insurance_Status Substance_Quantity Hyperlipidemia Family_Member Legal_Issues Race_Ethnicity Gender Geographic_Entity Sexual_Orientation Transportation Sexual_Activity Language Substance_Use ner_sdoh_social_environment_wip Extracts social environment terminologies related to Social Determinants of Health from various kinds of biomedical documents. Social_Support Chidhood_Event Social_Exclusion Violence_Abuse_Legal ner_sdoh_demographics_wip Extracts demographic information related to Social Determinants of Health from various kinds of biomedical documents. Family_Member Age Gender Geographic_Entity Race_Ethnicity Language Spiritual_Beliefs ner_sdoh_income_social_status_wip Extracts income and social status information related to Social Determinants of Health from various kinds of biomedical documents. Education Marital_Status Financial_Status Population_Group Employment Example: ... clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) sample_texts =&quot;Smith is a 55 years old, divorced Mexcian American woman with financial problems. She speaks spanish. She lives in an apartment. She has been struggling with diabetes for the past 10 years and has recently been experiencing frequent hospitalizations due to uncontrolled blood sugar levels. Smith works as a cleaning assistant and does not have access to health insurance or paid sick leave. She has a son student at college. Pt with likely long-standing depression. She is aware she needs rehab. Pt reprots having her catholic faith as a means of support as well. She has long history of etoh abuse, beginning in her teens. She reports she has been a daily drinker for 30 years, most recently drinking beer daily. She smokes a pack of cigarettes a day. She had DUI back in April and was due to be in court this week.&quot; Result: ++--++-+ |chunk |begin|end|ner_label | ++--++-+ |55 years old |11 |22 |Age | |divorced |25 |32 |Marital_Status | |Mexcian American |34 |49 |Race_Ethnicity | |financial problems|62 |79 |Financial_Status | |spanish |93 |99 |Language | |apartment |118 |126|Housing | |diabetes |158 |165|Other_Disease | |cleaning assistant|307 |324|Employment | |health insurance |354 |369|Insurance_Status | |son |401 |403|Family_Member | |student |405 |411|Education | |college |416 |422|Education | |depression |454 |463|Mental_Health | |rehab |489 |493|Access_To_Care | |catholic faith |518 |531|Spiritual_Beliefs | |support |547 |553|Social_Support | |etoh abuse |589 |598|Alcohol | |teens |618 |622|Age | |drinker |658 |664|Alcohol | |drinking beer |694 |706|Alcohol | |daily |708 |712|Substance_Frequency| |smokes |719 |724|Smoking | |a pack |726 |731|Substance_Quantity | |cigarettes |736 |745|Smoking | |a day |747 |751|Substance_Frequency| |DUI |762 |764|Legal_Issues | ++--++-+ We are releasing 8 new NER models which are trained by European Clinical Case Corpus (E3C), a project aimed at offering a freely available multilingual corpus of semantically annotated clinical narratives. ner_eu_clinical_case: This model extracts 6 different clinical entities based on medical taxonomies. ner_eu_clinical_condition: This model extracts one entity – clinical / medical conditions. model name lang predicted entities ner_eu_clinical_case es clinical_condition clinical_event bodypart units_measurements patient date_time ner_eu_clinical_case fr clinical_condition clinical_event bodypart units_measurements patient date_time ner_eu_clinical_case eu clinical_condition clinical_event bodypart units_measurements patient date_time ner_eu_clinical_condition en clinical_condition ner_eu_clinical_condition es clinical_condition ner_eu_clinical_condition eu clinical_condition ner_eu_clinical_condition fr clinical_condition ner_eu_clinical_condition it clinical_condition Example: word_embeddings = WordEmbeddingsModel.pretrained(&quot;w2v_cc_300d&quot;,&quot;es&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner = MedicalNerModel.pretrained(&quot;ner_eu_clinical_case&quot;, &quot;es&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) sample_text = &quot;&quot;&quot;Paciente de 59 años que refiere dificultad para caminar desde hace un mes aproximadamente. Presenta debilidad y dolor en los miembros inferiores, que mejora tras detenerse, acompañándose en ocasiones de lumbalgia no irradiada. En la exploración neurológica presenta habla hipofónica, facial centrado. Debido a la mala perfusión secundaria a la sepsis aparecieron lesiones necróticas en extremidades superiores y principalmente inferiores distales. Motilidad ocular interna y externa normal.&quot;&quot;&quot; Result: +++ |chunk |ner_label | +++ |Paciente de 59 años |patient | |refiere |clinical_event | |dificultad para caminar |clinical_event | |hace un mes aproximadamente|date_time | |debilidad |clinical_event | |dolor |clinical_event | |los miembros inferiores |bodypart | |mejora |clinical_event | |detenerse |clinical_event | |lumbalgia |clinical_event | |irradiada |clinical_event | |exploración |clinical_event | |habla |clinical_event | |hipofónica |clinical_event | |perfusión |clinical_event | |sepsis |clinical_event | |lesiones |clinical_event | |extremidades superiores |bodypart | |inferiores distales |bodypart | |Motilidad |clinical_event | |normal |units_measurements| +++ New Chunk Mapper Model for Mapping RxNorm Codes to Drug Brand Names We are releasing rxnorm_drug_brandname_mapper pretrained model that maps RxNorm and RxNorm Extension codes with their corresponding drug brand names. It returns 2 types of brand names called rxnorm_brandname and rxnorm_extension_brandname for the corresponding RxNorm or RxNorm Extension code. Example: ... chunkerMapper = ChunkMapperModel.pretrained(&quot;rxnorm_drug_brandname_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;rxnorm_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRels([&quot;rxnorm_brandname&quot;, &quot;rxnorm_extension_brandname&quot;]) sample_text= [&#39;metformin&#39;, &#39;advil&#39;] Result: +--+-+--+--+ | drug_name|rxnorm_result| mapping_result| relation | +--+-+--+--+ | metformin| 6809|Actoplus Met (metformin):::Avandamet (metformin...| rxnorm_brandname| | metformin| 6809|A FORMIN (metformin):::ABERIN MAX (metformin)::...|rxnorm_extension_brandname| | advil| 153010| Advil (Advil)| rxnorm_brandname| | advil| 153010| NONE|rxnorm_extension_brandname| +--+-+--+--+ New Text Classification Annotators (Architectures) For Training Text Classification Models Using SVM and Logistic Regression With Sentence Embeddings We have a new text classification architecture called GenericLogRegClassifierApproach that implements a multinomial Logistic Regression with sentence embeddings. This is a single layer neural network with the logistic function at the output. The input to the model is FeatureVector (from any sentence embeddings) and the output is Category annotations with labels and corresponding confidence scores. Training data requires “text” and their “label” columns only and the trained model will be a GenericLogRegClassifierModel(). We have another text classification architecture called GenericSVMClassifierApproach that implements SVM (Support Vector Machine) classification. The input to the model is FeatureVector (from any sentence embeddings) and the output is Category annotations with labels and corresponding confidence scores. Taining data requires “text” and their “label” columns only and the trained model will be a GenericSVMClassifierModel(). Input types: FEATURE_VECTOR Output type: CATEGORY Example: features_asm = sparknlp_jsl.base.FeaturesAssembler() .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCol(&quot;feature_vector&quot;) gcf_graph_builder = sparknlp_jsl.annotators.TFGraphBuilder() .setModelName(&quot;logreg_classifier&quot;) .setInputCols([&quot;feature_vector&quot;]) .setLabelColumn(&quot;label&quot;) .setGraphFolder(&quot;/tmp/&quot;) .setGraphFile(&quot;log_reg_graph.pb&quot;) log_reg_approach = sparknlp_jsl.annotators.GenericLogRegClassifierApproach() .setLabelColumn(&quot;label&quot;) .setInputCols([&quot;feature_vector&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(f&quot;/tmp/log_reg_graph.pb&quot;) .setEpochsNumber(10) .setBatchSize(1) .setLearningRate(0.001) One-Liner Clinical Deidentification Module Spark NLP for Healthcare provides functionality to apply Deidentification using one-liner module called Deid. The Deid module is a tool for deidentifying Protected Health Information (PHI) from data in a file path. It can be used with or without ant Spark NLP NER pipelines. It can apply deidentification and obfuscation on different columns at the same time. It returns the deidentification &amp; obfuscation results as a spark dataframe as well as a csv or json file saved locally. The module also includes functionality for applying Structured Deidentification task to data from a file path. The function, deidentify(), can be used with a custom pipeline or without defining any custom pipeline. structured_deidentifier() function can be used for the Structured Deidentification task. Please see this notebook for the detailed usage and explanation of all parameters. Check here for the documentation of the module. Deidentification with a custom pipeline Example: from sparknlp_jsl import Deid deid_implementor= Deid( # required: Spark session with spark-nlp-jsl jar spark ) res= deid_implementor.deidentify( # required: The path of the input file. Default is None. File type must be &#39;csv&#39; or &#39;json&#39;. input_file_path=&quot;data.csv&quot;, #optional: The path of the output file. Default is &#39;deidentified.csv&#39;. File type must be &#39;csv&#39; or &#39;json&#39;. output_file_path=&quot;deidentified.csv&quot;, #optional: The separator of the input csv file. Default is &quot; t&quot;. separator=&quot;,&quot;, #optional: A custom pipeline model to be used for deidentification. If not specified, the default is None. custom_pipeline=nlpModel, #optional: Fields to be deidentified and their deidentification modes, by default {&quot;text&quot;: &quot;mask&quot;} fields={&quot;text_column_1&quot;: &quot;text_column_1_deidentified&quot;, &quot;text_column_2&quot;: &quot;text_column_2_deidentified&quot;}, #optional: The masking policy. Default is &quot;entity_labels&quot;. masking_policy=&quot;fixed_length_chars&quot;, #optional: The fixed mask length. Default is 4. fixed_mask_length=4) Result: ++-+-+-+-+ | ID| text_column_1| text_column_1_deidentified| text_column_2| text_column_2_deidentified| ++-+-+-+-+ | 0|Record date : 2093-01-13 , David Hale , M.D . , Name : Hendrickson ...|Record date : ** , ** , M.D . , Name : ** MR .|Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-...|Date : 10-16-1991 PCP : Alveda Castles , 26 years-old , Record date...| ++-+-+-+-+ Deidentification with no custom pipeline Example: from sparknlp_jsl import Deid deid_implementor= Deid( # required: Spark session with spark-nlp-jsl jar spark ) res= deid_implementor.deidentify( # required: The path of the input file. Default is None. File type must be &#39;csv&#39; or &#39;json&#39;. input_file_path=&quot;data.csv&quot;, #optional: The path of the output file. Default is &#39;deidentified.csv&#39;. File type must be &#39;csv&#39; or &#39;json&#39;. output_file_path=&quot;deidentified.csv&quot;, #optional: The separator of the input csv file. Default is &quot; t&quot;. separator=&quot;,&quot;, #optional: Fields to be deidentified and their deidentification modes, by default {&quot;text&quot;: &quot;mask&quot;} fields={&quot;text&quot;: &quot;mask&quot;}, #optional: The masking policy. Default is &quot;entity_labels&quot;. masking_policy=&quot;entity_labels&quot;) Result: ++-+-+ | ID| text_original| text_deid| ++-+-+ | 0| &quot;| &quot;| | 1|Record date : 2093-01-13 , David Hale , M.D . , Name : Hendrickson ...|Record date : &lt;DATE&gt; , &lt;DOCTOR&gt; , M.D . , Name : &lt;PATIENT&gt; , MR # &lt;...| | 2| &quot;| &quot;| ++-+-+ Structured Deidentification Example: from sparknlp_jsl import Deid deid_implementor= Deid( # required: Spark session with spark-nlp-jsl jar spark ) res= deid_implementor.structured_deidentifier( #required: The path of the input file. Default is None. File type must be &#39;csv&#39; or &#39;json&#39;. input_file_path=&quot;data.csv&quot;, #optional: The path of the output file. Default is &#39;deidentified.csv&#39;. File type must be &#39;csv&#39; or &#39;json&#39;. output_file_path=&quot;deidentified.csv&quot;, #optional: The separator of the input csv file. Default is &quot; t&quot;. separator=&quot;,&quot;, #optional: A dictionary that contains the column names and the tags that should be used for deidentification. Default is {&quot;NAME&quot;:&quot;PATIENT&quot;,&quot;AGE&quot;:&quot;AGE&quot;} columns_dict= {&quot;NAME&quot;: &quot;ID&quot;, &quot;DOB&quot;: &quot;DATE&quot;}, #optional: The seed value for the random number generator. Default is {&quot;NAME&quot;: 23, &quot;AGE&quot;: 23} columns_seed= {&quot;NAME&quot;: 23, &quot;DOB&quot;: 23}, #optional: The source of the reference file. Default is faker. ref_source=&quot;faker&quot;, #optional: The number of days to be shifted. Default is None shift_days=5) Result: +-++--++-+ | NAME| DOB| ADDRESS|SBP| TEL| +-++--++-+ |[N2649912]|[18/02/1977]| 711 Nulla St.|140| 673 431234| | [W466004]|[28/02/1977]| 1 Green Avenue.|140|+23 (673) 431234| | [M403810]|[16/04/1900]|Calle del Liberta...|100| 912 345623| +-++--++-+ Different Validation Split Per Epoch In MedicalNerApproach The validation splits in MedicalNerApproach used to be static and same for every epoch. Now we can control with behaviour with a new parameter called setRandomValidationSplitPerEpoch(bool) and allow users to set random validation splits per epoch. Certification_Training Notebooks (Written In Johnsnowlabs Library) Moved to Parent Workshop Folder re-organize and re-locate open-source-nlp folder re-organize and re-locate healthcare-nlp folder Core Improvements and Bug Fixes New read_conll method for reading conll files as Conll.readDataset does but it returns dataframe with document(task) ids. Updated documentation Allow using FeatureAssembler in pretrained pipelines. Fixed RelationExtractionModel running in LightPipeline Fixed get_conll_data method issue New and Updated Notebooks New Clinical Deidentification Utility Module Notebook. Updated Clinical_Named_Entity_Recognition_Model with Conll.readDataset examples. Updated Clinical Text Classification with Spark NLP with new GenericLogRegClassifierApproach and GenericSVMClassifierApproach examples. New and Updated Demos SOCIAL DETERMINANT NER demo SOCIAL DETERMINANT CLASSIFICATION demo SOCIAL DETERMINANT GENERIC CLASSIFICATION demo 12 New Clinical Models and Pipelines Added &amp; Updated in Total ner_eu_clinical_case-&gt; es ner_eu_clinical_case-&gt; fr ner_eu_clinical_case-&gt; eu ner_eu_clinical_condition-&gt; en ner_eu_clinical_condition-&gt; es ner_eu_clinical_condition-&gt; fr ner_eu_clinical_condition-&gt; eu ner_eu_clinical_condition-&gt; it ner_sdoh_demographics_wip ner_sdoh_income_social_status_wip ner_sdoh_social_environment_wip ner_sdoh_wip rxnorm_drug_brandname_mapper For all Spark NLP for Healthcare models, please check: Models Hub Page Previous versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/licensed_release_notes",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/licensed_release_notes"
  },
  "93": {
    "id": "93",
    "title": "Serving Spark NLP&#58 MLFlow on Databricks",
    "content": "This is the first article of the “Serving Spark NLP via API” series, showcasing how to serve Spark NLP using Databricks Jobs and MLFlow Serve APIs. Don’t forget to check the other articles in this series, namely: How to serve Spark NLP using Microsoft Synapse ML, available here. How to server Spark NLP using FastAPI and LightPipelines, available here. Background Spark NLP is a Natural Language Understanding Library built on top of Apache Spark, leveranging Spark MLLib pipelines, that allows you to run NLP models at scale, including SOTA Transformers. Therefore, it’s the only production-ready NLP platform that allows you to go from a simple PoC on 1 driver node, to scale to multiple nodes in a cluster, to process big amounts of data, in a matter of minutes. Before starting, if you want to know more about all the advantages of using Spark NLP (as the ability to work at scale on air-gapped environments, for instance) we recommend you to take a look at the following resources: John Snow Labs webpage; The official technical documentation of Spark NLP; Spark NLP channel on Medium; Also, follow Veysel Kocaman, Data Scientist Lead and Head of Spark NLP for Healthcare, for the latests tips. Motivation Spark NLP is server-agnostic, what means it does not come with an integrated API server, but offers a lot of options to serve NLP models using Rest APIs. There is a wide range of possibilities to add a web server and serve Spark NLP pipelines using RestAPI, and in this series of articles we are only describing some of them. Let’s have an overview of how to use Databricks Jobs API and MLFlow Serve as an example for that purpose. Databricks Jobs and MLFlow Serve APIs About Databricks Databricks is an enterprise software company founded by the creators of Apache Spark. The company has also created MLflow, the Serialization and Experiment tracking library you can use (inside or outside databricks), as described in the section “Experiment Tracking”. Databricks develops a web-based platform for working with Spark, that provides automated cluster management and IPython-style notebooks. Their infrastructured is provided for training and production purposes, and is integrated in cloud platforms as Azure and AWS. Spark NLP is a proud partner of Databricks and we offer a seamless integration with them — see Install on Databricks. All Spark NLP capabilities run in Databricks, including MLFlow serialization and Experiment tracking, what can be used for serving Spark NLP for production purposes. About MLFlow MLFlow is a serialization and Experiment Tracking platform, which also natively supports Spark NLP. We have a documentation entry about MLFlow in the “Experiment Tracking” section. It’s highly recommended that you take a look before moving forward in this document, since we will use some of the concepts explained there. We will use MLFlow serialization to serve our Spark NLP models. Strengths Easily configurable and scalable clusters in Databricks Seamless integration of SPark NLP and Databricks for automatically creating Spark NLP clusters (check Install on Databricks URL) Integration with MLFlow, experiment tracking, etc. Configure your training and serving environments separately. Use your serving environment for inference and scale it as you need. Weaknesses This approach does not allow you to customize your endpoints, it uses Databricks JOBS API ones Requires some time and expertise in Databricks to configure everything properly Creating a cluster in Databricks As mentioned before, Spark NLP offers a seamless integration with Databricks. To create a cluster, please follow the instructions in Install on Databricks. That cluster can be then replicated (cloned) for production purposes later on. Configuring Databricks for serving Spark NLP on MLFlow In Databricks Runtime Version, select any Standard runtime, not ML ones… These add their version of MLFlow, and some incompatibilities may arise. For this example, we have used 8.3 (includes Apache Spark 3.1.1, Scala 2.12) The cluster instantiated is prepared to use Spark NLP, but to make it production-ready using MLFlow, we need to add the MLFlow jar, in addition to the Spark NLP jar, as shown in the “Experiment Tracking” section. In that case, we did it adding both jars… (&quot;spark.jars.packages&quot;:&quot; com.johnsnowlabs.nlp:spark-nlp_2.12:[YOUR_SPARKNLP_VERSION],org.mlflow:mlflow-spark:1.21.0&quot;) …into the SparkSession. However, in Databricks, you don’t instantiate programmatically a session, but you configure it in the Compute screen, selecting your Spark NLP cluster, and then going to Configuration -&gt; Advanced Options -&gt; Spark -&gt; Spark Config, as shown in the following image: In addition to Spark Config, we need to add the Spark NLP and MLFlow libraries to the Cluster. You can do that by going to Libraries inside your cluster. Make sure you have spark-nlp and mlflow. If not, you can install them either using PyPI or Maven artifacts. In the image below you can see the PyPI alternative: TIP: You can also use the Libraries section to add the jars (using Maven Coordinates) instead of setting them in the Spark Config, as showed before. Creating a notebook You are ready to create a notebook in Databricks and attach it to the recently created cluster. To do that, go to Create --&gt; Notebook, and select the cluster you want in the dropdown above your notebook. Make sure you have selected the cluster with the right Spark NLP + MLFlow configuration. To check everything is ok, run the following lines: To check the session is running: spark To check jars are in the session: spark.sparkContext.getConf().get(&#39;spark.jars.packages&#39;) You should see the following output from the last line (versions may differ depending on which ones you used to configure your cluster) Out[2]: &#39;com.johnsnowlabs.nlp:spark-nlp_2.12:[YOUR_SPARKNLP_VERSION],org.mlflow:mlflow-spark:1.21.0&#39; Logging the experiment in Databricks using MLFlow As explained in the “Experiment Tracking” section, MLFlow can log Spark MLLib / NLP Pipelines as experiments, to carry out runs on them, track versions, etc. MLFlow is natively integrated in Databricks, so we can leverage the mlflow.spark.log_model() function of the Spark flavour of MLFlow, to start tracking our Spark NLP pipelines. Let’s first import our libraries: import mlflow import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import pandas as pd from sparknlp.training import CoNLL import pyspark from pyspark.sql import SparkSession Then, create a Lemmatization pipeline: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = LemmatizerModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;prediction&quot;) # It&#39;s mandatory to call it prediction pipeline = Pipeline(stages=[ documentAssembler, tokenizer, lemmatizer ]) IMPORTANT: Last output column of the last component in the pipeline should be called prediction. Finally, let’s log the experiment. In the Experiment Tracking section, we used the pip_requirements parameter in the log_model() function to set the required libraries: But we mentioned using conda is also available. Let’s use conda in this example: conda_env = { &#39;channels&#39;: [&#39;conda-forge&#39;], &#39;dependencies&#39;: [ &#39;python=3.8.8&#39;, { &quot;pip&quot;: [ &#39;pyspark==3.1.1&#39;, &#39;mlflow==1.21.0&#39;, &#39;spark-nlp==[YOUR_SPARKNLP_VERSION]&#39; ] } ], &#39;name&#39;: &#39;mlflow-env&#39; } With this conda environment, we are ready to log our pipeline: mlflow.spark.log_model(p_model, &quot;lemmatizer&quot;, conda_env=conda_env) You should see an output similar to this one: (6) Spark Jobs (1) MLflow run *Logged 1 run to an experiment in MLflow. Learn more* Experiment UI On the top right corner of your notebook, you will see the Experiment widget, and inside, as shown in the image below. You can also access Experiments UI if you switch your environment from “Data Science &amp; Engineering” to “Machine Learning”, on the left panel… Once in the experiment UI, you will see the following screen, where your experiments are tracked. If you click on the Start Time cell of your experiment, you will reach the registered MLFlow run. On the left panel you will see the MLFlow model and some other artifacts, as the conda.yml and pip_requirements.txt that manage the dependencies of your models. On the right panel, you will see two snippets, about how to call to the model for inference internally from Databricks. Snippet for calling with a Pandas Dataframe: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; Load model as a Spark UDF. loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model) Predict on a Spark DataFrame. columns = list(df.columns) df.withColumn(&#39;predictions&#39;, loaded_model(*columns)).collect() Snippet for calling with a Spark Dataframe. We won’t include it in this documentation because that snippet does not include SPark NLP specificities. To make it work, the correct snippet should be: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model) ### Predict on a Spark DataFrame. res_spark = loaded_model.predict(df_1_spark.rdd) IMPORTANT: You will only get the last column (prediction) results, which is a list of Rows of Annotation Types. To convert the result list into a Spark Dataframe, use the following schema: import pyspark.sql.types as T import pyspark.sql.functions as f annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) And then, get the results (for example, in res_spark) and apply the schema: spark_res = spark.createDataFrame(res_pandas[0], schema=annotationType) Calling the experiment for production purposes using MLFlow Rest API Instead of choosing a Batch Inference, you can select REST API. This will lead you to another screen, when the model will be loaded for production purposes in an independent cluster. Once deployed, you will be able to: Check the endpoint URL to consume the model externally; Test the endpoint writing a json (in our example, ‘text’ is our first input col of the pipeline, so it shoud look similar to: {&quot;text&quot;: &quot;This is a test of how the lemmatizer works&quot;} You can see the response in the same screen. Check what is the Python code or cURL command to do that very same thing programatically. By just using that Python code, you can already consume it for production purposes from any external web app. IMPORTANT: As per 17/02/2022, there is an issue being studied by Databricks team, regarding the creation on the fly of job clusters to serve MLFlow models that require configuring the Spark Session with specific jars. This will be fixed in later versions of Databricks. In the meantime, the way to go is using Databricks Jobs API. Calling the experiment for production purposes using Databricks Asynchronous Jobs API Creating the notebook for the inference job And last, but not least, another approach to consume models for production purposes. the Jobs API. Databricks has its own API for managing jobs, that allows you to instantiate any notebook or script as a job, run it, stop it, and manage all the life cycle. And you can configure the cluster where this job will run before hand, what prevents having the issue described in point 3. To do that: Create a new production cluster, as described before, cloning you training environment but adapting it to your needs for production purposes. Make sure the Spark Config is right, as described at the beginning of this documentation. Create a new notebook. Always check that the jars are in the session: spark.sparkContext.getConf().get(&#39;spark.jars.packages&#39;) Out[2]: &#39;com.johnsnowlabs.nlp:spark-nlp_2.12:[YOUR_SPARKNLP_VERSION],org.mlflow:mlflow-spark:1.21.0&#39; Add the Spark NLP imports. import mlflow import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import pandas as pd from sparknlp.training import CoNLL import pyspark from pyspark.sql import SparkSession import pyspark.sql.types as T import pyspark.sql.functions as f import json Let’s define that an input param called text will be sent in the request. Let’s get the text from that parameter using dbutils. input = &quot;&quot; try: input = dbutils.widgets.get(&quot;text&quot;) print(&#39;&quot;text&quot; input found: &#39; + input) except: print(&#39;Unable to run: dbutils.widgets.get(&quot;text&quot;). Setting it to NOT_SET&#39;) input = &quot;NOT_SET&quot; Right now, the input text will be in input var. You can trigger an exception or set the input to some default value if the parameter does not come in the request. Let’s create a Spark Dataframe with the input df = spark.createDataFrame([[input]]).toDF(&#39;text&#39;) And now, we just need to use the snippet for Spark Dataframe to consume MLFlow models, described above: import mlflow import pyspark.sql.types as T import pyspark.sql.functions as f logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model) Predict on a Spark DataFrame. res_spark = loaded_model.predict(df_1_spark.rdd) annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) spark_res = spark.createDataFrame(res_spark[0], schema=annotationType) Let’s transform our lemmatized tokens from the Dataframe into a list of strings: lemmas = spark_res.select(&quot;result&quot;).collect() txt_results = [x[&#39;result&#39;] for x in lemmas] And finally, let’s use again dbutils to tell Databricks to spin off the run and return an exit parameter: the list of token strings. dbutils.notebook.exit(json.dumps({ &quot;status&quot;: &quot;OK&quot;, &quot;results&quot;: txt_results })) Configuring the job Last, but not least. We need to precreate the job, so that we run it from the API. We could do that using the API as well, but we will show you how to do it using the UI. On the left panel, go to Jobs and then Create Job. In the jobs screen, you will see you job created. It’s not running, it’s prepared to be called on demand, programatically or in the interface, with a text input param. Let’s see how to do that: Running the job In the jobs screen, if you click on the job, you will enter the Job screen, and be able to set your text input parameter and run the job manually. You can use this for testing purposes, but the interesting part is calling it externally, using the Databricks Jobs API. Using the Databricks Jobs API, from for example, Postman. POST HTTP request URL: https://[your_databricks_instance]/api/2.1/jobs/run-now Authorization: [use Bearer Token. You can get it from Databricks, Settings, User Settings, Generate New Token.] Body: { &quot;job_id&quot;: [job_id, check it in the Jobs screen], &quot;notebook_params&quot;: {&quot;text&quot;: &quot;This is an example of how well the lemmatizer works&quot;} } As it’s an asynchronous call, it will return the number a number of run, but no results. You will need to query for results using the number of the run and the following url https://[your_databricks_instance]/2.1/jobs/runs/get-output You will get a big json, but the most relevant info, the output, will be up to the end: Results (list of lemmatized words) {&quot;notebook_output&quot;: { &quot;status&quot;: &quot;OK&quot;, &quot;results&quot;: [&quot;This&quot;, &quot;is&quot;, &quot;a&quot;, &quot;example&quot;, &quot;of&quot;, &quot;how&quot;, &quot;lemmatizer&quot;, &quot;work&quot;] }} The notebook will be prepared in the job, but idle, until you call it programmatically, what will instantiate a run. Check the Jobs API for more information about what you can do with it and how to adapt it to your solutions for production purposes. Do you want to know more? Check how to productionize Spark NLP in our official documentation here Visit John Snow Labs and Spark NLP Technical Documentation websites Follow us on Medium: Spark NLP and Veysel Kocaman Write to support@johnsnowlabs.com for any additional request you may have",
    "url": "/docs/en/licensed_serving_spark_nlp_via_api_databricks_mlflow",
    "relUrl": "/docs/en/licensed_serving_spark_nlp_via_api_databricks_mlflow"
  },
  "94": {
    "id": "94",
    "title": "Serving Spark NLP&#58 FastAPI",
    "content": "This is the second article of the “Serving Spark NLP via API” series, showcasing how to serve Spark NLP using FastAPI and LightPipelines for a quick inference. Don’t forget to check the other articles in this series, namely: How to serve Spark NLP using Microsoft Synapse ML, available here. How to serve Spark NLP using Databricks Jobs and MLFlow Rest APIs, available here. Background Spark NLP is a Natural Language Understanding Library built on top of Apache Spark, leveranging Spark MLLib pipelines, that allows you to run NLP models at scale, including SOTA Transformers. Therefore, it’s the only production-ready NLP platform that allows you to go from a simple PoC on 1 driver node, to scale to multiple nodes in a cluster, to process big amounts of data, in a matter of minutes. Before starting, if you want to know more about all the advantages of using Spark NLP (as the ability to work at scale on air-gapped environments, for instance) we recommend you to take a look at the following resources: John Snow Labs webpage; The official technical documentation of Spark NLP; Spark NLP channel on Medium; Also, follow Veysel Kocaman, Data Scientist Lead and Head of Spark NLP for Healthcare, for the latests tips. Motivation Spark NLP is server-agnostic, what means it does not come with an integrated API server, but offers a lot of options to serve NLP models using Rest APIs. There is a wide range of possibilities to add a web server and serve Spark NLP pipelines using RestAPI, and in this series of articles we are only describing some of them. Let’s have an overview of how to use Microsoft’s Synapse ML as an example for that purpose. FastAPI and Spark NLP LightPipelines FastAPI is, as defined by the creators… …a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints. FastAPI provides with a very good latency and response times that, all along with the good performance of Spark NLP LightPipelines, makes this option the quickest one of the four described in the article. Read more about the performance advantages of using LightPipelines in this article created by John Snow Labs Data Scientist Lead Veysel Kocaman. Strengths Quickest approach Adds flexibility to build and adapt a custom API for your models Weaknesses LightPipelines are executed sequentially and don’t leverage the distributed computation that Spark Clusters provide. As an alternative, you can use FastAPI with default pipelines and a custom LoadBalancer, to distribute the calls over your cluster nodes. You can serve SparkNLP + FastAPI on Docker. To do that, we will create a project with the following files: Dockerfile: Image for creating a SparkNLP + FastAPI Docker image requirements.txt: PIP Requirements entrypoint.sh: Dockerfile entrypoint content/: folder containing FastAPI webapp and SparkNLP keys content/main.py: FastAPI webapp, entrypoint content/sparknlp_keys.json: SparkNLP keys (for Healthcare or OCR) Dockerfile The aim of this file is to create a suitable Docker Image with all the OS and Python libraries required to run SparkNLP. Also, adds a entry endpoint for the FastAPI server (see below) and a main folder containing the actual code to run a pipeline on an input text and return the expected values. FROM ubuntu:18.04 RUN apt-get update &amp;&amp; apt-get -y update RUN apt-get -y update &amp;&amp; apt-get install -y wget &amp;&amp; apt-get install -y jq &amp;&amp; apt-get install -y lsb-release &amp;&amp; apt-get install -y openjdk-8-jdk-headless &amp;&amp; apt-get install -y build-essential python3-pip &amp;&amp; pip3 -q install pip --upgrade &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* /usr/share/man /usr/share/doc /usr/share/doc-base ENV PYSPARK_DRIVER_PYTHON=python3 ENV PYSPARK_PYTHON=python3 ENV LC_ALL=C.UTF-8 ENV LANG=C.UTF-8 # We expose the FastAPI default port 8515 EXPOSE 8515 # Install all Python required libraries COPY requirements.txt / RUN pip install -r /requirements.txt # Adds the entrypoint to the FastAPI server COPY entrypoint.sh / RUN chmod +x /entrypoint.sh # In /content folder we will have our main.py and the license files COPY ./content/ /content/ WORKDIR content/ # We tell Docker to run this file when a container is instantiated ENTRYPOINT [&quot;/entrypoint.sh&quot;] requirements.txt This file describes which Python libraries will be required when creating the Docker image to run Spark NLP on FastAPI. pyspark==3.1.2 fastapi==0.70.1 uvicorn==0.16 wget==3.2 pandas==1.4.1 entrypoint.sh This file is the entry point of our Docker container, which carries out the following actions: Takes the sparknlp_keys.json and exports its values as environment variables, as required by Spark NLP for Healthcare. Installs the proper version of Spark NLP for Healthcare, getting the values from the license keys we have just exported in the previous step. Runs the main.py file, that will load the pipelines and create and endpoint to serve them. #!/bin/bash # Load the license from sparknlp_keys.json and export the values as OS variables export_json () { for s in $(echo $values | jq -r &#39;to_entries|map(&quot; (.key)= (.value|tostring)&quot;)|.[]&#39; $1 ); do export $s done } export_json &quot;/content/sparknlp_keys.json&quot; # Installs the proper version of Spark NLP for Healthcare pip install --upgrade spark-nlp-jsl==$JSL_VERSION --user --extra-index-url [https://pypi.johnsnowlabs.com/$SECRET](https://pypi.johnsnowlabs.com/$SECRET) if [ $? != 0 ]; then exit 1 fi # Script to create FastAPI endpoints and preloading pipelines for inference python3 /content/main.py content/main.py: Serving 2 pipelines in a FastAPI endpoint To maximize the performance and minimize the latency, we are going to store two Spark NLP pipelines in memory, so that we load only once (at server start) and we just use them everytime we get an API request to infer. To do this, let’s create a content/main.py Python script to download the required resources, store them in memory and serve them in Rest API endpoints. First, the import section import uvicorn, json, os from fastapi import FastAPI from sparknlp.annotator import * from sparknlp_jsl.annotator import * from sparknlp.base import * import sparknlp, sparknlp_jsl from sparknlp.pretrained import PretrainedPipeline app = FastAPI() pipelines = {} Then, let’s define the endpoint to serve the pipeline: @app.get(&quot;/benchmark/pipeline&quot;) async def get_one_sequential_pipeline_result(modelname, text=&#39;&#39;): return pipelines[modelname].annotate(text) Then, the startup event to preload the pipelines and start a Spark NLP Session: @app.on_event(&quot;startup&quot;) async def startup_event(): with open(&#39;/content/sparknlp_keys.json&#39;, &#39;r&#39;) as f: license_keys = json.load(f) spark = sparknlp_jsl.start(secret=license_keys[&#39;SECRET&#39;]) pipelines[&#39;ner_profiling_clinical&#39;] = PretrainedPipeline(&#39;ner_profiling_clinical&#39;, &#39;en&#39;, &#39;clinical/models&#39;) pipelines[&#39;clinical_deidentification&#39;] = PretrainedPipeline(&quot;clinical_deidentification&quot;, &quot;en&quot;, &quot;clinical/models&quot;) Finally, let’s run a uvicorn server, listening on port 8515 to the endpoints declared before: if __name__ == &quot;__main__&quot;: uvicorn.run(&#39;main:app&#39;, host=&#39;0.0.0.0&#39;, port=8515) content/sparknlp_keys.json For using Spark NLP for Healthcare, please add your Spark NLP for Healthcare license keys to content/sparknlp_keys.jsonDThe file is ready, you only need to fulfill with your own values taken from the json file John Snow Labs has provided you with. { &quot;AWS_ACCESS_KEY_ID&quot;: &quot;&quot;, &quot;AWS_SECRET_ACCESS_KEY&quot;: &quot;&quot;, &quot;SECRET&quot;: &quot;&quot;, &quot;SPARK_NLP_LICENSE&quot;: &quot;&quot;, &quot;JSL_VERSION&quot;: &quot;&quot;, &quot;PUBLIC_VERSION&quot;: &quot;&quot; } And now, let’s run the server! Creating the Docker image and running the container docker build -t johnsnowlabs/sparknlp:sparknlp_api . docker run -v jsl_keys.json:/content/sparknlp_keys.json -p 8515:8515 -it johnsnowlabs/sparknlp:sparknlp_api Consuming the API using a Python script Lets import some libraries import requests import time Then, let’s create a clinical note ner_text = &quot;&quot;&quot; A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting. The patient was prescribed 1 capsule of Advil 10 mg for 5 days and magnesium hydroxide 100mg/1ml suspension PO. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day. &quot;&quot;&quot; We have preloaded and served two Pretrained Pipelines: clinical_deidentification and ner_profiling_clinical . In modelname, let’s set which one we want to check # Change this line to execute any of the two pipelines modelname = &#39;clinical_deidentification&#39; # modelname = &#39;ner_profiling_clinical&#39; And finally, let’s use the requestslibrary to send a test request to the endpoint and get the results. query = f&quot;?modelname={modelname}&amp;text={ner_text}&quot; url = f&quot;http://localhost:8515/benchmark/pipeline{query}&quot; print(requests.get(url)) Results (original and deidentified texts in json format) &gt;&gt; { &#39;masked&#39;: [&#39;A &lt;AGE&gt; female with a history of gestational diabetes mellitus diagnosed ...], &#39;obfuscated&#39;: [&#39;A 48 female with a history of gestational diabetes mellitus diagnosed ...&#39;], &#39;ner_chunk&#39;: [&#39;28-year-old&#39;], &#39;sentence&#39;: [&#39;A 28-year-old female with a history of gestational diabetes mellitus diagnosed ...&#39;] } You can also prettify the json using the following function with the result of the annotate() function: def explode_annotate(ann_result): &#39;&#39;&#39; Function to convert result object to json input: raw result output: processed result dictionary &#39;&#39;&#39; result = {} for column, ann in ann_result[0].items(): result[column] = [] for lines in ann: content = { &quot;result&quot;: lines.result, &quot;begin&quot;: lines.begin, &quot;end&quot;: lines.end, &quot;metadata&quot;: dict(lines.metadata), } result[column].append(content) return result Do you want to know more? Check the example notebooks in the Spark NLP Workshop repository, available here Visit John Snow Labs and Spark NLP Technical Documentation websites Follow us on Medium: Spark NLP and Veysel Kocaman Write to support@johnsnowlabs.com for any additional request you may have",
    "url": "/docs/en/licensed_serving_spark_nlp_via_api_fastapi",
    "relUrl": "/docs/en/licensed_serving_spark_nlp_via_api_fastapi"
  },
  "95": {
    "id": "95",
    "title": "Serving Spark NLP&#58 SynapseML",
    "content": "This is the first article of the “Serving Spark NLP via API” series, showcasing how to serve Spark NLP using Synapse ML Don’t forget to check the other articles in this series, namely: How to server Spark NLP using FastAPI and LightPipelines, available here. How to serve Spark NLP using Databricks Jobs and MLFlow Rest APIs, available here. Background Spark NLP is a Natural Language Understanding Library built on top of Apache Spark, leveranging Spark MLLib pipelines, that allows you to run NLP models at scale, including SOTA Transformers. Therefore, it’s the only production-ready NLP platform that allows you to go from a simple PoC on 1 driver node, to scale to multiple nodes in a cluster, to process big amounts of data, in a matter of minutes. Before starting, if you want to know more about all the advantages of using Spark NLP (as the ability to work at scale on air-gapped environments, for instance) we recommend you to take a look at the following resources: John Snow Labs webpage; The official technical documentation of Spark NLP; Spark NLP channel on Medium; Also, follow Veysel Kocaman, Data Scientist Lead and Head of Spark NLP for Healthcare, for the latests tips. Motivation Spark NLP is server-agnostic, what means it does not come with an integrated API server, but offers a lot of options to serve NLP models using Rest APIs. There is a wide range of possibilities to add a web server and serve Spark NLP pipelines using RestAPI, and in this series of articles we are only describing some of them. Let’s have an overview of how to use Microsoft’s Synapse ML as an example for that purpose. Microsoft’s Synapse ML Synapse ML (previously named SparkMML) is, as they state in their official webpage: … an ecosystem of tools aimed towards expanding the distributed computing framework Apache Spark in several new directions. They offer a seamless integratation with OpenCV, LightGBM, Microsoft Cognitive Tool and, the most relevant for our use case, Spark Serving, an extension of *Spark Streaming *with an integrated server and a Load Balancer, that can attend multiple requests via Rest API, balance and attend them leveraging the capabilities of a Spark Cluster. That means that you can sin up a server and attend requests that will be distributed transparently over a Spark NLP cluster, in a very effortless way. Strengths Ready-to-use server Includes a Load Balancer Distributes the work over a Spark Cluster Can be used for both Spark NLP and Spark OCR Weaknesses For small use cases that don’t require big cluster processing, other approaches may be faster (as FastAPI using LightPipelines) Requires using an external Framework This approach does not allow you to customize your endpoints, it uses Synapse ML ones How to set up Synapse ML to serve Spark NLP pipelines We will skip here how to install Spark NLP. If you need to do that, please follow this official webpage about how to install Spark NLP or, if Spark NLP for Healthcare if you are using the Healthcare library. Synapse ML recommends using at least Spark 3.2, so first of all, let’s configure the Spark Session with the required jars packages(both for Synapse ML and Spark) with the the proper Spark version (take a look at the suffix spark-nlp-spark32) and also, very important, add to jars.repository the Maven repository for SynapseML. sparknlpjsl_jar = &quot;spark-nlp-jsl.jar&quot; from pyspark.sql import SparkSession spark = SparkSession.builder .appName(&quot;Spark&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;16G&quot;) .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.microsoft.azure:synapseml_2.12:0.9.5,com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:[YOUR_SPARKNLP_VERSION]) .config(&quot;spark.jars&quot;, sparknlpjsl_jar) .config(&quot;spark.jars.repositories&quot;, &quot;https://mmlspark.azureedge.net/maven&quot;) .getOrCreate() After the initialization, add your required imports (Spark NLP) and add to them the SynapseML-specific ones: import sparknlp import sparknlp_jsl ... import synapse.ml from synapse.ml.io import * Now, let’s create a Spark NLP for Healthcare pipeline to carry out Entity Resolution. document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetectorDL = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl_healthcare&quot;, &quot;en&quot;, &#39;clinical/models&#39;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) word_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter_icd = NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&#39;PROBLEM&#39;]) .setPreservePosition(False) c2doc = Chunk2Doc() .setInputCols(&quot;ner_chunk&quot;) .setOutputCol(&quot;ner_chunk_doc&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbiobert_base_cased_mli&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) .setCaseSensitive(False) icd_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_icd10cm_augmented_billable_hcc&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;icd10cm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) resolver_pipeline = Pipeline( stages = [ document_assembler, sentenceDetectorDL, tokenizer, word_embeddings, clinical_ner, ner_converter_icd, c2doc, sbert_embedder, icd_resolver ]) Let’s use a clinical note to test Synapse ML. clinical_note = &quot;&quot;&quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus (T2DM), one prior episode of HTG-induced pancreatitis three years prior to presentation, associated with an acute hepatitis, and obesity with a body mass index (BMI) of 33.5 kg/m2, presented with a one-week history of polyuria, polydipsia, poor appetite, and vomiting. Two weeks prior to presentation, she was treated with a five-day course of amoxicillin for a respiratory tract infection. She was on metformin, glipizide, and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG. She had been on dapagliflozin for six months at the time of presentation. Physical examination on presentation was significant for dry oral mucosa; significantly, her abdominal examination was benign with no tenderness, guarding, or rigidity.&quot;&quot;&quot; Since SynapseML serves a RestAPI, we will be sending JSON requests. Let’s define a simple json with the clinical note: data_json = {&quot;text&quot;: clinical_note } Now, let’s spin up a server using Synapse ML Spark Serving. It will consist of: a streaming server that will receive a json and transform it into a Spark Dataframe a call to Spark NLP transform on the dataframe, using the pipeline a write operation returning the output also in json format. #1: Creating the streaming server and transforming json to Spark Dataframe serving_input = spark.readStream.server() .address(&quot;localhost&quot;, 9999, &quot;benchmark_api&quot;) .option(&quot;name&quot;, &quot;benchmark_api&quot;) .load() .parseRequest(&quot;benchmark_api&quot;, data.schema) #2: Applying transform to the dataframe using our Spark NLP pipeline serving_output = resolver_p_model.transform(serving_input) .makeReply(&quot;icd10cm_code&quot;) #3: Returning the response in json format server = serving_output.writeStream .server() .replyTo(&quot;benchmark_api&quot;) .queryName(&quot;benchmark_query&quot;) .option(&quot;checkpointLocation&quot;, &quot;file:///tmp/checkpoints-{}&quot;.format(uuid.uuid1())) .start() And we are ready to test the endpoint using the requests library. import requests res = requests.post(&quot;http://localhost:9999/benchmark_api&quot;, data= json.dumps(data_json)) And last, but not least, let’s check the results: for i in range (0, len(response_list.json())): print(response_list.json()[i][&#39;result&#39;]) Results (list of ICD-10-CM codes from NER chunks) &gt;&gt; O2441 O2411 P702 K8520 B159 E669 Z6841 R35 R631 R630 R111... SynapseML on Databricks You can also run the above code in Databricks. To do that, you only need to remove the Creating a Spark Session, since Databricks manages that session for you. After we remove that part of the code from our notebook, we need to set the same configuration params in the Cluster Configuration, so that Databricks spins a cluster with the proper jars and config params (similarly to what we did programatically in Creating a Spark Session above, but using Databricks UI) To do so, go to Compute →Clusters in Databricks and create a new cluster (name it, for instance, Synapse). In your environment variables, as always, add the keys from your license in a key=value format Then, in Cluster → Libraries, you need to install: SynapseML jar (Maven → com.microsoft.azure:synapseml_2.12:0.9.5) Spark NLP jar ( Maven →com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:[YOUR_SPARKNLP_VERSION]) Spark NLP wheel (PyPi → spark-nlp==[YOUR_SPARKNLP_VERSION]) If you are using Spark NLP for Healthcare Spark NLP for Healthcare jar. Download the jar using the secret from your license, and then upload the jar to DBFS and add it in the Libraries section (DBFS/ADLS → dbfs:/FileStore/johnsnowlabs/libs/spark_nlp_jsl_[YOUR_SPARKNLP_VERSION].jar) Spark NLP for Healthcare wheel. Same that with the jar. Download the jar using the secret from your license, and then upload the jar to DBFS and add it in the Libraries section (DBFS/ADLS → dbfs:/FileStore/johnsnowlabs/libs/spark_nlp_jsl_[YOUR_SPARKNLP_VERSION].whl) And the rest of the code from the Importing all the libraries section and on remains exactly the same. Do you want to know more? Check the example notebooks in the Spark NLP Workshop repository, available here Visit John Snow Labs and Spark NLP Technical Documentation websites Follow us on Medium: Spark NLP and Veysel Kocaman Write to support@johnsnowlabs.com for any additional request you may have",
    "url": "/docs/en/licensed_serving_spark_nlp_via_api_synapseml",
    "relUrl": "/docs/en/licensed_serving_spark_nlp_via_api_synapseml"
  },
  "96": {
    "id": "96",
    "title": "Training",
    "content": "Training Datasets These are classes to load common datasets to train annotators for tasks such as Relation Model, Assertion models and more. Annotation tool json reader. All the annotations from Annotation Lab can be exported in a standard JSON format as shown below. The JSON holds multiple types of annotations like NER, Assertion, and Relations. To generate training datasets from the json, a utility class AnnotationToolJsonReader can be used, which can generate training datasets for training NER and Assertion models. AnnotationToolJsonReader Colab Notebook provides the code and details of processing the exported JSON to generate training datasets for NER and Assertion models in section 2. Users can distinguish between different label types by using constructor parameters described below. This notebook also explains how to connect to your Annotation Lab instance via API for uploading tasks, pre-annotations, and exporting entire projects. Input File Format: [ { &quot;completions&quot;: [ { &quot;created_ago&quot;: &quot;2020-05-18T20:48:18.117Z&quot;, &quot;created_username&quot;: &quot;admin&quot;, &quot;id&quot;: 3001, &quot;lead_time&quot;: 19.255, &quot;result&quot;: [ { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;o752YyB2g9&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 12, &quot;labels&quot;: [ &quot;AsPresent&quot; ], &quot;start&quot;: 3, &quot;text&quot;: &quot;have faith&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;wf2U3o7I6T&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 24, &quot;labels&quot;: [ &quot;AsPresent&quot; ], &quot;start&quot;: 16, &quot;text&quot;: &quot; to trust&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;Q3BkU5eZNx&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 40, &quot;labels&quot;: [ &quot;AsPresent&quot; ], &quot;start&quot;: 35, &quot;text&quot;: &quot;to the&quot; } } ] } ], &quot;created_at&quot;: &quot;2020-05-18 20:47:53&quot;, &quot;created_by&quot;: &quot;andres.fernandez&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;To have faith is to trust yourself to the water&quot; }, &quot;id&quot;: 3 }, { &quot;completions&quot;: [ { &quot;created_ago&quot;: &quot;2020-05-17T17:52:41.563Z&quot;, &quot;created_username&quot;: &quot;andres.fernandez&quot;, &quot;id&quot;: 1, &quot;lead_time&quot;: 31.449, &quot;result&quot;: [ { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;IQjoZJNKEv&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 12, &quot;labels&quot;: [ &quot;Disease&quot; ], &quot;start&quot;: 3, &quot;text&quot;: &quot;have faith&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;tHsbn4oYy5&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 46, &quot;labels&quot;: [ &quot;Treatment&quot; ], &quot;start&quot;: 42, &quot;text&quot;: &quot;water&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;IJHkc9bxJ-&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 12, &quot;labels&quot;: [ &quot;AsPresent&quot; ], &quot;start&quot;: 0, &quot;text&quot;: &quot;To have faith&quot; } } ] } ], &quot;created_at&quot;: &quot;2020-05-17 17:52:02&quot;, &quot;created_by&quot;: &quot;andres.fernandez&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;To have faith is to trust yourself to the water&quot; }, &quot;id&quot;: 0 }, { &quot;completions&quot;: [ { &quot;created_ago&quot;: &quot;2020-05-17T17:57:19.402Z&quot;, &quot;created_username&quot;: &quot;andres.fernandez&quot;, &quot;id&quot;: 1001, &quot;lead_time&quot;: 15.454, &quot;result&quot;: [ { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;j_lT0zwtrJ&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 46, &quot;labels&quot;: [ &quot;Disease&quot; ], &quot;start&quot;: 20, &quot;text&quot;: &quot;trust yourself to the water&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;e1FuGWu7EQ&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 33, &quot;labels&quot;: [ &quot;AsPresent&quot; ], &quot;start&quot;: 19, &quot;text&quot;: &quot; trust yourself&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;q0MCSM9SXz&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 12, &quot;labels&quot;: [ &quot;Treatment&quot; ], &quot;start&quot;: 0, &quot;text&quot;: &quot;To have faith&quot; } }, { &quot;from_name&quot;: &quot;ner&quot;, &quot;id&quot;: &quot;9R7dvPphPX&quot;, &quot;source&quot;: &quot;$text&quot;, &quot;to_name&quot;: &quot;text&quot;, &quot;type&quot;: &quot;labels&quot;, &quot;value&quot;: { &quot;end&quot;: 12, &quot;labels&quot;: [ &quot;AsPresent&quot; ], &quot;start&quot;: 0, &quot;text&quot;: &quot;To have faith&quot; } } ] } ], &quot;created_at&quot;: &quot;2020-05-17 17:52:54&quot;, &quot;created_by&quot;: &quot;andres.fernandez&quot;, &quot;data&quot;: { &quot;text&quot;: &quot;To have faith is to trust yourself to the water&quot; }, &quot;id&quot;: 1, &quot;predictions&quot;: [] } ] Constructor Parameters: assertion_labels: The assertions labels are used for the training dataset creation. excluded_labels: The assertions labels that are excluded for the training dataset creation. split_chars: The split chars that are used in the default tokenizer. context_chars: The context chars that are used in the default tokenizer. SDDLPath: The context chars that are used in the default tokenizer. Parameters for readDataset: spark: Initiated Spark Session with Spark NLP path: Path to the resource Refer to the documentation for more details on the API: Python API: Scala API: AnnotationToolJsonReader Show Example PythonScala from sparknlp_jsl.training import AnnotationToolJsonReader assertion_labels = [&quot;AsPresent&quot;,&quot;Absent&quot;] excluded_labels = [&quot;Treatment&quot;] split_chars = [&quot; &quot;, &quot; -&quot;] context_chars = [&quot;.&quot;, &quot;,&quot;, &quot;;&quot;, &quot;:&quot;, &quot;!&quot;, &quot;?&quot;, &quot;*&quot;, &quot;-&quot;, &quot;(&quot;, &quot;)&quot;, &quot; &quot;&quot;, &quot;&#39;&quot;,&quot;+&quot;,&quot;%&quot;,&quot;&#39;&quot;] SDDLPath = &quot;&quot; rdr = AnnotationToolJsonReader(assertion_labels = assertion_labels, excluded_labels = excluded_labels, split_chars = split_chars, context_chars = context_chars,SDDLPath=SDDLPath) path = &quot;src/test/resources/anc-pos-corpus-small/test-training.txt&quot; df = rdr.readDataset(spark, json_path) assertion_df = rdr.generateAssertionTrainSet(df) assertion_df.show() +--+--++--++ | text| target| label|start|end| +--+--++--++ |To have faith is ...| To have faith|AsPresent| 0| 2| |To have faith is ...| have faith|AsPresent| 1| 2| |To have faith is ...| to trust|AsPresent| 4| 5| |To have faith is ...| to the|AsPresent| 7| 8| |To have faith is ...| yourself|AsPresent| 6| 6| |To have faith is ...| To have faith|AsPresent| 0| 2| |To have faith is ...|trust yourself|AsPresent| 5| 6| +--+--++--++ import com.johnsnowlabs.nlp.training.POS val filename = &quot;src/test/resources/json_import.json&quot; val reader = new AnnotationToolJsonReader(assertionLabels=List(&quot;AsPresent&quot;,&quot;Absent&quot;).asJava, splitChars=List(&quot; &quot;, &quot; -&quot;).asJava, excludedLabels = List(&quot;Treatment&quot;).asJava) val df = reader.readDataset(ResourceHelper.spark, filename) val assertionDf = reader.generateAssertionTrainSet(df) assertionDf.show() +--+--++--++ | text| target| label|start|end| +--+--++--++ |To have faith is ...| To have faith|AsPresent| 0| 2| |To have faith is ...| have faith|AsPresent| 1| 2| |To have faith is ...| to trust|AsPresent| 4| 5| |To have faith is ...| to the|AsPresent| 7| 8| |To have faith is ...| yourself|AsPresent| 6| 6| |To have faith is ...| To have faith|AsPresent| 0| 2| |To have faith is ...|trust yourself|AsPresent| 5| 6| +--+--++--++ Assertion Trains AssertionDL, a deep Learning based approach used to extract Assertion Status from extracted entities and text. AssertionDLApproach Train a Assertion Model algorithm using deep learning. The training data should have annotations columns of type DOCUMENT, CHUNK, WORD_EMBEDDINGS, the labelcolumn (The assertion status that you want to predict), the start (the start index for the term that has the assertion status), the end column (the end index for the term that has the assertion status).This model use a deep learning to predict the entity. Excluding the label, this can be done with for example a SentenceDetector, a Chunk , a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). Input Annotator Types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output Annotator Type: ASSERTION Python API: AssertionDLApproach Scala API: AssertionDLApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline document_assembler = DocumentAssembler().setInputCol(&#39;text&#39;).setOutputCol(&#39;document&#39;) sentence_detector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) POSTag = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) chunker = Chunker() .setInputCols([&quot;pos&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;(&lt;NN&gt;)+&quot;]) pubmed = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) assertion_status = AssertionDLApproach() .setInputCols(&quot;sentence&quot;, &quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) .setLabelCol(&quot;label&quot;) .setLearningRate(0.01) .setDropout(0.15) .setBatchSize(16) .setEpochs(3) .setValidationSplit(0.2) .setIncludeConfidence(True) pipeline = Pipeline().setStages([ document_assembler, sentence_detector, tokenizer, POSTag, chunker, pubmed, assertion_status ]) conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) // This CoNLL dataset already includes the sentence, token, pos and label column with their respective annotator types. // If a custom dataset is used, these need to be defined. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.{Chunker, Tokenizer} import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotator.PerceptronModel import com.johnsnowlabs.nlp.annotators.assertion.dl.AssertionDLModel import com.johnsnowlabs.nlp.annotator.NerCrfApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val POSTag = PerceptronModel .pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val chunker = new Chunker() .setInputCols(Array(&quot;pos&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;(&lt;NN&gt;)+&quot;)) val pubmed = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val assertionStatus = new AssertionDLApproach() .setInputCols(&quot;sentence&quot;, &quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) .setLabelCol(&quot;label&quot;) .setLearningRate(0.01f) .setDropout(0.15f) .setBatchSize(16) .setEpochs(3) .setValidationSplit(0.2f) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, POSTag, chunker, pubmed, assertionStatus )) datasetPath = &quot;/../src/test/resources/rsAnnotations-1-120-random.csv&quot; train_data = SparkContextForTest.spark.read.option(&quot;header&quot;, &quot;true&quot;).csv(path=&quot;file:///&quot; + os.getcwd() + datasetPath) val pipelineModel = pipeline.fit(trainingData) AssertionLogRegApproach Train a Assertion Model algorithm using a regression log model. The training data should have annotations columns of type DOCUMENT, CHUNK, WORD_EMBEDDINGS, the labelcolumn (The assertion status that you want to predict), the start (the start index for the term that has the assertion status), the end column (the end index for the term that has the assertion status).This model use a deep learning to predict the entity. Excluding the label, this can be done with for example a SentenceDetector, a Chunk , a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). Input Annotator Types: DOCUMENT, CHUNK, WORD_EMBEDDINGS Output Annotator Type: ASSERTION Python API: AssertionLogRegApproach Scala API: AssertionLogRegApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline document_assembler = DocumentAssembler().setInputCol(&#39;text&#39;).setOutputCol(&#39;document&#39;) sentence_detector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) POSTag = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) chunker = Chunker() .setInputCols([&quot;pos&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;(&lt;NN&gt;)+&quot;]) pubmed = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) assertion_status = AssertionLogRegApproach() .setInputCols(&quot;sentence&quot;, &quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) .setLabelCol(&quot;label&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setEpochs(3) pipeline = Pipeline().setStages([ document_assembler, sentence_detector, tokenizer, POSTag, chunker, pubmed, assertion_status ]) conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) // This CoNLL dataset already includes the sentence, token, pos and label column with their respective annotator types. // If a custom dataset is used, these need to be defined. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.{Chunker, Tokenizer} import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotator.PerceptronModel import com.johnsnowlabs.nlp.annotators.assertion.dl.AssertionDLModel import com.johnsnowlabs.nlp.annotator.NerCrfApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val POSTag = PerceptronModel .pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val chunker = new Chunker() .setInputCols(Array(&quot;pos&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;(&lt;NN&gt;)+&quot;)) val pubmed = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val assertion = new AssertionLogRegApproach() .setLabelCol(&quot;label&quot;) .setInputCols(&quot;document&quot;, &quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) .setReg(0.01) .setBefore(11) .setAfter(13) .setStartCol(&quot;start&quot;) .setEndCol(&quot;end&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, POSTag, chunker, pubmed, assertion )) datasetPath = &quot;/../src/test/resources/rsAnnotations-1-120-random.csv&quot; train_data = SparkContextForTest.spark.read.option(&quot;header&quot;, &quot;true&quot;).csv(path=&quot;file:///&quot; + os.getcwd() + datasetPath) val pipelineModel = pipeline.fit(trainingData) Token Classification These are annotators that can be trained to recognize named entities in text. MedicalNer This Named Entity recognition annotator allows to train generic NER model based on Neural Networks. The architecture of the neural network is a Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. For instantiated/pretrained models, see NerDLModel. The training data should be a labeled Spark Dataset, in the format of CoNLL 2003 IOB with Annotation type columns. The data should have columns of type DOCUMENT, TOKEN, WORD_EMBEDDINGS and an additional label column of annotator type NAMED_ENTITY. Excluding the label, this can be done with for example a SentenceDetector, a Tokenizer and a WordEmbeddingsModel with clinical embeddings (any clinical word embeddings can be chosen). Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: MedicalNerApproach Scala API: MedicalNerApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp_jsl.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline # First extract the prerequisites for the NerDLApproach documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) clinical_embeddings = WordEmbeddingsModel.pretrained(&#39;embeddings_clinical&#39;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Then the training can start nerTagger = MedicalNerApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(2) .setBatchSize(64) .setRandomSeed(0) .setVerbose(1) .setValidationSplit(0.2) .setEvaluationLogExtended(True) .setEnableOutputLogs(True) .setIncludeConfidence(True) .setOutputLogsPath(&#39;ner_logs&#39;) .setGraphFolder(&#39;medical_ner_graphs&#39;) .setEnableMemoryOptimizer(True) #&gt;&gt; if you have a limited memory and a large conll file, you can set this True to train batch by batch pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, clinical_embeddings, nerTagger ]) # We use the text and labels from the CoNLL dataset conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.ner.MedicalNerApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline // First extract the prerequisites for the NerDLApproach val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger =new MedicalNerApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(5) .setLr(0.003f) .setBatchSize(8) .setRandomSeed(0) .setVerbose(1) .setEvaluationLogExtended(false) .setEnableOutputLogs(false) .setIncludeConfidence(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) // We use the text and labels from the CoNLL dataset val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) Text Classification These are annotators that can be trained to classify text into different classes, such as sentiment. DocumentLogRegClassifier Trains a model to classify documents with a Logarithmic Regression algorithm. Training data requires columns for text and their label. The result is a trained GenericClassifierModel. Input Annotator Types: TOKEN Output Annotator Type: CATEGORY Python API: DocumentLogRegClassifierApproach Scala API: DocumentLogRegClassifierApproach Show Example PythonScala import sparknlp from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline document_assembler = DocumentAssembler() .setInputCols(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) stopwords_cleaner = StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) stemmer = Stemmer() .setInputCols(&quot;cleanTokens&quot;) .setOutputCol(&quot;stem&quot;) gen_clf = DocumentLogRegClassifierApproach() .setLabelColumn(&quot;category&quot;) .setInputCols(&quot;stem&quot;) .setOutputCol(&quot;prediction&quot;) pipeline = Pipeline().setStages([ document_assembler, tokenizer, normalizer, stopwords_cleaner, stemmer, logreg ]) clf_model = pipeline.fit(data) import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.ner.MedicalNerApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline // First extract the prerequisites for the NerDLApproach val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger =new MedicalNerApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(5) .setLr(0.003f) .setBatchSize(8) .setRandomSeed(0) .setVerbose(1) .setEvaluationLogExtended(false) .setEnableOutputLogs(false) .setIncludeConfidence(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) // We use the text and labels from the CoNLL dataset val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) GenericClassifier Trains a TensorFlow model for generic classification of feature vectors. It takes FEATURE_VECTOR annotations from FeaturesAssembler as input, classifies them and outputs CATEGORY annotations. Please see the Parameters section for required training parameters. For a more extensive example please see the Spark NLP Workshop. Input Annotator Types: FEATURE_VECTOR Output Annotator Type: CATEGORY Python API: GenericClassifierApproach Scala API: GenericClassifierApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline features_asm = FeaturesAssembler() .setInputCols([&quot;feature_1&quot;, &quot;feature_2&quot;, &quot;...&quot;, &quot;feature_n&quot;]) .setOutputCol(&quot;features&quot;) gen_clf = GenericClassifierApproach() .setLabelColumn(&quot;target&quot;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(&quot;/path/to/graph_file.pb&quot;) .setEpochsNumber(50) .setBatchSize(100) .setFeatureScaling(&quot;zscore&quot;) .setlearningRate(0.001) .setFixImbalance(True) .setOutputLogsPath(&quot;logs&quot;) .setValidationSplit(0.2) # keep 20% of the data for validation purposes pipeline = Pipeline().setStages([ features_asm, gen_clf ]) clf_model = pipeline.fit(data) import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.ner.MedicalNerApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline // First extract the prerequisites for the NerDLApproach val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger =new MedicalNerApproach() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(5) .setLr(0.003f) .setBatchSize(8) .setRandomSeed(0) .setVerbose(1) .setEvaluationLogExtended(false) .setEnableOutputLogs(false) .setIncludeConfidence(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) // We use the text and labels from the CoNLL dataset val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) Relation Models RelationExtractionApproach Trains a Relation Extraction Model to predict attributes and relations for entities in a sentence. Relation Extraction is the key component for building relation knowledge graphs, and it is of crucial significance to natural language processing applications such as structured search, sentiment analysis, question answering, and summarization. The dataset will be a csv with the following that contains the following columns (sentence,chunk1,firstCharEnt1,lastCharEnt1,label1,chunk2,firstCharEnt2,lastCharEnt2,label2,rel), This annotator can be don with for example: Excluding the rel, this can be done with for example a SentenceDetector, a Tokenizer and a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). a Chunk can be created using the firstCharEnt1, lastCharEnt1,chunk1, label1 columns and firstCharEnt2, lastCharEnt2, chunk2, label2 columns An example of that dataset can be found in the following link i2b2_clinical_dataset sentence,chunk1,firstCharEnt1,lastCharEnt1,label1,chunk2,firstCharEnt2,lastCharEnt2,label2,rel Previous studies have reported the association of prodynorphin (PDYN) promoter polymorphism with temporal lobe epilepsy (TLE) susceptibility, but the results remain inconclusive.,PDYN,64,67,GENE,epilepsy,111,118,PHENOTYPE,0 The remaining cases, clinically similar to XLA, are autosomal recessive agammaglobulinemia (ARA).,XLA,43,45,GENE,autosomal recessive,52,70,PHENOTYPE,0 YAP/TAZ have been reported to be highly expressed in malignant tumors.,YAP,19,21,GENE,tumors,82,87,PHENOTYPE,0 Apart from that, no additional training data is needed. Input Annotator Types: WORD_EMBEDDINGS, POS, CHUNK, DEPENDENCY Output Annotator Type: CATEGORY Python API: RelationExtractionApproach Scala API: RelationExtractionApproach Show Example PythonScala import functools import numpy as np import pyspark.sql.functions as F import pyspark.sql.types as T from sparknlp.base import annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) @F.udf(T.ArrayType(annotationType)) def createTrainAnnotations(begin1, end1, begin2, end2, chunk1, chunk2, label1, label2): entity1 = sparknlp.annotation.Annotation(&quot;chunk&quot;, begin1, end1, chunk1, {&#39;entity&#39;: label1.upper(), &#39;sentence&#39;: &#39;0&#39;}, []) entity2 = sparknlp.annotation.Annotation(&quot;chunk&quot;, begin2, end2, chunk2, {&#39;entity&#39;: label2.upper(), &#39;sentence&#39;: &#39;0&#39;}, []) entity1.annotatorType = &quot;chunk&quot; entity2.annotatorType = &quot;chunk&quot; return [entity1, entity2] data = spark.read.option(&quot;header&quot;,&quot;true&quot;).format(&quot;csv&quot;).load(&quot;i2b2_clinical_rel_dataset.csv&quot;) data = data .withColumn(&quot;begin1i&quot;, F.expr(&quot;cast(firstCharEnt1 AS Int)&quot;)) .withColumn(&quot;end1i&quot;, F.expr(&quot;cast(lastCharEnt1 AS Int)&quot;)) .withColumn(&quot;begin2i&quot;, F.expr(&quot;cast(firstCharEnt2 AS Int)&quot;)) .withColumn(&quot;end2i&quot;, F.expr(&quot;cast(lastCharEnt2 AS Int)&quot;)) .where(&quot;begin1i IS NOT NULL&quot;) .where(&quot;end1i IS NOT NULL&quot;) .where(&quot;begin2i IS NOT NULL&quot;) .where(&quot;end2i IS NOT NULL&quot;) .withColumn( &quot;train_ner_chunks&quot;, createTrainAnnotations( &quot;begin1i&quot;, &quot;end1i&quot;, &quot;begin2i&quot;, &quot;end2i&quot;, &quot;chunk1&quot;, &quot;chunk2&quot;, &quot;label1&quot;, &quot;label2&quot; ).alias(&quot;train_ner_chunks&quot;, metadata={&#39;annotatorType&#39;: &quot;chunk&quot;})) documentAssembler = DocumentAssembler() .setInputCol(&quot;sentence&quot;) .setOutputCol(&quot;sentences&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;token&quot;) words_embedder = WordEmbeddingsModel() .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;embeddings&quot;) pos_tagger = PerceptronModel() .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;pos_tags&quot;) dependency_parser = DependencyParserModel() .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols([&quot;sentences&quot;, &quot;pos_tags&quot;, &quot;tokens&quot;]) .setOutputCol(&quot;dependencies&quot;) reApproach = RelationExtractionApproach() .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setLabelColumn(&quot;rel&quot;) .setEpochsNumber(70) .setBatchSize(200) .setDropout(0.5) .setLearningRate(0.001) .setModelFile(&quot;/content/RE_in1200D_out20.pb&quot;) .setFixImbalance(True) .setFromEntity(&quot;begin1i&quot;, &quot;end1i&quot;, &quot;label1&quot;) .setToEntity(&quot;begin2i&quot;, &quot;end2i&quot;, &quot;label2&quot;) .setOutputLogsPath(&#39;/content&#39;) train_pipeline = Pipeline(stages=[ documenter, tokenizer, words_embedder, pos_tagger, dependency_parser, reApproach ]) rel_model = train_pipeline.fit(data) import com.johnsnowlabs.nlp.{DocumentAssembler} import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.ner.{MedicalNerModel, NerConverter} import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel package com.johnsnowlabs.nlp.annotators.re.RelationExtractionApproach() import org.apache.spark.ml.Pipeline import org.apache.spark.sql.functions._ val data = spark.read.option(&quot;header&quot;,true).csv(&quot;src/test/resources/re/gene_hpi.csv&quot;).limit(10) def createTrainAnnotations = udf { ( begin1:Int, end1:Int, begin2:Int, end2:Int, chunk1:String, chunk2:String, label1:String, label2:String) =&gt; { val an1 = Annotation(CHUNK,begin1,end1,chunk1,Map(&quot;entity&quot; -&gt; label1.toUpperCase,&quot;sentence&quot; -&gt; &quot;0&quot;)) val an2 = Annotation(CHUNK,begin2,end2,chunk2,Map(&quot;entity&quot; -&gt; label2.toUpperCase,&quot;sentence&quot; -&gt; &quot;0&quot;)) Seq(an1,an2) } } val metadataBuilder: MetadataBuilder = new MetadataBuilder() val meta = metadataBuilder.putString(&quot;annotatorType&quot;, CHUNK).build() val dataEncoded = data .withColumn(&quot;begin1i&quot;, expr(&quot;cast(firstCharEnt1 AS Int)&quot;)) .withColumn(&quot;end1i&quot;, expr(&quot;cast(lastCharEnt1 AS Int)&quot;)) .withColumn(&quot;begin2i&quot;, expr(&quot;cast(firstCharEnt2 AS Int)&quot;)) .withColumn(&quot;end2i&quot;, expr(&quot;cast(lastCharEnt2 AS Int)&quot;)) .where(&quot;begin1i IS NOT NULL&quot;) .where(&quot;end1i IS NOT NULL&quot;) .where(&quot;begin2i IS NOT NULL&quot;) .where(&quot;end2i IS NOT NULL&quot;) .withColumn( &quot;train_ner_chunks&quot;, createTrainAnnotations( col(&quot;begin1i&quot;), col(&quot;end1i&quot;), col(&quot;begin2i&quot;), col(&quot;end2i&quot;), col(&quot;chunk1&quot;), col(&quot;chunk2&quot;), col(&quot;label1&quot;), col(&quot;label2&quot;) ).as(&quot;train_ner_chunks&quot;,meta)) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;sentence&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentences&quot;)) .setOutputCol(&quot;tokens&quot;) val embedder = ParallelDownload(WordEmbeddingsModel .pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;document&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;embeddings&quot;)) val posTagger = ParallelDownload(PerceptronModel .pretrained(&quot;pos_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;posTags&quot;)) val nerTagger = ParallelDownload(MedicalNerModel .pretrained(&quot;ner_events_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner_tags&quot;)) val nerConverter = new NerConverter() .setInputCols(Array(&quot;sentences&quot;, &quot;tokens&quot;, &quot;ner_tags&quot;)) .setOutputCol(&quot;nerChunks&quot;) val depencyParser = ParallelDownload(DependencyParserModel .pretrained(&quot;dependency_conllu&quot;, &quot;en&quot;) .setInputCols(Array(&quot;sentences&quot;, &quot;posTags&quot;, &quot;tokens&quot;)) .setOutputCol(&quot;dependencies&quot;)) val re = new RelationExtractionApproach() .setInputCols(Array(&quot;embeddings&quot;, &quot;posTags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;)) .setOutputCol(&quot;rel&quot;) .setLabelColumn(&quot;target_rel&quot;) .setEpochsNumber(30) .setBatchSize(200) .setlearningRate(0.001f) .setValidationSplit(0.05f) .setFromEntity(&quot;begin1i&quot;, &quot;end1i&quot;, &quot;label1&quot;) .setToEntity(&quot;end2i&quot;, &quot;end2i&quot;, &quot;label2&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embedder, posTagger, nerTagger, nerConverter, depencyParser, re).parallelDownload) val model = pipeline.fit(dataEncoded) Entity Resolution Those models predict what are the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.). SentenceEntityResolver Contains all the parameters and methods to train a SentenceEntityResolverModel. The model transforms a dataset with Input Annotation type SENTENCE_EMBEDDINGS, coming from e.g. BertSentenceEmbeddings and returns the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.) To use pretrained models please use SentenceEntityResolverModel and see the Models Hub for available models. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: ENTITY Python API: SentenceEntityResolverApproach Scala API: SentenceEntityResolverApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Training a SNOMED resolution model using BERT sentence embeddings # Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. documentAssembler = DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) bertEmbeddings = BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;bert_embeddings&quot;) snomedTrainingPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, bertEmbeddings ]) snomedTrainingModel = snomedTrainingPipeline.fit(data) snomedData = snomedTrainingModel.transform(data).cache() # Then the Resolver can be trained with bertExtractor = SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols([&quot;bert_embeddings&quot;]) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(False) snomedModel = bertExtractor.fit(snomedData) // Training a SNOMED resolution model using BERT sentence embeddings // Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. val documentAssembler = new DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val bertEmbeddings = BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;bert_embeddings&quot;) val snomedTrainingPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, bertEmbeddings )) val snomedTrainingModel = snomedTrainingPipeline.fit(data) val snomedData = snomedTrainingModel.transform(data).cache() // Then the Resolver can be trained with val bertExtractor = new SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols(&quot;bert_embeddings&quot;) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(false) val snomedModel = bertExtractor.fit(snomedData) ChunkEntityResolver Contains all the parameters and methods to train a ChunkEntityResolverModel. It transform a dataset with two Input Annotations of types TOKEN and WORD_EMBEDDINGS, coming from e.g. ChunkTokenizer and ChunkEmbeddings Annotators and returns the normalized entity for a particular trained ontology / curated dataset. (e.g. ICD-10, RxNorm, SNOMED etc.) To use pretrained models please use ChunkEntityResolverModel and see the Models Hub for available models. Input Annotator Types: TOKEN, WORD_EMBEDDINGS Output Annotator Type: ENTITY Python API: ChunkEntityResolverApproach Scala API: ChunkEntityResolverApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.common import * from sparknlp.annotator import * from sparknlp.training import * import sparknlp_jsl from sparknlp_jsl.base import * from sparknlp_jsl.annotator import * from pyspark.ml import Pipeline # Training a SNOMED model # Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data # and their labels. document = DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) chunk = Doc2Chunk() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;chunk&quot;) token = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_healthcare_100d&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) chunkEmb = ChunkEmbeddings() .setInputCols([&quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;chunk_embeddings&quot;) snomedTrainingPipeline = Pipeline().setStages([ document, chunk, token, embeddings, chunkEmb ]) snomedTrainingModel = snomedTrainingPipeline.fit(data) snomedData = snomedTrainingModel.transform(data).cache() # Then the Resolver can be trained with snomedExtractor = ChunkEntityResolverApproach() .setInputCols([&quot;token&quot;, &quot;chunk_embeddings&quot;]) .setOutputCol(&quot;recognized&quot;) .setNeighbours(1000) .setAlternatives(25) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setEnableWmd(True).setEnableTfidf(True).setEnableJaccard(True) .setEnableSorensenDice(True).setEnableJaroWinkler(True).setEnableLevenshtein(True) .setDistanceWeights([1, 2, 2, 1, 1, 1]) .setAllDistancesMetadata(True) .setPoolingStrategy(&quot;MAX&quot;) .setThreshold(1e32) model = snomedExtractor.fit(snomedData) // Training a SNOMED resolution model using BERT sentence embeddings // Define pre-processing pipeline for training data. It needs consists of columns for the normalized training data and their labels. val documentAssembler = new DocumentAssembler() .setInputCol(&quot;normalized_text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val bertEmbeddings = BertSentenceEmbeddings.pretrained(&quot;sent_biobert_pubmed_base_cased&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;bert_embeddings&quot;) val snomedTrainingPipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, bertEmbeddings )) val snomedTrainingModel = snomedTrainingPipeline.fit(data) val snomedData = snomedTrainingModel.transform(data).cache() // Then the Resolver can be trained with val bertExtractor = new SentenceEntityResolverApproach() .setNeighbours(25) .setThreshold(1000) .setInputCols(&quot;bert_embeddings&quot;) .setNormalizedCol(&quot;normalized_text&quot;) .setLabelCol(&quot;label&quot;) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDIAN&quot;) .setCaseSensitive(false) val snomedModel = bertExtractor.fit(snomedData)",
    "url": "/docs/en/licensed_training",
    "relUrl": "/docs/en/licensed_training"
  },
  "97": {
    "id": "97",
    "title": "Version Compatibility",
    "content": "Spark NLP for Healthcare Spark NLP (Public) 4.3.0 4.3.0 4.2.8 4.2.8 4.2.7 4.2.7 4.2.4 4.2.4 4.2.3 4.2.4 4.2.2 4.2.2 4.2.1 4.2.1 4.2.0 4.2.0 4.1.0 4.1.0 4.0.2 4.0.2 4.0.0 4.0.0 3.5.3 3.4.4 3.5.2 3.4.4 3.5.1 3.4.3 3.5.0 3.4.2 3.4.2 3.4.2 3.4.1 3.4.1 3.4.0 3.4.0 3.3.4 3.3.4 3.3.2 3.3.2 3.3.1 3.3.1 3.3.0 3.3.0 3.2.3 3.2.3 3.2.2 3.2.2 3.2.1 3.2.1 3.2.0 3.2.1 3.1.3 3.1.3 3.1.2 3.1.2 3.1.1 3.1.0 3.1.0 3.1.0 3.0.3 3.0.3 3.0.2 3.0.2 3.0.1 3.0.1 3.0.0 3.0.1 2.7.6 2.7.4 2.7.5 2.7.4 2.7.4 2.7.3 2.7.3 2.7.3 2.7.2 2.6.5 2.7.1 2.6.4 2.7.0 2.6.3 2.6.2 2.6.2 2.6.0 2.6.0 2.5.5 2.5.5 2.5.3 2.5.3 2.5.2 2.5.2 2.5.0 2.5.0 2.4.7 2.4.5 2.4.6 2.4.5 2.4.5 2.4.5 2.4.2 2.4.2 2.4.1 2.4.1 2.4.0 2.4.0 2.3.6 2.3.6 2.3.5 2.3.5 2.3.4 2.3.4 Spark NLP for Healthcare Spark OCR 4.2.4 4.3.0 4.2.3 4.2.4 4.2.1 4.2.0 4.1.0 4.1.0 4.0.0 4.0.0 3.5.3 3.13.0 3.5.1 3.12.0 3.5.0 3.11.0 3.4.2 3.11.0 3.4.1 3.11.0 3.4.0 3.11.0 3.3.4 3.10.0 3.3.2 3.9.0 3.3.1 3.9.0",
    "url": "/docs/en/licensed_version_compatibility",
    "relUrl": "/docs/en/licensed_version_compatibility"
  },
  "98": {
    "id": "98",
    "title": "Legal Document Splitting - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/long_document_splitting",
    "relUrl": "/long_document_splitting"
  },
  "99": {
    "id": "99",
    "title": "Mental Health - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/mental_health",
    "relUrl": "/mental_health"
  },
  "100": {
    "id": "100",
    "title": "Middle Eastern Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/middle_eastern_languages",
    "relUrl": "/middle_eastern_languages"
  },
  "101": {
    "id": "101",
    "title": "Experiment Tracking",
    "content": "Serialization and Experiment Tracking with MLFlow (Python) About MLFLow Spark NLP uses Spark MLlib Pipelines, what are natively supported by MLFlow. MLFlow is, as stated in their official webpage, an open source platform for the machine learning lifecycle, that includes: Mlflow Tracking: Record and query experiments: code, data, config, and results MLflow Projects: Package data science code in a format to reproduce runs on any platform MLflow Models: Deploy machine learning models in diverse serving environments Model Registry: Store, annotate, discover, and manage models in a central repository MLFlow is also integrated in Databricks, so you will be able to track your experiments in any Databricks environment, and even use MLFLow Model Registry to serve models for production purposes, using the REST API (see section “Productionizing Spark NLP”). We will be using in this documentation Jupyter Notebook syntax. Available configurations There are several ways of deploying a MLFlow Model Registry: 1) Scenario 1: MLflow on localhost with no Tracking Server: This scenario uses a localhost folder (./mlruns by default) to serialize and store your models, but there is no tracking server available (version tracking will be disabled). 2) Scenario 2: MLflow on localhost with a Tracking Server This scenario uses a localhost folder (./mlruns by default) to serialize and store your mdoels, and a database as a Tracking Sever. It uses SQLAlchemy under the hood, so the following databases are supported: mssql, postgresql, mysql, sqlite. We are going to show how to implement this scenario with a mysql database. 3) Scenario 3: MLflow on remote with a Tracking Server This scenario is a remote version of Scenario 2. It uses a remote S3 bucket to serialize and store your mdoels, and a database as a Tracking Sever. Again, it uses SQLAlchemy for the Tracking Server under the hood, so the following databases are supported: mssql, postgresql, mysql, sqlite. In this case, you can use any service as AWS RDS or Azure SQL Database. Requirements As we said before, we are going to showcase Scenario 2. Since we want to have a Experiment Tracking Server with mysql, we will need to install in our server the requirements for it. !sudo apt-get install -y python-mysqldb mysql-server libmysqlclient-dev Also, let’s install a mysql Python interface library, called pymsql, to access mysql databases. !pip install mysqlclient pymysql We will also need MLFlow (this example was tested with version 1.21.0) !pip install mlflow Finally, make sure you follow the Spark NLP installation, available here Instantiating a MySQL database We are going to use Docker to instantiate a MySQL container with a persistent volume, but you can install it directly on your machine without Docker. To do that, we will need to have installed (feel free to skip this step if you will install MySql without Docker): Docker Docker-compose In our case, I used this docker-compose.yml file to instantiate a mysql database with a persistent volume: version: &#39;3&#39; services: # MySQL mflow_models: container_name: mlflow_models image: mysql:8.0 command: mysqld --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: mlflow_models MYSQL_USER: jsl MYSQL_PASSWORD: passpass MYSQL_ALLOW_EMPTY_PASSWORD: &quot;yes&quot; ports: - &#39;3306:3306&#39; volumes: - &#39;./docker/db/data:/var/lib/mysql&#39; - &#39;./docker/db/my.cnf:/etc/mysql/conf.d/my.cnf&#39; - &#39;./docker/db/sql:/docker-entrypoint-initdb.d&#39; Just by executing the following command in the folder where your docker-compose.yml file is, you will have your MySQL engine, with a mlflow_models database running and prepared for MLFlow Experiment Tracking: !sudo docker-compose up -d . Make sure it’s running using the following command: `!docker ps | grep -o mlflow_models Connection string You will need a connection string that will tell MLFlow (SQLAlchemy) how to reach that database. Connections strings in SQLALchemy have this format: &lt;dialect&gt;+&lt;driver&gt;://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt; In our case, we declare a CONNECTION_STRING var as: CONNECTION_STRING = f&quot;mysql+pymysql://root:root@localhost:3306/mlflow_models&quot; Imports Let’s now import all the libraries we will need. Generic imports import json import os from sklearn.metrics import classification_report import time import mlflow from mlflow.models.signature import infer_signature from urllib.parse import urlparse import pandas as pd import glob Spark NLP imports import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline import pyspark.sql.functions as F from sparknlp.training import CoNLL from pyspark.sql import SparkSession Setting the connection string in MLFLow Now that we have imported mlflow, let’s set the connection string we had prepared before. mlflow.set_tracking_uri(CONNECTION_STRING) mlflow.get_tracking_uri() # This checks if it was set properly Constant with pip_requirements MLFLow requires either a conda_env (conda environment) definition of the requirements of your models, or a pip_requirements list with all pip libraries. We will use this second way, so let’s prepare the list with Spark NLP and MLFlow: PIP_REQUIREMENTS = [f&quot;sparknlp=={sparknlp.version()}&quot;, f&quot;mlflow=={mlflow.__version__}&quot;] PIP_REQUIREMENTS # This checks if it was set properly Training a NERDLApproach() We will be showcasing the serialization and experiment tracking of NERDLApproach(). There is one specific util that is able to parse the log of that approach in order to extract the metrics and charts. Let’s get it. Ner Log Parser Util !wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/utils/ner_image_log_parser.py Now, let’s import the library: import ner_image_log_parser Starting a SparkNLP session It’s important we create a Spark NLP Session using the Session Builder, since we need to specify the jars not only of Spark NLP, but also of MLFlow. def start(): builder = SparkSession.builder .appName(&quot;Spark NLP Licensed&quot;) .master(&quot;local[80]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;256G&quot;) .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.driver.maxResultSize&quot;,&quot;4000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.2,org.mlflow:mlflow-spark:1.21.0&quot;) return builder.getOrCreate() spark = start() Training dataset preparation Let’s download some training and test datasets: !wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.train !wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.testa TRAIN_DATASET = &quot;eng.train&quot; TEST_DATASET = &quot;eng.testa&quot; Let’s read the training dataset: training_data = CoNLL().readDataset(spark, TRAIN_DATASET) training_data.show(3) Let’s get the size: %%time TRAINING_SIZE = training_data.count() TRAINING_SIZE Hyperparameters configuration Let’s configure our hyperparameter values. MODEL_NAME = &#39;&#39; # Add your model name here. Example: clinical_ner EXPERIMENT_NAME = &#39;&#39; # Add your experiment name here. Example: testing_dropout OUTPUT_DIR = f&quot;{MODEL_NAME}_{EXPERIMENT_NAME}_output&quot; # Output folder of all your model artifacts MODEL_DIR = f&quot;model&quot; # Name of the folder where the MLFlow model will be stored MAX_EPOCHS = 10 # Adapt me to your experiment LEARNING_RATE = 0.003 # Adapt me to your experiment BATCH_SIZE = 2048 # Adapt me to your experiment RANDOM_SEED = 0 # Adapt me to your experiment VALIDATION_SPLIT = 0.1 # Adapt me to your experiment Creating the experiment Now, we are ready to instantiate an experiment in MLFlow EXPERIMENT_ID = mlflow.create_experiment(f&quot;{MODEL_NAME}_{EXPERIMENT_NAME}&quot;) Each time you want to test a different thing, change the EXPERIMENT_NAME and rerun the line above to create a new entry in the experiment. By changing the experiment name, a new experiment ID will be generated. Each experiment ID groups all runs in separates folder inside ./mlruns. Pipeline creation document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&#39;document&#39;]) .setOutputCol(&#39;sentence&#39;) token = Tokenizer() .setInputCols([&#39;sentence&#39;]) .setOutputCol(&#39;token&#39;) embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) ner_approach = NerDLApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(MAX_EPOCHS) .setLr(LEARNING_RATE) .setBatchSize(BATCH_SIZE) .setRandomSeed(RANDOM_SEED) .setVerbose(1) .setEnableOutputLogs(True) .setIncludeConfidence(True) .setIncludeAllConfidenceScores(True) .setEvaluationLogExtended(True) .setOutputLogsPath(OUTPUT_DIR) .setValidationSplit(VALIDATION_SPLIT) Let’s create a preprocessing pipeline without the NerDLApproach(): ner_preprocessing_pipeline = Pipeline(stages=[ document, sentence, token, embeddings ]) And a training pipeline with it: ner_training_pipeline = Pipeline(stages = ner_preprocessing_pipeline.getStages() + [ner_approach]) Preparing inference objects Now, let’s prepare the inference as well, since we will train and infer afterwards, and store all the results of training and inference as artifacts in our MLFlow object. Test dataset preparation test_data = CoNLL().readDataset(spark, TEST_DATASET) Setting the names of the inference objects INFERENCE_NAME = &quot;inference.parquet&quot; # This is the name of the results inference on the test dataset, serialized in parquet, CLASSIFICATION_REPORT_LOG_NAME = &quot;classification_report.txt&quot; # Name of the classification report from scikit-learn on Ner Entities PREC_REC_F1_NAME = &quot;precrecf1.jpg&quot; # Name of the precision-recall-f1 file MACRO_MICRO_AVG_NAME = &quot;macromicroavg.jpg&quot; # Name of the macro-micro-average file LOSS_NAME = &quot;loss.jpg&quot; # Name of the loss plot file Now, let’s run the experiment The experiment has already been created before (see “Creating the experiment” section). So we take the ID and start a run. Each time you run execute this cell, you will get a different run for the same experiment. If you want to change the experiment id (and name), go back to “Hyperparameters configuration”. As mentioned before, by changing the experiment name, a new experiment ID will be generated. Each experiment ID groups all runs in separates folder inside ./mlruns. with mlflow.start_run(experiment_id=EXPERIMENT_ID) as run: # Printing RUN and EXPERIMENT ID # ============================== print(f&quot;Model name: {MODEL_NAME}&quot;) RUN_ID = run.info.run_id print(f&quot;Run id: {RUN_ID}&quot;) EXPERIMENT_ID = run.info.experiment_id print(f&quot;Experiment id: {EXPERIMENT_ID}&quot;) # Training the model # ================== print(&quot;Starting training...&quot;) start = time.time() ner_model = ner_training_pipeline.fit(training_data) end = time.time() ELAPSED_SEC_TRAINING = end - start print(&quot;- Finished!&quot;) # Saving the model in TensorFlow (ready to be loaded using NerDLModel.load) # ============================== print(&quot;Saving the model...&quot;) ner_model.stages[-1].write().overwrite().save(f&quot;{OUTPUT_DIR}/{MODEL_DIR}/{MODEL_NAME}&quot;) print(&quot;- Finished!&quot;) # Loading the model (to check everything worked) # ============================== print(&quot;Loading back the model...&quot;) loaded_ner_model = NerDLModel.load(f&quot;{OUTPUT_DIR}/{MODEL_DIR}/{MODEL_NAME}&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) # Creating the inference pipeline with the loaded model # ============================== ner_prediction_pipeline = Pipeline(stages = ner_preprocessing_pipeline.getStages() + [loaded_ner_model]) # Triggering inference # ============================== print(&quot;Starting inference...&quot;) prediction_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) prediction_model = ner_prediction_pipeline.fit(prediction_data) start = time.time() prediction_model.transform(test_data).write.mode(&#39;overwrite&#39;).parquet(f&quot;{OUTPUT_DIR}/{INFERENCE_NAME}&quot;) end = time.time() ELAPSED_SEC_INFERENCE = end - start print(&quot;- Finished!&quot;) # Calculating NER metrics from logs using scikit-learn &#39;classification_report&#39; # ============================== print(&quot;Starting metric calculation...&quot;) predictions = spark.read.parquet(f&quot;{OUTPUT_DIR}/{INFERENCE_NAME}&quot;) preds_df = predictions.select(F.explode(F.arrays_zip(&#39;token.result&#39;,&#39;label.result&#39;,&#39;ner.result&#39;)).alias(&quot;cols&quot;)) .select(F.expr(&quot;cols[&#39;0&#39;]&quot;).alias(&quot;token&quot;), F.expr(&quot;cols[&#39;1&#39;]&quot;).alias(&quot;ground_truth&quot;), F.expr(&quot;cols[&#39;2&#39;]&quot;).alias(&quot;prediction&quot;)).toPandas() preds_df = preds_df.fillna(value=&#39;O&#39;) with open(f&#39;{OUTPUT_DIR}/{CLASSIFICATION_REPORT_LOG_NAME}&#39;, &#39;w&#39;) as f: metrics = classification_report(preds_df[&#39;ground_truth&#39;], preds_df[&#39;prediction&#39;]) f.write(metrics) metrics_dict = classification_report(preds_df[&#39;ground_truth&#39;], preds_df[&#39;prediction&#39;], output_dict=True) print(&quot;- Finished!&quot;) # Printing metrics # ============================== print(f&quot;Training dataset size: {TRAINING_SIZE}&quot;) print(f&quot;Training time (sec): {ELAPSED_SEC_TRAINING}&quot;) print(f&quot;Inference dataset size: {TEST_SIZE}&quot;) print(f&quot;Inference time (sec): {ELAPSED_SEC_INFERENCE}&quot;) print(f&quot;Metrics: n&quot;) print(metrics) # Logging all our params, metrics, charts and artifacts using MLFlow # - log_param: logs a configuration param # - log_artifacts: logs a folder and all its files # - log_artifact: adds a file # - log_metric: logs a metric, what allows you use the MLFlow UI to visually compare results # ============================== print(&quot;Logging params, artifacts, metrics and charts in MLFlow&quot;) mlflow.log_param(&quot;training_size&quot;, TRAINING_SIZE) mlflow.log_param(&quot;training_time&quot;, ELAPSED_SEC_TRAINING) mlflow.log_param(&quot;model_name&quot;, MODEL_NAME) mlflow.log_param(&quot;test_size&quot;, TEST_SIZE) mlflow.log_param(&quot;test_time&quot;, ELAPSED_SEC_INFERENCE) mlflow.log_param(&quot;run_id&quot;, RUN_ID) mlflow.log_param(&quot;max_epochs&quot;, MAX_EPOCHS) mlflow.log_param(&quot;learning_rate&quot;, LEARNING_RATE) mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE) mlflow.log_param(&quot;random_seed&quot;, RANDOM_SEED) mlflow.log_param(&quot;validation_split&quot;, VALIDATION_SPLIT) for file in glob.glob(f&quot;{OUTPUT_DIR}/*.log&quot;): images = {} images.update(ner_image_log_parser.get_charts(file, img_prec_rec_f1_path=f&quot;{OUTPUT_DIR}/{PREC_REC_F1_NAME}&quot;, img_macro_micro_avg_path=f&quot;{OUTPUT_DIR}/{MACRO_MICRO_AVG_NAME}&quot;)) images.update(ner_image_log_parser.loss_plot(file, img_loss_path=f&quot;{OUTPUT_DIR}/{LOSS_NAME}&quot;)) mlflow.log_artifacts(OUTPUT_DIR) mlflow.log_artifact(TRAIN_DATASET) mlflow.log_artifact(TEST_DATASET) for k,v in metrics_dict.items(): if isinstance(v, dict): for kv, vv in v.items(): mlflow.log_metric(f&quot;{k}_{kv}&quot;, vv) else: mlflow.log_metric(k, v) print(&quot;- Finished!&quot;) print(&quot;Logging the model in MLFlow&quot;) # ============================== # Logging the model to be explored in the MLFLow UI tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme # Model registry does not work with file store if tracking_url_type_store != &quot;file&quot;: # Register the model # There are other ways to use the Model Registry, which depends on the use case, # please refer to the doc for more information: # https://mlflow.org/docs/latest/model-registry.html#api-workflow mlflow.spark.log_model(ner_model, f&quot;{MODEL_NAME}_{EXPERIMENT_ID}_{RUN_ID}&quot;, registered_model_name=MODEL_NAME, pip_requirements=PIP_REQUIREMENTS) else: mlflow.spark.log_model(ner_model, f&quot;{MODEL_NAME}_{EXPERIMENT_ID}_{RUN_ID}&quot;, pip_requirements=PIP_REQUIREMENTS) print(&quot;- Finished!&quot;) # Saving the model, in case you want to export it # ============================== print(&quot;Saving the model...&quot;) input_example = predictions.select(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;).limit(1).toPandas() mlflow.spark.save_model(loaded_ner_model, MODEL_NAME, pip_requirements=PIP_REQUIREMENTS, input_example=input_example) print(&quot;- Finished!&quot;) This is an example of the output generated: Model name: NER_base_2048_mlflow Run id: 5f8601fbfc664b3b91c7c61cde31e16d Experiment id: 2 Starting training... - Finished! Saving the model... - Finished! Loading back the model... Starting inference... - Finished! Starting metric calculation... - Finished! Training dataset size: 14041 Training time (sec): 12000.3835768699646 Inference dataset size: 3250 Inference time (sec): 2900.713200330734253 Metrics: precision recall f1-score support B-LOC 0.85 0.82 0.83 1837 B-MISC 0.86 0.83 0.81 922 B-ORG 0.81 0.83 0.82 1341 B-PER 0.86 0.81 0.80 1842 I-LOC 0.80 0.80 0.80 257 I-MISC 0.80 0.80 0.80 346 I-ORG 0.83 0.89 0.80 751 I-PER 0.86 0.83 0.82 1307 O 0.81 0.98 0.84 43792 accuracy 0.87 52395 macro avg 0.88 0.83 0.88 52395 weighted avg 0.84 0.87 0.85 52395 Logging params, artifacts, metrics and charts in MLFlow - Finished! Logging the model in MLFlow Registered model &#39;NER_base_2048_mlflow&#39; already exists. Creating a new version of this model... 2021/11/25 11:51:24 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: NER_base_2048_mlflow, version 2 Created version &#39;2&#39; of model &#39;NER_base_2048_mlflow&#39;. - Finished! Saving the model... - Finished! MLFLow UI to check results Now, we just need to launch the MLFLow UI to see: All the experiments All the runs in each experiment The automatic versioning in the Tracking Server database in MySQL THe MLFlow model, and the TensorFlow version as well The UI for comparing the metrics we set using log_metrics The UI for visualizing the image artifacts we have logged (charts) etc !mlflow ui --backend-store-uri $CONNECTION_STRING Some example screenshots",
    "url": "/docs/en/mlflow",
    "relUrl": "/docs/en/mlflow"
  },
  "102": {
    "id": "102",
    "title": "Models",
    "content": "All the models available in the Annotation Lab are listed in this page. The models are either trained within the Annotation Lab, uploaded to Annotation Lab by admin users, or downloaded from NLP Models Hub. General information about the models like labels/categories and the source (downloaded/trained/uploaded) is viewable. It is possible to delete any model, or redownload failed ones from the options available under the more action menu on each model. All available models are listed in the Spark NLP Pipeline Config on the Setup Page of any project and are ready to be included in the Labeling Config for pre-annotation. Auto download of model dependencies Starting from version 2.8.0, Annotation Lab automatically downloads all the necessary dependencies along with the model saving users valuable time from manually downloading the dependencies. Previously, users had to first download the model from the Models Hub page (e.g. ner_healthcare_de) and then again download the necessary embeddings required to train the model (e.g. w2v_cc_300d). Custom Model Upload Custom models can be uploaded using the Upload button present in the top right corner of the page. The labels predicted by this model need to be specified in the upload form. Note: The models to upload need to be Spark NLP compatible.",
    "url": "/docs/en/alab/models",
    "relUrl": "/docs/en/alab/models"
  },
  "103": {
    "id": "103",
    "title": "Available Models and Pipelines",
    "content": "",
    "url": "/models",
    "relUrl": "/models"
  },
  "104": {
    "id": "104",
    "title": "Models Hub",
    "content": "Annotation Lab offers tight integration with NLP Models Hub. Any compatible model and embeddings can be downloaded and made available to the Annotation Lab users for pre-annotations either from within the application or via manual upload. NLP Models HUB page is accessible from the left navigation panel by users in the Admins group. The Models Hub page lists all the pre-trained models and embeddings from NLP Models Hub that are compatible with the Spark NLP version present in the Annotation Lab. Search Search features are offered to help users identify the models they need based on their names. Additional information such as Library Edition, task for which the model was build as well as publication date are also available on the model tile. Language of the model/embeddings is also available as well as a direct link to the model description page on the NLP Models Hub where you can get more details about the model and usage examples. Filter Users can use the Edition filter to search models specific to an edition. It includes all supported NLP editions: Healthcare, Opensource, Legal, Finance, and Visual. When selecting one option, e.g. “Legal”, users will be presented with all available models for that specific domain. This will ease the exploration of available models, which can then easily be downloaded and used within Annotation Lab projects. To make searching models/embeddings more efficient, Annotation Lab offers a Language filter. Users can select models/embeddings on the Models Hub page according to their language preference. Download By selecting one or multiple models from the list, users can download those to the Annotation Lab. The licensed (Healthcare, Visual, Finance or Legal) models and embeddings are available to download only when a valid license is present. One restriction on models download/upload is related to the available disk space. Any model download requires that the double of its size is available on the local storage. If enough space is not available then the download cannot proceed. Disk usage view, search, and filter features are available on the upper section of the Models Hub page. Benchmarking For the licensed models, benchmarking information is available on the Models Hub page. To check this click on the icon on the lower right side of the model tile. The benchmarking information can be used to guide the selection of the model you include in your project configuration.",
    "url": "/docs/en/alab/models_hub",
    "relUrl": "/docs/en/alab/models_hub"
  },
  "105": {
    "id": "105",
    "title": "NLP Server",
    "content": "This is a ready to use NLP Server for analyzing text documents using NLU library. Over 4500+ industry grade NLP Models in 300+ Languages are available to use via a simple and intuitive UI, without writing a line of code. For more expert users and more complex tasks, NLP Server also provides a REST API that can be used to process high amounts of data. The models, refered to as spells, are provided by the NLU library and powered by the most widely used NLP library in the industry, Spark NLP. NLP Server is free for everyone to download and use. There is no limitation in the amount of text to analyze. You can setup NLP-Server as a Docker Machine in any enviroment or get it via the AWS Marketplace in just 1 click. Web UI The Web UI is accessible at the following URL: http://localhost:5000/ It allows a very simple and intuitive interaction with the NLP Server. As a first step the user chooses the spell from the first dropdown. All NLU spells are available. Then the user has to provide a text document for analysis. This can be done by either copy/pasting text on the text box, or by uploading a csv/json file. After selecting the grouping option, the user clicks on the Preview button to get the results for the first 10 rows of text. REST API NLP Server includes a REST API which can be used to process any amount of data using NLU. Once you deploy the NLP Server, you can access the API documentation at the following URL http://localhost:5000/docs. Integrate via the Rest API Rest APIs are a popular way to integrate different services into one common platform. NLP Server offers its own API to offer a quick programmatic integration with customers’ services and applications. Bellow is a quick overview of the provided endpoints. More details are provided in the API documentation available http://localhost:5000/docs. Start to analyze Endpoint : /results Method : POST Content-Type (Format) : multipart/form-data Parameters: Spell – the spell that you want to use for this analyze (if you want to run multiple spells you should join them with space character) Data – The data to analyse that can be a single text or an array of strings or files. Grouping – can be choosen from [“document”, “sentence”, “entity”, “word”]. The default value is “” for automatic selection based on spell. Format – The format of the provided input. The default value is “text”. Response: uuid – the unique identifier for the analysis process. Check the status of an analysis process Endpoint : /results/{uuid}/status Method : GET Content-Type (Format) : application/json Response: code – the status code that can be one of “progress”, “success”, “failure”, “broken spell”, “invalid license”, “licensed spell with no license” message – the status message Get the results After ensuring the status of an analysis is “success” you can get the results: Endpoint : /results/{uuid} Method : GET Content-Type (Format) : application/json Parameters: target – if the specified target is “preview” you only get a small part of results. Response: A JSON object that contains the results generated by the spell (each spell has their own specific keys) How to use in Python import requests # Invoke Processing with tokenization spell r = requests.post(f&#39;http://localhost:5000/api/results&#39;,json={&quot;spell&quot;: &quot;tokenize&quot;,&quot;data&quot;: &quot;I love NLU! &lt;3&quot;}) # Use the uuid to get your processed data uuid = r.json()[&#39;uuid&#39;] # Get status of processing r = requests.get(f&#39;http://localhost:5000/api/results/{uuid}/status&#39;).json &gt;&gt;&gt; {&#39;status&#39;: {&#39;code&#39;: &#39;success&#39;, &#39;message&#39;: None}} # Get results r = requests.get(f&#39;http://localhost:5000/api/results/{uuid}&#39;).json() &gt;&gt;&gt; {&#39;sentence&#39;: {&#39;0&#39;: [&#39;I love NLU! &lt;3&#39;]}, &#39;document&#39;: {&#39;0&#39;: &#39;I love NLU! &lt;3&#39;}, &#39;token&#39;: {&#39;0&#39;: [&#39;I&#39;, &#39;love&#39;, &#39;NLU&#39;, &#39;!&#39;, &#39;&lt;3&#39;]}} Import a license key Thanks to the close integration between NLP Server and https://my.JohnSnowLabs.com website, users can easily select and import one of the available licenses to be used on NLP Server. The steps to execute for this are: 1.Click on Login via MYJSL button on the menu bar. 2.In the pop-up window click on the Authorize button. 3.After redirecting back to NLP Server click on the Choose License button. 4.In the modal choose the license that you want to use and then click on the Select button. 5.After the above steps you will see this success alert on the top right of the page. That confirms the import of license completed successfully.",
    "url": "/docs/en/nlp_server/nlp_server",
    "relUrl": "/docs/en/nlp_server/nlp_server"
  },
  "106": {
    "id": "106",
    "title": "Visual NLP (Spark OCR)",
    "content": "Spark OCR is another commercial extension of Spark NLP for optical character recognition from images, scanned PDF documents, Microsoft DOCX and DICOM files. If you want to try it out on your own documents click on the below button: Try Free Spark OCR is built on top of Apache Spark and offers the following capabilities: Image pre-processing algorithms to improve text recognition results: Adaptive thresholding &amp; denoising Skew detection &amp; correction Adaptive scaling Layout Analysis &amp; region detection Image cropping Removing background objects Text recognition, by combining NLP and OCR pipelines: Extracting text from images (optical character recognition) Support English, German, French, Spanish, Russian, Vietnamese and Arabic languages Extracting data from tables Recognizing and highlighting named entities in PDF documents Masking sensitive text in order to de-identify images Table detection and recognition from images Signature detection Visual document understanding Document classification Visual NER Output generation in different formats: PDF, images, or DICOM files with annotated or masked entities Digital text for downstream processing in Spark NLP or other libraries Structured data formats (JSON and CSV), as files or Spark data frames Scale out: distribute the OCR jobs across multiple nodes in a Spark cluster. Frictionless unification of OCR, NLP, ML &amp; DL pipelines. Spark OCR Workshop If you prefer learning by example, click the button below to checkout the workshop repository full of fresh examples. Spark OCR Workshop Below, you can follow a more theoretical and thorough quick start guide. Quickstart Examples Images The following code example creates an OCR Pipeline for processing image(s). The image file(s) can contain complex layout like columns, tables, images inside. PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers._ val imagePath = &quot;path to image files&quot; // Read image files as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) // Transform binary content to image val binaryToImage = new BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) // OCR val ocr = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) // Define Pipeline val pipeline = new Pipeline() pipeline.setStages(Array( binaryToImage, ocr )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = modelPipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image files&quot; # Read image files as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) # Transform binary content to image binaryToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # OCR ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) # Define Pipeline pipeline = PipelineModel(stages=[ binaryToImage, ocr ]) data = pipeline.transform(df) data.show() Scanned PDF files Next sample provides an example of OCR Pipeline for processing PDF files containing image data. In this case, the PdfToImage transformer is used to convert PDF file to a set of images. PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers._ val imagePath = &quot;path to pdf files&quot; // Read pdf files as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) // Transform PDF file to the image val pdfToImage = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) // OCR val ocr = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( pdfToImage, ocr )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = modelPipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to pdf files&quot; # Read pdf files as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) # Transform PDF file to the image pdfToImage = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # OCR ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) # Define pipeline pipeline = PipelineModel(stages=[ pdfToImage, ocr ]) data = pipeline.transform(df) data.show() PDF files (scanned or text) In the following code example we will create OCR Pipeline for processing PDF files that contain text or image data. For each PDF file, this pipeline will: extract the text from document and save it to the text column if text contains less than 10 characters (so the document isn’t PDF with text layout) it will process the PDF file as a scanned document: convert PDF file to an image detect and split image to regions run OCR and save output to the text column PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers._ val imagePath = &quot;path to PDF files&quot; // Read PDF files as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) // Extract text from PDF text layout val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) // In case of `text` column contains less then 10 characters, // pipeline run PdfToImage as fallback method val pdfToImage = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setFallBackCol(&quot;text&quot;) .setMinSizeBeforeFallback(10) // OCR val ocr = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( pdfToText, pdfToImage, ocr )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = modelPipeline.transform(df) data.show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to PDF files&quot; # Read PDF files as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) # Extract text from PDF text layout pdfToText = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) # In case of `text` column contains less then 10 characters, # pipeline run PdfToImage as fallback method pdfToImage = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setFallBackCol(&quot;text&quot;) .setMinSizeBeforeFallback(10) # OCR ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) # Define pipeline pipeline = PipelineModel(stages=[ pdfToText, pdfToImage, ocr, ]) data = pipeline.transform(df) data.show() Images (streaming mode) Next code segments provide an example of streaming OCR pipeline. It processes images and stores results to memory table. PythonScala val imagePath = &quot;path folder with images&quot; val batchDataFrame = spark.read.format(&quot;binaryFile&quot;).load(imagePath).limit(1) val pipeline = new Pipeline() pipeline.setStages(Array( binaryToImage, binarizer, ocr )) val modelPipeline = pipeline.fit(batchDataFrame) // Read files in streaming mode val dataFrame = spark.readStream .format(&quot;binaryFile&quot;) .schema(batchDataFrame.schema) .load(imagePath) // Call pipeline and store results to &#39;results&#39; memory table val query = modelPipeline.transform(dataFrame) .select(&quot;text&quot;, &quot;exception&quot;) .writeStream .format(&quot;memory&quot;) .queryName(&quot;results&quot;) .start() imagePath = &quot;path folder with images&quot; batchDataFrame = spark.read.format(&quot;binaryFile&quot;).load(imagePath).limit(1) pipeline = Pipeline() pipeline.setStages(Array( binaryToImage, binarizer, ocr )) modelPipeline = pipeline.fit(batchDataFrame) # Read files in streaming mode dataFrame = spark.readStream .format(&quot;binaryFile&quot;) .schema(batchDataFrame.schema) .load(imagePath) # Call pipeline and store results to &#39;results&#39; memory table query = modelPipeline.transform(dataFrame) .select(&quot;text&quot;, &quot;exception&quot;) .writeStream() .format(&quot;memory&quot;) .queryName(&quot;results&quot;) .start() For getting results from memory table following code could be used: PythonScala spark.table(&quot;results&quot;).select(&quot;path&quot;, &quot;text&quot;).show() spark.table(&quot;results&quot;).select(&quot;path&quot;, &quot;text&quot;).show() More details about Spark Structured Streaming could be found in spark documentation. Advanced Topics Error Handling Pipeline execution would not be interrupted in case of the runtime exceptions while processing some records. In this case OCR transformers would fill exception column that contains transformer name and exception. NOTE: Storing runtime errors to the exception field allows to process batch of files. Output Here is an output with exception when try to process js file using OCR pipeline: PythonScala result.select(&quot;path&quot;, &quot;text&quot;, &quot;exception&quot;).show(2, false) result.select(&quot;path&quot;, &quot;text&quot;, &quot;exception&quot;).show(2, False) +-+-+--+ |path |text |exception | +-+-+--+ |file:jquery-1.12.3.js | |BinaryToImage_c0311dc62161: Can&#39;t open file as image.| |file:image.png |I prefer the morning flight through Denver |null | +-+-+--+ Performance In case of big count of text PDF’s in dataset need have manual partitioning for avoid skew in partitions and effective utilize resources. For example the randomization could be used.",
    "url": "/docs/en/ocr",
    "relUrl": "/docs/en/ocr"
  },
  "107": {
    "id": "107",
    "title": "Installation",
    "content": "Spark OCR is built on top of Apache Spark. Currently, it supports 3.0., 2.4. and 2.3.* versions of Spark. It is recommended to have basic knowledge of the framework and a working environment before using Spark OCR. Refer to Spark documentation to get started with Spark. Spark OCR requires: Scala 2.11 or 2.12 related to the Spark version Python 3.7 + (in case using PySpark) Before you start, make sure that you have: Spark OCR jar file (or secret for download it) Spark OCR python wheel file License key If you don’t have a valid subscription yet and you want to test out the Spark OCR library press the button below: Try Free Spark OCR from Scala You can start a spark REPL with Scala by running in your terminal a spark-shell including the com.johnsnowlabs.nlp:spark-ocr_2.11:1.0.0 package: spark-shell --jars #### The #### is a secret url only available for license users. If you have purchased a license but did not receive it please contact us at info@johnsnowlabs.com. Start Spark OCR Session The following code will initialize the spark session in case you have run the jupyter notebook directly. If you have started the notebook using pyspark this cell is just ignored. Initializing the spark session takes some seconds (usually less than 1 minute) as the jar from the server needs to be loaded. The #### in .config(“spark.jars”, “####”) is a secret code, if you have not received it please contact us at info@johnsnowlabs.com. import org.apache.spark.sql.SparkSession val spark = SparkSession .builder() .appName(&quot;Spark OCR&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;4G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;2G&quot;) .config(&quot;spark.jars&quot;, &quot;####&quot;) .getOrCreate() Spark OCR from Python Install Python package Install python package using pip: pip install spark-ocr==1.8.0.spark24 --extra-index-url #### --ignore-installed The #### is a secret url only available for license users. If you have purchased a license but did not receive it please contact us at info@johnsnowlabs.com. Start Spark OCR Session Manually from pyspark.sql import SparkSession spark = SparkSession .builder .appName(&quot;Spark OCR&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;4G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;2G&quot;) .config(&quot;spark.jars&quot;, &quot;https://pypi.johnsnowlabs.com/####&quot;) .getOrCreate() Using Start function Another way to initialize SparkSession with Spark OCR to use start function in Python. Start function has following params: Param name Type Default Description secret string None Secret for download Spark OCR jar file jar_path string None Path to jar file in case you need to run spark session offline extra_conf SparkConf None Extra spark configuration master_url string local[*] Spark master url nlp_version string None Spark NLP version for add it Jar to session nlp_internal boolean/string None Run Spark session with Spark NLP Internal if set to ‘True’ or specify version nlp_secret string None Secret for get Spark NLP Internal jar keys_file string keys.json Name of the json file with license, secret and aws keys For start Spark session with Spark NLP please specify version of it in nlp_version param. Example: from sparkocr import start spark = start(secret=secret, nlp_version=&quot;2.4.4&quot;) Databricks The installation process to Databricks includes following steps: Installing Spark OCR library to Databricks and attaching it to the cluster Same step for Spark OCR python wheel file Adding license key Adding cluster init script for install dependencies Please look databricks python helpers for simplify install init script. Example notebooks: Spark OCR Databricks python notebooks Spark OCR Databricks Scala notebooks",
    "url": "/docs/en/ocr_install",
    "relUrl": "/docs/en/ocr_install"
  },
  "108": {
    "id": "108",
    "title": "Object detection",
    "content": "ImageHandwrittenDetector ImageHandwrittenDetector is a DL model for detect handwritten text on the image. It’s based on Cascade Region-based CNN network. Detector support following labels: ‘signature’ ‘date’ ‘name’ ‘title’ ‘address’ ‘others’ Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description scoreThreshold float 0.5 Score threshold for output regions. outputLabels Array[String]   White list for output labels. labels Array[String]   List of labels Output Columns Param name Type Default Column Data Description outputCol string table_regions array of [Coordinaties]ocr_structures#coordinate-schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect signature signature_detector = ImageHandwrittenDetector .pretrained(&quot;image_signature_detector_gsa0628&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;signature_regions&quot;) draw_regions = ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;signature_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, signature_detector, draw_regions ]) data = pipeline.transform(df) display_images(data, &quot;image_with_regions&quot;) import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect signature val signature_detector = ImageHandwrittenDetector .pretrained(&quot;image_signature_detector_gsa0628&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;signature_regions&quot;) val draw_regions = new ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;signature_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, signature_detector, draw_regions ]) val data = pipeline.transform(df) data.storeImage(&quot;image_with_regions&quot;) Output: ImageTextDetector ImageTextDetector is a DL model for detecting text on the image. It’s based on CRAFT network architecture. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description scoreThreshold float 0.9 Score threshold for output regions. Regions with an area below the threshold won’t be returned. sizeThreshold int 5 Threshold for the area of the detected regions. textThreshold float 0.4f Threshold for the score of a region potentially containing text. The region score represents the probability that a given pixel is the center of the character. Higher values for this threshold will result in that only regions for which the confidence of containing text is high will be returned. linkThreshold float 0.4f Threshold for the the link(affinity) score. The link score represents the space allowed between adjacent characters to be considered as a single word. width integer 0 Scale width to this value, if 0 use original width height integer 0 Scale height to this value, if 0 use original height Output Columns Param name Type Default Column Data Description outputCol string table_regions array of [Coordinaties]ocr_structures#coordinate-schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect text text_detector = ImageTextDetector .pretrained(&quot;text_detection_v1&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text_regions&quot;) .setSizeThreshold(10) .setScoreThreshold(0.9) .setLinkThreshold(0.4) .setTextThreshold(0.2) .setWidth(1512) .setHeight(2016) draw_regions = ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;text_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, text_detector, draw_regions ]) data = pipeline.transform(df) display_images(data, &quot;image_with_regions&quot;) import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect text val text_detector = ImageTextDetector .pretrained(&quot;text_detection_v1&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text_regions&quot;) val draw_regions = new ImageTextDetector() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;text_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) .setSizeThreshold(10) .setScoreThreshold(0.9) .setLinkThreshold(0.4) .setTextThreshold(0.2) .setWidth(1512) .setHeight(2016) pipeline = PipelineModel(stages=[ binary_to_image, text_detector, draw_regions ]) val data = pipeline.transform(df) data.storeImage(&quot;image_with_regions&quot;) Output: ImageTextDetectorV2 ImageTextDetectorV2 is a DL model for detecting text on images. It is based on the CRAFT network architecture with refiner net. Refiner net runs as postprocessing, and is able to merge single words regions into lines. Currently, it’s available only on Python side. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description scoreThreshold float 0.7 Score threshold for output regions. sizeThreshold int 10 Threshold for height of the detected regions. Regions with a height below the threshold won’t be returned. textThreshold float 0.4f Threshold for the score of a region potentially containing text. The region score represents the probability that a given pixel is the center of the character. Higher values for this threshold will result in that only regions for which the confidence of containing text is high will be returned. linkThreshold float 0.4f Threshold for the the link(affinity) score. The link score represents the space allowed between adjacent characters to be considered as a single word. width integer 1280 Width of the desired input image. Image will be resized to this width. withRefiner boolean false Enable to run refiner net as postprocessing step. Output Columns Param name Type Default Column Data Description outputCol string table_regions array of [Coordinaties]ocr_structures#coordinate-schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect text text_detector = ImageTextDetectorV2 .pretrained(&quot;image_text_detector_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text_regions&quot;) .setScoreThreshold(0.5) .setTextThreshold(0.2) .setSizeThreshold(10) .setWithRefiner(True) draw_regions = ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;text_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, text_detector, draw_regions ]) data = pipeline.transform(df) display_images(data, &quot;image_with_regions&quot;) not implemented",
    "url": "/docs/en/ocr_object_detection",
    "relUrl": "/docs/en/ocr_object_detection"
  },
  "109": {
    "id": "109",
    "title": "Spark OCR 2.3.x (Licensed)",
    "content": "Spark NLP comes with an OCR module that can read both PDF files and scanned images (requires Tesseract 4.x+). Installation Installing Tesseract As mentioned above, if you are dealing with scanned images instead of test-selectable PDF files you need to install tesseract 4.x+ on all the nodes in your cluster. Here how you can install it on Ubuntu/Debian: apt-get install tesseract-ocr In Databricks this command may result in installing tesseract 3.x instead of version 4.x. You can simply run this init script to install tesseract 4.x in your Databricks cluster: #!/bin/bash sudo apt-get install -y g++ # or clang++ (presumably) sudo apt-get install -y autoconf automake libtool sudo apt-get install -y pkg-config sudo apt-get install -y libpng-dev sudo apt-get install -y libjpeg8-dev sudo apt-get install -y libtiff5-dev sudo apt-get install -y zlib1g-dev ​ wget http://www.leptonica.org/source/leptonica-1.74.4.tar.gz tar xvf leptonica-1.74.4.tar.gz cd leptonica-1.74.4 ./configure make sudo make install ​ git clone --single-branch --branch 4.1 https://github.com/tesseract-ocr/tesseract.git cd tesseract ./autogen.sh ./configure make sudo make install sudo ldconfig ​ tesseract -v Quick start Let’s read a PDF file: import com.johnsnowlabs.nlp._ val ocrHelper = new OcrHelper() //If you do this locally you can use file:/// or hdfs:/// if the files are hosted in Hadoop val dataset = ocrHelper.createDataset(spark, &quot;/tmp/sample_article.pdf&quot;) If you are trying to extract text from scanned images in the format of PDF, please keep in mind to use these configs: ocrHelper.setPreferredMethod(&quot;image&quot;) ocrHelper.setFallbackMethod(false) ocrHelper.setMinSizeBeforeFallback(0) Configuration setPreferredMethod(text/image = text) either text or image will work. Defaults to text. Text mode works better and faster for digital or text scanned PDFs setFallbackMethod(boolean) on true, when text or image fail, it will fallback to the alternate method setMinSizeBeforeFallback(int = 1) number of characters to have at a minimum, before falling back. setPageSegMode(int = 3) image mode page segmentation mode setEngineMode(int = 1) image mode engine mode setPageIteratorLevel(int = 0) image mode page iteratior level setScalingFactor(float) Specifies the scaling factor to apply to images, in both axes, before OCR. It can scale up the image(factor &gt; 1.0) or scale it down(factor &lt; 1.0) setSplitPages(boolean = true) Whether to split pages into different rows and documents setSplitRegions(boolean = true) Whether to split by document regions. Works only in image mode. Enables split pages as well. setIncludeConfidence(boolean = false) setAutomaticSkewCorrection(use: boolean, half_angle: double = 5.0, resolution: double = 1.0) setAutomaticSizeCorrection(use: boolean, desired_size: int = 34) setEstimateNoise(string) image mode estimator noise level useErosion(use: boolean, kernel_size: int = 2, kernel_shape: Int = 0) image mode erosion Utilizing Spark NLP OCR Module Spark NLP OCR Module is not included within Spark NLP. It is not an annotator and not an extension to Spark ML. You can use OcrHelper to directly create spark dataframes from PDF. This will hold entire documents in single rows, meant to be later processed by a SentenceDetector. This way, you won’t be breaking the content in rows as if you were reading a standard document. Metadata columns are added automatically and will include page numbers, file name and other useful information per row. Python code from pyspark.sql import SparkSession from sparknlp.ocr import OcrHelper from sparknlp import DocumentAssembler data = OcrHelper().createDataset(spark = spark, input_path = &quot;/your/example.pdf&quot; ) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;) annotations = documentAssembler.transform(data) annotations.columns [&#39;text&#39;, &#39;pagenum&#39;, &#39;method&#39;, &#39;noiselevel&#39;, &#39;confidence&#39;, &#39;positions&#39;, &#39;filename&#39;, &#39;document&#39;] Scala code import com.johnsnowlabs.nlp.util.io.OcrHelper import com.johnsnowlabs.nlp.DocumentAssembler val myOcrHelper = new OcrHelper val data = myOcrHelper.createDataset(spark, &quot;/your/example.pdf&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;) val annotations = documentAssembler.transform(data) annotations.columns Array[String] = Array(text, pagenum, method, noiselevel, confidence, positions, filename, document) … where the text column of the annotations spark dataframe includes the text content of the PDF, pagenum the page number, etc… Creating an Array of Strings from PDF (For LightPipeline) Another way, would be to simply create an array of strings. This is useful for example if you are parsing a small amount of pdf files and would like to use LightPipelines instead. See an example below. Scala code import com.johnsnowlabs.nlp.util.io.OcrHelper import com.johnsnowlabs.nlp.{DocumentAssembler,LightPipeline} import com.johnsnowlabs.nlp.annotator.SentenceDetector import org.apache.spark.ml.Pipeline val myOcrHelper = new OcrHelper val raw = myOcrHelper.createMap(&quot;/pdfs/&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val lightPipeline = new LightPipeline(new Pipeline().setStages(Array(documentAssembler, sentenceDetector)).fit(Seq.empty[String].toDF(&quot;text&quot;))) val annotations = ligthPipeline.annotate(raw.values.toArray) Now to get the whole first PDF content in your /pdfs/ folder you can use: annotations(0)(&quot;document&quot;)(0) and to get the third sentence found in that first pdf: annotations(0)(&quot;sentence&quot;)(2) To get from the fifth pdf the second sentence: annotations(4)(&quot;sentence&quot;)(1) Similarly, the whole content of the fifth pdf can be retrieved by: annotations(4)(&quot;document&quot;)(0)",
    "url": "/docs/en/ocr_old",
    "relUrl": "/docs/en/ocr_old"
  },
  "110": {
    "id": "110",
    "title": "Pipeline components",
    "content": "PDF processing Next section describes the transformers that deal with PDF files with the purpose of extracting text and image data from PDF files. PdfToText PDFToText extracts text from selectable PDF (with text layout). Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PDF document originCol string path path to the original file Parameters Param name Type Default Description splitPage bool true Whether it needed to split document to pages textStripper   TextStripperType.PDF_TEXT_STRIPPER Extract unstructured text sort bool false Sort text during extraction with TextStripperType.PDF_LAYOUT_STRIPPER partitionNum int 0 Force repartition dataframe if set to value more than 0. onlyPageNum bool false Extract only page numbers. extractCoordinates bool false Extract coordinates and store to the positions column storeSplittedPdf bool false Store one page pdf’s for process it using PdfToImage. Output Columns Param name Type Default Column Data Description outputCol string text extracted text pageNumCol string pagenum page number or 0 when splitPage = false NOTE: For setting parameters use setParamName method. Example PythonScala from sparkocr.transformers import * pdfPath = &quot;path to pdf with text layout&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) transformer = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;pagenum&quot;) .setSplitPage(True) data = transformer.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() import com.johnsnowlabs.ocr.transformers.PdfToText val pdfPath = &quot;path to pdf with text layout&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val transformer = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;pagenum&quot;) .setSplitPage(true) val data = transformer.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() Output: +-+-+ |pagenum|text | +-+-+ |0 |This is a page. | |1 |This is another page. | |2 |Yet another page. | +-+-+ PdfToImage PdfToImage renders PDF to an image. To be used with scanned PDF documents. Output dataframe contains total_pages field with total number of pages. For process pdf with a big number of pages prefer to split pdf by setting splitNumBatch param. Number of partitions should be equal to number of cores/executors. Input Columns Param name Type Default Column Data Description inputCol string content binary representation of the PDF document originCol string path path to the original file fallBackCol string text extracted text from previous method for detect if need to run transformer as fallBack Parameters Param name Type Default Description splitPage bool true whether it needed to split document to pages minSizeBeforeFallback int 10 minimal count of characters to extract to decide, that the document is the PDF with text layout imageType ImageType ImageType.TYPE_BYTE_GRAY type of the image resolution int 300 Output image resolution in dpi keepInput boolean false Keep input column in dataframe. By default it is dropping. partitionNum int 0 Number of Spark RDD partitions (0 value - without repartition) binarization boolean false Enable/Disable binarization image after extract image. binarizationParams Array[String] null Array of Binarization params in key=value format. splitNumBatch int 0 Number of partitions or size of partitions, related to the splitting strategy. partitionNumAfterSplit int 0 Number of Spark RDD partitions after splitting pdf document (0 value - without repartition). splittingStategy SplittingStrategy SplittingStrategy.FIXED_SIZE_OF_PARTITION Splitting strategy. Output Columns Param name Type Default Column Data Description outputCol string image extracted image struct (Image schema) pageNumCol string pagenum page number or 0 when splitPage = false Example: PythonScala from sparkocr.transformers import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdfToImage = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;pagenum&quot;) .setSplitPage(True) data = pdfToImage.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() import com.johnsnowlabs.ocr.transformers.PdfToImage val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToImage = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;pagenum&quot;) .setSplitPage(true) val data = pdfToImage.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() ImageToPdf ImageToPdf transform image to Pdf document. If dataframe contains few records for same origin path, it groups image by origin column and create multipage PDF document. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string content binary representation of the PDF document Example: Read images and store them as single page PDF documents. PythonScala from sparkocr.transformers import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) # Define transformer for convert to Image struct binaryToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for store to PDF imageToPdf = ImageToPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;content&quot;) # Call transformers image_df = binaryToImage.transform(df) pdf_df = pdfToImage.transform(image_df) pdf_df.select(&quot;content&quot;).show() import com.johnsnowlabs.ocr.transformers._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(imagePath) // Define transformer for convert to Image struct val binaryToImage = new BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) // Define transformer for store to PDF val imageToPdf = new ImageToPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;content&quot;) // Call transformers val image_df = binaryToImage.transform(df) val pdf_df = pdfToImage.transform(image_df) pdf_df.select(&quot;content&quot;).show() TextToPdf TextToPdf renders ocr results to PDF document as text layout. Each symbol will render to the same position with the same font size as in original image or PDF. If dataframe contains few records for same origin path, it groups image by origin column and create multipage PDF document. Input Columns Param name Type Default Column Data Description inputCol string positions column with positions struct inputImage string image image struct (Image schema) inputText string text column name with recognized text originCol string path path to the original file inputContent string content column name with binary representation of original PDF file Output Columns Param name Type Default Column Data Description outputCol string pdf binary representation of the PDF document Example: Read PDF document, run OCR and render results to PDF document. PythonScala from sparkocr.transformers import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_image = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image_raw&quot;) binarizer = ImageBinarizer() .setInputCol(&quot;image_raw&quot;) .setOutputCol(&quot;image&quot;) .setThreshold(130) ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setIgnoreResolution(False) .setPageSegMode(PageSegmentationMode.SPARSE_TEXT) .setConfidenceThreshold(60) textToPdf = TextToPdf() .setInputCol(&quot;positions&quot;) .setInputImage(&quot;image&quot;) .setOutputCol(&quot;pdf&quot;) pipeline = PipelineModel(stages=[ pdf_to_image, binarizer, ocr, textToPdf ]) result = pipeline.transform(df).collect() # Store to file for debug with open(&quot;test.pdf&quot;, &quot;wb&quot;) as file: file.write(result[0].pdf) import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers._ val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToImage = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image_raw&quot;) .setResolution(400) val binarizer = new ImageBinarizer() .setInputCol(&quot;image_raw&quot;) .setOutputCol(&quot;image&quot;) .setThreshold(130) val ocr = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setIgnoreResolution(false) .setPageSegMode(PageSegmentationMode.SPARSE_TEXT) .setConfidenceThreshold(60) val textToPdf = new TextToPdf() .setInputCol(&quot;positions&quot;) .setInputImage(&quot;image&quot;) .setOutputCol(&quot;pdf&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( pdfToImage, binarizer, ocr, textToPdf )) val modelPipeline = pipeline.fit(df) val pdf = modelPipeline.transform(df) val pdfContent = pdf.select(&quot;pdf&quot;).collect().head.getAs[Array[Byte]](0) // store to file val tmpFile = Files.createTempFile(suffix=&quot;.pdf&quot;).toAbsolutePath.toString val fos = new FileOutputStream(tmpFile) fos.write(pdfContent) fos.close() println(tmpFile) PdfAssembler PdfAssembler group single page PDF documents by the filename and assemble muliplepage PDF document. Input Columns Param name Type Default Column Data Description inputCol string page_pdf binary representation of the PDF document originCol string path path to the original file pageNumCol string pagenum for compatibility with another transformers Output Columns Param name Type Default Column Data Description outputCol string pdf binary representation of the PDF document Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_image = PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setKeepInput(True) # Run OCR and render results to PDF ocr = ImageToTextPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;pdf_page&quot;) # Assemble multipage PDF pdf_assembler = PdfAssembler() .setInputCol(&quot;pdf_page&quot;) .setOutputCol(&quot;pdf&quot;) pipeline = PipelineModel(stages=[ pdf_to_image, ocr, pdf_assembler ]) pdf = pipeline.transform(df) pdfContent = pdf.select(&quot;pdf&quot;).collect().head.getAs[Array[Byte]](0) # store pdf to file with open(&quot;test.pdf&quot;, &quot;wb&quot;) as file: file.write(pdfContent[0].pdf) import java.io.FileOutputStream import java.nio.file.Files import com.johnsnowlabs.ocr.transformers._ val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdf_to_image = new PdfToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setKeepInput(True) // Run OCR and render results to PDF val ocr = new ImageToTextPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;pdf_page&quot;) // Assemble multipage PDF val pdf_assembler = new PdfAssembler() .setInputCol(&quot;pdf_page&quot;) .setOutputCol(&quot;pdf&quot;) // Create pipeline val pipeline = new Pipeline() .setStages(Array( pdf_to_image, ocr, pdf_assembler )) val pdf = pipeline.fit(df).transform(df) val pdfContent = pdf.select(&quot;pdf&quot;).collect().head.getAs[Array[Byte]](0) // store to pdf file val tmpFile = Files.createTempFile(&quot;with_regions_&quot;, s&quot;.pdf&quot;).toAbsolutePath.toString val fos = new FileOutputStream(tmpFile) fos.write(pdfContent) fos.close() println(tmpFile) PdfDrawRegions PdfDrawRegions transformer for drawing regions to Pdf document. Input Columns Param name Type Default Column Data Description inputCol string content binary representation of the PDF document originCol string path path to the original file inputRegionsCol string region input column which contain regions Parameters Param name Type Default Description lineWidth integer 1 line width for draw regions Output Columns Param name Type Default Column Data Description outputCol string pdf_regions binary representation of the PDF document Example: PythonScala from pyspark.ml import Pipeline from sparkocr.transformers import * from sparknlp.annotator import * from sparknlp.base import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;page&quot;) .setSplitPage(False) document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) entity_extractor = TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;./sparkocr/resources/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) position_finder = PositionFinder() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;coordinates&quot;) .setPageMatrixCol(&quot;positions&quot;) .setMatchingWindow(10) .setPadding(2) draw = PdfDrawRegions() .setInputRegionsCol(&quot;coordinates&quot;) .setOutputCol(&quot;pdf_with_regions&quot;) .setInputCol(&quot;content&quot;) .setLineWidth(1) pipeline = Pipeline(stages=[ pdf_to_text, document_assembler, sentence_detector, tokenizer, entity_extractor, position_finder, draw ]) pdfWithRegions = pipeline.fit(df).transform(df) pdfContent = pdfWithRegions.select(&quot;pdf_regions&quot;).collect().head.getAs[Array[Byte]](0) # store to pdf to tmp file with open(&quot;test.pdf&quot;, &quot;wb&quot;) as file: file.write(pdfContent[0].pdf_regions) import java.io.FileOutputStream import java.nio.file.Files import com.johnsnowlabs.ocr.transformers._ import com.johnsnowlabs.nlp.{DocumentAssembler, SparkAccessor} import com.johnsnowlabs.nlp.annotators._ import com.johnsnowlabs.nlp.util.io.ReadAs val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) val positionFinder = new PositionFinder() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;coordinates&quot;) .setPageMatrixCol(&quot;positions&quot;) .setMatchingWindow(10) .setPadding(2) val pdfDrawRegions = new PdfDrawRegions() .setInputRegionsCol(&quot;coordinates&quot;) // Create pipeline val pipeline = new Pipeline() .setStages(Array( pdfToText, documentAssembler, sentenceDetector, tokenizer, entityExtractor, positionFinder, pdfDrawRegions )) val pdfWithRegions = pipeline.fit(df).transform(df) val pdfContent = pdfWithRegions.select(&quot;pdf_regions&quot;).collect().head.getAs[Array[Byte]](0) // store to pdf to tmp file val tmpFile = Files.createTempFile(&quot;with_regions_&quot;, s&quot;.pdf&quot;).toAbsolutePath.toString val fos = new FileOutputStream(tmpFile) fos.write(pdfContent) fos.close() println(tmpFile) Results: PdfToTextTable Extract tables from Pdf document page. Input is a column with binary representation of PDF document. As output generate column with tables and tables text chunks coordinates (rows/cols). Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PDF document originCol string path path to the original file Parameters Param name Type Default Description pageIndex integer -1 Page index to extract Tables. guess bool false A logical indicating whether to guess the locations of tables on each page. method string decide Identifying the prefered method of table extraction: basic, spreadsheet. Output Columns Param name Type Default Column Data Description outputCol TableContainer tables Extracted tables Example: PythonScala from pyspark.ml import Pipeline from sparkocr.transformers import * from sparknlp.annotator import * from sparknlp.base import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text_table = PdfToTextTable() pdf_to_text_table.setInputCol(&quot;content&quot;) pdf_to_text_table.setOutputCol(&quot;table&quot;) pdf_to_text_table.setPageIndex(1) pdf_to_text_table.setMethod(&quot;basic&quot;) table = pdf_to_text_table.transform(df) # Show first row table.select(table[&quot;table.chunks&quot;].getItem(1)[&quot;chunkText&quot;]).show(1, False) import java.io.FileOutputStream import java.nio.file.Files import com.johnsnowlabs.ocr.transformers._ import com.johnsnowlabs.nlp.{DocumentAssembler, SparkAccessor} import com.johnsnowlabs.nlp.annotators._ import com.johnsnowlabs.nlp.util.io.ReadAs val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToTextTable = new PdfToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;table&quot;) .pdf_to_text_table.setPageIndex(1) .pdf_to_text_table.setMethod(&quot;basic&quot;) table = pdfToTextTable.transform(df) // Show first row table.select(table[&quot;table.chunks&quot;].getItem(1)[&quot;chunkText&quot;]).show(1, False) Output: ++ |table.chunks AS chunks#760[1].chunkText | ++ |[Mazda RX4, 21.0, 6, , 160.0, 110, 3.90, 2.620, 16.46, 0, 1, 4, 4]| ++ DOCX processing Next section describes the transformers that deal with DOCX files with the purpose of extracting text and table data from it. DocToText DocToText extracts text from the DOCX document. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the DOCX document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string text extracted text pageNumCol string pagenum for compatibility with another transformers NOTE: For setting parameters use setParamName method. Example PythonScala from sparkocr.transformers import * docPath = &quot;path to docx with text layout&quot; # Read DOCX file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = DocToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) data = transformer.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() import com.johnsnowlabs.ocr.transformers.DocToText val docPath = &quot;path to docx with text layout&quot; // Read DOCX file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new DocToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) val data = transformer.transform(df) data.select(&quot;pagenum&quot;, &quot;text&quot;).show() DocToTextTable DocToTextTable extracts table data from the DOCX documents. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PDF document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol TableContainer tables Extracted tables NOTE: For setting parameters use setParamName method. Example PythonScala from sparkocr.transformers import * docPath = &quot;path to docx with text layout&quot; # Read DOCX file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = DocToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;tables&quot;) data = transformer.transform(df) data.select(&quot;tables&quot;).show() import com.johnsnowlabs.ocr.transformers.DocToTextTable val docPath = &quot;path to docx with text layout&quot; // Read DOCX file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new DocToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;tables&quot;) val data = transformer.transform(df) data.select(&quot;tables&quot;).show() DocToPdf DocToPdf convert DOCX document to PDF document. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the DOCX document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string text binary representation of the PDF document NOTE: For setting parameters use setParamName method. Example PythonScala from sparkocr.transformers import * docPath = &quot;path to docx with text layout&quot; # Read DOCX file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = DocToPdf() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;pdf&quot;) data = transformer.transform(df) data.select(&quot;pdf&quot;).show() import com.johnsnowlabs.ocr.transformers.DocToPdf val docPath = &quot;path to docx with text layout&quot; // Read DOCX file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new DocToPdf() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;pdf&quot;) val data = transformer.transform(df) data.select(&quot;pdf&quot;).show() PptToTextTable PptToTextTable extracts table data from the PPT and PPTX documents. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PPT document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol TableContainer tables Extracted tables NOTE: For setting parameters use setParamName method. Example PythonScala from sparkocr.transformers import * docPath = &quot;path to docx with text layout&quot; # Read PPT file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = PptToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;tables&quot;) data = transformer.transform(df) data.select(&quot;tables&quot;).show() import com.johnsnowlabs.ocr.transformers.PptToTextTable val docPath = &quot;path to docx with text layout&quot; // Read PPT file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new PptToTextTable() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;tables&quot;) val data = transformer.transform(df) data.select(&quot;tables&quot;).show() PptToPdf PptToPdf convert PPT and PPTX documents to PDF document. Input Columns Param name Type Default Column Data Description inputCol string text binary representation of the PPT document originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string text binary representation of the PDF document NOTE: For setting parameters use setParamName method. Example PythonScala from sparkocr.transformers import * docPath = &quot;path to PPT with text layout&quot; # Read DOCX file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(docPath) transformer = PptToPdf() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;pdf&quot;) data = transformer.transform(df) data.select(&quot;pdf&quot;).show() import com.johnsnowlabs.ocr.transformers.PptToPdf val docPath = &quot;path to docx with text layout&quot; // Read PPT file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(docPath) val transformer = new PptToPdf() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;pdf&quot;) val data = transformer.transform(df) data.select(&quot;pdf&quot;).show() Dicom processing DicomToImage DicomToImage transforms dicom object (loaded as binary file) to image struct. Input Columns Param name Type Default Column Data Description inputCol string content binary dicom object originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string image extracted image struct (Image schema) pageNumCol integer pagenum page (image) number begin from 0 metadataCol string metadata Output column name for dicom metatdata ( json formatted ) Scala example: PythonScala from sparkocr.transformers import * dicomPath = &quot;path to dicom files&quot; # Read dicom file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(dicomPath) dicomToImage = DicomToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setMetadataCol(&quot;meta&quot;) data = dicomToImage.transform(df) data.select(&quot;image&quot;, &quot;pagenum&quot;, &quot;meta&quot;).show() import com.johnsnowlabs.ocr.transformers.DicomToImage val dicomPath = &quot;path to dicom files&quot; // Read dicom file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(dicomPath) val dicomToImage = new DicomToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setMetadataCol(&quot;meta&quot;) val data = dicomToImage.transform(df) data.select(&quot;image&quot;, &quot;pagenum&quot;, &quot;meta&quot;).show() ImageToDicom ImageToDicom transforms image to Dicom document. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) originCol string path path to the original file metadataCol string metadata dicom metatdata ( json formatted ) Output Columns Param name Type Default Column Data Description outputCol string dicom binary dicom object Scala example: PythonScala from sparkocr.transformers import * imagePath = &quot;path to image file&quot; # Read image file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(imagePath) binaryToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) image_df = binaryToImage.transform(df) imageToDicom = ImageToDicom() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;dicom&quot;) data = imageToDicom.transform(image_df) data.select(&quot;dicom&quot;).show() import com.johnsnowlabs.ocr.transformers.ImageToDicom val imagePath = &quot;path to image file&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToDicom = new ImageToDicom() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;dicom&quot;) val data = imageToDicom.transform(df) data.select(&quot;dicom&quot;).show() Image pre-processing Next section describes the transformers for image pre-processing: scaling, binarization, skew correction, etc. BinaryToImage BinaryToImage transforms image (loaded as binary file) to image struct. Input Columns Param name Type Default Column Data Description inputCol string content binary representation of the image originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string image extracted image struct (Image schema) Scala example: PythonScala from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(imagePath) binaryToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) data = binaryToImage.transform(df) data.select(&quot;image&quot;).show() import com.johnsnowlabs.ocr.transformers.BinaryToImage val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(imagePath) val binaryToImage = new BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) val data = binaryToImage.transform(df) data.select(&quot;image&quot;).show() GPUImageTransformer GPUImageTransformer allows to run image pre-processing operations on GPU. It supports the following operations: Scaling Otsu thresholding Huang thresholding Erosion Dilation GPUImageTransformer allows to add few operations. To add operations you need to call one of the methods with params: Method name Params Description addScalingTransform factor Scale image by scaling factor. addOtsuTransform   The automatic thresholder utilizes the Otsu threshold method. addHuangTransform   The automatic thresholder utilizes the Huang threshold method. addDilateTransform width, height Computes the local maximum of a pixels rectangular neighborhood. The rectangles size is specified by its half-width and half-height. addErodeTransform width, height Computes the local minimum of a pixels rectangular neighborhood. The rectangles size is specified by its half-width and half-height Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description imageType ImageType ImageType.TYPE_BYTE_BINARY Type of the output image gpuName string ”” GPU device name. Output Columns Param name Type Default Column Data Description outputCol string transformed_image image struct (Image schema) Example: PythonScala from sparkocr.transformers import * from sparkocr.enums import ImageType from sparkocr.utils import display_images imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) transformer = GPUImageTransformer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;transformed_image&quot;) .addHuangTransform() .addScalingTransform(3) .addDilateTransform(2, 2) .setImageType(ImageType.TYPE_BYTE_BINARY) pipeline = PipelineModel(stages=[ binary_to_image, transformer ]) result = pipeline.transform(df) display_images(result, &quot;transformed_image&quot;) import com.johnsnowlabs.ocr.transformers.GPUImageTransformer import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new GPUImageTransformer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;transformed_image&quot;) .addHuangTransform() .addScalingTransform(3) .addDilateTransform(2, 2) .setImageType(ImageType.TYPE_BYTE_BINARY) val data = transformer.transform(df) data.storeImage(&quot;transformed_image&quot;) ImageBinarizer ImageBinarizer transforms image to binary color schema, based on threshold. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description threshold int 170   Output Columns Param name Type Default Column Data Description outputCol string binarized_image image struct (Image schema) Example: PythonScala from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) binirizer = ImageBinarizer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;binary_image&quot;) .setThreshold(100) data = binirizer.transform(df) data.show() import com.johnsnowlabs.ocr.transformers.ImageBinarizer import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val binirizer = new ImageBinarizer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;binary_image&quot;) .setThreshold(100) val data = binirizer.transform(df) data.storeImage(&quot;binary_image&quot;) Original image: Binarized image with 100 threshold: ImageAdaptiveBinarizer Supported Methods: OTSU. Returns a single intensity threshold that separate pixels into two classes, foreground and background. Gaussian local thresholding. Thresholds the image using a locally adaptive threshold that is computed using a local square region centered on each pixel. The threshold is equal to the gaussian weighted sum of the surrounding pixels times the scale. Sauvola. Is a Local thresholding technique that are useful for images where the background is not uniform. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description width float 90 Width of square region. method TresholdingMethod TresholdingMethod.GAUSSIAN Method used to determine adaptive threshold. scale float 1.1f Scale factor used to adjust threshold. imageType ImageType ImageType.TYPE_BYTE_BINARY Type of the output image Output Columns Param name Type Default Column Data Description outputCol string binarized_image image struct (Image schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * from sparkocr.utils import display_image imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) adaptive_thresholding = ImageAdaptiveBinarizer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;binarized_image&quot;) .setWidth(100) .setScale(1.1) pipeline = PipelineModel(stages=[ binary_to_image, adaptive_thresholding ]) result = pipeline.transform(df) for r in result.select(&quot;image&quot;, &quot;corrected_image&quot;).collect(): display_image(r.image) display_image(r.corrected_image) import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val binirizer = new ImageAdaptiveBinarizer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;binary_image&quot;) .setWidth(100) .setScale(1.1) val data = binirizer.transform(df) data.storeImage(&quot;binary_image&quot;) ImageAdaptiveThresholding Compute a threshold mask image based on local pixel neighborhood and apply it to image. Also known as adaptive or dynamic thresholding. The threshold value is the weighted mean for the local neighborhood of a pixel subtracted by a constant. Supported methods: GAUSSIAN MEAN MEDIAN WOLF SINGH Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description blockSize int 170 Odd size of pixel neighborhood which is used to calculate the threshold value (e.g. 3, 5, 7, …, 21, …). method AdaptiveThresholdingMethod AdaptiveThresholdingMethod.GAUSSIAN Method used to determine adaptive threshold for local neighbourhood in weighted mean image. offset int   Constant subtracted from weighted mean of neighborhood to calculate the local threshold value. Default offset is 0. mode string   The mode parameter determines how the array borders are handled, where cval is the value when mode is equal to ‘constant’ cval int   Value to fill past edges of input if mode is ‘constant’. Output Columns Param name Type Default Column Data Description outputCol string binarized_image image struct (Image schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * from sparkocr.utils import display_image imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) adaptive_thresholding = ImageAdaptiveThresholding() .setInputCol(&quot;scaled_image&quot;) .setOutputCol(&quot;binarized_image&quot;) .setBlockSize(21) .setOffset(73) pipeline = PipelineModel(stages=[ binary_to_image, adaptive_thresholding ]) result = pipeline.transform(df) for r in result.select(&quot;image&quot;, &quot;corrected_image&quot;).collect(): display_image(r.image) display_image(r.corrected_image) // Implemented only for Python Original image: Binarized image: ImageScaler ImageScaler scales image by provided scale factor or needed output size. It supports keeping original ratio of image by padding the image in case fixed output size. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description scaleFactor double 1.0 scale factor keepRatio boolean false Keep original ratio of image width int 0 Output width of image height int 0 Outpu height of imgae Output Columns Param name Type Default Column Data Description outputCol string scaled_image scaled image struct (Image schema) Example: PythonScala from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) transformer = ImageScaler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;scaled_image&quot;) .setScaleFactor(0.5) data = transformer.transform(df) data.show() import com.johnsnowlabs.ocr.transformers.ImageScaler import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageScaler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;scaled_image&quot;) .setScaleFactor(0.5) val data = transformer.transform(df) data.storeImage(&quot;scaled_image&quot;) ImageAdaptiveScaler ImageAdaptiveScaler detects font size and scales image for have desired font size. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description desiredSize int 34 desired size of font in pixels Output Columns Param name Type Default Column Data Description outputCol string scaled_image scaled image struct (Image schema) Example: PythonScala from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) transformer = ImageAdaptiveScaler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;scaled_image&quot;) .setDesiredSize(34) data = transformer.transform(df) data.show() import com.johnsnowlabs.ocr.transformers.ImageAdaptiveScaler import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageAdaptiveScaler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;scaled_image&quot;) .setDesiredSize(34) val data = transformer.transform(df) data.storeImage(&quot;scaled_image&quot;) ImageSkewCorrector ImageSkewCorrector detects skew of the image and rotates it. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description rotationAngle double 0.0 rotation angle automaticSkewCorrection boolean true enables/disables adaptive skew correction halfAngle double 5.0 half the angle(in degrees) that will be considered for correction resolution double 1.0 The step size(in degrees) that will be used for generating correction angle candidates Output Columns Param name Type Default Column Data Description outputCol string corrected_image corrected image struct (Image schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * from sparkocr.utils import display_images imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) skew_corrector = ImageSkewCorrector() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;corrected_image&quot;) .setAutomaticSkewCorrection(True) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, skew_corrector ]) data = pipeline.transform(df) display_images(data, &quot;corrected_image&quot;) import com.johnsnowlabs.ocr.transformers.ImageSkewCorrector import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageSkewCorrector() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;corrected_image&quot;) .setAutomaticSkewCorrection(true) val data = transformer.transform(df) data.storeImage(&quot;corrected_image&quot;) Original image: Corrected image: ImageNoiseScorer ImageNoiseScorer computes noise score for each region. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) inputRegionsCol string regions regions Parameters Param name Type Default Description method NoiseMethod string NoiseMethod.RATIO method of computation noise score Output Columns Param name Type Default Column Data Description outputCol string noisescores noise score for each region Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * from sparkocr.enums import NoiseMethod imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) # Define transformer for detect regions layoutAnalyzer = ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) # Define transformer for compute noise level for each region noisescorer = ImageNoiseScorer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;noiselevel&quot;) .setInputRegionsCol(&quot;regions&quot;) .setMethod(NoiseMethod.VARIANCE) # Define pipeline pipeline = Pipeline() pipeline.setStages(Array( layoutAnalyzer, noisescorer )) data = pipeline.transform(df) data.select(&quot;path&quot;, &quot;noiselevel&quot;).show() import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.{ImageNoiseScorer, ImageLayoutAnalyzer} import com.johnsnowlabs.ocr.NoiseMethod import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect regions val layoutAnalyzer = new ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) // Define transformer for compute noise level for each region val noisescorer = new ImageNoiseScorer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;noiselevel&quot;) .setInputRegionsCol(&quot;regions&quot;) .setMethod(NoiseMethod.VARIANCE) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( layoutAnalyzer, noisescorer )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = modelPipeline.transform(df) data.select(&quot;path&quot;, &quot;noiselevel&quot;).show() Output: ++--+ |path |noiselevel | ++--+ |file:./noisy.png |[32.01805641767766, 32.312916551193354, 29.99257352247787, 30.62470388308217]| ++--+ ImageRemoveObjects python only ImageRemoveObjects to remove background objects. It supports removing: objects less than elements of font with minSizeFont size objects less than minSizeObject holes less than minSizeHole objects more than maxSizeObject Input Columns Param name Type Default Column Data Description inputCol string None image struct (Image schema) Parameters Param name Type Default Description minSizeFont int 10 Min size font in pt. minSizeObject int None Min size of object which will keep on image [*]. connectivityObject int 0 The connectivity defining the neighborhood of a pixel. minSizeHole int None Min size of hole which will keep on image[ *]. connectivityHole int 0 The connectivity defining the neighborhood of a pixel. maxSizeObject int None Max size of object which will keep on image [*]. connectivityMaxObject int 0 The connectivity defining the neighborhood of a pixel. [*] : None value disables removing objects. Output Columns Param name Type Default Column Data Description outputCol string None scaled image struct (Image schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) remove_objects = ImageRemoveObjects() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;corrected_image&quot;) .setMinSizeObject(20) pipeline = PipelineModel(stages=[ binary_to_image, remove_objects ]) data = pipeline.transform(df) // Implemented only for Python ImageMorphologyOperation python only ImageMorphologyOperationis a transformer for applying morphological operations to image. It supports following operation: Erosion Dilation Opening Closing Input Columns Param name Type Default Column Data Description inputCol string None image struct (Image schema) Parameters Param name Type Default Description operation MorphologyOperationType MorphologyOperationType.OPENING Operation type kernelShape KernelShape KernelShape.DISK Kernel shape. kernelSize int 1 Kernel size in pixels. Output Columns Param name Type Default Column Data Description outputCol string None scaled image struct (Image schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setOperation(MorphologyOperationType.OPENING) adaptive_thresholding = ImageAdaptiveThresholding() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;corrected_image&quot;) .setBlockSize(75) .setOffset(0) opening = ImageMorphologyOperation() .setInputCol(&quot;corrected_image&quot;) .setOutputCol(&quot;opening_image&quot;) .setkernelSize(1) pipeline = PipelineModel(stages=[ binary_to_image, adaptive_thresholding, opening ]) result = pipeline.transform(df) for r in result.select(&quot;image&quot;, &quot;corrected_image&quot;).collect(): display_image(r.image) display_image(r.corrected_image) // Implemented only for Python Original image: Opening image: ImageCropper ImageCropperis a transformer for cropping image. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description cropRectangle Rectangle Rectangle(0,0,0,0) Image rectangle. cropSquareType CropSquareType CropSquareType.TOP_LEFT Type of square. Output Columns Param name Type Default Column Data Description outputCol string cropped_image scaled image struct (Image schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) .setOperation(MorphologyOperationType.OPENING) cropper = ImageCropper() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cropped_image&quot;) .setCropRectangle((0, 0, 200, 110)) pipeline = PipelineModel(stages=[ binary_to_image, cropper ]) result = pipeline.transform(df) for r in result.select(&quot;image&quot;, &quot;cropped_image&quot;).collect(): display_image(r.image) display_image(r.cropped_image) import com.johnsnowlabs.ocr.transformers.ImageAdaptiveScaler import com.johnsnowlabs.ocr.OcrContext.implicits._ import java.awt.Rectangle val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val rectangle: Rectangle = new Rectangle(0, 0, 200, 110) val cropper: ImageCropper = new ImageCropper() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cropped_image&quot;) .setCropRectangle(rectangle) val data = transformer.transform(df) data.storeImage(&quot;cropped_image&quot;) Splitting image to regions ImageLayoutAnalyzer ImageLayoutAnalyzer analyzes the image and determines regions of text. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description pageSegMode PageSegmentationMode AUTO page segmentation mode pageIteratorLevel PageIteratorLevel BLOCK page iteration level ocrEngineMode EngineMode LSTM_ONLY OCR engine mode Output Columns Param name Type Default Column Data Description outputCol string region array of [Coordinaties]ocr_structures#coordinate-schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect regions layout_analyzer = ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, layout_analyzer ]) data = pipeline.transform(df) data.show() import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.{ImageSplitRegions, ImageLayoutAnalyzer} import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect regions val layoutAnalyzer = new ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) val data = layoutAnalyzer.transform(df) data.show() ImageSplitRegions ImageSplitRegions splits image into regions. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) inputRegionsCol string region array of [Coordinaties]ocr_structures#coordinate-schema) Parameters Param name Type Default Description explodeCols Array[string]   Columns which need to explode rotated boolean False Support rotated regions Output Columns Param name Type Default Column Data Description outputCol string region_image image struct (Image schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect regions layout_analyzer = ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) splitter = ImageSplitRegions() .setInputCol(&quot;image&quot;) .setRegionCol(&quot;regions&quot;) .setOutputCol(&quot;region_image&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, layout_analyzer, splitter ]) data = pipeline.transform(df) data.show() import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.{ImageSplitRegions, ImageLayoutAnalyzer} import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect regions val layoutAnalyzer = new ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) val splitter = new ImageSplitRegions() .setInputCol(&quot;image&quot;) .setRegionCol(&quot;regions&quot;) .setOutputCol(&quot;region_image&quot;) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( layoutAnalyzer, splitter )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = pipeline.transform(df) data.show() ImageDrawAnnotations ImageDrawAnnotations draw annotations with label and score to the image. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) inputChunksCol string region array of Annotation Parameters Param name Type Default Description lineWidth Int 4 Line width for draw rectangles fontSize Int 12 Font size for render labels and score rectColor Color Color.black Color of lines Output Columns Param name Type Default Column Data Description outputCol string image_with_chunks image struct (Image schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) tokenizer = HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) draw_annotations = ImageDrawAnnotations() .setInputCol(&quot;image&quot;) .setInputChunksCol(&quot;token&quot;) .setOutputCol(&quot;image_with_annotations&quot;) .setFilledRect(False) .setFontSize(40) .setRectColor(Color.red) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr, tokenizer, image_with_annotations ]) result = pipeline.transform(df) import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToHocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val tokenizer = new HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) val draw_annotations = new ImageDrawAnnotations() .setInputCol(&quot;image&quot;) .setInputChunksCol(&quot;token&quot;) .setOutputCol(&quot;image_with_annotations&quot;) .setFilledRect(False) .setFontSize(40) .setRectColor(Color.red) val pipeline = new Pipeline() pipeline.setStages(Array( imageToHocr, tokenizer, draw_annotations )) val modelPipeline = pipeline.fit(df) val result = modelPipeline.transform(df) ImageDrawRegions ImageDrawRegions draw regions with label and score to the image. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) inputRegionsCol string region array of [Coordinaties]ocr_structures#coordinate-schema) Parameters Param name Type Default Description lineWidth Int 4 Line width for draw rectangles fontSize Int 12 Font size for render labels and score rotated boolean False Support rotated regions Output Columns Param name Type Default Column Data Description outputCol string image_with_regions image struct (Image schema) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect regions layout_analyzer = ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) draw = ImageDrawRegions() .setInputCol(&quot;image&quot;) .setRegionCol(&quot;regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, layout_analyzer, draw ]) data = pipeline.transform(df) data.show() import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.{ImageSplitRegions, ImageLayoutAnalyzer} import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect regions val layoutAnalyzer = new ImageLayoutAnalyzer() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;regions&quot;) val draw = new ImageDrawRegions() .setInputCol(&quot;image&quot;) .setRegionCol(&quot;regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( layoutAnalyzer, draw )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val data = pipeline.transform(df) data.show() Characters recognition Next section describes the estimators for OCR ImageToText ImageToText runs OCR for input image, return recognized text to outputCol and positions with font size to ‘positionsCol’ column. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description pageSegMode PageSegmentationMode AUTO page segmentation mode pageIteratorLevel PageIteratorLevel BLOCK page iteration level ocrEngineMode EngineMode LSTM_ONLY OCR engine mode language Language Language.ENG language confidenceThreshold int 0 Confidence threshold. ignoreResolution bool false Ignore resolution from metadata of image. ocrParams array of strings [] Array of Ocr params in key=value format. pdfCoordinates bool false Transform coordinates in positions to PDF points. modelData string   Path to the local model data. modelType ModelType ModelType.BASE Model type downloadModelData bool false Download model data from JSL S3 withSpaces bool false Include spaces to output positions. keepLayout bool false Keep layout of text at result. outputSpaceCharacterWidth int 8 Output space character width in pts for layout keeper. Output Columns Param name Type Default Column Data Description outputCol string text Recognized text positionsCol string positions Positions of each block of text (related to pageIteratorLevel) in PageMatrix Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setOcrParams([&quot;preserve_interword_spaces=1&quot;, ]) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr ]) data = pipeline.transform(df) data.show() import com.johnsnowlabs.ocr.transformers.ImageToText import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setOcrParams(Array(&quot;preserve_interword_spaces=1&quot;)) val data = transformer.transform(df) print(data.select(&quot;text&quot;).collect()[0].text) Image: Output: FOREWORD Electronic design engineers are the true idea men of the electronic industries. They create ideas and use them in their designs, they stimu- late ideas in other designers, and they borrow and adapt ideas from others. One could almost say they feed on and grow on ideas. ImageToTextV2 ImageToTextV2 is based on the transformers architecture, and combines CV and NLP in one model. It is a visual encoder-decoder model. The Encoder is based on ViT, and the decoder on RoBERTa model. ImageToTextV2 can work on CPU, but GPU is preferred in order to achieve acceptable performance. ImageToTextV2 can receive regions representing single line texts, or regions coming from a text detection model. Input Columns Param name Type Default Column Data Description inputCols Array[string] [image] Can use as input image struct (Image schema) and regions. Parameters Param name Type Default Description lineTolerance integer 15 Line tolerance in pixels. It’s used for grouping text regions by lines. borderWidth integer 5 A value of more than 0 enables to border text regions with width equal to the value of the parameter. spaceWidth integer 10 A value of more than 0 enables to add white spaces between words on the image. Output Columns Param name Type Default Column Data Description outputCol string text Recognized text Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) text_detector = ImageTextDetectorV2 .pretrained(&quot;image_text_detector_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text_regions&quot;) .setWithRefiner(True) .setSizeThreshold(20) ocr = ImageToTextV2.pretrained(&quot;ocr_base_printed&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCols([&quot;image&quot;, &quot;text_regions&quot;]) .setOutputCol(&quot;text&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, text_detector, ocr ]) data = pipeline.transform(df) data.show() not implemented Image: Output: STARBUCKS STORE #10208 11302 EUCLID AVENUE CLEVELAND, OH (216) 229-0749 CHK 664290 12/07/2014 06:43 PM 1912003 DRAWER: 2. REG: 2 VT PEP MOCHA 4.95 SBUX CARD 4.95 XXXXXXXXXXXX3228 SUBTOTAL $4.95 TOTAL $4.95 CHANGE DUE $0.00 - CHECK CLOSED 12/07/2014 06:43 PM SBUX CARD X3228 NEW BALANCE: 37.45 CARD IS REGISTERED ImageToTextPdf ImageToTextPdf runs OCR for input image, render recognized text to the PDF as an invisible text layout with an original image. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) originCol string path path to the original file pageNumCol string pagenum for compatibility with another transformers Parameters Param name Type Default Description ocrParams array of strings [] Array of Ocr params in key=value format. Output Columns Param name Type Default Column Data Description outputCol string pdf Recognized text rendered to PDF PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToTextPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;pdf&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr ]) data = pipeline.transform(df) data.show() import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageToTextPdf() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;pdf&quot;) val data = transformer.transform(df) data.show() ImageToHocr ImageToHocr runs OCR for input image, return recognized text and bounding boxes to outputCol column in HOCR format. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description pageSegMode PageSegmentationMode AUTO page segmentation mode pageIteratorLevel PageIteratorLevel BLOCK page iteration level ocrEngineMode EngineMode LSTM_ONLY OCR engine mode language string eng language ignoreResolution bool true Ignore resolution from metadata of image. ocrParams array of strings [] Array of Ocr params in key=value format. Output Columns Param name Type Default Column Data Description outputCol string hocr Recognized text Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr ]) data = pipeline.transform(df) data.show() import com.johnsnowlabs.ocr.transformers.ImageToHocr import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val data = transformer.transform(df) print(data.select(&quot;hocr&quot;).collect()[0].hocr) Image: Output: &lt;div class=&#39;ocr_page&#39; id=&#39;page_1&#39; title=&#39;image &quot;&quot;; bbox 0 0 1280 467; ppageno 0&#39;&gt; &lt;div class=&#39;ocr_carea&#39; id=&#39;block_1_1&#39; title=&quot;bbox 516 80 780 114&quot;&gt; &lt;p class=&#39;ocr_par&#39; id=&#39;par_1_1&#39; lang=&#39;eng&#39; title=&quot;bbox 516 80 780 114&quot;&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_1&#39; title=&quot;bbox 516 80 780 114; baseline 0 -1; x_size 44; x_descenders 11; x_ascenders 11&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_1&#39; title=&#39;bbox 516 80 780 114; x_wconf 96&#39;&gt;FOREWORD&lt;/span&gt; &lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;div class=&#39;ocr_carea&#39; id=&#39;block_1_2&#39; title=&quot;bbox 40 237 1249 425&quot;&gt; &lt;p class=&#39;ocr_par&#39; id=&#39;par_1_2&#39; lang=&#39;eng&#39; title=&quot;bbox 40 237 1249 425&quot;&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_2&#39; title=&quot;bbox 122 237 1249 282; baseline 0.001 -12; x_size 45; x_descenders 12; x_ascenders 13&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_2&#39; title=&#39;bbox 122 237 296 270; x_wconf 96&#39;&gt;Electronic&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_3&#39; title=&#39;bbox 308 237 416 281; x_wconf 96&#39;&gt;design&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_4&#39; title=&#39;bbox 428 243 588 282; x_wconf 96&#39;&gt;engineers&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_5&#39; title=&#39;bbox 600 250 653 271; x_wconf 96&#39;&gt;are&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_6&#39; title=&#39;bbox 665 238 718 271; x_wconf 96&#39;&gt;the&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_7&#39; title=&#39;bbox 731 246 798 272; x_wconf 97&#39;&gt;true&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_8&#39; title=&#39;bbox 810 238 880 271; x_wconf 96&#39;&gt;idea&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_9&#39; title=&#39;bbox 892 251 963 271; x_wconf 96&#39;&gt;men&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_10&#39; title=&#39;bbox 977 238 1010 272; x_wconf 96&#39;&gt;of&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_11&#39; title=&#39;bbox 1021 238 1074 271; x_wconf 96&#39;&gt;the&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_12&#39; title=&#39;bbox 1086 239 1249 272; x_wconf 96&#39;&gt;electronic&lt;/span&gt; &lt;/span&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_3&#39; title=&quot;bbox 41 284 1248 330; baseline 0.002 -13; x_size 44; x_descenders 11; x_ascenders 12&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_13&#39; title=&#39;bbox 41 284 214 318; x_wconf 96&#39;&gt;industries.&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_14&#39; title=&#39;bbox 227 284 313 328; x_wconf 96&#39;&gt;They&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_15&#39; title=&#39;bbox 324 292 427 319; x_wconf 96&#39;&gt;create&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_16&#39; title=&#39;bbox 440 285 525 319; x_wconf 96&#39;&gt;ideas&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_17&#39; title=&#39;bbox 537 286 599 318; x_wconf 96&#39;&gt;and&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_18&#39; title=&#39;bbox 611 298 668 319; x_wconf 96&#39;&gt;use&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_19&#39; title=&#39;bbox 680 286 764 319; x_wconf 96&#39;&gt;them&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_20&#39; title=&#39;bbox 777 291 808 319; x_wconf 96&#39;&gt;in&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_21&#39; title=&#39;bbox 821 286 900 319; x_wconf 96&#39;&gt;their&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_22&#39; title=&#39;bbox 912 286 1044 330; x_wconf 96&#39;&gt;designs,&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_23&#39; title=&#39;bbox 1058 286 1132 330; x_wconf 93&#39;&gt;they&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_24&#39; title=&#39;bbox 1144 291 1248 320; x_wconf 92&#39;&gt;stimu-&lt;/span&gt; &lt;/span&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_4&#39; title=&quot;bbox 42 332 1247 378; baseline 0.002 -14; x_size 44; x_descenders 12; x_ascenders 12&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_25&#39; title=&#39;bbox 42 332 103 364; x_wconf 97&#39;&gt;late&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_26&#39; title=&#39;bbox 120 332 204 365; x_wconf 96&#39;&gt;ideas&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_27&#39; title=&#39;bbox 223 337 252 365; x_wconf 96&#39;&gt;in&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_28&#39; title=&#39;bbox 271 333 359 365; x_wconf 96&#39;&gt;other&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_29&#39; title=&#39;bbox 376 333 542 377; x_wconf 96&#39;&gt;designers,&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_30&#39; title=&#39;bbox 561 334 625 366; x_wconf 96&#39;&gt;and&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_31&#39; title=&#39;bbox 643 334 716 377; x_wconf 96&#39;&gt;they&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_32&#39; title=&#39;bbox 734 334 855 366; x_wconf 96&#39;&gt;borrow&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_33&#39; title=&#39;bbox 873 334 934 366; x_wconf 96&#39;&gt;and&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_34&#39; title=&#39;bbox 954 335 1048 378; x_wconf 96&#39;&gt;adapt&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_35&#39; title=&#39;bbox 1067 334 1151 367; x_wconf 96&#39;&gt;ideas&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_36&#39; title=&#39;bbox 1169 334 1247 367; x_wconf 96&#39;&gt;from&lt;/span&gt; &lt;/span&gt; &lt;span class=&#39;ocr_line&#39; id=&#39;line_1_5&#39; title=&quot;bbox 40 379 1107 425; baseline 0.002 -13; x_size 45; x_descenders 12; x_ascenders 12&quot;&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_37&#39; title=&#39;bbox 40 380 151 412; x_wconf 96&#39;&gt;others.&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_38&#39; title=&#39;bbox 168 383 238 412; x_wconf 96&#39;&gt;One&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_39&#39; title=&#39;bbox 252 379 345 412; x_wconf 96&#39;&gt;could&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_40&#39; title=&#39;bbox 359 380 469 413; x_wconf 96&#39;&gt;almost&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_41&#39; title=&#39;bbox 483 392 537 423; x_wconf 96&#39;&gt;say&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_42&#39; title=&#39;bbox 552 381 626 424; x_wconf 96&#39;&gt;they&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_43&#39; title=&#39;bbox 641 381 712 414; x_wconf 96&#39;&gt;feed&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_44&#39; title=&#39;bbox 727 393 767 414; x_wconf 96&#39;&gt;on&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_45&#39; title=&#39;bbox 783 381 845 414; x_wconf 96&#39;&gt;and&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_46&#39; title=&#39;bbox 860 392 945 425; x_wconf 97&#39;&gt;grow&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_47&#39; title=&#39;bbox 959 393 999 414; x_wconf 96&#39;&gt;on&lt;/span&gt; &lt;span class=&#39;ocrx_word&#39; id=&#39;word_1_48&#39; title=&#39;bbox 1014 381 1107 414; x_wconf 95&#39;&gt;ideas.&lt;/span&gt; &lt;/span&gt; &lt;/p&gt; &lt;/div&gt; &lt;/div&gt; ImageBrandsToText ImageBrandsToText runs OCR for specified brands of input image, return recognized text to outputCol and positions with font size to ‘positionsCol’ column. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description pageSegMode PageSegmentationMode AUTO page segmentation mode pageIteratorLevel PageIteratorLevel BLOCK page iteration level ocrEngineMode EngineMode LSTM_ONLY OCR engine mode language string eng language confidenceThreshold int 0 Confidence threshold. ignoreResolution bool true Ignore resolution from metadata of image. ocrParams array of strings [] Array of Ocr params in key=value format. brandsCoords string   Json with coordinates of brands. Output Columns Param name Type Default Column Data Description outputCol structure image_brands Structure with recognized text from brands. textCol string text Recognized text positionsCol string positions Positions of each block of text (related to pageIteratorLevel) Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageBrandsToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setBrandsCoords(&quot;&quot;&quot;[ { &quot;name&quot;:&quot;part_one&quot;, &quot;rectangle&quot;:{ &quot;x&quot;:286, &quot;y&quot;:65, &quot;width&quot;:542, &quot;height&quot;:342 } }, { &quot;name&quot;:&quot;part_two&quot;, &quot;rectangle&quot;:{ &quot;x&quot;:828, &quot;y&quot;:65, &quot;width&quot;:1126, &quot;height&quot;:329 } } ]&quot;&quot;&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr ]) data = pipeline.transform(df) data.show() import com.johnsnowlabs.ocr.transformers.ImageToText import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val transformer = new ImageBrandsToText() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;text&quot;) .setBrandsCoordsStr( &quot;&quot;&quot; [ { &quot;name&quot;:&quot;part_one&quot;, &quot;rectangle&quot;:{ &quot;x&quot;:286, &quot;y&quot;:65, &quot;width&quot;:542, &quot;height&quot;:342 } }, { &quot;name&quot;:&quot;part_two&quot;, &quot;rectangle&quot;:{ &quot;x&quot;:828, &quot;y&quot;:65, &quot;width&quot;:1126, &quot;height&quot;:329 } } ] &quot;&quot;&quot;.stripMargin) val data = transformer.transform(df) print(data.select(&quot;text&quot;).collect()[0].text) Other Next section describes the extra transformers PositionFinder PositionFinder find the position of input text entities in the original document. Input Columns Param name Type Default Column Data Description inputCols string image Input annotations columns pageMatrixCol string   Column name for Page Matrix schema Parameters Param name Type Default Description matchingWindow int 10 Textual range to match in context, applies in both direction windowPageTolerance boolean true whether or not to increase tolerance as page number grows padding int 5 padding for area Output Columns Param name Type Default Column Data Description outputCol string   Name of output column for store coordinates. Example: PythonScala from pyspark.ml import Pipeline from sparkocr.transformers import * from sparknlp.annotator import * from sparknlp.base import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;page&quot;) .setSplitPage(False) document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) entity_extractor = TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;./sparkocr/resources/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) position_finder = PositionFinder() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;coordinates&quot;) .setPageMatrixCol(&quot;positions&quot;) .setMatchingWindow(10) .setPadding(2) pipeline = Pipeline(stages=[ pdf_to_text, document_assembler, sentence_detector, tokenizer, entity_extractor, position_finder ]) results = pipeline.fit(df).transform(df) results.show() import com.johnsnowlabs.ocr.transformers._ import com.johnsnowlabs.nlp.{DocumentAssembler, SparkAccessor} import com.johnsnowlabs.nlp.annotators._ import com.johnsnowlabs.nlp.util.io.ReadAs val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) val positionFinder = new PositionFinder() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;coordinates&quot;) .setPageMatrixCol(&quot;positions&quot;) .setMatchingWindow(10) .setPadding(2) // Create pipeline val pipeline = new Pipeline() .setStages(Array( pdfToText, documentAssembler, sentenceDetector, tokenizer, entityExtractor, positionFinder )) val results = pipeline.fit(df).transform(df) results.show() UpdateTextPosition UpdateTextPosition update output text and keep old coordinates of original document. Input Columns Param name Type Default Column Data Description inputCol string positions Сolumn name with original positions struct InputText string replace_text Column name for New Text to replace Old one Output Columns Param name Type Default Column Data Description outputCol string output_positions Name of output column for updated positions struct. Example: PythonScala from pyspark.ml import Pipeline from sparkocr.transformers import * from sparknlp.annotator import * from sparknlp.base import * pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text = PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setPageNumCol(&quot;page&quot;) .setSplitPage(False) document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_detector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;tokens&quot;) spell = NorvigSweetingModel().pretrained(&quot;spellcheck_norvig&quot;, &quot;en&quot;) .setInputCols(&quot;tokens&quot;) .setOutputCol(&quot;spell&quot;) tokenAssem = TokenAssembler() .setInputCols(&quot;spell&quot;) .setOutputCol(&quot;newDocs&quot;) updatedText = UpdateTextPosition() .setInputCol(&quot;positions&quot;) .setOutputCol(&quot;output_positions&quot;) .setInputText(&quot;newDocs.result&quot;) pipeline = Pipeline(stages=[ document_assembler, sentence_detector, tokenizer, spell, tokenAssem, updatedText ]) results = pipeline.fit(df).transform(df) results.show() import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.spell.norvig.NorvigSweetingModel import com.johnsnowlabs.nlp.{DocumentAssembler, TokenAssembler} import com.johnsnowlabs.ocr.transformers._ import org.apache.spark.ml.Pipeline val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val token = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;tokens&quot;) val spell = NorvigSweetingModel.pretrained(&quot;spellcheck_norvig&quot;, &quot;en&quot;) .setInputCols(&quot;tokens&quot;) .setOutputCol(&quot;spell&quot;) val tokenAssem = new TokenAssembler() .setInputCols(&quot;spell&quot;) .setOutputCol(&quot;newDocs&quot;) val updatedText = new UpdateTextPosition() .setInputCol(&quot;positions&quot;) .setOutputCol(&quot;output_positions&quot;) .setInputText(&quot;newDocs.result&quot;) val pipeline = new Pipeline() .setStages(Array( pdfToText, documentAssembler, sentence, token, spell, tokenAssem, updatedText )) val results = pipeline.fit(df).transform(df) results.show() FoundationOneReportParser FoundationOneReportParser is a transformer for parsing FoundationOne reports. Current implementation supports parsing patient info, genomic, biomarker findings and gene lists from appendix. Output format is json. Input Columns Param name Type Default Column Data Description inputCol string text Сolumn name with text of report originCol string path path to the original file Output Columns Param name Type Default Column Data Description outputCol string report Name of output column with report in json format. Example: PythonScala from pyspark.ml import Pipeline from sparkocr.transformers import * from sparkocr.enums import TextStripperType pdfPath = &quot;path to pdf&quot; # Read PDF file as binary file df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) pdf_to_text = PdfToText() pdf_to_text.setInputCol(&quot;content&quot;) pdf_to_text.setOutputCol(&quot;text&quot;) pdf_to_text.setSplitPage(False) pdf_to_text.setTextStripper(TextStripperType.PDF_LAYOUT_TEXT_STRIPPER) genomic_parser = FoundationOneReportParser() genomic_parser.setInputCol(&quot;text&quot;) genomic_parser.setOutputCol(&quot;report&quot;) report = genomic_parser.transform(pdf_to_text.transform(df)).collect() import com.johnsnowlabs.ocr.transformers._ import org.apache.spark.ml.Pipeline val pdfPath = &quot;path to pdf&quot; // Read PDF file as binary file val df = spark.read.format(&quot;binaryFile&quot;).load(pdfPath) val pdfToText = new PdfToText() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;text&quot;) .setSplitPage(false) .setTextStripper(TextStripperType.PDF_LAYOUT_TEXT_STRIPPER) val genomicsParser = new FoundationOneReportParser() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;report&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( pdfToText, genomicsParser )) val modelPipeline = pipeline.fit(df) val report = modelPipeline.transform(df) Output: { &quot;Patient&quot; : { &quot;disease&quot; : &quot;Unknown primary melanoma&quot;, &quot;name&quot; : &quot;Lekavich Gloria&quot;, &quot;date_of_birth&quot; : &quot;11 November 1926&quot;, &quot;sex&quot; : &quot;Female&quot;, &quot;medical_record&quot; : &quot;11111&quot; }, &quot;Physician&quot; : { &quot;ordering_physician&quot; : &quot;Genes Pinley&quot;, &quot;medical_facility&quot; : &quot;Health Network Cancer Institute&quot;, &quot;additional_recipient&quot; : &quot;Nath&quot;, &quot;medical_facility_id&quot; : &quot;202051&quot;, &quot;pathologist&quot; : &quot;Manqju Nwath&quot; }, &quot;Specimen&quot; : { &quot;specimen_site&quot; : &quot;Rectum&quot;, &quot;specimen_id&quot; : &quot;AVS 1A&quot;, &quot;specimen_type&quot; : &quot;Slide&quot;, &quot;date_of_collection&quot; : &quot;20 March 2015&quot;, &quot;specimen_received&quot; : &quot;30 March 2015 &quot; }, &quot;Biomarker_findings&quot; : [ { &quot;name&quot; : &quot;Tumor Mutation Burden&quot;, &quot;state&quot; : &quot;TMB-Low (3Muts/Mb)&quot;, &quot;actionability&quot; : &quot;No therapies or clinical trials. &quot; } ], &quot;Genomic_findings&quot; : [ { &quot;name&quot; : &quot;FLT3&quot;, &quot;state&quot; : &quot;amplification&quot;, &quot;therapies_with_clinical_benefit_in_patient_tumor_type&quot; : [ &quot;none&quot; ], &quot;therapies_with_clinical_benefit_in_other_tumor_type&quot; : [ &quot;Sorafenib&quot;, &quot;Sunitinib&quot;, &quot;Ponatinib&quot; ] } ], &quot;Appendix&quot; : { &quot;dna_gene_list&quot; : [ &quot;ABL1&quot;, &quot;ACVR1B&quot;, &quot;AKT1&quot;, .... ], &quot;dna_gene_list_rearrangement&quot; : [ &quot;ALK&quot;, &quot;BCL2&quot;, &quot;BCR&quot;, .... ], &quot;additional_assays&quot; : [ &quot;Tumor Mutation Burden (TMB)&quot;, &quot;Microsatellite Status (MS)&quot; ] } } HocrDocumentAssembler HocrDocumentAssembler prepares data into a format that is processable by Spark NLP. Output Annotator Type: DOCUMENT Input Columns Param name Type Default Column Data Description inputCol string hocr Сolumn name with HOCR of the document Output Columns Param name Type Default Column Data Description outputCol string document Name of output column. Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) hocr_document_assembler = HocrDocumentAssembler() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;document&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr, hocr_document_assembler ]) result = pipeline.transform(df) result.select(&quot;document&quot;).show() import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToHocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val hocrDocumentAssembler = HocrDocumentAssembler() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;document&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( imageToHocr, hocrDocumentAssembler )) val modelPipeline = pipeline.fit(df) val result = modelPipeline.transform(df) result.select(&quot;document&quot;).show() Output: +--+ | document | +--+ | [[document, 0, 4392, Patient Nam Financial Numbe Random Hospital...| +--+ HocrTokenizer HocrTokenizer prepares into a format that is processable by Spark NLP. HocrTokenizer puts to metadata coordinates and ocr confidence. Output Annotator Type: TOKEN Input Columns Param name Type Default Column Data Description inputCol string hocr Сolumn name with HOCR of the document. Output Columns Param name Type Default Column Data Description outputCol string token Name of output column. Example: PythonScala from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) tokenizer = HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr, tokenizer ]) result = pipeline.transform(df) result.select(&quot;token&quot;).show() import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToHocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val tokenizer = HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( imageToHocr, tokenizer )) val modelPipeline = pipeline.fit(df) val result = modelPipeline.transform(df) result.select(&quot;token&quot;).show() Output: +--+ | token | +--+ | [[token, 0, 6, patient, [x -&gt; 2905, y -&gt; 527, height -&gt; 56, | | confidence -&gt; 95, word -&gt; Patient, width -&gt; 230], []], [token, 8, | |10, nam, [x -&gt; 3166, y -&gt; 526, height -&gt; 55, confidence -&gt; 95, word | |-&gt; Nam, width -&gt; 158], []] ... | +--+",
    "url": "/docs/en/ocr_pipeline_components",
    "relUrl": "/docs/en/ocr_pipeline_components"
  },
  "111": {
    "id": "111",
    "title": "Spark OCR release notes",
    "content": "4.3.1 Release date: 17-02-2023 We’re glad to announce that Visual NLP 😎 4.3.1 has been released. Highlights ImageTextCleaner &amp; ImageTableDetector have improved memory consumption. New Annotators supported in LightPipelines. Table extraction from Digital PDFs pipeline now entirely supported as a LightPipeline. ImageTextCleaner &amp; ImageTableDetector improved memory consumption ImageTextCleaner &amp; ImageTableDetector improved memory consumption: we reduced about 30% the memory consumption for this annotator making it more memory friendly and enabling running on memory constrained environments like Colab. New Annotators supported in LightPipelines Now the following annotators are supported in LightPipelines, PdfToHocr, HocrTokenizer, ImageTableDetector, ImageScaler, HocrToTextTable, Table extraction from Digital PDFs pipeline now entirely supported as a LightPipeline. Our Table Extraction from digital PDFs pipeline now supports running as a LightPipeline, check the updated notebook: SparkOCRPdfToTable.ipynb This release is compatible with Spark NLP for Healthcare 4.3.0, and Spark NLP 4.3.0. Previous versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/ocr_release_notes",
    "relUrl": "/docs/en/spark_ocr_versions/ocr_release_notes"
  },
  "112": {
    "id": "112",
    "title": "Structures and helpers",
    "content": "Schemas Image Schema Images are loaded as a DataFrame with a single column called “image.” It is a struct-type column, that contains all information about image: image: struct (nullable = true) | |-- origin: string (nullable = true) | |-- height: integer (nullable = false) | |-- width: integer (nullable = false) | |-- nChannels: integer (nullable = false) | |-- mode: integer (nullable = false) | |-- resolution: integer (nullable = true) | |-- data: binary (nullable = true) Fields Field name Type Description origin string source URI height integer image height in pixels width integer image width in pixels nChannels integer number of color channels mode ImageType the data type and channel order the data is stored in resolution integer resolution of image in dpi data binary image data in a binary format NOTE: Image data stored in a binary format. Image data is represented as a 3-dimensional array with the dimension shape (height, width, nChannels) and array values of type t specified by the mode field. Coordinate Schema element: struct (containsNull = true) | | |-- index: integer (nullable = false) | | |-- page: integer (nullable = false) | | |-- x: float (nullable = false) | | |-- y: float (nullable = false) | | |-- width: float (nullable = false) | | |-- height: float (nullable = false) Field name Type Description index integer Chunk index page integer Page number x float The lower left x coordinate y float The lower left y coordinate width float The width of the rectangle height float The height of the rectangle score float The score of the object label string The label of the object PageMatrix Schema element: struct (containsNull = true) | | |-- mappings: array[struct] (nullable = false) Field name Type Description mappings Array[Mapping] Array of mappings Mapping Schema element: struct (containsNull = true) | | |-- c: string (nullable = false) | | |-- p: integer (nullable = false) | | |-- x: float (nullable = false) | | |-- y: float (nullable = false) | | |-- width: float (nullable = false) | | |-- height: float (nullable = false) | | |-- fontSize: integer (nullable = false) Field name Type Description c string Character p integer Page number x float The lower left x coordinate y float The lower left y coordinate width float The width of the rectangle height float The height of the rectangle fontSize integer Font size in points Enums PageSegmentationMode OSD_ONLY: Orientation and script detection only. AUTO_OSD: Automatic page segmentation with orientation and script detection. AUTO_ONLY: Automatic page segmentation, but no OSD, or OCR. AUTO: Fully automatic page segmentation, but no OSD. SINGLE_COLUMN: Assume a single column of text of variable sizes. SINGLE_BLOCK_VERT_TEXT: Assume a single uniform block of vertically aligned text. SINGLE_BLOCK: Assume a single uniform block of text. SINGLE_LINE: Treat the image as a single text line. SINGLE_WORD: Treat the image as a single word. CIRCLE_WORD: Treat the image as a single word in a circle. SINGLE_CHAR: Treat the image as a single character. SPARSE_TEXT: Find as much text as possible in no particular order. SPARSE_TEXT_OSD: Sparse text with orientation and script detection. EngineMode TESSERACT_ONLY: Legacy engine only. OEM_LSTM_ONLY: Neural nets LSTM engine only. TESSERACT_LSTM_COMBINED: Legacy + LSTM engines. DEFAULT: Default, based on what is available. PageIteratorLevel BLOCK: Block of text/image/separator line. PARAGRAPH: Paragraph within a block. TEXTLINE: Line within a paragraph. WORD: Word within a text line. SYMBOL: Symbol/character within a word. Language ENG: English FRA: French SPA: Spanish RUS: Russian DEU: German VIE: Vietnamese ARA: Arabic ModelType BASE: Block of text/image/separator line. BEST: Paragraph within a block. FAST: Line within a paragraph. ImageType TYPE_BYTE_GRAY TYPE_BYTE_BINARY TYPE_3BYTE_BGR TYPE_4BYTE_ABGR NoiseMethod VARIANCE RATIO KernelShape SQUARE DIAMOND DISK OCTAHEDRON OCTAGON STAR MorphologyOperationType OPENING CLOSING EROSION DILATION CropSquareType TOP_LEFT TOP_CENTER TOP_RIGHT CENTER_LEFT CENTER CENTER_RIGHT BOTTOM_LEFT BOTTOM_CENTER BOTTOM_RIGHT SplittingStrategy FIXED_NUMBER_OF_PARTITIONS FIXED_SIZE_OF_PARTITION AdaptiveThresholdingMethod GAUSSIAN MEAN MEDIAN WOLF SINGH TresholdingMethod GAUSSIAN OTSU SAUVOLA WOLF CellDetectionAlgos CONTOURS - Detect cells in bordered tables MORPHOPS - Detected calls in: bordered, borderless and combined tables TableOutputFormat TABLE - Table struct format CSV - Comma separated CSV OCR implicits asImage asImage transforms binary content to Image schema. Parameters Param name Type Default Description outputCol string image output column name contentCol string content input column name with binary content pathCol string path input column name with path to original file Example: import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) df.show() storeImage storeImage stores the image(s) to tmp location and return Dataset with path(s) to stored image files. Parameters Param name Type Default Description inputColumn string   input column name with image struct formatName string png image format name prefix string sparknlp_ocr_ prefix for output file Example: import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) df.storeImage(&quot;image&quot;) showImages Show images on Databrics notebook. Parameters Param name Type Default Description field string image input column name with image struct limit integer 5 count of rows for display width string “800” width of image show_meta boolean true enable/disable displaying metadata of image Jupyter Python helpers display_image Show single image with metadata in Jupyter notebook. Parameters Param name Type Default Description width string “600” width of image show_meta boolean true enable/disable displaying metadata of image Example: from sparkocr.utils import display_image from sparkocr.transformers import BinaryToImage images_path = &quot;/tmp/ocr/images/*.tif&quot; images_example_df = spark.read.format(&quot;binaryFile&quot;).load(images_path).cache() display_image(BinaryToImage().transform(images_example_df).collect()[0].image) display_images Show images from dataframe. Parameters Param name Type Default Description field string image input column name with image struct limit integer 5 count of rows for display width string “600” width of image show_meta boolean true enable/disable displaying metadata of image Example: from sparkocr.utils import display_images from sparkocr.transformers import BinaryToImage images_path = &quot;/tmp/ocr/images/*.tif&quot; images_example_df = spark.read.format(&quot;binaryFile&quot;).load(images_path).cache() display_images(BinaryToImage().transform(images_example_df), limit=3) display_images_horizontal Show one or more images per row from dataframe. Parameters Param name Type Default Description fields string image comma separated input column names with image struct limit integer 5 count of rows for display width string “600” width of image show_meta boolean true enable/disable displaying metadata of image Example: from sparkocr.utils import display_images_horizontal display_images_horizontal(df_with_few_image_fields, fields=&quot;images, image_with_regions&quot;, limit=10) display_pdf Show pdf from dataframe. Parameters Param name Type Default Description field string content input column with binary representation of pdf limit integer 5 count of rows for display width string “600” width of image show_meta boolean true enable/disable displaying metadata of image Example: from sparkocr.utils import display_pdf pdf_df = spark.read.format(&quot;binaryFile&quot;).load(pdf_path) display_pdf(pdf_df) display_pdf_file Show pdf file using embedded pdf viewer. Parameters Param name Type Default Description pdf string   Path to the file name size integer size=(600, 500) count of rows for display Example: from sparkocr.utils import display_pdf_file display_pdf_file(&quot;path to the pdf file&quot;) Example output: display_table Display table from the dataframe. display_tables Display tables from the dataframe. It is useful for display results of table recognition from the multipage documents/few tables per page. Example output: Databricks Python helpers display_images Show images from dataframe. Parameters Param name Type Default Description field string image input column name with image struct limit integer 5 count of rows for display width string “800” width of image show_meta boolean true enable/disable displaying metadata of image Example: from sparkocr.databricks import display_images from sparkocr.transformers import BinaryToImage images_path = &quot;/tmp/ocr/images/*.tif&quot; images_example_df = spark.read.format(&quot;binaryFile&quot;).load(images_path).cache() display_images(BinaryToImage().transform(images_example_df), limit=3)",
    "url": "/docs/en/ocr_structures",
    "relUrl": "/docs/en/ocr_structures"
  },
  "113": {
    "id": "113",
    "title": "Table recognition",
    "content": "ImageTableDetector ImageTableDetector is a DL model for detecting tables on the image. It’s based on CascadeTabNet which used Cascade mask Region-based CNN High-Resolution Network (Cascade mask R-CNN HRNet). Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description scoreThreshold float 0.9 Score threshold for output regions. applyCorrection boolean false Enable correction of results. Output Columns Param name Type Default Column Data Description outputCol string table_regions array of [Coordinaties]ocr_structures#coordinate-schema) Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect tables val table_detector = ImageTableDetector .pretrained(&quot;general_model_table_detection_v2&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;table_regions&quot;) val draw_regions = new ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;table_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, table_detector, draw_regions ]) val data = pipeline.transform(df) data.storeImage(&quot;image_with_regions&quot;) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect tables table_detector = ImageTableDetector .pretrained(&quot;general_model_table_detection_v2&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCol(&quot;image&quot;) .setOutputCol(&quot;table_regions&quot;) draw_regions = ImageDrawRegions() .setInputCol(&quot;image&quot;) .setInputRegionsCol(&quot;table_regions&quot;) .setOutputCol(&quot;image_with_regions&quot;) pipeline = PipelineModel(stages=[ binary_to_image, table_detector, draw_regions ]) data = pipeline.transform(df) display_images(data, &quot;image_with_regions&quot;) Output: ImageTableCellDetector ImageTableCellDetector detect cells in a table image. It’s based on an image processing algorithm that detects horizontal and vertical lines. Current implementation support few algorithm for extract cells: CellDetectionAlgos.CONTOURS works only for bordered tables. CellDetectionAlgos.MORPHOPS works for bordered, borderless and combined tables. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) Parameters Param name Type Default Description algoType CellDetectionAlgos CellDetectionAlgos.CONTOURS Algorithm for detect cells. algoParams string row_treshold=0.05,row_treshold_wide=1.0, row_min_wide=5,column_treshold=0.05, column_treshold_wide=5,column_min_wide=5 Parameters of ‘MORPHOPS’ cells detection algorithm drawDetectedLines boolean false Enable to draw detected lines to the output image keepOriginalLines boolean false Keep original images on the output image Output Columns Param name Type Default Column Data Description outputCol string cells array of coordinates of cells outputImageCol string output_image output image Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect cells val transformer = new ImageTableCellDetector() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cells&quot;) val data = transformer.transform(df) data.select(&quot;cells&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) # Define transformer for detect cells transformer = ImageTableCellDetector .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cells&quot;) .setAlgoParams(&quot;row_treshold=0.05&quot;) pipeline = PipelineModel(stages=[ binary_to_image, transformer ]) data = pipeline.transform(df) data.select(&quot;cells&quot;).show() Image: Output:* +-+ | cells | +-+ ||[[[[15, 17, 224, 53]], [[241, 17, 179, 53]], [[423, 17, | | 194, 53]], [[619, 17, 164, 53]] .... | +-+ ImageCellsToTextTable ImageCellsToTextTable runs OCR for cells regions on image, return recognized text to outputCol as TableContainer structure. Input Columns Param name Type Default Column Data Description inputCol string image image struct (Image schema) cellsCol string celss Array of cells Parameters Param name Type Default Description strip bool true Strip output text. margin bool 1 Margin of cells in pixelx. pageSegMode PageSegmentationMode AUTO page segmentation mode ocrEngineMode EngineMode LSTM_ONLY OCR engine mode language Language Language.ENG language ocrParams array of strings [] Array of Ocr params in key=value format. pdfCoordinates bool false Transform coordinates in positions to PDF points. modelData string   Path to the local model data. modelType ModelType ModelType.BASE Model type downloadModelData bool false Download model data from JSL S3 outputFormat TableOutputFormat TableOutputFormat.TABLE Output format Output Columns Param name Type Default Column Data Description outputCol string table Recognized text as TableContainer Example: PythonScala import org.apache.spark.ml.Pipeline import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) // Define transformer for detect cells val cell_detector = new ImageTableCellDetector() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;cells&quot;) val table_recognition = new ImageCellsToTextTable() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;tables&quot;) .setMargin(2) // Define pipeline val pipeline = new Pipeline() pipeline.setStages(Array( cell_detector, table_recognition )) val modelPipeline = pipeline.fit(spark.emptyDataFrame) val results = modelPipeline.transform(df) results.select(&quot;tables&quot;) .withColumn(&quot;cells&quot;, explode(col(&quot;tables.chunks&quot;))) .select((0 until 7).map(i =&gt; col(&quot;cells&quot;)(i).getField(&quot;chunkText&quot;).alias(s&quot;col$i&quot;)): _*) .show(false) from pyspark.ml import PipelineModel import pyspark.sql.functions as f from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() binary_to_image.setImageType(ImageType.TYPE_BYTE_GRAY) binary_to_image.setInputCol(&quot;content&quot;) cell_detector = TableCellDetector() cell_detector.setInputCol(&quot;image&quot;) cell_detector.setOutputCol(&quot;cells&quot;) cell_detector.setKeepInput(True) table_recognition = ImageCellsToTextTable() table_recognition.setInputCol(&quot;image&quot;) table_recognition.setCellsCol(&#39;cells&#39;) table_recognition.setMargin(2) table_recognition.setStrip(True) table_recognition.setOutputCol(&#39;table&#39;) pipeline = PipelineModel(stages=[ binary_to_image, cell_detector, table_recognition ]) result = pipeline.transform(df) results.select(&quot;table&quot;) .withColumn(&quot;cells&quot;, f.explode(f.col(&quot;table.chunks&quot;))) .select([f.col(&quot;cells&quot;)[i].getField(&quot;chunkText&quot;).alias(f&quot;col{i}&quot;) for i in range(0, 7)]) .show(20, False) Image: Output: +-+-+--++--++-+ |col0 |col1 |col2 |col3 |col4 |col5 |col6 | +-+-+--++--++-+ |Order Date|Region |Rep |Item |Units|Unit Cost|Total | |1/23/10 |Ontario|Kivell |Binder|50 |$19.99 |$999.50| |2/9/10 |Ontario|Jardine |Pencil|36 |$4.99 |$179.64| |2/26/10 |Ontario|Gill |Pen |27 |$19.99 |$539.73| |3/15/10 |Alberta|Sorvino |Pencil|56 |$2.99 |$167.44| |4/1/10 |Quebec |Jones |Binder|60 |$4.99 |$299.40| |4/18/10 |Ontario|Andrews |Pencil|75 |$1.99 |$149.25| |5/5/10 |Ontario|Jardine |Pencil|90 |$4.99 |$449.10| |5/22/10 |Alberta|Thompson|Pencil|32 |$1.99 |$63.68 | +-+-+--++--++-+",
    "url": "/docs/en/ocr_table_recognition",
    "relUrl": "/docs/en/ocr_table_recognition"
  },
  "114": {
    "id": "114",
    "title": "Visual document understanding",
    "content": "NLP models are great at processing digital text, but many real-word applications use documents with more complex formats. For example, healthcare systems often include visual lab results, sequencing reports, clinical trial forms, and other scanned documents. When we only use an NLP approach for document understanding, we lose layout and style information - which can be vital for document image understanding. New advances in multi-modal learning allow models to learn from both the text in documents (via NLP) and visual layout (via computer vision). We provide multi-modal visual document understanding, built on Spark OCR based on the LayoutLM architecture. It achieves new state-of-the-art accuracy in several downstream tasks, including form understanding (from 70.7 to 79.3), receipt understanding (from 94.0 to 95.2) and document image classification (from 93.1 to 94.4). Please check also webinar: Visual Document Understanding with Multi-Modal Image &amp; Text Mining in Spark OCR 3 VisualDocumentClassifier VisualDocumentClassifier is a DL model for document classification using text and layout data. Currently available pretrained model on the Tobacco3482 dataset, that contains 3482 images belonging to 10 different classes (Resume, News, Note, Advertisement, Scientific, Report, Form, Letter, Email and Memo) Input Columns Param name Type Default Column Data Description inputCol string hocr Сolumn name with HOCR of the document Parameters Param name Type Default Description maxSentenceLength int 128 Maximum sentence length. caseSensitive boolean false Determines whether model is case sensitive. confidenceThreshold float 0f Confidence threshold. Output Columns Param name Type Default Column Data Description labelCol string label Name of output column with the predicted label. confidenceCol string confidence Name of output column with confidence. Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToHocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val visualDocumentClassifier = VisualDocumentClassifier .pretrained(&quot;visual_document_classifier_tobacco3482&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setMaxSentenceLength(128) .setInputCol(&quot;hocr&quot;) .setLabelCol(&quot;label&quot;) .setConfidenceCol(&quot;conf&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( imageToHocr, visualDocumentClassifier )) val modelPipeline = pipeline.fit(df) val result = modelPipeline.transform(df) result.select(&quot;label&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) document_classifier = VisualDocumentClassifier() .pretrained(&quot;visual_document_classifier_tobacco3482&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setMaxSentenceLength(128) .setInputCol(&quot;hocr&quot;) .setLabelCol(&quot;label&quot;) .setConfidenceCol(&quot;conf&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr, document_classifier, ]) result = pipeline.transform(df) result.select(&quot;label&quot;).show() Output: ++ | label| ++ |Letter| ++ VisualDocumentNER VisualDocumentNER is a DL model for NER documents using text and layout data. Currently available pre-trained model on the SROIE dataset. The dataset has 1000 whole scanned receipt images. Input Columns Param name Type Default Column Data Description inputCol string hocr Сolumn name with HOCR of the document Parameters Param name Type Default Description maxSentenceLength int 512 Maximum sentence length. caseSensitive boolean false Determines whether model is case sensitive. whiteList Array[String]   Whitelist of output labels Output Columns Param name Type Default Column Data Description outputCol string entities Name of output column with entities Annotation. Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; // Read image file as binary file val df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) .asImage(&quot;image&quot;) val imageToHocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) val visualDocumentNER = VisualDocumentNER .pretrained(&quot;visual_document_NER_SROIE0526&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) .setMaxSentenceLength(512) .setInputCol(&quot;hocr&quot;) val pipeline = new Pipeline() pipeline.setStages(Array( imageToHocr, visualDocumentNER )) val modelPipeline = pipeline.fit(df) val result = modelPipeline.transform(df) result.select(&quot;entities&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binary_to_image = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) document_ner = VisualDocumentNer() .pretrained(&quot;visual_document_NER_SROIE0526&quot;, &quot;en&quot;, &quot;public/ocr/models&quot;) .setMaxSentenceLength(512) .setInputCol(&quot;hocr&quot;) .setLabelCol(&quot;label&quot;) # Define pipeline pipeline = PipelineModel(stages=[ binary_to_image, ocr, document_ner, ]) result = pipeline.transform(df) result.select(&quot;entities&quot;).show() Output: +-+ |entities | +-+ |[[entity, 0, 0, O, [word -&gt; 0£0, token -&gt; 0£0], []], [entity, 0, 0, | | B-COMPANY, [word -&gt; AEON, token -&gt; aeon], []], [entity, 0, 0, B-COMPANY,| | [word -&gt; CO., token -&gt; co], ... | +-+ VisualDocumentNERv2 VisualDocumentNERv2 is a DL model for NER documents which is an improved version of VisualDocumentNER. There is available pretrained model trained on FUNSD dataset. The dataset comprises 199 real, fully annotated, scanned forms. Input Columns Param name Type Default Column Data Description inputCols Array[String]   Сolumn names for tokens of the document and image Parameters Param name Type Default Description maxSentenceLength int 512 Maximum sentence length. whiteList Array[String]   Whitelist of output labels Output Columns Param name Type Default Column Data Description outputCol string entities Name of output column with entities Annotation. Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; var dataFrame = spark.read.format(&quot;binaryFile&quot;).load(imagePath) var bin2imTransformer = new BinaryToImage() bin2imTransformer.setImageType(ImageType.TYPE_3BYTE_BGR) val ocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) .setIgnoreResolution(false) .setOcrParams(Array(&quot;preserve_interword_spaces=0&quot;)) val tokenizer = new HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) val visualDocumentNER = VisualDocumentNERv2 .pretrained(&quot;layoutlmv2_funsd&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCols(Array(&quot;token&quot;, &quot;image&quot;)) val pipeline = new Pipeline() .setStages(Array( bin2imTransformer, ocr, tokenizer, visualDocumentNER )) val results = pipeline .fit(dataFrame) .transform(dataFrame) .select(&quot;entities&quot;) .cache() result.select(&quot;entities&quot;).show() from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) .setIgnoreResolution(False) .setOcrParams([&quot;preserve_interword_spaces=0&quot;]) tokenizer = HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) ner = VisualDocumentNerV2() .pretrained(&quot;layoutlmv2_funsd&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCols([&quot;token&quot;, &quot;image&quot;]) .setOutputCol(&quot;entities&quot;) pipeline = PipelineModel(stages=[ binToImage, ocr, tokenizer, ner ]) result = pipeline.transform(df) result.withColumn(&#39;filename&#39;, path _array.getItem(f.size(path_array)- 1)) .withColumn(&quot;exploded_entities&quot;, f.explode(&quot;entities&quot;)) .select(&quot;filename&quot;, &quot;exploded_entities&quot;) .show(truncate=False) Output sample: ++-+ |filename |exploded_entities | ++-+ |form1.jpg|[entity, 0, 6, i-answer, [x -&gt; 1027, y -&gt; 89, height -&gt; 19, confidence -&gt; 96, word -&gt; Version:, width -&gt; 90], []] | |form1.jpg|[entity, 25, 35, b-header, [x -&gt; 407, y -&gt; 190, height -&gt; 37, confidence -&gt; 96, word -&gt; Institution, width -&gt; 241], []] | |form1.jpg|[entity, 37, 40, i-header, [x -&gt; 667, y -&gt; 190, height -&gt; 37, confidence -&gt; 96, word -&gt; Name, width -&gt; 130], []] | |form1.jpg|[entity, 42, 52, b-question, [x -&gt; 498, y -&gt; 276, height -&gt; 19, confidence -&gt; 96, word -&gt; Institution, width -&gt; 113], []]| |form1.jpg|[entity, 54, 60, i-question, [x -&gt; 618, y -&gt; 276, height -&gt; 19, confidence -&gt; 96, word -&gt; Address, width -&gt; 89], []] | ++-+ FormRelationExtractor FormRelationExtractor detect relation between keys and values detected by VisualDocumentNERv2. It can detect relations only for key/value in same line. Input Columns Param name Type Default Column Data Description inputCol String   Column name for entities Annotation Parameters Param name Type Default Description lineTolerance int 15 Line tolerance in pixels. This is the space between lines that will be assumed. It is used for grouping text regions by lines. keyPattern String question Pattern of entity name for keys in form. valuePattern String answer Pattern of entity name for values in form. Output Columns Param name Type Default Column Data Description outputCol string relations Name of output column with relation Annotations. Example: PythonScala import com.johnsnowlabs.ocr.transformers.* import com.johnsnowlabs.ocr.OcrContext.implicits._ val imagePath = &quot;path to image&quot; var dataFrame = spark.read.format(&quot;binaryFile&quot;).load(imagePath) var bin2imTransformer = new BinaryToImage() bin2imTransformer.setImageType(ImageType.TYPE_3BYTE_BGR) val ocr = new ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) .setIgnoreResolution(false) .setOcrParams(Array(&quot;preserve_interword_spaces=0&quot;)) val tokenizer = new HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) val visualDocumentNER = VisualDocumentNERv2 .pretrained(&quot;layoutlmv2_funsd&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCols(Array(&quot;token&quot;, &quot;image&quot;)) val relExtractor = new FormRelationExtractor() .setInputCol(&quot;entities&quot;) .setOutputCol(&quot;relations&quot;) val pipeline = new Pipeline() .setStages(Array( bin2imTransformer, ocr, tokenizer, visualDocumentNER, relExtractor )) val results = pipeline .fit(dataFrame) .transform(dataFrame) .select(&quot;relations&quot;) .cache() results.select(explode(&quot;relations&quot;)).show(3, False) from pyspark.ml import PipelineModel from sparkocr.transformers import * imagePath = &quot;path to image&quot; # Read image file as binary file df = spark.read .format(&quot;binaryFile&quot;) .load(imagePath) binToImage = BinaryToImage() .setInputCol(&quot;content&quot;) .setOutputCol(&quot;image&quot;) ocr = ImageToHocr() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;hocr&quot;) .setIgnoreResolution(False) .setOcrParams([&quot;preserve_interword_spaces=0&quot;]) tokenizer = HocrTokenizer() .setInputCol(&quot;hocr&quot;) .setOutputCol(&quot;token&quot;) ner = VisualDocumentNerV2() .pretrained(&quot;layoutlmv2_funsd&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) .setInputCols([&quot;token&quot;, &quot;image&quot;]) .setOutputCol(&quot;entities&quot;) rel_extractor = FormRelationExtractor() .setInputCol(&quot;entities&quot;) .setOutputCol(&quot;relations&quot;) pipeline = PipelineModel(stages=[ binToImage, ocr, tokenizer, ner, rel_extractor ]) result = pipeline.transform(df) result.select(explode(&quot;relations&quot;)).show(3, False) Output sample: ++ |col | ++ |[relation, 112, 134, Name: Dribbler, bbb, [bbox1 -&gt; 58 478 69 19, ...| |[relation, 136, 161, Study Date: 12-09-2006, 6:34, [bbox1 -&gt; 431 ... | |[relation, 345, 361, BP: 120 80 mmHg, [bbox1 -&gt; 790 478 30 19, ... | ++",
    "url": "/docs/en/ocr_visual_document_understanding",
    "relUrl": "/docs/en/ocr_visual_document_understanding"
  },
  "115": {
    "id": "115",
    "title": "Oncology - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/oncology",
    "relUrl": "/oncology"
  },
  "116": {
    "id": "116",
    "title": "Pipelines",
    "content": "Pretrained Pipelines have moved to Models Hub. Please follow this link for the updated list of all models and pipelines: Models Hub English NOTE: noncontrib pipelines are compatible with Windows operating systems. Pipelines Name Explain Document ML explain_document_ml Explain Document DL explain_document_dl Explain Document DL Win explain_document_dl_noncontrib Explain Document DL Fast explain_document_dl_fast Explain Document DL Fast Win explain_document_dl_fast_noncontrib Recognize Entities DL recognize_entities_dl Recognize Entities DL Win recognize_entities_dl_noncontrib OntoNotes Entities Small onto_recognize_entities_sm OntoNotes Entities Large onto_recognize_entities_lg Match Datetime match_datetime Match Pattern match_pattern Match Chunk match_chunks Match Phrases match_phrases Clean Stop clean_stop Clean Pattern clean_pattern Clean Slang clean_slang Check Spelling check_spelling Analyze Sentiment analyze_sentiment Analyze Sentiment DL analyze_sentimentdl_use_imdb Analyze Sentiment DL analyze_sentimentdl_use_twitter Dependency Parse dependency_parse explain_document_ml import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;The Paris metro will soon enter the 21st century, ditching single-use paper tickets for rechargeable electronic cards.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_ml,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 7 more fields] ++--+--+--+--+--+--+--+--+ | id| text| document| sentence| token| checked| lemmas| stems| pos| ++--+--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...| | 2|The Paris metro w...|[[document, 0, 11...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...| ++--+--+--+--+--+--+--+--+ */ explain_document_dl import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_dl,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 10 more fields] ++--+--+--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| checked| lemma| stem| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[token, 0, 5, Go...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...| | 2|The Paris metro w...|[[document, 0, 11...|[[token, 0, 2, Th...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 4, 8, Pa...| ++--+--+--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Google, TensorFlow] | |[Donald John Trump, United States]| +-+ */ recognize_entities_dl import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;recognize_entities_dl&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(entity_recognizer_dl,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 6 more fields] ++--+--+--+--+--+--+--+ | id| text| document| sentence| token| embeddings| ner| ner_converter| ++--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 5, Go...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...| | 2|Donald John Trump...|[[document, 0, 92...|[[document, 0, 92...|[[token, 0, 5, Do...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 16, D...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Google, TensorFlow] | |[Donald John Trump, United States]| +-+ */ onto_recognize_entities_sm Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the OntoNotes corpus and supports the identification of 18 entities. import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London, from 2008 to 2016, before rejoining Parliament. &quot;), (2, &quot;A little less than a decade later, dozens of self-driving startups have cropped up while automakers around the world clamor, wallet in hand, to secure their place in the fast-moving world of fully automated transportation.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;onto_recognize_entities_sm&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.1.0 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(onto_recognize_entities_sm,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 6 more fields] ++--+--+--+--+--+--+ | id| text| document| token| embeddings| ner| entities| ++--+--+--+--+--+--+ | 1|Johnson first ent...|[[document, 0, 17...|[[token, 0, 6, Jo...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 6, Jo...| | 2|A little less tha...|[[document, 0, 22...|[[token, 0, 0, A,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 32, A...| ++--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* ++ |result | ++ |[Johnson, first, 2001, Parliament, eight years, London, 2008 to 2016, Parliament]| |[A little less than a decade later, dozens] | ++ */ onto_recognize_entities_lg Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the OntoNotes corpus and supports the identification of 18 entities. import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London, from 2008 to 2016, before rejoining Parliament. &quot;), (2, &quot;A little less than a decade later, dozens of self-driving startups have cropped up while automakers around the world clamor, wallet in hand, to secure their place in the fast-moving world of fully automated transportation.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;onto_recognize_entities_lg&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.1.0 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(onto_recognize_entities_lg,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 6 more fields] ++--+--+--+--+--+--+ | id| text| document| token| embeddings| ner| entities| ++--+--+--+--+--+--+ | 1|Johnson first ent...|[[document, 0, 17...|[[token, 0, 6, Jo...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 6, Jo...| | 2|A little less tha...|[[document, 0, 22...|[[token, 0, 0, A,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 32, A...| ++--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Johnson, first, 2001, Parliament, eight years, London, 2008, 2016, Parliament]| |[A little less than a decade later, dozens] | +-+ */ match_datetime DateMatcher yyyy/MM/dd import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;I would like to come over and see you in 01/02/2019.&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;match_datetime&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(match_datetime,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 4 more fields] ++--+--+--+--+--+ | id| text| document| sentence| token| date| ++--+--+--+--+--+ | 1|I would like to c...|[[document, 0, 51...|[[document, 0, 51...|[[token, 0, 0, I,...|[[date, 41, 50, 2...| | 2|Donald John Trump...|[[document, 0, 92...|[[document, 0, 92...|[[token, 0, 5, Do...|[[date, 24, 36, 1...| ++--+--+--+--+--+ */ annotation.select(&quot;date.result&quot;).show(false) /* ++ |result | ++ |[2019/01/02]| |[1946/06/14]| ++ */ match_pattern RegexMatcher (match phone numbers) import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;You should call Mr. Jon Doe at +33 1 79 01 22 89&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;match_pattern&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(match_pattern,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 4 more fields] ++--+--+--+--+--+ | id| text| document| sentence| token| regex| ++--+--+--+--+--+ | 1|You should call M...|[[document, 0, 47...|[[document, 0, 47...|[[token, 0, 2, Yo...|[[chunk, 31, 47, ...| ++--+--+--+--+--+ */ annotation.select(&quot;regex.result&quot;).show(false) /* +-+ |result | +-+ |[+33 1 79 01 22 89]| +-+ */ match_chunks The pipeline uses regex &lt;DT/&gt;?/&lt;JJ/&gt;*&lt;NN&gt;+ import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;The book has many chapters&quot;), (2, &quot;the little yellow dog barked at the cat&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;match_chunks&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(match_chunks,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 5 more fields] ++--+--+--+--+--+--+ | id| text| document| sentence| token| pos| chunk| ++--+--+--+--+--+--+ | 1|The book has many...|[[document, 0, 25...|[[document, 0, 25...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[chunk, 0, 7, Th...| | 2|the little yellow...|[[document, 0, 38...|[[document, 0, 38...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[chunk, 0, 20, t...| ++--+--+--+--+--+--+ */ annotation.select(&quot;chunk.result&quot;).show(false) /* +--+ |result | +--+ |[The book] | |[the little yellow dog, the cat]| +--+ */ French Pipelines Name Explain Document Large explain_document_lg Explain Document Medium explain_document_md Entity Recognizer Large entity_recognizer_lg Entity Recognizer Medium entity_recognizer_md Feature Description NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura POS Trained by PerceptronApproach annotator on the Universal Dependencies Size Model size indicator, md and lg. The large pipeline uses glove_840B_300 and the medium uses glove_6B_300 WordEmbeddings French explain_document_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_lg&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,fr,public/models) testData: org.apache.spark.sql.DataFrame = [id: bigint, text: string] annotation: org.apache.spark.sql.DataFrame = [id: bigint, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[token, 0, 12, C...|[[pos, 0, 12, ADV...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[token, 0, 7, Em...|[[pos, 0, 7, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /*+-+ |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ French explain_document_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_md&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_md,fr,public/models) testData: org.apache.spark.sql.DataFrame = [id: bigint, text: string] annotation: org.apache.spark.sql.DataFrame = [id: bigint, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[token, 0, 12, C...|[[pos, 0, 12, ADV...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[token, 0, 7, Em...|[[pos, 0, 7, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, au CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ French entity_recognizer_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_lg&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ French entity_recognizer_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_md&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /*+-+ |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, au CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ Italian Pipelines Name Explain Document Large explain_document_lg Explain Document Medium explain_document_md Entity Recognizer Large entity_recognizer_lg Entity Recognizer Medium entity_recognizer_md Feature Description NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Lemma Trained by Lemmatizer annotator on DXC Technology dataset POS Trained by PerceptronApproach annotator on the Universal Dependencies Size Model size indicator, md and lg. The large pipeline uses glove_840B_300 and the medium uses glove_6B_300 WordEmbeddings Italian explain_document_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_lg&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[token, 0, 1, La...|[[pos, 0, 1, DET,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 3, 6, FI...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[token, 0, 4, Re...|[[pos, 0, 4, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +--+ |result | +--+ |[FIFA, Zidane, Materazzi] | |[Reims, Domani, Mondiali femminili]| +--+ */ Italian explain_document_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_md&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[token, 0, 1, La...|[[pos, 0, 1, DET,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 9, La...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[token, 0, 4, Re...|[[pos, 0, 4, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[La FIFA, Zidane, Materazzi]| |[Reims, Domani, Mondiali] | +-+ */ Italian entity_recognizer_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_lg&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 3, 6, FI...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +--+ |result | +--+ |[FIFA, Zidane, Materazzi] | |[Reims, Domani, Mondiali femminili]| +--+ */ Italian entity_recognizer_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_md&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 9, La...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[La FIFA, Zidane, Materazzi]| |[Reims, Domani, Mondiali] | +-+ */ Spanish Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.4.0 es   Download Explain Document Medium explain_document_md 2.4.0 es   Download Explain Document Large explain_document_lg 2.4.0 es   Download Entity Recognizer Small entity_recognizer_sm 2.4.0 es   Download Entity Recognizer Medium entity_recognizer_md 2.4.0 es   Download Entity Recognizer Large entity_recognizer_lg 2.4.0 es   Download Feature Description Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Size Model size indicator, sm, md, and lg. The small pipelines use glove_100d, the medium pipelines use glove_6B_300, and large pipelines use glove_840B_300 WordEmbeddings Russian Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.4.4 ru   Download Explain Document Medium explain_document_md 2.4.4 ru   Download Explain Document Large explain_document_lg 2.4.4 ru   Download Entity Recognizer Small entity_recognizer_sm 2.4.4 ru   Download Entity Recognizer Medium entity_recognizer_md 2.4.4 ru   Download Entity Recognizer Large entity_recognizer_lg 2.4.4 ru   Download Feature Description Lemma Trained by Lemmatizer annotator on the Universal Dependencies POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Dutch Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 nl   Download Explain Document Medium explain_document_md 2.5.0 nl   Download Explain Document Large explain_document_lg 2.5.0 nl   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 nl   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 nl   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 nl   Download Norwegian Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 no   Download Explain Document Medium explain_document_md 2.5.0 no   Download Explain Document Large explain_document_lg 2.5.0 no   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 no   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 no   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 no   Download Polish Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 pl   Download Explain Document Medium explain_document_md 2.5.0 pl   Download Explain Document Large explain_document_lg 2.5.0 pl   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 pl   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 pl   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 pl   Download Portuguese Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 pt   Download Explain Document Medium explain_document_md 2.5.0 pt   Download Explain Document Large explain_document_lg 2.5.0 pt   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 pt   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 pt   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 pt   Download Multi-language Pipeline Name Build lang Description Offline LanguageDetectorDL detect_language_7 2.5.2 xx   Download LanguageDetectorDL detect_language_20 2.5.2 xx   Download The model with 7 languages: Czech, German, English, Spanish, French, Italy, and Slovak The model with 20 languages: Bulgarian, Czech, German, Greek, English, Spanish, Finnish, French, Croatian, Hungarian, Italy, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Swedish, Turkish, and Ukrainian How to use Online To use Spark NLP pretrained pipelines, you can call PretrainedPipeline with pipeline’s name and its language (default is en): pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;) Same in Scala val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;, lang=&quot;en&quot;) Offline If you have any trouble using online pipelines or models in your environment (maybe it’s air-gapped), you can directly download them for offline use. After downloading offline models/pipelines and extracting them, here is how you can use them iside your code (the path could be a shared storage like HDFS in a cluster): val advancedPipeline = PipelineModel.load(&quot;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&quot;) // To use the loaded Pipeline for prediction advancedPipeline.transform(predictionDF)",
    "url": "/docs/en/pipelines",
    "relUrl": "/docs/en/pipelines"
  },
  "117": {
    "id": "117",
    "title": "Playground",
    "content": "The Playground feature of the NLP Lab allows users to deploy and test models, rules, and/or prompts without going through the project setup wizard. This simplifies the initial resources exploration, and facilitates experiments on custom data. Any model, rule, or prompt can now be selected and deployed for testing by clicking on the “Open in Playground” button. Experiment with Rules Rules can be deployed to the Playground from the rules page. When a particular rule is deployed in the playground, the user can also change the parameters of the rules via the rule definition form from the right side of the page. After saving the changes users need to click on the “Deploy” button to refresh the results of the pre-annotation on the provided text. Experiment with Prompts NLP Lab’s Playground also supports the deployment and testing of prompts. Users can quickly test the results of applying a prompt on custom text, can easily edit the prompt, save it, and deploy it right away to see the change in the pre-annotation results. Experiment with Models Any Classification, NER or Assertion Status model available on the NLP Lab can also be deployed to Playground for testing on custom text. Deployment of models and rules is supported by floating and air-gapped licenses. Healthcare, Legal, and Finance models require a license with their respective scopes to be deployed in Playground. Unlike pre-annotation servers, only one playground can be deployed at any given time. Direct Navigation to Active Playground Sessions Navigating between multiple projects to and from the playground experiments can be necessary, especially when you want to revisit a previously edited prompt or rule. This is why NLP Lab Playground now allow users to navigate to any active Playground session without having to redeploy the server. This feature enables users to check how their resources (models, rules and prompts) behave at project level, compare the preannotation results with ground truth, and quickly get back to experiments for modifying prompts or rules without losing progress or spending time on new deployments. This feature makes experimenting with NLP prompts and rules in a playground more efficient, streamlined, and productive. Automatic Deployment of Updated Rules/Prompts Another benefit of experimenting with NLP prompts and rules in the playground is the immediate feedback that you receive. When you make changes to the parameters of your rules or to the questions in your prompts, the updates are deployed instantly. Manually deploying the server is not necessary any more for changes made to Rules/Prompts to be reflected in the preannotation results. Once the changes are saved, by simply clicking on the Test button, updated results are presented. This allows you to experiment with a range of variables and see how each one affects the correctness and completeness of the results. The real-time feedback and immediate deployment of changes in the playground make it a powerful tool for pushing the boundaries of what is possible with language processing. Playground Server Destroyed after 5 Minutes of Inactivity When active, the NLP playground consumes resources from your server. For this reason, NLP Lab defines an idle time limit of 5 minutes after which the playground is automatically destroyed. This is done to ensure that the server resources are not being wasted on idle sessions. When the server is destroyed, a message is displayed, so users are aware that the session has ended. Users can view information regarding the reason for the Playground’s termination, and have the option to restart by pressing the Restart button. Playground Servers use Light Pipelines The replacement of regular preannotation pipelines with Light Pipelines has a significant impact on the performance of the NLP playground. Light pipelines allow for faster initial deployment, quicker pipeline update and fast processing of text data, resulting in overall quicker results in the UI. Direct Access to Model Details Page on the Playground Another useful feature of NLP Lab Playground is the ability to quickly and easily access information on the models being used. This information can be invaluable for users who are trying to gain a deeper understanding of the model’s inner workings and capabilities. In particular, by click on the model’s name it is now possible to navigate to the NLP Models hub page. This page provides users with additional details about the model, including its training data, architecture, and performance metrics. By exploring this information, users can gain a better understanding of the model’s strengths and weaknesses, and use this knowledge to make more informed decisions on how good the model is for the data they need to annotate.",
    "url": "/docs/en/alab/playground",
    "relUrl": "/docs/en/alab/playground"
  },
  "118": {
    "id": "118",
    "title": "Preannotation",
    "content": "Annotation Lab offers out-of-the-box support for Named Entity Recognition, Classification, Assertion Status, and Relations preannotations. These are extremely useful for bootstrapping any annotation project as the annotation team does not need to start the labeling from scratch but can leverage the existing knowledge transfer from domain experts to models. This way, the annotation efforts are significantly reduced. To run pre-annotation on one or several tasks, the Project Owner or the Manager must select the target tasks and click on the Pre-Annotate button from the top right side of the Tasks page. It will display a popup with information regarding the last deployment of the model server with the list of models deployed and the labels they predict. This information is crucial, especially when multiple users are doing training and deployment in parallel. So before doing preannotations on your tasks, carefully check the list of currently deployed models and their labels. If needed, users can deploy the models defined in the current project (based on the current Labeling Config) by clicking the Deploy button. After the deployment is complete, the preannotation can be triggered. Since Annotation Lab 3.0.0, multiple preannotation servers are available to preannotate the tasks of a project. The dialog box that opens when clicking the Pre-Annotate button on the Tasks page now lists available model servers in the options. Project Owners or Managers can now select the server to use. On selecting a model server, information about the configuration deployed on the server is shown on the popup so users can make an informed decision on which server to use. In case a preannotation server does not exist for the current project, the dialog box also offers the option to deploy a new server with the current project’s configuration. If this option is selected and enough resources are available (infrastructure capacity and a license if required) the server is deployed, and preannotation can be started. If there are no free resources, users can delete one or several existing servers from Clusters page under the Settings menu. Concurrency is not only supported between preannotation servers but also between training and preannotation. Users can have training running on one project and preannotation running on another project at the same time. Preannotation Approaches Pretrained Models On the Predefined Labels step of the Project Configuration page we can find the list of available models with their respective prediction labels. By selecting the relevant labels for your project and clicking the Add Label button you can add the predefined labels to your project configuration and take advantage of the Spark NLP auto labeling capabilities. In the example below, we are reusing the ner_posology model that comes with 7 labels related to drugs. In the same manner classification, assertion status or relation models can be added to the project configuration and used for preannotation purpose. Starting from version 4.3.0, Finance and Legal models downloaded from the Models Hub can be used for pre-annotation of NER, assertion status and classification projects. Visual NER models can now be downloaded from the NLP Models Hub, and used for pre-annotating image-based documents. Once you download the models from the Models Hub page, you can see the model’s label in the Predefined Label tab on the project configuration page. Rules Preannotation of NER projects can also be done using Rules. Rules are used to speed up the manual annotation process. Once a rule is defined, it is available for use in any project. However, for defining and running the rules we will need a [Healthcare NLP](/docs/en/licensed_install) license. In the example below, we are reusing the available rules for preannotation. Read more on how to create rules and reuse them to speed up the annotation process here. Text Preannotation Preannotation is available for projects with text contents as the tasks. When you setup a project to use existing Spark NLP models for pre-annotation, you can run the designated models on all of your tasks by pressing the Pre-Annotate button on the top-right corner of the Tasks page. As a result, all predicted labels for a given task will be available in the Prediction widget on the Labeling page. The predictions are not editable. You can only view and navigate those or compare those with older predictions. However, you can create a new completion based on a given prediction. All labels and relations from such a new completion are now editable. Visual Preannotation For running pre-annotation on one or several tasks, the Project Owner or the Manager must select the target tasks and can click on the Pre-Annotate button from the upper right side of the Tasks Page. It will display a popup with information regarding the last deployment of the model server, including the list of models deployed and the labels they predict. Known Limitations: When bulk pre-annotation runs on many tasks, the pre-annotation can fail due to memory issues. Preannotation currently works at the token level, and does not merge all tokens of a chunk into one entity. Pipeline Limitations Loading too many models in the preannotation server is not memory efficient and may not be practically required. Starting from version 1.8.0, Annotation Lab supports maximum of five different models to be used for the preannotation server deployment. Another restriction for Annotation Lab versions older than 4.2.0 is that two models trained on different embeddings cannot be used together in the same project. The Labeling Config will throw validation errors in any of the cases above, and we cannot save the configuration preventing preannotation server deployment.",
    "url": "/docs/en/alab/preannotation",
    "relUrl": "/docs/en/alab/preannotation"
  },
  "119": {
    "id": "119",
    "title": "Productionizing Spark NLP",
    "content": "Productionizing Spark NLP in Databricks This documentation page will describe how to use Databricks to run Spark NLP Pipelines for production purposes. About Databricks Databricks is an enterprise software company founded by the creators of Apache Spark. The company has also created MLflow, the Serialization and Experiment tracking library you can use (inside or outside databricks), as described in the section “Experiment Tracking”. Databricks develops a web-based platform for working with Spark, that provides automated cluster management and IPython-style notebooks. Their infrastructured is provided for training and production purposes, and is integrated in cloud platforms as Azure and AWS. Spark NLP is a proud partner of Databricks and we offer a seamless integration with them - see Install on Databricks. All Spark NLP capabilities run in Databricks, including MLFlow serialization and Experiment tracking, what can be used for serving Spark NLP for production purposes. About MLFlow MLFlow is a serialization and Experiment Tracking platform, which also natively suports Spark NLP. We have a documentation entry about MLFlow in the “Experiment Tracking” section. It’s highly recommended that you take a look before moving forward in this document, since we will use some of the concepts explained there. We will use MLFlow serialization to serve our Spark NLP models. Creating a cluster in Databricks As mentioned before, Spark NLP offers a seamless integration with Databricks. To create a cluster, please follow the instructions in Install on Databricks. That cluster can be then replicated (cloned) for production purposes later on. Configuring Databricks for Spark NLP and MLFlow In Databricks Runtime Version, select any Standard runtime, not ML ones.. These ones add their version of MLFlow, and some incompatibilities may arise. For this example, we have used 8.3 (includes Apache Spark 3.1.1, Scala 2.12) The cluster instantiated is prepared to use Spark NLP, but to make it production-ready using MLFlow, we need to add the MLFlow jar, in addition to the Spark NLP jar, as shown in the “Experiment Tracking” section. In that case, we did it instantiating adding both jars (&quot;spark.jars.packages&quot;:&quot; com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.2,org.mlflow:mlflow-spark:1.21.0&quot;) into the SparkSession. However, in Databricks, you don’t instantiate programatically a session, but you configure it in the Compute screen, selecting your Spark NLP cluster, and then going to Configuration -&gt; Advanced Options -&gt; Sparl -&gt; Spark Config, as shown in the following image: In addition to Spark Config, we need to add the Spark NLP and MLFlow libraries to the Cluster. You can do that by going to Libraries inside your cluster. Make sure you have spark-nlp and mlflow. If not, you can install them either using PyPI or Maven artifacts. In the image below you can see the PyPI alternative: Creating a notebook You are ready to create a notebook in Databricks and attach it to the recently created cluster. To do that, go to Create - Notebook, and select the cluster you want in the dropdown above your notebook. Make sure you have selected the cluster with the right Spark NLP + MLFlow configuration. To check everything is ok, run the following lines: 1) To check the session is running: spark 2) To check jars are in the session: spark.sparkContext.getConf().get(&#39;spark.jars.packages&#39;) You should see the following output from the last line (versions may differ depending on which ones you used to configure your cluster) Out[2]: &#39;com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.2,org.mlflow:mlflow-spark:1.21.0&#39; Logging the experiment in Databricks using MLFlow As explained in the “Experiment Tracking” section, MLFlow can log Spark MLLib / NLP Pipelines as experiments, to carry out runs on them, track versions, etc. MLFlow is natively integrated in Databricks, so we can leverage the mlflow.spark.log_model() function of the Spark flavour of MLFlow, to start tracking our Spark NLP pipelines. Let’s first import our libraries… import mlflow import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import pandas as pd from sparknlp.training import CoNLL import pyspark from pyspark.sql import SparkSession Then, create a Lemmatization pipeline: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = LemmatizerModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;prediction&quot;) # It&#39;s mandatory to call it prediction pipeline = Pipeline(stages=[ documentAssembler, tokenizer, lemmatizer ]) IMPORTANT: Last output column of the last component in the pipeline should be called prediction. Finally, let’s log the experiment. In the “Experiment Tracking” section, we used the pip_requirements parameter in the log_model() function to set the required libraries: But we mentioned using conda is also available. Let’s use conda in this example: conda_env = { &#39;channels&#39;: [&#39;conda-forge&#39;], &#39;dependencies&#39;: [ &#39;python=3.8.8&#39;, { &quot;pip&quot;: [ &#39;pyspark==3.1.1&#39;, &#39;mlflow==1.21.0&#39;, &#39;spark-nlp==3.3.2&#39; ] } ], &#39;name&#39;: &#39;mlflow-env&#39; } With this conda environment, we are ready to log our pipeline: mlflow.spark.log_model(p_model, &quot;lemmatizer&quot;, conda_env=conda_env) You should see an output similar to this one: (6) Spark Jobs (1) MLflow run Logged 1 run to an experiment in MLflow. Learn more Experiment UI On the top right corner of your notebook, you will see the Experiment widget, and inside, as shown in the image below. You can also access Experiments UI if you switch your environment from “Data Science &amp; Engineering” to “Machine Learning”, on the left panel… … or clicking on the “experiment” word in the cell output (it’s a link!) Once in the experiment UI, you will see the following screen, where your experiments are tracked. If you click on the Start Time cell of your experiment, you will reach the registered MLFlow run. On the left panel you will see the MLFlow model and some other artifacts, as the conda.yml and pip_requirements.txt that manage the dependencies of your models. On the right panel, you will see two snippets, about how to call to the model for inference internally from Databricks. 1) Snippet for calling with a Pandas Dataframe: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; # Load model as a Spark UDF. loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model) # Predict on a Spark DataFrame. columns = list(df.columns) df.withColumn(&#39;predictions&#39;, loaded_model(*columns)).collect() 2) Snippet for calling with a Spark Dataframe. We won’t include it in this documentation because that snippet does not include SPark NLP specificities. To make it work, the correct snippet should be: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model) # Predict on a Spark DataFrame. res_spark = loaded_model.predict(df_1_spark.rdd) IMPORTANT: You will only get the last column (prediction) results, which is a list of Rows of Annotation Types. To convert the result list into a Spark Dataframe, use the following schema: import pyspark.sql.types as T import pyspark.sql.functions as f annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) And then, get the results (for example, in res_spark) and apply the schema: spark_res = spark.createDataFrame(res_pandas[0], schema=annotationType) Calling the experiment for production purposes 1. Internally, if the data is in Databricks If your data lies in Datalake, in Spark Tables, or any other internal storage in Databricks, you just need to use the previous snippets (depending if you want to use Pandas or Spark Dataframes), and you are ready to go. Example for Spark Dataframes: Try to use Spark Dataframes by default, since converting from Spark Dataframes into Pandas triggers a collect() first, removing all the parallelism capabilities of Spark Dataframes. The next logical step is to create Notebooks to be called programatically using the snippets above, running into production clusters. There are two ways to do this: using Batch Inference or using Jobs. 2. Internally, using Batch Inference (with Spark Tables) If we come back to the experiment ui, you will see, above the Pandas and Spark snippets, a button with the text “Register Model”. If you do that, you will register the experiment to be called externally, either for Batch Inference or with a REST API (we will get there!). After clicking the Register Model button, you will see a link instead of the button, that will enabled after some seconds. By clicking that link, you will be redirected to the Model Inference screen. This new screen has a button on the top right, that says “Use model for inference”. By clicking on it, you will see two options: Batch Inference or REST API. Batch inference requires a Spark Table for input, and another for output, and after configuring them, what you will see is an auto-generated notebook to be executed on-demand, programatically or with crons, that is prepared to load the environment and do the inference, getting the text fron the input table and storing the results in the output table. This is an example of how the notebook looks like: 3. Externally, with the MLFlow Serve REST API Instead of chosing a Batch Inference, you can select REST API. This will lead you to another screen, when the model will be loaded for production purposes in an independent cluster. Once deployed, you will be able to: 1) Check the endpoint URL to consume the model externally; 2) Test the endpoint writing a json (in our example, ‘text’ is our first input col of the pipeline, so it shoud look similar to: {&quot;text&quot;: &quot;This is a test of how the lemmatizer works&quot;} You can see the response in the same screen. 3) Check what is the Python code or cURL command to do that very same thing programatically. By just using that Python code, you can already consume it for production purposes from any external web app. IMPORTANT: As per 26/11/2021, there is an issue being studied by Databricks team, regarding the creation on the fly of job clusters to serve MLFlow models. There is not a way to configure the Spark Session, so the jars are not loaded and the model fails to start. This will be fixed in later versions of Databricks. In the meantime, see a workaround in point 4. 4. Databricks Jobs asynchronous REST API Creating the notebook for the job And last, but not least, another approach to consume models for production purposes. the Jobs API. Databricks has its own API for managing jobs, that allows you to instantiate any notebook or script as a job, run it, stop it, and manage all the life cycle. And you can configure the cluster where this job will run before hand, what prevents having the issue described in point 3. To do that: 1) Create a new production cluster, as described before, cloning you training environment but adapting it to your needs for production purposes. Make sure the Spark Config is right, as described at the beginning of this documentation. 2) Create a new notebook. Always check that the jars are in the session: spark.sparkContext.getConf().get(&#39;spark.jars.packages&#39;) 3) Add the Spark NLP imports. import mlflow import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import pandas as pd from sparknlp.training import CoNLL import pyspark from pyspark.sql import SparkSession import pyspark.sql.types as T import pyspark.sql.functions as f import json 4) Let’s define that an input param called text will be sent in the request. Let’s get the text from that parameter using dbutils. input = &quot;&quot; try: input = dbutils.widgets.get(&quot;text&quot;) print(&#39;&quot;text&quot; input found: &#39; + input) except: print(&#39;Unable to run: dbutils.widgets.get(&quot;text&quot;). Setting it to NOT_SET&#39;) input = &quot;NOT_SET&quot; Right now, the input text will be in input var. You can trigger an exception or set the input to some default value if the parameter does not come in the request. 5) Let’s create a Spark Dataframe with the input df = spark.createDataFrame([[input]]).toDF(&#39;text&#39;) 6) And now, we just need to use the snippet for Spark Dataframe to consume MLFlow models, described above: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model) # Predict on a Spark DataFrame. res_spark = loaded_model.predict(df_1_spark.rdd) import pyspark.sql.types as T import pyspark.sql.functions as f annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) spark_res = spark.createDataFrame(res_spark[0], schema=annotationType) 7) Let’s transform our lemmatized tokens from the Dataframe into a list of strings: l = spark_res.select(&quot;result&quot;).collect() txt_results = [x[&#39;result&#39;] for x in l] 8) And finally, let’s use again dbutils to tell Databricks to spin off the run and return an exit parameter: the list of token strings. dbutils.notebook.exit(json.dumps({ &quot;status&quot;: &quot;OK&quot;, &quot;results&quot;: txt_results })) Configuring the job Last, but not least. We need to precreate the job, so that we run it from the API. We could do that using the API as well, but we will show you how to do it using the UI. On the left panel, go to Jobs and then Create Job. In the jobs screen, you will see you job created. It’s not running, it’s prepared to be called on demand, programatically or in the interface, with a text input param. Let’s see how to do that: Running the job 1) In the jobs screen, if you click on the job, you will enter the Job screen, and be able to set your text input parameter and run the job manually. You can use this for testing purpores, but the interesting part is calling it externally, using the Databricks Jobs API. 2) Using the Databricks Jobs API, from for example, Postman. POST HTTP request URL: https://[your_databricks_instance]/api/2.1/jobs/run-now Authorization: [use Bearer Token. You can get it from Databricks, Settings, User Settings, Generate New Token.] Body: { &quot;job_id&quot;: [job_id, check it in the Jobs screen], &quot;notebook_params&quot;: {&quot;text&quot;: &quot;This is an example of how well the lemmatizer works&quot;} } As it’s an asynchronous call, it will return the number a number of run, but no results. You will need to query for results using the number of the run and the following url https://[your_databricks_instance]/2.1/jobs/runs/get-output You will get a big json, but the most relevant info, the output, will be up to the end: {&quot;notebook_output&quot;: { &quot;status&quot;: &quot;OK&quot;, &quot;results&quot;: [&quot;This&quot;, &quot;is&quot;, &quot;a&quot;, &quot;example&quot;, &quot;of&quot;, &quot;how&quot;, &quot;lemmatizer&quot;, &quot;work&quot;] }} The notebook will be prepared in the job, but idle, until you call it programatically, what will instantiate a run. Check the Jobs API for more information about what you can do with it and how to adapt it to your solutions for production purposes. Productionizing Spark NLP using Synapse ML This is the first article of the “Serving Spark NLP via API” series, showcasing how to serve Sparkl NLP using Synapse ML and Fast API. There is another article in this series, that showcases how to serve Spark NLP using Databricks Jobs and MLFlow Rest APIs, available here. Background Spark NLP is a Natural Language Understanding Library built on top of Apache Spark, leveranging Spark MLLib pipelines, that allows you to run NLP models at scale, including SOTA Transformers. Therefore, it’s the only production-ready NLP platform that allows you to go from a simple PoC on 1 driver node, to scale to multiple nodes in a cluster, to process big amounts of data, in a matter of minutes. Before starting, if you want to know more about all the advantages of using Spark NLP (as the ability to work at scale on air-gapped environments, for instance) we recommend you to take a look at the following resources: John Snow Labs webpage; The official technical documentation of Spark NLP; Spark NLP channel on Medium; Also, follow Veysel Kocaman, Data Scientist Lead and Head of Spark NLP for Healthcare, for the latests tips. Motivation Spark NLP is server-agnostic, what means it does not come with an integrated API server, but offers a lot of options to serve NLP models using Rest APIs. This is first of a series of 2 articles that explain four options you can use to serve Spark NLP models via Rest API: Using Microsoft’s Synapse ML; Using FastAPI and LightPipelines; Using Databricks Batch API (see Part 2/2 here); Using MLFlow serve API in Databricks (see Part 2/2 here); All of them have their Strengths and weaknesses, so let’s go over them in detail. Microsoft’s Synapse ML Synapse ML (previously named SparkMML) is, as they state in their official webpage: … an ecosystem of tools aimed towards expanding the distributed computing framework Apache Spark in several new directions. They offer a seamless integratation with OpenCV, LightGBM, Microsoft Cognitive Tool and, the most relevant for our use case, Spark Serving, an extension of *Spark Streaming *with an integrated server and a Load Balancer, that can attend multiple requests via Rest API, balance and attend them leveraging the capabilities of a Spark Cluster. That means that you can sin up a server and attend requests that will be distributed transparently over a Spark NLP cluster, in a very effortless way. Strengths Ready-to-use server Includes a Load Balancer Distributes the work over a Spark Cluster Can be used for both Spark NLP and Spark OCR Weaknesses For small use cases that don’t require big cluster processing, other approaches may be faster (as FastAPI using LightPipelines) Requires using an external Framework This approach does not allow you to customize your endpoints, it uses Synapse ML ones How to set up Synapse ML to serve Spark NLP pipelines We will skip here how to install Spark NLP. If you need to do that, please follow this official webpage about how to install Spark NLP or, if Spark NLP for Healthcare if you are using the Healthcare library. Synapse ML recommends using at least Spark 3.2, so first of all, let’s configure the Spark Session with the required jars packages(both for Synapse ML and Spark) with the the proper Spark version (take a look at the suffix spark-nlp-spark32) and also, very important, add to jars.repository the Maven repository for SynapseML. **sparknlpjsl_jar =** &quot;spark-nlp-jsl.jar&quot; **from** pyspark.sql **import** SparkSession **spark =** *SparkSession***.**builder **.**appName(&quot;Spark&quot;) **.**master(&quot;local[*]&quot;) **.***config*(&quot;spark.driver.memory&quot;, &quot;16G&quot;) **.***config*(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) **.***config*(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) **.***config*(&quot;**spark.jars.packages**&quot;, &quot;com.microsoft.azure:synapseml_2.12:0.9.5,com.johnsnowlabs.nlp:spark-nlp-spark32_2.12:3.4.0&quot;) **.***config*(&quot;**spark.jars**&quot;, sparknlpjsl_jar) **.***config*(&quot;**spark.jars.repositories**&quot;, &quot;https://mmlspark.azureedge.net/maven&quot;) **.**getOrCreate() After the initialization, add your required imports (Spark NLP) and add to them the SynapseML-specific ones: **import** sparknlp **import** sparknlp_jsl ... **import** synapse.ml **from** synapse.ml.io **import** ***** Now, let’s create a Spark NLP for Healthcare pipeline to carry out Entity Resolution. **document_assembler =** *DocumentAssembler*() **.**setInputCol(&quot;text&quot;) **.**setOutputCol(&quot;document&quot;) **sentenceDetectorDL =** *SentenceDetectorDLModel***.**pretrained(&quot;sentence_detector_dl_healthcare&quot;, &quot;en&quot;, &#39;clinical/models&#39;) **.**setInputCols([&quot;document&quot;]) **.**setOutputCol(&quot;sentence&quot;) **tokenizer =** *Tokenizer*() **.**setInputCols([&quot;sentence&quot;]) **.**setOutputCol(&quot;token&quot;) **word_embeddings =** *WordEmbeddingsModel***.**pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) **.**setInputCols([&quot;sentence&quot;, &quot;token&quot;]) **.**setOutputCol(&quot;word_embeddings&quot;) **clinical_ner =** *MedicalNerModel***.**pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) **.**setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;]) **.**setOutputCol(&quot;ner&quot;) **ner_converter_icd =** *NerConverterInternal*() **.**setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) **.**setOutputCol(&quot;ner_chunk&quot;) **.**setWhiteList([&#39;PROBLEM&#39;]) **.**setPreservePosition(**False**) **c2doc =** *Chunk2Doc*() **.**setInputCols(&quot;ner_chunk&quot;) **.**setOutputCol(&quot;ner_chunk_doc&quot;) **sbert_embedder =** *BertSentenceEmbeddings***.**pretrained(&#39;sbiobert_base_cased_mli&#39;, &#39;en&#39;,&#39;clinical/models&#39;) **.**setInputCols([&quot;ner_chunk_doc&quot;]) **.**setOutputCol(&quot;sentence_embeddings&quot;) **.**setCaseSensitive(**False**) **icd_resolver =** *SentenceEntityResolverModel***.**pretrained(&quot;sbiobertresolve_icd10cm_augmented_billable_hcc&quot;,&quot;en&quot;, &quot;clinical/models&quot;) **.**setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) **.**setOutputCol(&quot;icd10cm_code&quot;) **.**setDistanceFunction(&quot;EUCLIDEAN&quot;) **resolver_pipeline =** *Pipeline*( stages **=** [ document_assembler, sentenceDetectorDL, tokenizer, word_embeddings, clinical_ner, ner_converter_icd, c2doc, sbert_embedder, icd_resolver ]) Let’s use a clinical note to test Synapse ML. **clinical_note =** &quot;&quot;&quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus (T2DM), one prior episode of HTG-induced pancreatitis three years prior to presentation, associated with an acute hepatitis, and obesity with a body mass index (BMI) of 33.5 kg/m2, presented with a one-week history of polyuria, polydipsia, poor appetite, and vomiting. Two weeks prior to presentation, she was treated with a five-day course of amoxicillin for a respiratory tract infection. She was on metformin, glipizide, and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG. She had been on dapagliflozin for six months at the time of presentation. Physical examination on presentation was significant for dry oral mucosa; significantly, her abdominal examination was benign with no tenderness, guarding, or rigidity.&quot;&quot;&quot; Since SynapseML serves a RestAPI, we will be sending JSON requests. Let’s define a simple json with the clinical note: **data_json =** {&quot;*text*&quot;: clinical_note } Now, let’s spin up a server using Synapse ML Spark Serving. It will consist of: a streaming server that will receive a json and transform it into a Spark Dataframe a call to Spark NLP transform on the dataframe, using the pipeline a write operation returning the output also in json format. #1: Creating the streaming server and transforming json to Spark Dataframe serving_input = spark.readStream.server() .address(“localhost”, 9999, “benchmark_api”) .option(“name”, “benchmark_api”) .load() .parseRequest(“benchmark_api”, data.schema) #2: Applying transform to the dataframe using our Spark NLP pipeline serving_output = resolver_p_model.transform(serving_input) .makeReply(“icd10cm_code”) #3: Returning the response in json format server = serving_output.writeStream .server() .replyTo(“benchmark_api”) .queryName(“benchmark_query”) .option(“checkpointLocation”, “file:///tmp/checkpoints-{}”.format(uuid.uuid1())) .start() And we are ready to test the endpoint using the requests library. **import** requests res **=** requests**.**post(&quot;http://localhost:9999/benchmark_api&quot;, data= json**.**dumps(data_json)) And last, but not least, let’s check the results: **for** i **in** range (0, len(response_list**.**json())): print(response_list**.**json()[i][&#39;result&#39;]) &gt;&gt;O2441 O2411 P702 K8520 B159 E669 Z6841 R35 R631 R630 R111... Productionizing Spark NLP using FastAPI and LightPipelines FastAPI is, as defined by the creators… …a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints. FastAPI provides with a very good latency and response times that, all along witht the good performance of Spark NLP LightPipelines, makes this option the quickest one of the four described in the article. Read more about the performance advantages of using *LightPipelines *in this article created by John Snow Labs Data Scientist Lead Veysel Kocaman. Strengths Quickest approach Adds flexibility to build and adapt a custom API for your models Weaknesses LightPipelines are executed sequentially and don’t leverage the distributed computation that Spark Clusters provide. As an alternative, you can use FastAPI with default pipelines and a custom LoadBalancer, to distribute the calls over your cluster nodes. You can serve SparkNLP + FastAPI on Docker. To do that, we will create a project with the following files: Dockerfile: Image for creating a SparkNLP + FastAPI Docker image requirements.txt: PIP Requirements entrypoint.sh: Dockerfile entrypoint content/: folder containing FastAPI webapp and SparkNLP keys content/main.py: FastAPI webapp, entrypoint content/sparknlp_keys.json: SparkNLP keys (for Healthcare or OCR) Dockerfile The aim of this file is to create a suitable Docker Image with all the OS and Python libraries required to run SparkNLP. Also, adds a entry endpoint for the FastAPI server (see below) and a main folder containing the actual code to run a pipeline on an input text and return the expected values. **FROM **ubuntu:18.04 **RUN **apt-get update &amp;&amp; apt-get -y update **RUN **apt-get -y update &amp;&amp; apt-get install -y wget &amp;&amp; apt-get install -y jq &amp;&amp; apt-get install -y lsb-release &amp;&amp; apt-get install -y openjdk-8-jdk-headless &amp;&amp; apt-get install -y build-essential python3-pip &amp;&amp; pip3 -q install pip --upgrade &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* /usr/share/man /usr/share/doc /usr/share/doc-base **ENV **PYSPARK_DRIVER_PYTHON=python3 **ENV **PYSPARK_PYTHON=python3 **ENV **LC_ALL=C.UTF-8 **ENV **LANG=C.UTF-8 **# We expose the FastAPI default port 8515** **EXPOSE **8515 **# Install all Python required libraries** **COPY **requirements.txt / **RUN **pip install -r /requirements.txt **# Adds the entrypoint to the FastAPI server** **COPY **entrypoint.sh / **RUN **chmod +x /entrypoint.sh **# In /content folder we will have our main.py and the license files COPY **./content/ /content/ **WORKDIR **content/ **# We tell Docker to run this file when a container is instantiated** **ENTRYPOINT **[&quot;/entrypoint.sh&quot;] requirements.txt This file describes which Python libraries will be required when creating the Docker image to run Spark NLP on FastAPI. **pyspark**==3.1.2 **fastapi**==0.70.1 **uvicorn**==0.16 **wget**==3.2 **pandas**==1.4.1 entrypoint.sh This file is the entry point of our Docker container, which carries out the following actions: Takes the sparknlp_keys.json and exports its values as environment variables, as required by Spark NLP for Healthcare. Installs the proper version of Spark NLP for Healthcare, getting the values from the license keys we have just exported in the previous step. Runs the main.py file, that will load the pipelines and create and endpoint to serve them. #!/bin/bash *# Load the license from sparknlp_keys.json and export the values as OS variables **export_json* () { for s in $(echo $values | jq -r ‘to_entries|map(“(.key)=(.value|tostring)”)|.[]’ $1 ); do export $s done } **export_json **“/content/sparknlp_keys.json” **# Installs the proper version of Spark NLP for Healthcare pip install **–upgrade spark-nlp-jsl==$JSL_VERSION –user –extra-index-url https://pypi.johnsnowlabs.com/$SECRET if [ $? != 0 ]; then exit 1 fi **# Script to create FastAPI endpoints and preloading pipelines for inference python3 **/content/main.py content/main.py: Serving 2 pipelines in a FastAPI endpoint To maximize the performance and minimize the latency, we are going to store two Spark NLP pipelines in memory, so that we load only once (at server start) and we just use them everytime we get an API request to infer. To do this, let’s create a content/main.py Python script to download the required resources, store them in memory and serve them in Rest API endpoints. First, the import section **import** uvicorn, json, os **from** fastapi **import** FastAPI **from** sparknlp.annotator **import** ***** **from **sparknlp_jsl.annotator **import ******* **from** sparknlp.base **import** ***** **import **sparknlp, sparknlp_jsl **from **sparknlp.pretrained **import** PretrainedPipeline app **=** FastAPI() pipelines **=** {} Then, let’s define the endpoint to serve the pipeline: **@app.get(&quot;/benchmark/pipeline&quot;)** **async** **def** get_one_sequential_pipeline_result(modelname, text**=**&#39;&#39;): **return** pipelines[modelname]**.**annotate(text) Then, the startup event to preload the pipelines and start a Spark NLP Session: **@app.on_event(&quot;startup&quot;)** **async** **def** startup_event(): **with** open(&#39;/content/sparknlp_keys.json&#39;, &#39;r&#39;) **as** f: license_keys **=** json**.**load(f) ** spark =** sparknlp_jsl**.**start(secret**=**license_keys[&#39;SECRE **pipelines**[&#39;ner_profiling_clinical&#39;] **=** *PretrainedPipeline*(&#39;ner_profiling_clinical&#39;, &#39;en&#39;, &#39;clinical/models&#39;) **pipelines**[&#39;clinical_deidentification&#39;] **=** *PretrainedPipeline*(&quot;clinical_deidentification&quot;, &quot;en&quot;, &quot;clinical/models&quot;) Finally, let’s run a uvicorn server, listening on port 8515 to the endpoints declared before: **if __name__ == &quot;__main__&quot;:** uvicorn**.**run(&#39;main:app&#39;, host**=**&#39;0.0.0.0&#39;, port**=**8515) content/sparknlp_keys.json For using Spark NLP for Healthcare, please add your Spark NLP for Healthcare license keys to content/sparknlp_keys.jsonDThe file is ready, you only need to fulfill with your own values taken from the json file John Snow Labs has provided you with. { &quot;**AWS_ACCESS_KEY_ID**&quot;: &quot;&quot;, &quot;**AWS_SECRET_ACCESS_KEY**&quot;: &quot;&quot;, &quot;**SECRET**&quot;: &quot;&quot;, &quot;**SPARK_NLP_LICENSE**&quot;: &quot;&quot;, &quot;**JSL_VERSION**&quot;: &quot;&quot;, &quot;**PUBLIC_VERSION**&quot;: &quot;&quot; } And now, let’s run the server! Creating the Docker image and running the container docker build -t johnsnowlabs/sparknlp:sparknlp_api . **docker run **-v jsl_keys.json:/content/sparknlp_keys.json -p 8515:8515 -it johnsnowlabs/sparknlp:sparknlp_api 2. Consuming the API using a Python script Lets import some libraries **import** requests **import** time Then, let’s create a clinical note **ner_text =** &quot;&quot;&quot; *A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting. The patient was prescribed 1 capsule of Advil 10 mg for 5 days and magnesium hydroxide 100mg/1ml suspension PO. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day.* &quot;&quot;&quot; We have preloaded and served two Pretrained Pipelines: clinical_deidentification and ner_profiling_clinical . In modelname, let’s set which one we want to check # Change this line to execute any of the two pipelines **modelname =** &#39;*clinical_deidentification*&#39; *# modelname = &#39;ner_profiling_clinical&#39;* And finally, let’s use the requestslibrary to send a test request to the endpoint and get the results. **query =** f&quot;?modelname={modelname}&amp;text={ner_text}&quot; **url =** f&quot;http://localhost:8515/benchmark/pipeline{query}&quot; **print**(requests**.**get(url)) &gt;&gt; {&#39;sentence&#39;: ..., &#39;masked&#39;: ..., &#39;ner_chunk&#39;: ..., } You can also prettify the json using the following function with the result of the annotate() function: **def explode_annotate(ann_result):** &#39;&#39;&#39; Function to convert result object to json input: raw result output: processed result dictionary &#39;&#39;&#39; result = {} for column, ann in ann_result[0].items(): result[column] = [] for lines in ann: content = { &quot;result&quot;: lines.result, &quot;begin&quot;: lines.begin, &quot;end&quot;: lines.end, &quot;metadata&quot;: dict(lines.metadata), } result[column].append(content) return result",
    "url": "/docs/en/production-readiness",
    "relUrl": "/docs/en/production-readiness"
  },
  "120": {
    "id": "120",
    "title": "Productivity",
    "content": "Analytics Charts By default, the Analytics page is disabled for every project because computing the analytical charts is a resource-intensive task and might temporarily influence the responsiveness of the application, especially when triggered in parallel with other training/preannotation jobs. However, users can file a request to enable the Analytics page which can be approved by any admin user. The request is published on the Analytics Requests page, visible to any admin user. Once the admin user approves the request, any team member can access the Analytics page. A refresh button is present on the top-right corner of the Analytics page. The Analytics charts doesn’t automatically reflect the changes made by the annotators (like creating tasks, adding new completion, etc.). Updating the analytics to reflect the latest changes can be done using the refresh button. Task Analytics To access Task Analytics, navigate on the first tab of the Analytics Dashboard, called Tasks. The following blog post explains how to Improve Annotation Quality using Task Analytics in the Annotation Lab. Below are the charts included in the Tasks section. Total number of task in the Project Total number of task in a Project in last 30 days Breakdown of task in the Project by Status Breakdown of task by author Summary of task status for each annotator Total number of label occurrences across all completions Average number of label occurrences for each completion Total number of label occurrences across all completions for each annotator Total vs distinct count of labels across all completions Average number of tokens by label Total number of label occurrences that include numeric values Team Productivity To access Team Productivity charts, navigate on the second tab of the Analytics Dashboard, called Team Productivity. The following blog post explains how to Keep Track of Your Team Productivity in the Annotation Lab. Below are the charts included in the Team Productivity section. Total number of completions in the Project Total number of completions in the Project in the last 30 days Total number of completions for each Annotator Total number of completions submitted over time for each Annotator Average time spent by the Annotator in each task Total number of completions submitted over time Inter-Annotator Agreement (IAA) Starting from version 2.8.0, Inter Annotator Agreement(IAA) charts allow the comparison between annotations produced by Annotators, Reviewers, or Managers. Inter Annotator Agreement charts can be used by Annotators, Reviewers, and Managers for identifying contradictions or disagreements within the starred completions (Ground Truth). When multiple annotators work on same tasks, IAA charts are handy to measure how well the annotations created by different annotators align. IAA chart can also be used to identify outliers in the labeled data, or to compare manual annotations with model predictions. To access IAA charts, navigate on the third tab of the Analytics Dashboard of NER projects, called Inter-Annotator Agreement. Several charts should appear on the screen with a default selection of annotators to compare. The dropdown selections on top-left corner of each chart allow you to change annotators for comparison purposes. There is another dropdown to select the label type for filtering between NER labels and Assertion Status labels for projects containing both NER and Assertion Status entities. It is also possible to download the data generated for some chart in CSV format by clicking the download button just below the dropdown selectors. Note: Only the Submitted and starred (Ground Truth) completions are used to render these charts. The following blog post explains how your team can Reach Consensus Faster by Using IAA Charts in the Annotation Lab. Below are the charts included in the Inter-Annotator Agreement section. High-level IAA between annotators on all common tasks IAA between annotators for each label on all common tasks Comparison of annotations by annotator on each chunk Comparison of annotations by model and annotator (Ground Truth) on each chunk All chunks annotated by an annotator Frequency of labels on chunks annotated by an annotator Frequency of a label on chunks annotated by each annotator Download data used for charts CSV file for specific charts can be downloaded using the new download button which will call specific API endpoints: /api/projects/{project_name}/charts/{chart_type}/download_csv",
    "url": "/docs/en/alab/productivity",
    "relUrl": "/docs/en/alab/productivity"
  },
  "121": {
    "id": "121",
    "title": "Project Configuration",
    "content": "Annotation Lab currently supports multiple predefined project configurations. The most popular ones are Text Classification, Named Entity Recognition (NER) and Visual NER. Create a setup from scratch or customize a predefined one according to your needs. For customizing a predefined configuration, click on the corresponding link in the table above and then navigate to the Labeling configuration tab and manually edit or update it to contain the labels you want. After you finish editing the labels you want to define for your project click the “Save” button. Project templates We currently support multiple predefined project configurations. The most popular ones are Text Classification, Named Entity Recognition and Visual NER. Content Type The first step when creating a new project or customizing an existing one is to choose what content you need to annotate. Five content types are currently supported: Audio, HTML, Image, PDF and Text. For each content type a list of available templates is available. You can pick any one of those as a starting point in your project configuration. For customizing a predefined configuration, choose a Content Type and then a template from the list. Then navigate to the Customize Labels tab and manually edit/update the configuration to contain the labels you need. Users can add custom labels and choices in the project configuration from the Visual tab for both text and Visual NER projects. After you finish editing the labels click the “Save” button. Named Entity Recognition Named Entity Recognition (NER) refers to the identification and classification of entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. The Annotation Lab offers support for two types of labels: Simple labels for NER or assertion models; Binary relations for relation extraction models. Assertion Labels The syntax for defining an Assertion Status label is the same as for the NER labels, with an additional attribute - assertion which should be set to true (see example below). This convention is defined by Annotation Lab users which we exploited for identifying the labels to include in the training and prediction of Assertion Models. A simple Labeling Config with Assertion Status defined should look like the following: &lt;View&gt; &lt;Labels name=&quot;ner&quot; toName=&quot;text&quot;&gt; &lt;Label value=&quot;Medicine&quot; background=&quot;orange&quot; hotkey=&quot;_&quot;/&gt; &lt;Label value=&quot;Condition&quot; background=&quot;orange&quot; hotkey=&quot;_&quot;/&gt; &lt;Label value=&quot;Procedure&quot; background=&quot;green&quot; hotkey=&quot;8&quot;/&gt; &lt;Label value=&quot;Absent&quot; assertion=&quot;true&quot; background=&quot;red&quot; hotkey=&quot;Z&quot;/&gt; &lt;Label value=&quot;Past&quot; assertion=&quot;true&quot; background=&quot;red&quot; hotkey=&quot;X&quot;/&gt; &lt;/Labels&gt; &lt;View style=&quot;height: 250px; overflow: auto;&quot;&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;/&gt; &lt;/View&gt; &lt;/View&gt; NOTE: Notice assertion=”true” in Absent and Past labels, which marks each of those labels as Assertion Status Labels. Classification The choices tag is used as part of the classification projects to create a group of choices. It can be used for a single or multiple-class classification. According to the parameters used along with the choices tag, annotators can select single or multiple choices. Parameters The Choices tag supports the following parameters/attributes: Param Type Default Description required boolean false Verify if a choice is selected requiredMessage string   Show a message if the required validation fails choice single | multiple single Allow user to select single or multiple answer showInline boolean false Show choices in a single visual line perRegion boolean   Use this attribute to select an option for a specific region rather than the entire task &lt;!--text classification labeling config--&gt; &lt;View&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;/&gt; &lt;Choices name=&quot;surprise&quot; toName=&quot;text&quot; choice=&quot;single&quot; required=&#39;true&#39; requiredMessage=&#39;Please select choice&#39;&gt; &lt;Choice value=&quot;surprise&quot;/&gt; &lt;Choice value=&quot;sadness&quot;/&gt; &lt;Choice value=&quot;fear&quot;/&gt; &lt;Choice value=&quot;joy&quot;/&gt; &lt;/Choices&gt; &lt;/View&gt; When using the perRegion attribute, choices can be defined for each chunk annotation as shown below: Relation Extraction Annotation Lab also offers support for relation extraction. Relations are introduced by simply specifying their label in the project configuration. &lt;Relations&gt; &lt;Relation value=&quot;CancerSize&quot; /&gt; &lt;Relation value=&quot;CancerLocation&quot;/&gt; &lt;Relation value=&quot;MetastasisLocation&quot;/&gt; &lt;/Relations&gt; Constraints for relation labeling While annotating projects with Relations between Entities, defining constraints (the direction, the domain, the co-domain) of relations is important. Annotation Lab offers a way to define such constraints by editing the Project Configuration. The Project Owner or Project Managers can specify which Relation needs to be bound to which Labels and in which direction. This will hide some Relations in Labeling Page for NER Labels which will simplify the annotation process and will avoid the creation of any incorrect relations in the scope of the project. To define such constraint, add allowed attribute to the tag: L1&gt;L2 means Relation can be created in the direction from Label L1 to Label L2, but not the other way around L1&lt;&gt;L2 means Relation can be created in either direction between Label L1 to Label L2 If the allowed attribute is not present in the tag, there is no such restriction. Below you can find a sample Project Configuration with constraints for Relation Labels: &lt;View&gt; &lt;Header value=&quot;Sample Project Configuration for Relations Annotation&quot;/&gt; &lt;Relations&gt; &lt;Relation value=&quot;Was In&quot; allowed=&quot;PERSON&gt;LOC&quot;/&gt; &lt;Relation value=&quot;Has Function&quot; allowed=&quot;LOC&gt;EVENT,PERSON&gt;MEDICINE&quot;/&gt; &lt;Relation value=&quot;Involved In&quot; allowed=&quot;PERSON&lt;&gt;EVENT&quot;/&gt; &lt;Relation value=&quot;No Constraints&quot;/&gt; &lt;/Relations&gt; &lt;Labels name=&quot;label&quot; toName=&quot;text&quot;&gt; &lt;Label value=&quot;PERSON&quot;/&gt; &lt;Label value=&quot;EVENT&quot;/&gt; &lt;Label value=&quot;MEDICINE&quot;/&gt; &lt;Label value=&quot;LOC&quot;/&gt; &lt;/Labels&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;/&gt; &lt;/View&gt;",
    "url": "/docs/en/alab/project_configuration",
    "relUrl": "/docs/en/alab/project_configuration"
  },
  "122": {
    "id": "122",
    "title": "Project Creation",
    "content": "New project Every project in Annotation Lab should have the following information: a unique name and a short description; a team of annotators, reviewers and a manager who will collaborate on the project; a configuration which specifies the type of annotations that will be created. You can create a new project using the dedicated wizard which will guide users through each step of the project creation and configuration process. Those steps are illustrated below. Project Description To open the project creation wizard click on the + New Project button on the Projects Dashboard, then provide the following information: a unique name or title; a sampling type which will define how the tasks assigned to annotators/reviewers will be served - randomly or sequentially; a short description that helps users quickly grasp the main purpose of the project; instructions for annotators or Annotation Guidelines which will help annotators and reviewers generate high quality annotations. NOTE: Reserved words cannot be used as project names. The use of keywords like count, permission, or name as project names generated UI glitches. To avoid such issues, these keywords are no longer accepted as project names. Adding Team Members When working in teams, projects can be shared with other team members. The user who creates a project is called a Project Owner. He/she has complete visibility and ownership of the project for its entire lifecycle. If the Project Owner is removed from the user database, then all his/her projects are transfered to a new project owner. The Project Owner can edit the project configuration, can import/export tasks, can create a project team that will work on his project and can access project analytics. When defining the project team, a project owner has access to three distinct roles: Annotator, Reviewer, and Manager. These are very useful for most of the workflows that our users follow. An Annotator is able to see the tasks which have been assigned to him or her and can create annotations on the documents. The Reviewer is able to see the work of the annotators and approve it or reject in case he finds issues that need to be solved. The Manager is able to see the work of the Annotators and of the Reviewers and he can assign tasks to team members. This is useful for eliminating work overlap and for a better management of the work load. To add a user to your project team, select your Project, then from the left side menu access the Setup option and then the Team option. On the Add Team Member page that opens, start typing the name of a user in the available text box. This will populate a list of available users having the username start with the characters you typed. From the dropdown select the user you want to add to your team. Select a role for the user and click on the Add to team button. In the Add Team Member page users can add/remove/update the team members even in the case of a large number of members. The team members are displayed in a tabular view. Each member has a priority assigned to them for CONLL export which can be changed by dragging users across the list. NOTE: The priority assigned for users in the Add Team Member page is taken into account by the Model Training script for differentiating among the available ground truth completions (when more than one is available for a task) in view of choosing the higer priority completion which will be used for model training. Learn more here. Project Configuration The Project Configuration itself is a multi-step process. The wizard will guide users through each step while providing useful information and hints for all available options. Clone You can create a copy of a project, by using the Clone option. The option to clone the project is also listed in the kebab menu of each project. The cloned project is differentiated as it contains cloned suffix in its project name. Export Projects can be exported. The option to export a project is listed in the kebab menu of each project. All project-related items such as tasks, project configuration, project members, task assignments, and comments are included in the export file. NOTE: Project export does not contain the model trained in the project as models are independent and not attached to a particular project. Import A project can be imported by uploading the project zip archive in the upload dialog box. When the project is imported back to Annotation Lab, all elements of the original project configuration will be included in the new copy. Project Grouping As the number of projects can grow significantly over time, for an easier management and organization of those, Annotation Lab allows project grouping. As such, a project owner can assign a group to one or several of his/her projects. Each group can be assigned a color which will be used to highlight projects included in that group. Once a project is assigned to a group, the group name will appear as a tag on the project tile. At any time a project can be remove from one group and added to another group. The list of visible projects can be filtered based on group name, or using the search functionality which applies to both group name and project name. Projects can be organized in custom groups, and each project card will inherit the group color so that the users can visually distinguish the projects easily in a large cluster of projects. The new color picker for the group is user-friendly and customizable.",
    "url": "/docs/en/alab/project_creation",
    "relUrl": "/docs/en/alab/project_creation"
  },
  "123": {
    "id": "123",
    "title": "Dashboard",
    "content": "When logging in to the Annotation Lab, the user sees the main Projects Dashboard. For each project, details like description, task counts, assigned groups, team members, etc. are available on the main dashboard so users can quickly identify the projects they need to work on, without navigating to the Project Details page. Projects can be filtered based on the creator: My Projects, created by the current user or Shared With Me, created by other users and shared with the current one. All Projects option combines the list of the projects created by the current user and those shared by others. The list of projects can be sorted according to the name of the project. Also, projects can be sorted in ascending or descending order according to the creation date. The filters associated with the Projects Dashboard are clear, simple, and precise to make the users more productive and efficient while working with a large number of projects. Searching features are also available and help users identify projects based on their name.",
    "url": "/docs/en/alab/project_dashboard",
    "relUrl": "/docs/en/alab/project_dashboard"
  },
  "124": {
    "id": "124",
    "title": "Prompts",
    "content": "NLP Lab offers support for prompt engineering. On the Prompts page, from the resources HUB, users can easily discover and explore the existing prompts or create new prompts for identifying entities or relations. Currently, NLP Lab supports prompts for Healthcare, Finance, and Legal domains applied using pre-trained question-answering language models published on the NLP Models Hub and available to download in one click. The main advantage behind the use of prompts in entity or relation recognition is the ease of definition. Non-technical domain experts can easily create prompts, test and edit them on the Playground on custom text snippets and, when ready, deploy them for pre-annotation as part of larger NLP projects. Together with rules, prompts are very handy in situations where no pre-trained models exist, for the target entities and domains. With rules and prompts the annotators never start their projects from scratch but can capitalize on the power of zero-shot models and rules to help them pre-annotate the simple entities and relations and speed up the annotation process. As such, the NLP Lab ensures fewer manual annotations are required from any given task. Creating NER Prompts NER prompts, can be used to identify entities in natural language text documents. Those can be created based on healthcare, finance, and legal zero-shot models selectable from the “Domain” dropdown. For one prompt, the user adds one or more questions for which the answer represents the target entity to annotate. Creating Relation Prompts Prompts can also be used to identify relations between entities for healthcare, finance, and legal domains. The domain-specific zero-shot model to use for detecting relation can be selected from the “Domain” dropdown. The relation prompts are defined by a pair of entities related by a predicate. The entities can be selected from the available dropdowns listing all entities available in the current NLP Lab (included in available NER models, prompts or rules) for the specified domain. Mix and Match models, rules, and prompts The project configuration page was simplified by grouping into one page all available resources that can be reused for pre-annotation: models, rules, and prompts. Users can easily mix and match the relevant resources and add them to their configuration. Note: One project configuration can only reuse the prompts defined by one single zero-shot model. Prompts created based on multiple zero-shot models (e.g. finance or legal or healthcare) cannot be mixed into the same project because of high resource consumption. Furthermore, all prompts require a license with a scope that matches the domain of the prompt. Zero-Shot Models available in the NLP Models Hub NLP Models Hub now lists the newly released zero-shot models that are used to define prompts. These models need to be downloaded to NLP Lab instance before prompts can be created. A valid license must be available for the models to be downloaded to NLP Lab.",
    "url": "/docs/en/alab/prompts",
    "relUrl": "/docs/en/alab/prompts"
  },
  "125": {
    "id": "125",
    "title": "Vaccines & Public Health - Biomedical NLP Demos & Notebooks",
    "content": "",
    "url": "/public_health",
    "relUrl": "/public_health"
  },
  "126": {
    "id": "126",
    "title": "Question Answering - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/question_answering",
    "relUrl": "/question_answering"
  },
  "127": {
    "id": "127",
    "title": "NLP Lab (Annotation Lab)",
    "content": "The Free No-Code NLP Lab A highly efficient End-to-End No Code NLP platform for all enterprise teams that need to: Annotate Text &amp; Images Train &amp; Tune NLP Models Speedup with AI Assisted Annotation Test for Responsible AI Manage Projects &amp; Teams Enterprise Security &amp; Privacy All that without writing a line of code! Install on AWS Install on Azure Productivity Never start from scratch Keep Annotators in the Zone Reach agreement quickly Auto NLP Active learning Deliver an accurate model, not just labels Built for High Compliance Enterprise Environments Teamwork Projects &amp; Teams Workflows Security Analytics Resources General tutorials Annotation best practices Tips and tricks Quick Intro Annotation Lab evolved to become the NLP Lab. NLP Lab is a Free End-to-End No-Code platform for document labeling and AI/ML model training. It enables domain experts (e.g. nurses, doctors, lawyers, accountants, investors, etc.) to extract meaningful facts from text documents, images or PDFs and train models that will automatically predict those facts on new documents. This is done by using state-of-the-art Spark NLP pre-trained models or by tuning models to better handle specific use cases. Based on an auto-scaling architecture powered by Kubernetes, it can scale to many teams and projects. Enterprise-grade security is provided for free including support for air-gap environments, zero data sharing, role-based access, full audit trails, MFA, and identity provider integrations. It allows powerful experiments for model training and finetuning, model testing, and model deployment as API endpoints. There is no limitation on the number of users, projects, tasks, models, or trainings that can be run with this subscription. Healthcare and Visual features are available via BYOL. Included Features: Annotation support for Text, Image, Audio, Video and HTML content; High productivity annotation UI with keyboard shortcuts and pre-annotations; Support for text annotation in 250+ languages; Out-of-the-box support for the following NLP tasks: Classification, Named Entity Recognition, Assertion Status, and Relation Extraction; Support for projects and teams: 30+ project templates; unlimited projects and users, project import, export and cloning, project grouping; Task assignment, tagging, and comments; duplicate tasks identification; task searching and filtering; Consensus analysis and Inter Annotator Agreement charts; Performance dashboards; Enterprise-level security and privacy: role-based access control, role-based views, annotation versioning, full audit trail, Single Sign on; AI-Assisted Annotation: never start from scratch but reuse existing models to pre-annotate tasks with the latest Spark NLP models for classification, NER, assertion status, and relation detection; Full Models Hub integration: you can explore available models and embeddings, download them with the click of a button and reuse those in your project configuration. Train Classification, NER, and Assertion Status models: use default parameters or easily tune them on the UI for different experiments; Active Learning automatically trains new versions of your models once new annotations are available; API access to all features for easy integration into custom data analysis pipelines;",
    "url": "/docs/en/alab/quickstart",
    "relUrl": "/docs/en/alab/quickstart"
  },
  "128": {
    "id": "128",
    "title": "Quick Start",
    "content": "Requirements &amp; Setup Spark NLP is built on top of Apache Spark 3.x. For using Spark NLP you need: Java 8 and 11 Apache Spark 3.3.x, 3.2.x, 3.1.x, 3.0.x It is recommended to have basic knowledge of the framework and a working environment before using Spark NLP. Please refer to Spark documentation to get started with Spark. Install Spark NLP in Python Scala and Java Databricks EMR Join our Slack channel Join our channel, to ask for help and share your feedback. Developers and users can help each other getting started here. Spark NLP Slack Spark NLP in Action Make sure to check out our demos built by Streamlit to showcase Spark NLP in action: Spark NLP Demo Spark NLP Examples If you prefer learning by example, check this repository: Spark NLP Examples It is full of fresh examples and even a docker container if you want to skip installation. Below, you can follow into a more theoretical and thorough quick start guide. Where to go next If you need more detailed information about how to install Spark NLP you can check the Installation page Detailed information about Spark NLP concepts, annotators and more may be found HERE",
    "url": "/docs/en/quickstart",
    "relUrl": "/docs/en/quickstart"
  },
  "129": {
    "id": "129",
    "title": "Radiology - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/radiology",
    "relUrl": "/radiology"
  },
  "130": {
    "id": "130",
    "title": "Recognize Entities - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/recognize_entitie",
    "relUrl": "/recognize_entitie"
  },
  "131": {
    "id": "131",
    "title": "Release Notes",
    "content": "0.7.1 Fields Details Name NLP Server Version 0.7.1 Type Patch Release Date 2022-06-17 Overview We are excited to release NLP Server v0.7.1! We are committed to continuously improve the experience for our users and make our product reliable and easy to use. This release focuses on solving a few bugs and improving the stability of the NLP Server. Key Information For smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications. NLP Server is available on both AWS and Azure marketplaces. Bug Fixes Issue when running NER ONTO spell. Issue when running dep spell. Since the spell was broken it is temporarily blacklisted. Document normalizer included the HTML, XML tags to the output even after normalization. Issue when running language translation spells &lt;from_lang&gt;.translate_to.&lt;to_lang&gt;. Upon cancelation of custom model uploading job exception was seen in the logs. Some few UI related issues and abnormalities during operation. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes"
  },
  "132": {
    "id": "132",
    "title": "Release Notes",
    "content": "4.7.1 Release date: 22-02-2023 The latest version of NLP Lab, version 4.7.1, brings several enhancements that are worth highlighting. One of the most notable improvements is in relation prompts. NLP Lab now offers support for combining NER models, prompts and rules when defining relation prompts. The playground feature in NLP Lab has also received some noteworthy upgrades in version 4.7.1. The “playground” environment was initially added to facilitate experiments with different NLP models, tweak prompts and rules, and explore the potential of language models in a safe, sandboxed environment. The improvements made to the playground in this version are expected to enhance the overall user experience, and to make the environment faster and more responsive. In addition to these improvements, the latest version of NLP Lab has extended support for importing large task archives. This means that users can now work with bigger datasets more efficiently, which will undoubtedly save them time and effort. Below are the specifics of the additions included in this release: Improvements in Prompts Build Relation Prompts using NER Models, Prompts and Rules In previous version, relation prompts could be defined based on NER models and rules. In this release, NLP Lab allows for NER prompts to be reused when defining relation prompts. To include a NER prompt within a relation prompt, users need to navigate to the Questions section of the Relation Prompt creation page and search for the prompt to reuse. Once the NER prompt has been selected, users can start defining the question patterns. For example, users could create prompts that identify the relationship between people and the organizations they work for, or prompts that identify the relationship between a place and its geographic coordinates. The ability to incorporate NER prompts into relation prompts is a significant advancement in prompts engineering, and it opens up new possibilities for more sophisticated and accurate natural language processing. Improvements in Playground Direct Navigation to Active Playground Sessions Navigating between multiple projects to and from the playground experiments can be necessary, especially when you want to revisit a previously edited prompt or rule. This is why NLP Lab Playground now allow users to navigate to any active Playground session without having to redeploy the server. This feature enables users to check how their resources (models, rules and prompts) behave at project level, compare the preannotation results with ground truth, and quickly get back to experiments for modifying prompts or rules without losing progress or spending time on new deployments. This feature makes experimenting with NLP prompts and rules in a playground more efficient, streamlined, and productive. Automatic Deployment of Updated Rules/Prompts Another benefit of experimenting with NLP prompts and rules in the playground is the immediate feedback that you receive. When you make changes to the parameters of your rules or to the questions in your prompts, the updates are deployed instantly. Manually deploying the server is not necessary any more for changes made to Rules/Prompts to be reflected in the preannotation results. Once the changes are saved, by simply clicking on the Test button, updated results are presented. This allows you to experiment with a range of variables and see how each one affects the correctness and completeness of the results. The real-time feedback and immediate deployment of changes in the playground make it a powerful tool for pushing the boundaries of what is possible with language processing. Playground Server Destroyed after 5 Minutes of Inactivity When active, the NLP playground consumes resources from your server. For this reason, NLP Lab defines an idle time limit of 5 minutes after which the playground is automatically destroyed. This is done to ensure that the server resources are not being wasted on idle sessions. When the server is destroyed, a message is displayed, so users are aware that the session has ended. Users can view information regarding the reason for the Playground’s termination, and have the option to restart by pressing the Restart button. Playground Servers use Light Pipelines The replacement of regular preannotation pipelines with Light Pipelines has a significant impact on the performance of the NLP playground. Light pipelines allow for faster initial deployment, quicker pipeline update and fast processing of text data, resulting in overall quicker results in the UI. Direct Access to Model Details Page on the Playground Another useful feature of NLP Lab Playground is the ability to quickly and easily access information on the models being used. This information can be invaluable for users who are trying to gain a deeper understanding of the model’s inner workings and capabilities. In particular, by click on the model’s name it is now possible to navigate to the NLP Models hub page. This page provides users with additional details about the model, including its training data, architecture, and performance metrics. By exploring this information, users can gain a better understanding of the model’s strengths and weaknesses, and use this knowledge to make more informed decisions on how good the model is for the data they need to annotate. Improvements in Task Import Support for Large Document Import One of the challenges when working on big annotation projects is dealing with large size tasks, especially when uploading them to the platform. This is particularly problematic for files/archives larger than 20 MB, which can often lead to timeouts and failed uploads. To address this issue, NLP Lab has implemented chunk file uploading on the task import page. Chunk file uploading is a method that breaks large files into smaller, more manageable chunks. This process makes the uploading of large files smoother and more reliable, as it reduces the risk of timeouts and failed uploads. This is especially important for NLP practitioners who work with large datasets, as it allows them to upload and process their data more quickly and effectively. Versions 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/release_notes",
    "relUrl": "/docs/en/alab/release_notes"
  },
  "133": {
    "id": "133",
    "title": "Spark NLP release notes",
    "content": "For all official releases please visit GitHub release notes",
    "url": "/docs/en/release_notes",
    "relUrl": "/docs/en/release_notes"
  },
  "134": {
    "id": "134",
    "title": "NLP Server release notes 0.4.0",
    "content": "0.4.0 Highlights This version of NLP Server offers support for licensed models and annotators. Users can now upload a Spark NLP for Healthcare license file and get access to a wide range of additional annotators and transformers. A valid license key also gives access to more than 400 state-of-the-art healthcare models. Those can be used via easy to learn NLU spells or via API calls. NLP Server now supports better handling of large amounts of data to quickly analyze via UI by offering support for uploading CSV files. Support for floating licenses. Users can now take advantage of the floating license flexibility and use those inside of the NLP Server. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_4_0",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_4_0"
  },
  "135": {
    "id": "135",
    "title": "NLP Server release notes 0.5.0",
    "content": "0.5.0 Highlights Support for easy license import from my.johnsnowlabs.com. Visualize annotation results with Spark NLP Display. Examples of results obtained using popular spells on sample texts have been added to the UI. Performance improvement when previewing the annotations. Support for 22 new models for 23 languages including various African and Indian languages as well as Medical Spanish models powered by NLU 3.4.1 Various bug fixes Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_5_0",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_5_0"
  },
  "136": {
    "id": "136",
    "title": "NLP Server release notes 0.6.0",
    "content": "0.6.0 Fields Details Name NLP Server Version 0.6.0 Type Minor Release Date 2022-04-06 Overview We are excited to release NLP Server v0.6.0! This new release comes with exciting new features and improvements that extend and enhance the capabilities of the NLP Server. This release comes with the ability to share the models with the Annotation Lab. This will enable easy access to custom models uploaded to or trained with the Annotation Lab or to pre-trained models downloaded to Annotation Lab from the NLP Models Hub. As such the NLP Server becomes an easy and quick tool for testing our trained models locally on your own infrastructure with zero data sharing. Another important feature we have introduced is the support for Spark OCR spells. Now we can upload images, PDFs, or other documents to the NLP Server and run OCR spells on top of it. The results of the processed documents are also available for export. The release also includes a few improvements to the existing features and some bug fixes. Key Information For a smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications NLP Server is now available on Azure Marketplace as well as on AWS marketplace. Major Features and Improvements Support for custom models trained with the Annotation Lab Models trained with the Annotation Lab are now available as “custom” spells in the NLP Server. Similarly, models manually uploaded to the Annotation Lab, or downloaded from the NLP Models Hub are also made available for use in the NLP Server. This is only supported in a docker setup at present when both tools are deployed in the same machine. Support for Spark OCR spells OCR spells are now supported by NLP Server in the presence of a valid OCR license. Users can upload an image, PDF, or other supported document format and run the OCR spells on it. The processed results are also available for download as a text document. It is also possible to upload multiple files at once for OCR operation. These files can be images, PDFs, word documents, or a zipped file. Other Improvements Now users can chain multiple spells together to analyze the input data. The order of operation on the input data will be in the sequence of the spell chain from left to right. NLP Server now supports more than 5000+ models in 250+ languages powered by NLU. Bug Fixes Not found error seen when running predictions using certain spells. The prediction job runs in an infinite loop when using certain spells. For input data having new line characters JSON exception was seen when processing the output from NLU. Incorrect license information was seen in the license popup. Spell field cleared abruptly when typing the spells. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_6_0",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_6_0"
  },
  "137": {
    "id": "137",
    "title": "NLP Server release notes 0.6.1",
    "content": "0.6.1 Fields Details Name NLP Server Version 0.6.1 Type Patch Release Date 2022-05-06 Overview We are excited to release NLP Server v0.6.1! We are continually committed towards improving the experience for our users and making our product reliable and easy to use. This release focuses on improving the stability of the NLP Server and cleaning up some annoying bugs. To enhance the user experience, the product now provides interactive and informative responses to the users. The improvements and bug fixes are mentioned in their respective sections below. Key Information For smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications. NLP Server is available on both AWS and Azure marketplace. Improvements Support for new models for Lemmatizers, Parts of Speech Taggers, and Word2Vec Embeddings for over 66 languages, with 20 languages being covered for the first time by NLP Server, including ancient and exotic languages like Ancient Greek, Old Russian, Old French and much more. Bug Fixes The prediction job runs in an infinite loop when using certain spells. Now after 3 retries it aborts the process and informs users appropriately. Issue when running lang spell for language classification. The prediction job runs in an infinite loop when incorrect data format is selected for a given input data. The API request for processing spell didn’t work when format parameter was not provided. Now it uses a default value in such case. Users were unable to login to their MYJSL account from NLP Server. Proper response when there is issue in internet connectivity when running spell. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_6_1",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_6_1"
  },
  "138": {
    "id": "138",
    "title": "NLP Server release notes 0.7.0",
    "content": "0.7.0 Fields Details Name NLP Server Version 0.7.0 Type Minor Release Date 2022-06-07 Overview We are excited to release NLP Server v0.7.0! This new release comes with an exciting new feature of table extraction from various file formats. Table extraction feature enables extracting tabular content from the document. This extracted content is available as JSON and hence can again be processed with different spells for further predictions. The various supported files formats are documents (pdf, doc, docx), slides (ppt, pptx), and zipped content containing the mentioned formats. The improvements are mentioned in their respective sections below. Key Information For smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications. NLP Server is available on both AWS and Azure marketplace. Major Features and Improvements Support for Table extraction NLP Server now supports extracting tabular content from various file types. The currently supported file types are documents (pdf, doc, docx), slides (ppt, pptx), and zipped content containing any of the mentioned formats. These extracted contents are available as JSON output from both UI and API that can easily be converted to suitable Data Frames (e.g., pandas DF) for further processing. The output of the table extraction process can also be viewed in the NLP Server UI as a flat table. Currently, if multiple tables are extracted from the document, then only one of the tables selected randomly will be shown as a preview in the UI. However, upon downloading all the extracted tables are exported in separate JSON dumps combined in a single zipped file. For this version, the table extraction on PDF files is successful only if the PDF contains necessary metadata about the table content. Other Improvements Support for over 600 new models, and over 75 new languages including ancient, dead, and extinct languages. Transformer-based embeddings and token classifiers are powered by state-of-the-art CamemBertEmbeddings and DeBertaForTokenClassification based architectures. Added Portuguese De-identification models, NER models for Gene detection, and RxNorm Sentence resolution model for mapping and extracting pharmaceutical actions as well as treatments. JSON payload is now supported in the request body when using create result API. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_7_0",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_7_0"
  },
  "139": {
    "id": "139",
    "title": "NLP Server release notes 0.7.1",
    "content": "0.7.1 Fields Details Name NLP Server Version 0.7.1 Type Patch Release Date 2022-06-17 Overview We are excited to release NLP Server v0.7.1! We are committed to continuously improve the experience for our users and make our product reliable and easy to use. This release focuses on solving a few bugs and improving the stability of the NLP Server. Key Information For smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications. NLP Server is available on both AWS and Azure marketplaces. Bug Fixes Issue when running NER ONTO spell. Issue when running dep spell. Since the spell was broken it is temporarily blacklisted. Document normalizer included the HTML, XML tags to the output even after normalization. Issue when running language translation spells &lt;from_lang&gt;.translate_to.&lt;to_lang&gt;. Upon cancelation of custom model uploading job exception was seen in the logs. Some few UI related issues and abnormalities during operation. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_7_1",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_7_1"
  },
  "140": {
    "id": "140",
    "title": "Spark NLP release notes 1.0.0",
    "content": "1.0.0 Release date: 12-02-2020 Overview Spark NLP OCR functionality was reimplemented as set of Spark ML transformers and moved to separate Spark OCR library. New Features Added extraction coordinates of each symbol in ImageToText Added ImageDrawRegions transformer Added ImageToPdf transformer Added ImageMorphologyOpening transformer Added ImageRemoveObjects transformer Added ImageAdaptiveThresholding transformer Enhancements Reimplement main functionality as Spark ML transformers Moved DrawRectangle functionality to PdfDrawRegions transformer Added ‘start’ function with support SparkMonitor initialization Moved PositionFinder to Spark OCR Bugfixes Fixed bug with transforming complex pdf to image Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_0_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_0_0"
  },
  "141": {
    "id": "141",
    "title": "Spark NLP release notes 1.10.0",
    "content": "1.10.0 Release date: 20-01-2021 Overview Support Microsoft Docx documents. New Features Added DocToText transformer for extract text from DOCX documents. Added DocToTextTable transformer for extract table data from DOCX documents. Added DocToPdf transformer for convert DOCX documents to PDF format. Bugfixes Fixed issue with loading model data on some cluster configurations Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_10_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_10_0"
  },
  "142": {
    "id": "142",
    "title": "Spark NLP release notes 1.11.0",
    "content": "1.11.0 Release date: 25-02-2021 Overview Support German, French, Spanish and Russian languages. Improving PositionsFinder and ImageToText for better support de-identification. New Features Loading model data from S3 in ImageToText. Added support German, French, Spanish, Russian languages in ImageToText. Added different OCR model types: Base, Best, Fast in ImageToText. Enhancements Added spaces symbols to the output positions in the ImageToText transformer. Eliminate python-levensthein from dependencies for simplify installation. Bugfixes Fixed issue with extracting coordinates in in ImageToText. Fixed loading model data on cluster in yarn mode. New notebooks Languages Support Image DeIdentification Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_11_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_11_0"
  },
  "143": {
    "id": "143",
    "title": "Spark NLP release notes 1.1.0",
    "content": "1.1.0 Release date: 03-03-2020 Overview This release contains improvements for preprocessing image before run OCR and added possibility to store results to PDF for keep original formatting. New Features Added auto calculation maximum size of objects for removing in ImageRemoveObjects. This improvement avoids to remove . and affect symbols with dots (i, !, ?). Added minSizeFont param to ImageRemoveObjects transformer for activate this functional. Added ocrParams parameter to ImageToText transformer for set any ocr params. Added extraction font size in ImageToText Added TextToPdf transformer for render text with positions to pdf file. Enhancements Added setting resolution in ImageToText. And added ignoreResolution param with default true value to ImageToText transformer for back compatibility. Added parsing resolution from image metadata in BinaryToImage transformer. Added storing resolution in PdfToImage transformer. Added resolution field to Image schema. Updated ‘start’ function for set ‘PYSPARK_PYTHON’ env variable. Improve auto-scaling/skew correction: improved access to images values removing unnecessary copies of images adding more test cases improving auto-correlation in auto-scaling. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_1_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_1_0"
  },
  "144": {
    "id": "144",
    "title": "Spark NLP release notes 1.1.1",
    "content": "1.1.1 Release date: 06-03-2020 Overview Integration with license server. Enhancements Added license validation. License can be set in following waysq: Environment variable. Set variable ‘JSL_OCR_LICENSE’. System property. Set property ‘jsl.sparkocr.settings.license’. Application.conf file. Set property ‘jsl.sparkocr.settings.license’. Added auto renew license using jsl license server. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_1_1",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_1_1"
  },
  "145": {
    "id": "145",
    "title": "Spark NLP release notes 1.1.2",
    "content": "1.1.2 Release date: 09-03-2020 Overview Minor improvements and fixes Enhancements Improved messages during license validation Bugfixes Fixed dependencies issue Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_1_2",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_1_2"
  },
  "146": {
    "id": "146",
    "title": "Spark NLP release notes 1.2.0",
    "content": "1.2.0 Release date: 08-04-2020 Overview Improved support Databricks and processing selectable pdfs. Enhancements Adapted Spark OCR for run on Databricks. Added rewriting positions in ImageToText when run together with PdfToText. Added ‘positionsCol’ param to ImageToText. Improved support Spark NLP. Changed start function. New Features Added showImage implicit to Dataframe for display images in Scala Databricks notebooks. Added display_images function for display images in Python Databricks notebooks. Added propagation selectable pdf file in TextToPdf. Added ‘inputContent’ param to ‘TextToPdf’. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_2_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_2_0"
  },
  "147": {
    "id": "147",
    "title": "Spark NLP release notes 1.3.0",
    "content": "1.3.0 Release date: 22-05-2020 Overview New functionality for de-identification problem. Enhancements Renamed TesseractOCR to ImageToText. Simplified installation. Added check license from SPARK_NLP_LICENSE env varibale. New Features Support storing for binaryFormat. Added support storing Image and PDF files. Support selectable pdf for TextToPdf transformer. Added UpdateTextPosition transformer. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_3_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_3_0"
  },
  "148": {
    "id": "148",
    "title": "Spark NLP release notes 1.4.0",
    "content": "1.4.0 Release date: 23-06-2020 Overview Added support Dicom format and improved support image morphological operations. Enhancements Updated start function. Improved support Spark NLP internal. ImageMorphologyOpening and ImageErosion are removed. Improved existing transformers for support de-identification Dicom documents. Added possibility to draw filled rectangles to ImageDrawRegions. New Features Support reading and writing Dicom documents. Added ImageMorphologyOperation transformer which support: erosion, dilation, opening and closing operations. Bugfixes Fixed issue in ImageToText related to extraction coordinates. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_4_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_4_0"
  },
  "149": {
    "id": "149",
    "title": "Spark NLP release notes 1.5.0",
    "content": "1.5.0 Release date: 22-07-2020 Overview FoundationOne report parsing support. Enhancements Optimized memory usage during image processing New Features Added FoundationOneReportParser which support parsing patient info, genomic and biomarker findings. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_5_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_5_0"
  },
  "150": {
    "id": "150",
    "title": "Spark NLP release notes 1.6.0",
    "content": "1.6.0 Release date: 05-09-2020 Overview Support parsing data from tables for selectable PDFs. New Features Added PdfToTextTable transformer for extract tables from Pdf document per each page. Added ImageCropper transformer for crop images. Added ImageBrandsToText transformer for detect text in defined areas. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_6_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_6_0"
  },
  "151": {
    "id": "151",
    "title": "Spark NLP release notes 1.7.0",
    "content": "1.7.0 Release date: 22-09-2020 Overview Support Spark 2.3.3. Bugfixes Restored read JPEG2000 image Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_7_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_7_0"
  },
  "152": {
    "id": "152",
    "title": "Spark NLP release notes 1.8.0",
    "content": "1.8.0 Release date: 20-11-2020 Overview Optimisation performance for processing multipage PDF documents. Support up to 10k pages per document. New Features Added ImageAdaptiveBinarizer Scala transformer with support: Gaussian local thresholding Otsu thresholding Sauvola local thresholding Added possibility to split pdf to small documents for optimize processing in PdfToImage. Enhancements Added applying binarization in PdfToImage for optimize memory usage. Added pdfCoordinates param to the ImageToText transformer. Added ‘total_pages’ field to the PdfToImage transformer. Added different splitting strategies to the PdfToImage transformer. Simplified paging PdfToImage when run it with splitting to small PDF. Added params to the PdfToText for disable extra functionality. Added master_url param to the python start function. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_8_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_8_0"
  },
  "153": {
    "id": "153",
    "title": "Spark NLP release notes 1.9.0",
    "content": "1.9.0 Release date: 11-12-2020 Overview Extension of FoundationOne report parser and support HOCR output format. New Features Added ImageToHocr transformer for recognize text from image and store it to HOCR format. Added parsing gene lists from ‘Appendix’ in FoundationOneReportParser transformer. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_1_9_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_1_9_0"
  },
  "154": {
    "id": "154",
    "title": "Annotation Lab Release Notes 2.0.1",
    "content": "2.0.1 Highlights Inter-Annotation Agreement Charts. To get a measure of how well multiple annotators can make the same annotation decision for a certain category, we are shipping seven different charts. To see these charts users can click on the third tab “Inter-Annotator Agreement” of the Analytics Dashboard of NER projects. There are dropdown boxes to change annotators for comparison purposes. It is also possible to download the data of some charts in CSV format by clicking the download button present at the bottom right corner of each of them. Updated CONLL Export. In previous versions, numerous files were created based on Tasks and Completions. There were issues in the Header and no sentences were detected. Also, some punctuations were not correctly exported or were missing. The new CONLL export implementation results in a single file and fixes all the above issues. As in previous versions, if only Starred completions are needed in the exported file, users can select the “Only ground truth” checkbox. Search tasks by label. Now, it is possible to list the tasks based on some annotation criteria. Examples of supported queries: “label: ABC”, “label: ABC=DEF”, “choice: Mychoice”, “label: ABC=DEF”. Validation of labels and models is done beforehand. An error message is shown if the label is incompatible with models. Transfer Learning support for Training Models. Now its is possible to continue model training from an already available model. If a Medical NER model is present in the system, the project owner or manager can go to Advanced Options settings of the Training section in the Setup Page and choose it to Fine Tune the model. When Fine Tuning is enabled, the embeddings that were used to train the model need to be present in the system. If present, it will be automatically selected, otherwise users need to go to the Models Hub page and download or upload it. Training Community Models without the need of License. In previous versions, Annotation Lab didn’t allow training without the presence of Spark NLP for Healthcare license. But now the training with community embeddings is allowed even without the presence of Valid license. Support for custom training scripts. If users want to change the default Training script present within the Annotation Lab, they can upload their own training pipeline. In the Training section of the Project Setup Page, only admin users can upload the training scripts. At the moment we are supporting the NER custom training script only. Users can now see a proper message on the Modelshub page when annotationlab is not connected to the internet (AWS S3 to be more precise). This happens in air-gapped environments or some issues in the enterprise network. Users now have the option to download the trained models from the Models Hub page. The download option is available under the overflow menu of each Model on the “Available Models” tab. Training Live Logs are improved in terms of content and readability. Not all Embeddings present in the Models Hub are supported by NER and Assertion Status Training. These are now properly validated from the UI. Conflict when trying to use deleted embeddings. The existence of the embeddings in training as well as in deployment is ensured and a readable message is shown to users. Support for adding custom CA certificate chain. Follow the instructions described in instruction.md file present in the installation artifact. Bug fixes When multiple paged OCR file was imported using Spark OCR, the task created did not have pagination. Due to a bug in the Assertion Status script, the training was not working at all. Any AdminUser could delete the main “admin” user as well as itself. We have added proper validation to avoid such situations. Read more Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_2_0_1",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_2_0_1"
  },
  "155": {
    "id": "155",
    "title": "Annotation Lab Release Notes 2.1.0",
    "content": "2.1.0 Highlights A new project configuration “Visual NER Labeling” was added, which provides the skeleton for text annotation on scanned images. Project Owners or Project Manager can train open-source models too. The UI components and navigation of Annotation Lab - as a SPA - continues to improve its performance. The application has an increased performance (security and bug fixes, general optimizations). More models &amp; embeddings included in the Annotation Lab image used for deployments. This should reduce the burden for system admins during the installation in air-gapped or enterprise environments. Easier way to add relations. Project Owners and Managers can see the proper status of tasks, taking into account their own completions. Security Fixes. We understand and take the security issues as the highest priority. On every release, we run our artifacts and images through series of security testings (Static Code analysis, PenTest, Images Vulnerabilities Test, AWS AMI Scan Test). This version resolves a few critical issues that were recently identified in Python Docker image we use. We have upgraded it to a higher version. Along with this upgrade, we have also refactored our codebase to pass our standard Static Code Analysis. Bug fixes An issue with using Uploaded models was fixed so any uploaded models can be loaded in Project Config and used for preannotation. Issues related to error messages when uploading a valid Spark OCR license and when trying to train NER models while Spark OCR license was expired are now fixed. The issue with exporting annotations in COCO format for image projects was fixed. Project Owners and Managers should be able to export COCO format which also includes images used for annotations. The bug reports related to unexpected scrolling of the Labeling page, issues in Swagger documentation, and typos in some hover texts are now fixed. Read more Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_2_1_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_2_1_0"
  },
  "156": {
    "id": "156",
    "title": "Annotation Lab Release Notes 2.2.20",
    "content": "2.2.2 Highlights Support for pretrained Relation Extraction and Assertion Status models. A valid Spark NLP for HealthCare License is needed to download pretrained models via the Models Hub page. After download, they can be added to the Project Config and used for preannotations. Support for uploading local images. Until this version, only images from remote URLs could be uploaded for Image projects. With this version the Annotation Lab supports uploading images from you local storage/computer. It is possible to either import one image or multiple images by zipping them together. The maximum image file size is 16 MB. If you need to upload files exceding the default configuration, please contact your system administrator who will change the limit size in the installation artifact and run the upgrade script. Improved support for Visual NER projects. A sample task can be imported from the Import page by clicking the “Add Sample Task” button. Also default config for the Visual NER project contains zoom feature which supports maximum possible width for low resolution images when zooming. Improved Relation Labeling. Creating numerous relations in a single task can look a bit clumsy. The limited space in Labeling screen, the relation arrows and different relation types all at once could create difficulty to visualize them properly. We improved the UX for this feature: Spaces between two lines if relations are present Ability to Filter by certain relations When hovered on one relation, only that is focused Miscellaneous. Generally when a first completion in a task is submitted, it is very likely for that completion to be the ground truth for that task. Starting with this version, the first submitted completion gets automatically starred. Hitting submit button on next completion, annotator are asked to either just submit or submit and star it. Bug fixes On restart of the Annotation Lab machine/VM all Downloaded models (from Models Hub) compatible with Spark NLP 3.1 version were deleted. We have now fixed this issue. Going forward, it is user’s responsibility to remove any incompatible models. Those will only be marked as “Incompatible” in Models Hub. This version also fixes some reported issues in training logs. The CONLL exports were including Assertion Status labels too. Going forward Assertion Status labels will be excluded given correct Project Config is setup. Read more Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_2_2_2",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_2_2_2"
  },
  "157": {
    "id": "157",
    "title": "Annotation Lab Release Notes 2.3.0",
    "content": "2.3.0 Highlights Multipage PDF annotation. Annotation Lab 2.3.0 supports the complete flow of import, annotation, and export for multi-page PDF files. Users have two options for importing a new PDF file into the Visual NER project: Import PDF file from local storage Add a link to the PDF file in the file attribute. After import, the user can see a task on the task page with a file name. On the labeling page, the user can view the PDF file with pagination so that the user can annotate the PDF one page at a time. After completing the annotation, the user can submit a task and it is ready to be exported to JSON and the user can import this exported file into any Visual NER project. [Note: Export in COCO format is not yet supported for PDF file] Redesign of the Project Setup Page. With the addition of many new features on every release, the Project Setup page became crowded and difficult to digest by users. In this release we have split its main components into multiple tabs: Project Description, Sharing, Configuration, and Training. Train and Test Dataset. Project Owner/Manager can tag the tasks that will be used for train and for test purposes. For this, two predefined tags will be available in all projects: Train and Test. Enhanced Relation Annotation. The user experience while annotating relations on the Labeling page has been improved. Annotators can now select the desired relation(s) by clicking the plus “+” sign present next to the relations arrow. Other UX improvements: Multiple items selection with Shift Key in Models Hub and Tasks page, Click instead of hover on more options in Models Hub and Tasks page, Tabbed View on the ModelsHub page. Bug fixes Validations related to Training Settings across different types of projects were fixed. It is not very common to upload an expired license given a valid license is already present. But in case users did that there was an issue while using a license in the spark session because only the last uploaded license was used. Now it has been fixed to use any valid license no matter the order of upload. Sometimes search and filters in the ModelsHub page were not working. Also, there was an issue while removing defined labels on the Upload Models Form. Both of these issues are fixed. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_2_3_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_2_3_0"
  },
  "158": {
    "id": "158",
    "title": "Spark NLP for Healthcare Release Notes 2.4.0",
    "content": "2.4.0 Overview We are glad to announce Spark NLP for Healthcare 2.4.0. This is an important release because of several refactorizations achieved in the core library, plus the introduction of several state of the art algorithms, new features and enhancements. We have included several architecture and performance improvements, that aim towards making the library more robust in terms of storage handling for Big Data. In the NLP aspect, we have introduced a ContextualParser, DocumentLogRegClassifier and a ChunkEntityResolverSelector. These last two Annotators also target performance time and memory consumption by lowering the order of computation and data loaded to memory in each step when designed following a hierarchical pattern. We have put a big effort on this one, so please enjoy and share your comments. Your words are always welcome through all our different channels. Thank you very much for your important doubts, bug reports and feedback; they are always welcome and much appreciated. New Features BigChunkEntityResolver Annotator: New experimental approach to reduce memory consumption at expense of disk IO. ContextualParser Annotator: New entity parser that works based on context parameters defined in a JSON file. ChunkEntityResolverSelector Annotator: New AnnotatorModel that takes advantage of the RecursivePipelineModel + LazyAnnotator pattern to annotate with different LazyAnnotators at runtime. DocumentLogregClassifier Annotator: New Annotator that provides a wrapped TFIDF Vectorizer + LogReg Classifier for TOKEN AnnotatorTypes (either at Document level or Chunk level) Enhancements normalizedColumn Param is no longer required in ChunkEntityResolver Annotator (defaults to the labelCol Param value). ChunkEntityResolverMetadata now has more data to infer whether the match is meaningful or not. Bugfixes Fixed a bug on ContextSpellChecker Annotator where unrecognized tokens would cause an exception if not in vocabulary. Fixed a bug on ChunkEntityResolver Annotator where undetermined results were coming out of negligible confidence scores for matches. Fixed a bug on ChunkEntityResolver Annotator where search would fail if the neighbours Param was grater than the number of nodes in the tree. Now it returns up to the number of nodes in the tree. Deprecations OCR Moves to its own JSL Spark OCR project. Infrastructure Spark NLP License is now required to utilize the library. Please follow the instructions on the shared email. Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_4_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_4_0"
  },
  "159": {
    "id": "159",
    "title": "Annotation Lab Release Notes 2.4.0",
    "content": "2.4.0 Annotation Lab v2.4.0 adds relation creation features for Visual NER projects and redesigns the Spark NLP Pipeline Config on the Project Setup Page. Several bug fixes and stabilizations are also included. Following are the highlights: Highlights Relations on Visual NER Projects. Annotators can create relations between annotated tokens/regions in Visual NER projects. This functionality is similar to what we already had in text-based projects. It is also possible to assign relation labels using the contextual widget (the “+” sign displayed next to the relation arrow). SparkNLP Pipeline Config page was redesigned. The SparkNLP Pipeline Config in the Setup Page was redesigned to ease filtering, collapsing, and expanding models and labels. For a more intuitive use, the Add Label button was moved to the top right side of the tab and no longer scrolls with the config list. This version also adds many improvements to the new Setup Page. The Training and Active Learning Tabs are only available to projects for which Annotation Lab supports training. When unsaved changes are present in the configuration, the user cannot move to the Training and Active Learning Tab and/or Training cannot be started. When the OCR server was not deployed, imports in the Visual NER project were failing. With this release, when a valid Spark OCR license is present, the OCR server is deployed and the import of pdf and image files is executed. Bug fixes When a login session is timed out and cookies are expired, users had to refresh the page to get the login screen. This known issue has been fixed and the user will be redirected to the login page. When a task was assigned to an annotator who does not have completions for it, the task status was shown incorrectly. This was fixed in this version. While preannotating tasks with some specific types of models, only the first few lines were annotated. We have fixed the Spark NLP pipeline for such models and now the entire document gets preannotations. When Spark NLP for Healthcare license was expired, the deployment was allowed but it used to fail. Now a proper message is shown to Project Owners/Managers and the deployment is not started in such cases. Inconsistencies were present in training logs and some embeddings were not successfully used for models training. Along with these fixes, several UI bugs are also fixed in this release. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_2_4_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_2_4_0"
  },
  "160": {
    "id": "160",
    "title": "Spark NLP for Healthcare Release Notes 2.4.1",
    "content": "2.4.1 Overview Introducing Spark NLP for Healthcare 2.4.1 after all the feedback we received in the form of issues and suggestions on our different communication channels. Even though 2.4.0 was very stable, version 2.4.1 is here to address minor bug fixes that we summarize in the following lines. Bugfixes Changing the license Spark property key to be “jsl” instead of “sparkjsl” as the latter generates inconsistencies Fix the alignment logic for tokens and chunks in the ChunkEntityResolverSelector because when tokens and chunks did not have the same begin-end indexes the resolution was not executed Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_4_1",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_4_1"
  },
  "161": {
    "id": "161",
    "title": "Spark NLP for Healthcare Release Notes 2.4.2",
    "content": "2.4.2 Overview We are glad to announce Spark NLP for Healthcare 2.4.2. As a new feature we are happy to introduce our new Disambiguation Annotator, which will let the users resolve different kind of entities based on Knowledge bases provided in the form of Records in a RocksDB database. We also enhanced / fixed DocumentLogRegClassifier, ChunkEntityResolverModel and ChunkEntityResolverSelector Annotators. New Features Disambiguation Annotator (NerDisambiguator and NerDisambiguatorModel) which accepts annotator types CHUNK and SENTENCE_EMBEDDINGS and returns DISAMBIGUATION annotator type. This output annotation type includes all the matches in the result and their similarity scores in the metadata. Enhancements ChunkEntityResolver Annotator now supports both EUCLIDEAN and COSINE distance for the KNN search and WMD calculation. Bugfixes Fixed a bug in DocumentLogRegClassifier Annotator to support its serialization to disk. Fixed a bug in ChunkEntityResolverSelector Annotator to group by both SENTENCE and CHUNK at the time of forwarding tokens and embeddings to the lazy annotators. Fixed a bug in ChunkEntityResolverModel in which the same exact embeddings was not included in the neighbours. Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_4_2",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_4_2"
  },
  "162": {
    "id": "162",
    "title": "Spark NLP for Healthcare Release Notes 2.4.5",
    "content": "2.4.5 Overview We are glad to announce Spark NLP for Healthcare 2.4.5. As a new feature we are happy to introduce our new EnsembleEntityResolver which allows our Entity Resolution architecture to scale up in multiple orders of magnitude and handle datasets of millions of records on a sub-log computation increase We also enhanced our ChunkEntityResolverModel with 5 new distance calculations with weighting-array and aggregation-strategy params that results in more levers to finetune its performance against a given dataset. New Features EnsembleEntityResolver consisting of an integrated TFIDF-Logreg classifier in the first layer + Multiple ChunkEntityResolvers in the second layer (one per each class) Five (5) new distances calculations for ChunkEntityResolver, namely: Token Based: TFIDF-Cosine, Jaccard, SorensenDice Character Based: JaroWinkler and Levenshtein Weight parameter that works as a multiplier for each distance result to be considered during their aggregation Three (3) aggregation strategies for the enabled distance in a particular instance, namely: AVERAGE, MAX and MIN Enhancements ChunkEntityResolver can now compute distances over all the neighbours found and return the metadata just for the best alternatives that meet the threshold; before it would calculate them over the neighbours and return them all in the metadata ChunkEntityResolver now has an extramassPenalty parameter to accoun for penalization of token-length difference in compared strings Metadata for the ChunkEntityResolver has been updated accordingly to reflect all new features StringDistances class has been included in utils to aid in the calculation and organization of different types of distances for Strings HasFeaturesJsl trait has been included to support the serialization of Features including [T] &lt;: AnnotatorModel[T] types Bugfixes Frequency calculation for WMD in ChunkEntityResolver has been adjusted to account for real word count representation AnnotatorType for DocumentLogRegClassifier has been changed to CATEGORY to align with classifiers in Open Source library Deprecations Legacy EntityResolver{Approach, Model} classes have been deprecated in favor of ChunkEntityResolver classes ChunkEntityResolverSelector classes has been deprecated in favor of EnsembleEntityResolver Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_4_5",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_4_5"
  },
  "163": {
    "id": "163",
    "title": "Spark NLP for Healthcare Release Notes 2.4.6",
    "content": "2.4.6 Overview We release Spark NLP for Healthcare 2.4.6 to fix some minor bugs. Bugfixes Updated IDF value calculation to be probabilistic based log[(N - df_t) / df_t + 1] as opposed to log[N / df_t] TFIDF cosine distance was being calculated with the rooted norms rather than with the original squared norms Validation of label cols is now performed at the beginning of EnsembleEntityResolver Environment Variable for License value named jsl.settings.license Now DocumentLogRegClassifier can be serialized from Python (bug introduced with the implementation of RecursivePipelines, LazyAnnotator attribute) Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_4_6",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_4_6"
  },
  "164": {
    "id": "164",
    "title": "Spark NLP for Healthcare Release Notes 2.5.0",
    "content": "2.5.0 Overview We are happy to bring you Spark NLP for Healthcare 2.5.0 with new Annotators, Models and Data Readers. Model composition and iteration is now faster with readers and annotators designed for real world tasks. We introduce ChunkMerge annotator to combine all CHUNKS extracted by different Entity Extraction Annotators. We also introduce an Annotation Reader for JSL AI Platform’s Annotation Tool. This release is also the first one to support the models: ner_large_clinical, ner_events_clinical, assertion_dl_large, chunkresolve_loinc_clinical, deidentify_large And of course we have fixed some bugs. New Features AnnotationToolJsonReader is a new class that imports a JSON from AI Platform’s Annotation Tool an generates NER and Assertion training datasets ChunkMerge Annotator is a new functionality that merges two columns of CHUNKs handling overlaps with a very straightforward logic: max coverage, max # entities ChunkMerge Annotator handles inputs from NerDLModel, RegexMatcher, ContextualParser, TextMatcher A DeIdentification pretrained model can now work in ‘mask’ or ‘obfuscate’ mode Enhancements DeIdentification Annotator has a more consistent API: mode param with values (‘mask’l’obfuscate’) to drive its behavior dateFormats param a list of string values to to select which dateFormats to obfuscate (and which to just mask) DeIdentification Annotator no longer automatically obfuscates dates. Obfuscation is now driven by mode and dateFormats params A DeIdentification pretrained model can now work in ‘mask’ or ‘obfuscate’ mode Bugfixes DeIdentification Annotator now correctly deduplicates protected entities coming from NER / Regex DeIdentification Annotator now indexes chunks correctly after merging them AssertionDLApproach Annotator can now be trained with the graph in any folder specified by setting graphFolder param AssertionDLApproach now has the setClasses param setter in Python wrapper JVM Memory and Kryo Max Buffer size increased to 32G and 2000M respectively in sparknlp_jsl.start(secret) function Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_5_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_5_0"
  },
  "165": {
    "id": "165",
    "title": "Annotation Lab Release Notes 2.5.0",
    "content": "2.5.0 Annotation Lab v2.5.0 introduces support for rule based annotations, new search feature and COCO format export for Visual NER projects. It also includes fixes for the recently identified security issues and other known bugs. Below are the highlights of this release. Highlights Rule Based Annotations. Spark NLP for Healthcare supports rule-based annotations via the ContextualParser Annotator. In this release Annotationlab adds support for creating and using ContextualParser rules in NER project. Any user with admin privilegis can see rules under the Available Rules tab on the Models Hub page and can create new rules using the + Add Rule button. After adding a rule on Models Hub page, the Project Owner or Manager can add the rule to the configuration of the project where he wants to use it. This can be done via the Rules tab from the Project Setup page under the Project Configuration tab. A valid Spark NLP for Healthcare licence is required to deploy rules from project config. Two types of rules are supported:1. Regex Based: User can enter the Regex which matches to the entities of the required label; and 2. Dictionary Based: User can create a dictionary of labels and user can upload the CSV of the list of entity that comes under the label. Search through Visual NER Projects. For the Visual NER Projects, it is now possible to search for a keyword inside of image/pdf based tasks using the search box available on the top of the Labeling page. Currently, the search is performed on the current page only. Furthermore, we have also extended the keyword-based task search already available for text-based projects for Visual NER Projects. On the Tasks page, use the search bar on the upper right side of the screen like you would do in other text-based projects, to identify all image/pdf tasks containing a given text. COCO export for pdf tasks in Visual NER Projects. Up until now, the COCO format export was limited to simple image documents. With version 2.5.0, this functionality is extended to single-page or multi-page pdf documents. In Classification Project, users are now able to use different layouts for the list of choices: layout=&quot;select&quot;: It will change choices from list of choices inline to dropdown layout. Possible values are &quot;select&quot;, &quot;inline&quot;, &quot;vertical&quot; choice=&quot;multiple&quot;: Allow user to select multiple values from dropdown. Possible values are: &quot;single&quot;, &quot;single-radio&quot;, &quot;multiple&quot; Better Toasts, Confirmation-Boxes and Masking UI on potentially longer operations. Security Fixes Annotationlab v2.5.0 got different Common Vulnerabilities and Exposures(CVE) issues fixed. As always, in this release we performed security scans to detect CVE issues, upgraded python packages to eliminate known vulnerabilities and also we made sure the CVE-2021-44228 (Log4j2 issue) is not present in any images used by Annotation Lab. A reported issue when logout endpoint was sometimes redirected to insecure http after access token expired was also fixed. Bug Fixes The Filters option in the Models Hub page was not working properly. Now the “Free/Licensed” filter can be selected/deselected without getting any error. After creating relations and saving/updating annotations for the Visual NER projects with multi-paged pdf files, the annotations and relations were not saved. An issue with missing text tokens in the exported JSON file for the Visual NER projects also have been fixed. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_2_5_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_2_5_0"
  },
  "166": {
    "id": "166",
    "title": "Spark NLP for Healthcare Release Notes 2.5.2",
    "content": "2.5.2 Overview We are really happy to bring you Spark NLP for Healthcare 2.5.2, with a couple new features and several enhancements in our existing annotators. This release was mainly dedicated to generate adoption in our AnnotationToolJsonReader, a connector that provide out-of-the-box support for out Annotation Tool and our practices. Also the ChunkMerge annotator has ben provided with extra functionality to remove entire entity types and to modify some chunk’s entity type We also dedicated some time in finalizing some refactorization in DeIdentification annotator, mainly improving type consistency and case insensitive entity dictionary for obfuscation. Thanks to the community for all the feedback and suggestions, it’s really comfortable to navigate together towards common functional goals that keep us agile in the SotA. New Features Brand new IOBTagger Annotator NerDL Metrics provides an intuitive DataFrame API to calculate NER metrics at tag (token) and entity (chunk) level Enhancements AnnotationToolJsonReader includes parameters for document cleanup, sentence boundaries and tokenizer split chars AnnotationToolJsonReader uses the task title if present and uses IOBTagger annotator AnnotationToolJsonReader has improved alignment in assertion train set generation by using an alignTol parameter as tollerance in chunk char alignment DeIdentification refactorization: Improved typing and replacement logic, case insensitive entities for obfuscation ChunkMerge Annotator now handles: Drop all chunks for an entity Replace entity name Change entity type for a specific (chunk, entity) pair Drop specific (chunk, entity) pairs caseSensitive param to EnsembleEntityResolver Output logs for AssertionDLApproach loss Disambiguator is back with improved dependency management Bugfixes Bugfix in python when Annotators shared domain parts across public and internal Bugfix in python when ChunkMerge annotator was loaded from disk ChunkMerge now weights the token coverage correctly when multiple multi-token entities overlap Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_5_2",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_5_2"
  },
  "167": {
    "id": "167",
    "title": "Spark NLP for Healthcare Release Notes 2.5.3",
    "content": "2.5.3 Overview We are pleased to announce the release of Spark NLP for Healthcare 2.5.3. This time we include four (4) new Annotators: FeatureAssembler, GenericClassifier, Yake Keyword Extractor and NerConverterInternal. We also include helper classes to read datasets from CodiEsp and Cantemist Spanish NER Challenges. This is also the first release to support the following models: ner_diag_proc (spanish), ner_neoplasms (spanish), ner_deid_enriched (english). We have also included Bugifxes and Enhancements for AnnotationToolJsonReader and ChunkMergeModel. New Features FeatureAssembler Transformer: Receives a list of column names containing numerical arrays and concatenates them to form one single feature_vector annotation GenericClassifier Annotator: Receives a feature_vector annotation and outputs a category annotation Yake Keyword Extraction Annotator: Receives a token annotation and outputs multi-token keyword annotations NerConverterInternal Annotator: Similar to it’s open source counterpart in functionality, performs smarter extraction for complex tokenizations and confidence calculation Readers for CodiEsp and Cantemist Challenges Enhancements AnnotationToolJsonReader includes parameter for preprocessing pipeline (from Document Assembling to Tokenization) AnnotationToolJsonReader includes parameter to discard specific entity types Bugfixes ChunkMergeModel now prioritizes highest number of different entities when coverage is the same Models We have 2 new spanish models for Clinical Entity Recognition: ner_diag_proc and ner_neoplasms We have a new english Named Entity Recognition model for deidentification: ner_deid_enriched Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_5_3",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_5_3"
  },
  "168": {
    "id": "168",
    "title": "Spark NLP for Healthcare Release Notes 2.5.5",
    "content": "2.5.5 Overview We are very happy to release Spark NLP for Healthcare 2.5.5 with a new state-of-the-art RelationExtraction annotator to identify relationships between entities coming from our pretrained NER models. This is also the first release to support Relation Extraction with the following two (2) models: re_clinical and re_posology in the clinical/models repository. We also include multiple bug fixes as usual. New Features RelationExtraction annotator that receives WORD_EMBEDDINGS, POS, CHUNK, DEPENDENCY and returns the CATEGORY of the relationship and a confidence score. Enhancements AssertionDL Annotator now keeps logs of the metrics while training DeIdentification now has a default behavior of merging entities close in Levenshtein distance with setConsistentObfuscation and setSameEntityThreshold params. DeIdentification now has a specific parameter setObfuscateDate to obfuscate dates (which will be otherwise just masked). The only formats obfuscated when the param is true will be the ones present in dateFormats param. NerConverterInternal now has a greedyMode param that will merge all contiguous tags of the same type regardless of boundary tags like “B”,”E”,”S”. AnnotationToolJsonReader includes mergeOverlapping parameter to merge (or not) overlapping entities from the Annotator jsons i.e. not included in the assertion list. Bugfixes DeIdentification documentation bug fix (typo) DeIdentification training bug fix in obfuscation dictionary IOBTagger now has the correct output type NAMED_ENTITY Deprecations EnsembleEntityResolver has been deprecated Models We have 2 new english Relationship Extraction model for Clinical and Posology NERs: re_clinical: with ner_clinical and embeddings_clinical re_posology: with ner_posology and embeddings_clinical Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_5_5",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_5_5"
  },
  "169": {
    "id": "169",
    "title": "Spark NLP for Healthcare Release Notes 2.6.0",
    "content": "2.6.0 Overview We are honored to announce that Spark NLP Enterprise 2.6.0 has been released. The first time ever, we release three pretrained clinical pipelines to save you from building pipelines from scratch. Pretrained pipelines are already fitted using certain annotators and transformers according to various use cases. The first time ever, we are releasing 3 licensed German models for healthcare and Legal domains. Models Pretrained Pipelines: The first time ever, we release three pretrained clinical pipelines to save you from building pipelines from scratch. Pretrained pipelines are already fitted using certain annotators and transformers according to various use cases and you can use them as easy as follows: pipeline = PretrainedPipeline(&#39;explain_clinical_doc_carp&#39;, &#39;en&#39;, &#39;clinical/models&#39;) pipeline.annotate(&#39;my string&#39;) Pipeline descriptions: explain_clinical_doc_carp a pipeline with ner_clinical, assertion_dl, re_clinical and ner_posology. It will extract clinical and medication entities, assign assertion status and find relationships between clinical entities. explain_clinical_doc_era a pipeline with ner_clinical_events, assertion_dl and re_temporal_events_clinical. It will extract clinical entities, assign assertion status and find temporal relationships between clinical entities. recognize_entities_posology a pipeline with ner_posology. It will only extract medication entities. More information and examples are available here: https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/11.Pretrained_Clinical_Pipelines.ipynb. Pretrained Named Entity Recognition and Relationship Extraction Models (English) RE models: re_temporal_events_clinical re_temporal_events_enriched_clinical re_human_phenotype_gene_clinical re_drug_drug_interaction_clinical re_chemprot_clinical NER models: ner_human_phenotype_gene_clinical ner_human_phenotype_go_clinical ner_chemprot_clinical More information and examples here: https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/10.Clinical_Relation_Extraction.ipynb Pretrained Named Entity Recognition and Relationship Extraction Models (German) The first time ever, we are releasing 3 licensed German models for healthcare and Legal domains. German Clinical NER model for 19 clinical entities German Legal NER model for 19 legal entities German ICD-10GM More information and examples here: https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/14.German_Healthcare_Models.ipynb https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/15.German_Legal_Model.ipynb Other Pretrained Models We now have Named Entity Disambiguation model out of the box. Disambiguation models map words of interest, such as names of persons, locations and companies, from an input text document to corresponding unique entities in a target Knowledge Base (KB). https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/12.Named_Entity_Disambiguation.ipynb Due to ongoing requests about Clinical Entity Resolvers, we release a notebook to let you see how to train an entity resolver using an open source dataset based on Snomed. https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/13.Snomed_Entity_Resolver_Model_Training.ipynb Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_6_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_6_0"
  },
  "170": {
    "id": "170",
    "title": "Annotation Lab Release Notes 2.6.0",
    "content": "2.6.0 Annotation Lab v2.6.0 improves the performance of the Project Setup page, adds a “View as” option in the Labeling Page, improves the layout of OCR-ed documents, adds the option to stop training and model server deployment from UI. Many more cool features are also delivered in this version to enhance usability and stabilize the product. Here are details of features and bug fixes included in this release. Highlights Performance improvement in Setup page. In previous versions of Annotation Lab, changes in Project Configuration would take a long time to validate if that project included a high volume of completions. The configuration validation time is now almost instant, even for projects with thousand of tasks. Multiple tests were conducted on projects with more than 13K+ tasks and thousands of extractions per task. For all of those test situations, the validation of the Project Configuration took under 2 seconds. Those tests results were replicated for all types of projects including NER, Image, Audio, Classification, and HTML projects. New “View as” option in the labeling screen. When a user has multiple roles (Manager, Annotator, Reviewer), the Labeling Page should present and render different content and specific UX, depending on the role impersonated by the user. For a better user experience, this version adds a “View as” switch in the Labeling Page. Once the “View as” option is used to select a certain role, the selection is preserved even when the tab is closed or refreshed. OCR Layout improvement. In previous versions of the Annotation Lab, layout was not preserved in OCRed tasks. Recognized texts would be placed in a top to bottom approach without considering the paragraph each token belonged to. From this version on, we are using layout-preserving transformers from Spark OCR. As a result, tokens that belong to the same paragraph are now grouped together, producing more meaningful output. Ability to stop training and model server deployment. Up until now, training and model server deployment could be stopped by system admins only. This version of Annotation Lab provides Project Owners/Managers with the option to stop these processes simply by clicking a button in the UI. This option is necessary in many cases, such as when a manager/project owner starts the training process on a big project that takes a lot of resources and time, blocking access to preannotations to the other projects. Display meaningful message when training fails due to memory issues. In case the training of a model fails due to memory issues, the reason for the failure are available via the UI (i.e. out of memory error). Allow combining NER labels and Classification classes from Spark NLP pipeline config. The earlier version had an issue with adding choice from the predefined classification model to an existing NER project. This issue has been fixed in this version. Bug Fixes Previously there was a UI reloading issue when a User was removed from the “Annotators” user group, which has now been fixed. The user can log in without the reloading issue, a warning is shown in UI regarding the missing annotator privilege. Also, setting up the HTML NER Tagging project was not possible in the earlier version which has been fixed in this release. On the labeling page, the renamed title of the next served task was not displayed. Similarly, in the Import page, the count of the tasks imported was missing in the Import Status dialog box. Now both these issues are fixed. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_2_6_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_2_6_0"
  },
  "171": {
    "id": "171",
    "title": "Spark NLP for Healthcare Release Notes 2.6.2",
    "content": "2.6.2 Overview We are very happy to announce that version 2.6.2 of Spark NLP Enterprise is ready to be installed and used. We are making available Named Entity Recognition, Sentence Classification and Entity Resolution models to analyze Adverse Drug Events in natural language text from clinical domains. Models NERs We are pleased to announce that we have a brand new named entity recognition (NER) model for Adverse Drug Events (ADE) to extract ADE and DRUG entities from a given text. ADE NER will have four versions in the library, trained with different size of word embeddings: ner_ade_bioert (768d Bert embeddings) ner_ade_clinicalbert (768d Bert embeddings) ner_ade_clinical (200d clinical embeddings) ner_ade_healthcare (100d healthcare embeddings) More information and examples here We are also releasing our first clinical pretrained classifier for ADE classification tasks. This new ADE classifier is trained on various ADE datasets, including the mentions in tweets to represent the daily life conversations as well. So it works well on the texts coming from academic context, social media and clinical notes. It’s trained with Clinical Biobert embeddings, which is the most powerful contextual language model in the clinical domain out there. Classifiers ADE classifier will have two versions in the library, trained with different Bert embeddings: classifierdl_ade_bioert (768d BioBert embeddings) classifierdl_adee_clinicalbert (768d ClinicalBert embeddings) More information and examples here Pipeline By combining ADE NER and Classifier, we are releasing a new pretrained clinical pipeline for ADE tasks to save you from building pipelines from scratch. Pretrained pipelines are already fitted using certain annotators and transformers according to various use cases and you can use them as easy as follows: pipeline = PretrainedPipeline(&#39;explain_clinical_doc_ade&#39;, &#39;en&#39;, &#39;clinical/models&#39;) pipeline.annotate(&#39;my string&#39;) explain_clinical_doc_ade is bundled with ner_ade_clinicalBert, and classifierdl_ade_clinicalBert. It can extract ADE and DRUG clinical entities, and then assign ADE status to a text (True means ADE, False means not related to ADE). More information and examples here Entity Resolver We are releasing the first Entity Resolver for Athena (Automated Terminology Harmonization, Extraction and Normalization for Analytics, http://athena.ohdsi.org/) to extract concept ids via standardized medical vocabularies. For now, it only supports conditions section and can be used to map the clinical conditions with the corresponding standard terminology and then get the concept ids to store them in various database schemas. It is named as chunkresolve_athena_conditions_healthcare. We added slim versions of several clinical NER models that are trained with 100d healthcare word embeddings, which is lighter and smaller in size. ner_healthcare assertion_dl_healthcare ner_posology_healthcare ner_events_healthcare Graph Builder Spark NLP Licensed version has several DL based annotators (modules) such as NerDL, AssertionDL, RelationExtraction and GenericClassifier, and they are all based on Tensorflow (tf) with custom graphs. In order to make the creating and customizing the tf graphs for these models easier for our licensed users, we added a graph builder to the Python side of the library. Now you can customize your graphs and use them in the respected models while training a new DL model. from sparknlp_jsl.training import tf_graph tf_graph.build(&quot;relation_extraction&quot;,build_params={&quot;input_dim&quot;: 6000, &quot;output_dim&quot;: 3, &#39;batch_norm&#39;:1, &quot;hidden_layers&quot;: [300, 200], &quot;hidden_act&quot;: &quot;relu&quot;, &#39;hidden_act_l2&#39;:1}, model_location=&quot;.&quot;, model_filename=&quot;re_with_BN&quot;) More information and examples here Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_6_2",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_6_2"
  },
  "172": {
    "id": "172",
    "title": "Spark NLP for Healthcare Release Notes 2.7.0",
    "content": "2.7.0 We are glad to announce that Spark NLP for Healthcare 2.7 has been released ! In this release, we introduce the following features: 1. Text2SQL Text2SQL Annotator that translates natural language text into SQL queries against a predefined database schema, which is one of the most sought-after features of NLU. With the help of a pretrained text2SQL model, you will be able to query your database without writing a SQL query: Example 1 Query: What is the name of the nurse who has the most appointments? Generated SQL query from the model: SELECT T1.Name FROM Nurse AS T1 JOIN Appointment AS T2 ON T1.EmployeeID = T2.PrepNurse GROUP BY T2.prepnurse ORDER BY count(*) DESC LIMIT 1 Response:   Name 0 Carla Espinosa Example 2 Query: How many patients do each physician take care of? List their names and number of patients they take care of. Generated SQL query from the model: SELECT T1.Name, count(*) FROM Physician AS T1 JOIN Patient AS T2 ON T1.EmployeeID = T2.PCP GROUP BY T1.Name Response:   Name count(*) 0 Christopher Turk 1 1 Elliot Reid 2 2 John Dorian 1 For now, it only comes with one pretrained model (trained on Spider dataset) and new pretrained models will be released soon. Check out the Colab notebook to see more examples and run on your data. 2. SentenceEntityResolvers In addition to ChunkEntityResolvers, we now release our first BioBert-based entity resolvers using the SentenceEntityResolver annotator. It’s fully trainable and comes with several pretrained entity resolvers for the following medical terminologies: CPT: biobertresolve_cpt ICDO: biobertresolve_icdo ICD10CM: biobertresolve_icd10cm ICD10PCS: biobertresolve_icd10pcs LOINC: biobertresolve_loinc SNOMED_CT (findings): biobertresolve_snomed_findings SNOMED_INT (clinical_findings): biobertresolve_snomed_findings_int RXNORM (branded and clinical drugs): biobertresolve_rxnorm_bdcd Example: text = &#39;He has a starvation ketosis but nothing significant for dry oral mucosa&#39; df = get_icd10_codes (light_pipeline_icd10, &#39;icd10cm_code&#39;, text)   chunks begin end code 0 a starvation ketosis 7 26 E71121 1 dry oral mucosa 66 80 K136 Check out the Colab notebook to see more examples and run on your data. You can also train your own entity resolver using any medical terminology like MedRa and UMLS. Check this notebook to learn more about training from scratch. 3. ChunkMerge Annotator In order to use multiple NER models in the same pipeline, Spark NLP Healthcare has ChunkMerge Annotator that is used to return entities from each NER model by overlapping. Now it has a new parameter to avoid merging overlapping entities (setMergeOverlapping) to return all the entities regardless of char indices. It will be quite useful to analyze what every NER module returns on the same text. 4. Starting SparkSession We now support starting SparkSession with a different version of the open source jar and not only the one it was built against by sparknlp_jsl.start(secret, public=&quot;x.x.x&quot;) for extreme cases. 5. Biomedical NERs We are releasing 3 new biomedical NER models trained with clinical embeddings (all one single entity models) ner_bacterial_species (comprising of Linneaus and Species800 datasets) ner_chemicals (general purpose and bio chemicals, comprising of BC4Chem and BN5CDR-Chem) ner_diseases_large (comprising of ner_disease, NCBI_Disease and BN5CDR-Disease) We are also releasing the biobert versions of the several clinical NER models stated below: ner_clinical_biobert ner_anatomy_biobert ner_bionlp_biobert ner_cellular_biobert ner_deid_biobert ner_diseases_biobert ner_events_biobert ner_jsl_biobert ner_chemprot_biobert ner_human_phenotype_gene_biobert ner_human_phenotype_go_biobert ner_posology_biobert ner_risk_factors_biobert Metrics (micro averages excluding O’s):   model_name clinical_glove_micro biobert_micro 0 ner_chemprot_clinical 0.816 0.803 1 ner_bionlp 0.748 0.808 2 ner_deid_enriched 0.934 0.918 3 ner_posology 0.915 0.911 4 ner_events_clinical 0.801 0.809 5 ner_clinical 0.873 0.884 6 ner_posology_small 0.941   7 ner_human_phenotype_go_clinical 0.922 0.932 8 ner_drugs 0.964   9 ner_human_phenotype_gene_clinical 0.876 0.870 10 ner_risk_factors 0.728   11 ner_cellular 0.813 0.812 12 ner_posology_large 0.921   13 ner_anatomy 0.851 0.831 14 ner_deid_large 0.942   15 ner_diseases 0.960 0.966 In addition to these, we release two new German NER models: ner_healthcare_slim (‘TIME_INFORMATION’, ‘MEDICAL_CONDITION’, ‘BODY_PART’, ‘TREATMENT’, ‘PERSON’, ‘BODY_PART’) ner_traffic (extract entities regarding traffic accidents e.g. date, trigger, location etc.) 6. PICO Classifier Successful evidence-based medicine (EBM) applications rely on answering clinical questions by analyzing large medical literature databases. In order to formulate a well-defined, focused clinical question, a framework called PICO is widely used, which identifies the sentences in a given medical text that belong to the four components: Participants/Problem (P) (e.g., diabetic patients), Intervention (I) (e.g., insulin), Comparison (C) (e.g., placebo) and Outcome (O) (e.g., blood glucose levels). Spark NLP now introduces a pretrained PICO Classifier that is trained with Biobert embeddings. Example: text = “There appears to be no difference in smoking cessation effectiveness between 1mg and 0.5mg varenicline.” pico_lp_pipeline.annotate(text)[&#39;class&#39;][0] ans: CONCLUSIONS Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_0"
  },
  "173": {
    "id": "173",
    "title": "Annotation Lab Release Notes 2.7.0",
    "content": "2.7.0 Release date: 17-02-2022 Annotation Lab 2.7.0 is here! This is another feature reach release from John Snow Labs - Annotation Lab Team. It is powered by the latest Spark NLP and Spark NLP for Healthcare libraries and offers improved support for Rule Base Annotation. With the upgrade of Spark NLP libraries, the Models Hub page inside the application gets more than 100 new models for English along with the introduction of Spanish and German models. In Visual NER projects it is now easier to annotate cross line chunks. As always, there are many security and stabilizations shipped. Highlights Annotation Lab 2.7.0 includes Spark NLP 3.4.1 and Spark NLP for Healthcare. Model training is now significantly faster and issues related to Rule-based annotation have been solved. The Models Hub has increased the list of models and old incompatible models are now marked as “incompatible”. If there are any incompatible models downloaded on the machine, we recommend deleting them. Spanish and German Models have been added to Models Hub. In previous versions of the Annotation Lab, the Models Hub only offered English language models. But from version 2.7.0, models for two other languages are included as well, namely Spanish and German. It is possible to download or upload these models and use them for preannotation, in the same way as for English language models. Rule-Based Annotation improvement. Rule-based annotation, introduced in 2.6.0 with limited options, was improved in this release. The Rule creation UI form was simplified and extended, and help tips were added on each field. While creating a rule, the user can define the scope of the rule as being sentence or document. A new toggle parameter Complete Match Regex is added to the rules. It can be toggled on to preannotate the entity that exactly matches the regex or dictionary value regardless of the Match Scope. Also case-sensitive is always true (and hence the toggle is hidden in this case) for REGEX while the case-sensitive toggle for dictionary can be toggled on or off. Users can now download the uploaded dictionary of an existing rule. In the previous release, if a dictionary-based rule was defined with an invalid CSV file, the preannotation server would crash and would only recover when the rule was removed from the configuration. This issue has been fixed and it is also possible to upload both vertical and horizontal CSV files consisting of multi-token dictionary values. Flexible annotations for Visual NER Projects. The chunk annotation feature added to Visual NER projects, allows the annotation of several consecutive tokens as one chunk. It also supports multiple lines selection. Users can now select multiple tokens and annotate them together in Visual NER Projects. The label assigned to a connected group can be updated. This change will apply to all regions in the group. Constraints for relation labeling can be defined. While annotating projects with Relations between Entities, defining constraints (the direction, the domain, the co-domain) of relations is important. Annotation Lab 2.7.0 offers a way to define such constraints by editing the Project Configuration. The Project Owner or Project Managers can specify which Relation needs to be bound to which Labels and in which direction. This will hide some Relations in Labeling Page for NER Labels which will simplify the annotation process and will avoid the creation of any incorrect relations in the scope of the project. Security Security issues related to SQL Injection Vulnerability and Host Header Attack were fixed in this release. Bug Fixes Issues related to chunk annotation; Incorrect bounding boxes, Multiple stacking of bounding boxes, Inconsistent IDs of the regions, unchanged labels of one connected region to other were identified and fixed and annotators can now select multiple tokens at once and annotate them as a single chunk In the previous release, after an Assertion Status model was trained, it would get deployed without the NER model and hence the preannotation was not working as expected. Going forward, the trained Assertion Model cannot be deployed for projects without a NER model. For this to happen, the “Yes” button in the confirmation box for deploying an assertion model right after training is enabled only when the Project Configuration consists of at least one NER model. A bug in the default Project templates (Project Setup page) was preventing users to create projects using “Conditional classification” and “Pairwise comparison” templates. These default Project templates can be used with no trouble as any other 40+ default templates. Reviewers were able to view unassigned submitted tasks via the “Next” button on the Labeling page. This bug is also fixed now and the reviewers can only see tasks that are assigned to them both on the Task List page or while navigating through the “Next” button. For better user experience, the labeling page has been optimized and the tasks on the page render quicker than in previous versions. When adding a user to the UserAdmins group, the delay in enabling the checkbox has been fixed. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_2_7_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_2_7_0"
  },
  "174": {
    "id": "174",
    "title": "Spark NLP for Healthcare Release Notes 2.7.1",
    "content": "2.7.1 We are glad to announce that Spark NLP for Healthcare 2.7.1 has been released ! In this release, we introduce the following features: 1. Sentence BioBert and Bluebert Transformers that are fine tuned on MedNLI dataset. Sentence Transformers offers a framework that provides an easy method to compute dense vector representations for sentences and paragraphs (also known as sentence embeddings). The models are based on BioBert and BlueBert, and are tuned specifically to meaningful sentence embeddings such that sentences with similar meanings are close in vector space. These are the first PyTorch based models we managed to port into Spark NLP. Here is how you can load these: sbiobert_embeddins = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) sbluebert_embeddins = BertSentenceEmbeddings .pretrained(&quot;sbluebert_base_cased_mli&quot;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) 2. SentenceEntityResolvers powered by s-Bert embeddings. The advent of s-Bert sentence embeddings changed the landscape of Clinical Entity Resolvers completely in Spark NLP. Since s-Bert is already tuned on MedNLI (medical natural language inference) dataset, it is now capable of populating the chunk embeddings in a more precise way than before. Using sbiobert_base_cased_mli, we trained the following Clinical Entity Resolvers: sbiobertresolve_icd10cm sbiobertresolve_icd10pcs sbiobertresolve_snomed_findings (with clinical_findings concepts from CT version) sbiobertresolve_snomed_findings_int (with clinical_findings concepts from INT version) sbiobertresolve_snomed_auxConcepts (with Morph Abnormality, Procedure, Substance, Physical Object, Body Structure concepts from CT version) sbiobertresolve_snomed_auxConcepts_int (with Morph Abnormality, Procedure, Substance, Physical Object, Body Structure concepts from INT version) sbiobertresolve_rxnorm sbiobertresolve_icdo sbiobertresolve_cpt Code sample: (after getting the chunk from ChunkConverter) c2doc = Chunk2Doc().setInputCols(&quot;ner_chunk&quot;).setOutputCol(&quot;ner_chunk_doc&quot;) sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) snomed_ct_resolver = SentenceEntityResolverModel .pretrained(&quot;sbiobertresolve_snomed_findings&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_ct_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) Output:   chunks begin end code resolutions 2 COPD 113 116 13645005 copd - chronic obstructive pulmonary disease 8 PTCA 324 327 373108000 post percutaneous transluminal coronary angioplasty (finding) 16 close monitoring 519 534 417014005 on examination - vigilance See the notebook for details. 3. We are releasing the following pretrained clinical NER models: ner_drugs_large (trained with medications dataset, and extracts drugs with the dosage, strength, form and route at once as a single entity; entities: drug) ner_deid_sd_large (extracts PHI entities, trained with augmented dataset) ner_anatomy_coarse (trained with enriched anatomy NER dataset; entities: anatomy) ner_anatomy_coarse_biobert chunkresolve_ICD10GM_2021 (German ICD10GM resolver) We are also releasing two new NER models: ner_aspect_based_sentiment (extracts positive, negative and neutral aspects about restaurants from the written feedback given by reviewers. ) ner_financial_contract (extract financial entities from contracts. See the notebook for details.) Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_1",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_1"
  },
  "175": {
    "id": "175",
    "title": "Annotation Lab Release Notes 2.7.1",
    "content": "2.7.1 Release date: 22-02-2022 Annotation Lab v2.7.1 introduces an upgrade to K3s v1.22.4 and support for Redhat. It also includes improvements and fixes for identified bug. Below are the highlights of this release. Highlights For new installations, Annotation Lab is now installed on top of K3s v1.22.4. In near future we will provide similar support for existing installations. AWS market place also runs using the upgraded version. With this release Annotation lab can be installed on RedHat servers. Annotation lab 2.7.1 included release version of Spark NLP 3.4.1 and Spark NLP for Healthcare Bug Fixes In the previous release, saving Visual NER project configuration took a long time. With this release, the issue has been fixed and the Visual NER project can be created instantly. Due to a bug in Relation Constraint, all the relations we visible when the UI was refreshed. This issue has been resolved and only a valid list of relations is shown after the UI is refreshed. Previously, labels with spaces at the end were considered different. This has been fixed such that the label name with or without space at the end is treated as the same label. Importing multiple images as a zip file was not working correctly in the case of Visual NER. This issue was fixed. This version also fixes issues in Transfer Learning/Fine Tuning some NER models. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_2_7_1",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_2_7_1"
  },
  "176": {
    "id": "176",
    "title": "Spark NLP for Healthcare Release Notes 2.7.2",
    "content": "2.7.2 We are glad to announce that Spark NLP for Healthcare 2.7.2 has been released ! In this release, we introduce the following features: Far better accuracy for resolving medication terms to RxNorm codes: ondansetron 8 mg tablet&#39; -&gt; &#39;312086 Far better accuracy for resolving diagnosis terms to ICD-10-CM codes: TIA -&gt; transient ischemic attack (disorder) ‘S0690’ New ability to map medications to pharmacological actions (PA): &#39;metformin&#39; -&gt; ‘Hypoglycemic Agents’ 2 new greedy named entity recognition models for medication details: ner_drugs_greedy: ‘magnesium hydroxide 100mg/1ml PO’ ` ner_posology _greedy: ‘12 units of insulin lispro’ ` New model to classify the gender of a patient in a given medical note: &#39;58yo patient with a family history of breast cancer&#39; -&gt; ‘female’ And starting customized spark sessions with rich parameters params = {&quot;spark.driver.memory&quot;:&quot;32G&quot;, &quot;spark.kryoserializer.buffer.max&quot;:&quot;2000M&quot;, &quot;spark.driver.maxResultSize&quot;:&quot;2000M&quot;} spark = sparknlp_jsl.start(secret, params=params) State-of-the-art accuracy is achieved using new healthcare-tuned BERT Sentence Embeddings (s-Bert). The following sections include more details, metrics, and examples. Named Entity Recognizers for Medications A new medication NER (ner_drugs_greedy) that joins the drug entities with neighboring entities such as dosage, route, form and strength; and returns a single entity drug. This greedy NER model would be highly useful if you want to extract a drug with its context and then use it to get a RxNorm code (drugs may get different RxNorm codes based on the dosage and strength information). Metrics label tp fp fn prec rec f1 I-DRUG 37423 4179 3773 0.899 0.908 0.904 B-DRUG 29699 2090 1983 0.934 0.937 0.936 A new medication NER (ner_posology_greedy) that joins the drug entities with neighboring entities such as dosage, route, form and strength. It also returns all the other medication entities even if not related to (or joined with) a drug. Now we have five different medication-related NER models. You can see the outputs from each model below: Text = ‘‘The patient was prescribed 1 capsule of Advil 10 mg for 5 days and magnesium hydroxide 100mg/1ml suspension PO. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, and metformin 1000 mg two times a day.’’ a. ner_drugs_greedy   chunks begin end entities 0 1 capsule of Advil 10 mg 27 50 DRUG 1 magnesium hydroxide 100mg/1ml PO 67 98 DRUG 2 40 units of insulin glargine 168 195 DRUG 3 12 units of insulin lispro 207 232 DRUG b. ner_posology_greedy   chunks begin end entities 0 1 capsule of Advil 10 mg 27 50 DRUG 1 magnesium hydroxide 100mg/1ml PO 67 98 DRUG 2 for 5 days 52 61 DURATION 3 40 units of insulin glargine 168 195 DRUG 4 at night 197 204 FREQUENCY 5 12 units of insulin lispro 207 232 DRUG 6 with meals 234 243 FREQUENCY 7 metformin 1000 mg 250 266 DRUG 8 two times a day 268 282 FREQUENCY c. ner_drugs   chunks begin end entities 0 Advil 40 44 DrugChem 1 magnesium hydroxide 67 85 DrugChem 2 metformin 261 269 DrugChem d.ner_posology   chunks begin end entities 0 1 27 27 DOSAGE 1 capsule 29 35 FORM 2 Advil 40 44 DRUG 3 10 mg 46 50 STRENGTH 4 for 5 days 52 61 DURATION 5 magnesium hydroxide 67 85 DRUG 6 100mg/1ml 87 95 STRENGTH 7 PO 97 98 ROUTE 8 40 units 168 175 DOSAGE 9 insulin glargine 180 195 DRUG 10 at night 197 204 FREQUENCY 11 12 units 207 214 DOSAGE 12 insulin lispro 219 232 DRUG 13 with meals 234 243 FREQUENCY 14 metformin 250 258 DRUG 15 1000 mg 260 266 STRENGTH 16 two times a day 268 282 FREQUENCY e. ner_drugs_large   chunks begin end entities 0 Advil 10 mg 40 50 DRUG 1 magnesium hydroxide 100mg/1ml PO. 67 99 DRUG 2 insulin glargine 180 195 DRUG 3 insulin lispro 219 232 DRUG 4 metformin 1000 mg 250 266 DRUG Patient Gender Classification This model detects the gender of the patient in the clinical document. It can classify the documents into Female, Male and Unknown. We release two models: ‘Classifierdl_gender_sbert’ (more accurate, works with licensed sbiobert_base_cased_mli) ‘Classifierdl_gender_biobert’ (works with biobert_pubmed_base_cased) The models are trained on more than four thousands clinical documents (radiology reports, pathology reports, clinical visits etc.), annotated internally. Metrics (Classifierdl_gender_sbert)   precision recall f1-score support Female 0.9224 0.8954 0.9087 239 Male 0.7895 0.8468 0.8171 124 Text= ‘‘social history: shows that does not smoke cigarettes or drink alcohol, lives in a nursing home. family history: shows a family history of breast cancer.’’ gender_classifier.annotate(text)[&#39;class&#39;][0] &gt;&gt; `Female` See this Colab notebook for further details. a. classifierdl_gender_sbert document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) .setMaxSentenceLength(512) gender_classifier = ClassifierDLModel .pretrained(&#39;classifierdl_gender_sbert&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;document&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;class&quot;) gender_pred_pipeline = Pipeline( stages = [ document, sbert_embedder, gender_classifier ]) b. classifierdl_gender_biobert documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) clf_tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) biobert_embeddings = BertEmbeddings().pretrained(&#39;biobert_pubmed_base_cased&#39;) .setInputCols([&quot;document&quot;,&#39;token&#39;]) .setOutputCol(&quot;bert_embeddings&quot;) biobert_embeddings_avg = SentenceEmbeddings() .setInputCols([&quot;document&quot;, &quot;bert_embeddings&quot;]) .setOutputCol(&quot;sentence_bert_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) genderClassifier = ClassifierDLModel.pretrained(&#39;classifierdl_gender_biobert&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;document&quot;, &quot;sentence_bert_embeddings&quot;]) .setOutputCol(&quot;gender&quot;) gender_pred_pipeline = Pipeline( stages = [ documentAssembler, clf_tokenizer, biobert_embeddings, biobert_embeddings_avg, genderClassifier ]) New ICD10CM and RxCUI resolvers powered by s-Bert embeddings The advent of s-Bert sentence embeddings changed the landscape of Clinical Entity Resolvers completely in Spark NLP. Since s-Bert is already tuned on MedNLI (medical natural language inference) dataset, it is now capable of populating the chunk embeddings in a more precise way than before. We now release two new resolvers: sbiobertresolve_icd10cm_augmented (augmented with synonyms, four times richer than previous resolver accuracy: 73% for top-1 (exact match), 89% for top-5 (previous accuracy was 59% and 64% respectively) sbiobertresolve_rxcui (extract RxNorm concept unique identifiers to map with ATC or durg families) accuracy: 71% for top-1 (exact match), 72% for top-5 (previous accuracy was 22% and 41% respectively) a. ICD10CM augmented resolver Text = “This is an 82 year old male with a history of prior tobacco use , hypertension , chronic renal insufficiency , COPD , gastritis , and TIA who initially presented to Braintree with a non-ST elevation MI and Guaiac positive stools , transferred to St . Margaret&#39;s Center for Women &amp; Infants for cardiac catheterization with PTCA to mid LAD lesion complicated by hypotension and bradycardia requiring Atropine , IV fluids and transient dopamine possibly secondary to vagal reaction , subsequently transferred to CCU for close monitoring , hemodynamically stable at the time of admission to the CCU . “   chunk begin end code term 0 hypertension 66 77 I10 hypertension 1 chronic renal insufficiency 81 107 N189 chronic renal insufficiency 2 COPD 111 114 J449 copd - chronic obstructive pulmonary disease 3 gastritis 118 126 K2970 gastritis 4 TIA 134 136 S0690 transient ischemic attack (disorder) 5 a non-ST elevation MI 180 200 I219 silent myocardial infarction (disorder) 6 Guaiac positive stools 206 227 K921 guaiac-positive stools 7 mid LAD lesion 330 343 I2102 stemi involving left anterior descending coronary artery 8 hypotension 360 370 I959 hypotension 9 bradycardia 376 386 O9941 bradycardia b. RxCUI resolver Text= “He was seen by the endocrinology service and she was discharged on 50 mg of eltrombopag oral at night, 5 mg amlodipine with meals, and metformin 1000 mg two times a day . “   chunk begin end code term 0 50 mg of eltrombopag oral 67 91 825427 eltrombopag 50 MG Oral Tablet 1 5 mg amlodipine 103 117 197361 amlodipine 5 MG Oral Tablet 2 metformin 1000 mg 135 151 861004 metformin hydrochloride 1000 MG Oral Tablet Using this new resolver and some other resources like Snomed Resolver, RxTerm, MESHPA and ATC dictionary, you can link the drugs to the pharmacological actions (PA), ingredients and the disease treated with that. Code sample: (after getting the chunk from ChunkConverter) c2doc = Chunk2Doc().setInputCols(&quot;ner_chunk&quot;).setOutputCol(&quot;ner_chunk_doc&quot;) sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) icd10_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_icd10cm_augmented&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;icd10cm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) See the notebook for details. Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_2",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_2"
  },
  "177": {
    "id": "177",
    "title": "Annotation Lab Release Notes 2.7.2",
    "content": "2.7.2 Release date: 28-02-2022 Annotation Lab v2.7.2 includes Visual NER improvements Bug Fixes The text token in Visual NER project were missing in some cases when the labeling setting “Select regions after creating” was disabled. Now the setting is always enabled when labeling a Visual NER project. Previously, without any changes made by the user on the configuration page “unsaved changes” message used to pop up. Now, the message only pops up when there is an unsaved configuration change. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_2_7_2",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_2_7_2"
  },
  "178": {
    "id": "178",
    "title": "Spark NLP for Healthcare Release Notes 2.7.3",
    "content": "2.7.3 We are glad to announce that Spark NLP for Healthcare 2.7.3 has been released! Highlights: Introducing a brand-new RelationExtractionDL Annotator – Achieving SOTA results in clinical relation extraction using BioBert. Massive Improvements &amp; feature enhancements in De-Identification module: Introduction of faker augmentation in Spark NLP for Healthcare to generate random data for obfuscation in de-identification module. Brand-new annotator for Structured De-Identification. Drug Normalizer: Normalize medication-related phrases (dosage, form and strength) and abbreviations in text and named entities extracted by NER models. Confidence scores in assertion output : just like NER output, assertion models now also support confidence scores for each prediction. Cosine similarity metrics in entity resolvers to get more informative and semantically correct results. AuxLabel in the metadata of entity resolvers to return additional mappings. New Relation Extraction models to extract relations between body parts and clinical entities. New Entity Resolver models to extract billable medical codes. New Clinical Pretrained NER models. Bug fixes &amp; general improvements. Matching the version with Spark NLP open-source v2.7.3. 1. Improvements in De-Identification Module: Integration of faker library to automatically generate random data like names, dates, addresses etc so users dont have to specify dummy data (custom obfuscation files can still be used). It also improves the obfuscation results due to a bigger pool of random values. How to use: Set the flag setObfuscateRefSource to faker deidentification = DeIdentification() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_chunk&quot;]) .setOutputCol(&quot;deidentified&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) For more details: Check out this notebook 2. Structured De-Identification Module: Introduction of a new annotator to handle de-identification of structured data. it allows users to define a mapping of columns and their obfuscation policy. Users can also provide dummy data and map them to columns they want to replace values in. How to use: obfuscator = StructuredDeidentification (spark,{&quot;NAME&quot;:&quot;PATIENT&quot;,&quot;AGE&quot;:&quot;AGE&quot;}, obfuscateRefSource = &quot;faker&quot;) obfuscator_df = obfuscator.obfuscateColumns(df) obfuscator_df.select(&quot;NAME&quot;,&quot;AGE&quot;).show(truncate=False) Example: Input Data: Name Age Cecilia Chapman 83 Iris Watson 9 Bryar Pitts 98 Theodore Lowe 16 Calista Wise 76 Deidentified: Name Age Menne Erdôs 20 Longin Robinson 31 Flynn Fiedlerová 50 John Wakeland 21 Vanessa Andersson 12 For more details: Check out this notebook. 3. Introducing SOTA relation extraction model using BioBert A brand-new end-to-end trained BERT model, resulting in massive improvements. Another new annotator (ReChunkFilter) is also developed for this new model to allow syntactic features work well with BioBert to extract relations. How to use: re_ner_chunk_filter = RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) .setRelationPairs(pairs) .setMaxSyntacticDistance(4) re_model = RelationExtractionDLModel() .pretrained(“redl_temporal_events_biobert”, &quot;en&quot;, &quot;clinical/models&quot;) .setPredictionThreshold(0.9) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) Benchmarks: on benchmark datasets model Spark NLP ML model Spark NLP DL model benchmark re_temporal_events_clinical 68.29 71.0 80.2 1 re_clinical 56.45 69.2 68.2 2 re_human_pheotype_gene_clinical - 87.9 67.2 3 re_drug_drug_interaction - 72.1 83.8 4 re_chemprot 76.69 94.1 83.64 5 on in-house annotations model Spark NLP ML model Spark NLP DL model re_bodypart_problem 84.58 85.7 re_bodypart_procedure 61.0 63.3 re_date_clinical 83.0 84.0 re_bodypart_direction 93.5 92.5 For more details: Check out the notebook or modelshub. 4. Drug Normalizer: Standardize units of drugs and handle abbreviations in raw text or drug chunks identified by any NER model. This normalization significantly improves performance of entity resolvers. How to use: drug_normalizer = DrugNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;document_normalized&quot;) .setPolicy(&quot;all&quot;) #all/abbreviations/dosages Examples: drug_normalizer.transform(&quot;adalimumab 54.5 + 43.2 gm”) &gt;&gt;&gt; &quot;adalimumab 97700 mg&quot; Changes: combine 54.5 + 43.2 and normalize gm to mg drug_normalizer.transform(&quot;Agnogenic one half cup”) &gt;&gt;&gt; &quot;Agnogenic 0.5 oral solution&quot; Changes: replace one half to the 0.5, normalize cup to the oral solution drug_normalizer.transform(&quot;interferon alfa-2b 10 million unit ( 1 ml ) injec”) &gt;&gt;&gt; &quot;interferon alfa - 2b 10000000 unt ( 1 ml ) injection &quot; Changes: convert 10 million unit to the 10000000 unt, replace injec with injection For more details: Check out this notebook 5. Assertion models to support confidence in output: Just like NER output, assertion models now also provides confidence scores for each prediction. chunks entities assertion confidence a headache PROBLEM present 0.9992 anxious PROBLEM conditional 0.9039 alopecia PROBLEM absent 0.9992 pain PROBLEM absent 0.9238 .setClasses() method is deprecated in AssertionDLApproach and users do not need to specify number of classes while training, as it will be inferred from the dataset. 6. New Relation Extraction Models: We are also releasing new relation extraction models to link the clinical entities to body parts and dates. These models are trained using binary relation extraction approach for better accuracy. - re_bodypart_direction : Relation Extraction between Body Part and Direction entities. Example: Text: “MRI demonstrated infarction in the upper brain stem , left cerebellum and right basil ganglia” relations entity1 chunk1 entity2 chunk2 confidence 1 Direction upper bodyPart brain stem 0.999 0 Direction upper bodyPart cerebellum 0.999 0 Direction upper bodyPart basil ganglia 0.999 0 bodyPart brain stem Direction left 0.999 0 bodyPart brain stem Direction right 0.999 1 Direction left bodyPart cerebellum 1.0 0 Direction left bodyPart basil ganglia 0.976 0 bodyPart cerebellum Direction right 0.953 1 Direction right bodyPart basil ganglia 1.0 - re_bodypart_problem : Relation Extraction between Body Part and Problem entities. Example: Text: “No neurologic deficits other than some numbness in his left hand.” relation entity1 chunk1 entity2 chunk2 confidence 0 Symptom neurologic deficits bodyPart hand 1 1 Symptom numbness bodyPart hand 1 - re_bodypart_proceduretest : Relation Extraction between Body Part and Procedure, Test entities. Example: Text: “TECHNIQUE IN DETAIL: After informed consent was obtained from the patient and his mother, the chest was scanned with portable ultrasound.” relation entity1 chunk1 entity2 chunk2 confidence 1 bodyPart chest Test portable ultrasound 0.999 -re_date_clinical : Relation Extraction between Date and different clinical entities. Example: Text: “This 73 y/o patient had CT on 1/12/95, with progressive memory and cognitive decline since 8/11/94.” relations entity1 chunk1 entity2 chunk2 confidence 1 Test CT Date 1/12/95 1.0 1 Symptom progressive memory and cognitive decline Date 8/11/94 1.0 How to use: re_model = RelationExtractionModel() .pretrained(&quot;re_bodypart_direction&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setMaxSyntacticDistance(4) .setRelationPairs([‘Internal_organ_or_component’, ‘Direction’]) For more details: Check out the notebook or modelshub. New matching scheme for entity resolvers - improved accuracy: Adding the option to use cosine similarity to resolve entities and find closest matches, resulting in better, more semantically correct results. 7. New Resolver Models using JSL SBERT: sbiobertresolve_icd10cm_augmented sbiobertresolve_cpt_augmented sbiobertresolve_cpt_procedures_augmented sbiobertresolve_icd10cm_augmented_billable_hcc sbiobertresolve_hcc_augmented Returning auxilary columns mapped to resolutions: Chunk entity resolver and sentence entity resolver now returns auxilary data that is mapped the resolutions during training. This will allow users to get multiple resolutions with single model without using any other annotator in the pipeline (In order to get billable codes otherwise there needs to be other modules in the same pipeline) Example: sbiobertresolve_icd10cm_augmented_billable_hcc Input Text: “bladder cancer” idx chunks code resolutions all_codes billable hcc_status hcc_score all_distances 0 bladder cancer C679 [‘bladder cancer’, ‘suspected bladder cancer’, ‘cancer in situ of urinary bladder’, ‘tumor of bladder neck’, ‘malignant tumour of bladder neck’] [‘C679’, ‘Z126’, ‘D090’, ‘D494’, ‘C7911’] [‘1’, ‘1’, ‘1’, ‘1’, ‘1’] [‘1’, ‘0’, ‘0’, ‘0’, ‘1’] [‘11’, ‘0’, ‘0’, ‘0’, ‘8’] [‘0.0000’, ‘0.0904’, ‘0.0978’, ‘0.1080’, ‘0.1281’] sbiobertresolve_cpt_augmented Input Text: “ct abdomen without contrast” idx cpt code distance resolutions 0 74150 0.0802 Computed tomography, abdomen; without contrast material 1 65091 0.1312 Evisceration of ocular contents; without implant 2 70450 0.1323 Computed tomography, head or brain; without contrast material 3 74176 0.1333 Computed tomography, abdomen and pelvis; without contrast material 4 74185 0.1343 Magnetic resonance imaging without contrast 5 77059 0.1343 Magnetic resonance imaging without contrast 8. New Pretrained Clinical NER Models NER Radiology Input Text: “Bilateral breast ultrasound was subsequently performed, which demonstrated an ovoid mass measuring approximately 0.5 x 0.5 x 0.4 cm in diameter located within the anteromedial aspect of the left shoulder. This mass demonstrates isoechoic echotexture to the adjacent muscle, with no evidence of internal color flow. This may represent benign fibrous tissue or a lipoma.” idx chunks entities 0 Bilateral Direction 1 breast BodyPart 2 ultrasound ImagingTest 3 ovoid mass ImagingFindings 4 0.5 x 0.5 x 0.4 Measurements 5 cm Units 6 anteromedial aspect Direction 7 left Direction 8 shoulder BodyPart 9 mass ImagingFindings 10 isoechoic echotexture ImagingFindings 11 muscle BodyPart 12 internal color flow ImagingFindings 13 benign fibrous tissue ImagingFindings 14 lipoma Disease_Syndrome_Disorder Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_3",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_3"
  },
  "179": {
    "id": "179",
    "title": "Spark NLP for Healthcare Release Notes 2.7.4",
    "content": "2.7.4 We are glad to announce that Spark NLP for Healthcare 2.7.4 has been released! Highlights: Introducing a new annotator to extract chunks with NER tags using regex-like patterns: NerChunker. Introducing two new annotators to filter chunks: ChunkFilterer and AssertionFilterer. Ability to change the entity type in NerConverterInternal without using ChunkMerger (setReplaceDict). In DeIdentification model, ability to use faker and static look-up lists at the same time randomly in Obfuscation mode. New De-Identification NER model, augmented with synthetic datasets to detect uppercased name entities. Bug fixes &amp; general improvements. 1. NerChunker: Similar to what we used to do in POSChunker with POS tags, now we can also extract phrases that fits into a known pattern using the NER tags. NerChunker would be quite handy to extract entity groups with neighboring tokens when there is no pretrained NER model to address certain issues. Lets say we want to extract clinical findings and body parts together as a single chunk even if there are some unwanted tokens between. How to use: ner_model = NerDLModel.pretrained(&quot;ner_radiology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) ner_chunker = NerChunker(). .setInputCols([&quot;sentence&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setRegexParsers([&quot;&lt;IMAGINGFINDINGS&gt;*&lt;BODYPART&gt;&quot;]) text = &#39;She has cystic cyst on her kidney.&#39; &gt;&gt; ner tags: [(cystic, B-IMAGINGFINDINGS), (cyst,I-IMAGINGFINDINGS), (kidney, B-BODYPART) &gt;&gt; ner_chunk: [&#39;cystic cyst on her kidney&#39;] 2. ChunkFilterer: ChunkFilterer will allow you to filter out named entities by some conditions or predefined look-up lists, so that you can feed these entities to other annotators like Assertion Status or Entity Resolvers. It can be used with two criteria: isin and regex. How to use: ner_model = NerDLModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) chunk_filterer = ChunkFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;) .setOutputCol(&quot;chunk_filtered&quot;) .setCriteria(&quot;isin&quot;) .setWhiteList([&#39;severe fever&#39;,&#39;sore throat&#39;]) text = &#39;Patient with severe fever, sore throat, stomach pain, and a headache.&#39; &gt;&gt; ner_chunk: [&#39;severe fever&#39;,&#39;sore throat&#39;,&#39;stomach pain&#39;,&#39;headache&#39;] &gt;&gt; chunk_filtered: [&#39;severe fever&#39;,&#39;sore throat&#39;] 3. AssertionFilterer: AssertionFilterer will allow you to filter out the named entities by the list of acceptable assertion statuses. This annotator would be quite handy if you want to set a white list for the acceptable assertion statuses like present or conditional; and do not want absent conditions get out of your pipeline. How to use: clinical_assertion = AssertionDLModel.pretrained(&quot;assertion_dl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) assertion_filterer = AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;assertion_filtered&quot;) .setWhiteList([&quot;present&quot;]) text = &#39;Patient with severe fever and sore throat, but no stomach pain.&#39; &gt;&gt; ner_chunk: [&#39;severe fever&#39;,&#39;sore throat&#39;,&#39;stomach pain&#39;,&#39;headache&#39;] &gt;&gt; assertion_filtered: [&#39;severe fever&#39;,&#39;sore throat&#39;] Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_4",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_4"
  },
  "180": {
    "id": "180",
    "title": "Spark NLP for Healthcare Release Notes 2.7.5",
    "content": "2.7.5 We are glad to announce that Spark NLP for Healthcare 2.7.5 has been released! Highlights: New pretrained Relation Extraction model to link clinical tests to test results and dates to clinical entities: re_test_result_date Adding two new Admission and Discharge entities to ner_events_clinical and renaming it to ner_events_admission_clinical Improving ner_deid_enriched NER model to cover Doctor and Patient name entities in various context and notations. Bug fixes &amp; general improvements. 1. re_test_result_date : text = “Hospitalized with pneumonia in June, confirmed by a positive PCR of any specimen, evidenced by SPO2 &lt;/= 93% or PaO2/FiO2 &lt; 300 mmHg”   Chunk-1 Entity-1 Chunk-2 Entity-2 Relation 0 pneumonia Problem june Date is_date_of 1 PCR Test positive Test_Result is_result_of 2 SPO2 Test 93% Test_Result is_result_of 3 PaO2/FiO2 Test 300 mmHg Test_Result is_result_of 2. ner_events_admission_clinical : ner_events_clinical NER model is updated &amp; improved to include Admission and Discharge entities. text =”She is diagnosed as cancer in 1991. Then she was admitted to Mayo Clinic in May 2000 and discharged in October 2001”   chunk entity 0 diagnosed OCCURRENCE 1 cancer PROBLEM 2 1991 DATE 3 admitted ADMISSION 4 Mayo Clinic CLINICAL_DEPT 5 May 2000 DATE 6 discharged DISCHARGE 7 October 2001 DATE 3. Improved ner_deid_enriched : PHI NER model is retrained to cover Doctor and Patient name entities even there is a punctuation between tokens as well as all upper case or lowercased. text =”A . Record date : 2093-01-13 , DAVID HALE , M.D . , Name : Hendrickson , Ora MR . # 7194334 Date : 01/13/93 PCP : Oliveira , 25 month years-old , Record date : 2079-11-09 . Cocke County Baptist Hospital . 0295 Keats Street”   chunk entity 0 2093-01-13 MEDICALRECORD 1 DAVID HALE DOCTOR 2 Hendrickson , Ora PATIENT 3 7194334 MEDICALRECORD 4 01/13/93 DATE 5 Oliveira DOCTOR 6 25 AGE 7 2079-11-09 MEDICALRECORD 8 Cocke County Baptist Hospital HOSPITAL 9 0295 Keats Street STREET Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_5",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_5"
  },
  "181": {
    "id": "181",
    "title": "Spark NLP for Healthcare Release Notes 2.7.6",
    "content": "2.7.6 We are glad to announce that Spark NLP for Healthcare 2.7.6 has been released! Highlights: New pretrained Radiology Assertion Status model to assign Confirmed, Suspected, Negative assertion scopes to imaging findings or any clinical tests. Obfuscating the same sensitive information (patient or doctor name) with the same fake names across the same clinical note. Version compatibility checker for the pretrained clinical models and builds to keep up with the latest development efforts in production. Adding more English names to faker module in Deidentification. Updated &amp; improved clinical SentenceDetectorDL model. New upgrades on ner_deid_large and ner_deid_enriched NER models to cover more use cases with better resolutions. Adding more examples to workshop repo for Scala users to practice more on healthcare annotators. Bug fixes &amp; general improvements. 1. Radiology Assertion Status Model We trained a new assertion model to assign Confirmed, Suspected, Negative assertion scopes to imaging findings or any clinical tests. It will try to assign these statuses to any named entity you would feed to the assertion annotater in the same pipeline. radiology_assertion = AssertionDLModel.pretrained(&quot;assertion_dl_radiology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) text = Blunting of the left costophrenic angle on the lateral view posteriorly suggests a small left pleural effusion. No right-sided pleural effusion or pneumothorax is definitively seen. There are mildly displaced fractures of the left lateral 8th and likely 9th ribs. sentences chunk ner_label sent_id assertion Blunting of the left costophrenic angle on the lateral view posteriorly suggests a small left pleural effusion. Blunting ImagingFindings 0 Confirmed Blunting of the left costophrenic angle on the lateral view posteriorly suggests a small left pleural effusion. effusion ImagingFindings 0 Suspected No right-sided pleural effusion or pneumothorax is definitively seen. effusion ImagingFindings 1 Negative No right-sided pleural effusion or pneumothorax is definitively seen. pneumothorax ImagingFindings 1 Negative There are mildly displaced fractures of the left lateral 8th and likely 9th ribs. displaced fractures ImagingFindings 2 Confirmed You can also use this with AssertionFilterer to return clinical findings from a note only when it is i.e. confirmed or suspected. assertion_filterer = AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;assertion_filtered&quot;) .setWhiteList([&quot;confirmed&quot;,&quot;suspected&quot;]) &gt;&gt; [&quot;displaced fractures&quot;, &quot;effusion&quot;] 2. Obfuscating with the same fake name across the same note: obfuscation = DeIdentification() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_chunk&quot;]) .setOutputCol(&quot;deidentified&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateDate(True) .setSameEntityThreshold(0.8) .setObfuscateRefSource(&quot;faker&quot;) text =&#39;&#39;&#39; Provider: David Hale, M.D. Pt: Jessica Parker David told Jessica that she will need to visit the clinic next month.&#39;&#39;&#39;   sentence obfuscated 0 Provider: David Hale, M.D. Provider: Dennis Perez, M.D. 1 Pt: Jessica Parker Pt: Gerth Bayer 2 David told Jessica that she will need to visit the clinic next month. Dennis told Gerth that she will need to visit the clinic next month. 3. Library Version Compatibility Table : We are releasing the version compatibility table to help users get to see which Spark NLP licensed version is built against which core (open source) version. We are going to release a detailed one after running some tests across the jars from each library. Healthcare Public 2.7.6 2.7.4 2.7.5 2.7.4 2.7.4 2.7.3 2.7.3 2.7.3 2.7.2 2.6.5 2.7.1 2.6.4 2.7.0 2.6.3 2.6.2 2.6.2 2.6.0 2.6.0 2.5.5 2.5.5 2.5.3 2.5.3 2.5.2 2.5.2 2.5.0 2.5.0 2.4.7 2.4.5 2.4.6 2.4.5 2.4.5 2.4.5 2.4.2 2.4.2 2.4.1 2.4.1 2.4.0 2.4.0 2.3.6 2.3.6 2.3.5 2.3.5 2.3.4 2.3.4 4. Pretrained Models Version Control : Due to active release cycle, we are adding &amp; training new pretrained models at each release and it might be tricky to maintain the backward compatibility or keep up with the latest models, especially for the users using our models locally in air-gapped networks. We are releasing a new utility class to help you check your local &amp; existing models with the latest version of everything we have up to date. This is an highly experimental feature of which we plan to improve and add more capability later on. from sparknlp_jsl.check_compatibility import Compatibility checker = sparknlp_jsl.Compatibility() result = checker.find_version(aws_access_key_id=license_keys[&#39;AWS_ACCESS_KEY_ID&#39;], aws_secret_access_key=license_keys[&#39;AWS_SECRET_ACCESS_KEY&#39;], metadata_path=None, model = &#39;all&#39; , # or a specific model name target_version=&#39;all&#39;, cache_pretrained_path=&#39;/home/ubuntu/cache_pretrained&#39;) &gt;&gt; result[&#39;outdated_models&#39;] [{&#39;model_name&#39;: &#39;clinical_ner_assertion&#39;, &#39;current_version&#39;: &#39;2.4.0&#39;, &#39;latest_version&#39;: &#39;2.6.4&#39;}, {&#39;model_name&#39;: &#39;jsl_rd_ner_wip_greedy_clinical&#39;, &#39;current_version&#39;: &#39;2.6.1&#39;, &#39;latest_version&#39;: &#39;2.6.2&#39;}, {&#39;model_name&#39;: &#39;ner_anatomy&#39;, &#39;current_version&#39;: &#39;2.4.2&#39;, &#39;latest_version&#39;: &#39;2.6.4&#39;}, {&#39;model_name&#39;: &#39;ner_aspect_based_sentiment&#39;, &#39;current_version&#39;: &#39;2.6.2&#39;, &#39;latest_version&#39;: &#39;2.7.2&#39;}, {&#39;model_name&#39;: &#39;ner_bionlp&#39;, &#39;current_version&#39;: &#39;2.4.0&#39;, &#39;latest_version&#39;: &#39;2.7.0&#39;}, {&#39;model_name&#39;: &#39;ner_cellular&#39;, &#39;current_version&#39;: &#39;2.4.2&#39;, &#39;latest_version&#39;: &#39;2.5.0&#39;}] &gt;&gt; result[&#39;version_comparison_dict&#39;] [{&#39;clinical_ner_assertion&#39;: {&#39;current_version&#39;: &#39;2.4.0&#39;, &#39;latest_version&#39;: &#39;2.6.4&#39;}}, {&#39;jsl_ner_wip_clinical&#39;: {&#39;current_version&#39;: &#39;2.6.5&#39;, &#39;latest_version&#39;: &#39;2.6.1&#39;}}, {&#39;jsl_ner_wip_greedy_clinical&#39;: {&#39;current_version&#39;: &#39;2.6.5&#39;, &#39;latest_version&#39;: &#39;2.6.5&#39;}}, {&#39;jsl_ner_wip_modifier_clinical&#39;: {&#39;current_version&#39;: &#39;2.6.4&#39;, &#39;latest_version&#39;: &#39;2.6.4&#39;}}, {&#39;jsl_rd_ner_wip_greedy_clinical&#39;: {&#39;current_version&#39;: &#39;2.6.1&#39;,&#39;latest_version&#39;: &#39;2.6.2&#39;}}] 5. Updated Pretrained Models: (requires fresh .pretraned()) ner_deid_large ner_deid_enriched Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_6",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_2_7_6"
  },
  "182": {
    "id": "182",
    "title": "Annotation Lab Release Notes 2.8.0",
    "content": "2.8.0 Release date: 19-03-2022 Annotation Lab 2.8.0 simplifies the annotation workflows, adds dynamic pagination features, supports cross-page NER annotation and relation definition for text projects, adds UI features for infrastructure configuration and backup, updates the way the analytics dashboards are processed, offers improved support for rules and support for model training in German and Spanish. Highlights New features offered by Annotation Lab: Dynamic Task Pagination replaced the &lt;pagebreak&gt; style pagination. Cross Page Annotation is now supported for NER and Relation annotations. Simplified workflow are now supported for simpler projects. Furthermore, overall work progress has been added on the Labeling Page. Infrastucture used for preannotation and training can now be configured from the Annotation Lab UI. Support for training German and Spanish models. Some changes in Analytics Dashboard were implemented. By default, the Analytics dashboard page is now disabled. Users can request Admin to enable the Analytics page. The refresh of the charts is done manually. Import &amp; Export Rules from the Model Hub page. Download model dependencies is now automatic. The project configuration box can now be edited in full screen mode. Trim leading and ending spaces in annotated chunks. Reserved words cannot be used in project names. Task numbering now start from 1. ‘a’ was removed as hotkey for VisualNER multi-chunk selection. Going forward only use ‘shift’ key for chunk selection. Only alphanumeric characters can be used as the Task Tag Names. Allow the export of tasks without completions. Bug Fixes On the Labeling Page, the following issues related to completions were identified and fixed: In the Visual NER Project, when an annotator clicks on the Next button to load the next available task, the PDF was not correctly loaded and the text selection doesn’t work properly. Shortcut keys were not working when creating new completions. It happened that completions were no longer visible after creating relations. Previously, after each project configuration edit, when validating the correctness of the configuration the cursor position was reset to the end of the config file. The user had to manually move the cursor back to its previous position to continue editing. Now, the cursor position is saved so that the editing can continue with ease. Removing a user from the “UserAdmins” group was not possible. This has been fixed. Any user can be added or removed from the “UserAdmins”. In previous versions, choosing an already existing name for the current project did not show any error messages. Now, an error message appears on the right side of the screen asking users to choose another name for the project. In the previous version, when a user was deleted and a new user with the same name was created through Keycloak, on the next login the UI did not load. Now, this issue was fixed. Validations were added to Swagger API for completions data such that random values could not be added to completion data via API. Previously, when a rule was edited, previously deployed preannotation pipelines using the edited rules were not updated to use the latest version of the rule. Now the user is notified about the edited rule via an alert message “Redeploy preannotation server to apply these changes” on the rule edit form so that the users can redeploy the preannotation model. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_2_8_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_2_8_0"
  },
  "183": {
    "id": "183",
    "title": "Spark NLP for Healthcare Release Notes 3.0.0",
    "content": "3.0.0 We are very excited to announce that Spark NLP for Healthcare 3.0.0 has been released! This has been one of the biggest releases we have ever done and we are so proud to share this with our customers. Highlights: Spark NLP for Healthcare 3.0.0 extends the support for Apache Spark 3.0.x and 3.1.x major releases on Scala 2.12 with both Hadoop 2.7. and 3.2. We now support all 4 major Apache Spark and PySpark releases of 2.3.x, 2.4.x, 3.0.x, and 3.1.x helping the customers to migrate from earlier Apache Spark versions to newer releases without being worried about Spark NLP support. Highlights: Support for Apache Spark and PySpark 3.0.x on Scala 2.12 Support for Apache Spark and PySpark 3.1.x on Scala 2.12 Migrate to TensorFlow v2.3.1 with native support for Java to take advantage of many optimizations for CPU/GPU and new features/models introduced in TF v2.x A brand new MedicalNerModel annotator to train &amp; load the licensed clinical NER models. Two times faster NER and Entity Resolution due to new batch annotation technique. Welcoming 9x new Databricks runtimes to our Spark NLP family: Databricks 7.3 Databricks 7.3 ML GPU Databricks 7.4 Databricks 7.4 ML GPU Databricks 7.5 Databricks 7.5 ML GPU Databricks 7.6 Databricks 7.6 ML GPU Databricks 8.0 Databricks 8.0 ML (there is no GPU in 8.0) Databricks 8.1 Beta Welcoming 2x new EMR 6.x series to our Spark NLP family: EMR 6.1.0 (Apache Spark 3.0.0 / Hadoop 3.2.1) EMR 6.2.0 (Apache Spark 3.0.1 / Hadoop 3.2.1) Starting Spark NLP for Healthcare 3.0.0 the default packages for CPU and GPU will be based on Apache Spark 3.x and Scala 2.12. Deprecated Text2SQL annotator is deprecated and will not be maintained going forward. We are working on a better and faster version of Text2SQL at the moment and will announce soon. 1. MedicalNerModel Annotator Starting Spark NLP for Healthcare 3.0.0, the licensed clinical and biomedical pretrained NER models will only work with this brand new annotator called MedicalNerModel and will not work with NerDLModel in open source version. In order to make this happen, we retrained all the clinical NER models (more than 80) and uploaded to models hub. Example: clinical_ner = MedicalNerModel.pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) 2. Speed Improvements A new batch annotation technique implemented in Spark NLP 3.0.0 for NerDLModel,BertEmbeddings, and BertSentenceEmbeddings annotators will be reflected in MedicalNerModel and it improves prediction/inferencing performance radically. From now on the batchSize for these annotators means the number of rows that can be fed into the models for prediction instead of sentences per row. You can control the throughput when you are on accelerated hardware such as GPU to fully utilise it. Here are the overall speed comparison: Now, NER inference and Entity Resolution are two times faster on CPU and three times faster on GPU. 3. JSL Clinical NER Model We are releasing the richest clinical NER model ever, spanning over 80 entities. It has been under development for the last 6 months and we manually annotated more than 4000 clinical notes to cover such a high number of entities in a single model. It has 4 variants at the moment: jsl_ner_wip_clinical jsl_ner_wip_greedy_clinical jsl_ner_wip_modifier_clinical jsl_rd_ner_wip_greedy_clinical Entities: Kidney_Disease, HDL, Diet, Test, Imaging_Technique, Triglycerides, Obesity, Duration, Weight, Social_History_Header, ImagingTest, Labour_Delivery, Disease_Syndrome_Disorder, Communicable_Disease, Overweight, Units, Smoking, Score, Substance_Quantity, Form, Race_Ethnicity, Modifier, Hyperlipidemia, ImagingFindings, Psychological_Condition, OtherFindings, Cerebrovascular_Disease, Date, Test_Result, VS_Finding, Employment, Death_Entity, Gender, Oncological, Heart_Disease, Medical_Device, Total_Cholesterol, ManualFix, Time, Route, Pulse, Admission_Discharge, RelativeDate, O2_Saturation, Frequency, RelativeTime, Hypertension, Alcohol, Allergen, Fetus_NewBorn, Birth_Entity, Age, Respiration, Medical_History_Header, Oxygen_Therapy, Section_Header, LDL, Treatment, Vital_Signs_Header, Direction, BMI, Pregnancy, Sexually_Active_or_Sexual_Orientation, Symptom, Clinical_Dept, Measurements, Height, Family_History_Header, Substance, Strength, Injury_or_Poisoning, Relationship_Status, Blood_Pressure, Drug, Temperature, EKG_Findings, Diabetes, BodyPart, Vaccine, Procedure, Dosage 4. JSL Clinical Assertion Model We are releasing a brand new clinical assertion model, supporting 8 assertion statuses. jsl_assertion_wip Assertion Labels : Present, Absent, Possible, Planned, Someoneelse, Past, Family, Hypotetical 5. Library Version Compatibility Table : Spark NLP for Healthcare 3.0.0 is compatible with Spark NLP 3.0.1 6. Pretrained Models Version Control (Beta): Due to active release cycle, we are adding &amp; training new pretrained models at each release and it might be tricky to maintain the backward compatibility or keep up with the latest models, especially for the users using our models locally in air-gapped networks. We are releasing a new utility class to help you check your local &amp; existing models with the latest version of everything we have up to date. You will not need to specify your AWS credentials from now on. This is the second version of the model checker we released with 2.7.6 and will replace that soon. from sparknlp_jsl.compatibility_beta import CompatibilityBeta compatibility = CompatibilityBeta(spark) print(compatibility.findVersion(&quot;ner_deid&quot;)) 7. Updated Pretrained Models: (requires fresh .pretraned()) None Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_0_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_0_0"
  },
  "184": {
    "id": "184",
    "title": "Annotation Lab Release Notes 3.0.0",
    "content": "3.0.0 Release date: 06-04-2022 We are very excited to release Annotation Lab 3.0.0 with support for Floating Licenses and for parallel training and preannotation jobs, created on demand by Project Owners and Managers across various projects. Below are more details about the release. Highlights Annotation Lab now supports floating licenses with different scopes (ocr: training, ocr: inference, healthcare: inference, healthcare: training). Depending on the scope of the available license, users can perform model training and/or deploy preannotation servers. Licenses are a must only for training Spark NLP for Healthcare models and for deploying Spark NLP for Healthcare models as preannotation servers. Parallel Trainings and Preannotations. Annotation Lab now offers support for running model training and document preannotation across multiple projects and/or teams in parallel. If the infrastructure dedicated to the Annotation Lab includes sufficient resources, each team/project can run smoothly without being blocked. On demand deployment of preannotation servers and training jobs: Deploy a new training job Deploy a new preannotation server OCR and Visual NER servers The infrastucture page now hosts a new tab for managing preannotation, training and OCR servers. New options available on preannotate action. Updates for the license page. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_3_0_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_3_0_0"
  },
  "185": {
    "id": "185",
    "title": "Spark NLP release notes 3.0.0",
    "content": "3.0.0 Release date: 02-04-2021 Overview We are very excited to release Spark OCR 3.0.0! Spark OCR 3.0.0 extends the support for Apache Spark 3.0.x and 3.1.x major releases on Scala 2.12 with both Hadoop 2.7. and 3.2. We will support all 4 major Apache Spark and PySpark releases of 2.3.x, 2.4.x, 3.0.x, and 3.1.x. Spark OCR started to support Tensorflow models. First model is VisualDocumentClassifier. New Features Support for Apache Spark and PySpark 3.0.x on Scala 2.12 Support for Apache Spark and PySpark 3.1.x on Scala 2.12 Support 9x new Databricks runtimes: Databricks 7.3 Databricks 7.3 ML GPU Databricks 7.4 Databricks 7.4 ML GPU Databricks 7.5 Databricks 7.5 ML GPU Databricks 7.6 Databricks 7.6 ML GPU Databricks 8.0 Databricks 8.0 ML (there is no GPU in 8.0) Databricks 8.1 Support 2x new EMR 6.x: EMR 6.1.0 (Apache Spark 3.0.0 / Hadoop 3.2.1) EMR 6.2.0 (Apache Spark 3.0.1 / Hadoop 3.2.1) VisualDocumentClassifier model for classification documents using text and layout data. Added support Vietnamese language. New notebooks Visual Document Classifier Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_0_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_0_0"
  },
  "186": {
    "id": "186",
    "title": "Spark NLP for Healthcare Release Notes 3.0.1",
    "content": "3.0.1 We are very excited to announce that Spark NLP for Healthcare 3.0.1 has been released! Highlights: Fixed problem in Assertion Status internal tokenization (reported in Spark-NLP #2470). Fixes in the internal implementation of DeIdentificationModel/Obfuscator. Being able to disable the use of regexes in the Deidentification process Other minor bug fixes &amp; general improvements. DeIdentificationModel Annotator New seed parameter. Now we have the possibility of using a seed to guide the process of obfuscating entities and returning the same result across different executions. To make that possible a new method setSeed(seed:Int) was introduced. Example: Return obfuscated documents in a repeatable manner based on the same seed. Scala deIdentification = DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(true) Python de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(True) This seed controls how the obfuscated values are picked from a set of obfuscation candidates. Fixing the seed allows the process to be replicated. Example: Given the following input to the deidentification: &quot;David Hale was in Cocke County Baptist Hospital. David Hale&quot; If the annotator is set up with a seed of 10: Scala val deIdentification = new DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(true) Python de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(True) The result will be the following for any execution, &quot;Brendan Kitten was in New Megan.Brendan Kitten&quot; Now if we set up a seed of 32, Scala val deIdentification = new DeIdentification() .setInputCols(Array(&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(32) .setIgnoreRegex(true) Python de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateRefSource(&quot;faker&quot;) .setSeed(10) .setIgnoreRegex(True) The result will be the following for any execution, &quot;Louise Pear was in Lake Edward.Louise Pear&quot; New ignoreRegex parameter. You can now choose to completely disable the use of regexes in the deidentification process by setting the setIgnoreRegex param to True. Example: Scala DeIdentificationModel.setIgnoreRegex(true) Python DeIdentificationModel().setIgnoreRegex(True) The default value for this param is False meaning that regexes will be used by default. New supported entities for Deidentification &amp; Obfuscation: We added new entities to the default supported regexes: SSN - Social security number. PASSPORT - Passport id. DLN - Department of Labor Number. NPI - National Provider Identifier. C_CARD - The id number for credits card. IBAN - International Bank Account Number. DEA - DEA Registration Number, which is an identifier assigned to a health care provider by the United States Drug Enforcement Administration. We also introduced new Obfuscator cases for these new entities. Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_0_1",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_0_1"
  },
  "187": {
    "id": "187",
    "title": "Annotation Lab Release Notes 3.0.1",
    "content": "3.0.1 Release date: 12-04-2022 Annotation Lab v3.0.1 includes some CVE issues are fixed along with application bug fixes Bug Fixes When licensed model is trained, label “label” was added to prediction entities Expired license icon is seen after the user enters new floating license In airgaped machine, deployed licensed preannotation server is shown as open source in active-servers page Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_3_0_1",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_3_0_1"
  },
  "188": {
    "id": "188",
    "title": "Spark NLP for Healthcare Release Notes 3.0.2",
    "content": "3.0.2 We are very excited to announce that Spark NLP for Healthcare 3.0.2 has been released! This release includes bug fixes and some compatibility improvements. Highlights Dictionaries for Obfuscator were augmented with more than 10K names. Improved support for spark 2.3 and spark 2.4. Bug fixes in DrugNormalizer. New Features Provide confidence scores for all available tags in MedicalNerModel, MedicalNerModel before 3.0.2 [[named_entity, 0, 9, B-PROBLEM, [word -&gt; Pneumonia, confidence -&gt; 0.9998], []] Now in Spark NLP for Healthcare 3.0.2 [[named_entity, 0, 9, B-PROBLEM, [B-PROBLEM -&gt; 0.9998, I-TREATMENT -&gt; 0.0, I-PROBLEM -&gt; 0.0, I-TEST -&gt; 0.0, B-TREATMENT -&gt; 1.0E-4, word -&gt; Pneumonia, B-TEST -&gt; 0.0], []] Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_0_2",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_0_2"
  },
  "189": {
    "id": "189",
    "title": "Spark NLP for Healthcare Release Notes 3.0.3",
    "content": "3.0.3 We are glad to announce that Spark NLP for Healthcare 3.0.3 has been released! Highlights Five new entity resolution models to cover UMLS, HPO and LIONC terminologies. New feature for random displacement of dates on deidentification model. Five new pretrained pipelines to map terminologies across each other (from UMLS to ICD10, from RxNorm to MeSH etc.) AnnotationToolReader support for Spark 2.3. The tool that helps model training on Spark-NLP to leverage data annotated using JSL Annotation Tool now has support for Spark 2.3. Updated documentation (Scaladocs) covering more APIs, and examples. Five new resolver models: sbiobertresolve_umls_major_concepts: This model returns CUI (concept unique identifier) codes for Clinical Findings, Medical Devices, Anatomical Structures and Injuries &amp; Poisoning terms. sbiobertresolve_umls_findings: This model returns CUI (concept unique identifier) codes for 200K concepts from clinical findings. sbiobertresolve_loinc: Map clinical NER entities to LOINC codes using sbiobert. sbluebertresolve_loinc: Map clinical NER entities to LOINC codes using sbluebert. sbiobertresolve_HPO: This model returns Human Phenotype Ontology (HPO) codes for phenotypic abnormalities encountered in human diseases. It also returns associated codes from the following vocabularies for each HPO code: * MeSH (Medical Subject Headings) * SNOMED * UMLS (Unified Medical Language System ) * ORPHA (international reference resource for information on rare diseases and orphan drugs) * OMIM (Online Mendelian Inheritance in Man) Related Notebook: Resolver Models New feature on Deidentification Module isRandomDateDisplacement(True): Be able to apply a random displacement on obfuscation dates. The randomness is based on the seed. Fix random dates when the format is not correct. Now you can repeat an execution using a seed for dates. Random dates will be based on the seed. Five new healthcare code mapping pipelines: icd10cm_umls_mapping: This pretrained pipeline maps ICD10CM codes to UMLS codes without using any text data. You’ll just feed white space-delimited ICD10CM codes and it will return the corresponding UMLS codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;icd10cm&#39;: [&#39;M89.50&#39;, &#39;R82.2&#39;, &#39;R09.01&#39;], &#39;umls&#39;: [&#39;C4721411&#39;, &#39;C0159076&#39;, &#39;C0004044&#39;]} mesh_umls_mapping: This pretrained pipeline maps MeSH codes to UMLS codes without using any text data. You’ll just feed white space-delimited MeSH codes and it will return the corresponding UMLS codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;mesh&#39;: [&#39;C028491&#39;, &#39;D019326&#39;, &#39;C579867&#39;], &#39;umls&#39;: [&#39;C0970275&#39;, &#39;C0886627&#39;, &#39;C3696376&#39;]} rxnorm_umls_mapping: This pretrained pipeline maps RxNorm codes to UMLS codes without using any text data. You’ll just feed white space-delimited RxNorm codes and it will return the corresponding UMLS codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;rxnorm&#39;: [&#39;1161611&#39;, &#39;315677&#39;, &#39;343663&#39;], &#39;umls&#39;: [&#39;C3215948&#39;, &#39;C0984912&#39;, &#39;C1146501&#39;]} rxnorm_mesh_mapping: This pretrained pipeline maps RxNorm codes to MeSH codes without using any text data. You’ll just feed white space-delimited RxNorm codes and it will return the corresponding MeSH codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;rxnorm&#39;: [&#39;1191&#39;, &#39;6809&#39;, &#39;47613&#39;], &#39;mesh&#39;: [&#39;D001241&#39;, &#39;D008687&#39;, &#39;D019355&#39;]} snomed_umls_mapping: This pretrained pipeline maps SNOMED codes to UMLS codes without using any text data. You’ll just feed white space-delimited SNOMED codes and it will return the corresponding UMLS codes as a list. If there is no mapping, the original code is returned with no mapping. {&#39;snomed&#39;: [&#39;733187009&#39;, &#39;449433008&#39;, &#39;51264003&#39;], &#39;umls&#39;: [&#39;C4546029&#39;, &#39;C3164619&#39;, &#39;C0271267&#39;]} Related Notebook: Healthcare Code Mapping Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_0_3",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_0_3"
  },
  "190": {
    "id": "190",
    "title": "Spark NLP release notes 3.10.0",
    "content": "3.10.0 Release date: 10-01-2022 Overview Form recognition using LayoutLMv2 and text detection. New Features Added VisualDocumentNERv2 transformer Added DL based ImageTextDetector transformer Support rotated regions in ImageSplitRegions Support rotated regions in ImageDrawRegions New Models LayoutLMv2 fine-tuned on FUNSD dataset Text detection model based on CRAFT architecture New notebooks Text Detection Visual Document NER v2 Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_10_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_10_0"
  },
  "191": {
    "id": "191",
    "title": "Spark NLP release notes 3.11.0",
    "content": "3.11.0 Release date: 28-02-2022 Overview We are glad to announce that Spark OCR 3.11.0 has been released!. This release comes with new models, new features, bug fixes, and notebook examples. New Features Added ImageTextDetectorV2 Python Spark-OCR Transformer for detecting printed and handwritten text using CRAFT architecture with Refiner Net. Added ImageTextRecognizerV2 Python Spark-OCR Transformer for recognizing printed and handwritten text based on Deep Learning Transformer Architecture. Added FormRelationExtractor for detecting relations between key and value entities in forms. Added the capability of fine tuning VisualDocumentNerV2 models for key-value pairs extraction. New Models ImageTextDetectorV2: this extends the ImageTextDetectorV1 character level text detection model with a refiner net architecture. ImageTextRecognizerV2: Text recognition for printed text based on the Deep Learning Transformer Architecture. New notebooks SparkOcrImageToTextV2 ImageTextDetectorV2 Visual Document NER v2 SparkOcrFormRecognition SparkOCRVisualDocumentNERv2FineTune Creating Rest a API with Synapse to extract text from images, SparkOcrRestApi Creating Rest a API with Synapse to extract text from PDFs, SparkOcrRestApiPdf Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_11_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_11_0"
  },
  "192": {
    "id": "192",
    "title": "Spark NLP release notes 3.12.0",
    "content": "3.12.0 Release date: 14-04-2022 Overview We’re glad to announce that Spark OCR 3.12.0 has been released! This release comes with new models for Handwritten Text Recognition, Spark 3.2 support, bug fixes, and notebook examples. New Features Added to the ImageTextDetectorV2: New parameter ‘mergeIntersects’: merge bounding boxes corresponding to detected text regions, when multiple bounding boxes that belong to the same text line overlap. New parameter ‘forceProcessing’: now you can force processing of the results to avoid repeating the computation of results in pipelines where the same results are consumed by different transformers. New feature: sizeThreshold parameter sets the expected size for the recognized text. From now on, text size will be automatically detected when sizeThreshold is set to -1. Added to the ImageToTextV2: New parameter ‘usePandasUdf’: support PandasUdf to allow batch processing internally. New support for formatted output, and HOCR. ocr.setOutputFormat(OcrOutputFormat.HOCR) ocr.setOutputFormat(OcrOutputFormat.FORMATTED_TEXT) Support for Spark 3.2: We added support for the latest Spark version, check installation instructions below. Known problems &amp; workarounds: SPARK-38330: S3 access issues, there’s a workaround using the following settings, //Scala spark.sparkContext.hadoopConfiguration.set(&quot;fs.s3a.path.style.access&quot;, &quot;true&quot;) #Python spark.sparkContext._jsc.hadoopConfiguration().set(&quot;fs.s3a.path.style.access&quot;, &quot;true&quot;) SPARK-37577: changes in default behavior of query optimizer, it is already handled in start() function, or if you start the context manually, setting the following Spark properties, #Python spark.conf.set(&quot;spark.sql.optimizer.expression.nestedPruning.enabled&quot;, False) spark.conf.set(&quot;spark.sql.optimizer.nestedSchemaPruning.enabled&quot;, False) Improved documentation on the website. New Models ocr_small_printed: Text recognition small model for printed text based on ImageToTextV2 ocr_small_handwritten: Text recognition small model for handwritten text based on ImageToTextV2 ocr_base_handwritten: Text recognition base model for handwritten text based on ImageToTextV2 Bug Fixes display_table() function failing to display tables coming from digital PDFs. New notebooks SparkOcrImageToTextV2OutputFormats.ipynb, different output formats for ImageToTextV2. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_12_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_12_0"
  },
  "193": {
    "id": "193",
    "title": "Spark NLP release notes 3.13.0",
    "content": "3.13.0 Release date: 25-05-2022 We are glad to announce that Spark OCR 3.13.0 has been released!. This release focuses around VisualDocumentNer models, adding ability to fine-tune, fixing bugs, and to leverage the Annotation Lab to generate training data. New Features VisualDocumentNerV21: Now you can fine tune models VisualDocumentNerV21 models on your own dataset. AlabReaders: New class to allow training data from the Annotation Lab to be imported into Spark OCR. Currently, the reader supports Visual Ner only. Bug Fixes Feature extraction on VisualDocumentNer has been improved. New notebooks SparkOcrFormRecognitionFineTuning.ipynb, end to end example on Visual Document Ner Fine-Tuning. Databricks notebooks on Github Spark-OCR Workshop repository have been updated, and fixed. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_13_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_13_0"
  },
  "194": {
    "id": "194",
    "title": "Spark NLP release notes 3.14.0",
    "content": "3.14.0 Release date: 13-06-2022 Overview We are glad to announce that Spark OCR 3.14.0 has been released!. This release focuses around Visual Document Classification models, native Image Preprocessing on the JVM, and bug fixes. New Features VisualDocumentClassifierv2: New annotator for classifying documents based on multimodal(text + images) features. VisualDocumentClassifierv3: New annotator for classifying documents based on image features. ImageTransformer: New transformer that provides different image transformations on the JVM. Supported transforms are Scaling, Adaptive Thresholding, Median Blur, Dilation, Erosion, and Object Removal. New notebooks SparkOCRVisualDocumentClassifierv2.ipynb, example of Visual Document Classification using multimodal (text + visual) features. SparkOCRVisualDocumentClassifierv3.ipynb, example of Visual Document Classification using only visual features. SparkOCRCPUImageOperations.ipynb, example of ImageTransformer. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_14_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_14_0"
  },
  "195": {
    "id": "195",
    "title": "Spark NLP for Healthcare Release Notes 3.1.0",
    "content": "3.1.0 We are glad to announce that Spark NLP for Healthcare 3.1.0 has been released! Highlights Improved load time &amp; memory consumption for SentenceResolver models. New JSL Bert Models. JSL SBert Model Speed Benchmark. New ICD10CM resolver models. New Deidentification NER models. New column returned in DeidentificationModel New Reidentification feature New Deidentification Pretrained Pipelines Chunk filtering based on confidence Extended regex dictionary fuctionallity in Deidentification Enhanced RelationExtractionDL Model to create and identify relations between entities across the entire document MedicalNerApproach can now accept a graph file directly. MedicalNerApproach can now accept a user-defined name for log file. More improvements in Scaladocs. Bug fixes in Deidentification module. New notebooks. Sentence Resolver Models load time improvement Sentence resolver models now have faster load times, with a speedup of about 6X when compared to previous versions. Also, the load process now is more memory friendly meaning that the maximum memory required during load time is smaller, reducing the chances of OOM exceptions, and thus relaxing hardware requirements. New JSL SBert Models We trained new sBert models in TF2 and fined tuned on MedNLI, NLI and UMLS datasets with various parameters to cover common NLP tasks in medical domain. You can find the details in the following table. sbiobert_jsl_cased sbiobert_jsl_umls_cased sbert_jsl_medium_uncased sbert_jsl_medium_umls_uncased sbert_jsl_mini_uncased sbert_jsl_mini_umls_uncased sbert_jsl_tiny_uncased sbert_jsl_tiny_umls_uncased JSL SBert Model Speed Benchmark JSL SBert Model Base Model Is Cased Train Datasets Inference speed (100 rows) sbiobert_jsl_cased biobert_v1.1_pubmed Cased medNLI, allNLI 274,53 sbiobert_jsl_umls_cased biobert_v1.1_pubmed Cased medNLI, allNLI, umls 274,52 sbert_jsl_medium_uncased uncased_L-8_H-512_A-8 Uncased medNLI, allNLI 80,40 sbert_jsl_medium_umls_uncased uncased_L-8_H-512_A-8 Uncased medNLI, allNLI, umls 78,35 sbert_jsl_mini_uncased uncased_L-4_H-256_A-4 Uncased medNLI, allNLI 10,68 sbert_jsl_mini_umls_uncased uncased_L-4_H-256_A-4 Uncased medNLI, allNLI, umls 10,29 sbert_jsl_tiny_uncased uncased_L-2_H-128_A-2 Uncased medNLI, allNLI 4,54 sbert_jsl_tiny_umls_uncased uncased_L-2_H-128_A-2 Uncased medNLI, allNL, umls 4,54 New ICD10CM resolver models: These models map clinical entities and concepts to ICD10 CM codes using sentence bert embeddings. They also return the official resolution text within the brackets inside the metadata. Both models are augmented with synonyms, and previous augmentations are flexed according to cosine distances to unnormalized terms (ground truths). sbiobertresolve_icd10cm_slim_billable_hcc: Trained with classic sbiobert mli. (sbiobert_base_cased_mli) Models Hub Page : https://nlp.johnsnowlabs.com/2021/05/25/sbiobertresolve_icd10cm_slim_billable_hcc_en.html sbertresolve_icd10cm_slim_billable_hcc_med: Trained with new jsl sbert(sbert_jsl_medium_uncased) Models Hub Page : https://nlp.johnsnowlabs.com/2021/05/25/sbertresolve_icd10cm_slim_billable_hcc_med_en.html Example: ‘bladder cancer’ sbiobertresolve_icd10cm_augmented_billable_hcc chunks code all_codes resolutions all_distances 100x Loop(sec) bladder cancer C679 [C679, Z126, D090, D494, C7911] [bladder cancer, suspected bladder cancer, cancer in situ of urinary bladder, tumor of bladder neck, malignant tumour of bladder neck] [0.0000, 0.0904, 0.0978, 0.1080, 0.1281] 26,9 ` sbiobertresolve_icd10cm_slim_billable_hcc` chunks code all_codes resolutions all_distances 100x Loop(sec) bladder cancer D090 [D090, D494, C7911, C680, C679] [cancer in situ of urinary bladder [Carcinoma in situ of bladder], tumor of bladder neck [Neoplasm of unspecified behavior of bladder], malignant tumour of bladder neck [Secondary malignant neoplasm of bladder], carcinoma of urethra [Malignant neoplasm of urethra], malignant tumor of urinary bladder [Malignant neoplasm of bladder, unspecified]] [0.0978, 0.1080, 0.1281, 0.1314, 0.1284] 20,9 sbertresolve_icd10cm_slim_billable_hcc_med chunks code all_codes resolutions all_distances 100x Loop(sec) bladder cancer C671 [C671, C679, C61, C672, C673] [bladder cancer, dome [Malignant neoplasm of dome of bladder], cancer of the urinary bladder [Malignant neoplasm of bladder, unspecified], prostate cancer [Malignant neoplasm of prostate], cancer of the urinary bladder] [0.0894, 0.1051, 0.1184, 0.1180, 0.1200] 12,8 New Deidentification NER Models We trained four new NER models to find PHI data (protected health information) that may need to be deidentified. ner_deid_generic_augmented and ner_deid_subentity_augmented models are trained with a combination of 2014 i2b2 Deid dataset and in-house annotations as well as some augmented version of them. Compared to the same test set coming from 2014 i2b2 Deid dataset, we achieved a better accuracy and generalisation on some entity labels as summarised in the following tables. We also trained the same models with glove_100d embeddings to get more memory friendly versions. ner_deid_generic_augmented : Detects PHI 7 entities (DATE, NAME, LOCATION, PROFESSION, CONTACT, AGE, ID). Models Hub Page : https://nlp.johnsnowlabs.com/2021/06/01/ner_deid_generic_augmented_en.html entity ner_deid_large (v3.0.3 and before) ner_deid_generic_augmented (v3.1.0) CONTACT 0.8695 0.9592 NAME 0.9452 0.9648 DATE 0.9778 0.9855 LOCATION 0.8755 0.923 ner_deid_subentity_augmented: Detects PHI 23 entities (MEDICALRECORD, ORGANIZATION, DOCTOR, USERNAME, PROFESSION, HEALTHPLAN, URL, CITY, DATE, LOCATION-OTHER, STATE, PATIENT, DEVICE, COUNTRY, ZIP, PHONE, HOSPITAL, EMAIL, IDNUM, SREET, BIOID, FAX, AGE) Models Hub Page : https://nlp.johnsnowlabs.com/2021/06/01/ner_deid_subentity_augmented_en.html entity ner_deid_enriched (v3.0.3 and before) ner_deid_subentity_augmented (v3.1.0) HOSPITAL 0.8519 0.8983 DATE 0.9766 0.9854 CITY 0.7493 0.8075 STREET 0.8902 0.9772 ZIP 0.8 0.9504 PHONE 0.8615 0.9502 DOCTOR 0.9191 0.9347 AGE 0.9416 0.9469 ner_deid_generic_glove: Small version of ner_deid_generic_augmented and detects 7 entities. ner_deid_subentity_glove: Small version of ner_deid_subentity_augmented and detects 23 entities. Example: Scala ... val deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;ner&quot;) ... val nlpPipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, word_embeddings, deid_ner, ner_converter)) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) val result = pipeline.fit(Seq.empty[&quot;A. Record date : 2093-01-13, David Hale, M.D., Name : Hendrickson, Ora MR. # 7194334 Date : 01/13/93 PCP : Oliveira, 25 -year-old, Record date : 1-11-2000. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227.&quot;].toDS.toDF(&quot;text&quot;)).transform(data) Python ... deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, word_embeddings, deid_ner, ner_converter]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame(pd.DataFrame({&quot;text&quot;: [&quot;&quot;&quot;A. Record date : 2093-01-13, David Hale, M.D., Name : Hendrickson, Ora MR. # 7194334 Date : 01/13/93 PCP : Oliveira, 25 -year-old, Record date : 1-11-2000. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227.&quot;&quot;&quot;]}))) Results: +--+-+ |chunk |ner_label | +--+-+ |2093-01-13 |DATE | |David Hale |DOCTOR | |Hendrickson, Ora |PATIENT | |7194334 |MEDICALRECORD| |01/13/93 |DATE | |Oliveira |DOCTOR | |25-year-old |AGE | |1-11-2000 |DATE | |Cocke County Baptist Hospital|HOSPITAL | |0295 Keats Street. |STREET | |(302) 786-5227 |PHONE | |Brothers Coal-Mine |ORGANIZATION | +--+-+ New column returned in DeidentificationModel DeidentificationModel now can return a new column to save the mappings between the mask/obfuscated entities and original entities. This column is optional and you can set it up with the .setReturnEntityMappings(True) method. The default value is False. Also, the name for the column can be changed using the following method; .setMappingsColumn(&quot;newAlternativeName&quot;) The new column will produce annotations with the following structure, Annotation( type: chunk, begin: 17, end: 25, result: 47, metadata:{ originalChunk -&gt; 01/13/93 //Original text of the chunk chunk -&gt; 0 // The number of the chunk in the sentence beginOriginalChunk -&gt; 95 // Start index of the original chunk endOriginalChunk -&gt; 102 // End index of the original chunk entity -&gt; AGE // Entity of the chunk sentence -&gt; 2 // Number of the sentence } ) New Reidentification feature With the new ReidetificationModel, the user can go back to the original sentences using the mappings columns and the deidentification sentences. Example: Scala val redeidentification = new ReIdentification() .setInputCols(Array(&quot;mappings&quot;, &quot;deid_chunks&quot;)) .setOutputCol(&quot;original&quot;) Python reDeidentification = ReIdentification() .setInputCols([&quot;mappings&quot;,&quot;deid_chunks&quot;]) .setOutputCol(&quot;original&quot;) New Deidentification Pretrained Pipelines We developed a clinical_deidentification pretrained pipeline that can be used to deidentify PHI information from medical texts. The PHI information will be masked and obfuscated in the resulting text. The pipeline can mask and obfuscate AGE, CONTACT, DATE, ID, LOCATION, NAME, PROFESSION, CITY, COUNTRY, DOCTOR, HOSPITAL, IDNUM, MEDICALRECORD, ORGANIZATION, PATIENT, PHONE, PROFESSION, STREET, USERNAME, ZIP, ACCOUNT, LICENSE, VIN, SSN, DLN, PLATE, IPADDR entities. Models Hub Page : clinical_deidentification There is also a lightweight version of the same pipeline trained with memory efficient glove_100dembeddings. Here are the model names: clinical_deidentification clinical_deidentification_glove Example: Python: from sparknlp.pretrained import PretrainedPipeline deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;, &quot;en&quot;, &quot;clinical/models&quot;) deid_pipeline.annotate(&quot;Record date : 2093-01-13, David Hale, M.D. IP: 203.120.223.13. The driver&#39;s license no:A334455B. the SSN:324598674 and e-mail: hale@gmail.com. Name : Hendrickson, Ora MR. # 719435 Date : 01/13/93. PCP : Oliveira, 25 years-old. Record date : 2079-11-09, Patient&#39;s VIN : 1HGBH41JXMN109286.&quot;) Scala: import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;,&quot;en&quot;,&quot;clinical/models&quot;) val result = deid_pipeline.annotate(&quot;Record date : 2093-01-13, David Hale, M.D. IP: 203.120.223.13. The driver&#39;s license no:A334455B. the SSN:324598674 and e-mail: hale@gmail.com. Name : Hendrickson, Ora MR. # 719435 Date : 01/13/93. PCP : Oliveira, 25 years-old. Record date : 2079-11-09, Patient&#39;s VIN : 1HGBH41JXMN109286.&quot;) Result: {&#39;sentence&#39;: [&#39;Record date : 2093-01-13, David Hale, M.D.&#39;, &#39;IP: 203.120.223.13.&#39;, &#39;The driver&#39;s license no:A334455B.&#39;, &#39;the SSN:324598674 and e-mail: hale@gmail.com.&#39;, &#39;Name : Hendrickson, Ora MR. # 719435 Date : 01/13/93.&#39;, &#39;PCP : Oliveira, 25 years-old.&#39;, &#39;Record date : 2079-11-09, Patient&#39;s VIN : 1HGBH41JXMN109286.&#39;], &#39;masked&#39;: [&#39;Record date : &lt;DATE&gt;, &lt;DOCTOR&gt;, M.D.&#39;, &#39;IP: &lt;IPADDR&gt;.&#39;, &#39;The driver&#39;s license &lt;DLN&gt;.&#39;, &#39;the &lt;SSN&gt; and e-mail: &lt;EMAIL&gt;.&#39;, &#39;Name : &lt;PATIENT&gt; MR. # &lt;MEDICALRECORD&gt; Date : &lt;DATE&gt;.&#39;, &#39;PCP : &lt;DOCTOR&gt;, &lt;AGE&gt; years-old.&#39;, &#39;Record date : &lt;DATE&gt;, Patient&#39;s VIN : &lt;VIN&gt;.&#39;], &#39;obfuscated&#39;: [&#39;Record date : 2093-01-18, Dr Alveria Eden, M.D.&#39;, &#39;IP: 001.001.001.001.&#39;, &#39;The driver&#39;s license K783518004444.&#39;, &#39;the SSN-400-50-8849 and e-mail: Merilynn@hotmail.com.&#39;, &#39;Name : Charls Danger MR. # J3366417 Date : 01-18-1974.&#39;, &#39;PCP : Dr Sina Sewer, 55 years-old.&#39;, &#39;Record date : 2079-11-23, Patient&#39;s VIN : 6ffff55gggg666777.&#39;], &#39;ner_chunk&#39;: [&#39;2093-01-13&#39;, &#39;David Hale&#39;, &#39;no:A334455B&#39;, &#39;SSN:324598674&#39;, &#39;Hendrickson, Ora&#39;, &#39;719435&#39;, &#39;01/13/93&#39;, &#39;Oliveira&#39;, &#39;25&#39;, &#39;2079-11-09&#39;, &#39;1HGBH41JXMN109286&#39;]} Chunk filtering based on confidence We added a new annotator ChunkFiltererApproach that allows to load a csv with both entities and confidence thresholds. This annotator will produce a ChunkFilterer model. You can load the dictionary with the following property setEntitiesConfidenceResource(). An example dictionary is: TREATMENT,0.7 With that dictionary, the user can filter the chunks corresponding to treatment entities which have confidence lower than 0.7. Example: We have a ner_chunk column and sentence column with the following data: Ner_chunk |[{chunk, 141, 163, the genomicorganization, {entity -&gt; TREATMENT, sentence -&gt; 0, chunk -&gt; 0, confidence -&gt; 0.57785}, []}, {chunk, 209, 267, a candidate gene forType II diabetes mellitus, {entity -&gt; PROBLEM, sentence -&gt; 0, chunk -&gt; 1, confidence -&gt; 0.6614286}, []}, {chunk, 394, 408, byapproximately, {entity -&gt; TREATMENT, sentence -&gt; 1, chunk -&gt; 2, confidence -&gt; 0.7705}, []}, {chunk, 478, 508, single nucleotide polymorphisms, {entity -&gt; TREATMENT, sentence -&gt; 2, chunk -&gt; 3, confidence -&gt; 0.7204666}, []}, {chunk, 559, 581, aVal366Ala substitution, {entity -&gt; TREATMENT, sentence -&gt; 2, chunk -&gt; 4, confidence -&gt; 0.61505}, []}, {chunk, 588, 601, an 8 base-pair, {entity -&gt; TREATMENT, sentence -&gt; 2, chunk -&gt; 5, confidence -&gt; 0.29226667}, []}, {chunk, 608, 625, insertion/deletion, {entity -&gt; PROBLEM, sentence -&gt; 3, chunk -&gt; 6, confidence -&gt; 0.9841}, []}]| +- Sentence [{document, 0, 298, The human KCNJ9 (Kir 3.3, GIRK3) is a member of the G-protein-activated inwardly rectifying potassium (GIRK) channel family.Here we describe the genomicorganization of the KCNJ9 locus on chromosome 1q21-23 as a candidate gene forType II diabetes mellitus in the Pima Indian population., {sentence -&gt; 0}, []}, {document, 300, 460, The gene spansapproximately 7.6 kb and contains one noncoding and two coding exons ,separated byapproximately 2.2 and approximately 2.6 kb introns, respectively., {sentence -&gt; 1}, []}, {document, 462, 601, We identified14 single nucleotide polymorphisms (SNPs), including one that predicts aVal366Ala substitution, and an 8 base-pair, {sentence -&gt; 2}, []}, {document, 603, 626, (bp) insertion/deletion., {sentence -&gt; 3}, []}] We can filter the entities using the following annotator: chunker_filter = ChunkFiltererApproach().setInputCols(&quot;sentence&quot;, &quot;ner_chunk&quot;) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;regex&quot;) .setRegex([&quot;.*&quot;]) .setEntitiesConfidenceResource(&quot;entities_confidence.csv&quot;) Where entities-confidence.csv has the following data: TREATMENT,0.7 PROBLEM,0.9 We can use that chunk_filter: chunker_filter.fit(data).transform(data) Producing the following entities: |[{chunk, 394, 408, byapproximately, {entity -&gt; TREATMENT, sentence -&gt; 1, chunk -&gt; 2, confidence -&gt; 0.7705}, []}, {chunk, 478, 508, single nucleotide polymorphisms, {entity -&gt; TREATMENT, sentence -&gt; 2, chunk -&gt; 3, confidence -&gt; 0.7204666}, []}, {chunk, 608, 625, insertion/deletion, {entity -&gt; PROBLEM, sentence -&gt; 3, chunk -&gt; 6, confidence -&gt; 0.9841}, []}]| As you can see, only the treatment entities with confidence score of more than 0.7, and the problem entities with confidence score of more than 0.9 have been kept in the output. Extended regex dictionary fuctionallity in Deidentification The RegexPatternsDictionary can now use a regex that spawns the 2 previous token and the 2 next tokens. That feature is implemented using regex groups. Examples: Given the sentence The patient with ssn 123123123 we can use the following regex to capture the entitty ssn ( d{9}) Given the sentence The patient has 12 years we can use the following regex to capture the entitty ( d{2}) years Enhanced RelationExtractionDL Model to create and identify relations between entities across the entire document A new option has been added to RENerChunksFilter to support pairing entities from different sentences using .setDocLevelRelations(True), to pass to the Relation Extraction Model. The RelationExtractionDL Model has also been updated to process document-level relations. How to use: re_dl_chunks = RENerChunksFilter() .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setDocLevelRelations(True) .setMaxSyntacticDistance(7) .setOutputCol(&quot;redl_ner_chunks&quot;) Examples: Given a document containing multiple sentences: John somkes cigrettes. He also consumes alcohol., now we can generate relation pairs across sentences and relate alcohol with John . Set NER graph explicitely in MedicalNerApproach Now MedicalNerApproach can receives the path to the graph directly. When a graph location is provided through this method, previous graph search behavior is disabled. MedicalNerApproach.setGraphFile(graphFilePath) MedicalNerApproach can now accept a user-defined name for log file. Now MedicalNerApproach can accept a user-defined name for the log file. If not such a name is provided, the conventional naming will take place. MedicalNerApproach.setLogPrefix(&quot;oncology_ner&quot;) This will result in oncology_ner_20210605_141701.log filename being used, in which the 20210605_141701 is a timestamp. New Notebooks A new notebook to reproduce our peer-reviewed NER paper (https://arxiv.org/abs/2011.06315) New databricks case study notebooks. In these notebooks, we showed the examples of how to work with oncology notes dataset and OCR on databricks for both DBr and community edition versions. Updated Resolver Models We updated sbiobertresolve_snomed_findings and sbiobertresolve_cpt_procedures_augmented resolver models to reflect the latest changes in the official terminologies. Getting Started with Spark NLP for Healthcare Notebook in Databricks We prepared a new notebook for those who want to get started with Spark NLP for Healthcare in Databricks : Getting Started with Spark NLP for Healthcare Notebook Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_1_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_1_0"
  },
  "196": {
    "id": "196",
    "title": "Annotation Lab Release Notes 3.1.0",
    "content": "3.1.0 Release date: 04-05-2022 We are very excited to release Annotation Lab v3.1.0 which includes support for training large documents, improvements for Visual NER Projects, security fixes and stabilizations. Here are the highlights: Highlights Support Training of large documents. Spark NLP feature called Memory Optimization Approach is enabled when the training data is greater then 5MB which enables training of model on machines with lower memory resources. Improvements in Visual NER Projects: Users can provide title in the input JSON along with the URL for tasks to import. This sets the title of the task accordingly. JSON export for the Visual NER projects contains both chunk and token-level annotations. Sample tasks can be imported into the Visual NER project using any available OCR server (created by another project). Multi-chunk annotation can be done without changing the start token when the end token is the last word on the document. For Visual NER project, users can export tasks in the VOC format for multi-page tasks with/without completions. During restoring backup file in the previous versions, the SECRETS (kubernetes) of the old machine needed manual transfer to the target machine. With v3.1.0, all the SECRETS are backed-up automatically along with database backup and hence they are restored without any hassle. Integration with my.johnsnowlabs.com, this means the available licenses can be easily imported by Admin users of Annotation Lab without having to download or copy them manually. The maximum number of words/tokens that can be set in a single page in labeling screen is now limited to 1000. For a large number of multiple relations, the previous version of Annotation Lab used Prev and Next identifiers which was not optimal for mapping to the correct pairs. For increased usability and clarity , the pair connections now use numerical values. While creating new (Contextual Parser) Rules using dictionary, the uploaded CSV file is validated based on: CSV should not contain any null values, CSV should either be a single row or single column. Admin users are now able to remove unused licenses. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_3_1_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_3_1_0"
  },
  "197": {
    "id": "197",
    "title": "Spark NLP release notes 3.1.0",
    "content": "3.1.0 Release date: 16-04-2021 Overview Image processing on GPU. It is in 3.5 times faster than on CPU. More details please read in GPU image preprocessing in Spark OCR New Features GPUImageTransformer with support: scaling, erosion, delation, Otsu and Huang thresholding. Added display_images util function for displaying images from Spark DataFrame in Jupyter notebooks. Enhancements Improve display_image util function. Bug fixes Fixed issue with extra dependencies in start function New notebooks GPU image processing Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_1_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_1_0"
  },
  "198": {
    "id": "198",
    "title": "Spark NLP for Healthcare Release Notes 3.1.1",
    "content": "3.1.1 We are glad to announce that Spark NLP for Healthcare 3.1.1 has been released! Highlights MedicalNerModel new parameter includeAllConfidenceScores. MedicalNerModel new parameter inferenceBatchSize. New Resolver Models Updated Resolver Models Getting Started with Spark NLP for Healthcare Notebook in Databricks MedicalNer new parameter includeAllConfidenceScores You can now customize whether you will require confidence score for every token(both entities and non-entities) at the output of the MedicalNerModel, or just for the tokens recognized as entities. MedicalNerModel new parameter inferenceBatchSize You can now control the batch size used during inference as a separate parameter from the one you used during training of the model. This can be useful in the situation in which the hardware on which you run inference has different capacity. For example, when you have lower available memory during inference, you can reduce the batch size. New Resolver Models We trained three new sentence entity resolver models. sbertresolve_snomed_bodyStructure_med and sbiobertresolve_snomed_bodyStructure models map extracted medical (anatomical structures) entities to Snomed codes (body structure version). sbertresolve_snomed_bodyStructure_med : Trained with using sbert_jsl_medium_uncased embeddings. sbiobertresolve_snomed_bodyStructure : Trained with using sbiobert_base_cased_mli embeddings. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) jsl_sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbert_jsl_medium_uncased&#39;,&#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) snomed_resolver = SentenceEntityResolverModel.pretrained(&quot;sbertresolve_snomed_bodyStructure_med, &quot;en&quot;, &quot;clinical/models) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) snomed_pipelineModel = PipelineModel( stages = [ documentAssembler, jsl_sbert_embedder, snomed_resolver]) snomed_lp = LightPipeline(snomed_pipelineModel) result = snomed_lp.fullAnnotate(&quot;Amputation stump&quot;) Result: | | chunks | code | resolutions | all_codes | all_distances | |:|:--|:|:|:|:-| | 0 | amputation stump | 38033009 | [Amputation stump, Amputation stump of upper limb, Amputation stump of left upper limb, Amputation stump of lower limb, Amputation stump of left lower limb, Amputation stump of right upper limb, Amputation stump of right lower limb, ...]| [&#39;38033009&#39;, &#39;771359009&#39;, &#39;771364008&#39;, &#39;771358001&#39;, &#39;771367001&#39;, &#39;771365009&#39;, &#39;771368006&#39;, ...] | [&#39;0.0000&#39;, &#39;0.0773&#39;, &#39;0.0858&#39;, &#39;0.0863&#39;, &#39;0.0905&#39;, &#39;0.0911&#39;, &#39;0.0972&#39;, ...] | sbiobertresolve_icdo_augmented : This model maps extracted medical entities to ICD-O codes using sBioBert sentence embeddings. This model is augmented using the site information coming from ICD10 and synonyms coming from SNOMED vocabularies. It is trained with a dataset that is 20x larger than the previous version of ICDO resolver. Given the oncological entity found in the text (via NER models like ner_jsl), it returns top terms and resolutions along with the corresponding ICD-10 codes to present more granularity with respect to body parts mentioned. It also returns the original histological behavioral codes and descriptions in the aux metadata. Example: ... chunk2doc = Chunk2Doc().setInputCols(&quot;ner_chunk&quot;).setOutputCol(&quot;ner_chunk_doc&quot;) sbert_embedder = BertSentenceEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) icdo_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_icdo_augmented&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, word_embeddings, clinical_ner, ner_converter, chunk2doc, sbert_embedder, icdo_resolver]) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = nlpPipeline.fit(empty_data) results = model.transform(spark.createDataFrame([[&quot;The patient is a very pleasant 61-year-old female with a strong family history of colon polyps. The patient reports her first polyps noted at the age of 50. We reviewed the pathology obtained from the pericardectomy in March 2006, which was diagnostic of mesothelioma. She also has history of several malignancies in the family. Her father died of a brain tumor at the age of 81. Her sister died at the age of 65 breast cancer. She has two maternal aunts with history of lung cancer both of whom were smoker. Also a paternal grandmother who was diagnosed with leukemia at 86 and a paternal grandfather who had B-cell lymphoma.&quot;]]).toDF(&quot;text&quot;)) Result: +--+--++--+-+-+-+ | chunk|begin|end| entity| code| all_k_resolutions| all_k_codes| +--+--++--+-+-+-+ | mesothelioma| 255|266|Oncological|9971/3||C38.3|malignant mediastinal ...|9971/3||C38.3:::8854/3...| |several malignancies| 293|312|Oncological|8894/3||C39.8|overlapping malignant ...|8894/3||C39.8:::8070/2...| | brain tumor| 350|360|Oncological|9562/0||C71.9|cancer of the brain:::...|9562/0||C71.9:::9070/3...| | breast cancer| 413|425|Oncological|9691/3||C50.9|carcinoma of breast:::...|9691/3||C50.9:::8070/2...| | lung cancer| 471|481|Oncological|8814/3||C34.9|malignant tumour of lu...|8814/3||C34.9:::8550/3...| | leukemia| 560|567|Oncological|9670/3||C80.9|anemia in neoplastic d...|9670/3||C80.9:::9714/3...| | B-cell lymphoma| 610|624|Oncological|9818/3||C77.9|secondary malignant ne...|9818/3||C77.9:::9655/3...| +--+--++--+-+-+-+ Updated Resolver Models We updated sbiobertresolve_snomed_findings and sbiobertresolve_cpt_procedures_augmented resolver models to reflect the latest changes in the official terminologies. Getting Started with Spark NLP for Healthcare Notebook in Databricks We prepared a new notebook for those who want to get started with Spark NLP for Healthcare in Databricks : Getting Started with Spark NLP for Healthcare Notebook Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_1_1",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_1_1"
  },
  "199": {
    "id": "199",
    "title": "Annotation Lab Release Notes 3.1.1",
    "content": "3.1.1 Release date: 09-05-2022 We are very excited to announce the release of Annotation Lab v3.1.1 which includes Support excel import, CVE fixes and stabilization. Here are the highlights: Highlights Fix more CVEs of docker images Change ClusterRole and ClusterRolebinding to Role and Rolebinding for backup When a trained model is deployed by active learning, “active-learning” is seen in Deployedby column Fix for visibility icon used for connected words Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_3_1_1",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_3_1_1"
  },
  "200": {
    "id": "200",
    "title": "Spark NLP for Healthcare Release Notes 3.1.2",
    "content": "3.1.2 We are glad to announce that Spark NLP for Healthcare 3.1.2 has been released!. This release comes with new features, new models, bug fixes, and examples. Highlights Support for Fine-tuning of Ner models. More builtin(pre-defined) graphs for MedicalNerApproach. Date Normalizer. New Relation Extraction Models for ADE. Bug Fixes. Support for user-defined Custom Transformer. Java Workshop Examples. Deprecated Compatibility class in Python. Support for Fine Tuning of Ner models Users can now resume training/fine-tune existing(already trained) Spark NLP MedicalNer models on new data. Users can simply provide the path to any existing MedicalNer model and train it further on the new dataset: ner_tagger = MedicalNerApproach().setPretrainedModelPath(&quot;/path/to/trained/medicalnermodel&quot;) If the new dataset contains new tags/labels/entities, users can choose to override existing tags with the new ones. The default behaviour is to reset the list of existing tags and generate a new list from the new dataset. It is also possible to preserve the existing tags by setting the ‘overrideExistingTags’ parameter: ner_tagger = MedicalNerApproach() .setPretrainedModelPath(&quot;/path/to/trained/medicalnermodel&quot;) .setOverrideExistingTags(False) Setting overrideExistingTags to false is intended to be used when resuming trainig on the same, or very similar dataset (i.e. with the same tags or with just a few different ones). If tags overriding is disabled, and new tags are found in the training set, then the approach will try to allocate them to unused output nodes, if any. It is also possible to override specific tags of the old model by mapping them to new tags: ner_tagger = MedicalNerApproach() .setPretrainedModelPath(&quot;/path/to/trained/medicalnermodel&quot;) .setOverrideExistingTags(False) .setTagsMapping(&quot;B-PER,B-VIP&quot;, &quot;I-PER,I-VIP&quot;) In this case, the new tags B-VIP and I-VIP will replace the already trained tags ‘B-PER’ and ‘I-PER’. Unmapped old tags will remain in use and unmapped new tags will be allocated to new outpout nodes, if any. Jupyter Notebook: [Finetuning Medical NER Model Notebook] (https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.5.Resume_MedicalNer_Model_Training.ipynb) More builtin graphs for MedicalNerApproach Seventy new TensorFlow graphs have been added to the library of available graphs which are used to train MedicalNer models. The graph with the optimal set of parameters is automatically chosen by MedicalNerApproach. DateNormalizer New annotator that normalize dates to the format YYYY/MM/DD. This annotator identifies dates in chunk annotations, and transform these dates to the format YYYY/MM/DD. Both the input and output formats for the annotator are chunk. Example: sentences = [ [&#39;08/02/2018&#39;], [&#39;11/2018&#39;], [&#39;11/01/2018&#39;], [&#39;12Mar2021&#39;], [&#39;Jan 30, 2018&#39;], [&#39;13.04.1999&#39;], [&#39;3April 2020&#39;], [&#39;next monday&#39;], [&#39;today&#39;], [&#39;next week&#39;], ] df = spark.createDataFrame(sentences).toDF(&quot;text&quot;) document_assembler = DocumentAssembler().setInputCol(&#39;text&#39;).setOutputCol(&#39;document&#39;) chunksDF = document_assembler.transform(df) aa = map_annotations_col(chunksDF.select(&quot;document&quot;), lambda x: [Annotation(&#39;chunk&#39;, a.begin, a.end, a.result, a.metadata, a.embeddings) for a in x], &quot;document&quot;, &quot;chunk_date&quot;, &quot;chunk&quot;) dateNormalizer = DateNormalizer().setInputCols(&#39;chunk_date&#39;).setOutputCol(&#39;date&#39;).setAnchorDateYear(2021).setAnchorDateMonth(2).setAnchorDateDay(27) dateDf = dateNormalizer.transform(aa) dateDf.select(&quot;date.result&quot;,&quot;text&quot;).show() +--+-+ |text | date | +--+-+ |08/02/2018 |2018/08/02| |11/2018 |2018/11/DD| |11/01/2018 |2018/11/01| |12Mar2021 |2021/03/12| |Jan 30, 2018|2018/01/30| |13.04.1999 |1999/04/13| |3April 2020 |2020/04/03| |next Monday |2021/06/19| |today |2021/06/12| |next week |2021/06/19| +--+-+ New Relation Extraction Models for ADE We are releasing new Relation Extraction models for ADE (Adverse Drug Event). This model is available in both RelationExtraction and Bert based RelationExtractionDL versions, and is capabale of linking drugs with ADE mentions. Example ade_re_model = new RelationExtractionModel().pretrained(&#39;ner_ade_clinical&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunk&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setPredictionThreshold(0.5) .setRelationPairs([&#39;ade-drug&#39;, &#39;drug-ade&#39;]) pipeline = Pipeline(stages=[documenter, sentencer, tokenizer, pos_tagger, words_embedder, ner_tagger, ner_converter, dependency_parser, re_ner_chunk_filter, re_model]) text =&quot;&quot;&quot;A 30 year old female presented with tense bullae due to excessive use of naproxin, and leg cramps relating to oxaprozin.&quot;&quot;&quot; p_model = pipeline.fit(spark.createDataFrame([[text]]).toDF(&quot;text&quot;)) result = p_model.transform(data) Results | | chunk1 | entity1 | chunk2 | entity2 | result | |:|:--|:--|:--|:-|--:| | 0 | tense bullae | ADE | naproxin | DRUG | 1 | | 1 | tense bullae | ADE | oxaprozin | DRUG | 0 | | 2 | naproxin | DRUG | leg cramps | ADE | 0 | | 3 | leg cramps | ADE | oxaprozin | DRUG | 1 | Benchmarking Model: re_ade_clinical precision recall f1-score support 0 0.85 0.89 0.87 1670 1 0.88 0.84 0.86 1673 micro avg 0.87 0.87 0.87 3343 macro avg 0.87 0.87 0.87 3343 weighted avg 0.87 0.87 0.87 3343 Model: redl_ade_biobert Relation Recall Precision F1 Support 0 0.894 0.946 0.919 1011 1 0.963 0.926 0.944 1389 Avg. 0.928 0.936 0.932 Weighted Avg. 0.934 0.934 0.933 Bug Fixes RelationExtractionDLModel had an issue(BufferOverflowException) on versions 3.1.0 and 3.1.1, which is fixed with this release. Some pretrained RelationExtractionDLModels got outdated after release 3.0.3, new updated models were created, tested and made available to be used with versions 3.0.3, and later. Some SentenceEntityResolverModels which did not work with Spark 2.4/2.3 were fixed. Support for user-defined Custom Transformer. Utility classes to define custom transformers in python are included in this release. This allows users to define functions in Python to manipulate Spark-NLP annotations. This new Transformers can be added to pipelines like any of the other models you’re already familiar with. Example how to use the custom transformer. def myFunction(annotations): # lower case the content of the annotations return [a.copy(a.result.lower()) for a in annotations] custom_transformer = CustomTransformer(f=myFunction).setInputCol(&quot;ner_chunk&quot;).setOutputCol(&quot;custom&quot;) outputDf = custom_transformer.transform(outdf).select(&quot;custom&quot;).toPandas() Java Workshop Examples Add Java examples in the workshop repository. https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/java/healthcare Deprecated Compatibility class in Python Due to active release cycle, we are adding &amp; training new pretrained models at each release and it might be tricky to maintain the backward compatibility or keep up with the latest models and versions, especially for the users using our models locally in air-gapped networks. We are releasing a new utility class to help you check your local &amp; existing models with the latest version of everything we have up to date. You will not need to specify your AWS credentials from now on. This new class is now replacing the previous Compatibility class written in Python and CompatibilityBeta class written in Scala. from sparknlp_jsl.compatibility import Compatibility compatibility = Compatibility(spark) print(compatibility.findVersion(&#39;sentence_detector_dl_healthcare&#39;)) Output [{&#39;name&#39;: &#39;sentence_detector_dl_healthcare&#39;, &#39;sparkVersion&#39;: &#39;2.4&#39;, &#39;version&#39;: &#39;2.6.0&#39;, &#39;language&#39;: &#39;en&#39;, &#39;date&#39;: &#39;2020-09-13T14:44:42.565&#39;, &#39;readyToUse&#39;: &#39;true&#39;}, {&#39;name&#39;: &#39;sentence_detector_dl_healthcare&#39;, &#39;sparkVersion&#39;: &#39;2.4&#39;, &#39;version&#39;: &#39;2.7.0&#39;, &#39;language&#39;: &#39;en&#39;, &#39;date&#39;: &#39;2021-03-16T08:42:34.391&#39;, &#39;readyToUse&#39;: &#39;true&#39;}] Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_1_2",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_1_2"
  },
  "201": {
    "id": "201",
    "title": "Spark NLP for Healthcare Release Notes 3.1.3",
    "content": "3.1.3 We are glad to announce that Spark NLP for Healthcare 3.1.3 has been released!. This release comes with new features, new models, bug fixes, and examples. Highlights New Relation Extraction model and a Pretrained pipeline for extracting and linking ADEs New Entity Resolver model for SNOMED codes ChunkConverter Annotator BugFix: getAnchorDateMonth method in DateNormalizer. BugFix: character map in MedicalNerModel fine-tuning. New Relation Extraction model and a Pretrained pipeline for extracting and linking ADEs We are releasing a new Relation Extraction Model for ADEs. This model is trained using Bert Word embeddings (biobert_pubmed_base_cased), and is capable of linking ADEs and Drugs. Example: re_model = RelationExtractionModel() .pretrained(&quot;re_ade_biobert&quot;, &quot;en&quot;, &#39;clinical/models&#39;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setMaxSyntacticDistance(3) #default: 0 .setPredictionThreshold(0.5) #default: 0.5 .setRelationPairs([&quot;ade-drug&quot;, &quot;drug-ade&quot;]) # Possible relation pairs. Default: All Relations. nlp_pipeline = Pipeline(stages=[documenter, sentencer, tokenizer, words_embedder, pos_tagger, ner_tagger, ner_chunker, dependency_parser, re_model]) light_pipeline = LightPipeline(nlp_pipeline.fit(spark.createDataFrame([[&#39;&#39;]]).toDF(&quot;text&quot;))) text =&quot;&quot;&quot;Been taking Lipitor for 15 years , have experienced sever fatigue a lot!!! . Doctor moved me to voltaren 2 months ago , so far , have only experienced cramps&quot;&quot;&quot; annotations = light_pipeline.fullAnnotate(text) We also have a new pipeline comprising of all models related to ADE(Adversal Drug Event) as part of this release. This pipeline includes classification, NER, assertion and relation extraction models. Users can now use this pipeline to get classification result, ADE and Drug entities, assertion status for ADE entities, and relations between ADE and Drug entities. Example: pretrained_ade_pipeline = PretrainedPipeline(&#39;explain_clinical_doc_ade&#39;, &#39;en&#39;, &#39;clinical/models&#39;) result = pretrained_ade_pipeline.fullAnnotate(&quot;&quot;&quot;Been taking Lipitor for 15 years , have experienced sever fatigue a lot!!! . Doctor moved me to voltaren 2 months ago , so far , have only experienced cramps&quot;&quot;&quot;)[0] Results: Class: True NER_Assertion: | | chunk | entitiy | assertion | |-|-||-| | 0 | Lipitor | DRUG | - | | 1 | sever fatigue | ADE | Conditional | | 2 | voltaren | DRUG | - | | 3 | cramps | ADE | Conditional | Relations: | | chunk1 | entitiy1 | chunk2 | entity2 | relation | |-|-||-||-| | 0 | sever fatigue | ADE | Lipitor | DRUG | 1 | | 1 | cramps | ADE | Lipitor | DRUG | 0 | | 2 | sever fatigue | ADE | voltaren | DRUG | 0 | | 3 | cramps | ADE | voltaren | DRUG | 1 | New Entity Resolver model for SNOMED codes We are releasing a new SentenceEntityResolver model for SNOMED codes. This model also includes AUX SNOMED concepts and can find codes for Morph Abnormality, Procedure, Substance, Physical Object, and Body Structure entities. In the metadata, the all_k_aux_labels can be divided to get further information: ground truth, concept, and aux . In the example shared below the ground truth is Atherosclerosis, concept is Observation, and aux is Morph Abnormality. Example: snomed_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_snomed_findings_aux_concepts&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) snomed_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, snomed_resolver]) snomed_lp = LightPipeline(snomed_pipelineModel) result = snomed_lp.fullAnnotate(&quot;atherosclerosis&quot;) Results: | | chunks | code | resolutions | all_codes | all_k_aux_labels | all_distances | |:|:-|:|:-|:|:|:| | 0 | atherosclerosis | 38716007 | [atherosclerosis, atherosclerosis, atherosclerosis, atherosclerosis, atherosclerosis, atherosclerosis, atherosclerosis artery, coronary atherosclerosis, coronary atherosclerosis, coronary atherosclerosis, coronary atherosclerosis, coronary atherosclerosis, arteriosclerosis, carotid atherosclerosis, cardiovascular arteriosclerosis, aortic atherosclerosis, aortic atherosclerosis, atherosclerotic ischemic disease] | [38716007, 155382007, 155414001, 195251000, 266318005, 194848007, 441574008, 443502000, 41702007, 266231003, 155316000, 194841001, 28960008, 300920004, 39468009, 155415000, 195252007, 129573006] | &#39;Atherosclerosis&#39;, &#39;Observation&#39;, &#39;Morph Abnormality&#39; | [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0280, 0.0451, 0.0451, 0.0451, 0.0451, 0.0451, 0.0462, 0.0477, 0.0466, 0.0490, 0.0490, 0.0485 | ChunkConverter Annotator Allows to use RegexMather chunks as NER chunks and feed the output to the downstream annotators like RE or Deidentification. document_assembler = DocumentAssembler().setInputCol(&#39;text&#39;).setOutputCol(&#39;document&#39;) sentence_detector = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) regex_matcher = RegexMatcher() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;regex&quot;) .setExternalRules(path=&quot;../src/test/resources/regex-matcher/rules.txt&quot;,delimiter=&quot;,&quot;) chunkConverter = ChunkConverter().setInputCols(&quot;regex&quot;).setOutputCol(&quot;chunk&quot;) Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_1_3",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_1_3"
  },
  "202": {
    "id": "202",
    "title": "Spark NLP for Healthcare Release Notes 3.2.0",
    "content": "3.2.0 We are glad to announce that Spark NLP Healthcare 3.2.0 has been released!. Highlights New Sentence Boundary Detection Model for Healthcare text New Assertion Status Models New Sentence Entity Resolver Model Finetuning Sentence Entity Resolvers with Your Data New Clinical NER Models New CMS-HCC risk-adjustment score calculation module New Embedding generation module for entity resolution New Sentence Boundary Detection Model for Healthcare text We are releasing an updated Sentence Boundary detection model to identify complex sentences containing multiple measurements, and punctuations. This model is trained on an in-house dataset. Example: Python: ... documenter = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentencerDL = SentenceDetectorDLModel .pretrained(&quot;sentence_detector_dl_healthcare&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) text = &quot;&quot;&quot;He was given boluses of MS04 with some effect.he has since been placed on a PCA . He takes 80 mg. of ativan at home ativan for anxiety, with 20 meq kcl po, 30 mmol K-phos iv and 2 gms mag so4 iv. Size: Prostate gland measures 10x1.1x 4.9 cm (LS x AP x TS). Estimated volume is 51.9 ml. and is mildly enlarged in size.Normal delineation pattern of the prostate gland is preserved. &quot;&quot;&quot; sd_model = LightPipeline(PipelineModel(stages=[documenter, sentencerDL])) result = sd_model.fullAnnotate(text) Results: | s.no | sentences | |--:|:| | 0 | He was given boluses of MS04 with some effect. | | 1 | he has since been placed on a PCA . | | 2 | He takes 80 mg. of ativan at home ativan for anxiety, | | | with 20 meq kcl po, 30 mmol K-phos iv and 2 gms mag so4 iv. | | 3 | Size: Prostate gland measures 10x1.1x 4.9 cm (LS x AP x TS). | | 4 | Estimated volume is | | | 51.9 ml. and is mildly enlarged in size. | | 5 | Normal delineation pattern of the prostate gland is preserved. | New Assertion Status Models We are releasing two new Assertion Status Models based on the BiLSTM architecture. Apart from what we released in other assertion models, an in-house annotations on a curated dataset (6K clinical notes) is used to augment the base assertion dataset (2010 i2b2/VA). assertion_jsl: This model can classify the assertions made on given medical concepts as being Present, Absent, Possible, Planned, Someoneelse, Past, Family, None, Hypotetical. assertion_jsl_large: This model can classify the assertions made on given medical concepts as being present, absent, possible, planned, someoneelse, past. assertion_dl vs assertion_jsl: chunks entities assertion_dl assertion_jsl Mesothelioma PROBLEM present Present CVA PROBLEM absent Absent cancer PROBLEM associated_with_someone_else Family her INR TEST present Planned Amiodarone TREATMENT hypothetical Hypothetical lymphadenopathy PROBLEM absent Absent stage III disease PROBLEM possible Possible IV piggyback TREATMENT conditional Past Example: Python: ... clinical_assertion = AssertionDLModel.pretrained(&quot;assertion_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) nlpPipeline = Pipeline(stages=[documentAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, ner_converter, clinical_assertion]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) result = model.transform(spark.createDataFrame([[&quot;The patient is a 41-year-old and has a nonproductive cough that started last week. She has had right-sided chest pain radiating to her back with fever starting today. She has no nausea. She has a history of pericarditis and pericardectomy in May 2006 and developed cough with right-sided chest pain, and went to an urgent care center and Chest x-ray revealed right-sided pleural effusion. In family history, her father has a colon cancer history.&quot;]], [&quot;text&quot;]) Results: +-+--++-+-++ |chunk |begin|end|ner_label |sent_id|assertion| +-+--++-+-++ |nonproductive cough|35 |53 |Symptom |0 |Present | |last week |68 |76 |RelativeDate |0 |Past | |chest pain |103 |112|Symptom |1 |Present | |fever |141 |145|VS_Finding |1 |Present | |today |156 |160|RelativeDate |1 |Present | |nausea |174 |179|Symptom |2 |Absent | |pericarditis |203 |214|Disease_Syndrome_Disorder|3 |Past | |pericardectomy |220 |233|Procedure |3 |Past | |May 2006 |238 |245|Date |3 |Past | |cough |261 |265|Symptom |3 |Past | |chest pain |284 |293|Symptom |3 |Past | |Chest x-ray |334 |344|Test |3 |Past | |pleural effusion |367 |382|Disease_Syndrome_Disorder|3 |Past | |colon cancer |421 |432|Oncological |4 |Family | +-+--++-+-++ New Sentence Entity Resolver Model We are releasing sbiobertresolve_rxnorm_disposition model that maps medication entities (like drugs/ingredients) to RxNorm codes and their dispositions using sbiobert_base_cased_mli Sentence Bert Embeddings. In the result, look for the aux_label parameter in the metadata to get dispositions that were divided by |. Example: Python: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbiobert_base_cased_mli&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) rxnorm_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_rxnorm_disposition&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;rxnorm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, rxnorm_resolver ]) rxnorm_lp = LightPipeline(pipelineModel) result = rxnorm_lp.fullAnnotate(&quot;belimumab 80 mg/ml injectable solution&quot;) Results: | | chunks | code | resolutions | all_codes | all_k_aux_labels | all_distances | |:|:--|:--|:--|:--|:--|:-| | 0 |belimumab 80 mg/ml injectable solution | 1092440 | [belimumab 80 mg/ml injectable solution, belimumab 80 mg/ml injectable solution [benlysta], ifosfamide 80 mg/ml injectable solution, belimumab 80 mg/ml [benlysta], belimumab 80 mg/ml, ...]| [1092440, 1092444, 107034, 1092442, 1092438, ...] | [Immunomodulator, Immunomodulator, Alkylating agent, Immunomodulator, Immunomodulator, ...] | [0.0000, 0.0145, 0.0479, 0.0619, 0.0636, ...] | Finetuning Sentence Entity Resolvers with Your Data Instead of starting from scratch when training a new Sentence Entity Resolver model, you can train a new model by adding your new data to the pretrained model. There’s a new method setPretrainedModelPath(path), which allows you to point the training process to an existing model, and allows you to initialize your model with the data from the pretrained model. When both the new data and the pretrained model contain the same code, you will see both of the results at the top. Here is a sample notebook : Finetuning Sentence Entity Resolver Model Notebook Example: In the example below, we changed the code of sepsis to X1234 and re-retrain the main ICD10-CM model with this new dataset. So we want to see the X1234 code as a result in the all_codes. Python: ... bertExtractor = SentenceEntityResolverApproach() .setNeighbours(50) .setThreshold(1000) .setInputCols(&quot;sentence_embeddings&quot;) .setNormalizedCol(&quot;description_normalized&quot;) # concept_name .setLabelCol(&quot;code&quot;) # concept_code .setOutputCol(&quot;recognized_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) .setCaseSensitive(False) .setUseAuxLabel(True) # if exist .setPretrainedModelPath(&quot;path_to_a_pretrained_model&quot;) new_model = bertExtractor.fit(&quot;new_dataset&quot;) new_model.save(&quot;models/new_resolver_model&quot;) # save and use later ... resolver_model = SentenceEntityResolverModel.load(&quot;models/new_resolver_model&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;output_code&quot;) pipelineModel = PipelineModel( stages = [ documentAssembler, sentence_embedder, resolver_model]) light_model = LightPipeline(pipelineModel) light_model.fullAnnotate(&quot;sepsis&quot;) Main Model Results: chunks begin end code all_codes resolutions all_k_aux_labels all_distances sepsis 0 5 A4189 [A4189, L419, A419, A267, E771, …] [sepsis [Other specified sepsis], parapsoriasis [Parapsoriasis, unspecified], postprocedural sepsis [Sepsis, unspecified organism], erysipelothrix sepsis [Erysipelothrix sepsis], fucosidosis [Defects in glycoprotein degradation], … ] [1|1|2, 1|1|2, 1|1|2, 1|1|2, 1|1|23, …] [0.0000, 0.2079, 0.2256, 0.2359, 0.2399,…] Re-Trained Model Results: chunks begin end code all_codes resolutions all_k_aux_labels all_distances sepsis 0 5 X1234 [X1234, A4189, A419, L419, A267, …] [sepsis [Sepsis, new resolution], sepsis [Other specified sepsis], SEPSIS [Sepsis, unspecified organism], parapsoriasis [Parapsoriasis, unspecified], erysipelothrix sepsis [Erysipelothrix sepsis], … ] [1|1|74, 1|1|2, 1|1|2, 1|1|2, 1|1|2, …] [0.0000, 0.0000, 0.0000, 0.2079, 0.2359, …] New Clinical NER Models ner_jsl_slim: This model is trained based on ner_jsl model with more generalized entities. (Death_Entity, Medical_Device, Vital_Sign, Alergen, Drug, Clinical_Dept, Lifestyle, Symptom, Body_Part, Physical_Measurement, Admission_Discharge, Date_Time, Age, Birth_Entity, Header, Oncological, Substance_Quantity, Test_Result, Test, Procedure, Treatment, Disease_Syndrome_Disorder, Pregnancy_Newborn, Demographics) ner_jsl vs ner_jsl_slim: chunks ner_jsl ner_jsl_slim Description: Section_Header Header atrial fibrillation Heart_Disease Disease_Syndrome_Disorder August 24, 2007 Date Date_Time transpleural fluoroscopy Procedure Test last week RelativeDate Date_Time She Gender Demographics fever VS_Finding Vital_Sign PAST MEDICAL HISTORY: Medical_History_Header Header Pericardial window Internal_organ_or_component Body_Part FAMILY HISTORY: Family_History_Header Header CVA Cerebrovascular_Disease Disease_Syndrome_Disorder diabetes Diabetes Disease_Syndrome_Disorder married Relationship_Status Demographics alcohol Alcohol Lifestyle illicit drug Substance Lifestyle Coumadin Drug_BrandName Drug Blood pressure 123/95 Blood_Pressure Vital_Sign heart rate 83 Pulse Vital_Sign anticoagulated Drug_Ingredient Drug Example: Python: ... embeddings_clinical = WordEmbeddingsModel().pretrained(&#39;embeddings_clinical&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&#39;sentence&#39;, &#39;token&#39;]) .setOutputCol(&#39;embeddings&#39;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_jsl_slim&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings_clinical, clinical_ner, ner_converter]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame([[&quot;HISTORY: 30-year-old female presents for digital bilateral mammography secondary to a soft tissue lump palpated by the patient in the upper right shoulder. The patient has a family history of breast cancer within her mother at age 58. Patient denies personal history of breast cancer.&quot;]], [&quot;text&quot;])) Results: | | chunk | entity | |:|:--|:-| | 0 | HISTORY: | Header | | 1 | 30-year-old | Age | | 2 | female | Demographics | | 3 | mammography | Test | | 4 | soft tissue lump | Symptom | | 5 | shoulder | Body_Part | | 6 | breast cancer | Oncological | | 7 | her mother | Demographics | | 8 | age 58 | Age | | 9 | breast cancer | Oncological | ner_jsl_biobert : This model is the BioBert version of ner_jsl model and trained with biobert_pubmed_base_cased embeddings. ner_jsl_greedy_biobert : This model is the BioBert version of ner_jsl_greedy models and trained with biobert_pubmed_base_cased embeddings. Example: Python: ... embeddings_clinical = BertEmbeddings.pretrained(&#39;biobert_pubmed_base_cased&#39;) .setInputCols([&#39;sentence&#39;, &#39;token&#39;]) .setOutputCol(&#39;embeddings&#39;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_jsl_greedy_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings_clinical, clinical_ner, ner_converter]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame([[&quot;The patient is a 21-day-old Caucasian male here for 2 days of congestion - mom has been suctioning yellow discharge from the patient&#39;s nares, plus she has noticed some mild problems with his breathing while feeding (but negative for any perioral cyanosis or retractions). One day ago, mom also noticed a tactile temperature and gave the patient Tylenol. Baby also has had some decreased p.o. intake. His normal breast-feeding is down from 20 minutes q.2h. to 5 to 10 minutes secondary to his respiratory congestion. He sleeps well, but has been more tired and has been fussy over the past 2 days. The parents noticed no improvement with albuterol treatments given in the ER. His urine output has also decreased; normally he has 8 to 10 wet and 5 dirty diapers per 24 hours, now he has down to 4 wet diapers per 24 hours. Mom denies any diarrhea. His bowel movements are yellow colored and soft in nature.&quot;]], [&quot;text&quot;])) Results: | | chunk | entity | |:|:--|:--| | 0 | 21-day-old | Age | | 1 | Caucasian | Race_Ethnicity | | 2 | male | Gender | | 3 | for 2 days | Duration | | 4 | congestion | Symptom | | 5 | mom | Gender | | 6 | suctioning yellow discharge | Symptom | | 7 | nares | External_body_part_or_region | | 8 | she | Gender | | 9 | mild problems with his breathing while feeding | Symptom | | 10 | perioral cyanosis | Symptom | | 11 | retractions | Symptom | | 12 | One day ago | RelativeDate | | 13 | mom | Gender | | 14 | tactile temperature | Symptom | | 15 | Tylenol | Drug | | 16 | Baby | Age | | 17 | decreased p.o. intake | Symptom | | 18 | His | Gender | | 19 | breast-feeding | External_body_part_or_region | | 20 | q.2h | Frequency | | 21 | to 5 to 10 minutes | Duration | | 22 | his | Gender | | 23 | respiratory congestion | Symptom | | 24 | He | Gender | | 25 | tired | Symptom | | 26 | fussy | Symptom | | 27 | over the past 2 days | RelativeDate | | 28 | albuterol | Drug | | 29 | ER | Clinical_Dept | | 30 | His | Gender | | 31 | urine output has also decreased | Symptom | | 32 | he | Gender | | 33 | per 24 hours | Frequency | | 34 | he | Gender | | 35 | per 24 hours | Frequency | | 36 | Mom | Gender | | 37 | diarrhea | Symptom | | 38 | His | Gender | | 39 | bowel | Internal_organ_or_component | New CMS-HCC risk-adjustment score calculation module We are releasing a new module to calculate medical risk adjusment score by using the Centers for Medicare &amp; Medicaid Service (CMS) risk adjustment model. The main input to this model are ICD codes of the diseases. After getting ICD codes of diseases by Spark NLP Healthcare ICD resolvers, risk score can be calculated by this module in spark environment. Current supported version for the model is CMS-HCC V24. The model needs following parameters in order to calculate the risk score: ICD Codes Age Gender The eligibility segment of the patient Original reason for entitlement If the patient is in Medicaid or not If the patient is disabled or not Example: Python: sample_patients.show() Results: +-++++ |Patient_ID|ICD_codes |Age|Gender| +-++++ |101 |[E1169, I5030, I509, E852] |64 |F | |102 |[G629, D469, D6181] |77 |M | |103 |[D473, D473, D473, M069, C969]|16 |F | +-++++ Python: from sparknlp_jsl.functions import profile df = df.withColumn(&quot;hcc_profile&quot;, profile(df.ICD_codes, df.Age, df.Gender)) df = df.withColumn(&quot;hcc_profile&quot;, F.from_json(F.col(&quot;hcc_profile&quot;), schema)) df= df.withColumn(&quot;risk_score&quot;, df.hcc_profile.getItem(&quot;risk_score&quot;)) .withColumn(&quot;hcc_lst&quot;, df.hcc_profile.getItem(&quot;hcc_map&quot;)) .withColumn(&quot;parameters&quot;, df.hcc_profile.getItem(&quot;parameters&quot;)) .withColumn(&quot;details&quot;, df.hcc_profile.getItem(&quot;details&quot;)) df.select(&#39;Patient_ID&#39;, &#39;risk_score&#39;,&#39;ICD_codes&#39;, &#39;Age&#39;, &#39;Gender&#39;).show(truncate=False ) df.show(truncate=100, vertical=True) Results: +-+-++++ |Patient_ID|risk_score|ICD_codes |Age|Gender| +-+-++++ |101 |0.827 |[E1169, I5030, I509, E852] |64 |F | |102 |1.845 |[G629, D469, D6181] |77 |M | |103 |1.288 |[D473, D473, D473, M069, C969]|16 |F | +-+-++++ RECORD 0- Patient_ID | 101 ICD_codes | [E1169, I5030, I509, E852] Age | 64 Gender | F Eligibility_Segment | CNA OREC | 0 Medicaid | false Disabled | false hcc_profile | {{&quot;CNA_HCC18&quot;:0.302,&quot;CNA_HCC85&quot;:0.331,&quot;CNA_HCC23&quot;:0.194,&quot;CNA_D3&quot;:0.0,&quot;CNA_HCC85_gDiabetesMellit&quot;:... risk_score | 0.827 hcc_lst | {&quot;E1169&quot;:[&quot;HCC18&quot;],&quot;I5030&quot;:[&quot;HCC85&quot;],&quot;I509&quot;:[&quot;HCC85&quot;],&quot;E852&quot;:[&quot;HCC23&quot;]} parameters | {&quot;elig&quot;:&quot;CNA&quot;,&quot;age&quot;:64,&quot;sex&quot;:&quot;F&quot;,&quot;origds&quot;:&#39;0&#39;,&quot;disabled&quot;:false,&quot;medicaid&quot;:false} details | {&quot;CNA_HCC18&quot;:0.302,&quot;CNA_HCC85&quot;:0.331,&quot;CNA_HCC23&quot;:0.194,&quot;CNA_D3&quot;:0.0,&quot;CNA_HCC85_gDiabetesMellit&quot;:0.0} -RECORD 1- Patient_ID | 102 ICD_codes | [G629, D469, D6181] Age | 77 Gender | M Eligibility_Segment | CNA OREC | 0 Medicaid | false Disabled | false hcc_profile | {{&quot;CNA_M75_79&quot;:0.473,&quot;CNA_D1&quot;:0.0,&quot;CNA_HCC46&quot;:1.372}, [&quot;D1&quot;,&quot;HCC46&quot;], {&quot;D469&quot;:[&quot;HCC46&quot;]}, {&quot;elig&quot;... risk_score | 1.845 hcc_lst | {&quot;D469&quot;:[&quot;HCC46&quot;]} parameters | {&quot;elig&quot;:&quot;CNA&quot;,&quot;age&quot;:77,&quot;sex&quot;:&quot;M&quot;,&quot;origds&quot;:&#39;0&#39;,&quot;disabled&quot;:false,&quot;medicaid&quot;:false} details | {&quot;CNA_M75_79&quot;:0.473,&quot;CNA_D1&quot;:0.0,&quot;CNA_HCC46&quot;:1.372} -RECORD 2- Patient_ID | 103 ICD_codes | [D473, D473, D473, M069, C969] Age | 16 Gender | F Eligibility_Segment | CNA OREC | 0 Medicaid | false Disabled | false hcc_profile | {{&quot;CNA_HCC10&quot;:0.675,&quot;CNA_HCC40&quot;:0.421,&quot;CNA_HCC48&quot;:0.192,&quot;CNA_D3&quot;:0.0}, [&quot;HCC10&quot;,&quot;HCC40&quot;,&quot;HCC48&quot;,&quot;... risk_score | 1.288 hcc_lst | {&quot;D473&quot;:[&quot;HCC48&quot;],&quot;M069&quot;:[&quot;HCC40&quot;],&quot;C969&quot;:[&quot;HCC10&quot;]} parameters | {&quot;elig&quot;:&quot;CNA&quot;,&quot;age&quot;:16,&quot;sex&quot;:&quot;F&quot;,&quot;origds&quot;:&#39;0&#39;,&quot;disabled&quot;:false,&quot;medicaid&quot;:false} details | {&quot;CNA_HCC10&quot;:0.675,&quot;CNA_HCC40&quot;:0.421,&quot;CNA_HCC48&quot;:0.192,&quot;CNA_D3&quot;:0.0} Here is a sample notebook : Calculating Medicare Risk Adjustment Score New Embedding generation module for entity resolution We are releasing a new annotator BertSentenceChunkEmbeddings to let users aggregate sentence embeddings and ner chunk embeddings to get more specific and accurate resolution codes. It works by averaging context and chunk embeddings to get contextual information. This is specially helpful when ner chunks do not have additional information (like body parts or severity) as explained in the example below. Input to this annotator is the context (sentence) and ner chunks, while the output is embedding for each chunk that can be fed to the resolver model. The setChunkWeight parameter can be used to control the influence of surrounding context. Example below shows the comparison of old vs new approach. Sample Notebook: Improved_Entity_Resolution_with_SentenceChunkEmbeddings Example: Python: ... sentence_chunk_embeddings = BertSentenceChunkEmbeddings .pretrained(&quot;sbiobert_base_cased_mli&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;ner_chunk&quot;]) .setOutputCol(&quot;sentence_chunk_embeddings&quot;) .setChunkWeight(0.5) resolver = SentenceEntityResolverModel.pretrained(&#39;sbiobertresolve_icd10cm&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_chunk_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) text = &quot;&quot;&quot;A 20 year old female patient badly tripped while going down stairs. She complains of right leg pain. Her x-ray showed right hip fracture. Hair line fractures also seen on the left knee joint. She also suffered from trauma and slight injury on the head. OTHER CONDITIONS: She was also recently diagnosed with diabetes, which is of type 2. &quot;&quot;&quot; nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings_clinical, clinical_ner, ner_converter, sentence_chunk_embeddings, resolver]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame([[text]], [&quot;text&quot;])) Results: | | chunk | entity | code_with_old_approach | resolutions_with_old_approach | code_with_new_approach | resolutions_with_new_approach | |:|:--|:--|:--|:--|:--|:-| | 0 | leg pain | Symptom | R1033 | Periumbilical pain | M79661 | Pain in right lower leg | | 1 | hip fracture | Injury_or_Poisoning | M84459S | Pathological fracture, hip, unspecified, sequela | M84451S | Pathological fracture, right femur, sequela | | 2 | Hair line fractures | Injury_or_Poisoning | S070XXS | Crushing injury of face, sequela | S92592P | Other fracture of left lesser toe(s), subsequent encounter for fracture with malunion | | 3 | trauma | Injury_or_Poisoning | T794XXS | Traumatic shock, sequela | S0083XS | Contusion of other part of head, sequela | | 4 | slight injury | Injury_or_Poisoning | B03 | Smallpox | S0080XD | Unspecified superficial injury of other part of head, subsequent encounter | | 5 | diabetes | Diabetes | E118 | Type 2 diabetes mellitus with unspecified complications | E1169 | Type 2 diabetes mellitus with other specified complication | To see more, please check : Spark NLP Healthcare Workshop Repo Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_2_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_2_0"
  },
  "203": {
    "id": "203",
    "title": "Annotation Lab Release Notes 3.2.0",
    "content": "3.2.0 Release date: 31-05-2022 We are very excited to announce the release of Annotation Lab v3.2.0 which includes new and exciting features such as Project cloning and Project backup, Evaluation of Pretrained Models, and Search feature in the Visual NER Project. Support for Multiple files import, ability to view statuses of Model Servers and Training Jobs, and prioritization of completions for CONLL export. Spark NLP and Spark OCR libraries were also upgraded, and some security fixes and stabilizations were also implemented. Here are the highlights: Highlights Import/export of an entire Project. All project-related items (tasks, project configuration, project members, task assignments) can be imported/exported. In addition, users can also clone an existing project. Evaluate Named Entity Models. Project Owner and/or Manager can now test and evaluate annotated tasks against the Pretrained NER models in the Training &amp; Active Learning Settings tab, configured NER models will be tested against the tasks tagged as test. Statuses of Training and Preannotation Server. A new column, status, is added to the server page that gives the status of training and preannotation servers. Also if any issues are encountered during server initialization, those are displayed on mouse-over the status value. Import Multiple Files. Project Owners or Managers can now upload multiple files at once in bulk. Prioritize Annotators For Data Export. When multiple completions are available for the same task, the CONLL export will include completions from higher priority members. Network Policies have been implemented which specify how a pod is allowed to communicate with various network “entities” over the network. The entities that are required to function in Annotation Lab were clearly identified and only traffic coming from them is now allowed. Support for airgap licenses with scope. Previously airgap licenses with scopes were missrecognized as floating licenses. Upgraded Spark NLP and Spark NLP for Health Care v3.4.1 and Spark OCR v3.12.0 Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_3_2_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_3_2_0"
  },
  "204": {
    "id": "204",
    "title": "Spark NLP release notes 3.2.0",
    "content": "3.2.0 Release date: 28-05-2021 Overview Multi-modal visual document understanding, built on the LayoutLM architecture. It achieves new state-of-the-art accuracy in several downstream tasks, including form understanding and receipt understanding. New Features VisualDocumentNER is a DL model for NER problem using text and layout data. Currently available pre-trained model on the SROIE dataset. Enhancements Added support SPARK_OCR_LICENSE env key for read license. Update dependencies and sync Spark versions with Spark NLP. Bugfixes Fixed an issue that some ImageReaderSpi plugins are unavailable in the fat jar. New notebooks Visual Document NER Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_2_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_2_0"
  },
  "205": {
    "id": "205",
    "title": "Spark NLP for Healthcare Release Notes 3.2.1",
    "content": "3.2.1 We are glad to announce that Spark NLP Healthcare 3.2.1 has been released!. Highlights Deprecated ChunkEntityResolver. New BERT-Based NER Models HCC module added support for versions v22 and v23. Updated Notebooks for resolvers and graph builders. New TF Graph Builder. New BERT-Based NER Models We have two new BERT-based token classifier NER models. These models are the first clinical NER models that use the BertForTokenCLassification approach that was introduced in Spark NLP 3.2.0. bert_token_classifier_ner_clinical: This model is BERT-based version of ner_clinical model. This new model is 4% better than the legacy NER model (MedicalNerModel) that is based on BiLSTM-CNN-Char architecture. Metrics: precision recall f1-score support PROBLEM 0.88 0.92 0.90 30276 TEST 0.91 0.86 0.88 17237 TREATMENT 0.87 0.88 0.88 17298 O 0.97 0.97 0.97 202438 accuracy 0.95 267249 macro avg 0.91 0.91 0.91 267249 weighted avg 0.95 0.95 0.95 267249 Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl_healthcare&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;sentence&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, tokenClassifier, ner_converter ]) p_model = pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) text = &#39;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting . Two weeks prior to presentation , she was treated with a five-day course of amoxicillin for a respiratory tract infection . She was on metformin , glipizide , and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG . She had been on dapagliflozin for six months at the time of presentation . Physical examination on presentation was significant for dry oral mucosa ; significantly , her abdominal examination was benign with no tenderness , guarding , or rigidity . Pertinent laboratory findings on admission were : serum glucose 111 mg/dl , bicarbonate 18 mmol/l , anion gap 20 , creatinine 0.4 mg/dL , triglycerides 508 mg/dL , total cholesterol 122 mg/dL , glycated hemoglobin ( HbA1c ) 10% , and venous pH 7.27 . Serum lipase was normal at 43 U/L . Serum acetone levels could not be assessed as blood samples kept hemolyzing due to significant lipemia . The patient was initially admitted for starvation ketosis , as she reported poor oral intake for three days prior to admission . However , serum chemistry obtained six hours after presentation revealed her glucose was 186 mg/dL , the anion gap was still elevated at 21 , serum bicarbonate was 16 mmol/L , triglyceride level peaked at 2050 mg/dL , and lipase was 52 U/L . The β-hydroxybutyrate level was obtained and found to be elevated at 5.29 mmol/L - the original sample was centrifuged and the chylomicron layer removed prior to analysis due to interference from turbidity caused by lipemia again . The patient was treated with an insulin drip for euDKA and HTG with a reduction in the anion gap to 13 and triglycerides to 1400 mg/dL , within 24 hours . Her euDKA was thought to be precipitated by her respiratory tract infection in the setting of SGLT2 inhibitor use . The patient was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should be discontinued indefinitely . She had close follow-up with endocrinology post discharge .&#39; res = p_model.transform(spark.createDataFrame([[text]]).toDF(&quot;text&quot;)).collect() res[0][&#39;label&#39;] bert_token_classifier_ner_jsl: This model is BERT-based version of ner_jsl model. This new model is better than the legacy NER model (MedicalNerModel) that is based on BiLSTM-CNN-Char architecture. Metrics: precision recall f1-score support Admission_Discharge 0.84 0.97 0.90 415 Age 0.96 0.96 0.96 2434 Alcohol 0.75 0.83 0.79 145 Allergen 0.33 0.16 0.22 25 BMI 1.00 0.77 0.87 26 Birth_Entity 1.00 0.17 0.29 12 Blood_Pressure 0.86 0.88 0.87 597 Cerebrovascular_Disease 0.74 0.77 0.75 266 Clinical_Dept 0.90 0.92 0.91 2385 Communicable_Disease 0.70 0.59 0.64 85 Date 0.95 0.98 0.96 1438 Death_Entity 0.83 0.83 0.83 59 Diabetes 0.95 0.95 0.95 350 Diet 0.60 0.49 0.54 229 Direction 0.88 0.90 0.89 6187 Disease_Syndrome_Disorder 0.90 0.89 0.89 13236 Dosage 0.57 0.49 0.53 263 Drug 0.91 0.93 0.92 15926 Duration 0.82 0.85 0.83 1218 EKG_Findings 0.64 0.70 0.67 325 Employment 0.79 0.85 0.82 539 External_body_part_or_region 0.84 0.84 0.84 4805 Family_History_Header 1.00 1.00 1.00 889 Fetus_NewBorn 0.57 0.56 0.56 341 Form 0.53 0.43 0.48 81 Frequency 0.87 0.90 0.88 1718 Gender 0.98 0.98 0.98 5666 HDL 0.60 1.00 0.75 6 Heart_Disease 0.88 0.88 0.88 2295 Height 0.89 0.96 0.92 134 Hyperlipidemia 1.00 0.95 0.97 194 Hypertension 0.95 0.98 0.97 566 ImagingFindings 0.66 0.64 0.65 601 Imaging_Technique 0.62 0.67 0.64 108 Injury_or_Poisoning 0.85 0.83 0.84 1680 Internal_organ_or_component 0.90 0.91 0.90 21318 Kidney_Disease 0.89 0.89 0.89 446 LDL 0.88 0.97 0.92 37 Labour_Delivery 0.82 0.71 0.76 306 Medical_Device 0.89 0.93 0.91 12852 Medical_History_Header 0.96 0.97 0.96 1013 Modifier 0.68 0.60 0.64 1398 O2_Saturation 0.84 0.82 0.83 199 Obesity 0.96 0.98 0.97 130 Oncological 0.88 0.96 0.92 1635 Overweight 0.80 0.80 0.80 10 Oxygen_Therapy 0.91 0.92 0.92 231 Pregnancy 0.81 0.83 0.82 439 Procedure 0.91 0.91 0.91 14410 Psychological_Condition 0.81 0.81 0.81 354 Pulse 0.85 0.95 0.89 389 Race_Ethnicity 1.00 1.00 1.00 163 Relationship_Status 0.93 0.91 0.92 57 RelativeDate 0.83 0.86 0.84 1562 RelativeTime 0.74 0.79 0.77 431 Respiration 0.99 0.95 0.97 221 Route 0.68 0.69 0.69 597 Section_Header 0.97 0.98 0.98 28580 Sexually_Active_or_Sexual_Orientation 1.00 0.64 0.78 14 Smoking 0.83 0.90 0.86 225 Social_History_Header 0.95 0.99 0.97 825 Strength 0.71 0.55 0.62 227 Substance 0.85 0.81 0.83 193 Substance_Quantity 0.00 0.00 0.00 28 Symptom 0.84 0.86 0.85 23092 Temperature 0.94 0.97 0.96 410 Test 0.84 0.88 0.86 9050 Test_Result 0.84 0.84 0.84 2766 Time 0.90 0.81 0.86 140 Total_Cholesterol 0.69 0.95 0.80 73 Treatment 0.73 0.72 0.73 506 Triglycerides 0.83 0.80 0.81 30 VS_Finding 0.76 0.77 0.76 588 Vaccine 0.70 0.84 0.76 92 Vital_Signs_Header 0.95 0.98 0.97 2223 Weight 0.88 0.89 0.88 306 O 0.97 0.96 0.97 253164 accuracy 0.94 445974 macro avg 0.82 0.82 0.81 445974 weighted avg 0.94 0.94 0.94 445974 Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl_healthcare&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;sentence&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, tokenClassifier, ner_converter ]) p_model = pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) text = &#39;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting . Two weeks prior to presentation , she was treated with a five-day course of amoxicillin for a respiratory tract infection . She was on metformin , glipizide , and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG . She had been on dapagliflozin for six months at the time of presentation . Physical examination on presentation was significant for dry oral mucosa ; significantly , her abdominal examination was benign with no tenderness , guarding , or rigidity . Pertinent laboratory findings on admission were : serum glucose 111 mg/dl , bicarbonate 18 mmol/l , anion gap 20 , creatinine 0.4 mg/dL , triglycerides 508 mg/dL , total cholesterol 122 mg/dL , glycated hemoglobin ( HbA1c ) 10% , and venous pH 7.27 . Serum lipase was normal at 43 U/L . Serum acetone levels could not be assessed as blood samples kept hemolyzing due to significant lipemia . The patient was initially admitted for starvation ketosis , as she reported poor oral intake for three days prior to admission . However , serum chemistry obtained six hours after presentation revealed her glucose was 186 mg/dL , the anion gap was still elevated at 21 , serum bicarbonate was 16 mmol/L , triglyceride level peaked at 2050 mg/dL , and lipase was 52 U/L . The β-hydroxybutyrate level was obtained and found to be elevated at 5.29 mmol/L - the original sample was centrifuged and the chylomicron layer removed prior to analysis due to interference from turbidity caused by lipemia again . The patient was treated with an insulin drip for euDKA and HTG with a reduction in the anion gap to 13 and triglycerides to 1400 mg/dL , within 24 hours . Her euDKA was thought to be precipitated by her respiratory tract infection in the setting of SGLT2 inhibitor use . The patient was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should be discontinued indefinitely . She had close follow-up with endocrinology post discharge .&#39; res = p_model.transform(spark.createDataFrame([[text]]).toDF(&quot;text&quot;)).collect() res[0][&#39;label&#39;] HCC module added support for versions v22 and v23 Now we can use the version 22 and the version 23 for the new HCC module to calculate CMS-HCC Risk Adjustment score. Added the following parameters elig, orec and medicaid on the profiles functions. These parameters may not be stored in clinical notes, and may require to be imported from other sources. elig : The eligibility segment of the patient. Allowed values are as follows: - &quot;CFA&quot;: Community Full Benefit Dual Aged - &quot;CFD&quot;: Community Full Benefit Dual Disabled - &quot;CNA&quot;: Community NonDual Aged - &quot;CND&quot;: Community NonDual Disabled - &quot;CPA&quot;: Community Partial Benefit Dual Aged - &quot;CPD&quot;: Community Partial Benefit Dual Disabled - &quot;INS&quot;: Long Term Institutional - &quot;NE&quot;: New Enrollee - &quot;SNPNE&quot;: SNP NE orec: Original reason for entitlement code. - &quot;0&quot;: Old age and survivor&#39;s insurance - &quot;1&quot;: Disability insurance benefits - &quot;2&quot;: End-stage renal disease - &quot;3&quot;: Both DIB and ESRD medicaid: If the patient is in Medicaid or not. Required parameters should be stored in Spark dataframe. df.show(truncate=False) +++++--+-+--+ |hcc_profileV24 |icd10_code |age|gender|eligibility|orec|medicaid| +++++--+-+--+ |{&quot;hcc_lst&quot;:[...|[E1169, I5030, I509, E852] |64 |F |CFA |0 |true | |{&quot;hcc_lst&quot;:[...|[G629, D469, D6181] |77 |M |CND |1 |false | |{&quot;hcc_lst&quot;:[...|[D473, D473, D473, M069, C969]|16 |F |CPA |3 |true | +++++--+-+--+ The content of the hcc_profileV24 column is a JSON-parsable string, like in the following example, { &quot;hcc_lst&quot;: [ &quot;HCC18&quot;, &quot;HCC85_gDiabetesMellit&quot;, &quot;HCC85&quot;, &quot;HCC23&quot;, &quot;D3&quot; ], &quot;details&quot;: { &quot;CNA_HCC18&quot;: 0.302, &quot;CNA_HCC85&quot;: 0.331, &quot;CNA_HCC23&quot;: 0.194, &quot;CNA_D3&quot;: 0.0, &quot;CNA_HCC85_gDiabetesMellit&quot;: 0.0 }, &quot;hcc_map&quot;: { &quot;E1169&quot;: [ &quot;HCC18&quot; ], &quot;I5030&quot;: [ &quot;HCC85&quot; ], &quot;I509&quot;: [ &quot;HCC85&quot; ], &quot;E852&quot;: [ &quot;HCC23&quot; ] }, &quot;risk_score&quot;: 0.827, &quot;parameters&quot;: { &quot;elig&quot;: &quot;CNA&quot;, &quot;age&quot;: 56, &quot;sex&quot;: &quot;F&quot;, &quot;origds&quot;: false, &quot;disabled&quot;: false, &quot;medicaid&quot;: false } } We can import different CMS-HCC model versions as seperate functions and use them in the same program. from sparknlp_jsl.functions import profile,profileV22,profileV23 df = df.withColumn(&quot;hcc_profileV24&quot;, profile(df.icd10_code, df.age, df.gender, df.eligibility, df.orec, df.medicaid )) df.withColumn(&quot;hcc_profileV22&quot;, profileV22(df.codes, df.age, df.sex,df.elig,df.orec,df.medicaid)) df.withColumn(&quot;hcc_profileV23&quot;, profileV23(df.codes, df.age, df.sex,df.elig,df.orec,df.medicaid)) df.show(truncate=False) +-++++--+-+--+ |risk_score|icd10_code |age|gender|eligibility|orec|medicaid| +-++++--+-+--+ |0.922 |[E1169, I5030, I509, E852] |64 |F |CFA |0 |true | |3.566 |[G629, D469, D6181] |77 |M |CND |1 |false | |1.181 |[D473, D473, D473, M069, C969]|16 |F |CPA |3 |true | +-++++--+-+--+ Updated Notebooks for resolvers and graph builders We have updated the resolver notebooks on spark-nlp-workshop repo with new BertSentenceChunkEmbeddings annotator. This annotator lets users aggregate sentence embeddings and ner chunk embeddings to get more specific and accurate resolution codes. It works by averaging context and chunk embeddings to get contextual information. Input to this annotator is the context (sentence) and ner chunks, while the output is embedding for each chunk that can be fed to the resolver model. The setChunkWeight parameter can be used to control the influence of surrounding context. Example below shows the comparison of old vs new approach. text ner_chunk entity icd10_code all_codes resolutions icd10_code_SCE all_codes_SCE resolutions_SCE Two weeks prior to presentation, she was treated with a five-day course of amoxicillin for a respiratory tract infection. a respiratory tract infection PROBLEM J988 [J988, J069, A499, J22, J209,…] [respiratory tract infection, upper respiratory tract infection, bacterial respiratory infection, acute respiratory infection, bronchial infection,…] Z870 [Z870, Z8709, J470, J988, A499,… [history of acute lower respiratory tract infection (situation), history of acute lower respiratory tract infection, bronchiectasis with acute lower respiratory infection, rti - respiratory tract infection, bacterial respiratory infection,… Here are the updated resolver notebooks: 3.Clinical_Entity_Resolvers.ipynb 24.Improved_Entity_Resolvers_in_SparkNLP_with_sBert.ipynb You can also check for more examples of this annotator: 24.1.Improved_Entity_Resolution_with_SentenceChunkEmbeddings.ipynb We have updated TF Graph builder notebook to show how to create TF graphs with TF2.x. Here is the updated notebook: 17.Graph_builder_for_DL_models.ipynb To see more, please check: Spark NLP Healthcare Workshop Repo New TF Graph Builder TF graph builder to create graphs and train DL models for licensed annotators (MedicalNer, Relation Extraction, Assertion and Generic Classifier) is made compatible with TF2.x. To see how to create TF Graphs, you can check here: 17.Graph_builder_for_DL_models.ipynb Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_2_1",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_2_1"
  },
  "206": {
    "id": "206",
    "title": "Spark NLP for Healthcare Release Notes 3.2.2",
    "content": "3.2.2 We are glad to announce that Spark NLP Healthcare 3.2.2 has been released!. Highlights New NER Model For Detecting Drugs, Posology, and Administration Cycles New Sentence Entity Resolver Models New Router Annotator To Use Multiple Resolvers Optimally In the Same Pipeline Re-Augmented Deidentification NER Model New NER Model For Detecting Drugs, Posology, and Administration Cycles We are releasing a new NER posology model ner_posology_experimental. This model is based on the original ner_posology_large model, but trained with additional clinical trials data to detect experimental drugs, experiment cycles, cycle counts, and cycles numbers. Supported Entities: Administration, Cyclenumber, Strength, Cycleday, Duration, Cyclecount, Route, Form, Frequency, Cyclelength, Drug, Dosage Example: ... word_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_posology_experimental&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, word_embeddings, clinical_ner, ner_converter]) model = nlp_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame([[&quot;Y-90 Humanized Anti-Tac: 10 mCi (if a bone marrow transplant was part of the patient&#39;s previous therapy) or 15 mCi of yttrium labeled anti-TAC; followed by calcium trisodium Inj (Ca DTPA).. n nCalcium-DTPA: Ca-DTPA will be administered intravenously on Days 1-3 to clear the radioactive agent from the body.&quot;]]).toDF(&quot;text&quot;)) Results: | | chunk | begin | end | entity | |:|:-|--:|:|:| | 0 | Y-90 Humanized Anti-Tac | 0 | 22 | Drug | | 1 | 10 mCi | 25 | 30 | Dosage | | 2 | 15 mCi | 108 | 113 | Dosage | | 3 | yttrium labeled anti-TAC | 118 | 141 | Drug | | 4 | calcium trisodium Inj | 156 | 176 | Drug | | 5 | Calcium-DTPA | 191 | 202 | Drug | | 6 | Ca-DTPA | 205 | 211 | Drug | | 7 | intravenously | 234 | 246 | Route | | 8 | Days 1-3 | 251 | 258 | Cycleday | New Sentence Entity Resolver Models We have two new sentence entity resolver models trained with using sbert_jsl_medium_uncased embeddings. sbertresolve_rxnorm_disposition : This model maps medication entities (like drugs/ingredients) to RxNorm codes and their dispositions using sbert_jsl_medium_uncased Sentence Bert Embeddings. If you look for a faster inference with just drug names (excluding dosage and strength), this version of RxNorm model would be a better alternative. In the result, look for the aux_label parameter in the metadata to get dispositions divided by |. Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbert_jsl_medium_uncased&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) rxnorm_resolver = SentenceEntityResolverModel.pretrained(&quot;sbertresolve_rxnorm_disposition&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;rxnorm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) rxnorm_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, rxnorm_resolver]) rxnorm_lp = LightPipeline(rxnorm_pipelineModel) rxnorm_lp = LightPipeline(pipelineModel) result = rxnorm_lp.fullAnnotate(&quot;alizapride 25 mg/ml&quot;) Result: | | chunks | code | resolutions | all_codes | all_k_aux_labels | all_distances | |:|:-|:-|:|:-|:|:--| | 0 |alizapride 25 mg/ml | 330948 | [alizapride 25 mg/ml, alizapride 50 mg, alizapride 25 mg/ml oral solution, adalimumab 50 mg/ml, adalimumab 100 mg/ml [humira], adalimumab 50 mg/ml [humira], alirocumab 150 mg/ml, ...]| [330948, 330949, 249531, 358817, 1726845, 576023, 1659153, ...] | [Dopamine receptor antagonist, Dopamine receptor antagonist, Dopamine receptor antagonist, -, -, -, -, ...] | [0.0000, 0.0936, 0.1166, 0.1525, 0.1584, 0.1567, 0.1631, ...] | sbertresolve_snomed_conditions : This model maps clinical entities (domain: Conditions) to Snomed codes using sbert_jsl_medium_uncased Sentence Bert Embeddings. Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbert_jsl_medium_uncased&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) snomed_resolver = SentenceEntityResolverModel.pretrained(&quot;sbertresolve_snomed_conditions&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) snomed_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, snomed_resolver ]) snomed_lp = LightPipeline(snomed_pipelineModel) result = snomed_lp.fullAnnotate(&quot;schizophrenia&quot;) Result: | | chunks | code | resolutions | all_codes | all_distances | |:|:--|:|:-|:|:--| | 0 | schizophrenia | 58214004 | [schizophrenia, chronic schizophrenia, borderline schizophrenia, schizophrenia, catatonic, subchronic schizophrenia, ...]| [58214004, 83746006, 274952002, 191542003, 191529003, 16990005, ...] | 0.0000, 0.0774, 0.0838, 0.0927, 0.0970, 0.0970, ...] | New Router Annotator To Use Multiple Resolvers Optimally In the Same Pipeline Normally, when we need to use more than one sentence entity resolver models in the same pipeline, we used to hit BertSentenceEmbeddings annotator more than once given the number of different resolver models in the same pipeline. Now we are introducing a solution with the help of Router annotator that could allow us to feed all the NER chunks to BertSentenceEmbeddings at once and then route the output of Sentence Embeddings to different resolver models needed. You can find an example of how to use this annotator in the updated 3.Clinical_Entity_Resolvers.ipynb Notebook Example: ... # to get PROBLEM entitis clinical_ner = MedicalNerModel().pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;clinical_ner&quot;) clinical_ner_chunk = NerConverter() .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;clinical_ner&quot;) .setOutputCol(&quot;clinical_ner_chunk&quot;) .setWhiteList([&quot;PROBLEM&quot;]) # to get DRUG entities posology_ner = MedicalNerModel().pretrained(&quot;ner_posology&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;posology_ner&quot;) posology_ner_chunk = NerConverter() .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;posology_ner&quot;) .setOutputCol(&quot;posology_ner_chunk&quot;) .setWhiteList([&quot;DRUG&quot;]) # merge the chunks into a single ner_chunk chunk_merger = ChunkMergeApproach() .setInputCols(&quot;clinical_ner_chunk&quot;,&quot;posology_ner_chunk&quot;) .setOutputCol(&quot;final_ner_chunk&quot;) .setMergeOverlapping(False) # convert chunks to doc to get sentence embeddings of them chunk2doc = Chunk2Doc().setInputCols(&quot;final_ner_chunk&quot;).setOutputCol(&quot;final_chunk_doc&quot;) sbiobert_embeddings = BertSentenceEmbeddings.pretrained(&quot;sbiobert_base_cased_mli&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;final_chunk_doc&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) # filter PROBLEM entity embeddings router_sentence_icd10 = Router() .setInputCols(&quot;sbert_embeddings&quot;) .setFilterFieldsElements([&quot;PROBLEM&quot;]) .setOutputCol(&quot;problem_embeddings&quot;) # filter DRUG entity embeddings router_sentence_rxnorm = Router() .setInputCols(&quot;sbert_embeddings&quot;) .setFilterFieldsElements([&quot;DRUG&quot;]) .setOutputCol(&quot;drug_embeddings&quot;) # use problem_embeddings only icd_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_icd10cm_slim_billable_hcc&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;clinical_ner_chunk&quot;, &quot;problem_embeddings&quot;]) .setOutputCol(&quot;icd10cm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) # use drug_embeddings only rxnorm_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_rxnorm&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;posology_ner_chunk&quot;, &quot;drug_embeddings&quot;]) .setOutputCol(&quot;rxnorm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, clinical_ner_chunk, posology_ner, posology_ner_chunk, chunk_merger, chunk2doc, sbiobert_embeddings, router_sentence_icd10, router_sentence_rxnorm, icd_resolver, rxnorm_resolver ]) Re-Augmented Deidentification NER Model We re-augmented ner_deid_subentity_augmented deidentification NER model improving the previous metrics by 2%. Example: ... deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, word_embeddings, deid_ner, ner_converter]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) results = model.transform(spark.createDataFrame(pd.DataFrame({&quot;text&quot;: [&quot;&quot;&quot;A. Record date : 2093-01-13, David Hale, M.D., Name : Hendrickson, Ora MR. # 7194334 Date : 01/13/93 PCP : Oliveira, 25 -year-old, Record date : 1-11-2000. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227.&quot;&quot;&quot;]}))) Results: +--+-+ |chunk |ner_label | +--+-+ |2093-01-13 |DATE | |David Hale |DOCTOR | |Hendrickson, Ora |PATIENT | |7194334 |MEDICALRECORD| |01/13/93 |DATE | |Oliveira |DOCTOR | |25-year-old |AGE | |1-11-2000 |DATE | |Cocke County Baptist Hospital|HOSPITAL | |0295 Keats Street. |STREET | |(302) 786-5227 |PHONE | |Brothers Coal-Mine |ORGANIZATION | +--+-+ To see more, please check: Spark NLP Healthcare Workshop Repo Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_2_2",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_2_2"
  },
  "207": {
    "id": "207",
    "title": "Spark NLP for Healthcare Release Notes 3.2.3",
    "content": "3.2.3 We are glad to announce that Spark NLP Healthcare 3.2.3 has been released!. Highlights New BERT-Based Deidentification NER Model New Sentence Entity Resolver Models For German Language New Spell Checker Model For Drugs Allow To Use Disambiguator Pretrained Model Allow To Use Seeds in StructuredDeidentification Added Compatibility with Tensorflow 1.15 For Graph Generation. New Setup Videos New BERT-Based Deidentification NER Model We have a new bert_token_classifier_ner_deid model that is BERT-based version of ner_deid_subentity_augmented and annotates text to find protected health information that may need to be de-identified. It can detect 23 different entities (MEDICALRECORD, ORGANIZATION, DOCTOR, USERNAME, PROFESSION, HEALTHPLAN, URL, CITY, DATE, LOCATION-OTHER, STATE, PATIENT, DEVICE, COUNTRY, ZIP, PHONE, HOSPITAL, EMAIL, IDNUM, SREET, BIOID, FAX, AGE). Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_deid&quot;, &quot;en&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;document&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[documentAssembler, tokenizer, tokenClassifier, ner_converter]) p_model = pipeline.fit(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [&#39;&#39;]}))) text = &quot;&quot;&quot;A. Record date : 2093-01-13, David Hale, M.D. Name : Hendrickson, Ora MR. # 7194334. PCP : Oliveira, non-smoking. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227. Patient&#39;s complaints first surfaced when he started working for Brothers Coal-Mine.&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [text]}))) Results: +--+-+ |chunk |ner_label | +--+-+ |2093-01-13 |DATE | |David Hale |DOCTOR | |Hendrickson, Ora |PATIENT | |7194334 |MEDICALRECORD| |Oliveira |PATIENT | |Cocke County Baptist Hospital|HOSPITAL | |0295 Keats Street |STREET | |302) 786-5227 |PHONE | |Brothers Coal-Mine |ORGANIZATION | +--+-+ New Sentence Entity Resolver Models For German Language We are releasing two new Sentence Entity Resolver Models for German language that use sent_bert_base_cased (de) embeddings. sbertresolve_icd10gm : This model maps extracted medical entities to ICD10-GM codes for the German language. Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&quot;sent_bert_base_cased&quot;, &quot;de&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) icd10gm_resolver = SentenceEntityResolverModel.pretrained(&quot;sbertresolve_icd10gm&quot;, &quot;de&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;icd10gm_code&quot;) icd10gm_pipelineModel = PipelineModel( stages = [documentAssembler, sbert_embedder, icd10gm_resolver]) icd_lp = LightPipeline(icd10gm_pipelineModel) icd_lp.fullAnnotate(&quot;Dyspnoe&quot;) Results : chunk code resolutions all_codes all_distances Dyspnoe C671 Dyspnoe, Schlafapnoe, Dysphonie, Frühsyphilis, Hyperzementose, Hypertrichose, … [R06.0, G47.3, R49.0, A51, K03.4, L68, …] [0.0000, 2.5602, 3.0529, 3.3310, 3.4645, 3.7148, …] sbertresolve_snomed : This model maps extracted medical entities to SNOMED codes for the German language. Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&quot;sent_bert_base_cased&quot;, &quot;de&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) snomed_resolver = SentenceEntityResolverModel.pretrained(&quot;sbertresolve_snomed&quot;, &quot;de&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) snomed_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, snomed_resolver]) snomed_lp = LightPipeline(snomed_pipelineModel) snomed_lp.fullAnnotate(&quot;Bronchialkarzinom &quot;) Results : chunk code resolutions all_codes all_distances Bronchialkarzinom 22628 Bronchialkarzinom, Bronchuskarzinom, Rektumkarzinom, Klavikulakarzinom, Lippenkarzinom, Urothelkarzinom, … [22628, 111139, 18116, 107569, 18830, 22909, …] [0.0000, 0.0073, 0.0090, 0.0098, 0.0098, 0.0102, …] New Spell Checker Model For Drugs We are releasing new spellcheck_drug_norvig model that detects and corrects spelling errors of drugs in a text based on the Norvig’s approach. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) spell = NorvigSweetingModel.pretrained(&quot;spellcheck_drug_norvig&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) pipeline = Pipeline( stages = [documentAssembler, tokenizer, spell]) model = pipeline.fit(spark.createDataFrame([[&#39;&#39;]]).toDF(&#39;text&#39;)) lp = LightPipeline(model) lp.annotate(&quot;You have to take Neutrcare and colfosrinum and a bit of Fluorometholne &amp; Ribotril&quot;) Results : Original text : You have to take Neutrcare and colfosrinum and a bit of fluorometholne &amp; Ribotril Corrected text : You have to take Neutracare and colforsinum and a bit of fluorometholone &amp; Rivotril Allow to use Disambiguator pretrained model. Now we can use the NerDisambiguatorModel as a pretrained model to disambiguate person entities. text = &quot;The show also had a contestant named Brad Pitt&quot; + &quot;who later defeated Christina Aguilera on the way to become Female Vocalist Champion in the 1989 edition of Star Search in the United States. &quot; data = SparkContextForTest.spark.createDataFrame([ [text]]) .toDF(&quot;text&quot;).cache() da = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sd = SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) tk = Tokenizer().setInputCols(&quot;sentence&quot;).setOutputCol(&quot;token&quot;) emb = WordEmbeddingsModel.pretrained().setOutputCol(&quot;embs&quot;) semb = SentenceEmbeddings().setInputCols(&quot;sentence&quot;, &quot;embs&quot;).setOutputCol(&quot;sentence_embeddings&quot;) ner = NerDLModel.pretrained().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embs&quot;).setOutputCol(&quot;ner&quot;) nc = NerConverter().setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;).setOutputCol(&quot;ner_chunk&quot;).setWhiteList([&quot;PER&quot;]) NerDisambiguatorModel.pretrained().setInputCols(&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;).setOutputCol(&quot;disambiguation&quot;) pl = Pipeline().setStages([da, sd, tk, emb, semb, ner, nc, disambiguator]) data = pl.fit(data).transform(data) data.select(&quot;disambiguation&quot;).show(10, False) +-+ |disambiguation | +-+ |[[disambiguation, 65, 82, http://en.wikipedia.org/?curid=144171, http://en.wikipedia.org/?curid=6636454, [chunk -&gt; Christina Aguilera, titles -&gt; christina aguilera ::::: christina aguilar, links -&gt; http://en.wikipedia.org/?curid=144171 ::::: http://en.wikipedia.org/?curid=6636454, beginInText -&gt; 65, scores -&gt; 0.9764155197864447, 0.9727793647472524, categories -&gt; Musicians, Singers, Actors, Businesspeople, Musicians, Singers, ids -&gt; 144171, 6636454, endInText -&gt; 82], []]]| +-+ - Allow to use seeds in StructuredDeidentification Now, we can use a seed for a specific column. The seed is used to randomly select the entities used during obfuscation mode. By providing the same seed, you can replicate the same mapping multiple times. df = spark.createDataFrame([ [&quot;12&quot;, &quot;12&quot;, &quot;Juan García&quot;], [&quot;24&quot;, &quot;56&quot;, &quot;Will Smith&quot;], [&quot;56&quot;, &quot;32&quot;, &quot;Pedro Ximénez&quot;] ]).toDF(&quot;ID1&quot;, &quot;ID2&quot;, &quot;NAME&quot;) obfuscator = StructuredDeidentification(spark=spark, columns={&quot;ID1&quot;: &quot;ID&quot;, &quot;ID2&quot;: &quot;ID&quot;, &quot;NAME&quot;: &quot;PATIENT&quot;}, columnsSeed={&quot;ID1&quot;: 23, &quot;ID2&quot;: 23}, obfuscateRefSource=&quot;faker&quot;) result = obfuscator.obfuscateColumns(df) result.show(truncate=False) +-+-+-+ |ID1 |ID2 |NAME | +-+-+-+ |[D3379888]|[D3379888]|[Raina Cleaves] | |[R8448971]|[M8851891]|[Jennell Barre] | |[M8851891]|[L5448098]|[Norene Salines]| +-+-+-+ Here, you can see that as we have provided the same seed `23` for columns `ID1`, and `ID2`, the number `12` which is appears twice in the first row is mapped to the same randomly generated id `D3379888` each time. Added compatibility with Tensorflow 1.15 for graph generation Some users reported problems while using graphs generated by Tensorflow 2.x. We provide compatibility with Tensorflow 1.15 in the tf_graph_1x module, that can be used like this, from sparknlp_jsl.training import tf_graph_1x In next releases, we will provide full support for graph generation using Tensorflow 2.x. New Setup Videos Now we have videos showing how to setup Spark NLP, Spark NLP for Healthcare and Spark OCR on UBUNTU. How to Setup Spark NLP on UBUNTU How to Setup Spark NLP for HEALTHCARE on UBUNTU How to Setup Spark OCR on UBUNTU To see more, please check: Spark NLP Healthcare Workshop Repo Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_2_3",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_2_3"
  },
  "208": {
    "id": "208",
    "title": "Spark NLP for Healthcare Release Notes 3.3.0",
    "content": "3.3.0 We are glad to announce that Spark NLP Healthcare 3.3.0 has been released!. Highlights NER Finder Pretrained Pipelines to Run Run 48 different Clinical NER and 21 Different Biobert Models At Once Over the Input Text 3 New Sentence Entity Resolver Models (3-char ICD10CM, RxNorm_NDC, HCPCS) Updated UMLS Entity Resolvers (Dropping Invalid Codes) 5 New Clinical NER Models (Trained By BertForTokenClassification Approach) Radiology NER Model Trained On cheXpert Dataset New Speed Benchmarks on Databricks NerConverterInternal Fixes Simplified Setup and Recommended Use of start() Function NER Evaluation Metrics Fix New Notebooks (Including How to Use SparkNLP with Neo4J) NER Finder Pretrained Pipelines to Run Run 48 different Clinical NER and 21 Different Biobert Models At Once Over the Input Text We are releasing two new NER Pretrained Pipelines that can be used to explore all the available pretrained NER models at once. You can check NER Profiling Notebook to see how to use these pretrained pipelines. ner_profiling_clinical : When you run this pipeline over your text, you will end up with the predictions coming out of each of the 48 pretrained clinical NER models trained with embeddings_clinical. Clinical NER Model List ner_ade_clinical ner_posology_greedy ner_risk_factors jsl_ner_wip_clinical ner_human_phenotype_gene_clinical jsl_ner_wip_greedy_clinical ner_cellular ner_cancer_genetics jsl_ner_wip_modifier_clinical ner_drugs_greedy ner_deid_sd_large ner_diseases nerdl_tumour_demo ner_deid_subentity_augmented ner_jsl_enriched ner_genetic_variants ner_bionlp ner_measurements_clinical ner_diseases_large ner_radiology ner_deid_augmented ner_anatomy ner_chemprot_clinical ner_posology_experimental ner_drugs ner_deid_sd ner_posology_large ner_deid_large ner_posology ner_deidentify_dl ner_deid_enriched ner_bacterial_species ner_drugs_large ner_clinical_large jsl_rd_ner_wip_greedy_clinical ner_medmentions_coarse ner_radiology_wip_clinical ner_clinical ner_chemicals ner_deid_synthetic ner_events_clinical ner_posology_small ner_anatomy_coarse ner_human_phenotype_go_clinical ner_jsl_slim ner_jsl ner_jsl_greedy ner_events_admission_clinical ner_profiling_biobert : When you run this pipeline over your text, you will end up with the predictions coming out of each of the 21 pretrained clinical NER models trained with biobert_pubmed_base_cased. BioBert NER Model List ner_cellular_biobert ner_diseases_biobert ner_events_biobert ner_bionlp_biobert ner_jsl_greedy_biobert ner_jsl_biobert ner_anatomy_biobert ner_jsl_enriched_biobert ner_human_phenotype_go_biobert ner_deid_biobert ner_deid_enriched_biobert ner_clinical_biobert ner_anatomy_coarse_biobert ner_human_phenotype_gene_biobert ner_posology_large_biobert jsl_rd_ner_wip_greedy_biobert ner_posology_biobert jsl_ner_wip_greedy_biobert ner_chemprot_biobert ner_ade_biobert ner_risk_factors_biobert You can also check Models Hub page for more information about all these NER models and more. Example : from sparknlp.pretrained import PretrainedPipeline ner_profiling_pipeline = PretrainedPipeline(&#39;ner_profiling_biobert&#39;, &#39;en&#39;, &#39;clinical/models&#39;) result = ner_profiling_pipeline.annotate(&quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting .&quot;) Results : sentence : [&#39;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting .&#39;] token : [&#39;A&#39;, &#39;28-year-old&#39;, &#39;female&#39;, &#39;with&#39;, &#39;a&#39;, &#39;history&#39;, &#39;of&#39;, &#39;gestational&#39;, &#39;diabetes&#39;, &#39;mellitus&#39;, &#39;diagnosed&#39;, &#39;eight&#39;, &#39;years&#39;, &#39;prior&#39;, &#39;to&#39;, &#39;presentation&#39;, &#39;and&#39;, &#39;subsequent&#39;, &#39;type&#39;, &#39;two&#39;, &#39;diabetes&#39;, &#39;mellitus&#39;, &#39;(&#39;, &#39;T2DM&#39;, &#39;),&#39;, &#39;one&#39;, &#39;prior&#39;, &#39;episode&#39;, &#39;of&#39;, &#39;HTG-induced&#39;, &#39;pancreatitis&#39;, &#39;three&#39;, &#39;years&#39;, &#39;prior&#39;, &#39;to&#39;, &#39;presentation&#39;, &#39;,&#39;, &#39;associated&#39;, &#39;with&#39;, &#39;an&#39;, &#39;acute&#39;, &#39;hepatitis&#39;, &#39;,&#39;, &#39;and&#39;, &#39;obesity&#39;, &#39;with&#39;, &#39;a&#39;, &#39;body&#39;, &#39;mass&#39;, &#39;index&#39;, &#39;(&#39;, &#39;BMI&#39;, &#39;)&#39;, &#39;of&#39;, &#39;33.5&#39;, &#39;kg/m2&#39;, &#39;,&#39;, &#39;presented&#39;, &#39;with&#39;, &#39;a&#39;, &#39;one-week&#39;, &#39;history&#39;, &#39;of&#39;, &#39;polyuria&#39;, &#39;,&#39;, &#39;polydipsia&#39;, &#39;,&#39;, &#39;poor&#39;, &#39;appetite&#39;, &#39;,&#39;, &#39;and&#39;, &#39;vomiting&#39;, &#39;.&#39;] ner_cellular_biobert_chunks : [] ner_diseases_biobert_chunks : [&#39;gestational diabetes mellitus&#39;, &#39;type two diabetes mellitus&#39;, &#39;T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;hepatitis&#39;, &#39;obesity&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_events_biobert_chunks : [&#39;gestational diabetes mellitus&#39;, &#39;eight years&#39;, &#39;presentation&#39;, &#39;type two diabetes mellitus ( T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;three years&#39;, &#39;presentation&#39;, &#39;an acute hepatitis&#39;, &#39;obesity&#39;, &#39;a body mass index&#39;, &#39;BMI&#39;, &#39;presented&#39;, &#39;a one-week&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_bionlp_biobert_chunks : [] ner_jsl_greedy_biobert_chunks : [&#39;28-year-old&#39;, &#39;female&#39;, &#39;gestational diabetes mellitus&#39;, &#39;eight years prior&#39;, &#39;type two diabetes mellitus&#39;, &#39;T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;three years prior&#39;, &#39;acute hepatitis&#39;, &#39;obesity&#39;, &#39;body mass index&#39;, &#39;BMI ) of 33.5 kg/m2&#39;, &#39;one-week&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_jsl_biobert_chunks : [&#39;28-year-old&#39;, &#39;female&#39;, &#39;gestational diabetes mellitus&#39;, &#39;eight years prior&#39;, &#39;type two diabetes mellitus&#39;, &#39;T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;three years prior&#39;, &#39;acute&#39;, &#39;hepatitis&#39;, &#39;obesity&#39;, &#39;body mass index&#39;, &#39;BMI ) of 33.5 kg/m2&#39;, &#39;one-week&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_anatomy_biobert_chunks : [&#39;body&#39;] ner_jsl_enriched_biobert_chunks : [&#39;28-year-old&#39;, &#39;female&#39;, &#39;gestational diabetes mellitus&#39;, &#39;type two diabetes mellitus&#39;, &#39;T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;acute&#39;, &#39;hepatitis&#39;, &#39;obesity&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_human_phenotype_go_biobert_chunks : [&#39;obesity&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;] ner_deid_biobert_chunks : [&#39;eight years&#39;, &#39;three years&#39;] ner_deid_enriched_biobert_chunks : [] ner_clinical_biobert_chunks : [&#39;gestational diabetes mellitus&#39;, &#39;subsequent type two diabetes mellitus ( T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;an acute hepatitis&#39;, &#39;obesity&#39;, &#39;a body mass index ( BMI )&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_anatomy_coarse_biobert_chunks : [&#39;body&#39;] ner_human_phenotype_gene_biobert_chunks : [&#39;obesity&#39;, &#39;mass&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;vomiting&#39;] ner_posology_large_biobert_chunks : [] jsl_rd_ner_wip_greedy_biobert_chunks : [&#39;gestational diabetes mellitus&#39;, &#39;type two diabetes mellitus&#39;, &#39;T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;acute hepatitis&#39;, &#39;obesity&#39;, &#39;body mass index&#39;, &#39;33.5&#39;, &#39;kg/m2&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_posology_biobert_chunks : [] jsl_ner_wip_greedy_biobert_chunks : [&#39;28-year-old&#39;, &#39;female&#39;, &#39;gestational diabetes mellitus&#39;, &#39;eight years prior&#39;, &#39;type two diabetes mellitus&#39;, &#39;T2DM&#39;, &#39;HTG-induced pancreatitis&#39;, &#39;three years prior&#39;, &#39;acute hepatitis&#39;, &#39;obesity&#39;, &#39;body mass index&#39;, &#39;BMI ) of 33.5 kg/m2&#39;, &#39;one-week&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_chemprot_biobert_chunks : [] ner_ade_biobert_chunks : [&#39;pancreatitis&#39;, &#39;acute hepatitis&#39;, &#39;polyuria&#39;, &#39;polydipsia&#39;, &#39;poor appetite&#39;, &#39;vomiting&#39;] ner_risk_factors_biobert_chunks : [&#39;diabetes mellitus&#39;, &#39;subsequent type two diabetes mellitus&#39;, &#39;obesity&#39;] 3 New Sentence Entity Resolver Models (3-char ICD10CM, RxNorm_NDC, HCPCS) sbiobertresolve_hcpcs : This model maps extracted medical entities to Healthcare Common Procedure Coding System (HCPCS) codes using sbiobert_base_cased_mli sentence embeddings. It also returns the domain information of the codes in the all_k_aux_labels parameter in the metadata of the result. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbiobert_base_cased_mli&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) hcpcs_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_hcpcs&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;hcpcs_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) hcpcs_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, hcpcs_resolver]) res = hcpcs_pipelineModel.transform(spark.createDataFrame([[&quot;Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type&quot;]]).toDF(&quot;text&quot;)) Results : ner_chunk hcpcs_code all_codes all_resolutions domain Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type L8001 [L8001, L8002, L8000, L8033, L8032, …] ‘Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, unilateral, any size, any type’, ‘Breast prosthesis, mastectomy bra, with integrated breast prosthesis form, bilateral, any size, any type’, ‘Breast prosthesis, mastectomy bra, without integrated breast prosthesis form, any size, any type’, ‘Nipple prosthesis, custom fabricated, reusable, any material, any type, each’, … Device, Device, Device, Device, Device, … sbiobertresolve_icd10cm_generalised : This model maps medical entities to 3 digit ICD10CM codes (according to ICD10 code structure the first three characters represent general type of the injury or disease). Difference in results (compared with sbiobertresolve_icd10cm) can be observed in the example below. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbiobert_base_cased_mli&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) icd_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_icd10cm_generalised&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;icd_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) icd_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, icd_resolver]) res = icd_pipelineModel.transform(spark.createDataFrame([[&quot;82 - year-old male with a history of hypertension , chronic renal insufficiency , COPD , and gastritis&quot;]]).toDF(&quot;text&quot;)) Results : | | chunk | entity | code_3char | code_desc_3char | code_full | code_full_description | distance | all_k_resolutions_3char | all_k_codes_3char | |:|:-|:--|:--|:-|:-|:|-:|:|:--| | 0 | hypertension | SYMPTOM | I10 | hypertension | I150 | Renovascular hypertension | 0 | [hypertension, hypertension (high blood pressure), h/o: hypertension, ...] | [I10, I15, Z86, Z82, I11, R03, Z87, E27] | | 1 | chronic renal insufficiency | SYMPTOM | N18 | chronic renal impairment | N186 | End stage renal disease | 0.014 | [chronic renal impairment, renal insufficiency, renal failure, anaemi ...] | [N18, P96, N19, D63, N28, Z87, N17, N25, R94] | | 2 | COPD | SYMPTOM | J44 | chronic obstructive lung disease (disorder) | I2781 | Cor pulmonale (chronic) | 0.1197 | [chronic obstructive lung disease (disorder), chronic obstructive pul ...] | [J44, Z76, J81, J96, R06, I27, Z87] | | 3 | gastritis | SYMPTOM | K29 | gastritis | K5281 | Eosinophilic gastritis or gastroenteritis | 0 | gastritis:::bacterial gastritis:::parasitic gastritis | [K29, B96, K93] | sbiobertresolve_rxnorm_ndc : This model maps DRUG entities to rxnorm codes and their National Drug Codes (NDC) using sbiobert_base_cased_mli sentence embeddings. You can find all NDC codes of drugs seperated by | in the all_k_aux_labels parameter of the metadata. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbiobert_base_cased_mli&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) rxnorm_ndc_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_rxnorm_ndc&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;rxnorm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) rxnorm_ndc_pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, rxnorm_ndc_resolver]) res = rxnorm_ndc_pipelineModel.transform(spark.createDataFrame([[&quot;activated charcoal 30000 mg powder for oral suspension&quot;]]).toDF(&quot;text&quot;)) Results : chunk rxnorm_code all_codes resolutions all_k_aux_labels all_distances activated charcoal 30000 mg powder for oral suspension 1440919 1440919, 808917, 1088194, 1191772, 808921,… activated charcoal 30000 MG Powder for Oral Suspension, Activated Charcoal 30000 MG Powder for Oral Suspension, wheat dextrin 3000 MG Powder for Oral Solution [Benefiber], cellulose 3000 MG Oral Powder [Unifiber], fosfomycin 3000 MG Powder for Oral Solution [Monurol] … 69784030828, 00395052791, 08679001362|86790016280|00067004490, 46017004408|68220004416, 00456430001,… 0.0000, 0.0000, 0.1128, 0.1148, 0.1201,… Updated UMLS Entity Resolvers (Dropping Invalid Codes) UMLS model sbiobertresolve_umls_findings and sbiobertresolve_umls_major_concepts were updated by dropping the invalid codes using the latest UMLS release done May 2021. 5 New Clinical NER Models (Trained By BertForTokenClassification Approach) We are releasing four new BERT-based NER models. bert_token_classifier_ner_ade : This model is BERT-Based version of ner_ade_clinical model and performs 5% better. It can detect drugs and adverse reactions of drugs in reviews, tweets, and medical texts using DRUG and ADE labels. Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_ade&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;document&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[documentAssembler, tokenizer, tokenClassifier, ner_converter]) p_model = pipeline.fit(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [&#39;&#39;]}))) test_sentence = &quot;&quot;&quot;Been taking Lipitor for 15 years , have experienced severe fatigue a lot!!! . Doctor moved me to voltaren 2 months ago , so far , have only experienced cramps&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +--++ |chunk |ner_label| +--++ |Lipitor |DRUG | |severe fatigue|ADE | |voltaren |DRUG | |cramps |ADE | +--++ bert_token_classifier_ner_jsl_slim : This model is BERT-Based version of ner_jsl_slim model and 2% better than the legacy NER model (MedicalNerModel) that is based on BiLSTM-CNN-Char architecture. It can detect Death_Entity, Medical_Device, Vital_Sign, Alergen, Drug, Clinical_Dept, Lifestyle, Symptom, Body_Part, Physical_Measurement, Admission_Discharge, Date_Time, Age, Birth_Entity, Header, Oncological, Substance_Quantity, Test_Result, Test, Procedure, Treatment, Disease_Syndrome_Disorder, Pregnancy_Newborn, Demographics entities. Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_jsl_slim&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[documentAssembler, sentence_detector, tokenizer, tokenClassifier, ner_converter]) p_model = pipeline.fit(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [&#39;&#39;]}))) test_sentence = &quot;&quot;&quot;HISTORY: 30-year-old female presents for digital bilateral mammography secondary to a soft tissue lump palpated by the patient in the upper right shoulder. The patient has a family history of breast cancer within her mother at age 58. Patient denies personal history of breast cancer.&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +-++ |chunk |ner_label | +-++ |HISTORY: |Header | |30-year-old |Age | |female |Demographics| |mammography |Test | |soft tissue lump|Symptom | |shoulder |Body_Part | |breast cancer |Oncological | |her mother |Demographics| |age 58 |Age | |breast cancer |Oncological | +-++ bert_token_classifier_ner_drugs : This model is BERT-based version of ner_drugs model and detects drug chemicals. This new model is 3% better than the legacy NER model (MedicalNerModel) that is based on BiLSTM-CNN-Char architecture. Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_drugs&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;sentence&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[documentAssembler, sentenceDetector, tokenizer, tokenClassifier, ner_converter]) model = pipeline.fit(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [&#39;&#39;]}))) test_sentence = &quot;&quot;&quot;The human KCNJ9 (Kir 3.3, GIRK3) is a member of the G-protein-activated inwardly rectifying potassium (GIRK) channel family. Here we describe the genomicorganization of the KCNJ9 locus on chromosome 1q21-23 as a candidate gene forType II diabetes mellitus in the Pima Indian population. The gene spansapproximately 7.6 kb and contains one noncoding and two coding exons separated byapproximately 2.2 and approximately 2.6 kb introns, respectively. We identified14 single nucleotide polymorphisms (SNPs), including one that predicts aVal366Ala substitution, and an 8 base-pair (bp) insertion/deletion. Ourexpression studies revealed the presence of the transcript in various humantissues including pancreas, and two major insulin-responsive tissues: fat andskeletal muscle. The characterization of the KCNJ9 gene should facilitate furtherstudies on the function of the KCNJ9 protein and allow evaluation of thepotential role of the locus in Type II diabetes.BACKGROUND: At present, it is one of the most important issues for the treatment of breast cancer to develop the standard therapy for patients previously treated with anthracyclines and taxanes. With the objective of determining the usefulnessof vinorelbine monotherapy in patients with advanced or recurrent breast cancerafter standard therapy, we evaluated the efficacy and safety of vinorelbine inpatients previously treated with anthracyclines and taxanes.&quot;&quot;&quot; result = model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +--++ |chunk |ner_label| +--++ |potassium |DrugChem | |nucleotide |DrugChem | |anthracyclines|DrugChem | |taxanes |DrugChem | |vinorelbine |DrugChem | |vinorelbine |DrugChem | |anthracyclines|DrugChem | |taxanes |DrugChem | +--++ bert_token_classifier_ner_anatomy : This model is BERT-Based version of ner_anatomy model and 3% better. It can detect Anatomical_system, Cell, Cellular_component, Developing_anatomical_structure, Immaterial_anatomical_entity, Multi-tissue_structure, Organ, Organism_subdivision, Organism_substance, Pathological_formation, Tissue entities. Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_anatomy&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;sentence&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[documentAssembler, sentenceDetector, tokenizer, tokenClassifier, ner_converter]) pp_model = pipeline.fit(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [&#39;&#39;]}))) test_sentence = &quot;&quot;&quot;This is an 11-year-old female who comes in for two different things. 1. She was seen by the allergist. No allergies present, so she stopped her Allegra, but she is still real congested and does a lot of snorting. They do not notice a lot of snoring at night though, but she seems to be always like that. 2. On her right great toe, she has got some redness and erythema. Her skin is kind of peeling a little bit, but it has been like that for about a week and a half now. nGeneral: Well-developed female, in no acute distress, afebrile. nHEENT: Sclerae and conjunctivae clear. Extraocular muscles intact. TMs clear. Nares patent. A little bit of swelling of the turbinates on the left. Oropharynx is essentially clear. Mucous membranes are moist. nNeck: No lymphadenopathy. nChest: Clear. nAbdomen: Positive bowel sounds and soft. nDermatologic: She has got redness along her right great toe, but no bleeding or oozing. Some dryness of her skin. Her toenails themselves are very short and even on her left foot and her left great toe the toenails are very short.&quot;&quot;&quot; result = pp_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +-+-+ |chunk |ner_label | +-+-+ |great toe |Multi-tissue_structure| |skin |Organ | |conjunctivae |Multi-tissue_structure| |Extraocular muscles|Multi-tissue_structure| |Nares |Multi-tissue_structure| |turbinates |Multi-tissue_structure| |Oropharynx |Multi-tissue_structure| |Mucous membranes |Tissue | |Neck |Organism_subdivision | |bowel |Organ | |great toe |Multi-tissue_structure| |skin |Organ | |toenails |Organism_subdivision | |foot |Organism_subdivision | |great toe |Multi-tissue_structure| |toenails |Organism_subdivision | +-+-+ bert_token_classifier_ner_bacteria : This model is BERT-Based version of ner_bacterial_species model and detects different types of species of bacteria in clinical texts using SPECIES label. Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_bacteria&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ner_converter = NerConverter() .setInputCols([&quot;document&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) pipeline = Pipeline(stages=[documentAssembler, tokenizer, tokenClassifier, ner_converter]) p_model = pipeline.fit(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [&#39;&#39;]}))) test_sentence = &quot;&quot;&quot;Based on these genetic and phenotypic properties, we propose that strain SMSP (T) represents a novel species of the genus Methanoregula, for which we propose the name Methanoregula formicica sp. nov., with the type strain SMSP (T) (= NBRC 105244 (T) = DSM 22288 (T)).&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +--++ |chunk |ner_label| +--++ |SMSP (T) |SPECIES | |Methanoregula formicica|SPECIES | |SMSP (T) |SPECIES | +--++ Radiology NER Model Trained On cheXpert Dataset Ner NER model ner_chexpert trained on Radiology Chest reports to extract anatomical sites and observation entities. The model achieves 92.8% and 77.4% micro and macro f1 scores on the cheXpert dataset. Example : ... embeddings_clinical = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_chexpert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... nlpPipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, embeddings_clinical, clinical_ner, ner_converter]) model = nlpPipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)) EXAMPLE_TEXT = &quot;&quot;&quot;FINAL REPORT HISTORY : Chest tube leak , to assess for pneumothorax . FINDINGS : In comparison with study of ___ , the endotracheal tube and Swan - Ganz catheter have been removed . The left chest tube remains in place and there is no evidence of pneumothorax. Mild atelectatic changes are seen at the left base.&quot;&quot;&quot; results = model.transform(spark.createDataFrame([[EXAMPLE_TEXT]]).toDF(&quot;text&quot;)) Results : | | chunk | label | |:|:-|:--| | 0 | endotracheal tube | OBS | | 1 | Swan - Ganz catheter | OBS | | 2 | left chest | ANAT | | 3 | tube | OBS | | 4 | in place | OBS | | 5 | pneumothorax | OBS | | 6 | Mild atelectatic changes | OBS | | 7 | left base | ANAT | New Speed Benchmarks on Databricks We prepared a speed benchmark table by running a NER pipeline on various number of cluster configurations (worker number, driver node, specs etc) and also writing the results to parquet or delta formats. You can find all the details of these tries in here : Speed Benchmark Table NerConverterInternal Fixes Now NerConverterInternal can deal with tags that have some dash (-) charachter like B-GENE-N and B-GENE-Y. Simplified Setup and Recommended Use of start() Function Starting with this release, we are shipping AWS credentials inside Spark NLP Healthcare’s license. This removes the requirement of setting the AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY environment variables. To use this feature, you just need to make sure that you always call the start() function at the beginning of your program, from sparknlp_jsl import start spark = start() import com.johnsnowlabs.util.start val spark = start() If for some reason you don’t want to use this mechanism, the keys will continue to be shipped separately, and the environment variables will continue to work as they did in the past. Ner Evaluation Metrics Fix Bug fixed in the NerDLMetrics package. Previously, the full_chunk option was using greedy approach to merge chunks for a strict evaluation, which has been fixed to merge chunks using IOB scheme to get accurate entities boundaries and metrics. Also, the tag option has been fixed to get metrics that align with the default NER logs. New Notebooks Clinical Relation Extraction Knowledge Graph with Neo4j Notebook NER Profiling Pretrained Pipelines Notebook New Databricks Detecting Adverse Drug Events From Conversational Texts case study notebook. To see more, please check : Spark NLP Healthcare Workshop Repo Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_3_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_3_0"
  },
  "209": {
    "id": "209",
    "title": "Annotation Lab Release Notes 3.3.0",
    "content": "3.3.0 Release date: 21-06-2022 We are very excited to announce the release of Annotation Lab v3.3.0 which includes a highly requested new feature for displaying the confidence scores for NER preannotations as well as the ability to filter preannotations by confidence. Also, benchmarking data can now be checked for some of the models on the Models Hub page. This version also includes IAA charts for Visual NER Projects, upgrades of the Spark NLP libraries and fixes for some of the identified Common Vulnerabilities and Exposures (CVEs). Below are more details on the release content. Highlights Confidence Scores for Preannotations, When running preannotations on a Text project, one extra piece of information is now present for the automatic annotations - the confidence score. This score is used to show the confidence the model has for each of the labeled chunks. It is calculated based on the benchmarking information of the model used to preannotate and on the score of each prediction. The confidence score is available when working on Named Entity Recognition, Relation, Assertion, and Classification projects and is also generated when using NER Rules. On the Labeling screen, when selecting the Prediction widget, users can see that all preannotation in the Results section now have a score assigned to them. IAA charts are now available for Visual NER Projects, IAA (Inter-Annotator Agreement) charts were available only for text-based projects. With this release, Annotation Lab supports IAA charts for Visual NER project as well. Auto-save completions, the work of annotators is automatically saved behind the scenes. This way, the user does not risk losing his/her work in case of unforeseen events and does not have to frequently hit the Save/Update button. Improvement of UX for Active Learning, information about the previously triggered Active learning is displayed along with the number of completions required for the next training. Also when the conditions that trigger active learning for a project using a healthcare model are met and all available licenses are in use, an error message appears on the Training and Active Learning page informing the user to make room for the new training server Support for BertForSequenceClassification and MedicalBertForSequenceClassification models, From this version on, support was added for BertForTokenClassification, MedicalBertForTokenClassifier, BertForSequenceClassification and MedicalBertForSequenceClassification. Upgraded Spark NLP and Spark NLP for Health Care v3.5.3 and Spark OCR v3.13.0. With this we have also updated the list of supported models into the Models Hub page. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_3_3_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_3_3_0"
  },
  "210": {
    "id": "210",
    "title": "Spark NLP release notes 3.3.0",
    "content": "3.3.0 Release date: 14-06-2021 Overview Table detection and recognition for scanned documents. For table detection we added ImageTableDetector. It’s based on CascadeTabNet which used Cascade mask Region-based CNN High-Resolution Network (Cascade mask R-CNN HRNet). The model was pre-trained on the COCO dataset and fine-tuned on ICDAR 2019 competitions dataset for table detection. It demonstrates state of the art results for ICDAR 2013 and TableBank. And top results for ICDAR 2019. More details please read in Table Detection &amp; Extraction in Spark OCR New Features ImageTableDetector is a DL model for detect tables on the image. ImageTableCellDetector is a transformer for detect regions of cells in the table image. ImageCellsToTextTable is a transformer for extract text from the detected cells. New notebooks Image Table Detection example Image Cell Recognition example Image Table Recognition Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_3_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_3_0"
  },
  "211": {
    "id": "211",
    "title": "Spark NLP for Healthcare Release Notes 3.3.1",
    "content": "3.3.1 We are glad to announce that Spark NLP Healthcare 3.3.1 has been released!. Highlights New ChunkKeyPhraseExtraction Annotator New BERT-Based NER Models New UMLS Sentence Entity Resolver Models Updated RxNorm Entity Resolver Model (Dropping Invalid Codes) New showVersion() Method in Compatibility Class New Docker Images for Spark NLP for Healthcare and Spark OCR New and Updated Deidentification() Parameters New Python API Documentation Updated Spark NLP For Healthcare Notebooks and New Notebooks New ChunkKeyPhraseExtraction Annotator We are releasing ChunkKeyPhraseExtraction annotator that leverages Sentence BERT embeddings to select keywords and key phrases that are most similar to a document. This annotator can be fed by either the output of NER model, NGramGenerator or YAKE, and could be used to generate similarity scores for each NER chunk that is coming out of any (clinical) NER model. That is, you can now sort your clinical entities by the importance of them with respect to document or sentence that they live in. Additionally, you can also use this new annotator to grab new clinical chunks that are missed by a pretrained NER model as well as summarizing the whole document into a few important sentences or phrases. You can find more examples in ChunkKeyPhraseExtraction notebook Example : ... ngram_ner_key_phrase_extractor = ChunkKeyPhraseExtraction.pretrained(&quot;sbert_jsl_medium_uncased &quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setTopN(5) .setDivergence(0.4) .setInputCols([&quot;sentences&quot;, &quot;merged_chunks&quot;]) .setOutputCol(&quot;key_phrases&quot;) ... text = &quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting . Two weeks prior to presentation, she was treated with a five-day course of amoxicillin for a respiratory tract infection. She was on metformin , glipizide , and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG. She had been on dapagliflozin for six months at the time of presentation . Physical examination on presentation was significant for dry oral mucosa ; significantly, her abdominal examination was benign with no tenderness , guarding , or rigidity . Pertinent laboratory findings on admission were: serum glucose 111 mg/dl , bicarbonate 18 mmol/l , anion gap 20 , creatinine 0.4 mg/dL , triglycerides 508 mg/dL , total cholesterol 122 mg/dL , glycated hemoglobin ( HbA1c ) 10% , and venous pH 7.27. Serum lipase was normal at 43 U/L . Serum acetone levels could not be assessed as blood samples kept hemolyzing due to significant lipemia .&quot; textDF = spark.createDataFrame([[text]]).toDF(&quot;text&quot;) ngram_ner_results = ngram_ner_pipeline.transform(textDF) Results : +--++-+-+--+ |key_phrase |source|DocumentSimilarity |MMRScore |sentence| +--++-+-+--+ |type two diabetes mellitus|NER |0.7639750686118073 |0.4583850593816694 |0 | |HTG-induced pancreatitis |ngrams|0.66933222897749 |0.10416352343367463|0 | |vomiting |ngrams|0.5824238088130589 |0.14864183399720493|0 | |history polyuria |ngrams|0.46337313737310987|0.0959500325843913 |0 | |28-year-old female |ngrams|0.31692529374916967|0.10043002919664669|0 | +--++-+-+--+ New BERT-Based NER Models We have two new BERT-Based token classifier NER models. bert_token_classifier_ner_chemicals : This model is BERT-based version of ner_chemicals model and can detect chemical compounds (CHEM) in the medical texts. Metrics : precision recall f1-score support B-CHEM 0.94 0.92 0.93 30731 I-CHEM 0.95 0.93 0.94 31270 accuracy 0.99 62001 macro avg 0.96 0.95 0.96 62001 weighted avg 0.99 0.93 0.96 62001 Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_chemicals&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ... test_sentence = &quot;&quot;&quot;The results have shown that the product p - choloroaniline is not a significant factor in chlorhexidine - digluconate associated erosive cystitis. A high percentage of kanamycin - colistin and povidone - iodine irrigations were associated with erosive cystitis.&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame([[test_sentence]]).toDF(&quot;text&quot;)) Results : +++ |chunk |ner_label| +++ |p - choloroaniline |CHEM | |chlorhexidine - digluconate|CHEM | |kanamycin |CHEM | |colistin |CHEM | |povidone - iodine |CHEM | +++ bert_token_classifier_ner_chemprot : This model is BERT-based version of ner_chemprot_clinical model and can detect chemical compounds and genes (CHEMICAL, GENE-Y, GENE-N) in the medical texts. Metrics : precision recall f1-score support B-CHEMICAL 0.80 0.79 0.80 8649 B-GENE-N 0.53 0.56 0.54 2752 B-GENE-Y 0.71 0.73 0.72 5490 I-CHEMICAL 0.82 0.79 0.81 1313 I-GENE-N 0.62 0.62 0.62 1993 I-GENE-Y 0.75 0.72 0.74 2420 accuracy 0.96 22617 macro avg 0.75 0.74 0.75 22617 weighted avg 0.83 0.73 0.78 22617 Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_chemprot&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ... test_sentence = &quot;Keratinocyte growth factor and acidic fibroblast growth factor are mitogens for primary cultures of mammary epithelium.&quot; result = p_model.transform(spark.createDataFrame([[test_sentence]]).toDF(&quot;text&quot;)) Results : +-++ |chunk |ner_label| +-++ |Keratinocyte growth factor |GENE-Y | |acidic fibroblast growth factor|GENE-Y | +-++ New UMLS Sentence Entity Resolver Models We are releasing two new UMLS Sentence Entity Resolver models trained on 2021AB UMLS dataset and map clinical entities to UMLS CUI codes. sbiobertresolve_umls_disease_syndrome : This model is trained on the Disease or Syndrome category using sbiobert_base_cased_mli embeddings. Example : ... resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_umls_disease_syndrome&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) ... data = spark.createDataFrame([[&quot;&quot;&quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus (T2DM), one prior episode of HTG-induced pancreatitis three years prior to presentation, associated with an acute hepatitis, and obesity with a body mass index (BMI) of 33.5 kg/m2, presented with a one-week history of polyuria, polydipsia, poor appetite, and vomiting.&quot;&quot;&quot;]]).toDF(&quot;text&quot;) results = model.fit(data).transform(data) Results : | | chunk | code | code_description | all_k_codes | all_k_codes_desc | |:|:--|:|:--|:-|:| | 0 | gestational diabetes mellitus | C0085207 | gestational diabetes mellitus | [&#39;C0085207&#39;, &#39;C0032969&#39;, &#39;C2063017&#39;, &#39;C1283034&#39;, &#39;C0271663&#39;] | [&#39;gestational diabetes mellitus&#39;, &#39;pregnancy diabetes mellitus&#39;, &#39;pregnancy complicated by diabetes mellitus&#39;, &#39;maternal diabetes mellitus&#39;, &#39;gestational diabetes mellitus, a2&#39;] | | 1 | subsequent type two diabetes mellitus | C0348921 | pre-existing type 2 diabetes mellitus | [&#39;C0348921&#39;, &#39;C1719939&#39;, &#39;C0011860&#39;, &#39;C0877302&#39;, &#39;C0271640&#39;] | [&#39;pre-existing type 2 diabetes mellitus&#39;, &#39;disorder associated with type 2 diabetes mellitus&#39;, &#39;diabetes mellitus, type 2&#39;, &#39;insulin-requiring type 2 diabetes mellitus&#39;, &#39;secondary diabetes mellitus&#39;] | | 2 | HTG-induced pancreatitis | C0376670 | alcohol-induced pancreatitis | [&#39;C0376670&#39;, &#39;C1868971&#39;, &#39;C4302243&#39;, &#39;C0267940&#39;, &#39;C2350449&#39;] | [&#39;alcohol-induced pancreatitis&#39;, &#39;toxic pancreatitis&#39;, &#39;igg4-related pancreatitis&#39;, &#39;hemorrhage pancreatitis&#39;, &#39;graft pancreatitis&#39;] | | 3 | an acute hepatitis | C0019159 | acute hepatitis | [&#39;C0019159&#39;, &#39;C0276434&#39;, &#39;C0267797&#39;, &#39;C1386146&#39;, &#39;C2063407&#39;] | [&#39;acute hepatitis a&#39;, &#39;acute hepatitis a&#39;, &#39;acute hepatitis&#39;, &#39;acute infectious hepatitis&#39;, &#39;acute hepatitis e&#39;] | | 4 | obesity | C0028754 | obesity | [&#39;C0028754&#39;, &#39;C0342940&#39;, &#39;C0342942&#39;, &#39;C0857116&#39;, &#39;C1561826&#39;] | [&#39;obesity&#39;, &#39;abdominal obesity&#39;, &#39;generalized obesity&#39;, &#39;obesity gross&#39;, &#39;overweight and obesity&#39;] | | 5 | polyuria | C0018965 | hematuria | [&#39;C0018965&#39;, &#39;C0151582&#39;, &#39;C3888890&#39;, &#39;C0268556&#39;, &#39;C2936921&#39;] | [&#39;hematuria&#39;, &#39;uricosuria&#39;, &#39;polyuria-polydipsia syndrome&#39;, &#39;saccharopinuria&#39;, &#39;saccharopinuria&#39;] | | 6 | polydipsia | C0268813 | primary polydipsia | [&#39;C0268813&#39;, &#39;C0030508&#39;, &#39;C3888890&#39;, &#39;C0393777&#39;, &#39;C0206085&#39;] | [&#39;primary polydipsia&#39;, &#39;parasomnia&#39;, &#39;polyuria-polydipsia syndrome&#39;, &#39;hypnogenic paroxysmal dystonias&#39;, &#39;periodic hypersomnias&#39;] | | 7 | poor appetite | C0003123 | lack of appetite | [&#39;C0003123&#39;, &#39;C0011168&#39;, &#39;C0162429&#39;, &#39;C1282895&#39;, &#39;C0039338&#39;] | [&#39;lack of appetite&#39;, &#39;poor swallowing&#39;, &#39;poor nutrition&#39;, &#39;neurologic unpleasant taste&#39;, &#39;taste dis&#39;] | | 8 | vomiting | C0152164 | periodic vomiting | [&#39;C0152164&#39;, &#39;C0267172&#39;, &#39;C0152517&#39;, &#39;C0011119&#39;, &#39;C0152227&#39;] | [&#39;periodic vomiting&#39;, &#39;habit vomiting&#39;, &#39;viral vomiting&#39;, &#39;choking&#39;, &#39;tearing&#39;] | sbiobertresolve_umls_clinical_drugs : This model is trained on the Clinical Drug category using sbiobert_base_cased_mli embeddings. Example : ... resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_umls_clinical_drugs&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) ... data = spark.createDataFrame([[&quot;&quot;&quot;She was immediately given hydrogen peroxide 30 mg to treat the infection on her leg, and has been advised Neosporin Cream for 5 days. She has a history of taking magnesium hydroxide 100mg/1ml and metformin 1000 mg.&quot;&quot;&quot;]]).toDF(&quot;text&quot;) results = model.fit(data).transform(data) Results : | | chunk | code | code_description | all_k_codes | all_k_codes_desc | |:|:|:|:|:-|:-| | 0 | hydrogen peroxide 30 mg | C1126248 | hydrogen peroxide 30 mg/ml | [&#39;C1126248&#39;, &#39;C0304655&#39;, &#39;C1605252&#39;, &#39;C0304656&#39;, &#39;C1154260&#39;] | [&#39;hydrogen peroxide 30 mg/ml&#39;, &#39;hydrogen peroxide solution 30%&#39;, &#39;hydrogen peroxide 30 mg/ml [proxacol]&#39;, &#39;hydrogen peroxide 30 mg/ml cutaneous solution&#39;, &#39;benzoyl peroxide 30 mg/ml&#39;] | | 1 | Neosporin Cream | C0132149 | neosporin cream | [&#39;C0132149&#39;, &#39;C0358174&#39;, &#39;C0357999&#39;, &#39;C0307085&#39;, &#39;C0698810&#39;] | [&#39;neosporin cream&#39;, &#39;nystan cream&#39;, &#39;nystadermal cream&#39;, &#39;nupercainal cream&#39;, &#39;nystaform cream&#39;] | | 2 | magnesium hydroxide 100mg/1ml | C1134402 | magnesium hydroxide 100 mg | [&#39;C1134402&#39;, &#39;C1126785&#39;, &#39;C4317023&#39;, &#39;C4051486&#39;, &#39;C4047137&#39;] | [&#39;magnesium hydroxide 100 mg&#39;, &#39;magnesium hydroxide 100 mg/ml&#39;, &#39;magnesium sulphate 100mg/ml injection&#39;, &#39;magnesium sulfate 100 mg&#39;, &#39;magnesium sulfate 100 mg/ml&#39;] | | 3 | metformin 1000 mg | C0987664 | metformin 1000 mg | [&#39;C0987664&#39;, &#39;C2719784&#39;, &#39;C0978482&#39;, &#39;C2719786&#39;, &#39;C4282269&#39;] | [&#39;metformin 1000 mg&#39;, &#39;metformin hydrochloride 1000 mg&#39;, &#39;metformin hcl 1000mg tab&#39;, &#39;metformin hydrochloride 1000 mg [fortamet]&#39;, &#39;metformin hcl 1000mg sa tab&#39;] | Updated RxNorm Entity Resolver Model (Dropping Invalid Codes) sbiobertresolve_rxnorm model was updated by dropping invalid codes using 02 August 2021 RxNorm dataset. New showVersion() Method in Compatibility Class We added the .showVersion() method in our Compatibility class that shows the name of the models and the version in a pretty way. compatibility = Compatibility() compatibility.showVersion(&#39;sentence_detector_dl_healthcare&#39;) After the execution you will see the following table, ++++ | Pipeline/Model | lang | version | ++++ | sentence_detector_dl_healthcare | en | 2.6.0 | | sentence_detector_dl_healthcare | en | 2.7.0 | | sentence_detector_dl_healthcare | en | 3.2.0 | ++++ New Docker Images for Spark NLP for Healthcare and Spark OCR We are releasing new Docker Images for Spark NLP for Healthcare and Spark OCR containing a jupyter environment. Users having a valid license can run the image on their local system, and connect to pre-configured jupyter instance without installing the library on their local system. Spark NLP for Healthcare Docker Image For running Spark NLP for Healthcare inside a container: Instructions: Spark NLP for Healthcare Docker Image Video Instructions: Youtube Video Spark NLP for Healthcare &amp; OCR Docker Image For users who want to run Spark OCR and then feed the output of OCR pipeline to healthcare modules to process further: Instructions: Spark NLP for Healthcare &amp; OCR Docker Image New and Updated Deidentification() Parameters New Parameter : setBlackList() : List of entities ignored for masking or obfuscation.The default values are: SSN, PASSPORT, DLN, NPI, C_CARD, IBAN, DEA. Updated Parameter : .setObfuscateRefSource() : It was set faker as default. New Python API Documentation We have new Spark NLP for Healthcare Python API Documentation . This page contains information how to use the library with Python examples. Updated Spark NLP For Healthcare Notebooks and New Notebooks New BertForTokenClassification NER Model Training with Transformers Notebook for showing how to train a BertForTokenClassification NER model with transformers and then import into Spark NLP. New ChunkKeyPhraseExtraction notebook for showing how to get chunk key phrases using ChunkKeyPhraseExtraction. Updated all Spark NLP For Healthcare Notebooks with v3.3.0 by adding the new features. To see more, please check : Spark NLP Healthcare Workshop Repo Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_3_1",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_3_1"
  },
  "212": {
    "id": "212",
    "title": "Annotation Lab Release Notes 3.3.1",
    "content": "3.3.1 Release date: 24-06-2022 We are very excited to announce the release of Annotation Lab v3.3.1 which includes updated Active Learning messages, bug fixed for importing dictionary rule, NER projects and Visual NER projects . Here are the highlights: Highlights Updated Active Learning statuses Fix for importing Visual NER task exported before v3.2.0. Fix for import of project from Windows OS Fix for import of dictionary rules Fix for show Score text is Results widget on the labeling page when the confidence score is null Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_3_3_1",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_3_3_1"
  },
  "213": {
    "id": "213",
    "title": "Spark NLP for Healthcare Release Notes 3.3.2",
    "content": "3.3.2 We are glad to announce that Spark NLP Healthcare 3.3.2 has been released!. Highlights New Clinical NER Models and Spanish NER Model New BERT-Based Clinical NER Models Updated Clinical NER Model New NER Model Class Distribution Feature New RxNorm Sentence Entity Resolver Model New Spanish SNOMED Sentence Entity Resolver Model New Clinical Question vs Statement BertForSequenceClassification model New Sentence Entity Resolver Fine-Tune Features (Overwriting and Drop Code) Updated ICD10CM Entity Resolver Models Updated NER Profiling Pretrained Pipelines New ChunkSentenceSplitter Annotator Updated Spark NLP For Healthcare Notebooks and New Notebooks New Clinical NER Models (including a new Spanish one) We are releasing three new clinical NER models trained by MedicalNerApproach(). roberta_ner_diag_proc : This models leverages Spanish Roberta Biomedical Embeddings (roberta_base_biomedical) to extract two entities, Diagnosis and Procedures (DIAGNOSTICO, PROCEDIMIENTO). It’s a renewed version of ner_diag_proc_es, available here, that was trained with embeddings_scielowiki_300d embeddings instead. Example : ... embeddings = RoBertaEmbeddings.pretrained(&quot;roberta_base_biomedical&quot;, &quot;es&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner = MedicalNerModel.pretrained(&quot;roberta_ner_diag_proc&quot;, &quot;es&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&#39;sentence&#39;, &#39;token&#39;, &#39;ner&#39;]) .setOutputCol(&#39;ner_chunk&#39;) pipeline = Pipeline(stages = [ documentAssembler, sentenceDetector, tokenizer, embeddings, ner, ner_converter]) empty = spark.createDataFrame([[&#39;&#39;]]).toDF(&quot;text&quot;) p_model = pipeline.fit(empty) test_sentence = &#39;Mujer de 28 años con antecedentes de diabetes mellitus gestacional diagnosticada ocho años antes de la presentación y posterior diabetes mellitus tipo dos (DM2), un episodio previo de pancreatitis inducida por HTG tres años antes de la presentación, asociado con una hepatitis aguda, y obesidad con un índice de masa corporal (IMC) de 33,5 kg / m2, que se presentó con antecedentes de una semana de poliuria, polidipsia, falta de apetito y vómitos. Dos semanas antes de la presentación, fue tratada con un ciclo de cinco días de amoxicilina por una infección del tracto respiratorio. Estaba tomando metformina, glipizida y dapagliflozina para la DM2 y atorvastatina y gemfibrozil para la HTG. Había estado tomando dapagliflozina durante seis meses en el momento de la presentación. El examen físico al momento de la presentación fue significativo para la mucosa oral seca; significativamente, su examen abdominal fue benigno sin dolor a la palpación, protección o rigidez. Los hallazgos de laboratorio pertinentes al ingreso fueron: glucosa sérica 111 mg / dl, bicarbonato 18 mmol / l, anión gap 20, creatinina 0,4 mg / dl, triglicéridos 508 mg / dl, colesterol total 122 mg / dl, hemoglobina glucosilada (HbA1c) 10%. y pH venoso 7,27. La lipasa sérica fue normal a 43 U / L. Los niveles séricos de acetona no pudieron evaluarse ya que las muestras de sangre se mantuvieron hemolizadas debido a una lipemia significativa. La paciente ingresó inicialmente por cetosis por inanición, ya que refirió una ingesta oral deficiente durante los tres días Previous a la admisión. Sin embargo, la química sérica obtenida seis horas después de la presentación reveló que su glucosa era de 186 mg / dL, la brecha aniónica todavía estaba elevada a 21, el bicarbonato sérico era de 16 mmol / L, el nivel de triglicéridos alcanzó un máximo de 2050 mg / dL y la lipasa fue de 52 U / L. Se obtuvo el nivel de β-hidroxibutirato y se encontró que estaba elevado a 5,29 mmol / L; la muestra original se centrifugó y la capa de quilomicrones se eliminó antes del análisis debido a la interferencia de la turbidez causada por la lipemia nuevamente. El paciente fue tratado con un goteo de insulina para euDKA y HTG con una reducción de la brecha aniónica a 13 y triglicéridos a 1400 mg / dL, dentro de las 24 horas. Se pensó que su euDKA fue precipitada por su infección del tracto respiratorio en el contexto del uso del inhibidor de SGLT2. La paciente fue atendida por el servicio de endocrinología y fue dada de alta con 40 unidades de insulina glargina por la noche, 12 unidades de insulina lispro con las comidas y metformina 1000 mg dos veces al día. Se determinó que todos los inhibidores de SGLT2 deben suspenderse indefinidamente. Tuvo un seguimiento estrecho con endocrinología post alta.&#39; res = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +++ | text|ner_label | +++ | diabetes mellitus gestacional|DIAGNOSTICO| | diabetes mellitus tipo dos|DIAGNOSTICO| | DM2|DIAGNOSTICO| | pancreatitis inducida por HTG|DIAGNOSTICO| | hepatitis aguda|DIAGNOSTICO| | obesidad|DIAGNOSTICO| | índice de masa corporal|DIAGNOSTICO| | IMC|DIAGNOSTICO| | poliuria|DIAGNOSTICO| | polidipsia|DIAGNOSTICO| | vómitos|DIAGNOSTICO| |infección del tracto respiratorio|DIAGNOSTICO| | DM2|DIAGNOSTICO| | HTG|DIAGNOSTICO| | dolor|DIAGNOSTICO| | rigidez|DIAGNOSTICO| | cetosis|DIAGNOSTICO| |infección del tracto respiratorio|DIAGNOSTICO| ++--+ ner_covid_trials : This model is trained to extract covid-specific medical entities in clinical trials. It supports the following entities ranging from virus type to trial design: Stage, Severity, Virus, Trial_Design, Trial_Phase, N_Patients, Institution, Statistical_Indicator, Section_Header, Cell_Type, Cellular_component, Viral_components, Physiological_reaction, Biological_molecules, Admission_Discharge, Age, BMI, Cerebrovascular_Disease, Date, Death_Entity, Diabetes, Disease_Syndrome_Disorder, Dosage, Drug_Ingredient, Employment, Frequency, Gender, Heart_Disease, Hypertension, Obesity, Pulse, Race_Ethnicity, Respiration, Route, Smoking, Time, Total_Cholesterol, Treatment, VS_Finding, Vaccine . Example : ... covid_ner = MedicalNerModel.pretrained(&#39;ner_covid_trials&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = covid_model.transform(spark.createDataFrame(pd.DataFrame({&quot;text&quot;: [&quot;&quot;&quot;In December 2019 , a group of patients with the acute respiratory disease was detected in Wuhan , Hubei Province of China . A month later , a new beta-coronavirus was identified as the cause of the 2019 coronavirus infection . SARS-CoV-2 is a coronavirus that belongs to the group of β-coronaviruses of the subgenus Coronaviridae . The SARS-CoV-2 is the third known zoonotic coronavirus disease after severe acute respiratory syndrome ( SARS ) and Middle Eastern respiratory syndrome ( MERS ). The diagnosis of SARS-CoV-2 recommended by the WHO , CDC is the collection of a sample from the upper respiratory tract ( nasal and oropharyngeal exudate ) or from the lower respiratory tract such as expectoration of endotracheal aspirate and bronchioloalveolar lavage and its analysis using the test of real-time polymerase chain reaction ( qRT-PCR ).&quot;&quot;&quot;]}))) Results : | | chunk | begin | end | entity | |:|:|--:|:|:--| | 0 | December 2019 | 3 | 15 | Date | | 1 | acute respiratory disease | 48 | 72 | Disease_Syndrome_Disorder | | 2 | beta-coronavirus | 146 | 161 | Virus | | 3 | 2019 coronavirus infection | 198 | 223 | Disease_Syndrome_Disorder | | 4 | SARS-CoV-2 | 227 | 236 | Virus | | 5 | coronavirus | 243 | 253 | Virus | | 6 | β-coronaviruses | 284 | 298 | Virus | | 7 | subgenus Coronaviridae | 307 | 328 | Virus | | 8 | SARS-CoV-2 | 336 | 345 | Virus | | 9 | zoonotic coronavirus disease | 366 | 393 | Disease_Syndrome_Disorder | | 10 | severe acute respiratory syndrome | 401 | 433 | Disease_Syndrome_Disorder | | 11 | SARS | 437 | 440 | Disease_Syndrome_Disorder | | 12 | Middle Eastern respiratory syndrome | 448 | 482 | Disease_Syndrome_Disorder | | 13 | MERS | 486 | 489 | Disease_Syndrome_Disorder | | 14 | SARS-CoV-2 | 511 | 520 | Virus | | 15 | WHO | 541 | 543 | Institution | | 16 | CDC | 547 | 549 | Institution | ner_chemd_clinical : This model extract the names of chemical compounds and drugs in medical texts. The entities that can be detected are as follows : SYSTEMATIC, IDENTIFIERS, FORMULA, TRIVIAL, ABBREVIATION, FAMILY, MULTIPLE . For reference click here . Example : ... chemd_ner = MedicalNerModel.pretrained(&#39;ner_chemd&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = chemd_model.transform(spark.createDataFrame(pd.DataFrame({&quot;text&quot;: [&quot;&quot;&quot;Isolation, Structure Elucidation, and Iron-Binding Properties of Lystabactins, Siderophores Isolated from a Marine Pseudoalteromonas sp. The marine bacterium Pseudoalteromonas sp. S2B, isolated from the Gulf of Mexico after the Deepwater Horizon oil spill, was found to produce lystabactins A, B, and C (1-3), three new siderophores. The structures were elucidated through mass spectrometry, amino acid analysis, and NMR. The lystabactins are composed of serine (Ser), asparagine (Asn), two formylated/hydroxylated ornithines (FOHOrn), dihydroxy benzoic acid (Dhb), and a very unusual nonproteinogenic amino acid, 4,8-diamino-3-hydroxyoctanoic acid (LySta). The iron-binding properties of the compounds were investigated through a spectrophotometric competition.&quot;&quot;&quot;]}))) Results : +-++ |chunk |ner_label | +-++ |Lystabactins |FAMILY | |lystabactins A, B, and C |MULTIPLE | |amino acid |FAMILY | |lystabactins |FAMILY | |serine |TRIVIAL | |Ser |FORMULA | |asparagine |TRIVIAL | |Asn |FORMULA | |formylated/hydroxylated ornithines|FAMILY | |FOHOrn |FORMULA | |dihydroxy benzoic acid |SYSTEMATIC | |amino acid |FAMILY | |4,8-diamino-3-hydroxyoctanoic acid|SYSTEMATIC | |LySta |ABBREVIATION| +-++ New BERT-Based Clinical NER Models We have two new BERT-Based token classifier NER models. bert_token_classifier_ner_bionlp : This model is BERT-based version of ner_bionlp model and can detect biological and genetics terms in cancer-related texts. (Amino_acid, Anatomical_system, Cancer, Cell, Cellular_component, Developing_anatomical_Structure, Gene_or_gene_product, Immaterial_anatomical_entity, Multi-tissue_structure, Organ, Organism, Organism_subdivision, Simple_chemical, Tissue) Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_bionlp&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ... test_sentence = &quot;&quot;&quot;Both the erbA IRES and the erbA/myb virus constructs transformed erythroid cells after infection of bone marrow or blastoderm cultures. The erbA/myb IRES virus exhibited a 5-10-fold higher transformed colony forming efficiency than the erbA IRES virus in the blastoderm assay.&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +-+-+ |chunk |ner_label | +-+-+ |erbA IRES |Organism | |erbA/myb virus |Organism | |erythroid cells |Cell | |bone marrow |Multi-tissue_structure| |blastoderm cultures|Cell | |erbA/myb IRES virus|Organism | |erbA IRES virus |Organism | |blastoderm |Cell | +-+-+ bert_token_classifier_ner_cellular : This model is BERT-based version of ner_cellular model and can detect molecular biology-related terms (DNA, Cell_type, Cell_line, RNA, Protein) in medical texts. Metrics : precision recall f1-score support B-DNA 0.87 0.77 0.82 1056 B-RNA 0.85 0.79 0.82 118 B-cell_line 0.66 0.70 0.68 500 B-cell_type 0.87 0.75 0.81 1921 B-protein 0.90 0.85 0.88 5067 I-DNA 0.93 0.86 0.90 1789 I-RNA 0.92 0.84 0.88 187 I-cell_line 0.67 0.76 0.71 989 I-cell_type 0.92 0.76 0.84 2991 I-protein 0.94 0.80 0.87 4774 accuracy 0.80 19392 macro avg 0.76 0.81 0.78 19392 weighted avg 0.89 0.80 0.85 19392 Example : ... tokenClassifier = BertForTokenClassification.pretrained(&quot;bert_token_classifier_ner_cellular&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ... test_sentence = &quot;&quot;&quot;Detection of various other intracellular signaling proteins is also described. Genetic characterization of transactivation of the human T-cell leukemia virus type 1 promoter: Binding of Tax to Tax-responsive element 1 is mediated by the cyclic AMP-responsive members of the CREB/ATF family of transcription factors. To achieve a better understanding of the mechanism of transactivation by Tax of human T-cell leukemia virus type 1 Tax-responsive element 1 (TRE-1), we developed a genetic approach with Saccharomyces cerevisiae. We constructed a yeast reporter strain containing the lacZ gene under the control of the CYC1 promoter associated with three copies of TRE-1. Expression of either the cyclic AMP response element-binding protein (CREB) or CREB fused to the GAL4 activation domain (GAD) in this strain did not modify the expression of the reporter gene. Tax alone was also inactive.&quot;&quot;&quot; result = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +-++ |chunk |ner_label| +-++ |intracellular signaling proteins |protein | |human T-cell leukemia virus type 1 promoter|DNA | |Tax |protein | |Tax-responsive element 1 |DNA | |cyclic AMP-responsive members |protein | |CREB/ATF family |protein | |transcription factors |protein | |Tax |protein | |human T-cell leukemia virus type 1 |DNA | |Tax-responsive element 1 |DNA | |TRE-1 |DNA | |lacZ gene |DNA | |CYC1 promoter |DNA | |TRE-1 |DNA | |cyclic AMP response element-binding protein|protein | |CREB |protein | |CREB |protein | |GAL4 activation domain |protein | |GAD |protein | |reporter gene |DNA | |Tax |protein | +-++ Updated Clinical NER Model We have updated ner_jsl_enriched model by enriching the training data using clinical trials data to make it more robust. This model is capable of predicting up to 87 different entities and is based on ner_jsl model. Here are the entities this model can detect; Social_History_Header, Oncology_Therapy, Blood_Pressure, Respiration, Performance_Status, Family_History_Header, Dosage, Clinical_Dept, Diet, Procedure, HDL, Weight, Admission_Discharge, LDL, Kidney_Disease, Oncological, Route, Imaging_Technique, Puerperium, Overweight, Temperature, Diabetes, Vaccine, Age, Test_Result, Employment, Time, Obesity, EKG_Findings, Pregnancy, Communicable_Disease, BMI, Strength, Tumor_Finding, Section_Header, RelativeDate, ImagingFindings, Death_Entity, Date, Cerebrovascular_Disease, Treatment, Labour_Delivery, Pregnancy_Delivery_Puerperium, Direction, Internal_organ_or_component, Psychological_Condition, Form, Medical_Device, Test, Symptom, Disease_Syndrome_Disorder, Staging, Birth_Entity, Hyperlipidemia, O2_Saturation, Frequency, External_body_part_or_region, Drug_Ingredient, Vital_Signs_Header, Substance_Quantity, Race_Ethnicity, VS_Finding, Injury_or_Poisoning, Medical_History_Header, Alcohol, Triglycerides, Total_Cholesterol, Sexually_Active_or_Sexual_Orientation, Female_Reproductive_Status, Relationship_Status, Drug_BrandName, RelativeTime, Duration, Hypertension, Metastasis, Gender, Oxygen_Therapy, Pulse, Heart_Disease, Modifier, Allergen, Smoking, Substance, Cancer_Modifier, Fetus_NewBorn, Height . Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_jsl_enriched&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = model.transform(spark.createDataFrame([[&quot;The patient is a 21-day-old Caucasian male here for 2 days of congestion - mom has been suctioning yellow discharge from the patient&#39;s nares, plus she has noticed some mild problems with his breathing while feeding (but negative for any perioral cyanosis or retractions). One day ago, mom also noticed a tactile temperature and gave the patient Tylenol. Baby also has had some decreased p.o. intake. His normal breast-feeding is down from 20 minutes q.2h. to 5 to 10 minutes secondary to his respiratory congestion. He sleeps well, but has been more tired and has been fussy over the past 2 days. The parents noticed no improvement with albuterol treatments given in the ER. His urine output has also decreased; normally he has 8 to 10 wet and 5 dirty diapers per 24 hours, now he has down to 4 wet diapers per 24 hours. Mom denies any diarrhea. His bowel movements are yellow colored and soft in nature.&quot;]], [&quot;text&quot;])) Results : | | chunk | begin | end | entity | |:|:|--:|:|:--| | 0 | 21-day-old | 17 | 26 | Age | | 1 | Caucasian | 28 | 36 | Race_Ethnicity | | 2 | male | 38 | 41 | Gender | | 3 | 2 days | 52 | 57 | Duration | | 4 | congestion | 62 | 71 | Symptom | | 5 | mom | 75 | 77 | Gender | | 6 | suctioning yellow discharge | 88 | 114 | Symptom | | 7 | nares | 135 | 139 | External_body_part_or_region | | 8 | she | 147 | 149 | Gender | | 9 | mild | 168 | 171 | Modifier | | 10 | problems with his breathing while feeding | 173 | 213 | Symptom | | 11 | perioral cyanosis | 237 | 253 | Symptom | | 12 | retractions | 258 | 268 | Symptom | | 13 | One day ago | 272 | 282 | RelativeDate | | 14 | mom | 285 | 287 | Gender | | 15 | tactile temperature | 304 | 322 | Symptom | | 16 | Tylenol | 345 | 351 | Drug_BrandName | | 17 | Baby | 354 | 357 | Age | | 18 | decreased p.o. intake | 377 | 397 | Symptom | | 19 | His | 400 | 402 | Gender | | 20 | q.2h | 450 | 453 | Frequency | | 21 | 5 to 10 minutes | 459 | 473 | Duration | | 22 | his | 488 | 490 | Gender | | 23 | respiratory congestion | 492 | 513 | Symptom | | 24 | He | 516 | 517 | Gender | | 25 | tired | 550 | 554 | Symptom | | 26 | fussy | 569 | 573 | Symptom | | 27 | over the past 2 days | 575 | 594 | RelativeDate | | 28 | albuterol | 637 | 645 | Drug_Ingredient | | 29 | ER | 671 | 672 | Clinical_Dept | | 30 | His | 675 | 677 | Gender | | 31 | urine output has also decreased | 679 | 709 | Symptom | | 32 | he | 721 | 722 | Gender | | 33 | per 24 hours | 760 | 771 | Frequency | | 34 | he | 778 | 779 | Gender | | 35 | per 24 hours | 807 | 818 | Frequency | | 36 | Mom | 821 | 823 | Gender | | 37 | diarrhea | 836 | 843 | Symptom | | 38 | His | 846 | 848 | Gender | | 39 | bowel | 850 | 854 | Internal_organ_or_component | New NER Model Class Distribution Feature getTrainingClassDistribution : This parameter returns the distribution of labels used when training the NER model. Example: ner_model.getTrainingClassDistribution() &gt;&gt; {&#39;B-Disease&#39;: 2536, &#39;O&#39;: 31659, &#39;I-Disease&#39;: 2960} New RxNorm Sentence Entity Resolver Model sbiobertresolve_rxnorm_augmented : This model maps clinical entities and concepts (like drugs/ingredients) to RxNorm codes using sbiobert_base_cased_mli Sentence Bert Embeddings. It trained on the augmented version of the dataset which is used in previous RxNorm resolver models. Additionally, this model returns concept classes of the drugs in all_k_aux_labels column. New Spanish SNOMED Sentence Entity Resolver Model robertaresolve_snomed : This models leverages Spanish Roberta Biomedical Embeddings (roberta_base_biomedical) at sentence-level to map ner chunks into Spanish SNOMED codes. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLModel.pretrained() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) word_embeddings = RoBertaEmbeddings.pretrained(&quot;roberta_base_biomedical&quot;, &quot;es&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;roberta_embeddings&quot;) ner = MedicalNerModel.pretrained(&quot;roberta_ner_diag_proc&quot;,&quot;es&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;roberta_embeddings&quot;) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) c2doc = Chunk2Doc() .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;ner_chunk_doc&quot;) chunk_embeddings = SentenceEmbeddings() .setInputCols([&quot;ner_chunk_doc&quot;, &quot;roberta_embeddings&quot;]) .setOutputCol(&quot;chunk_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) er = SentenceEntityResolverModel.pretrained(&quot;robertaresolve_snomed&quot;, &quot;es&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk_doc&quot;, &quot;chunk_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) snomed_training_pipeline = Pipeline(stages = [ documentAssembler, sentenceDetector, tokenizer, word_embeddings, ner, ner_converter, c2doc, chunk_embeddings, er]) empty = spark.createDataFrame([[&#39;&#39;]]).toDF(&quot;text&quot;) p_model = snomed_pipeline .fit(empty) test_sentence = &#39;Mujer de 28 años con antecedentes de diabetes mellitus gestacional diagnosticada ocho años antes de la presentación y posterior diabetes mellitus tipo dos (DM2), un episodio previo de pancreatitis inducida por HTG tres años antes de la presentación, asociado con una hepatitis aguda, y obesidad con un índice de masa corporal (IMC) de 33,5 kg / m2, que se presentó con antecedentes de una semana de poliuria, polidipsia, falta de apetito y vómitos. Dos semanas antes de la presentación, fue tratada con un ciclo de cinco días de amoxicilina por una infección del tracto respiratorio. Estaba tomando metformina, glipizida y dapagliflozina para la DM2 y atorvastatina y gemfibrozil para la HTG. Había estado tomando dapagliflozina durante seis meses en el momento de la presentación. El examen físico al momento de la presentación fue significativo para la mucosa oral seca; significativamente, su examen abdominal fue benigno sin dolor a la palpación, protección o rigidez. Los hallazgos de laboratorio pertinentes al ingreso fueron: glucosa sérica 111 mg / dl, bicarbonato 18 mmol / l, anión gap 20, creatinina 0,4 mg / dl, triglicéridos 508 mg / dl, colesterol total 122 mg / dl, hemoglobina glucosilada (HbA1c) 10%. y pH venoso 7,27. La lipasa sérica fue normal a 43 U / L. Los niveles séricos de acetona no pudieron evaluarse ya que las muestras de sangre se mantuvieron hemolizadas debido a una lipemia significativa. La paciente ingresó inicialmente por cetosis por inanición, ya que refirió una ingesta oral deficiente durante los tres días Previous a la admisión. Sin embargo, la química sérica obtenida seis horas después de la presentación reveló que su glucosa era de 186 mg / dL, la brecha aniónica todavía estaba elevada a 21, el bicarbonato sérico era de 16 mmol / L, el nivel de triglicéridos alcanzó un máximo de 2050 mg / dL y la lipasa fue de 52 U / L. Se obtuvo el nivel de β-hidroxibutirato y se encontró que estaba elevado a 5,29 mmol / L; la muestra original se centrifugó y la capa de quilomicrones se eliminó antes del análisis debido a la interferencia de la turbidez causada por la lipemia nuevamente. El paciente fue tratado con un goteo de insulina para euDKA y HTG con una reducción de la brecha aniónica a 13 y triglicéridos a 1400 mg / dL, dentro de las 24 horas. Se pensó que su euDKA fue precipitada por su infección del tracto respiratorio en el contexto del uso del inhibidor de SGLT2. La paciente fue atendida por el servicio de endocrinología y fue dada de alta con 40 unidades de insulina glargina por la noche, 12 unidades de insulina lispro con las comidas y metformina 1000 mg dos veces al día. Se determinó que todos los inhibidores de SGLT2 deben suspenderse indefinidamente. Tuvo un seguimiento estrecho con endocrinología post alta.&#39; res = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: [test_sentence]}))) Results : +-+-+-+--+ | | ner_chunk | entity | snomed_code| |-+-+-+--| | 0 | diabetes mellitus gestacional | DIAGNOSTICO | 11687002 | | 1 | diabetes mellitus tipo dos ( | DIAGNOSTICO | 44054006 | | 2 | pancreatitis | DIAGNOSTICO | 75694006 | | 3 | HTG | DIAGNOSTICO | 266569009 | | 4 | hepatitis aguda | DIAGNOSTICO | 37871000 | | 5 | obesidad | DIAGNOSTICO | 5476005 | | 6 | índice de masa corporal | DIAGNOSTICO | 162859006 | | 7 | poliuria | DIAGNOSTICO | 56574000 | | 8 | polidipsia | DIAGNOSTICO | 17173007 | | 9 | falta de apetito | DIAGNOSTICO | 49233005 | | 10 | vómitos | DIAGNOSTICO | 422400008 | | 11 | infección | DIAGNOSTICO | 40733004 | | 12 | HTG | DIAGNOSTICO | 266569009 | | 13 | dolor | DIAGNOSTICO | 22253000 | | 14 | rigidez | DIAGNOSTICO | 271587009 | | 15 | cetosis | DIAGNOSTICO | 2538008 | | 16 | infección | DIAGNOSTICO | 40733004 | +-+-+-+--+ New Clinical Question vs Statement BertForSequenceClassification model bert_sequence_classifier_question_statement_clinical : This model classifies sentences into one of these two classes: question (interrogative sentence) or statement (declarative sentence) and trained with BertForSequenceClassification. This model is at first trained on SQuAD and SPAADIA dataset and then fine tuned on the clinical visit documents and MIMIC-III dataset annotated in-house. Using this model, you can find the question statements and exclude &amp; utilize in the downstream tasks such as NER and relation extraction models. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLModel.pretrained() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) seq = BertForSequenceClassification.pretrained(&#39;bert_sequence_classifier_question_statement_clinical&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;label&quot;) .setCaseSensitive(True) pipeline = Pipeline(stages = [ documentAssembler, sentenceDetector, tokenizer, seq]) test_sentences = [&quot;&quot;&quot;Hello I am going to be having a baby throughand have just received my medical results before I have my tubes tested. I had the tests on day 23 of my cycle. My progresterone level is 10. What does this mean? What does progesterone level of 10 indicate? Your progesterone report is perfectly normal. We expect this result on day 23rd of the cycle.So there&#39;s nothing to worry as it&#39;s perfectly alright&quot;&quot;&quot;] res = p_model.transform(spark.createDataFrame(pd.DataFrame({&#39;text&#39;: test_sentences}))) Results : +--++ |sentence |label | +--++ |Hello I am going to be having a baby throughand have just received my medical results before I have my tubes tested.|statement| |I had the tests on day 23 of my cycle. |statement| |My progresterone level is 10. |statement| |What does this mean? |question | |What does progesterone level of 10 indicate? |question | |Your progesterone report is perfectly normal. We expect this result on day 23rd of the cycle. |statement| |So there&#39;s nothing to worry as it&#39;s perfectly alright |statement| +--+ Metrics : precision recall f1-score support question 0.97 0.94 0.96 243 statement 0.98 0.99 0.99 729 accuracy 0.98 972 macro avg 0.98 0.97 0.97 972 weighted avg 0.98 0.98 0.98 972 New Sentence Entity Resolver Fine-Tune Features (Overwriting and Drop Code) .setOverwriteExistingCode() : This parameter provides overwriting codes over the existing codes if in pretrained Sentence Entity Resolver Model. For example, you want to add a new term to a pretrained resolver model, and if the code of term already exists in the pretrained model, when you .setOverwriteExistingCode(True), it removes all the same codes and their descriptions from the model, then you will have just the new term with its code in the fine-tuned model. .setDropCodesList() : This parameter drops list of codes from a pretrained Sentence Entity Resolver Model. For more examples, please check Fine-Tuning Sentence Entity Resolver Notebook Updated ICD10CM Entity Resolver Models We have updated sbiobertresolve_icd10cm_augmented model with ICD10CM 2022 Dataset and sbiobertresolve_icd10cm_augmented_billable_hcc model by dropping invalid codes. Updated NER Profiling Pretrained Pipelines We have updated ner_profiling_clinical and ner_profiling_biobert pretrained pipelines by adding new clinical NER models and NER model outputs to the previous versions. In this way, you can see all the NER labels of tokens. For examples, please check NER Profiling Pretrained Pipeline Notebook. New ChunkSentenceSplitter Annotator We are releasing ChunkSentenceSplitter annotator that splits documents or sentences by chunks provided. Splitted parts can be named with the splitting chunks. By using this annotator, you can do some some tasks like splitting clinical documents according into sections in accordance with CDA (Clinical Document Architecture). Example : ... ner_converter = NerConverter() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&quot;Header&quot;]) chunkSentenceSplitter = ChunkSentenceSplitter() .setInputCols(&quot;ner_chunk&quot;,&quot;document&quot;) .setOutputCol(&quot;paragraphs&quot;) .setGroupBySentences(True) .setDefaultEntity(&quot;Intro&quot;) .setInsertChunk(False) ... text = [&quot;&quot;&quot;INTRODUCTION: Right pleural effusion and suspected malignant mesothelioma. PREOPERATIVE DIAGNOSIS: Right pleural effusion and suspected malignant mesothelioma. POSTOPERATIVE DIAGNOSIS: Right pleural effusion, suspected malignant mesothelioma. PROCEDURE: Right VATS pleurodesis and pleural biopsy.&quot;&quot;&quot;] results = pipeline_model.transform(df) Results : +-++ | result|entity| +-++ |INTRODUCTION: Right pleural effusion and suspected malignant mesoth...|Header| |PREOPERATIVE DIAGNOSIS: Right pleural effusion and suspected malig...|Header| |POSTOPERATIVE DIAGNOSIS: Right pleural effusion, suspected malignan...|Header| | PROCEDURE: Right VATS pleurodesis and pleural biopsy|Header| +-++ By using .setInsertChunk() parameter you can remove the chunk from splitted parts. Example : chunkSentenceSplitter = ChunkSentenceSplitter() .setInputCols(&quot;ner_chunk&quot;,&quot;document&quot;) .setOutputCol(&quot;paragraphs&quot;) .setGroupBySentences(True) .setDefaultEntity(&quot;Intro&quot;) .setInsertChunk(False) paragraphs = chunkSentenceSplitter.transform(results) df = paragraphs.selectExpr(&quot;explode(paragraphs) as result&quot;) .selectExpr(&quot;result.result&quot;, &quot;result.metadata.entity&quot;, &quot;result.metadata.splitter_chunk&quot;) Results : +--+++ | result|entity| splitter_chunk| +--+++ | Right pleural effusion and suspected malignant...|Header| INTRODUCTION:| | Right pleural effusion and suspected malignan...|Header| PREOPERATIVE DIAGNOSIS:| | Right pleural effusion, suspected malignant me...|Header|POSTOPERATIVE DIAGNOSIS:| | Right VATS pleurodesis and pleural biopsy|Header| PROCEDURE:| +--+++ Updated Spark NLP For Healthcare Notebooks NER Profiling Pretrained Pipeline Notebook . Fine-Tuning Sentence Entity Resolver Notebook To see more, please check : Spark NLP Healthcare Workshop Repo Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_3_2",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_3_2"
  },
  "214": {
    "id": "214",
    "title": "Spark NLP for Healthcare Release Notes 3.3.4",
    "content": "3.3.4 We are glad to announce that Spark NLP Healthcare 3.3.4 has been released! Highlights New Clinical NER Models New NER Model Finder Pretrained Pipeline New Relation Extraction Model New LOINC, MeSH, NDC and SNOMED Entity Resolver Models Updated RxNorm Sentence Entity Resolver Model New Shift Days Feature in StructuredDeid Deidentification Module New Multiple Chunks Merge Ability in ChunkMergeApproach New setBlackList Feature in ChunkMergeApproach New setBlackList Feature in NerConverterInternal New setLabelCasing Feature in MedicalNerModel New Update Models Functionality New and Updated Notebooks New Clinical NER Models We have three new clinical NER models. ner_deid_subentity_augmented_i2b2 : This model annotates text to find protected health information(PHI) that may need to be removed. It is trained with 2014 i2b2 dataset (no augmentation applied) and can detect MEDICALRECORD, ORGANIZATION, DOCTOR, USERNAME, PROFESSION, HEALTHPLAN, URL, CITY, DATE, LOCATION-OTHER, STATE, PATIENT, DEVICE, COUNTRY, ZIP, PHONE, HOSPITAL, EMAIL, IDNUM, SREET, BIOID, FAX, AGE entities. Example : ... deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented_i2b2&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = ner_model.transform(spark.createDataFrame([[&quot;A. Record date : 2093-01-13, David Hale, M.D., Name : Hendrickson, Ora MR. # 7194334 Date : 01/13/93 PCP : Oliveira, 25 years old, Record date : 1-11-2000. Cocke County Baptist Hospital. 0295 Keats Street. Phone +1 (302) 786-5227. Patient&#39;s complaints first surfaced when he started working for Brothers Coal-Mine.&quot;]], [&quot;text&quot;])) Results : +--+-+ |chunk |ner_label | +--+-+ |2093-01-13 |DATE | |David Hale |DOCTOR | |Hendrickson, Ora |PATIENT | |7194334 |MEDICALRECORD| |01/13/93 |DATE | |Oliveira |DOCTOR | |25 |AGE | |1-11-2000 |DATE | |Cocke County Baptist Hospital|HOSPITAL | |0295 Keats Street |STREET | |(302) 786-5227 |PHONE | |Brothers Coal-Mine Corp |ORGANIZATION | +--+-+ ner_biomarker : This model is trained to extract biomarkers, therapies, oncological, and other general concepts from text. Following are the entities it can detect: Oncogenes, Tumor_Finding, UnspecificTherapy, Ethnicity, Age, ResponseToTreatment, Biomarker, HormonalTherapy, Staging, Drug, CancerDx, Radiotherapy, CancerSurgery, TargetedTherapy, PerformanceStatus, CancerModifier, Radiological_Test_Result, Biomarker_Measurement, Metastasis, Radiological_Test, Chemotherapy, Test, Dosage, Test_Result, Immunotherapy, Date, Gender, Prognostic_Biomarkers, Duration, Predictive_Biomarkers Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_biomarker&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = ner_model.transform(spark.createDataFrame([[&quot;Here , we report the first case of an intraductal tubulopapillary neoplasm of the pancreas with clear cell morphology . Immunohistochemistry revealed positivity for Pan-CK , CK7 , CK8/18 , MUC1 , MUC6 , carbonic anhydrase IX , CD10 , EMA , β-catenin and e-cadherin .&quot;]], [&quot;text&quot;])) Results : | | ner_chunk | entity | confidence | |:|:-|:-|-:| | 0 | intraductal | CancerModifier | 0.9934 | | 1 | tubulopapillary | CancerModifier | 0.6403 | | 2 | neoplasm of the pancreas | CancerDx | 0.758825 | | 3 | clear cell | CancerModifier | 0.9633 | | 4 | Immunohistochemistry | Test | 0.9534 | | 5 | positivity | Biomarker_Measurement | 0.8795 | | 6 | Pan-CK | Biomarker | 0.9975 | | 7 | CK7 | Biomarker | 0.9975 | | 8 | CK8/18 | Biomarker | 0.9987 | | 9 | MUC1 | Biomarker | 0.9967 | | 10 | MUC6 | Biomarker | 0.9972 | | 11 | carbonic anhydrase IX | Biomarker | 0.937567 | | 12 | CD10 | Biomarker | 0.9974 | | 13 | EMA | Biomarker | 0.9899 | | 14 | β-catenin | Biomarker | 0.8059 | | 15 | e-cadherin | Biomarker | 0.9806 | ner_nihss : NER model that can identify entities according to NIHSS guidelines for clinical stroke assessment to evaluate neurological status in acute stroke patients. Here are the labels it can detect : 11_ExtinctionInattention, 6b_RightLeg, 1c_LOCCommands, 10_Dysarthria, NIHSS, 5_Motor, 8_Sensory, 4_FacialPalsy, 6_Motor, 2_BestGaze, Measurement, 6a_LeftLeg, 5b_RightArm, 5a_LeftArm, 1b_LOCQuestions, 3_Visual, 9_BestLanguage, 7_LimbAtaxia, 1a_LOC . Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_nihss&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = ner_model.transform(spark.createDataFrame([[&quot;Abdomen , soft , nontender . NIH stroke scale on presentation was 23 to 24 for , one for consciousness , two for month and year and two for eye / grip , one to two for gaze , two for face , eight for motor , one for limited ataxia , one to two for sensory , three for best language and two for attention . On the neurologic examination the patient was intermittently&quot;]], [&quot;text&quot;])) Results : | | chunk | entity | |:|:-|:-| | 0 | NIH stroke scale | NIHSS | | 1 | 23 to 24 | Measurement | | 2 | one | Measurement | | 3 | consciousness | 1a_LOC | | 4 | two | Measurement | | 5 | month and year and | 1b_LOCQuestions | | 6 | two | Measurement | | 7 | eye / grip | 1c_LOCCommands | | 8 | one to | Measurement | | 9 | two | Measurement | | 10 | gaze | 2_BestGaze | | 11 | two | Measurement | | 12 | face | 4_FacialPalsy | | 13 | eight | Measurement | | 14 | one | Measurement | | 15 | limited | 7_LimbAtaxia | | 16 | ataxia | 7_LimbAtaxia | | 17 | one to two | Measurement | | 18 | sensory | 8_Sensory | | 19 | three | Measurement | | 20 | best language | 9_BestLanguage | | 21 | two | Measurement | | 22 | attention | 11_ExtinctionInattention | New NER Model Finder Pretrained Pipeline We are releasing new ner_model_finder pretrained pipeline trained with bert embeddings that can be used to find the most appropriate NER model given the entity name. Example : from sparknlp.pretrained import PretrainedPipeline finder_pipeline = PretrainedPipeline(&quot;ner_model_finder&quot;, &quot;en&quot;, &quot;clinical/models&quot;) result = finder_pipeline.fullAnnotate(&quot;psychology&quot;) Results : entity top models all models resolutions psychology [‘ner_medmentions_coarse’, ‘jsl_rd_ner_wip_greedy_clinical’, ‘ner_jsl_enriched’, ‘ner_jsl’, ‘jsl_ner_wip_modifier_clinical’, ‘ner_jsl_greedy’] [‘ner_medmentions_coarse’, ‘jsl_rd_ner_wip_greedy_clinical’, ‘ner_jsl_enriched’, ‘ner_jsl’, ‘jsl_ner_wip_modifier_clinical’, ‘ner_jsl_greedy’]:::[‘jsl_rd_ner_wip_greedy_clinical’, ‘ner_jsl_enriched’, ‘ner_jsl_slim’, ‘ner_jsl’, ‘jsl_ner_wip_modifier_clinical,… psychological condition:::clinical department::: … New Relation Extraction Model We are releasing new redl_nihss_biobert relation extraction model that can relate scale items and their measurements according to NIHSS guidelines. Example : ... re_model = RelationExtractionDLModel() .pretrained(&#39;redl_nihss_biobert&#39;, &#39;en&#39;, &quot;clinical/models&quot;) .setPredictionThreshold(0.5) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) ... sample_text = &quot;There , her initial NIHSS score was 4 , as recorded by the ED physicians . This included 2 for weakness in her left leg and 2 for what they felt was subtle ataxia in her left arm and leg .&quot; result = re_model.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : | chunk1 | entity1 | entity1_begin | entity1_end | entity2 | chunk2 | entity2_begin | entity2_end | relation | |:--|:-|-:|--:|:|:|-:|--:|:--| | initial NIHSS score | NIHSS | 12 | 30 | Measurement | 4 | 36 | 36 | Has_Value | | left leg | 6a_LeftLeg | 111 | 118 | Measurement | 2 | 89 | 89 | Has_Value | | subtle ataxia in her left arm and leg | 7_LimbAtaxia | 149 | 185 | Measurement | 2 | 124 | 124 | Has_Value | | left leg | 6a_LeftLeg | 111 | 118 | Measurement | 4 | 36 | 36 | 0 | | initial NIHSS score | NIHSS | 12 | 30 | Measurement | 2 | 124 | 124 | 0 | | subtle ataxia in her left arm and leg | 7_LimbAtaxia | 149 | 185 | Measurement | 4 | 36 | 36 | 0 | | subtle ataxia in her left arm and leg | 7_LimbAtaxia | 149 | 185 | Measurement | 2 | 89 | 89 | 0 | New LOINC, MeSH, NDC and SNOMED Entity Resolver Models We have four new Sentence Entity Resolver Models. sbiobertresolve_mesh : This model maps clinical entities to Medical Subject Heading (MeSH) codes using sbiobert_base_cased_mli Sentence Bert Embeddings. Example : ... mesh_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_mesh&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;mesh_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) .setCaseSensitive(False) ... sample_text = &quot;&quot;&quot;She was admitted to the hospital with chest pain and found to have bilateral pleural effusion, the right greater than the left. We reviewed the pathology obtained from the pericardectomy in March 2006, which was diagnostic of mesothelioma. At this time, chest tube placement for drainage of the fluid occurred and thoracoscopy with fluid biopsies, which were performed, which revealed malignant mesothelioma.&quot;&quot;&quot; result = resolver_model.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : +--++-+-+-+-+ | ner_chunk| entity| mesh_code| all_codes| resolutions| distances| +--++-+-+-+-+ | chest pain| PROBLEM| D002637|D002637:::D059350:::D019547:::D020069:::D015746:::D000072716:::D005157:::D059265:::D001416:::D048...|Chest Pain:::Chronic Pain:::Neck Pain:::Shoulder Pain:::Abdominal Pain:::Cancer Pain:::Facial Pai...|0.0000:::0.0577:::0.0587:::0.0601:::0.0658:::0.0704:::0.0712:::0.0741:::0.0766:::0.0778:::0.0794:...| |bilateral pleural effusion| PROBLEM| D010996|D010996:::D010490:::D011654:::D016724:::D010995:::D016066:::D011001:::D007819:::D035422:::D004653...|Pleural Effusion:::Pericardial Effusion:::Pulmonary Edema:::Empyema, Pleural:::Pleural Diseases::...|0.0309:::0.1010:::0.1115:::0.1213:::0.1218:::0.1398:::0.1425:::0.1401:::0.1451:::0.1464:::0.1464:...| | the pathology| TEST| D010336|D010336:::D010335:::D001004:::D020969:::C001675:::C536472:::D004194:::D003951:::D013631:::C535329...|Pathology:::Pathologic Processes:::Anus Diseases:::Disease Attributes:::malformins:::Upington dis...|0.0788:::0.0977:::0.1364:::0.1396:::0.1419:::0.1459:::0.1418:::0.1393:::0.1514:::0.1541:::0.1491:...| | the pericardectomy|TREATMENT| D010492|D010492:::D011670:::D018700:::D020884:::D011672:::D005927:::D064727:::D002431:::C000678968:::D011...|Pericardiectomy:::Pulpectomy:::Pleurodesis:::Colpotomy:::Pulpotomy:::Glossectomy:::Posterior Caps...|0.1098:::0.1448:::0.1801:::0.1852:::0.1871:::0.1923:::0.1901:::0.2023:::0.2075:::0.2010:::0.1996:...| | mesothelioma| PROBLEM|D000086002|D000086002:::C535700:::D009208:::D032902:::D018301:::D018199:::C562740:::C000686536:::D018276:::D...|Mesothelioma, Malignant:::Malignant mesenchymal tumor:::Myoepithelioma:::Ganoderma:::Neoplasms, M...|0.0813:::0.1515:::0.1599:::0.1810:::0.1864:::0.1881:::0.1907:::0.1938:::0.1924:::0.1876:::0.2040:...| | chest tube placement|TREATMENT| D015505|D015505:::D019616:::D013896:::D012124:::D013906:::D013510:::D020708:::D035423:::D013903:::D000066...|Chest Tubes:::Thoracic Surgical Procedures:::Thoracic Diseases:::Respiratory Care Units:::Thoraco...|0.0557:::0.1473:::0.1598:::0.1604:::0.1725:::0.1651:::0.1795:::0.1760:::0.1804:::0.1846:::0.1883:...| | drainage of the fluid|TREATMENT| D004322|D004322:::D018495:::C045413:::D021061:::D045268:::D018508:::D005441:::D015633:::D014906:::D001834...|Drainage:::Fluid Shifts:::Bonain&#39;s liquid:::Liquid Ventilation:::Flowmeters:::Water Purification:...|0.1141:::0.1403:::0.1582:::0.1549:::0.1586:::0.1626:::0.1599:::0.1655:::0.1667:::0.1656:::0.1741:...| | thoracoscopy|TREATMENT| D013906|D013906:::D020708:::D035423:::D013905:::D035441:::D013897:::D001468:::D000069258:::D013909:::D013...|Thoracoscopy:::Thoracoscopes:::Thoracic Cavity:::Thoracoplasty:::Thoracic Wall:::Thoracic Duct:::...|0.0000:::0.0359:::0.0744:::0.1007:::0.1070:::0.1143:::0.1186:::0.1257:::0.1228:::0.1356:::0.1354:...| | fluid biopsies| TEST|D000073890|D000073890:::D010533:::D020420:::D011677:::D017817:::D001706:::D005441:::D005751:::D013582:::D000...|Liquid Biopsy:::Peritoneal Lavage:::Cyst Fluid:::Punctures:::Nasal Lavage Fluid:::Biopsy:::Fluids...|0.1408:::0.1612:::0.1763:::0.1744:::0.1744:::0.1810:::0.1744:::0.1828:::0.1896:::0.1909:::0.1950:...| | malignant mesothelioma| PROBLEM|D000086002|D000086002:::C535700:::C562740:::D009236:::D007890:::D012515:::D009208:::C009823:::C000683999:::C...|Mesothelioma, Malignant:::Malignant mesenchymal tumor:::Hemangiopericytoma, Malignant:::Myxosarco...|0.0737:::0.1106:::0.1658:::0.1627:::0.1660:::0.1639:::0.1728:::0.1676:::0.1791:::0.1843:::0.1849:...| +-+--++-+-+-+-+ sbiobertresolve_ndc : This model maps clinical entities and concepts (like drugs/ingredients) to National Drug Codes using sbiobert_base_cased_mli Sentence Bert Embeddings. Also, if a drug has more than one NDC code, it returns all available codes in the all_k_aux_label column separated by | symbol. Example : ... ndc_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_ndc&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;ndc_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) .setCaseSensitive(False) ... sample_text = &quot;&quot;&quot;The patient was transferred secondary to inability and continue of her diabetes, the sacral decubitus, left foot pressure wound, and associated complications of diabetes. She is given aspirin 81 mg, folic acid 1 g daily, insulin glargine 100 UNT/ML injection and metformin 500 mg p.o. p.r.n.&quot;&quot;&quot; result = resolver_model.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : +-++--++--+--+--+ | ner_chunk|entity| ndc_code| description| all_codes| all_resolutions| other ndc codes| +-++--++--+--+--+ | aspirin 81 mg| DRUG|73089008114| aspirin 81 mg/81mg, 81 mg in 1 carton , capsule|[73089008114, 71872708704, 71872715401, 68210101500, 69536028110, 63548086706, 71679001000, 68196090051, 00113400500, 69536018112, 73089008112, 63981056362, 63739043402, 63548086705, 00113046708, 7...|[aspirin 81 mg/81mg, 81 mg in 1 carton , capsule, aspirin 81 mg 81 mg/1, 4 blister pack in 1 bag , tablet, aspirin 81 mg/1, 1 blister pack in 1 bag , tablet, coated, aspirin 81 mg/1, 1 bag in 1 dru...| [-, -, -, -, -, -, -, -, -, -, -, 63940060962, -, -, -, -, -, -, -, -, 70000042002|00363021879|41250027408|36800046708|59779027408|49035027408|71476010131|81522046708|30142046708, -, -, -, -]| | folic acid 1 g| DRUG|43744015101| folic acid 1 g/g, 1 g in 1 package , powder|[43744015101, 63238340000, 66326050555, 51552041802, 51552041805, 63238340001, 81919000204, 51552041804, 66326050556, 51552106301, 51927003300, 71092997701, 51927296300, 51552146602, 61281900002, 6...|[folic acid 1 g/g, 1 g in 1 package , powder, folic acid 1 kg/kg, 1 kg in 1 bottle , powder, folic acid 1 kg/kg, 1 kg in 1 drum , powder, folic acid 1 g/g, 5 g in 1 container , powder, folic acid 1...| [-, -, -, -, -, -, -, -, -, -, -, 51552139201, -, -, -, 81919000203, -, 81919000201, -, -, -, -, -, -, -]| |insulin glargine 100 UNT/ML injection| DRUG|00088502101|insulin glargine 100 [iu]/ml, 1 vial, glass in 1 package , injection, solution|[00088502101, 00088222033, 49502019580, 00002771563, 00169320111, 00088250033, 70518139000, 00169266211, 50090127600, 50090407400, 00002771559, 00002772899, 70518225200, 70518138800, 00024592410, 0...|[insulin glargine 100 [iu]/ml, 1 vial, glass in 1 package , injection, solution, insulin glargine 100 [iu]/ml, 1 vial, glass in 1 carton , injection, solution, insulin glargine 100 [iu]/ml, 1 vial ...|[-, -, -, 00088221900, -, -, 50090139800|00088502005, -, 70518146200|00169368712, 00169368512|73070020011, 00088221905|49502019675|50090406800, -, 73070010011|00169750111|50090495500, 66733077301|0...| | metformin 500 mg| DRUG|70010006315| metformin hydrochloride 500 mg/500mg, 500 mg in 1 drum , tablet|[70010006315, 62207041613, 71052050750, 62207049147, 71052091050, 25000010197, 25000013498, 25000010198, 71052063005, 51662139201, 70010049118, 70882012456, 71052011005, 71052065905, 71052050850, 1...|[metformin hydrochloride 500 mg/500mg, 500 mg in 1 drum , tablet, metformin hcl 500 mg/kg, 50 kg in 1 drum , powder, 5-fluorouracil 500 g/500g, 500 g in 1 container , powder, metformin er 500 mg 50...| [-, -, -, 70010049105, -, -, -, -, -, -, -, -, -, -, -, 71800000801|42571036007, -, -, -, -, -, -, -, -, -]| +-++--++--+--+--+ sbiobertresolve_loinc_augmented : This model maps extracted clinical NER entities to LOINC codes using sbiobert_base_cased_mli Sentence Bert Embeddings. It is trained on the augmented version of the dataset which is used in previous LOINC resolver models. Example : ... loinc_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_loinc_augmented&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;loinc_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) .setCaseSensitive(False) ... sample_text=&quot;&quot;&quot;The patient is a 22-year-old female with a history of obesity. She has a Body mass index (BMI) of 33.5 kg/m2, aspartate aminotransferase 64, and alanine aminotransferase 126. Her hgba1c is 8.2%.&quot;&quot;&quot; result = resolver_model.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : +--+--+++-+--+--+ | chunk|begin|end|entity|Loinc_Code| all_codes| resolutions| +--+--+++-+--+--+ | Body mass index| 74| 88| Test| LP35925-4|LP35925-4:::BDYCRC:::LP172732-2:::39156-5:::LP7...|body mass index:::body circumference:::body mus...| |aspartate aminotransferase| 111|136| Test| LP15426-7|LP15426-7:::14409-7:::LP307348-5:::LP15333-5:::...|aspartate aminotransferase::: aspartate transam...| | alanine aminotransferase| 146|169| Test| LP15333-5|LP15333-5:::LP307326-1:::16324-6:::LP307348-5::...|alanine aminotransferase:::alanine aminotransfe...| | hgba1c| 180|185| Test| 17855-8|17855-8:::4547-6:::55139-0:::72518-4:::45190-6:...| hba1c::: hgb a1::: hb1::: hcds1::: hhc1::: htr...| +--+--+++-+--+--+ sbiobertresolve_clinical_snomed_procedures_measurements : This model maps medical entities to SNOMED codes using sent_biobert_clinical_base_cased Sentence Bert Embeddings. The corpus of this model includes Procedures and Measurement domains. Example : ... snomed_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_clinical_snomed_procedures_measurements&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) ... light_model = LightPipeline(resolver_model) result = light_model.fullAnnotate([&#39;coronary calcium score&#39;, &#39;heart surgery&#39;, &#39;ct scan&#39;, &#39;bp value&#39;]) Results : | | chunk | code | code_description | all_k_codes | all_k_resolutions | |:|:--|-:|:|:--|:-| | 0 | coronary calcium score | 450360000 | Coronary artery calcium score | [&#39;450360000&#39;, &#39;450734004&#39;, &#39;1086491000000104&#39;, &#39;1086481000000101&#39;, &#39;762241007&#39;] | [&#39;Coronary artery calcium score&#39;, &#39;Coronary artery calcium score&#39;, &#39;Dundee Coronary Risk Disk score&#39;, &#39;Dundee Coronary Risk rank&#39;, &#39;Dundee Coronary Risk Disk&#39;] | | 1 | heart surgery | 2598006 | Open heart surgery | [&#39;2598006&#39;, &#39;64915003&#39;, &#39;119766003&#39;, &#39;34068001&#39;, &#39;233004008&#39;] | [&#39;Open heart surgery&#39;, &#39;Operation on heart&#39;, &#39;Heart reconstruction&#39;, &#39;Heart valve replacement&#39;, &#39;Coronary sinus operation&#39;] | | 2 | ct scan | 303653007 | CT of head | [&#39;303653007&#39;, &#39;431864000&#39;, &#39;363023007&#39;, &#39;418272005&#39;, &#39;241577003&#39;] | [&#39;CT of head&#39;, &#39;CT guided injection&#39;, &#39;CT of site&#39;, &#39;CT angiography&#39;, &#39;CT of spine&#39;] | | 3 | bp value | 75367002 | Blood pressure | [&#39;75367002&#39;, &#39;6797001&#39;, &#39;723232008&#39;, &#39;46973005&#39;, &#39;427732000&#39;] | [&#39;Blood pressure&#39;, &#39;Mean blood pressure&#39;, &#39;Average blood pressure&#39;, &#39;Blood pressure taking&#39;, &#39;Speed of blood pressure response&#39;] | Updated RxNorm Sentence Entity Resolver Model We have updated sbiobertresolve_rxnorm_augmented model training on an augmented version of the dataset used in previous versions of the model. New Shift Days Feature in StructuredDeid Deidentification Module Now we can shift n days in the structured deidentification when the column is a Date. Example : df = spark.createDataFrame([ [&quot;Juan García&quot;, &quot;13/02/1977&quot;, &quot;711 Nulla St.&quot;, &quot;140&quot;, &quot;673 431234&quot;], [&quot;Will Smith&quot;, &quot;23/02/1977&quot;, &quot;1 Green Avenue.&quot;, &quot;140&quot;, &quot;+23 (673) 431234&quot;], [&quot;Pedro Ximénez&quot;, &quot;11/04/1900&quot;, &quot;Calle del Libertador, 7&quot;, &quot;100&quot;, &quot;912 345623&quot;] ]).toDF(&quot;NAME&quot;, &quot;DOB&quot;, &quot;ADDRESS&quot;, &quot;SBP&quot;, &quot;TEL&quot;) obfuscator = StructuredDeidentification(spark=spark, columns={&quot;NAME&quot;: &quot;ID&quot;, &quot;DOB&quot;: &quot;DATE&quot;}, columnsSeed={&quot;NAME&quot;: 23, &quot;DOB&quot;: 23}, obfuscateRefSource=&quot;faker&quot;, days=5 ) result = obfuscator.obfuscateColumns(self.df) result.show(truncate=False) Results : +-++--++-+ |NAME |DOB |ADDRESS |SBP|TEL | +-++--++-+ |[T1825511]|[18/02/1977]|711 Nulla St. |140|673 431234 | |[G6835267]|[28/02/1977]|1 Green Avenue. |140|+23 (673) 431234| |[S2371443]|[16/04/1900]|Calle del Libertador, 7|100|912 345623 | +-++--++-+ New Multiple Chunks Merge Ability in ChunkMergeApproach Updated ChunkMergeApproach to admit N input cols (.setInputCols(&quot;ner_chunk&quot;,&quot;ner_chunk_1&quot;,&quot;ner_chunk_2&quot;)). The input columns must be chunk columns. Example : ... deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_large&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setWhiteList([&#39;DATE&#39;, &#39;AGE&#39;, &#39;NAME&#39;, &#39;PROFESSION&#39;, &#39;ID&#39;]) medical_ner = MedicalNerModel.pretrained(&quot;ner_events_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner2&quot;) ner_converter_2 = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner2&quot;]) .setOutputCol(&quot;ner_chunk_2&quot;) ssn_parser = ContextualParserApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;entity_ssn&quot;) .setJsonPath(&quot;../../src/test/resources/ssn.json&quot;) .setCaseSensitive(False) .setContextMatch(False) chunk_merge = ChunkMergeApproach() .setInputCols(&quot;entity_ssn&quot;,&quot;ner_chunk&quot;,&quot;ner_chunk_2&quot;) .setOutputCol(&quot;deid_merged_chunk&quot;) .setChunkPrecedence(&quot;field&quot;) ... New setBlackList Feature in ChunkMergeApproach Now we can filter out the entities in the ChunkMergeApproach using a black list .setBlackList([&quot;NAME&quot;,&quot;ID&quot;]). The entities specified in the blackList will be excluded from the final entity list. Example : chunk_merge = ChunkMergeApproach() .setInputCols(&quot;entity_ssn&quot;,&quot;ner_chunk&quot;) .setOutputCol(&quot;deid_merged_chunk&quot;) .setBlackList([&quot;NAME&quot;,&quot;ID&quot;]) New setBlackList Feature in NerConverterInternal Now we can filter out the entities in the NerConverterInternal using a black list .setBlackList([&quot;Drug&quot;,&quot;Treatment&quot;]). The entities specified in the blackList will be excluded from the final entity list. Example : ner = MedicalNerModel.pretrained(&quot;ner_jsl_slim&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) converter = NerConverterInternal() .setInputCols(&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;) .setOutputCol(&quot;entities&quot;) .setBlackList([&quot;Drug&quot;,&quot;Treatment&quot;]) New setLabelCasing Feature in MedicalNerModel Now we can decide if we want to return the tags in upper or lower case with setLabelCasing(). That method convert the I-tags and B-tags in lower or upper case during the inference. The values will be ‘lower’ for lower case and ‘upper’ for upper case. Example : ... ner_tagger = MedicalNerModel() .pretrained(&quot;ner_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) .setLabelCasing(&quot;lower&quot;) ... results = LightPipeline(pipelineModel).annotate(&quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus &quot;) results[&quot;ner_tags&quot;] Results : [&#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-problem&#39;, &#39;I-problem&#39;, &#39;I-problem&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;O&#39;, &#39;B-problem&#39;, &#39;I-problem&#39;, &#39;I-problem&#39;, &#39;I-problem&#39;, &#39;I-problem&#39;] New Update Models Functionality We developed a new utility function called UpdateModels that allows you to refresh your cache_pretrained folder without running any annotator or manually checking. It has two methods; UpdateModels.updateCacheModels() : This method lets you update all the models existing in the cache_pretrained folder. It downloads the latest version of all the models existing in the cache_pretrained. Example : # Models in /cache_pretrained ls ~/cache_pretrained &gt;&gt; ner_clinical_large_en_3.0.0_2.3_1617206114650/ # Update models in /cache_pretrained from sparknlp_jsl.updateModels import UpdateModels UpdateModels.updateCacheModels() Results : # Updated models in /cache_pretrained ls ~/cache_pretrained &gt;&gt; ner_clinical_large_en_3.0.0_2.3_1617206114650/ ner_clinical_large_en_3.0.0_3.0_1617206114650/ UpdateModels.updateModels(&quot;11/24/2021&quot;) : This method lets you download all the new models uploaded to the Models Hub starting from a cut-off date (i.e. the last sync update). Example : # Models in /cache_pretrained ls ~/cache_pretrained &gt;&gt; ner_clinical_large_en_3.0.0_2.3_1617206114650/ ner_clinical_large_en_3.0.0_3.0_1617206114650/ # Update models in /cache_pretrained according to date from sparknlp_jsl.updateModels import UpdateModels UpdateModels.updateModels(&quot;11/24/2021&quot;) Results : # Updated models in /cache_pretrained ls ~/cache_pretrained &gt;&gt;ner_clinical_large_en_3.0.0_2.3_1617206114650/ ner_clinical_large_en_3.0.0_3.0_1617206114650/ ner_model_finder_en_3.3.2_2.4_1637761259895/ sbertresolve_ner_model_finder_en_3.3.2_2.4_1637764208798/ New and Updated Notebooks We have a new Connect to Annotation Lab via API Notebook you can find how to; upload pre-annotations to ALAB import a project form ALAB and convert to CoNLL file upload tasks without pre-annotations We have updated Clinical Relation Extraction Notebook by adding a Relation Extraction Model-NER Model-Relation Pairs table that can be used to get the most optimal results when using these models. To see more, please check : Spark NLP Healthcare Workshop Repo Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_3_4",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_3_4"
  },
  "215": {
    "id": "215",
    "title": "Spark NLP for Healthcare Release Notes 3.4.0",
    "content": "3.4.0 We are glad to announce that Spark NLP Healthcare 3.4.0 has been released! This is a massive release: new features, new models, academic papers, and more! Highlights New German Deidentification NER Models New German Deidentification Pretrained Pipeline New Clinical NER Models New AnnotationMerger Annotator New MedicalBertForTokenClassifier Annotator New BERT-Based Clinical NER Models New Clinical Relation Extraction Models New LOINC, SNOMED, UMLS and Clinical Abbreviation Entity Resolver Models New ICD10 to ICD9 Code Mapping Pretrained Pipeline New Clinical Sentence Embedding Models Printing Validation and Test Logs for MedicalNerApproach and AssertionDLApproach Filter Only the Regex Entities Feature in Deidentification Annotator Add .setMaskingPolicy Parameter in Deidentification Annotator Add .cache_folder Parameter in UpdateModels.updateCacheModels() S3 Access Credentials No Longer Shipped Along Licenses Enhanced Security for the Library and log4shell Update New Peer-Reviewed Conference Paper on Clinical Relation Extraction New Peer-Reviewed Conference Paper on Adverse Drug Events Extraction New and Updated Notebooks New German Deidentification NER Models We trained two new NER models to find PHI data (protected health information) that may need to be deidentified in German. ner_deid_generic and ner_deid_subentity models are trained with in-house annotations. ner_deid_generic : Detects 7 PHI entities in German (DATE, NAME, LOCATION, PROFESSION, CONTACT, AGE, ID). ner_deid_subentity : Detects 12 PHI sub-entities in German (PATIENT, HOSPITAL, DATE, ORGANIZATION, CITY, STREET, USERNAME, PROFESSION, PHONE, COUNTRY, DOCTOR, AGE). Example : ... embeddings = WordEmbeddingsModel.pretrained(&quot;w2v_cc_300d&quot;,&quot;de&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_generic&quot;, &quot;de&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) deid_sub_entity_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity&quot;, &quot;de&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_sub_entity&quot;) ... text = &quot;&quot;&quot;Michael Berger wird am Morgen des 12 Dezember 2018 ins St. Elisabeth-Krankenhaus in Bad Kissingen eingeliefert. Herr Berger ist 76 Jahre alt und hat zu viel Wasser in den Beinen.&quot;&quot;&quot; result = model.transform(spark.createDataFrame([[text]], [&quot;text&quot;])) Results : +-+-+-+ |chunk |ner_deid_generic_chunk|ner_deid_subentity_chunk | +-+-+-+ |Michael Berger |NAME |PATIENT | |12 Dezember 2018 |DATE |DATE | |St. Elisabeth-Krankenhaus|LOCATION |HOSPITAL | |Bad Kissingen |LOCATION |CITY | |Berger |NAME |PATIENT | |76 |AGE |AGE | +-+-+-+ New German Deidentification Pretrained Pipeline We developed a clinical deidentification pretrained pipeline that can be used to deidentify PHI information from German medical texts. The PHI information will be masked and obfuscated in the resulting text. The pipeline can mask and obfuscate PATIENT, HOSPITAL, DATE, ORGANIZATION, CITY, STREET, USERNAME, PROFESSION, PHONE, COUNTRY, DOCTOR, AGE, CONTACT, ID, PHONE, ZIP, ACCOUNT, SSN, DLN, PLATE entities. Example : ... from sparknlp.pretrained import PretrainedPipeline deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;, &quot;de&quot;, &quot;clinical/models&quot;) text = &quot;&quot;&quot;Zusammenfassung : Michael Berger wird am Morgen des 12 Dezember 2018 ins St.Elisabeth Krankenhaus in Bad Kissingen eingeliefert. Herr Michael Berger ist 76 Jahre alt und hat zu viel Wasser in den Beinen. Persönliche Daten : ID-Nummer: T0110053F Platte A-BC124 Kontonummer: DE89370400440532013000 SSN : 13110587M565 Lizenznummer: B072RRE2I55 Adresse : St.Johann-Straße 13 19300&quot;&quot;&quot; result = deid_pipe.annotate(text) print(&quot; n&quot;.join(result[&#39;masked&#39;])) print(&quot; n&quot;.join(result[&#39;obfuscated&#39;])) print(&quot; n&quot;.join(result[&#39;masked_with_chars&#39;])) print(&quot; n&quot;.join(result[&#39;masked_fixed_length_chars&#39;])) Results : Zusammenfassung : &lt;PATIENT&gt; wird am Morgen des &lt;DATE&gt; ins &lt;HOSPITAL&gt; eingeliefert. Herr &lt;PATIENT&gt; ist &lt;AGE&gt; Jahre alt und hat zu viel Wasser in den Beinen. Persönliche Daten : ID-Nummer: &lt;ID&gt; Platte &lt;PLATE&gt; Kontonummer: &lt;ACCOUNT&gt; SSN : &lt;SSN&gt; Lizenznummer: &lt;DLN&gt; Adresse : &lt;STREET&gt; &lt;ZIP&gt; Zusammenfassung : Herrmann Kallert wird am Morgen des 11-26-1977 ins International Neuroscience eingeliefert. Herr Herrmann Kallert ist 79 Jahre alt und hat zu viel Wasser in den Beinen. Persönliche Daten : ID-Nummer: 136704D357 Platte QA348G Kontonummer: 192837465738 SSN : 1310011981M454 Lizenznummer: XX123456 Adresse : Klingelhöferring 31206 Zusammenfassung : **** wird am Morgen des **** ins **** eingeliefert. Herr **** ist **** Jahre alt und hat zu viel Wasser in den Beinen. Persönliche Daten : ID-Nummer: **** Platte **** Kontonummer: **** SSN : **** Lizenznummer: **** Adresse : **** **** Zusammenfassung : [************] wird am Morgen des [**************] ins [**********************] eingeliefert. Herr [************] ist ** Jahre alt und hat zu viel Wasser in den Beinen. Persönliche Daten : ID-Nummer: [*******] Platte [*****] Kontonummer: [********************] SSN : [**********] Lizenznummer: [*********] Adresse : [*****************] [***] New Clinical NER Models We have two new clinical NER models. ner_abbreviation_clinical : This model is trained to extract clinical abbreviations and acronyms in texts and labels these entities as ABBR. Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_abbreviation_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = ner_model.transform(spark.createDataFrame([[&quot;Gravid with estimated fetal weight of 6-6/12 pounds. LOWER EXTREMITIES: No edema. LABORATORY DATA: Laboratory tests include a CBC which is normal. Blood Type: AB positive. Rubella: Immune. VDRL: Nonreactive. Hepatitis C surface antigen: Negative. HIV: Negative. One-Hour Glucose: 117. Group B strep has not been done as yet.&quot;]], [&quot;text&quot;])) Results : +--++ |chunk|ner_label| +--++ |CBC |ABBR | |AB |ABBR | |VDRL |ABBR | |HIV |ABBR | +--++ ner_drugprot_clinical : This model detects chemical compounds/drugs and genes/proteins in medical text and research articles. Here are the labels it can detect : GENE, CHEMICAL, GENE_AND_CHEMICAL. Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_drugprot_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... results = ner_model.transform(spark.createDataFrame([[&quot;Anabolic effects of clenbuterol on skeletal muscle are mediated by beta 2-adrenoceptor activation&quot;]], [&quot;text&quot;])) Results : | | chunk | ner_label | |:|:|:| | 0 | clenbuterol | CHEMICAL | | 1 | beta 2-adrenoceptor | GENE | New AnnotationMerger Annotator A new annotator: AnnotationMerger. Besides NERs, now we will be able to merge results of Relation Extraction models and Assertion models as well. Therefore, it can merge results of Relation Extraction models, NER models, and Assertion Status models. Example-1 : ... annotation_merger = AnnotationMerger() .setInputCols(&quot;ade_relations&quot;, &quot;pos_relations&quot;, &quot;events_relations&quot;) .setInputType(&quot;category&quot;) .setOutputCol(&quot;all_relations&quot;) ... results = ann_merger_model.transform(spark.createDataFrame([[&quot;The patient was prescribed 1 unit of naproxen for 5 days after meals for chronic low back pain. The patient was also given 1 unit of oxaprozin daily for rheumatoid arthritis presented with tense bullae and cutaneous fragility on the face and the back of the hands.&quot;]], [&quot;text&quot;])) Results-1 : | | all_relations | all_relations_entity1 | all_relations_chunk1 | all_relations_entity2 | all_relations_chunk2 | |:|:-|:|:--|:|:-| | 0 | 1 | DRUG | oxaprozin | ADE | tense bullae | | 1 | 1 | DRUG | oxaprozin | ADE | cutaneous fragility on the face and the back of the hands | | 2 | DOSAGE-DRUG | DOSAGE | 1 unit | DRUG | naproxen | | 3 | DRUG-DURATION | DRUG | naproxen | DURATION | for 5 days | | 4 | DOSAGE-DRUG | DOSAGE | 1 unit | DRUG | oxaprozin | | 5 | DRUG-FREQUENCY | DRUG | oxaprozin | FREQUENCY | daily | | 6 | OVERLAP | TREATMENT | naproxen | DURATION | 5 days | | 7 | OVERLAP | TREATMENT | oxaprozin | FREQUENCY | daily | | 8 | BEFORE | TREATMENT | oxaprozin | PROBLEM | rheumatoid arthritis | | 9 | AFTER | TREATMENT | oxaprozin | OCCURRENCE | presented | | 10 | OVERLAP | FREQUENCY | daily | PROBLEM | rheumatoid arthritis | | 11 | OVERLAP | FREQUENCY | daily | PROBLEM | tense bullae | | 12 | OVERLAP | FREQUENCY | daily | PROBLEM | cutaneous fragility on the face | | 13 | BEFORE | PROBLEM | rheumatoid arthritis | OCCURRENCE | presented | | 14 | OVERLAP | PROBLEM | rheumatoid arthritis | PROBLEM | tense bullae | | 15 | OVERLAP | PROBLEM | rheumatoid arthritis | PROBLEM | cutaneous fragility on the face | | 16 | BEFORE | OCCURRENCE | presented | PROBLEM | tense bullae | | 17 | BEFORE | OCCURRENCE | presented | PROBLEM | cutaneous fragility on the face | | 18 | OVERLAP | PROBLEM | tense bullae | PROBLEM | cutaneous fragility on the face | Example-2 : ... ner_annotation_merger = AnnotationMerger() .setInputCols(&quot;ner_chunk&quot;, &quot;radiology_ner_chunk&quot;, &quot;jsl_ner_chunk&quot;) .setInputType(&quot;chunk&quot;) .setOutputCol(&quot;all_ners&quot;) assertion_annotation_merger = AnnotationMerger() .setInputCols(&quot;clinical_assertion&quot;, &quot;radiology_assertion&quot;, &quot;jsl_assertion&quot;) .setInputType(&quot;assertion&quot;) .setOutputCol(&quot;all_assertions&quot;) ... results = ann_merger_model.transform(spark.createDataFrame([[&quot;The patient was prescribed 1 unit of naproxen for 5 days after meals for chronic low back pain. The patient was also given 1 unit of oxaprozin daily for rheumatoid arthritis presented with tense bullae and cutaneous fragility on the face and the back of the hands.&quot;]], [&quot;text&quot;])) Results-2 : | | ners | all_assertions | |:|:--|:--| | 0 | naproxen | present | | 1 | chronic low back pain | present | | 2 | oxaprozin | present | | 3 | rheumatoid arthritis | present | | 4 | tense bullae | present | | 5 | cutaneous fragility on the face | present | | 6 | low back | Confirmed | | 7 | pain | Confirmed | | 8 | rheumatoid arthritis | Confirmed | | 9 | tense bullae | Confirmed | | 10 | cutaneous | Confirmed | | 11 | fragility | Confirmed | | 12 | face | Confirmed | | 13 | back | Confirmed | | 14 | hands | Confirmed | | 15 | 1 unit | Present | | 16 | naproxen | Past | | 17 | for 5 days | Past | | 18 | chronic | Someoneelse | | 19 | low | Past | | 20 | back pain | Present | | 21 | 1 unit | Past | | 22 | oxaprozin | Past | | 23 | daily | Past | | 24 | rheumatoid arthritis | Present | | 25 | tense | Present | | 26 | bullae | Present | | 27 | cutaneous fragility | Present | | 28 | face | Someoneelse | | 29 | back of the hands | Present | New MedicalBertForTokenClassifier Annotator We developed a new annotator called MedicalBertForTokenClassifier that can load BERT-Based clinical token classifier models head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. New BERT-Based Clinical NER Models Here are the MedicalBertForTokenClassifier Models we have in the library at the moment: bert_token_classifier_ner_ade bert_token_classifier_ner_anatomy bert_token_classifier_ner_bionlp bert_token_classifier_ner_cellular bert_token_classifier_ner_chemprot bert_token_classifier_ner_chemicals bert_token_classifier_ner_jsl_slim bert_token_classifier_ner_jsl bert_token_classifier_ner_deid bert_token_classifier_ner_drugs bert_token_classifier_ner_clinical bert_token_classifier_ner_bacteria In addition, we are releasing a new BERT-Based clinical NER model named bert_token_classifier_drug_development_trials. It is a MedicalBertForTokenClassification NER model to identify concepts related to drug development including Trial Groups , End Points , Hazard Ratio, and other entities in free text. It can detect the following entities: Patient_Count, Duration, End_Point, Value, Trial_Group, Hazard_Ratio, Total_Patients Example : ... tokenClassifier= MedicalBertForTokenClassifier.pretrained(&quot;bert_token_classifier_drug_development_trials&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ... results = ner_model.transform(spark.createDataFrame([[&quot;In June 2003, the median overall survival with and without topotecan were 4.0 and 3.6 months, respectively. The best complete response ( CR ) , partial response ( PR ) , stable disease and progressive disease were observed in 23, 63, 55 and 33 patients, respectively, with topotecan, and 11, 61, 66 and 32 patients, respectively, without topotecan.&quot;]], [&quot;text&quot;])) Results : | | chunk | entity | |:|:|:--| | 0 | median | Duration | | 1 | overall survival | End_Point | | 2 | with | Trial_Group | | 3 | without topotecan | Trial_Group | | 4 | 4.0 | Value | | 5 | 3.6 months | Value | | 6 | 23 | Patient_Count | | 7 | 63 | Patient_Count | | 8 | 55 | Patient_Count | | 9 | 33 patients | Patient_Count | | 10 | topotecan | Trial_Group | | 11 | 11 | Patient_Count | | 12 | 61 | Patient_Count | | 13 | 66 | Patient_Count | | 14 | 32 patients | Patient_Count | | 15 | without topotecan | Trial_Group | New Clinical Relation Extraction Models We have two new clinical Relation Extraction models for detecting interactions between drugs and proteins. These models work hand-in-hand with the new ner_drugprot_clinical NER model and detect following relations between entities: INHIBITOR, DIRECT-REGULATOR, SUBSTRATE, ACTIVATOR, INDIRECT-UPREGULATOR, INDIRECT-DOWNREGULATOR, ANTAGONIST, PRODUCT-OF, PART-OF, AGONIST. redl_drugprot_biobert : This model was trained using BERT and performs with higher accuracy. re_drugprot_clinical : This model was trained using RelationExtractionApproach(). Example : ... drugprot_ner_tagger = MedicalNerModel.pretrained(&quot;ner_drugprot_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;sentences&quot;, &quot;tokens&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner_tags&quot;) ... drugprot_re_biobert = RelationExtractionDLModel() .pretrained(&#39;redl_drugprot_biobert&#39;, &quot;en&quot;, &quot;clinical/models&quot;) .setPredictionThreshold(0.9) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) drugprot_re_clinical = RelationExtractionModel() .pretrained(&quot;re_drugprot_clinical&quot;, &quot;en&quot;, &#39;clinical/models&#39;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setMaxSyntacticDistance(4) .setPredictionThreshold(0.9) .setRelationPairs([&#39;CHEMICAL-GENE&#39;]) ... sample_text = &quot;Lipid specific activation of the murine P4-ATPase Atp8a1 (ATPase II). The asymmetric transbilayer distribution of phosphatidylserine (PS) in the mammalian plasma membrane and secretory vesicles is maintained, in part, by an ATP-dependent transporter. This aminophospholipid &quot;flippase&quot; selectively transports PS to the cytosolic leaflet of the bilayer and is sensitive to vanadate, Ca(2+), and modification by sulfhydryl reagents. Although the flippase has not been positively identified, a subfamily of P-type ATPases has been proposed to function as transporters of amphipaths, including PS and other phospholipids. A candidate PS flippase ATP8A1 (ATPase II), originally isolated from bovine secretory vesicles, is a member of this subfamily based on sequence homology to the founding member of the subfamily, the yeast protein Drs2, which has been linked to ribosomal assembly, the formation of Golgi-coated vesicles, and the maintenance of PS asymmetry.&quot; result = re_model.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : ++--+-+--+--+-+-+--+--+-+ | relation| entity1|entity1_begin|entity1_end| chunk1|entity2|entity2_begin|entity2_end| chunk2|confidence| ++--+-+--+--+-+-+--+--+-+ |SUBSTRATE|CHEMICAL| 308| 310| PS| GENE| 275| 283| flippase| 0.998399| |ACTIVATOR|CHEMICAL| 1563| 1578| sn-1,2-glycerol| GENE| 1479| 1509|plasma membrane P...| 0.999304| |ACTIVATOR|CHEMICAL| 1563| 1578| sn-1,2-glycerol| GENE| 1511| 1517| Atp8a1| 0.979057| ++--+-+--+--+-+-+--+--+-+ New LOINC, SNOMED, UMLS and Clinical Abbreviation Entity Resolver Models We have five new Sentence Entity Resolver models. sbiobertresolve_clinical_abbreviation_acronym : This model maps clinical abbreviations and acronyms to their meanings using sbiobert_base_cased_mli Sentence Bert Embeddings. It is a part of ongoing research we have been running in-house, and trained with a limited dataset. We’ll be updating &amp; enriching the model in the upcoming releases. Example : ... abbr_resolver = SentenceEntityResolverModel.pretraind(&quot;sbiobertresolve_clinical_abbreviation_acronym&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;merged_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;abbr_meaning&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) ... sample_text = &quot;HISTORY OF PRESENT ILLNESS: The patient three weeks ago was seen at another clinic for upper respiratory infection-type symptoms. She was diagnosed with a viral infection and had used OTC medications including Tylenol, Sudafed, and Nyquil.&quot; results = abb_model.transform(spark.createDataFrame([[sample_text]]).toDF(&#39;text&#39;)) Results : | sent_id | ner_chunk | entity | abbr_meaning | all_k_results | all_k_resolutions | |-:|:|:|:--|:--|:| | 0 | OTC | ABBR | over the counter | [&#39;over the counter&#39;, &#39;ornithine transcarbamoylase&#39;, &#39;enteric-coated&#39;, &#39;thyroxine&#39;] | [&#39;OTC&#39;, &#39;OTC&#39;, &#39;EC&#39;, &#39;T4&#39;] | sbiobertresolve_umls_drug_substance : This model maps clinical entities to UMLS CUI codes. It is trained on 2021AB UMLS dataset. The complete dataset has 127 different categories, and this model is trained on the Clinical Drug, Pharmacologic Substance, Antibiotic, Hazardous or Poisonous Substance categories using sbiobert_base_cased_mli embeddings. Example : ... umls_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_umls_drug_substance&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) ... results = model.fullAnnotate([&#39;Dilaudid&#39;, &#39;Hydromorphone&#39;, &#39;Exalgo&#39;, &#39;Palladone&#39;, &#39;Hydrogen peroxide 30 mg&#39;, &#39;Neosporin Cream&#39;, &#39;Magnesium hydroxide 100mg/1ml&#39;, &#39;Metformin 1000 mg&#39;]) Results : | | chunk | code | code_description | all_k_code_desc | all_k_codes | |:|:|:|:|:-|:-| | 0 | Dilaudid | C0728755 | dilaudid | [&#39;C0728755&#39;, &#39;C0719907&#39;, &#39;C1448344&#39;, &#39;C0305924&#39;, &#39;C1569295&#39;] | [&#39;dilaudid&#39;, &#39;Dilaudid HP&#39;, &#39;Disthelm&#39;, &#39;Dilaudid Injection&#39;, &#39;Distaph&#39;] | | 1 | Hydromorphone | C0012306 | HYDROMORPHONE | [&#39;C0012306&#39;, &#39;C0700533&#39;, &#39;C1646274&#39;, &#39;C1170495&#39;, &#39;C0498841&#39;] | [&#39;HYDROMORPHONE&#39;, &#39;Hydromorphone HCl&#39;, &#39;Phl-HYDROmorphone&#39;, &#39;PMS HYDROmorphone&#39;, &#39;Hydromorphone injection&#39;] | | 2 | Exalgo | C2746500 | Exalgo | [&#39;C2746500&#39;, &#39;C0604734&#39;, &#39;C1707065&#39;, &#39;C0070591&#39;, &#39;C3660437&#39;] | [&#39;Exalgo&#39;, &#39;exaltolide&#39;, &#39;Exelgyn&#39;, &#39;Extacol&#39;, &#39;exserohilone&#39;] | | 3 | Palladone | C0730726 | palladone | [&#39;C0730726&#39;, &#39;C0594402&#39;, &#39;C1655349&#39;, &#39;C0069952&#39;, &#39;C2742475&#39;] | [&#39;palladone&#39;, &#39;Palladone-SR&#39;, &#39;Palladone IR&#39;, &#39;palladiazo&#39;, &#39;palladia&#39;] | | 4 | Hydrogen peroxide 30 mg | C1126248 | hydrogen peroxide 30 MG/ML | [&#39;C1126248&#39;, &#39;C0304655&#39;, &#39;C1605252&#39;, &#39;C0304656&#39;, &#39;C1154260&#39;] | [&#39;hydrogen peroxide 30 MG/ML&#39;, &#39;Hydrogen peroxide solution 30%&#39;, &#39;hydrogen peroxide 30 MG/ML [Proxacol]&#39;, &#39;Hydrogen peroxide 30 mg/mL cutaneous solution&#39;, &#39;benzoyl peroxide 30 MG/ML&#39;] | | 5 | Neosporin Cream | C0132149 | Neosporin Cream | [&#39;C0132149&#39;, &#39;C0306959&#39;, &#39;C4722788&#39;, &#39;C0704071&#39;, &#39;C0698988&#39;] | [&#39;Neosporin Cream&#39;, &#39;Neosporin Ointment&#39;, &#39;Neomycin Sulfate Cream&#39;, &#39;Neosporin Topical Ointment&#39;, &#39;Naseptin cream&#39;] | | 6 | Magnesium hydroxide 100mg/1ml | C1134402 | magnesium hydroxide 100 MG | [&#39;C1134402&#39;, &#39;C1126785&#39;, &#39;C4317023&#39;, &#39;C4051486&#39;, &#39;C4047137&#39;] | [&#39;magnesium hydroxide 100 MG&#39;, &#39;magnesium hydroxide 100 MG/ML&#39;, &#39;Magnesium sulphate 100mg/mL injection&#39;, &#39;magnesium sulfate 100 MG&#39;, &#39;magnesium sulfate 100 MG/ML&#39;] | | 7 | Metformin 1000 mg | C0987664 | metformin 1000 MG | [&#39;C0987664&#39;, &#39;C2719784&#39;, &#39;C0978482&#39;, &#39;C2719786&#39;, &#39;C4282269&#39;] | [&#39;metformin 1000 MG&#39;, &#39;metFORMIN hydrochloride 1000 MG&#39;, &#39;METFORMIN HCL 1000MG TAB&#39;, &#39;metFORMIN hydrochloride 1000 MG [Fortamet]&#39;, &#39;METFORMIN HCL 1000MG SA TAB&#39;] | sbiobertresolve_loinc_cased : This model maps extracted clinical NER entities to LOINC codes using sbiobert_base_cased_mli Sentence Bert Embeddings. It is trained with augmented cased concept names since sbiobert model is cased. Example : ... loinc_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_loinc_cased&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) ... sample_text= &quot;&quot;&quot;The patient is a 22-year-old female with a history of obesity. She has a BMI of 33.5 kg/m2, aspartate aminotransferase 64, and alanine aminotransferase 126. Her hemoglobin is 8.2%.&quot;&quot;&quot; result = model.transform(spark.createDataFrame([[sample_text]], [&quot;text&quot;])) Results : +-++--+-+--+ | ner_chunk|entity| resolution| all_codes| resolutions| +-++--+-+--+ | BMI| Test| LP35925-4|[LP35925-4, 59574-4, BDYCRC, 73964-9, 59574-4,... |[Body mass index (BMI), Body mass index, Body circumference, Body muscle mass, Body mass index (BMI) [Percentile], ... | | aspartate aminotransferase| Test| 14409-7|[14409-7, 1916-6, 16325-3, 16324-6, 43822-6, 308... |[Aspartate aminotransferase, Aspartate aminotransferase/Alanine aminotransferase, Alanine aminotransferase/Aspartate aminotransferase, Alanine aminotransferase, Aspartate aminotransferase [Prese... | | alanine aminotransferase| Test| 16324-6|[16324-6, 16325-3, 14409-7, 1916-6, 59245-1, 30... |[Alanine aminotransferase, Alanine aminotransferase/Aspartate aminotransferase, Aspartate aminotransferase, Aspartate aminotransferase/Alanine aminotransferase, Alanine glyoxylate aminotransfer,... | | hemoglobin| Test| 14775-1|[14775-1, 16931-8, 12710-0, 29220-1, 15082-1, 72... |[Hemoglobin, Hematocrit/Hemoglobin, Hemoglobin pattern, Haptoglobin, Methemoglobin, Oxyhemoglobin, Hemoglobin test status, Verdohemoglobin, Hemoglobin A, Hemoglobin distribution width, Myoglobin,... | +-++--+-+--+ sbluebertresolve_loinc_uncased : This model maps extracted clinical NER entities to LOINC codes using sbluebert_base_uncased_mli Sentence Bert Embeddings. It trained on the augmented version of the uncased (lowercased) dataset which is used in previous LOINC resolver models. Example : ... loinc_resolver = SentenceEntityResolverModel.pretrained(&quot;sbluebertresolve_loinc_uncased&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;jsl_ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) ... sample_text= &quot;&quot;&quot;The patient is a 22-year-old female with a history of obesity. She has a BMI of 33.5 kg/m2, aspartate aminotransferase 64, and alanine aminotransferase 126. Her hgba1c is 8.2%.&quot;&quot;&quot; result = model.transform(spark.createDataFrame([[sample_text]], [&quot;text&quot;])) Results : +-++--+-+--+ | ner_chunk|entity| resolution| all_codes| resolutions| +-++--+-+--+ | BMI| Test| 39156-5|[39156-5, LP35925-4, BDYCRC, 73964-9, 59574-4,...] |[Body mass index, Body mass index (BMI), Body circumference, Body muscle mass, Body mass index (BMI) [Percentile], ...] | | aspartate aminotransferase| Test| 14409-7|[&#39;14409-7&#39;, &#39;16325-3&#39;, &#39;1916-6&#39;, &#39;16324-6&#39;,...] |[&#39;Aspartate aminotransferase&#39;, &#39;Alanine aminotransferase/Aspartate aminotransferase&#39;, &#39;Aspartate aminotransferase/Alanine aminotransferase&#39;, &#39;Alanine aminotransferase&#39;, ...] | | alanine aminotransferase| Test| 16324-6|[&#39;16324-6&#39;, &#39;1916-6&#39;, &#39;16325-3&#39;, &#39;59245-1&#39;,...] |[&#39;Alanine aminotransferase&#39;, &#39;Aspartate aminotransferase/Alanine aminotransferase&#39;, &#39;Alanine aminotransferase/Aspartate aminotransferase&#39;, &#39;Alanine glyoxylate aminotransferase&#39;,...] | | hgba1c| Test| 41995-2|[&#39;41995-2&#39;, &#39;LP35944-5&#39;, &#39;LP19717-5&#39;, &#39;43150-2&#39;,...]|[&#39;Hemoglobin A1c&#39;, &#39;HbA1c measurement device&#39;, &#39;HBA1 gene&#39;, &#39;HbA1c measurement device panel&#39;, ...] | +-++--+++ sbiobertresolve_snomed_drug : This model maps detected drug entities to SNOMED codes using sbiobert_base_cased_mli Sentence Bert Embeddings. Example : ... snomed_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_snomed_drug&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) ... sample_text = &quot;She is given Fragmin 5000 units subcutaneously daily, OxyContin 30 mg p.o. q.12 h., folic acid 1 mg daily, levothyroxine 0.1 mg p.o. daily, Avandia 4 mg daily, aspirin 81 mg daily, Neurontin 400 mg p.o. t.i.d., magnesium citrate 1 bottle p.o. p.r.n., sliding scale coverage insulin.&quot; results = model.transform(spark.createDataFrame([[sample_text]]).toDF(&#39;text&#39;)) Results : +--++--+--+++ | ner_chunk|entity| snomed_code| resolved_text| all_k_results| all_k_resolutions| +--++--+--+++ | Fragmin| DRUG| 9487801000001106| Fragmin|9487801000001106:::130752006:::28999000:::953500100000110...|Fragmin:::Fragilysin:::Fusarin:::Femulen:::Fumonisin:::Fr...| | OxyContin| DRUG| 9296001000001100| OxyCONTIN|9296001000001100:::373470001:::230091000001108:::55452001...|OxyCONTIN:::Oxychlorosene:::Oxyargin:::oxyCODONE:::Oxymor...| | folic acid| DRUG| 63718003| Folic acid|63718003:::6247001:::226316008:::432165000:::438451000124...|Folic acid:::Folic acid-containing product:::Folic acid s...| | levothyroxine| DRUG|10071011000001106| Levothyroxine|10071011000001106:::710809001:::768532006:::126202002:::7...|Levothyroxine:::Levothyroxine (substance):::Levothyroxine...| | Avandia| DRUG| 9217601000001109| avandia|9217601000001109:::9217501000001105:::12226401000001108::...|avandia:::avandamet:::Anatera:::Intanza:::Avamys:::Aragam...| | aspirin| DRUG| 387458008| Aspirin|387458008:::7947003:::5145711000001107:::426365001:::4125...|Aspirin:::Aspirin-containing product:::Aspirin powder:::A...| | Neurontin| DRUG| 9461401000001102| neurontin|9461401000001102:::130694004:::86822004:::952840100000110...|neurontin:::Neurolysin:::Neurine (substance):::Nebilet:::...| |magnesium citrate| DRUG| 12495006|Magnesium citrate|12495006:::387401007:::21691008:::15531411000001106:::408...|Magnesium citrate:::Magnesium carbonate:::Magnesium trisi...| | insulin| DRUG| 67866001| Insulin|67866001:::325072002:::414515005:::39487003:::411530000::...|Insulin:::Insulin aspart:::Insulin detemir:::Insulin-cont...| +--++--+--+++ New ICD10 to ICD9 Code Mapping Pretrained Pipeline We are releasing new icd10_icd9_mapping pretrained pipeline. This pretrained pipeline maps ICD10 codes to ICD9 codes without using any text data. You’ll just feed a comma or white space-delimited ICD10 codes and it will return the corresponding ICD9 codes as a list. Example : from sparknlp.pretrained import PretrainedPipeline pipeline = PretrainedPipeline(&quot;icd10_icd9_mapping&quot;, &quot;en&quot;, &quot;clinical/models&quot;) pipeline.annotate(&#39;E669 R630 J988&#39;) Results : {&#39;document&#39;: [&#39;E669 R630 J988&#39;], &#39;icd10&#39;: [&#39;E669&#39;, &#39;R630&#39;, &#39;J988&#39;], &#39;icd9&#39;: [&#39;27800&#39;, &#39;7830&#39;, &#39;5198&#39;]} Code Descriptions: | | ICD10 | Details | |:|:|:--| | 0 | E669 | Obesity | | 1 | R630 | Anorexia | | 2 | J988 | Other specified respiratory disorders | | | ICD9 | Details | |:|:|:--| | 0 | 27800 | Obesity | | 1 | 7830 | Anorexia | | 2 | 5198 | Other diseases of respiratory system | New Clinical Sentence Embedding Models We have two new clinical Sentence Embedding models. sbiobert_jsl_rxnorm_cased : This model maps sentences &amp; documents to a 768 dimensional dense vector space by using average pooling on top of BioBert model. It’s also fine-tuned on RxNorm dataset to help generalization over medication-related datasets. Example : ... sentence_embeddings = BertSentenceEmbeddings.pretrained(&quot;sbiobert_jsl_rxnorm_cased&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;sbioert_embeddings&quot;) ... sbert_jsl_medium_rxnorm_uncased : This model maps sentences &amp; documents to a 512-dimensional dense vector space by using average pooling on top of BERT model. It’s also fine-tuned on the RxNorm dataset to help generalization over medication-related datasets. Example : ... sentence_embeddings = BertSentenceEmbeddings.pretrained(&quot;sbert_jsl_medium_rxnorm_uncased&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;sbert_embeddings&quot;) ... Printing Validation and Test Logs in MedicalNerApproach and AssertionDLApproach Now we can check validation loss and test loss for each epoch in the logs created during trainings of MedicalNerApproach and AssertionDLApproach. Epoch 15/15 started, lr: 9.345794E-4, dataset size: 1330 Epoch 15/15 - 56.65s - loss: 37.58828 - avg training loss: 1.7899181 - batches: 21 Quality on validation dataset (20.0%), validation examples = 266 time to finish evaluation: 8.11s Total validation loss: 15.1930 Avg validation loss: 2.5322 label tp fp fn prec rec f1 I-Disease 707 72 121 0.9075738 0.8538647 0.8799004 B-Disease 657 81 60 0.8902439 0.916318 0.90309274 tp: 1364 fp: 153 fn: 181 labels: 2 Macro-average prec: 0.89890885, rec: 0.88509136, f1: 0.8919466 Micro-average prec: 0.89914304, rec: 0.8828479, f1: 0.89092094 Quality on test dataset: time to finish evaluation: 9.11s Total test loss: 17.7705 Avg test loss: 1.6155 label tp fp fn prec rec f1 I-Disease 663 113 126 0.85438144 0.8403042 0.8472843 B-Disease 631 122 77 0.8379814 0.8912429 0.86379194 tp: 1294 fp: 235 fn: 203 labels: 2 Macro-average prec: 0.8461814, rec: 0.86577356, f1: 0.85586536 Micro-average prec: 0.8463048, rec: 0.86439544, f1: 0.8552544 Filter Only the Regex Entities Feature in Deidentification Annotator The setBlackList() method will be able to filter just the detected Regex Entities. Before this change we filtered the chunks and the regex entities. Add .setMaskingPolicy Parameter in Deidentification Annotator Now we can have three modes to mask the entities in the Deidentification annotator. You can select the modes using the .setMaskingPolicy(&quot;entity_labels&quot;). The methods are the followings: “entity_labels”: Mask with the entity type of that chunk. (default) “same_length_chars”: Mask the deid entities with same length of asterix (*) with brackets ([,]) on both end. “fixed_length_chars”: Mask the deid entities with a fixed length of asterix (*). The length is setting up using the setFixedMaskLength(4) method. Given the following sentence John Snow is a good guy. the result will be: “entity_labels”: &lt;NAME&gt; is a good guy. “same_length_chars”: [*******] is a good guy. “fixed_length_chars”: **** is a good guy. Example Masked with entity labels DATE &lt;DATE&gt;, &lt;DOCTOR&gt;, The driver&#39;s license &lt;DLN&gt;. Masked with chars DATE [**********], [***********], The driver&#39;s license [*********]. Masked with fixed length chars DATE ****, ****, The driver&#39;s license ****. Obfuscated DATE 07-04-1981, Dr Vivian Irving, The driver&#39;s license K272344712994. Add .cache_folder Parameter in UpdateModels.updateCacheModels() This parameter lets user to define custom local paths for the folder on which pretrained models are saved (rather than using default cached_pretrained folder). This cache_folder must be a path (“hdfs:..”,”file:…”). UpdateModels.updateCacheModels(&quot;file:/home/jsl/cache_pretrained_2&quot;) UpdateModels.updateModels(&quot;12/01/2021&quot;,&quot;file:/home/jsl/cache_pretrained_2&quot;) The cache folder used by default is the folder loaded in the spark configuration ` spark.jsl.settings.pretrained.cache_folder.The default value for that property is ~/cache_pretrained` S3 Access Credentials No Longer Shipped Along Licenses S3 access credentials are no longer being shipped with licenses. Going forward, we’ll use temporal S3 access credentials which will be periodically refreshed. All this will happen automatically and will be transparent to the user. Still, for those users who would need to perform manual tasks involving access to S3, there’s a mechanism to get access to the set of credentials being used by the library at any given time. from sparknlp_jsl import get_credentials get_credentials(spark) Enhanced Security for the Library and log4shell Update On top of periodical security checks on the library code, 3rd party dependencies were analyzed, and some dependencies reported as containing vulnerabilities were replaced by more secure options. Also, the library was analyzed in the context of the recently discovered threat(CVE-2021-45105) on the log4j library. Spark NLP for Healthcare does not depend on the log4j library by itself, but the library gets loaded through some of its dependencies. It’s worth noting that the version of log4j dependency that will be in the classpath when running Spark NLP for Healthcare is 1.x, which would make the system vulnerable to CVE-2021-4104, instead of CVE-2021-45105. CVE-2021-4104 is related to the JMSAppender. Spark NLP for Healthcare does not provide any log4j configuration, so it’s up to the user to follow the recommendation of avoiding the use of the JMSAppender. New Peer-Reviewed Conference Paper on Clinical Relation Extraction We publish a new peer-reviewed conference paper titled Deeper Clinical Document Understanding Using Relation Extraction explaining the applications of Relation Extraction in a text mining framework comprising of Named Entity Recognition (NER) and Relation Extraction (RE) models. The paper is accepted to SDU (Scientific Document Understanding) workshop at AAAI-2022 conference and claims new SOTA scores on 5 out of 7 Biomedical &amp; Clinical Relation Extraction (RE) tasks. Dataset FCNN BioBERT Curr-SOTA i2b2-Temporal 68.7 73.6 72.41 i2b2-Clinical 60.4 69.1 67.97 DDI 69.2 72.1 84.1 CPI 65.8 74.3 88.9 PGR 81.2 87.9 79.4 ADE Corpus 89.2 90.0 83.7 Posology 87.8 96.7 96.1 Macro-averaged F1 scores of both RE models on public datasets. FCNN refers to the Speed-Optimized FCNN architecture, while BioBERT refers to the AccuracyOptimized BioBERT architecture. The SOTA metrics are obtained from (Guan et al. 2020), (Ningthoujam et al. 2019), (Asada, Miwa, and Sasaki 2020), (Phan et al. 2021), (Sousa and Couto 2020), (Crone 2020), and (Yang et al. 2021) respectively. New Peer-Reviewed Conference Paper on Adverse Drug Events Extraction We publish a new peer-reviewed conference paper titled Mining Adverse Drug Reactions from Unstructured Mediums at Scale proposing an end-to-end Adverse Drug Event mining solution using Classification, NER, and Relation Extraction Models. The paper is accepted to W3PHIAI (INTERNATIONAL WORKSHOP ON HEALTH INTELLIGENCE) workshop at AAAI-2022 conference, and claims new SOTA scores on 1 benchmark dataset for Classification, 3 benchmark datasets for NER, and 1 benchmark dataset for Relation Extraction. Task Dataset Spark NLP Curr-SOTA Classification ADE 85.96 87.0 Classification CADEC 86.69 81.5 Entity Recognition ADE 91.75 91.3 Entity Recognition CADEC 78.36 71.9 Entity Recognition SMM4H 76.73 67.81 Relation Extraction ADE 90.0 83.7 All F1 scores are Macro-averaged New and Updated Notebooks We have two new Notebooks: Chunk Sentence Splitter Notebook that involves usage of ChunkSentenceSplitter annotator. Clinical Relation Extraction Spark NLP Paper Reproduce Notebook that can be used for reproducing the results in Deeper Clinical Document Understanding Using Relation Extraction paper. We have updated our existing notebooks by adding new features and functionalities. Here are updated notebooks: Clinical Named Entity Recognition Model Clinical Entity Resolver Models Clinical DeIdentification Clinical NER Chunk Merger Pretrained Clinical Pipelines Healthcare Code Mapping Improved Entity Resolvers in Spark NLP with sBert To see more, please check : Spark NLP Healthcare Workshop Repo Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_4_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_4_0"
  },
  "216": {
    "id": "216",
    "title": "Annotation Lab Release Notes 3.4.0",
    "content": "3.4.0 Release date: 01-08-2022 We are very excited to release Annotation Lab v3.4.0 with support for Visual NER Automated Preannotation and Model Training. Spark NLP and Spark NLP for Healthcare libraries are upgraded to version 4.0. As always known security and bug fixes are also included with it. Here are the highlights of this release: Highlights Visual NER Training support. Annotation Lab offers the ability to train Visual NER models, apply active learning for automatic model training, and preannotate image-based tasks with existing models in order to accelerate annotation work. Floating or airgap licenses with scope ocr: inference and ocr: training are required for preannotation and training respectively. The minimal required training configuration is 64 GB RAM, 16 Core CPU for Visual NER Training. Visual NER Preannotation. For running preannotation on one or several tasks, the Project Owner or the Manager must select the target tasks and can click on the Preannotate button from the upper right side of the Tasks Page. The minimal required preannotation configuration is 32 GB RAM, 2 Core CPU for Visual NER Model. Spark NLP and Spark NLP for Healthcare upgrades. Annotation Lab 3.4.0 uses Spark NLP 4.0.0, Spark NLP for Healthcare 4.0.2 and Spark OCR 3.13.0. Confusion Matrix for Classification Projects. A checkbox is now added on the training page to enable the generation of confusion matrix for classification projects. The confusion matrix is visible in the live training logs as well as in the downloaded training logs. Project Import Improvements. The name of the imported project is set according to the name of the imported zip file. Users can now make changes in the content of the exported zip and then zip it back for import into Annotation Lab. Task Pagination in Labeling page. Tasks are paginated based on the number of characters they contain. Confidence filter slider is now visible only for preannotations. Previously the confidence filter was applied to both predictions and completions. Since all manual annotations have a confidence score of 1, we decided to only show and apply the confidence filter when the prediction widget is selected. Swagger Docs Changes. API docs have been restructured for an easier use and new methods have been added to mirror the new functionalities offered via the UI. Confidence score for Rules preannotations. Confidence of rule-based preannotations is now visible on the Labeling screen, the same as that of model-based preannotation. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_3_4_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_3_4_0"
  },
  "217": {
    "id": "217",
    "title": "Spark NLP release notes 3.4.0",
    "content": "3.4.0 Release date: 30-06-2021 Overview Signature Detection in image-based documents. More details please read in Signature Detection in Spark OCR New Features ImageSignatureDetector is a DL model for detecting signature on the image. New notebooks Image Signature Detection example Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_4_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_4_0"
  },
  "218": {
    "id": "218",
    "title": "Spark NLP for Healthcare Release Notes 3.4.1",
    "content": "3.4.1 We are glad to announce that Spark NLP Healthcare 3.4.1 has been released! Highlights Brand new Spanish deidentification NER models Brand new Spanish deidentification pretrained pipeline New clinical NER model to detect supplements New RxNorm sentence entity resolver model New EntityChunkEmbeddings annotator New MedicalBertForSequenceClassification annotator New MedicalDistilBertForSequenceClassification annotator New MedicalDistilBertForSequenceClassification and MedicalBertForSequenceClassification models Redesign of the ContextualParserApproach annotator getClasses method in RelationExtractionModel and RelationExtractionDLModel annotators Label customization feature for RelationExtractionModel and RelationExtractionDL models useBestModel parameter in MedicalNerApproach annotator Early stopping feature in MedicalNerApproach annotator Multi-Language support for faker and regex lists of Deidentification annotator Spark 3.2.0 compatibility for the entire library Saving visualization feature in spark-nlp-display library Deploying a custom Spark NLP image (for opensource, healthcare, and Spark OCR) to an enterprise version of Kubernetes: OpenShift New speed benchmarks table on databricks New &amp; Updated Notebooks List of recently updated or added models Brand New Spanish Deidentification NER Models We trained two new NER models to find PHI data (protected health information) that may need to be deidentified in Spanish. ner_deid_generic and ner_deid_subentity models are trained with in-house annotations. Both also are available for using Roberta Spanish Clinical Embeddings and sciwiki 300d. ner_deid_generic : Detects 7 PHI entities in Spanish (DATE, NAME, LOCATION, PROFESSION, CONTACT, AGE, ID). ner_deid_subentity : Detects 13 PHI sub-entities in Spanish (PATIENT, HOSPITAL, DATE, ORGANIZATION, E-MAIL, USERNAME, LOCATION, ZIP, MEDICALRECORD, PROFESSION, PHONE, DOCTOR, AGE). Example : ... embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_sciwiki_300d&quot;,&quot;es&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_generic&quot;, &quot;es&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) deid_sub_entity_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity&quot;, &quot;es&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_sub_entity&quot;) ... text = &quot;&quot;&quot;Antonio Pérez Juan, nacido en Cadiz, España. Aún no estaba vacunado, se infectó con Covid-19 el dia 14/03/2020 y tuvo que ir al Hospital. Fue tratado con anticuerpos monoclonales en la Clinica San Carlos..&quot;&quot;&quot; result = model.transform(spark.createDataFrame([[text]], [&quot;text&quot;])) Results : | chunk | ner_deid_generic_chunk | ner_deid_subentity_chunk | |--||--| | Antonio Pérez Juan | NAME | PATIENT | | Cádiz | LOCATION | LOCATION | | España | LOCATION | LOCATION | | 14/03/2022 | DATE | DATE | | Clínica San Carlos | LOCATION | HOSPITAL | Brand New Spanish Deidentification Pretrained Pipeline We developed a clinical deidentification pretrained pipeline that can be used to deidentify PHI information from Spanish medical texts. The PHI information will be masked and obfuscated in the resulting text. The pipeline can mask, fake or obfuscate the following entities: AGE, DATE, PROFESSION, E-MAIL, USERNAME, LOCATION, DOCTOR, HOSPITAL, PATIENT, URL, IP, MEDICALRECORD, IDNUM, ORGANIZATION, PHONE, ZIP, ACCOUNT, SSN, PLATE, SEX and IPADDR. from sparknlp.pretrained import PretrainedPipeline deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;, &quot;es&quot;, &quot;clinical/models&quot;) sample_text = &quot;&quot;&quot;Datos del paciente. Nombre: Jose . Apellidos: Aranda Martinez. NHC: 2748903. NASS: 26 37482910.&quot;&quot;&quot; result = deid_pipe.annotate(text) print(&quot; n&quot;.join(result[&#39;masked&#39;])) print(&quot; n&quot;.join(result[&#39;masked_with_chars&#39;])) print(&quot; n&quot;.join(result[&#39;masked_fixed_length_chars&#39;])) print(&quot; n&quot;.join(result[&#39;obfuscated&#39;])) Results: Masked with entity labels Datos del paciente. Nombre: &lt;PATIENT&gt; . Apellidos: &lt;PATIENT&gt;. NHC: &lt;SSN&gt;. NASS: &lt;SSN&gt; &lt;SSN&gt; Masked with chars Datos del paciente. Nombre: [**] . Apellidos: [*************]. NHC: [*****]. NASS: [**] [******] Masked with fixed length chars Datos del paciente. Nombre: **** . Apellidos: ****. NHC: ****. NASS: **** **** Obfuscated Datos del paciente. Nombre: Sr. Lerma . Apellidos: Aristides Gonzalez Gelabert. NHC: BBBBBBBBQR648597. NASS: 041010000011 RZRM020101906017 04. New Clinical NER Model to Detect Supplements We are releasing ner_supplement_clinical model that can extract benefits of using drugs for certain conditions. It can label detected entities as CONDITION and BENEFIT. Also this model is trained on the dataset that is released by Spacy in their HealthSea product. Here is the benchmark comparison of both versions: Entity Spark NLP Spacy-HealthSea BENEFIT 0.8729641 0.8330684 CONDITION 0.8339274 0.8333333 Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_supplement_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) ... results = ner_model.transform(spark.createDataFrame([[&quot;Excellent!. The state of health improves, nervousness disappears, and night sleep improves. It also promotes hair and nail growth.&quot;]], [&quot;text&quot;])) Results : +++ | chunk | ner_label | +++ | nervousness | CONDITION | | night sleep improves | BENEFIT | | hair | BENEFIT | | nail | BENEFIT | +++ New RxNorm Sentence Entity Resolver Model sbiobertresolve_rxnorm_augmented_re : This model maps clinical entities and concepts (like drugs/ingredients) to RxNorm codes without specifying the relations between the entities (relations are calculated on the fly inside the annotator) using sbiobert_base_cased_mli Sentence Bert Embeddings (EntityChunkEmbeddings). Example : ... rxnorm_resolver = SentenceEntityResolverModel .pretrained(&quot;sbiobertresolve_rxnorm_augmented_re&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;entity_chunk_embeddings&quot;]) .setOutputCol(&quot;rxnorm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) ... New EntityChunkEmbeddings Annotator We have a new EntityChunkEmbeddings annotator to compute a weighted average vector representing entity-related vectors. The model’s input usually consists of chunks of recognized named entities produced by MedicalNerModel. We can specify relations between the entities by the setTargetEntities() parameter, and the internal Relation Extraction model finds related entities and creates a chunk. Embedding for the chunk is calculated according to the weights specified in the setEntityWeights() parameter. For instance, the chunk warfarin sodium 5 MG Oral Tablet has DRUG, STRENGTH, ROUTE, and FORM entity types. Since DRUG label is the most prominent label for resolver models, now we can assign weight to prioritize DRUG label (i.e {&quot;DRUG&quot;: 0.8, &quot;STRENGTH&quot;: 0.2, &quot;ROUTE&quot;: 0.2, &quot;FORM&quot;: 0.2} as shown below). In other words, embeddings of these labels are multipled by the assigned weights such as DRUG by 0.8. For more details and examples, please check Sentence Entity Resolvers with EntityChunkEmbeddings Notebook in the Spark NLP workshop repo. Example : ... drug_chunk_embeddings = EntityChunkEmbeddings() .pretrained(&quot;sbiobert_base_cased_mli&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;drug_chunk_embeddings&quot;) .setMaxSyntacticDistance(3) .setTargetEntities({&quot;DRUG&quot;: [&quot;STRENGTH&quot;, &quot;ROUTE&quot;, &quot;FORM&quot;]}) .setEntityWeights({&quot;DRUG&quot;: 0.8, &quot;STRENGTH&quot;: 0.2, &quot;ROUTE&quot;: 0.2, &quot;FORM&quot;: 0.2}) rxnorm_resolver = SentenceEntityResolverModel .pretrained(&quot;sbiobertresolve_rxnorm_augmented_re&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;drug_chunk_embeddings&quot;]) .setOutputCol(&quot;rxnorm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) rxnorm_weighted_pipeline_re = Pipeline( stages = [ documenter, sentence_detector, tokenizer, embeddings, posology_ner_model, ner_converter, pos_tager, dependency_parser, drug_chunk_embeddings, rxnorm_resolver]) sampleText = [&quot;The patient was given metformin 500 mg, 2.5 mg of coumadin and then ibuprofen.&quot;, &quot;The patient was given metformin 400 mg, coumadin 5 mg, coumadin, amlodipine 10 MG&quot;] data_df = spark.createDataFrame(sample_df) results = rxnorm_weighted_pipeline_re.fit(data_df).transform(data_df) The internal relation extraction creates the chunks here, and the embedding is computed according to the weights. Results : +--+-+--+--+ |index| chunk|rxnorm_code_weighted_08_re| Concept_Name| +--+-+--+--+ | 0|metformin 500 mg| 860974|metformin hydrochloride 500 MG:::metformin 500 ...| | 0| 2.5 mg coumadin| 855313|warfarin sodium 2.5 MG [Coumadin]:::warfarin so...| | 0| ibuprofen| 1747293|ibuprofen Injection:::ibuprofen Pill:::ibuprofe...| | 1|metformin 400 mg| 332809|metformin 400 MG:::metformin 250 MG Oral Tablet...| | 1| coumadin 5 mg| 855333|warfarin sodium 5 MG [Coumadin]:::warfarin sodi...| | 1| coumadin| 202421|Coumadin:::warfarin sodium 2 MG/ML Injectable S...| | 1|amlodipine 10 MG| 308135|amlodipine 10 MG Oral Tablet:::amlodipine 10 MG...| +--+-+--+--+ New MedicalBertForSequenceClassification Annotator We developed a new annotator called MedicalBertForSequenceClassification. It can load BERT Models with sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for multi-class document classification tasks. New MedicalDistilBertForSequenceClassification Annotator We developed a new annotator called MedicalDistilBertForSequenceClassification. It can load DistilBERT Models with sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for multi-class document classification tasks. New MedicalDistilBertForSequenceClassification and MedicalBertForSequenceClassification Models We are releasing a new MedicalDistilBertForSequenceClassification model and three new MedicalBertForSequenceClassification models. bert_sequence_classifier_ade_biobert: a classifier for detecting if a sentence is talking about a possible ADE (TRUE, FALSE) bert_sequence_classifier_gender_biobert: a classifier for detecting the gender of the main subject of the sentence (MALE, FEMALE, UNKNOWN) bert_sequence_classifier_pico_biobert: a classifier for detecting the class of a sentence according to PICO framework (CONCLUSIONS, DESIGN_SETTING,INTERVENTION, PARTICIPANTS, FINDINGS, MEASUREMENTS, AIMS) Example : ... sequenceClassifier = MedicalBertForSequenceClassification.pretrained(&quot;bert_sequence_classifier_pico&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;,&quot;token&quot;]) .setOutputCol(&quot;class&quot;) ... sample_text = &quot;To compare the results of recording enamel opacities using the TF and modified DDE indices.&quot; result = sequence_clf_model.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : +-+--+ |text |label| +-+--+ |To compare the results of recording enamel opacities using the TF and modified DDE indices.|AIMS | +-+--+ distilbert_sequence_classifier_ade : This model is a DistilBertForSequenceClassification model for classifying clinical texts whether they contain ADE (TRUE, FALSE). Example : ... sequenceClassifier = MedicalDistilBertForSequenceClassification .pretrained(&#39;distilbert_sequence_classifier_ade&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&#39;token&#39;, &#39;document&#39;]) .setOutputCol(&#39;class&#39;) ... sample_text = &quot;I felt a bit drowsy and had blurred vision after taking Aspirin.&quot; result = sequence_clf_model.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : +-+--+ |text |label| +-+--+ |I felt a bit drowsy and had blurred vision after taking Aspirin.| True| +-+--+ Redesign of the ContextualParserApproach Annotator We’ve dropped the annotator’s contextMatch parameter and removed the need for a context field when feeding a JSON configuration file to the annotator. Context information can now be fully defined using the prefix, suffix and contextLength fields in the JSON configuration file. We’ve also fixed issues with the contextException field in the JSON configuration file - it was mismatching values in documents with several sentences and ignoring exceptions situated to the right of a word/token. The ruleScope field in the JSON configuration file can now be set to document instead of sentence. This allows you to match multi-word entities like “New York” or “Salt Lake City”. You can do this by setting &quot;ruleScope&quot; : &quot;document&quot; in the JSON configuration file and feeding a dictionary (csv or tsv) to the annotator with its setDictionary parameter. These changes also mean that we’ve dropped the updateTokenizer parameter since the new capabilities of ruleScope improve the user experience for matching multi-word entities. You can now feed in a dictionary in your chosen format - either vertical or horizontal. You can set that with the following parameter: setDictionary(&quot;dictionary.csv&quot;, options={&quot;orientation&quot;:&quot;vertical&quot;}) Lastly, there was an improvement made to the confidence value calculation process to better measure successful hits. For more explanation and examples, please check this Contextual Parser medium article and Contextual Parser Notebook. getClasses Method in RelationExtractionModel and RelationExtractionDLModel Annotators Now you can use getClasses() method for checking the relation labels of RE models (RelationExtractionModel and RelationExtractionDLModel) like MedicalNerModel(). Example : clinical_re_Model = RelationExtractionModel() .pretrained(&quot;re_temporal_events_clinical&quot;, &quot;en&quot;, &#39;clinical/models&#39;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) clinical_re_Model.getClasses() Output : [&#39;OVERLAP&#39;, &#39;BEFORE&#39;, &#39;AFTER&#39;] Label Customization Feature for RelationExtractionModel and RelationExtractionDL Models We are releasing label customization feature for Relation Extraction and Relation Extraction DL models by using .setCustomLabels() parameter. Example : ... reModel = RelationExtractionModel.pretrained(&quot;re_ade_clinical&quot;, &quot;en&quot;, &#39;clinical/models&#39;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setMaxSyntacticDistance(10) .setRelationPairs([&quot;drug-ade, ade-drug&quot;]) .setCustomLabels({&quot;1&quot;: &quot;is_related&quot;, &quot;0&quot;: &quot;not_related&quot;}) redl_model = RelationExtractionDLModel.pretrained(&#39;redl_ade_biobert&#39;, &#39;en&#39;, &quot;clinical/models&quot;) .setPredictionThreshold(0.5) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) .setCustomLabels({&quot;1&quot;: &quot;is_related&quot;, &quot;0&quot;: &quot;not_related&quot;}) ... sample_text = &quot;I experienced fatigue and muscle cramps after taking Lipitor but no more adverse after passing Zocor.&quot; result = model.transform(spark.createDataFrame([[sample_text]]).toDF(&#39;text&#39;)) Results : +--+-+-+-+-+-+ | relation|entity1| chunk1|entity2| chunk2|confidence| +--+-+-+-+-+-+ | is_related| ADE| fatigue| DRUG|Lipitor| 0.9999825| |not_related| ADE| fatigue| DRUG| Zocor| 0.9960077| | is_related| ADE|muscle cramps| DRUG|Lipitor| 1.0| |not_related| ADE|muscle cramps| DRUG| Zocor| 0.94971| +--+-+-+-+-+-+ useBestModel Parameter in MedicalNerApproach Annotator Introducing useBestModel param in MedicalNerApproach annotator. This param preserves and restores the model that has achieved the best performance at the end of the training. The priority is metrics from testDataset (micro F1), metrics from validationSplit (micro F1), and if none is set it will keep track of loss during the training. Example : med_ner = MedicalNerApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) ... ... .setUseBestModel(True) Early Stopping Feature in MedicalNerApproach Annotator Introducing earlyStopping feature for MedicalNerApproach(). You can stop training at the point when the perforfmance on test/validation dataset starts to degrage. Two params are added to MedicalNerApproach() in order to use this feature: earlyStoppingCriterion : (float) This is used set the minimal improvement of the test metric to terminate training. The metric monitored is the same as the metrics used in useBestModel (macro F1 when using test/validation set, loss otherwise). Default is 0 which means no early stopping is applied. earlyStoppingPatience: (int), the number of epoch without improvement which will be tolerated. Default is 0, which means that early stopping will occur at the first time when performance in the current epoch is no better than in the previous epoch. Example : med_ner = MedicalNerApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) ... ... .setTestDataset(test_data_parquet_path) .setEarlyStoppingCriterion(0.01) .setEarlyStoppingPatience(3) Multi-Language Support for Faker and Regex Lists of Deidentification Annotator We have a new .setLanguage() parameter in order to use internal Faker and Regex list for multi-language texts. When you are working with German and Spanish texts for a Deidentification, you can set this parameter to de for German and es for Spanish. Default value of this parameter is en. Example : deid_obfuscated = DeIdentification() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_chunk&quot;]) .setOutputCol(&quot;obfuscated&quot;) .setMode(&quot;obfuscate&quot;) .setLanguage(&#39;de&#39;) .setObfuscateRefSource(&quot;faker&quot;) Spark 3.2.0 Compatibility for the Entire Library Now we can use the Spark 3.2.0 version for Spark NLP for Healthcare by setting spark32=True in sparknlp_jsl.start() function. ! pip install --ignore-installed -q pyspark==3.2.0 import sparknlp_jsl spark = sparknlp_jsl.start(SECRET, spark32=True) Saving Visualization Feature in spark-nlp-display Library We have a new save_path parameter in spark-nlp-display library for saving any visualization results in Spark NLP. Example : from sparknlp_display import NerVisualizer visualiser = NerVisualizer() visualiser.display(light_result[0], label_col=&#39;ner_chunk&#39;, document_col=&#39;document&#39;, save_path=&quot;display_result.html&quot;) Deploying a Custom Spark NLP Image (for opensource, healthcare, and Spark OCR) to an Enterprise Version of Kubernetes: OpenShift Spark NLP for opensource, healthcare, and SPARK OCR is now available for Openshift - enterprise version of Kubernetes. For deployment, please refer to: Github Link: https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/platforms/openshift Youtube: https://www.youtube.com/watch?v=FBes-6ylFrM&amp;ab_channel=JohnSnowLabs New Speed Benchmarks Table on Databricks We prepared a speed benchmark table by running a clinical BERT For Token Classification model pipeline on various number of repartitioning and writing the results to parquet or delta formats. You can find the details here : Clinical Bert For Token Classification Benchmark Experiment. New &amp; Updated Notebooks We have updated our existing workshop notebooks with v3.4.0 by adding new features and functionalities. You can find the workshop notebooks updated with previous versions in the branches named with the relevant version. We have updated the ContextualParser Notebook with the new updates in this version. We have a new Sentence Entity Resolvers with EntityChunkEmbeddings Notebook for the new EntityChunkEmbeddings annotator. To see more, please check : Spark NLP Healthcare Workshop Repo List of Recently Updated or Added Models bert_sequence_classifier_ade_en bert_sequence_classifier_gender_biobert_en bert_sequence_classifier_pico_biobert_en distilbert_sequence_classifier_ade_en bert_token_classifier_ner_supplement_en deid_pipeline_es ner_deid_generic_es ner_deid_generic_roberta_es ner_deid_subentity_es ner_deid_subentity_roberta_es ner_nature_nero_clinical_en ner_supplement_clinical_en sbiobertresolve_clinical_abbreviation_acronym_en sbiobertresolve_rxnorm_augmented_re For all Spark NLP for healthcare models, please check : Models Hub Page Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_4_1",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_4_1"
  },
  "219": {
    "id": "219",
    "title": "Annotation Lab Release Notes 3.4.1",
    "content": "3.4.1 Release date: 05-08-2022 Annotation Lab v3.4.1 has beed released and it includes Models Hub and Visual NER bug fixes. Here are the highlights of this release: Highlights Confidence score of labels predicted by Visual NER model is now displayed in the labeling page. Missing image issues that appeared when deleting a task in Visual NER project has been fixed. Jumpy screen on annotating Visual NER tasks is resolved. Addition of new models supported in Spark NLP 4.0.0 Upgrade TensorFlow to 2.7.1 and PySpark to 3.2.0 Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_3_4_1",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_3_4_1"
  },
  "220": {
    "id": "220",
    "title": "Spark NLP for Healthcare Release Notes 3.4.2",
    "content": "3.4.2 We are glad to announce that Spark NLP Healthcare 3.4.2 has been released! Highlights New RCT Classifier, NER models and pipeline (Deidentification) Setting the scope window (target area) dynamically in Assertion Status detection models Reading JSON files (exported from ALAB) from HDFS with AnnotationJsonReader Allow users to write Tensorflow graphs to HDFS Serving Spark NLP on APIs Updated documentation on installing Spark NLP for Healthcare in AWS EMR (Jupyter, Livy, Yarn, Hadoop) New series of notebooks to reproduce the academic papers published by our colleagues PySpark tutorial notebooks to let non-Spark users get started with Apache Spark ecosystem in Python New &amp; updated notebooks List of recently updated or added models New RCT Classifier, NER Models and Pipeline (Deidentification) We are releasing a new bert_sequence_classifier_rct_biobert model, four new Spanish deidentification NER models (ner_deid_generic_augmented, ner_deid_subentity_augmented, ner_deid_generic_roberta_augmented, ner_deid_subentity_roberta_augmented) and a pipeline (clinical_deidentification_augmented). bert_sequence_classifier_rct_biobert: This model can classify the sections within abstract of scientific articles regarding randomized clinical trials (RCT) (BACKGROUND, CONCLUSIONS, METHODS, OBJECTIVE, RESULTS). Example : ... sequenceClassifier_model = MedicalBertForSequenceClassification.pretrained(&quot;bert_sequence_classifier_rct_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;,&#39;token&#39;]) .setOutputCol(&quot;class&quot;) ... sample_text = &quot;Previous attempts to prevent all the unwanted postoperative responses to major surgery with an epidural hydrophilic opioid , morphine , have not succeeded . The authors &#39; hypothesis was that the lipophilic opioid fentanyl , infused epidurally close to the spinal-cord opioid receptors corresponding to the dermatome of the surgical incision , gives equal pain relief but attenuates postoperative hormonal and metabolic responses more effectively than does systemic fentanyl .&quot; result = sequence_clf_model.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) &gt;&gt; class: &#39;BACKGROUND&#39; ner_deid_generic_augmented, ner_deid_subentity_augmented, ner_deid_generic_roberta_augmented, ner_deid_subentity_roberta_augmented models and clinical_deidentification_augmented pipeline : You can use either sciwi-embeddings (300 dimensions) or the Roberta Clinical Embeddings (infix _roberta_) with these NER models. These models and pipeline are different to their non-augmented versions in the following: They are trained with more data, now including an in-house annotated deidentification dataset; New SEX tag is available for all of them. This tag is now included in the NER and has been improved with more rules in the ContextualParsers of the pipeline, resulting in having a bigger recall to detect the sex of the patient. New STREET, CITY and COUNTRY entities are added to subentity versions. For more details and examples, please check Clinical Deidentification in Spanish notebook. Example : ... embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_sciwiki_300d&quot;,&quot;es&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_generic_augmented&quot;, &quot;es&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) deid_sub_entity_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_augmented&quot;, &quot;es&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_sub_entity&quot;) ... Results : chunk entity_subentity entity_generic -- - Antonio Miguel Martínez PATIENT NAME un varón SEX SEX 35 AGE AGE auxiliar de enfermería PROFESSION PROFESSION Cadiz CITY LOCATION España COUNTRY LOCATION Clinica San Carlos HOSPITAL LOCATION Setting the Scope Window (Target Area) Dynamically in Assertion Status Detection Models This parameter allows you to train the Assertion Status Models to focus on specific context windows when resolving the status of a NER chunk. The window is in format [X,Y] being X the number of tokens to consider on the left of the chunk, and Y the max number of tokens to consider on the right. Let’s take a look at what different windows mean: By default, the window is [-1,-1] which means that the Assertion Status will look at all of the tokens in the sentence/document (up to a maximum of tokens set in setMaxSentLen()). [0,0] means “don’t pay attention to any token except the ner_chunk”, what basically is not considering any context for the Assertion resolution. [9,15] is what empirically seems to be the best baseline, meaning that we look up to 9 tokens on the left and 15 on the right of the ner chunk to understand the context and resolve the status. Check this scope window tuning assertion status detection notebook that illustrates the effect of the different windows and how to properly fine-tune your AssertionDLModels to get the best of them. Example : assertion_status = AssertionDLApproach() .setGraphFolder(&quot;assertion_dl/&quot;) .setInputCols(&quot;sentence&quot;, &quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;assertion&quot;) ... ... .setScopeWindow([9, 15]) # NEW! Scope Window! Reading JSON Files (Exported from ALAB) From HDFS with AnnotationJsonReader Now we can read the dataframe from a HDFS that we read the files from in our cluster. Example : filename = &quot;hdfs:///user/livy/import.json&quot; reader = AnnotationToolJsonReader(assertion_labels = [&#39;AsPresent&#39;, &#39;AsAbsent&#39;, &#39;AsConditional&#39;, &#39;AsHypothetical&#39;, &#39;Family&#39;, &#39;AsPossible&#39;, &#39;AsElse&#39;]) df = reader.readDataset(spark, filename) Allow Users Write Tensorflow Graphs to HDFS Now we can save custom Tensorflow graphs to the HDFS that mainly being used in a cluster environment. tf_graph.build(&quot;ner_dl&quot;, build_params={&quot;embeddings_dim&quot;: 200, &quot;nchars&quot;: 128, &quot;ntags&quot;: 12, &quot;is_medical&quot;: 1}, model_location=&quot;hdfs:///user/livy&quot;, model_filename=&quot;auto&quot;) Serving Spark NLP on APIs Two new notebooks and a series of blog posts / Medium articles have been created to guide Spark NLP users to serve Spark NLP on a RestAPI. The notebooks can be found here. The articles can be found in the Technical Documentation of Spark NLP, available here and also in Medium: Serving Spark NLP via API (1/3): Microsoft’s Synapse ML Serving Spark NLP via API (2/3): FastAPI and LightPipelines Serving Spark NLP via API (3/3): Databricks Jobs and MLFlow Serve APIs The difference between both approaches are the following: SynapseML is a Microsoft Azure Open Source library used to carry out ML at scale. In this case, we use the Spark Serving feature, that leverages Spark Streaming and adds a web server with a Load Balancer, allowing concurrent processing of Spark NLP calls. Best approach if you look for scalability with Load Balancing. FastAPI + LightPipelines: A solution to run Spark NLP using a FastAPI webserver. It uses LightPipelines, what means having a very good performance but not leveraging Spark Clusters. Also, no Load Balancer is available in the suggestion, but you can create your own. Best approach if you look for performance. Databricks and MLFlow: Using MLFlow Serve or Databricks Jobs APIs to serve for inference Spark NLP pipelines from within Databricks. Best approach if you look for scalability within Databricks. Updated Documentation on Installing Spark NLP For Healthcare in AWS EMR (Jupyter, Livy, Yarn, Hadoop) Ready-to-go Spark NLP for Healthcare environment in AWS EMR. Full instructions are here. New Series of Notebooks to Reproduce the Academic Papers Published by Our Colleagues You can find all these notebooks here PySpark Tutorial Notebooks to Let Non-Spark Users to Get Started with Apache Spark Ecosystem in Python John Snow Labs has created a series of 8 notebooks to go over PySpark from zero to hero. Notebooks cover PySpark essentials, DataFrame creation, querying, importing data from different formats, functions / udfs, Spark MLLib examples (regression, classification, clustering) and Spark NLP best practises (usage of parquet, repartition, coalesce, custom annotators, etc). You can find all these notebooks here. New &amp; Updated Notebooks Series of academic notebooks : A new series of academic paper notebooks, available here Clinical_Deidentification_in_Spanish.ipynb: A notebook showcasing Clinical Deidentification in Spanish, available here. Clinical_Deidentification_Comparison.ipynb: A new series of comparisons between different Deidentification libraries. So far, it contains Spark NLP for Healthcare and ScrubaDub with Spacy Transformers. Available here. Scope_window_tuning_assertion_status_detection.ipynb: How to finetune Assertion Status using the Scope Window. Available here Clinical_Longformer_vs_BertSentence_&amp;_USE.ipynb: A Comparison of how Clinical Longformer embeddings, averaged by the Sentence Embeddings annotator, performs compared to BioBert and UniversalSentenceEncoding. Link here. Serving_SparkNLP_with_Synapse.ipynb: Serving SparkNLP for production purposes using Synapse ML. Available here Serving_SparkNLP_with_FastAPI_and_LP.ipynb: Serving SparkNLP for production purposes using FastAPI, RestAPI and LightPipelines. Available here Series of PySpark tutorial notebooks: Available here List of Recently Updated or Added Models sbiobertresolve_hcpcs bert_sequence_classifier_rct_biobert ner_deid_generic_augmented_es ner_deid_subentity_augmented_es ner_deid_generic_roberta_augmented_es ner_deid_subentity_roberta_augmented_es clinical_deidentification_augmented_es For all Spark NLP for healthcare models, please check : Models Hub Page Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_4_2",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_4_2"
  },
  "221": {
    "id": "221",
    "title": "Spark NLP for Healthcare Release Notes 3.5.0",
    "content": "3.5.0 We are glad to announce that Spark NLP Healthcare 3.5.0 has been released! Highlights Zero-shot Relation Extraction to extract relations between clinical entities with no training dataset Deidentification: New French Deidentification NER models and pipeline New Italian Deidentification NER models and pipeline Check our reference table for French and Italian deidentification metrics Added French support to the “fake” generation of data (aka data obfuscation) in the Deidentification annotator Deidentification benchmark: Spark NLP vs Cloud Providers (AWS, Azure, GCP) Graph generation: ChunkMapperApproach to augment NER chunks extracted by Spark NLP with a custom graph-like dictionary of relationships New Relation Extraction features: Configuration of case sensitivity in the name of the relations in Relation Extraction Models Models and Demos: We have reached 600 clinical models and pipelines, what sums up to 5000+ overall models in Models Hub! Check our new live demos including multilanguage deidentification to anonymize clinical notes in 5 different languages Generate Dataframes to train Assertion Status models using JSON Files exported from Annotation Lab (ALAB) Guide about how to scale from PoC to Production using Spark NLP for Healthcare in our new Medium Article, available here Core improvements: Contextual Parser (our Rule-based NER annotator) is now much more performant! Bug fixing and compatibility additions affecting and improving some behaviours of AssertionDL, BertSentenceChunkEmbeddings, AssertionFilterer and EntityRulerApproach New notebooks: zero-shot relation extraction and Deidentification benchmark vs Cloud Providers Zero-shot Relation Extraction to extract relations between clinical entities with no training dataset This release includes a zero-shot relation extraction model that leverages BertForSequenceClassificaiton to return, based on a predefined set of relation candidates (including no-relation / O), which one has the higher probability to be linking two entities. The dataset will be a csv which contains the following columns: sentence, chunk1, firstCharEnt1, lastCharEnt1, label1, chunk2, firstCharEnt2, lastCharEnt2, label2, rel. For example, let’s take a look at this dataset (columns chunk1, rel, chunk2 and sentence): +-+-+-+--+ | chunk1 | rel | chunk2 | sentence | |-+-+-+--| | light-headedness | PIP | diaphoresis | She states this light-headedness is often associated with shortness of breath and diaphoresis occasionally with nausea . | | respiratory rate | O | saturation | VITAL SIGNS - Temp 98.8 , pulse 60 , BP 150/94 , respiratory rate 18 , and saturation 96% on room air . | | lotions | TrNAP | incisions | No lotions , creams or powders to incisions . | | abdominal ultrasound | TeRP | gallbladder sludge | Abdominal ultrasound on 2/23/00 - This study revealed gallbladder sludge but no cholelithiasis . | | ir placement of a drainage catheter | TrAP | his abdominopelvic fluid collection | At that time he was made NPO with IVF , placed on Ampicillin / Levofloxacin / Flagyl and underwent IR placement of a drainage catheter for his abdominopelvic fluid collection | +-+-+-+--+ The relation types (TeRP, TrAP, PIP, TrNAP, etc…) are described here Let’s take a look at the first sentence! She states this light-headedness is often associated with shortness of breath and diaphoresis occasionally with nausea As we see in the table, the sentences includes a PIP relationship (Medical problem indicates medical problem), meaning that in that sentence, chunk1 (light-headedness) indicates chunk2 (diaphoresis). We set a list of candidates tags ([PIP, TrAP, TrNAP, TrWP, O]) and candidate sentences ([light-headedness caused diaphoresis, light-headedness was administered for diaphoresis, light-headedness was not given for diaphoresis, light-headedness worsened diaphoresis]), meaning that: PIP is expressed by light-headedness caused diaphoresis TrAP is expressed by light-headedness was administered for diaphoresis TrNAP is expressed by light-headedness was not given for diaphoresis TrWP is expressed by light-headedness worsened diaphoresis or something generic, like O is expressed by light-headedness and diaphoresis… We will get that the biggest probability of is PIP, since it’s phrase light-headedness caused diaphoresis is the most similar relationship expressing the meaning in the original sentence (light-headnedness is often associated with ... and diaphoresis) The example code is the following: ... re_ner_chunk_filter = sparknlp_jsl.annotator.RENerChunksFilter() .setRelationPairs([&quot;problem-test&quot;,&quot;problem-treatment&quot;]) .setMaxSyntacticDistance(4) .setDocLevelRelations(False) .setInputCols([&quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;re_ner_chunks&quot;) # The relations are defined by a map- keys are relation label, values are lists of predicated statements. The variables in curly brackets are NER entities, there could be more than one, e.g. &quot; improves &quot; re_model = sparknlp_jsl.annotator.ZeroShotRelationExtractionModel .pretrained(&quot;re_zeroshot_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setRelationalCategories({ &quot;CURE&quot;: [&quot; cures .&quot;], &quot;IMPROVE&quot;: [&quot; improves .&quot;, &quot; cures .&quot;], &quot;REVEAL&quot;: [&quot; reveals .&quot;]}) .setMultiLabel(False) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) pipeline = sparknlp.base.Pipeline() .setStages([documenter, tokenizer, sentencer, words_embedder, pos_tagger, ner_tagger, ner_converter, dependency_parser, re_ner_chunk_filter, re_model]) data = spark.createDataFrame( [[&quot;Paracetamol can alleviate headache or sickness. An MRI test can be used to find cancer.&quot;]] ).toDF(&quot;text&quot;) model = pipeline.fit(data) results = model.transform(data) results .selectExpr(&quot;explode(relations) as relation&quot;) .show(truncate=False) Results: +-+ |relation | +-+ |{category, 534, 613, REVEAL, {entity1_begin -&gt; 48, relation -&gt; REVEAL, hypothesis -&gt; An MRI test reveals cancer., confidence -&gt; 0.9760039, nli_prediction -&gt; entail, entity1 -&gt; TEST, syntactic_distance -&gt; 4, chunk2 -&gt; cancer, entity2_end -&gt; 85, entity1_end -&gt; 58, entity2_begin -&gt; 80, entity2 -&gt; PROBLEM, chunk1 -&gt; An MRI test, sentence -&gt; 1}, []} | |{category, 267, 357, IMPROVE, {entity1_begin -&gt; 0, relation -&gt; IMPROVE, hypothesis -&gt; Paracetamol improves sickness., confidence -&gt; 0.98819494, nli_prediction -&gt; entail, entity1 -&gt; TREATMENT, syntactic_distance -&gt; 3, chunk2 -&gt; sickness, entity2_end -&gt; 45, entity1_end -&gt; 10, entity2_begin -&gt; 38, entity2 -&gt; PROBLEM, chunk1 -&gt; Paracetamol, sentence -&gt; 0}, []}| |{category, 0, 90, IMPROVE, {entity1_begin -&gt; 0, relation -&gt; IMPROVE, hypothesis -&gt; Paracetamol improves headache., confidence -&gt; 0.9929625, nli_prediction -&gt; entail, entity1 -&gt; TREATMENT, syntactic_distance -&gt; 2, chunk2 -&gt; headache, entity2_end -&gt; 33, entity1_end -&gt; 10, entity2_begin -&gt; 26, entity2 -&gt; PROBLEM, chunk1 -&gt; Paracetamol, sentence -&gt; 0}, []} | +-+ Take a look at the example notebook here. Stay tuned for the few-shot Annotator to be release soon! New French Deidentification NER models and pipeline We trained two new NER models to find PHI data (protected health information) that may need to be deidentified in French. ner_deid_generic and ner_deid_subentity models are trained with in-house annotations. ner_deid_generic : Detects 7 PHI entities in French (DATE, NAME, LOCATION, PROFESSION, CONTACT, AGE, ID). ner_deid_subentity : Detects 15 PHI sub-entities in French (PATIENT, HOSPITAL, DATE, ORGANIZATION, E-MAIL, USERNAME, ZIP, MEDICALRECORD, PROFESSION, PHONE, DOCTOR, AGE, STREET, CITY, COUNTRY). Example : ... embeddings = WordEmbeddingsModel.pretrained(&quot;w2v_cc_300d&quot;, &quot;fr&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_generic&quot;, &quot;fr&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) deid_sub_entity_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity&quot;, &quot;fr&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_sub_entity&quot;) ... text = &quot;&quot;&quot;J&#39;ai vu en consultation Michel Martinez (49 ans) adressé au Centre Hospitalier De Plaisir pour un diabète mal contrôlé avec des symptômes datant de Mars 2015.&quot;&quot;&quot; result = model.transform(spark.createDataFrame([[text]], [&quot;text&quot;])) Results : | chunk | ner_deid_generic_chunk | ner_deid_subentity_chunk | |-||--| | Michel Martinez | NAME | PATIENT | | 49 ans | AGE | AGE | | Centre Hospitalier De Plaisir | LOCATION | HOSPITAL | | Mars 2015 | DATE | DATE | We also developed a clinical deidentification pretrained pipeline that can be used to deidentify PHI information from French medical texts. The PHI information will be masked and obfuscated in the resulting text. The pipeline can mask and obfuscate the following entities: DATE, AGE, SEX, PROFESSION, ORGANIZATION, PHONE, E-MAIL, ZIP, STREET, CITY, COUNTRY, PATIENT, DOCTOR, HOSPITAL, MEDICALRECORD, SSN, IDNUM, ACCOUNT, PLATE, USERNAME, URL, and IPADDR. from sparknlp.pretrained import PretrainedPipeline deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;, &quot;fr&quot;, &quot;clinical/models&quot;) text = &quot;&quot;&quot;PRENOM : Jean NOM : Dubois NUMÉRO DE SÉCURITÉ SOCIALE : 1780160471058 ADRESSE : 18 Avenue Matabiau VILLE : Grenoble CODE POSTAL : 38000&quot;&quot;&quot; result = deid_pipeline.annotate(text) Results: Masked with entity labels PRENOM : &lt;PATIENT&gt; NOM : &lt;PATIENT&gt; NUMÉRO DE SÉCURITÉ SOCIALE : &lt;SSN&gt; ADRESSE : &lt;STREET&gt; VILLE : &lt;CITY&gt; CODE POSTAL : &lt;ZIP&gt; Masked with chars PRENOM : [**] NOM : [****] NUMÉRO DE SÉCURITÉ SOCIALE : [***********] ADRESSE : [****************] VILLE : [******] CODE POSTAL : [***] Masked with fixed length chars PRENOM : **** NOM : **** NUMÉRO DE SÉCURITÉ SOCIALE : **** ADRESSE : **** VILLE : **** CODE POSTAL : **** Obfuscated PRENOM : Mme Olivier NOM : Mme Traore NUMÉRO DE SÉCURITÉ SOCIALE : 164033818514436 ADRESSE : 731, boulevard de Legrand VILLE : Sainte Antoine CODE POSTAL : 37443 New Italian Deidentification NER models and pipeline We trained two new NER models to find PHI data (protected health information) that may need to be deidentified in Italian. ner_deid_generic and ner_deid_subentity models are trained with in-house annotations. ner_deid_generic : Detects 8 PHI entities in Italian (DATE, NAME, LOCATION, PROFESSION, CONTACT, AGE, ID, SEX). ner_deid_subentity : Detects 19 PHI sub-entities in Italian (DATE, AGE, SEX, PROFESSION, ORGANIZATION, PHONE, EMAIL, ZIP, STREET, CITY, COUNTRY, PATIENT, DOCTOR, HOSPITAL, MEDICALRECORD, SSN, IDNUM, USERNAME, URL). Example : ... embeddings = WordEmbeddingsModel.pretrained(&quot;w2v_cc_300d&quot;, &quot;it&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) deid_ner = MedicalNerModel.pretrained(&quot;ner_deid_generic&quot;, &quot;it&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) deid_sub_entity_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity&quot;, &quot;it&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_sub_entity&quot;) ... text = &quot;&quot;&quot;Ho visto Gastone Montanariello (49 anni) riferito all&#39; Ospedale San Camillo per diabete mal controllato con sintomi risalenti a marzo 2015.&quot;&quot;&quot; result = model.transform(spark.createDataFrame([[text]], [&quot;text&quot;])) Results : | chunk | ner_deid_generic_chunk | ner_deid_subentity_chunk | |-||--| | Gastone Montanariello| NAME | PATIENT | | 49 | AGE | AGE | | Ospedale San Camillo | LOCATION | HOSPITAL | | marzo 2015 | DATE | DATE | We also developed a clinical deidentification pretrained pipeline that can be used to deidentify PHI information from Italian medical texts. The PHI information will be masked and obfuscated in the resulting text. The pipeline can mask and obfuscate the following entities: DATE, AGE, SEX, PROFESSION, ORGANIZATION, PHONE, E-MAIL, ZIP, STREET, CITY, COUNTRY, PATIENT, DOCTOR, HOSPITAL, MEDICALRECORD, SSN, IDNUM, ACCOUNT, PLATE, USERNAME, URL, and IPADDR. from sparknlp.pretrained import PretrainedPipeline deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;, &quot;it&quot;, &quot;clinical/models&quot;) sample_text = &quot;&quot;&quot;NOME: Stefano Montanariello CODICE FISCALE: YXYGXN51C61Y662I INDIRIZZO: Viale Burcardo 7 CODICE POSTALE: 80139&quot;&quot;&quot; result = deid_pipeline.annotate(sample_text) Results: Masked with entity labels NOME: &lt;PATIENT&gt; CODICE FISCALE: &lt;SSN&gt; INDIRIZZO: &lt;STREET&gt; CODICE POSTALE: &lt;ZIP&gt; Masked with chars NOME: [*******************] CODICE FISCALE: [**************] INDIRIZZO: [**************] CODICE POSTALE: [***] Masked with fixed length chars NOME: **** CODICE FISCALE: **** INDIRIZZO: **** CODICE POSTALE: **** Obfuscated NOME: Stefania Gregori CODICE FISCALE: UIWSUS86M04J604B INDIRIZZO: Viale Orlando 808 CODICE POSTALE: 53581 Check our reference table for French and Italian deidentification metrics Please find this reference table with metrics comparing F1 score for the available entities in French and Italian clinical pipelines: |Entity Label |Italian|French| |-|-|| |PATIENT |0.9069 |0.9382| |DOCTOR |0.9171 |0.9912| |HOSPITAL |0.8968 |0.9375| |DATE |0.9835 |0.9849| |AGE |0.9832 |0.8575| |PROFESSION |0.8864 |0.8147| |ORGANIZATION |0.7385 |0.7697| |STREET |0.9754 |0.8986| |CITY |0.9678 |0.8643| |COUNTRY |0.9262 |0.8983| |PHONE |0.9815 |0.9785| |USERNAME |0.9091 |0.9239| |ZIP |0.9867 |1.0 | |E-MAIL |1 |1.0 | |MEDICALRECORD|0.8085 |0.939 | |SSN |0.9286 |N/A | |URL |1 |N/A | |SEX |0.9697 |N/A | |IDNUM |0.9576 |N/A | Added French support in Deidentification Annotator for data obfuscation Our Deidentificator annotator is now able to obfuscate entities (coming from a deid NER model) with fake data in French language. Example: Example code: ... embeddings = WordEmbeddingsModel.pretrained(&quot;w2v_cc_300d&quot;, &quot;fr&quot;).setInputCols([&quot;sentence&quot;, &quot;token&quot;]).setOutputCol(&quot;word_embeddings&quot;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity&quot;, &quot;fr&quot;, &quot;clinical/models&quot;).setInputCols([&quot;sentence&quot;,&quot;token&quot;, &quot;word_embeddings&quot;]).setOutputCol(&quot;ner&quot;) ner_converter = NerConverter().setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]).setOutputCol(&quot;ner_chunk&quot;) de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateDate(True) .setRefSep(&quot;#&quot;) .setDateTag(&quot;DATE&quot;) .setLanguage(&quot;fr&quot;) .setObfuscateRefSource(&#39;faker&#39;) pipeline = Pipeline() .setStages([ documentAssembler, sentenceDetector, tokenizer, embeddings, clinical_ner, ner_converter, de_identification ]) sentences = [ [&quot;&quot;&quot;J&#39;ai vu en consultation Michel Martinez (49 ans) adressé au Centre Hospitalier De Plaisir pour un diabète mal contrôlé avec des symptômes datant&quot;&quot;&quot;] ] my_input_df = spark.createDataFrame(sentences).toDF(&quot;text&quot;) output = pipeline.fit(my_input_df).transform(my_input_df) ... Entities detected: ++-+ |token |entity | ++-+ |J&#39;ai |O | |vu |O | |en |O | |consultation|O | |Michel |B-PATIENT | |Martinez |I-PATIENT | |( |O | |49 |B-AGE | |ans |O | |) |O | |adressé |O | |au |O | |Centre |B-HOSPITAL| |Hospitalier |I-HOSPITAL| |De |I-HOSPITAL| |Plaisir |I-HOSPITAL| |pour |O | |un |O | |diabète |O | |mal |O | ++-+ Obfuscated sentence: +--+ |result | +--+ |[J&#39;ai vu en consultation Sacrispeyre Ligniez (86 ans) adressé au Centre Hospitalier Pierre Futin pour un diabète mal contrôlé avec des symptômes datant]| +--+ Deidentification benchmark: Spark NLP vs Cloud Providers (AWS, Azure, GCP) We have published a new notebook with a benchmark and the reproduceable code, comparing Spark NLP for Healthcare Deidentification capabilities of one of our English pipelines (clinical_deidentification_glove_augmented) versus: AWS Comprehend Medical Azure Cognitive Services GCP Data Loss Prevention The notebook is available here, and the results are the following: SPARK NLP AWS AZURE GCP AGE 1 0.96 0.93 0.9 DATE 1 0.99 0.9 0.96 DOCTOR 0.98 0.96 0.7 0.6 HOSPITAL 0.92 0.89 0.72 0.72 LOCATION 0.9 0.81 0.87 0.73 PATIENT 0.96 0.95 0.78 0.48 PHONE 1 1 0.8 0.97 ID 0.93 0.93 - - ChunkMapperApproach: mapping extracted entities to an ontology (Json dictionary) with relations We have released a new annotator, called ChunkMapperApproach(), that receives a ner_chunk and a Json with a mapping of NER entities and relations, and returns the ner_chunk augmented with the relations from the Json ontology. Example of a small ontology with relations: Giving the map with entities and relationships stored in mapper.json, we will use an NER to detect entities in a text and, in case any of them is found, the ChunkMapper will augment the output with the relationships from this dictionary: {&quot;mappings&quot;: [{ &quot;key&quot;: &quot;metformin&quot;, &quot;relations&quot;: [{ &quot;key&quot;: &quot;action&quot;, &quot;values&quot; : [&quot;hypoglycemic&quot;, &quot;Drugs Used In Diabets&quot;] },{ &quot;key&quot;: &quot;treatment&quot;, &quot;values&quot; : [&quot;diabetes&quot;, &quot;t2dm&quot;] }] }] text = [&quot;&quot;&quot;The patient was prescribed 1 unit of Advil for 5 days after meals. The patient was also given 1 unit of Metformin daily. He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day.&quot;&quot;&quot;] ... nerconverter = NerConverterInternal() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;ner_chunk&quot;) chunkerMapper = ChunkMapperApproach() .setInputCols(&quot;ner_chunk&quot;) .setOutputCol(&quot;relations&quot;) .setDictionary(&quot;mapper.json&quot;) .setRel(&quot;action&quot;) pipeline = Pipeline().setStages([document_assembler,sentence_detector,tokenizer, ner, nerconverter, chunkerMapper]) res = pipeline.fit(test_data).transform(test_data) res.select(F.explode(&#39;ner_chunk.result&#39;).alias(&quot;chunks&quot;)).show(truncate=False) Entities: +-+ |chunks | +-+ |Metformin | |insulin glargine| |insulin lispro | |metformin | |mg | |times | +-+ Checking the relations: ... pd_df = res.select(F.explode(&#39;relations&#39;).alias(&#39;res&#39;)).select(&#39;res.result&#39;, &#39;res.metadata&#39;).toPandas() ... Results: Entity: metformin Main relation: hypoglycemic Other relations (included in metadata): Drugs Used In Diabets Configuration of case sensitivity in the name of the relations in Relation Extraction Models We have added a new parameter, called ‘relationPairsCaseSensitive’, which affects the way setRelationPairs works. If relationPairsCaseSensitive is True, then the pairs of entities in the dataset should match the pairs in setRelationPairs in their specific case (case sensitive). By default it’s set to False, meaning that the match of those relation names is case insensitive. Before 3.5.0, .setRelationPairs([&quot;dosage-drug&quot;]) would not return relations if it was trained with a relation called DOSAGE-DRUG (different casing). Now, setting .setRelationPairs([&quot;dosage-drug&quot;])and relationPairsCaseSensitive(False) or just leaving it by default, it will return any dosage-drug or DOSAGE-DRUG relationship. Example of usage in Python: ... reModel = RelationExtractionModel() .pretrained(&quot;posology_re&quot;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setMaxSyntacticDistance(4) .setRelationPairs([&quot;dosage-drug&quot;]) .setRelationPairsCaseSensitive(False) .setOutputCol(&quot;relations_case_insensitive&quot;) ... This will return relations named dosage-drug, DOSAGE-DRUG, etc. We have reached the milestone of 600 clinical models (and 5000+ models overall) ! 🥳 This release added to Spark NLP Models Hub 100+ pretrained clinical pipelines, available to use as one-liners, including some of the most used NER models, namely: ner_deid_generic_pipeline_de: German deidentification pipeline with aggregated (generic) labels ner_deid_subentity_pipeline_de: German deidentification pipeline with specific (subentity) labels ner_clinical_biobert_pipeline_en: A pretrained pipeline based on ner_clinical_biobert to carry out NER on BioBERT embeddings ner_abbreviation_clinical_pipeline_en: A pretrained pipeline based on ner_abbreviation_clinical that detects medical acronyms and abbreviations ner_ade_biobert_pipeline_en: A pretrained pipeline based on ner_ade_biobert to carry out Adverse Drug Events NER recognition using BioBERT embeddings ner_ade_clinical_pipeline_en: Similar to the previous one, but using clinical_embeddings ner_radiology_pipeline_en: A pretrained pipeline to detect Radiology entities (coming from ner_radiology_wip model) ner_events_clinical_pipeline_en: A pretrained pipeline to extract Clinical Events related entities (leveraging ner_events_clinical) ner_anatomy_biobert_pipeline_en: A pretrained pipeline to extract Anamoty entities (from ner_anamoty_biobert) …100 more Here is how you can use any of the pipelines with one line of code: from sparknlp.pretrained import PretrainedPipeline pipeline = PretrainedPipeline(&quot;explain_clinical_doc_medication&quot;, &quot;en&quot;, &quot;clinical/models&quot;) result = pipeline.fullAnnotate(&quot;&quot;&quot;The patient is a 30-year-old female with a long history of insulin dependent diabetes, type 2. She received a course of Bactrim for 14 days for UTI. She was prescribed 5000 units of Fragmin subcutaneously daily, and along with Lantus 40 units subcutaneously at bedtime.&quot;&quot;&quot;)[0] Results: +-+-++ | | chunks | entities | |:|:|:--| | 0 | insulin | DRUG | | 1 | Bactrim | DRUG | | 2 | for 14 days | DURATION | | 3 | 5000 units | DOSAGE | | 4 | Fragmin | DRUG | | 5 | subcutaneously | ROUTE | | 6 | daily | FREQUENCY | | 7 | Lantus | DRUG | | 8 | 40 units | DOSAGE | | 9 | subcutaneously | ROUTE | | 10 | at bedtime | FREQUENCY | +-+-++ +-+-++-+ | | chunks | entities | assertion | |:|:|:--|:| | 0 | insulin | DRUG | Present | | 1 | Bactrim | DRUG | Past | | 2 | Fragmin | DRUG | Planned | | 3 | Lantus | DRUG | Planned | +-+-++-+ +-+--++--+-+ | relation | entity1 | chunk1 | entity2 | chunk2 | |:|:-|:--|:-|:| | DRUG-DURATION | DRUG | Bactrim | DURATION | for 14 days | | DOSAGE-DRUG | DOSAGE | 5000 units | DRUG | Fragmin | | DRUG-ROUTE | DRUG | Fragmin | ROUTE | subcutaneously | | DRUG-FREQUENCY | DRUG | Fragmin | FREQUENCY | daily | | DRUG-DOSAGE | DRUG | Lantus | DOSAGE | 40 units | | DRUG-ROUTE | DRUG | Lantus | ROUTE | subcutaneously | | DRUG-FREQUENCY | DRUG | Lantus | FREQUENCY | at bedtime | +-+--++--+-+ We have updated our 11.Pretrained_Clinical_Pipelines.ipynb notebook to properly show this addition. Don’t forget to check it out! All of our scalable, production-ready Spark NLP Clinical Models and Pipelines can be found in our Models Hub Finally, we have added two new entityMapper models: drug_ontology and section_mapper For all Spark NLP for healthcare models, please check our Models Hub webpage Have you checked our demo page? New several demos were created, available at https://nlp.johnsnowlabs.com/demos In this release we feature the Multilingual deidentification, showcasing how to deidentify clinical texts in English, Spanish, German, French and Italian. This demo is available here For the rest of the demos, please visit Models Hub Demos Page Generate Dataframes to train Assertion Status Models using JSON files exported from Annotation Lab (ALAB) Now we can generate a dataframe that can be used to train an AssertionDLModel by using the output of AnnotationToolJsonReader.generatePlainAssertionTrainSet(). The dataframe contains all the columns that you need for training. Example : filename = &quot;../json_import.json&quot; reader = AnnotationToolJsonReader(assertion_labels = [&#39;AsPresent&#39;, &#39;AsAbsent&#39;, &#39;AsConditional&#39;, &#39;AsHypothetical&#39;, &#39;AsFamily&#39;, &#39;AsPossible&#39;, &#39;AsElse&#39;]) df = reader.readDataset(spark, filename) reader.generatePlainAssertionTrainSet(df).show(truncate=False) Results : +-+--+--++--++ |task_id|sentence |begin|end|ner |assertion| +-+--+--++--++ |1 |Patient has a headache for the last 2 weeks |2 |3 |a headache |AsPresent| +-+--+--++--++ Understand how to scale from a PoC to Production using Spark NLP for Healthcare in our new Medium Article, available here We receive many questions about how Spark work distribution is carried out, what specially becomes important before making the leap from a PoC to a big scalable, production-ready cluster. This article helps you understand: How many different ways to create a cluster are available, as well as their advantages and disadvantages; How to scale all of them; How to take advantage of autoscalability and autotermination policy in Cloud Providers; Which are the steps to take depending on your infrastructure, to make the leap to production; If you need further assistance, please reach our Support team at support@johnsnowlabs.com Contextual Parser (our Rule-based NER annotator) is now much more performant! Contextual Parser has been improved in terms of performance. These are the metrics comparing 3.4.2 and 3.5.0 4 cores and 30 GB RAM ===================== 10 MB 20 MB 30MB 50MB 3.4.2 349 786 982 1633 3.5.0 142 243 352 556 8 cores and 60 GB RAM ===================== 10 MB 20 MB 30MB 50MB 3.4.2 197 373 554 876 3.5.0 79 136 197 294 We have reached the milestone of 600 clinical demos! During this release, we included: More than 100+ recently created clinical models and pipelines, including NER, NER+RE, NER+Assertion+RE, etc. Added two new entityMapper models: drug_action_treatment_mapper and normalized_section_header_mapper For all Spark NLP for healthcare models, please check : Models Hub Page Bug fixing and compatibility additions This is the list of fixed issues and bugs, as well as one compatibility addition between EntityRuler and AssertionFiltered: Error in AssertionDLApproach and AssertionLogRegApproach: an error was being triggered wthen the dataset contained long (64bits) instead of 32 bits integers for the start / end columns. Now this bug is fixed. Error in BertSentenceChunkEmbeddings: loading a model after downloading it with pretrained() was triggering an error. Now you can load any model after downloading it with pretrained(). Adding setIncludeConfidence to AssertionDL Python version, where it was missing. Now, it’s included in both Python and Scala, as described here Making EntityRuler and AssertionFiltered compatible: AssertionFilterer annotator that is being used to filter the entities based on entity labels now can be used by EntityRulerApproach, a rule based entity extractor: Path(&quot;test_file.jsonl&quot;).write_text(json.dumps({&quot;id&quot;:&quot;cough&quot;,&quot;label&quot;:&quot;COUGH&quot;,&quot;patterns&quot;:[&quot;cough&quot;,&quot;coughing&quot;]})) ... entityRuler = EntityRulerApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;ner_chunk&quot;) .setPatternsResource(&quot;test_file.jsonl&quot;, ReadAs.TEXT, {&quot;format&quot;: &quot;jsonl&quot;}) clinical_assertion = AssertionDLModel.pretrained(&quot;assertion_dl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) assertion_filterer = AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;assertion_filtered&quot;) .setWhiteList([&quot;present&quot;]) ... empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) ruler_model = rulerPipeline.fit(empty_data) text = &quot;I have a cough but no fatigue or chills.&quot; ruler_light_model = LightPipeline(ruler_model).fullAnnotate(text)[0][&#39;assertion_filtered&#39;] Result: Annotation(chunk, 9, 13, cough, {&#39;entity&#39;: &#39;COUGH&#39;, &#39;id&#39;: &#39;cough&#39;, &#39;sentence&#39;: &#39;0&#39;})] New notebooks: zero-shot relation extraction and Deidentification benchmark (Spark NLP and Cloud Providers) Check these recently notebooks created by our Healthcare team and available in our Spark NLP Workshop git repo, where you can find many more. Zero-shot Relation Extraction, available here. Deidentification benchmark (SparkNLP and Cloud Providers), available here Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_5_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_5_0"
  },
  "222": {
    "id": "222",
    "title": "Annotation Lab Release Notes 3.5.0",
    "content": "3.5.0 Release date: 25-08-2022 Annotation Lab 3.5.0 add support for out-of-the-box usage of Multilingual Models as well as support for some of the European Language Models: Romanian, Portuguese, Danish and Italian. It also provides support for split dataset using Test/Train tags in classification project and allows NER pretrained models evaluation with floating license. The release also includes fixes for known security vulnerabilities and for some bug reported by our user community. Here are the highlights of this release: Highlights Support for Multilingual Models. Previously, only multilingual embeddings were available in Models Hub page. A new language filter has been added to the Models hub page to make searching for all available multilingual models and embeddings more efficient. User can select the target language and then explore the set of relevant multilingual models and embeddings. Expended Support for European Language Models. Annotation Lab now offers support for four new European languages Romanian, Portuguese, Italian, and Danish, on top of English, Spanish, and German, already supported in previous versions. Many pretrained models in those languages are now available to download from the NLP Models Hub and easily use to preannotate documents on the Annotation Lab. Use Test/Train Tags for Classification Training Experiments. The Test/Train split of annotated tasks can be used when training classification models. When this option is checked on the Training Settings, all tasks that have the Test tag are used as test datasets. All tasks tagged as Train together with all other non Test tasks will be used as a training dataset. NER Model Evaluation available for Floating License. Project Owner and/or Manager can evaluate pretrained NER models against a set of annotated tasks in the presence of floating licenses. Earlier, this feature was only available in the presence of airgap licenses. Chunks preannotation in VisualNER. Annotation Lab 3.4.0 which first published the visual NER preannotation and visual NER model training could only create token level preannotations. With version 3.5.0, individual tokens are combined into one chunk entity and shown as merged to the user. Benchmarking Information for Models Trained with Annotation Lab. With version 3.5.0 benchmarking information is available for models trained within Annotation Lab. User can go to the Available Models Tab of the Models Hub page and view the benchmarking data by clicking the small graph icon next to the model. Configuration for Annotation Lab Deployment. The resources allocated to Annotation Lab deployment can be configured via the resource values in the annotationlab-updater.sh. The instruction to change the parameters are available in the instruction.md file. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_3_5_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_3_5_0"
  },
  "223": {
    "id": "223",
    "title": "Spark NLP release notes 3.5.0",
    "content": "3.5.0 Release date: 15-07-2021 Overview Improve table detection and table recognition. More details please read in Extract Tabular Data from PDF in Spark OCR New Features Added new method to ImageTableCellDetector which support borderless tables and combined tables. Added Wolf and Singh adaptive binarization methods to the ImageAdaptiveThresholding. Enhancements Added possibility to use different type of images as input for ImageTableDetector. Added display_pdf and display_images_horizontal util functions. New notebooks Tables Recognition from PDF Pdf de-identification on Databricks Dicom de-identification on Databricks Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_5_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_5_0"
  },
  "224": {
    "id": "224",
    "title": "Spark NLP for Healthcare Release Notes 3.5.1",
    "content": "3.5.1 We are glad to announce that 3.5.1 version of Spark NLP for Healthcare has been released! Highlights Deidentification: New Portuguese Deidentification NER models and pretrained pipeline. This is the 6th supported language for deidentification (English, German, Spanish, Italian, French and Portuguese). New pretrained models and pipelines: New RxNorm Sentence Entity Resolver model to map and extract pharmaceutical actions (e.g. analgesic, hypoglycemic) as well as treatments (e.g. backache, diabetes) along with the RxNorm code resolved (sbiobertresolve_rxnorm_action_treatment) New RCT classification models and pretrained pipelines to classify the sections within the abstracts of scientific articles regarding randomized clinical trials (RCT). (rct_binary_classifier_use, rct_binary_classifier_biobert, bert_sequence_classifier_binary_rct_biobert, rct_binary_classifier_use_pipeline, rct_binary_classifier_biobert_pipeline, bert_sequence_classifier_binary_rct_biobert_pipeline) New features: Add getClasses() attribute for MedicalBertForTokenClassifier and MedicalBertForSequenceClassification to find out the entity classes of the models Download the AnnotatorModels from the healthcare library using the Healthcare version instead of the open source version (the pretrained models were used to be dependent on open source Spark NLP version before) New functionality to download and extract clinical models from S3 via direct zip url. Core improvements: Fixing the confidence scores in MedicalNerModel when setIncludeAllConfidenceScores is true Graph_builder relation_extraction model file name extension problem with auto parameter. List of recently updated or added models Portuguese Deidentification Models This is the 6th supported language for deidentification (English, German, Spanish, Italian, French and Portuguese). This version includes two Portuguese deidentification models to mask or obfuscate Protected Health Information in the Portuguese language. The models are the following: ner_deid_generic: extracts Name, Profession, Age, Date, Contact (Telephone numbers, Email addresses), Location (Address, City, Postal code, Hospital Name, Organization), ID (Social Security numbers, Medical record numbers) and Sex entities. See Model Hub Page for details. ner_deid_subentity: Patient (name), Hospital (name), Date, Organization, City, ID, Street, Sex, Email, ZIP, Profession, Phone, Country, Doctor (name) and Age See Model Hub Page for details. You will use the w2v_cc_300d Portuguese Embeddings with these models. The pipeline should look as follows: ... word_embeddings = WordEmbeddingsModel.pretrained(&quot;w2v_cc_300d&quot;, &quot;pt&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_subentity = MedicalNerModel.pretrained(&quot;ner_deid_subentity&quot;, &quot;pt&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;ner_deid_subentity&quot;) ner_converter_subentity = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner_deid_subentity&quot;]) .setOutputCol(&quot;ner_chunk_subentity&quot;) ner_generic = MedicalNerModel.pretrained(&quot;ner_deid_generic&quot;, &quot;pt&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;ner_deid_generic&quot;) ner_converter_generic = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner_deid_generic&quot;]) .setOutputCol(&quot;ner_chunk_generic&quot;) nlpPipeline = Pipeline(stages=[ documentAssembler, sentencerDL, tokenizer, word_embeddings, ner_subentity, ner_converter_subentity, ner_generic, ner_converter_generic, ]) text = &quot;&quot;&quot;Detalhes do paciente. Nome do paciente: Pedro Gonçalves NHC: 2569870. Endereço: Rua Das Flores 23. Código Postal: 21754-987. Dados de cuidados. Data de nascimento: 10/10/1963. Idade: 53 anos Data de admissão: 17/06/2016. Doutora: Maria Santos&quot;&quot;&quot; data = spark.createDataFrame([[text]]).toDF(&quot;text&quot;) results = nlpPipeline.fit(data).transform(data) Results: +--+-+ |chunk |ner_generic_label|ner_subentity_label| +--+-+ |Pedro Gonçalves | NAME | PATIENT | |2569870 | ID | ID | |Rua Das Flores 23| LOCATION | STREET | |21754-987 | LOCATION | ZIP | |10/10/1963 | DATE | DATE | |53 | AGE | AGE | |17/06/2016 | DATE | DATE | |Maria Santos | NAME | DOCTOR | +--+-+ We also include a Clinical Deidentification Pipeline for Portuguese that uses ner_deid_subentity NER model and also several ContextualParsers for rule based contextual Named Entity Recognition tasks. It’s available to be used as follows: from sparknlp.pretrained import PretrainedPipeline deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;, &quot;pt&quot;, &quot;clinical/models&quot;) The pretrained pipeline comes with Deidentification and Obfuscation capabilities as shows the following example: text = &quot;&quot;&quot;RELAÇÃO HOSPITALAR NOME: Pedro Gonçalves NHC: MVANSK92F09W408A ENDEREÇO: Rua Burcardo 7 CÓDIGO POSTAL: 80139 DATA DE NASCIMENTO: 03/03/1946 IDADE: 70 anos SEXO: Homens E-MAIL: pgon21@tim.pt DATA DE ADMISSÃO: 12/12/2016 DOUTORA: Eva Andrade RELATO CLÍNICO: 70 anos, aposentado, sem alergia a medicamentos conhecida, com a seguinte história: ex-acidente de trabalho com fratura de vértebras e costelas; operado de doença de Dupuytren na mão direita e ponte ílio-femoral esquerda; diabetes tipo II, hipercolesterolemia e hiperuricemia; alcoolismo ativo, fuma 20 cigarros/dia. Ele foi encaminhado a nós por apresentar hematúria macroscópica pós-evacuação em uma ocasião e microhematúria persistente posteriormente, com evacuação normal. O exame físico mostrou bom estado geral, com abdome e genitais normais; o toque retal foi compatível com adenoma de próstata grau I/IV. A urinálise mostrou 4 hemácias/campo e 0-5 leucócitos/campo; o resto do sedimento era normal. O hemograma é normal; a bioquímica mostrou uma glicemia de 169 mg/dl e triglicerídeos 456 mg/dl; função hepática e renal são normais. PSA de 1,16 ng/ml. DIRIGIDA A: Dr. Eva Andrade - Centro Hospitalar do Medio Ave - Avenida Dos Aliados, 56 E-MAIL: evandrade@poste.pt &quot;&quot;&quot; result = deid_pipeline.annotate(text) Results: | | Sentence | Masked | Masked with Chars | Masked with Fixed Chars | Obfuscated | |:|:-|:|:-|:--|:-| | 0 | RELAÇÃO HOSPITALAR | RELAÇÃO HOSPITALAR | RELAÇÃO HOSPITALAR | RELAÇÃO HOSPITALAR | RELAÇÃO HOSPITALAR | | | NOME: Pedro Gonçalves | NOME: &lt;DOCTOR&gt; | NOME: [*************] | NOME: **** | NOME: Isabel Magalhães | | 1 | NHC: MVANSK92F09W408A | NHC: &lt;ID&gt; | NHC: [**************] | NHC: **** | NHC: 124 445 311 | | 2 | ENDEREÇO: Rua Burcardo 7 | ENDEREÇO: &lt;STREET&gt; | ENDEREÇO: [************] | ENDEREÇO: **** | ENDEREÇO: Rua de Santa María, 100 | | 3 | CÓDIGO POSTAL: 80139 | CÓDIGO POSTAL: &lt;ZIP&gt; | CÓDIGO POSTAL: [***] | CÓDIGO POSTAL: **** | CÓDIGO POSTAL: 1000-306 | | | DATA DE NASCIMENTO: 03/03/1946 | DATA DE NASCIMENTO: &lt;DATE&gt; | DATA DE NASCIMENTO: [********] | DATA DE NASCIMENTO: **** | DATA DE NASCIMENTO: 04/04/1946 | | 4 | IDADE: 70 anos | IDADE: &lt;AGE&gt; anos | IDADE: ** anos | IDADE: **** anos | IDADE: 46 anos | | 5 | SEXO: Homens | SEXO: &lt;SEX&gt; | SEXO: [****] | SEXO: **** | SEXO: Mulher | | 6 | E-MAIL: pgon21@tim.pt | E-MAIL: &lt;EMAIL&gt; | E-MAIL: [***********] | E-MAIL: **** | E-MAIL: eric.shannon@geegle.com | | | DATA DE ADMISSÃO: 12/12/2016 | DATA DE ADMISSÃO: &lt;DATE&gt; | DATA DE ADMISSÃO: [********] | DATA DE ADMISSÃO: **** | DATA DE ADMISSÃO: 23/12/2016 | | 7 | DOUTORA: Eva Andrade | DOUTORA: &lt;DOCTOR&gt; | DOUTORA: [*********] | DOUTORA: **** | DOUTORA: Isabel Magalhães | See Model Hub Page for details. Check Spark NLP Portuguese capabilities in 4.7.Clinical_Deidentification_in_Portuguese.ipynb notebook we have prepared for you. New RxNorm Sentence Entity Resolver Model (sbiobertresolve_rxnorm_action_treatment) We are releasing sbiobertresolve_rxnorm_action_treatment model that maps clinical entities and concepts (like drugs/ingredients) to RxNorm codes using sbiobert_base_cased_mli Sentence Bert Embeddings. This resolver model maps and extracts pharmaceutical actions (e.g analgesic, hypoglycemic) as well as treatments (e.g backache, diabetes) along with the RxNorm code resolved. Actions and treatments of the drugs are returned in all_k_aux_labels column. See Model Card for details. Example : documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;ner_chunk&quot;) sbert_embedder = BertSentenceEmbeddings.pretrained(&#39;sbiobert_base_cased_mli&#39;, &#39;en&#39;,&#39;clinical/models&#39;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) rxnorm_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_rxnorm_action_treatment&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;rxnorm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) pipelineModel = PipelineModel( stages = [ documentAssembler, sbert_embedder, rxnorm_resolver]) lp_model = LightPipeline(pipelineModel) text = [&quot;Zita 200 mg&quot;, &quot;coumadin 5 mg&quot;, &#39;avandia 4 mg&#39;] result= lp_model.annotate(text) Results* : | | ner_chunk | rxnorm_code | action | treatment | |:|:--|--:|:-|| | 0 | Zita 200 mg | 104080 | [&#39;Analgesic&#39;, &#39;Antacid&#39;, &#39;Antipyretic&#39;] | [&#39;Backache&#39;, &#39;Pain&#39;, &#39;Sore Throat&#39;]| | 1 | coumadin 5 mg | 855333 | [&#39;Anticoagulant&#39;] | [&#39;Cerebrovascular Accident&#39;] | | 2 | avandia 4 mg | 261242 | [&#39;Drugs Used In Diabets&#39;,&#39;Hypoglycemic&#39;]| [&#39;Diabetes Mellitus&#39;, ...] | | New RCT Classification Models and Pretrained Pipelines We are releasing new Randomized Clinical Trial (RCT) classification models and pretrained pipelines that can classify the sections within the abstracts of scientific articles regarding randomized clinical trials (RCT). Classification Models: rct_binary_classifier_use (Models Hub page) rct_binary_classifier_biobert (Models Hub page) bert_sequence_classifier_binary_rct_biobert (Models Hub page) Pretrained Pipelines: rct_binary_classifier_use_pipeline (Models Hub page) rct_binary_classifier_biobert_pipeline (Models Hub page) bert_sequence_classifier_binary_rct_biobert_pipeline (Models Hub page) Classification Model Example : ... use = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) classifier_dl = ClassifierDLModel.pretrained(&#39;rct_binary_classifier_use&#39;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCol(&quot;class&quot;) use_clf_pipeline = Pipeline( stages = [ document_assembler, use, classifier_dl ]) sample_text = &quot;&quot;&quot;Abstract:Based on the American Society of Anesthesiologists&#39; Practice Guidelines for Sedation and Analgesia by Non-Anesthesiologists (ASA-SED), a sedation training course aimed at improving medical safety was developed by the Japanese Association for Medical Simulation in 2011. This study evaluated the effect of debriefing on participants&#39; perceptions of the essential points of the ASA-SED. A total of 38 novice doctors participated in the sedation training course during the research period. Of these doctors, 18 participated in the debriefing group, and 20 participated in non-debriefing group. Scoring of participants&#39; guideline perceptions was conducted using an evaluation sheet (nine items, 16 points) created based on the ASA-SED. The debriefing group showed a greater perception of the ASA-SED, as reflected in the significantly higher scores on the evaluation sheet (median, 16 points) than the control group (median, 13 points; p &lt; 0.05). No significant differences were identified before or during sedation, but the difference after sedation was significant (p &lt; 0.05). Debriefing after sedation training courses may contribute to better perception of the ASA-SED, and may lead to enhanced attitudes toward medical safety during sedation and analgesia. &quot;&quot;&quot; result = use_clf_pipeline.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : &gt;&gt; class: True Pretrained Pipeline Example : from sparknlp.pretrained import PretrainedPipeline pipeline = PretrainedPipeline(&quot;rct_binary_classifier_use_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) text = &quot;&quot;&quot;Abstract:Based on the American Society of Anesthesiologists&#39; Practice Guidelines for Sedation and Analgesia by Non-Anesthesiologists (ASA-SED), a sedation training course aimed at improving medical safety was developed by the Japanese Association for Medical Simulation in 2011. This study evaluated the effect of debriefing on participants&#39; perceptions of the essential points of the ASA-SED. A total of 38 novice doctors participated in the sedation training course during the research period. Of these doctors, 18 participated in the debriefing group, and 20 participated in non-debriefing group. Scoring of participants&#39; guideline perceptions was conducted using an evaluation sheet (nine items, 16 points) created based on the ASA-SED. The debriefing group showed a greater perception of the ASA-SED, as reflected in the significantly higher scores on the evaluation sheet (median, 16 points) than the control group (median, 13 points; p &lt; 0.05). No significant differences were identified before or during sedation, but the difference after sedation was significant (p &lt; 0.05). Debriefing after sedation training courses may contribute to better perception of the ASA-SED, and may lead to enhanced attitudes toward medical safety during sedation and analgesia. &quot;&quot;&quot; result = pipeline.annotate(text) Results : &gt;&gt; class: True New Features Add getClasses() attribute to MedicalBertForTokenClassifier and MedicalBertForSequenceClassification Now you can use getClasses() method for checking the entity labels of MedicalBertForTokenClassifier and MedicalBertForSequenceClassification like MedicalNerModel. tokenClassifier = MedicalBertForTokenClassifier.pretrained(&quot;bert_token_classifier_ner_ade&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) .setMaxSentenceLength(512) tokenClassifier.getClasses() [&#39;B-DRUG&#39;, &#39;I-ADE&#39;, &#39;I-DRUG&#39;, &#39;O&#39;, &#39;B-ADE&#39;] Download the AnnotatorModels from the healthcare library using the Healthcare version instead of the open source version Now we download the private models using the Healthcare version instead of the open source version (the pretrained models were used to be dependent on open source Spark NLP version before). New functionality to download and extract clinical models from S3 via direct link. Now, you can download clinical models from S3 via direct link directly by downloadModelDirectly method. See the Models Hub Page to find out the download url of each model. from sparknlp.pretrained import ResourceDownloader #The first argument is the path to the zip file and the second one is the folder. ResourceDownloader.downloadModelDirectly(&quot;clinical/models/assertion_dl_en_2.0.2_2.4_1556655581078.zip&quot;, &quot;clinical/models&quot;) Core improvements: Fix MedicalNerModel confidence scores when setIncludeAllConfidenceScores is True A mismatch problem between the tag with the highest confidence score and the predicted tag in MedicalNerModel is resolved. Graph_builder relation_extraction model file name extension problem with auto param A naming problem which occurs while generating a graph for Relation Extraction via graph builder was resolved. Now, the TF graph is generated with the correct extension (.pb). List of Recently Updated or Added Models ner_deid_generic_pt ner_deid_subentity_pt clinical_deidentification_pt sbiobertresolve_rxnorm_action_treatment rct_binary_classifier_use rct_binary_classifier_biobert bert_sequence_classifier_binary_rct_biobert rct_binary_classifier_use_pipeline rct_binary_classifier_biobert_pipeline bert_sequence_classifier_binary_rct_biobert_pipeline sbiobertresolve_ndc Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_5_1",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_5_1"
  },
  "225": {
    "id": "225",
    "title": "Spark NLP for Healthcare Release Notes 3.5.2",
    "content": "3.5.2 Highlights TFGraphBuilder annotator to create graphs for training NER, Assertion, Relation Extraction, and Generic Classifier models Default TF graphs added for AssertionDLApproach to let users train models without custom graphs New functionalities in ContextualParserApproach Printing the list of clinical pretrained models and pipelines with one-liner New clinical models Clinical NER model (ner_biomedical_bc2gm) Clinical ChunkMapper models (abbreviation_mapper, rxnorm_ndc_mapper, drug_brandname_ndc_mapper, rxnorm_action_treatment_mapper) Bug fixes New and updated notebooks List of recently updated or added models TFGraphBuilder annotator to create graphs for Training NER, Assertion, Relation Extraction, and Generic Classifier Models We have a new annotator used to create graphs in the model training pipeline. TFGraphBuilder inspects the data and creates the proper graph if a suitable version of TensorFlow (&lt;= 2.7 ) is available. The graph is stored in the defined folder and loaded by the approach. You can use this builder with MedicalNerApproach, RelationExtractionApproach, AssertionDLApproach, and GenericClassifierApproach Example: graph_folder_path = &quot;./medical_graphs&quot; med_ner_graph_builder = TFGraphBuilder() .setModelName(&quot;ner_dl&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setGraphFile(&quot;auto&quot;) .setHiddenUnitsNumber(20) .setGraphFolder(graph_folder_path) med_ner = MedicalNerApproach() ... .setGraphFolder(graph_folder) medner_pipeline = Pipeline()([ ..., med_ner_graph_builder, med_ner ]) For more examples, please check TFGraph Builder Notebook. Default TF graphs added for AssertionDLApproach to let users train models without custom graphs We added default TF graphs for the AssertionDLApproach to let users train assertion models without specifying any custom TF graph. Default Graph Features: Feature Sizes: 100, 200, 768 Number of Classes: 2, 4, 8 New Functionalities in ContextualParserApproach Added .setOptionalContextRules parameter that allows to output regex matches regardless of context match (prefix, suffix configuration). Allows sending a JSON string of the configuration file to setJsonPath parameter. Confidence Value Scenarios: When there is regex match only, the confidence value will be 0.5. When there are regex and prefix matches together, the confidence value will be &gt; 0.5 depending on the distance between target token and the prefix. When there are regex and suffix matches together, the confidence value will be &gt; 0.5 depending on the distance between target token and the suffix. When there are regex, prefix, and suffix matches all together, the confidence value will be &gt; than the other scenarios. Example: jsonString = { &quot;entity&quot;: &quot;CarId&quot;, &quot;ruleScope&quot;: &quot;sentence&quot;, &quot;completeMatchRegex&quot;: &quot;false&quot;, &quot;regex&quot;: &quot; d+&quot;, &quot;prefix&quot;: [&quot;red&quot;], &quot;contextLength&quot;: 100 } with open(&quot;jsonString.json&quot;, &quot;w&quot;) as f: json.dump(jsonString, f) contextual_parser = ContextualParserApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;entity&quot;) .setJsonPath(&quot;jsonString.json&quot;) .setCaseSensitive(True) .setOptionalContextRules(True) Printing the List of Clinical Pretrained Models and Pipelines with One-Liner Now we can check what the clinical model names are of a specific annotator and the names of clinical pretrained pipelines in a language. Listing Clinical Model Names: Example: from sparknlp_jsl.pretrained import InternalResourceDownloader InternalResourceDownloader.showPrivateModels(&quot;AssertionDLModel&quot;) Results: +--+++ | Model | lang | version | +--+++ | assertion_ml | en | 2.0.2 | | assertion_dl | en | 2.0.2 | | assertion_dl_healthcare | en | 2.7.2 | | assertion_dl_biobert | en | 2.7.2 | | assertion_dl | en | 2.7.2 | | assertion_dl_radiology | en | 2.7.4 | | assertion_jsl_large | en | 3.1.2 | | assertion_jsl | en | 3.1.2 | | assertion_dl_scope_L10R10 | en | 3.4.2 | | assertion_dl_biobert_scope_L10R10 | en | 3.4.2 | +--+++ Listing Clinical Pretrained Pipelines: from sparknlp_jsl.pretrained import InternalResourceDownloader InternalResourceDownloader.showPrivatePipelines(&quot;en&quot;) +--+++ | Pipeline | lang | version | +--+++ | clinical_analysis | en | 2.4.0 | | clinical_ner_assertion | en | 2.4.0 | | clinical_deidentification | en | 2.4.0 | | clinical_analysis | en | 2.4.0 | | explain_clinical_doc_ade | en | 2.7.3 | | icd10cm_snomed_mapping | en | 2.7.5 | | recognize_entities_posology | en | 3.0.0 | | explain_clinical_doc_carp | en | 3.0.0 | | recognize_entities_posology | en | 3.0.0 | | explain_clinical_doc_ade | en | 3.0.0 | | explain_clinical_doc_era | en | 3.0.0 | | icd10cm_snomed_mapping | en | 3.0.2 | | snomed_icd10cm_mapping | en | 3.0.2 | | icd10cm_umls_mapping | en | 3.0.2 | | snomed_umls_mapping | en | 3.0.2 | | ... | ... | ... | +--+++ New ner_biomedical_bc2gm NER Model This model has been trained to extract genes/proteins from a medical text. See Model Card for more details. Example : ... ner = MedicalNerModel.pretrained(&quot;ner_biomedical_bc2gm&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... text = spark.createDataFrame([[&quot;Immunohistochemical staining was positive for S-100 in all 9 cases stained, positive for HMB-45 in 9 (90%) of 10, and negative for cytokeratin in all 9 cases in which myxoid melanoma remained in the block after previous sections.&quot;]]).toDF(&quot;text&quot;) result = model.transform(text) Results : +--++ |chunk |ner_label | +--++ |S-100 |GENE_PROTEIN| |HMB-45 |GENE_PROTEIN| |cytokeratin|GENE_PROTEIN| +--++ New Clinical ChunkMapper Models We have 4 new ChunkMapper models and a new Chunk Mapping Notebook for showing their examples. drug_brandname_ndc_mapper: This model maps drug brand names to corresponding National Drug Codes (NDC). Product NDCs for each strength are returned in result and metadata. See Model Card for more details. Example : document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;chunk&quot;) chunkerMapper = ChunkMapperModel.pretrained(&quot;drug_brandname_ndc_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;chunk&quot;]) .setOutputCol(&quot;ndc&quot;) .setRel(&quot;Strength_NDC&quot;) model = PipelineModel(stages=[document_assembler, chunkerMapper]) light_model = LightPipeline(model) res = light_model.fullAnnotate([&quot;zytiga&quot;, &quot;ZYVOX&quot;, &quot;ZYTIGA&quot;]) Results : +-+--+--+ | Brandname | Strenth_NDC | Other_NDSs | +-+--+--+ | zytiga | 500 mg/1 | 57894-195 | [&#39;250 mg/1 | 57894-150&#39;] | | ZYVOX | 600 mg/300mL | 0009-4992 | [&#39;600 mg/300mL | 66298-7807&#39;, &#39;600 mg/300mL | 0009-7807&#39;] | | ZYTIGA | 500 mg/1 | 57894-195 | [&#39;250 mg/1 | 57894-150&#39;] | +-+--+--+ abbreviation_mapper: This model maps abbreviations and acronyms of medical regulatory activities with their definitions. See Model Card for details. Example: input = [&quot;&quot;&quot;Gravid with estimated fetal weight of 6-6/12 pounds. LABORATORY DATA: Laboratory tests include a CBC which is normal. HIV: Negative. One-Hour Glucose: 117. Group B strep has not been done as yet.&quot;&quot;&quot;] &gt;&gt; output: ++-+ |Abbreviation|Definition | ++-+ |CBC |complete blood count | |HIV |human immunodeficiency virus| ++-+ rxnorm_action_treatment_mapper: RxNorm and RxNorm Extension codes with their corresponding action and treatment. Action refers to the function of the drug in various body systems; treatment refers to which disease the drug is used to treat. See Model Card for more details. Example: input = [&#39;Sinequan 150 MG&#39;, &#39;Zonalon 50 mg&#39;] &gt;&gt; output: ++++ |chunk |rxnorm_code |Action | ++++ |Sinequan 150 MG|1000067 |Antidepressant | |Zonalon 50 mg |103971 |Analgesic | ++++ rxnorm_ndc_mapper: This pretrained model maps RxNorm and RxNorm Extension codes with corresponding National Drug Codes (NDC). See Model Card for more details. Example: input = [&#39;doxepin hydrochloride 50 MG/ML&#39;, &#39;macadamia nut 100 MG/ML&#39;] &gt;&gt; output: ++++ |chunk |rxnorm_code |Product NDC | ++++ |doxepin hydrochloride 50 MG/ML|1000091 |00378-8117 | |macadamia nut 100 MG/ML |212433 |00064-2120 | ++++ Bug Fixes We fixed some issues in DrugNormalizer, DateNormalizer and ContextualParserApproach annotators. DateNormalizer : We fixed some relative date issues and also DateNormalizer takes account the Leap years now. DrugNormalizer : Fixed some formats. ContextualParserApproach : Computing the right distance for prefix. Extracting the right content for suffix. Handling special characters in prefix and suffix. New and Updated Notebooks We prepared Spark NLP for Healthcare 3hr Notebook to cover mostly used components of Spark NLP in ODSC East 2022-3 hours hands-on workshop on ‘Modular Approach to Solve Problems at Scale in Healthcare NLP’. You can also find its Databricks version here. New Chunk Mapping Notebook for showing the examples of Chunk Mapper models. Updated healthcare tutorial notebooks for Databricks with sparknlp_jsl v3.5.1 We have a new Databricks healthcare tutorials folder in which you can find all Spark NLP for Healthcare Databricks tutorial notebooks. Updated Graph Builder Notebook by adding the examples of new TFGraphBuilder annotator. List of Recently Updated or Added Models sbiobertresolve_rxnorm_action_treatment ner_biomedical_bc2gm abbreviation_mapper rxnorm_ndc_mapper drug_brandname_ndc_mapper sbiobertresolve_cpt_procedures_measurements_augmented sbiobertresolve_icd10cm_slim_billable_hcc sbiobertresolve_icd10cm_slim_normalized For all Spark NLP for healthcare models, please check : Models Hub Page Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_5_2",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_5_2"
  },
  "226": {
    "id": "226",
    "title": "Spark NLP for Healthcare Release Notes 3.5.3",
    "content": "3.5.3 Highlights New rxnorm_mapper model New ChunkMapperFilterer annotator to filter ChunkMapperModel results New features Add the setReplaceLabels parameter that allows replacing the non-conventional labels without using an external source file in the NerConverterInternal(). Case sensitivity can be set in ChunkMapperApproach and ChunkMapperModel through setLowerCase() parameter. Return multiple relations at a time in ChunkMapperModel models via setRels() parameter. Filter the multi-token chunks separated with whitespace in ChunkMapperApproach by setAllowMultiTokenChunk() parameter. New license validation policy in License Validator. Bug fixes Updated notebooks List of recently updated or added models New rxnorm_mapper Model We are releasing rxnorm_mapper model that maps clinical entities and concepts to corresponding rxnorm codes. See Model Hub Page for details. Example : ... chunkerMapper = ChunkMapperModel.pretrained(&quot;rxnorm_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRel(&quot;rxnorm_code&quot;) ... sample_text = &quot;The patient was given Zyrtec 10 MG, Adapin 10 MG Oral Capsule, Septi-Soothe 0.5 Topical Spray&quot; Results : +++ |chunk |rxnorm_mappings| +++ |Zyrtec 10 MG |1011483 | |Adapin 10 MG Oral Capsule |1000050 | |Septi-Soothe 0.5 Topical Spray|1000046 | +++ New ChunkMapperFilterer Annotator to Filter ChunkMapperModel Results ChunkMapperFilterer annotator allows filtering of the chunks that were passed through the ChunkMapperModel. If setReturnCriteria() is set as &quot;success&quot;, only the chunks which are mapped by ChunkMapperModel are returned. Otherwise, if setReturnCriteria() is set as &quot;fail&quot;, only the chunks which are not mapped by ChunkMapperModel are returned. Example : ... cfModel = ChunkMapperFilterer() .setInputCols([&quot;ner_chunk&quot;,&quot;mappings&quot;]) .setOutputCol(&quot;chunks_filtered&quot;) .setReturnCriteria(&quot;success&quot;) #or &quot;fail&quot; ... sample_text = &quot;The patient was given Warfarina Lusa and amlodipine 10 mg. Also, he was given Aspagin, coumadin 5 mg and metformin&quot; .setReturnCriteria(&quot;success&quot;) Results : +--++--+--+ |begin|end| entity| mappings| +--++--+--+ | 22| 35| DRUG|Warfarina Lusa| +--++--+--+ .setReturnCriteria(&quot;fail&quot;) Results : +--++--++ |begin|end| entity| not mapped| +--++--++ | 41| 50| DRUG| amlodipine| | 80| 86| DRUG| Aspagin| | 89| 96| DRUG| coumadin| | 115|123| DRUG| metformin| +--++--++ New Features: Add setReplaceLabels Parameter That Allows Replacing the Non-Conventional Labels Without Using an External Source File in the NerConverterInternal(). Now you can replace the labels in NER models with custom labels by using .setReplaceLabels parameter with NerConverterInternal annotator. In this way, you will not need to use any other external source file to replace the labels with custom ones. Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;, &quot;word_embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ner_converter_original = NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;original_label&quot;) ner_converter_replaced = NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;replaced_label&quot;) .setReplaceLabels({&quot;Drug_Ingredient&quot; : &quot;Drug&quot;,&#39;Drug_BrandName&#39;:&#39;Drug&#39;}) ... sample_text = &quot;The patient was given Warfarina Lusa and amlodipine 10 mg. Also, he was given Aspagin, coumadin 5 mg, and metformin&quot; Results : +--+--+++--+ |chunk |begin|end|original_label |replaced_label| +--+--+++--+ |Warfarina Lusa|22 |35 |Drug_BrandName |Drug | |amlodipine |41 |50 |Drug_Ingredient|Drug | |10 mg |52 |56 |Strength |Strength | |he |65 |66 |Gender |Gender | |Aspagin |78 |84 |Drug_BrandName |Drug | |coumadin |87 |94 |Drug_Ingredient|Drug | |5 mg |96 |99 |Strength |Strength | |metformin |106 |114|Drug_Ingredient|Drug | +--+--+++--+ Case Sensitivity in ChunkMapperApproach and ChunkMapperModel Through setLowerCase() Parameter The case status of ChunkMapperApproach and ChunkMapperModel can be set by using setLowerCase() parameter. Example : ... chunkerMapperapproach = ChunkMapperApproach() .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setDictionary(&quot;mappings.json&quot;) .setRel(&quot;action&quot;) .setLowerCase(True) #or False ... sentences = [[&quot;&quot;&quot;The patient was given Warfarina lusa and amlodipine 10 mg, coumadin 5 mg. The patient was given Coumadin&quot;&quot;&quot;]] setLowerCase(True) Results : ++--+ |chunk |mapped | ++--+ |Warfarina lusa |540228 | |amlodipine |329526 | |coumadin |202421 | |Coumadin |202421 | ++--+ setLowerCase(False) Results : ++--+ |chunk |mapped | ++--+ |Warfarina lusa |NONE | |amlodipine |329526 | |coumadin |NONE | |Coumadin |202421 | ++--+ Return Multiple Relations At a Time In ChunkMapper Models Via setRels() Parameter Multiple relations for the same chunk can be set with the setRels() parameter in both ChunkMapperApproach and ChunkMapperModel. Example : ... chunkerMapperapproach = ChunkMapperApproach() .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setDictionary(&quot;mappings.json&quot;) .setRels([&quot;action&quot;,&quot;treatment&quot;]) .setLowerCase(True) ... sample_text = &quot;The patient was given Warfarina Lusa.&quot; Results : +--++--+-++ |begin|end| entity| mappings| relation| +--++--+-++ | 22| 35|Warfarina Lusa|Anticoagulant| action| | 22| 35|Warfarina Lusa|Heart Disease|treatment| +--++--+-++ Filter the Multi-Token Chunks Separated With Whitespace in ChunkMapperApproach and ChunkMapperModel by setAllowMultiTokenChunk() Parameter The chunks that include multi-tokens separated by a whitespace, can be filtered by using setAllowMultiTokenChunk() parameter. Example : ... chunkerMapper = ChunkMapperApproach() .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setDictionary(&quot;mappings.json&quot;) .setLowerCase(True) .setRels([&quot;action&quot;, &quot;treatment&quot;]) .setAllowMultiTokenChunk(False) ... sample_text = &quot;The patient was given Warfarina Lusa&quot; setAllowMultiTokenChunk(False) Results : +--++--+--+--+ |begin|end| chunk|mappings|relation| +--++--+--+--+ | 22| 35|Warfarina Lusa| NONE| null| +--++--+--+--+ setAllowMultiTokenChunk(True) Results : +--++--+-++ |begin|end| chunk| mappings| relation| +--++--+-++ | 22| 35|Warfarina Lusa|Anticoagulant| action| | 22| 35|Warfarina Lusa|Heart Disease|treatment| +--++--+-++ New License Validation Policies in License Validator A new version of the License Validator has been included in Spark NLP for Healthcare. This License Validator checks the compatibility between the type of your license and the environment you are using, allowing the license to be used only for the environment it was requested (single-node, cluster, databricks, etc) and the number of concurrent sessions (floating or not-floating). You can check which type of license you have in my.johnsnowlabs.com -&gt; My Subscriptions. If your license stopped working, please contact support@johnsnowlabs.com so that it can be checked the difference between the environment your license was requested for and the one it’s currently being used. Bug Fixes We fixed some issues in AnnotationToolJsonReader tool, DrugNormalizer and ContextualParserApproach annotators. DrugNormalizer : Fixed some issues that affect the performance. ContextualParserApproach : Fixed the issue in the computation of indices for documents with more than one sentence while defining the rule-scope field as a document. AnnotationToolJsonReader : Fixed an issue where relation labels were not being extracted from the Annotation Lab json file export. Updated Notebooks Clinical Named Entity Recognition Notebook .setReplaceLabels parameter example was added. Chunk Mapping Notebook New case sensitivity, selecting multiple relations, filtering multi-token chunks and ChunkMapperFilterer features were added. List of Recently Updated Models sbiobertresolve_icdo_augmented rxnorm_mapper For all Spark NLP for healthcare models, please check: Models Hub Page Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_5_3",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_3_5_3"
  },
  "227": {
    "id": "227",
    "title": "Spark NLP release notes 3.6.0",
    "content": "3.6.0 Release date: 05-08-2021 Overview Handwritten detection and visualization improvement. New Features Added ImageHandwrittenDetector for detecting ‘signature’, ‘date’, ‘name’, ‘title’, ‘address’ and others handwritten text. Added rendering labels and scores in ImageDrawRegions. Added possibility to scale image to fixed size in ImageScaler with keeping original ratio. Enhancements Support new version of pip for installing python package Added support string labels for detectors Added an auto inferencing of the input shape for detector models New license validator Bugfixes Fixed display BGR images in display functions New and updated notebooks Image Signature Detection example Image Handwritten Detection example Image Scaler example Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_6_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_6_0"
  },
  "228": {
    "id": "228",
    "title": "Spark NLP release notes 3.7.0",
    "content": "3.7.0 Release date: 30-08-2021 Overview Improve table recognition and render OCR results to the PDF with original image New Features Added ImageToTextPdf transformer for storing recognized text to the searchable PDF with original image Added PdfAssembler for assembling multipage PDF document from single page PDF documents Enhancements Added support dbfs for store models. This allow to use models on Databricks. Improved ImageTableCellDetector algorithms Added params for tuning ImageTableCellDetector algorithms Added possibility to render detected lines to the original image in ImageTableCellDetector Added support to store recognized results to CSV in ImageCellsToTextTable Added display_table and display_tables functions Added display_pdf_file function for displaying pdf in embedded pdf viewer Updated license validator New and updated notebooks Process multiple page scanned PDF (New) Image Table Detection example Image Cell Recognition example Image Table Recognition Tables Recognition from PDF Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_7_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_7_0"
  },
  "229": {
    "id": "229",
    "title": "Spark NLP release notes 3.8.0",
    "content": "3.8.0 Release date: 14-09-2021 Overview Support Microsoft PPT and PPTX documents. New Features Added PptToPdf transformer for converting PPT and PPTX slides to the PDF document. Added PptToTextTable transformer for extracting tables from PPT and PPTX slides. New and updated notebooks Convert PPT to PDF (New) Extract tables from PPT (New) Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_8_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_8_0"
  },
  "230": {
    "id": "230",
    "title": "Spark NLP release notes 3.9.0",
    "content": "3.9.0 Release date: 20-10-2021 Overview Improve visualization and support Spark NLP. New Features Added HocrTokenizer Added HocrDocumentAssembler Added ImageDrawAnnotations Added support Arabic language in ImageToText and ImageToHocr Enhancements Added postprocessing to the ImageTableDetector Added Spark NLP by default to spark session in start function Changed default value for ignoreResolution param in ImageToText Updated license-validator. Added support floating license and set AWS keys from license. Added ‘whiteList’ param to the VisualDocumentNER New and updated notebooks Spark OCR HOCR Visual Document NER Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_9_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_9_0"
  },
  "231": {
    "id": "231",
    "title": "Spark NLP release notes 3.9.1",
    "content": "3.9.1 Release date: 02-11-2021 Overview Added preserving of original file formatting Enhancements Added keepLayout param to the ImageToText New and updated notebooks Preserve Original Formatting Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_3_9_1",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_3_9_1"
  },
  "232": {
    "id": "232",
    "title": "Spark NLP for Healthcare Release Notes 4.0.0",
    "content": "4.0.0 Highlights 8 new chunk mapper models and 9 new pretrained chunk mapper pipelines to convert one medical terminology to another (Snomed to ICD10, RxNorm to UMLS etc.) 2 new medical NER models (ner_clinical_trials_abstracts and ner_pathogen) and pretrained NER pipelines 20 new biomedical NER models based on the LivingNER corpus in 8 languages (English, Spanish, French, Italian, Portuguese, Romanian, Catalan and Galician) 2 new medical NER models for Romanian language (ner_clinical, ner_clinical_bert) Deidentification support for Romanian language (ner_deid_subentity, ner_deid_subentity_bert and a pretrained deidentification pipeline) The first public health model: Emotional stress classifier (bert_sequence_classifier_stress) ResolverMerger annotator to merge the results of ChunkMapperModel and SentenceEntityResolverModel annotators New Shortest Context Match and Token Index Features in ContextualParserApproach Prettified relational categories in ZeroShotRelationExtractionModel annotator Create graphs for open source NerDLApproach with the TFGraphBuilder Spark NLP for Healthcare library installation with Poetry (dependency management and packaging tool) Bug fixes Updated notebooks List of recently updated or added models (50+ new medical models and pipelines) 8 New Chunk Mapper Models and 9 New Pretrained Chunk Mapper Pipelines to Convert One Medical Terminology to Another (Snomed to ICD10, RxNorm to UMLS etc.) We are releasing 8 new ChunkMapperModel models and 9 new pretrained pipelines for mapping clinical codes with their corresponding. Mapper Models: Mapper Name Source Target snomed_icd10cm_mapper SNOMED CT ICD-10-CM icd10cm_snomed_mapper ICD-10-CM SNOMED CT snomed_icdo_mapper SNOMED CT ICD-O icdo_snomed_mapper ICD-O SNOMED CT rxnorm_umls_mapper RxNorm UMLS icd10cm_umls_mapper ICD-10-CM UMLS mesh_umls_mapper MeSH UMLS snomed_umls_mapper SNOMED CT UMLS Example: ... snomed_resolver = SentenceEntityResolverModel.pretrained(&quot;sbertresolve_snomed_conditions&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;snomed_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) chunkerMapper = ChunkMapperModel.pretrained(&quot;snomed_icd10cm_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;snomed_code&quot;]) .setOutputCol(&quot;icd10cm_mappings&quot;) .setRels([&quot;icd10cm_code&quot;]) pipeline = PipelineModel( stages = [ documentAssembler, sbert_embedder, snomed_resolver, chunkerMapper ]) light_pipeline= LightPipeline(pipeline) result = light_pipeline.fullAnnotate(&quot;Radiating chest pain&quot;) Results : | | ner_chunk | snomed_code | icd10cm_mappings | |:|:|--:|:-| | 0 | Radiating chest pain | 10000006 | R07.9 | Pretrained Pipelines: Pipeline Name Source Target icd10cm_snomed_mapping ICD-10-CM SNOMED CT snomed_icd10cm_mapping SNOMED CT ICD-10-CM icdo_snomed_mapping ICD-O SNOMED CT snomed_icdo_mapping SNOMED CT ICD-O rxnorm_ndc_mapping RxNorm NDC icd10cm_umls_mapping ICD-10-CM UMLS mesh_umls_mapping MeSH UMLS rxnorm_umls_mapping RxNorm UMLS snomed_umls_mapping SOMED CT UMLS Example: from sparknlp.pretrained import PretrainedPipeline pipeline= PretrainedPipeline(&quot;rxnorm_umls_mapping&quot;, &quot;en&quot;, &quot;clinical/models&quot;) result= pipeline.annotate(&quot;1161611 315677&quot;) Results : {&#39;document&#39;: [&#39;1161611 315677&#39;], &#39;rxnorm_code&#39;: [&#39;1161611&#39;, &#39;315677&#39;], &#39;umls_code&#39;: [&#39;C3215948&#39;, &#39;C0984912&#39;]} 2 New Medical NER Models (ner_clinical_trials_abstracts and ner_pathogene) and Pretrained NER Pipelines ner_clinical_trials_abstracts: This model can extract concepts related to clinical trial design, diseases, drugs, population, statistics and publication. It can detect Age, AllocationRatio, Author, BioAndMedicalUnit, CTAnalysisApproach, CTDesign, Confidence, Country, DisorderOrSyndrome, DoseValue, Drug, DrugTime, Duration, Journal, NumberPatients, PMID, PValue, PercentagePatients, PublicationYear, TimePoint, Value entities. See Model Hub Page for details. Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_clinical_trials_abstracts&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) ... sample_text = &quot;A one-year, randomised, multicentre trial comparing insulin glargine with NPH insulin in combination with oral agents in patients with type 2 diabetes.&quot; bert_token_classifier_ner_clinical_trials_abstracts: This model is the BERT-based version of ner_clinical_trials_abstracts model and it can detect Age, AllocationRatio, Author, BioAndMedicalUnit, CTAnalysisApproach, CTDesign, Confidence, Country, DisorderOrSyndrome, DoseValue, Drug, DrugTime, Duration, Journal, NumberPatients, PMID, PValue, PercentagePatients, PublicationYear, TimePoint, Value entities. See Model Hub Page for details. Example : ... tokenClassifier = MedicalBertForTokenClassifier.pretrained(&quot;bert_token_classifier_ner_clinical_trials_abstracts&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;sentence&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ... sample_text = &quot;A one-year, randomised, multicentre trial comparing insulin glargine with NPH insulin in combination with oral agents in patients with type 2 diabetes.&quot; ner_clinical_trials_abstracts_pipeline: This pretrained pipeline is build upon the ner_clinical_trials_abstracts model and it can extract Age, AllocationRatio, Author, BioAndMedicalUnit, CTAnalysisApproach, CTDesign, Confidence, Country, DisorderOrSyndrome, DoseValue, Drug, DrugTime, Duration, Journal, NumberPatients, PMID, PValue, PercentagePatients, PublicationYear, TimePoint, Value entities. See Model Hub Page for details. Example : pipeline = PretrainedPipeline(&quot;ner_clinical_trials_abstracts_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) result = pipeline.fullAnnotate(&quot;A one-year, randomised, multicentre trial comparing insulin glargine with NPH insulin in combination with oral agents in patients with type 2 diabetes.&quot;) Results : +-++ | chunk| ner_label| +-++ | randomised| CTDesign| | multicentre| CTDesign| |insulin glargine| Drug| | NPH insulin| Drug| | type 2 diabetes|DisorderOrSyndrome| +-++ ner_pathogen: This model is trained for detecting medical conditions (influenza, headache, malaria, etc), medicine (aspirin, penicillin, methotrexate) and pathogenes (Corona Virus, Zika Virus, E. Coli, etc) in clinical texts. It can extract Pathogen, MedicalCondition, Medicine entities. See Model Hub Page for details. Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_pathogen&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... sample_text = &quot;Racecadotril is an antisecretory medication and it has better tolerability than loperamide. Diarrhea is the condition of having loose, liquid or watery bowel movements each day. Signs of dehydration often begin with loss of the normal stretchiness of the skin. While it has been speculated that rabies virus, Lyssavirus and Ephemerovirus could be transmitted through aerosols, studies have concluded that this is only feasible in limited conditions.&quot; ner_pathogen_pipeline: This pretrained pipeline is build upon the ner_pathogen model and it can extract Pathogen, MedicalCondition, Medicine entities. See Model Hub Page for details. Example : pipeline = PretrainedPipeline(&quot;ner_pathogen_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) result = pipeline.fullAnnotate(&quot;Racecadotril is an antisecretory medication and it has better tolerability than loperamide. Diarrhea is the condition of having loose, liquid or watery bowel movements each day. Signs of dehydration often begin with loss of the normal stretchiness of the skin. While it has been speculated that rabies virus, Lyssavirus and Ephemerovirus could be transmitted through aerosols, studies have concluded that this is only feasible in limited conditions.&quot;) Results : ++-+ |chunk |ner_label | ++-+ |Racecadotril |Medicine | |loperamide |Medicine | |Diarrhea |MedicalCondition| |dehydration |MedicalCondition| |rabies virus |Pathogen | |Lyssavirus |Pathogen | |Ephemerovirus |Pathogen | ++-+ ner_biomedical_bc2gm_pipeline : This pretrained pipeline can extract genes/proteins from medical texts by labelling them as GENE_PROTEIN. See Model Hub Page for details. Example : pipeline = PretrainedPipeline(&quot;ner_biomedical_bc2gm_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) result = pipeline.fullAnnotate(&quot;&quot;&quot;Immunohistochemical staining was positive for S-100 in all 9 cases stained, positive for HMB-45 in 9 (90%) of 10, and negative for cytokeratin in all 9 cases in which myxoid melanoma remained in the block after previous sections.&quot;&quot;&quot;) Results : +--++ |chunk |ner_label | +--++ |S-100 |GENE_PROTEIN| |HMB-45 |GENE_PROTEIN| |cytokeratin|GENE_PROTEIN| +--++ 20 New Biomedical NER Models Based on the [LivingNER corpus] in 8 Languages We are releasing 20 new NER and MedicalBertForTokenClassifier models for *English, French, Italian, Portuguese, Romanian, Catalan and Galician languages that are trained on the LivingNER multilingual corpus and for Spanish that is trained on LivingNER corpus is composed of clinical case reports extracted from miscellaneous medical specialties including COVID, oncology, infectious diseases, tropical medicine, urology, pediatrics, and others. These models can detect living species as HUMAN and SPECIES entities in clinical texts. Here is the list of model names and their embeddings used while training: Language Annotator Embeddings Model Name es MedicalBertForTokenClassification   bert_token_classifier_ner_living_species es MedicalNerModel bert_base_cased_es ner_living_species_bert es MedicalNerModel roberta_base_biomedical_es ner_living_species_roberta es MedicalNerModel embeddings_scielo_300d_es ner_living_species_300 es MedicalNerModel w2v_cc_300d_es ner_living_species en MedicalBertForTokenClassification   bert_token_classifier_ner_living_species en MedicalNerModel embeddings_clinical_en ner_living_species en MedicalNerModel biobert_pubmed_base_cased_en ner_living_species_biobert fr MedicalNerModel w2v_cc_300d_fr ner_living_species fr MedicalNerModel bert_embeddings_bert_base_fr_cased ner_living_species_bert pt MedicalBertForTokenClassification   bert_token_classifier_ner_living_species pt MedicalNerModel w2v_cc_300d_pt ner_living_species pt MedicalNerModel roberta_embeddings_BR_BERTo_pt ner_living_species_roberta pt MedicalNerModel biobert_embeddings_biomedical_pt ner_living_species_bert it MedicalBertForTokenClassification   bert_token_classifier_ner_living_species it MedicalNerModel bert_base_italian_xxl_cased_it ner_living_species_bert it MedicalNerModel w2v_cc_300d_it ner_living_species ro MedicalNerModel bert_base_cased_ro ner_living_species_bert cat MedicalNerModel w2v_cc_300d_cat ner_living_species gal MedicalNerModel w2v_cc_300d_gal ner_living_species Example : ... clinical_ner = MedicalNerModel.pretrained(&quot;ner_living_species&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner_tags&quot;) ... results = ner_model.transform(spark.createDataFrame([[&quot;&quot;&quot;Patient aged 61 years; no known drug allergies, smoker of 63 packs/year, significant active alcoholism, recently diagnosed hypertension. He came to the emergency department approximately 4 days ago with a frontal headache coinciding with a diagnosis of hypertension, for which he was started on antihypertensive treatment. The family reported that they found him &quot;slower&quot; accompanied by behavioural alterations; with no other accompanying symptoms.Physical examination: Glasgow Glasgow 15; neurological examination without focality except for bradypsychia and disorientation in time, person and space. Afebrile. BP: 159/92; heart rate 70 and O2 Sat: 93%; abdominal examination revealed hepatomegaly of two finger widths with no other noteworthy findings. CBC: Legionella antigen and pneumococcus in urine negative.&quot;&quot;&quot;]], [&quot;text&quot;])) Results : ++-+ |ner_chunk |label | ++-+ |Patient |HUMAN | |family |HUMAN | |person |HUMAN | |Legionella |SPECIES| |pneumococcus|SPECIES| ++-+ 2 New Medical NER Models for Romanian Language We trained ner_clinical and ner_clinical_bert models that can detect Measurements, Form, Symptom, Route, Procedure, Disease_Syndrome_Disorder, Score, Drug_Ingredient, Pulse, Frequency, Date, Body_Part, Drug_Brand_Name, Time, Direction, Dosage, Medical_Device, Imaging_Technique, Test, Imaging_Findings, Imaging_Test, Test_Result, Weight, Clinical_Dept and Units entities in Romanian clinical texts. ner_clinical: This model is trained with w2v_cc_300d embeddings model. Example : ... embeddings = WordEmbeddingsModel.pretrained(&quot;w2v_cc_300d&quot;,&quot;ro&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_clinical&quot;, &quot;ro&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;word_embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... sample_text = &quot;Aorta ascendenta inlocuita cu proteza de Dacron de la nivelul anulusului pana pe segmentul ascendent distal pe o lungime aproximativa de 75 mm.&quot; ner_clinical_bert: This model is trained with bert_base_cased embeddings model. Example : ... embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;ro&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_clinical_bert&quot;, &quot;ro&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;word_embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... sample_text = &quot;Aorta ascendenta inlocuita cu proteza de Dacron de la nivelul anulusului pana pe segmentul ascendent distal pe o lungime aproximativa de 75 mm.&quot; Results : +-+--+ | chunks| entities| +-+--+ | Aorta ascendenta| Body_Part| | proteza de Dacron|Medical_Device| | anulusului| Body_Part| |segmentul ascendent| Body_Part| | distal| Direction| | 75| Measurements| | mm| Units| +-+--+ Deidentification Support for Romanian Language (ner_deid_subentity, ner_deid_subentity_bert and a Pretrained Deidentification Pipeline) We trained two new NER models to find PHI data (protected health information) that may need to be deidentified in Romanian. ner_deid_subentity and ner_deid_subentity_bert models are trained with in-house annotations and can detect 17 different entities (AGE, CITY, COUNTRY, DATE, DOCTOR, EMAIL, FAX, HOSPITAL, IDNUM, LOCATION-OTHER, MEDICALRECORD, ORGANIZATION, PATIENT, PHONE, PROFESSION, STREET, ZIP). ner_deid_subentity: This model is trained with w2v_cc_300d embeddings model. See Model Hub Page for details. Example : ... embeddings = WordEmbeddingsModel.pretrained(&quot;w2v_cc_300d&quot;,&quot;ro&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity&quot;, &quot;ro&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;word_embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... sample_text = &quot;&quot;&quot; Spitalul Pentru Ochi de Deal, Drumul Oprea Nr. 972 Vaslui, 737405 România Tel: +40(235)413773 Data setului de analize: 25 May 2022 15:36:00 Nume si Prenume : BUREAN MARIA, Varsta: 77 Medic : Agota Evelyn Tımar C.N.P : 2450502264401&quot;&quot;&quot; ner_deid_subentity_bert: This model is trained with bert_base_cased embeddings model. See Model Hub Page for details. Example : ... embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;ro&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;word_embeddings&quot;) clinical_ner = MedicalNerModel.pretrained(&quot;ner_deid_subentity_bert&quot;, &quot;ro&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;word_embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... text = &quot;&quot;&quot; Spitalul Pentru Ochi de Deal, Drumul Oprea Nr. 972 Vaslui, 737405 România Tel: +40(235)413773 Data setului de analize: 25 May 2022 15:36:00 Nume si Prenume : BUREAN MARIA, Varsta: 77 Medic : Agota Evelyn Tımar C.N.P : 2450502264401&quot;&quot;&quot; Results : +-++ |chunk |ner_label| +-++ |Spitalul Pentru Ochi de Deal|HOSPITAL | |Drumul Oprea Nr |STREET | |Vaslui |CITY | |737405 |ZIP | |+40(235)413773 |PHONE | |25 May 2022 |DATE | |BUREAN MARIA |PATIENT | |77 |AGE | |Agota Evelyn Tımar |DOCTOR | |2450502264401 |IDNUM | +-++ clinical_deidentification: This pretrained pipeline that can be used to deidentify PHI information from Romanian medical texts. The PHI information will be masked and obfuscated in the resulting text. The pipeline can mask and obfuscate ACCOUNT, PLATE, LICENSE, AGE, CITY, COUNTRY, DATE, DOCTOR, EMAIL, FAX, HOSPITAL, IDNUM, LOCATION-OTHER, MEDICALRECORD, ORGANIZATION, PATIENT, PHONE, PROFESSION, STREET, ZIP entities. See Model Hub Page for details. Example : from sparknlp.pretrained import PretrainedPipeline deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;, &quot;ro&quot;, &quot;clinical/models&quot;) text = &quot;Varsta : 77, Nume si Prenume : BUREAN MARIA, Data setului de analize: 25 May 2022, Licență : B004256985M, Înmatriculare : CD205113, Cont : FXHZ7170951927104999&quot; result = deid_pipeline.annotate(text) print(&quot; nMasked with entity labels&quot;) print(&quot;-&quot;*30) print(&quot; n&quot;.join(result[&#39;masked&#39;])) print(&quot; nMasked with chars&quot;) print(&quot;-&quot;*30) print(&quot; n&quot;.join(result[&#39;masked_with_chars&#39;])) print(&quot; nMasked with fixed length chars&quot;) print(&quot;-&quot;*30) print(&quot; n&quot;.join(result[&#39;masked_fixed_length_chars&#39;])) print(&quot; nObfuscated&quot;) print(&quot;-&quot;*30) print(&quot; n&quot;.join(result[&#39;obfuscated&#39;])) Results : Masked with entity labels Varsta : &lt;AGE&gt;, Nume si Prenume : &lt;PATIENT&gt;, Data setului de analize: &lt;DATE&gt;, Licență : &lt;LICENSE&gt;, Înmatriculare : &lt;PLATE&gt;, Cont : &lt;ACCOUNT&gt; Masked with chars Varsta : **, Nume si Prenume : [**********], Data setului de analize: [*********], Licență : [*********], Înmatriculare : [******], Cont : [******************] Masked with fixed length chars Varsta : ****, Nume si Prenume : ****, Data setului de analize: ****, Licență : ****, Înmatriculare : ****, Cont : **** Obfuscated Varsta : 91, Nume si Prenume : Dragomir Emilia, Data setului de analize: 01-04-2001, Licență : T003485962M, Înmatriculare : AR-65-UPQ, Cont : KHHO5029180812813651 The First Public Health Model: Emotional Stress Classifier We are releasing a new bert_sequence_classifier_stress model that can classify whether the content of a text expresses emotional stress. It is a PHS-BERT-based model and trained with the Dreaddit dataset. Example : ... sequenceClassifier = MedicalBertForSequenceClassification.pretrained(&quot;bert_sequence_classifier_stress&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;,&quot;token&quot;]) .setOutputCol(&quot;class&quot;) sample_text = &quot;No place in my city has shelter space for us, and I won&#39;t put my baby on the literal street. What cities have good shelter programs for homeless mothers and children?&quot; Results : +-+--+ |text | class| +-+--+ |No place in my city has shelter space for us, and I won&#39;t put my baby on the literal street. What cities have good shelter programs for homeless mothers and children?|[stress]| +-+--+ ResolverMerger Annotator to Merge the Results of ChunkMapperModel and SentenceEntityResolverModel Annotators ResolverMerger annotator allows to merge the results of ChunkMapperModel and SentenceEntityResolverModel annotators. You can detect your results that fail by ChunkMapperModel with ChunkMapperFilterer and then merge your resolver and mapper results with ResolverMerger. Example : ... chunkerMapper = ChunkMapperModel.pretrained(&quot;rxnorm_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;chunk&quot;]) .setOutputCol(&quot;RxNorm_Mapper&quot;) .setRel(&quot;rxnorm_code&quot;) cfModel = ChunkMapperFilterer() .setInputCols([&quot;chunk&quot;, &quot;RxNorm_Mapper&quot;]) .setOutputCol(&quot;chunks_fail&quot;) .setReturnCriteria(&quot;fail&quot;) ... resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_rxnorm_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;chunks_fail&quot;, &quot;sentence_embeddings&quot;]) .setOutputCol(&quot;resolver_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) resolverMerger = ResolverMerger() .setInputCols([&quot;resolver_code&quot;,&quot;RxNorm_Mapper&quot;]) .setOutputCol(&quot;RxNorm&quot;) ... Results : +--+--++-+-+ |chunk |RxNorm_Mapper |chunks_fail |resolver_code|RxNorm | +--+--++-+-+ |[Adapin 10 MG, coumadin 5 mg] |[1000049, NONE] |[coumadin 5 mg]|[855333] |[1000049, 855333] | |[Avandia 4 mg, Tegretol, zytiga]|[NONE, 203029, 1100076]|[Avandia 4 mg] |[261242] |[261242, 203029, 1100076]| +--+--++-+-+ New Shortest Context Match and Token Index Features in ContextualParserApproach We have new functionalities in ContextualParserApproach to make it more performant. setShortestContextMatch() parameter will allow stop looking for matches in the text when a token defined as a suffix is found. Also it will keep tracking of the last mathced prefix and subsequent mathches with suffix. Now the index of the matched token can be found in metadata. Example : ... contextual_parser = ContextualParserApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;entity&quot;) .setJsonPath(&quot;cities.json&quot;) .setCaseSensitive(True) .setDictionary(&#39;cities.tsv&#39;, options={&quot;orientation&quot;:&quot;vertical&quot;}) .setShortestContextMatch(True) ... sample_text = &quot;Peter Parker is a nice guy and lives in Chicago.&quot; Results : +-++-+ |chunk |ner_label|tokenIndex| +-++-+ |Chicago|City |9 | +-++-+ Prettified relational categories in ZeroShotRelationExtractionModel annotator Now you can setRelationalCategories() between the entity labels by using a single {} instead of two. Example : re_model = ZeroShotRelationExtractionModel.pretrained(&quot;re_zeroshot_biobert&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;re_ner_chunks&quot;, &quot;sentences&quot;]) .setOutputCol(&quot;relations&quot;) .setRelationalCategories({&quot;ADE&quot;: [&quot;{DRUG} causes {PROBLEM}.&quot;]}) Create Graphs for Open Source NerDLApproach with the TFGraphBuilder Now you can create graphs for model training with NerDLApproach by using the new setIsMedical() parameter of TFGraphBuilder annotator. If setIsMedical(True), the model can be trained with MedicalNerApproach, but if it is setIsMedical(False) it can be used with NerDLApproach for training non-medical models. graph_folder_path = &quot;./graphs&quot; ner_graph_builder = TFGraphBuilder() .setModelName(&quot;ner_dl&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setGraphFile(&quot;auto&quot;) .setHiddenUnitsNumber(20) .setGraphFolder(graph_folder_path) .setIsMedical(False) ner = NerDLApproach() ... .setGraphFolder(graph_folder_path) ner_pipeline = Pipeline()([ ..., ner_graph_builder, ner ]) Spark NLP for Healthcare Library Installation with Poetry Documentation (dependency management and packaging tool). We have a new documentation page for showing Spark NLP for Healthcare installation with Poetry. You can find it here. Bug fixes ContextualParserApproach: Fixed the bug using a dictionary and document rule scope in JSON config file. RENerChunksFilter: Preparing a pretrained pipeline with RENerChunksFilter annotator issue is fixed. Updated Notebooks ZeroShot Clinical Relation Extraction Notebook: Added new features, visualization and new examples. Clinical_Entity_Resolvers Notebook: Added an example of ResolverMerger. Chunk Mapping Notebook: Added new models into the model list and an example of mapper pretrained pipelines. Healthcare Code Mapping Notebook: Added all mapper pretrained pipeline examples. List of Recently Updated and Added Models ner_pathogene ner_pathogen_pipeline ner_clinical_trials_abstracts bert_token_classifier_ner_clinical_trials_abstracts ner_clinical_trials_abstracts_pipeline ner_biomedical_bc2gm_pipeline bert_sequence_classifier_stress icd10cm_snomed_mapper snomed_icd10cm_mapper snomed_icdo_mapper icdo_snomed_mapper rxnorm_umls_mapper icd10cm_umls_mapper mesh_umls_mapper snomed_umls_mapper icd10cm_snomed_mapping snomed_icd10cm_mapping icdo_snomed_mapping snomed_icdo_mapping rxnorm_ndc_mapping icd10cm_umls_mapping mesh_umls_mapping rxnorm_umls_mapping snomed_umls_mapping drug_action_tretment_mapper normalized_section_header_mapper drug_brandname_ndc_mapper abbreviation_mapper rxnorm_ndc_mapper rxnorm_action_treatment_mapper rxnorm_mapper ner_deid_subentity -&gt; ro ner_deid_subentity_bert -&gt; ro clinical_deidentification -&gt; ro ner_clinical -&gt; ro ner_clinical_bert -&gt; ro bert_token_classifier_ner_living_species -&gt; es ner_living_species_bert -&gt; es ner_living_species_roberta -&gt; es ner_living_species_300 -&gt; es ner_living_species -&gt; es bert_token_classifier_ner_living_species -&gt; en ner_living_species -&gt; en ner_living_species_biobert -&gt; en ner_living_species -&gt; fr ner_living_species_bert -&gt; fr bert_token_classifier_ner_living_species -&gt; pt ner_living_species -&gt; pt ner_living_species_roberta -&gt; pt ner_living_species_bert -&gt; pt bert_token_classifier_ner_living_species -&gt; it ner_living_species_bert -&gt; it ner_living_species -&gt; pt ner_living_species_bert -&gt; ro ner_living_species -&gt; ro ner_living_species -&gt; gal For all Spark NLP for healthcare models, please check: Models Hub Page Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_0_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_0_0"
  },
  "233": {
    "id": "233",
    "title": "Spark NLP release notes 4.0.0",
    "content": "4.0.0 Release date: 16-07-2022 Overview We are very glad to announce that Spark OCR 4.0.0 has been released! This release comes with new models, new functionality, bug fixes, and compatibility with 4.0.0 versions of Spark NLP and Spark NLP for Healthcare. New Features New DicomMetadataDeidentifier class to help deidentifying metadata of dicom files. Example Notebook. New helper function display_dicom() to help displaying DICOM files in notebooks. New DicomDrawRegions that can clean burned pixels for removing PHI. Improved support for DICOM files containing 12bit images. Bug Fixes Fixes on the Visual NER Finetuning process including VisualDocumentNERv2 and AlabReader. Improved exception handling for VisualDocumentClassifier models. New Models New LayoutLMv3 based Visual Document NER: layoutlmv3_finetuned_funsd. Improved handwritten detection ocr_base_handwritten_v2. VisualDocumentClassifierV2: layoutlmv2_rvl_cdip_40k. This model adds more data compared to layoutlmv2_rvl_cdip_1500, and achieves an accuracy of 88%. Compatibility Updates Deprecated Spark 2.3 and Spark 2.4 support. Tested compatibility with Spark-NLP and Spark NLP for Healthcare 4.0.0. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_4_0_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_4_0_0"
  },
  "234": {
    "id": "234",
    "title": "Spark NLP for Healthcare Release Notes 4.0.2",
    "content": "4.0.2 Highlights 16 new text classification models for English and Spanish social media text related to public health topics (stress, domestic violence, vaccine status, drug reviews etc.) Pretrained medication NER pipeline to augment posology NER models with Drugbank dataset Pretrained medication resolver pipeline to extract RxNorm, UMLS, NDC, SNOMED CT codes and action/treatments. New disease NER model for Spanish language 5 new chunk mapper models to convert clinical entities to relevant medical terminology (UMLS) 5 new pretrained resolver pipelines to convert clinical entities to relevant medical terminology (UMLS) New Relation Extraction model to detect Drug and ADE relations New module for converting Annotation Lab (ALAB) exports into formats suitable for training new models Updated De-identification pretrained pipelines New setBlackList() parameter in ChunkFilterer() annotator New Doc2ChunkInternal() annotator Listing clinical pretrained models and pipelines with one-liner Bug fixes New and updated notebooks List of recently updated or added models (40+ new models and pipelines) 16 New Classification Models for English and Spanish Social Media Texts Related to Public Health Topics (Stress, Domestic Violence, Vaccine Status, Drug Reviews etc.) We are releasing 11 new MedicalBertForSequenceClassification models to classify text from social media data for English and Spanish languages. model name description predicted entities bert_sequence_classifier_ade_augmented this model classify tweets reporting ADEs (Adverse Drug Events). ADE noADE bert_sequence_classifier_health_mandates_stance_tweet this model classifies stance in tweets about health mandates. FAVOR AGAINST NONE bert_sequence_classifier_health_mandates_premise_tweet this model classifies premise in tweets about health mandates. has_premse has_no_premse bert_sequence_classifier_treatement_changes_sentiment_tweet this model classifies treatment changes reviews in tweets as negative and positive. positive negative bert_sequence_classifier_drug_reviews_webmd this model classifies drug reviews from WebMD as negative and positive. positive negative bert_sequence_classifier_self_reported_age_tweet this model classifies if there is a self-reported age in social media data. self_report_age no_report bert_sequence_classifier_self_reported_symptoms_tweet this model classifies self-reported COVID-19 symptoms in Spanish language tweets. Lit-News_mentions Self_reports non_personal_reports bert_sequence_classifier_self_reported_vaccine_status_tweet this model classifies self-reported COVID-19 vaccination status in tweets. Vaccine_chatter Self_reports bert_sequence_classifier_self_reported_partner_violence_tweet this model classifies self-reported Intimate partner violence (IPV) in tweets. intimate_partner_violence non_intimate_partner_violence bert_sequence_classifier_exact_age_reddit this model classifies if there is a self-reported age in social media forum posts (Reddit). self_report_age no_report bert_sequence_classifier_self_reported_stress_tweet this model classifies stress in social media (Twitter) posts in the self-disclosure category. stressed not-stressed Example : ... sequenceClassifier = MedicalBertForSequenceClassification.pretrained(&quot;bert_sequence_classifier_exact_age_reddit&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;class&quot;) ... sample_text = [&quot;Is it bad for a 19 year old it&#39;s been getting worser.&quot;, &quot;I was about 10. So not quite as young as you but young.&quot;] Results : +-+--+ |text |class | +-+--+ |Is it bad for a 19 year old its been getting worser. |[self_report_age]| |I was about 10. So not quite as young as you but young.|[no_report] | +-+--+ We are releasing 5 new public health classification models. model name description predicted entities bert_sequence_classifier_health_mentions This model can classify public health mentions in social media text figurative_mention other_mention health_mention classifierdl_health_mentions This model can classify public health mentions in social media text figurative_mention other_mention health_mention bert_sequence_classifier_vaccine_sentiment This model can extract information from COVID-19 Vaccine-related tweets neutral positive negative classifierdl_vaccine_sentiment This model can extract information from COVID-19 Vaccine-related tweets neutral positive negative bert_sequence_classifier_stressor This model can classify source of emotional stress in text. Family_Issues Financial_Problem Health_Fatigue_or_Physical Pain Other School Work Social_Relationships Example : ... sequenceClassifier = MedicalBertForSequenceClassification.pretrained(&quot;bert_sequence_classifier_health_mentions&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;,&quot;token&quot;]) .setOutputCol(&quot;class&quot;) ... sample_text =[&quot;Another uncle of mine had a heart attack and passed away. Will be cremated Saturday I think I ve gone numb again RIP Uncle Mike&quot;, &quot;I don&#39;t wanna fall in love. If I ever did that, I think I&#39;d have a heart attack&quot;, &quot;Aluminum is a light metal that causes dementia and Alzheimer&#39;s disease. You should never put aluminum into your body (including deodorants).&quot;] Results : +--+--+ |text |result | +--+--+ |Another uncle of mine had a heart attack and passed away. Will be cremated Saturday I think I ve gone numb again RIP Uncle Mike |[health_mention] | |I don&#39;t wanna fall in love. If I ever did that, I think I&#39;d have a heart attack |[figurative_mention]| |Aluminum is a light metal that causes dementia and Alzheimer&#39;s disease. You should never put aluminum into your body (including deodorants).|[other_mention] | +--+--+ Pretrained Medication NER Pipeline to Augmented Posology NER Models with Drugbank Dataset We are releasing a medication NER pretrained pipeline to extract medications in clinical text. It’s an augmented version of posology NER model with Drugbank datasets and can retun all the medications with a single line of code without building a pipeline with models. ner_medication_pipeline: This pretrained pipeline can detect medication entities and label them as DRUG in clinical text. See Models Hub Page for more details. Example : from sparknlp.pretrained import PretrainedPipeline medication_pipeline = PretrainedPipeline(&quot;ner_medication_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) text = &quot;&quot;&quot;The patient was prescribed metformin 1000 MG, and glipizide 2.5 MG. The other patient was given Fragmin 5000 units, Xenaderm to wounds topically b.i.d. and OxyContin 30 mg.&quot;&quot;&quot; Results : |--|--| | chunk | ner_label | |--|--| | metformin 1000 MG | DRUG | | glipizide 2.5 MG | DRUG | | Fragmin 5000 units | DRUG | | Xenaderm | DRUG | | OxyContin 30 mg | DRUG | |--|--| Pretrained Medication Resolver Pipeline to Extract RxNorm, UMLS, NDC , SNOMED CT Codes and Action/Treatments We are releasing a medication resolver pipeline to extract medications and and resolve RxNorm, UMLS, NDC, SNOMED CT codes and action/treatments in clinical text. You can get those codes if available with a single line of code without building a pipeline with models. medication_resolver_pipeline: This pretrained pipeline can detect medication entities and resolve codes if available. Example : from sparknlp.pretrained import PretrainedPipeline medication_pipeline = PretrainedPipeline(&quot;medication_resolver_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) text = &quot;&quot;&quot;The patient was prescribed Mycobutn 150 MG, Salagen 5 MG oral tablet, The other patient is given Lescol 40 MG and Lidoderm 0.05 MG/MG, triazolam 0.125 MG Oral Tablet, metformin hydrochloride 1000 MG Oral Tablet&quot;&quot;&quot; Results : ||-||--|-|-|||-| | ner_chunk | RxNorm_Chunk | Action | Treatment | UMLS | SNOMED_CT | NDC_Product | NDC_Package | entity | ||-||--|-|-|||-| | Mycobutn 150 MG | 103899 | Antimiycobacterials | Infection | C0353536 | - | 00013-5301 | 00013-5301-17 | DRUG | | Salagen 5 MG oral tablet | 1000915 | Antiglaucomatous | Cancer | C0361693 | - | 59212-0705 | 59212-0705-10 | DRUG | | Lescol 40 MG | 103919 | Hypocholesterolemic | Heterozygous Familial Hypercholesterolemia | C0353573 | - | 00078-0234 | 00078-0234-05 | DRUG | | Lidoderm 0.05 MG/MG | 1011705 | Anesthetic | Pain | C0875706 | - | 00247-2129 | 00247-2129-30 | DRUG | | triazolam 0.125 MG Oral Tablet | 198317 | - | - | C0690642 | 373981005 | 00054-4858 | 00054-4858-25 | DRUG | | metformin hydrochloride 1000 MG Oral Tablet | 861004 | - | - | C0978482 | 376701008 | 00093-7214 | 00185-0221-01 | DRUG | ||-||--|-|-|||-| New Disease NER Model for Spanish Language We are releasing a new MedicalBertForTokenClassifier model to extract disease entities from social media text in Spanish. bert_token_classifier_disease_mentions_tweet: This model can extract disease entities in Spanish tweets and label them as ENFERMEDAD (disease). See Models Hub Page for more details. Example : ... tokenClassifier = MedicalBertForTokenClassifier.pretrained(&quot;bert_token_classifier_disease_mentions_tweet&quot;, &quot;es&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;sentence&quot;) .setOutputCol(&quot;label&quot;) .setCaseSensitive(True) ... example_text = &quot;&quot;&quot;El diagnóstico fueron varios. Principal: Neumonía en el pulmón derecho. Sinusitis de caballo, Faringitis aguda e infección de orina, también elevada. Gripe No. Estuvo hablando conmigo, sin exagerar, mas de media hora, dándome ánimo y fuerza y que sabe, porque ha visto&quot;&quot;&quot; Results : ++-+ |chunk |ner_label | ++-+ |Neumonía en el pulmón|ENFERMEDAD| |Sinusitis |ENFERMEDAD| |Faringitis aguda |ENFERMEDAD| |infección de orina |ENFERMEDAD| |Gripe |ENFERMEDAD| ++-+ 5 new Chunk Mapper Models to Convert Clinical Entities to Relevant Medical Terminology (UMLS) We are releasing 5 new ChunkMapperModel models to map clinical entities with their corresponding UMLS CUI codes. Mapper Name Source Target umls_clinical_drugs_mapper Drugs UMLS CUI umls_clinical_findings_mapper Clinical Findings UMLS CUI umls_disease_syndrome_mapper Disease and Syndromes UMLS CUI umls_major_concepts_mapper Clinical Major Concepts UMLS CUI umls_drug_substance_mapper Drug Substances UMLS CUI Example : ... ner_model = MedicalNerModel.pretrained(&quot;ner_posology_greedy&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;clinical_ner&quot;) ner_model_converter = NerConverterInternal() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;clinical_ner&quot;) .setOutputCol(&quot;ner_chunk&quot;) chunkerMapper = ChunkMapperModel.pretrained(&quot;umls_drug_substance_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRels([&quot;umls_code&quot;]) .setLowerCase(True) ... example_text = &quot;&quot;&quot;The patient was given metformin, lenvatinib and lavender 700 ml/ml&quot;&quot;&quot; Results : ++++ | ner_chunk|ner_label|umls_code| ++++ | metformin| DRUG| C0025598| | lenvatinib| DRUG| C2986924| |lavender 700 ml/ml| DRUG| C0772360| ++++ 5 new Pretrained Resolver Pipelines to Convert Clinical Entities to Relevant Medical Terminology (UMLS) We now have 5 new resolver PretrainedPipeline to convert clinical entities to their UMLS CUI codes. You just need to feed your text and it will return the corresponding UMLS codes. Pipeline Name Entity Target umls_drug_resolver_pipeline Drugs UMLS CUI umls_clinical_findings_resolver_pipeline Clinical Findings UMLS CUI umls_disease_syndrome_resolver_pipeline Disease and Syndromes UMLS CUI umls_major_concepts_resolver_pipeline Clinical Major Concepts UMLS CUI umls_drug_substance_resolver_pipeline Drug Substances UMLS CUI Example : from sparknlp.pretrained import PretrainedPipeline pipeline= PretrainedPipeline(&quot;umls_clinical_findings_resolver_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) sample_text = &quot;HTG-induced pancreatitis associated with an acute hepatitis, and obesity&quot; Results : +-+++ |chunk |ner_label|umls_code| +-+++ |HTG-induced pancreatitis |PROBLEM |C1963198 | |an acute hepatitis |PROBLEM |C4750596 | |obesity |PROBLEM |C1963185 | +-+++ New Relation Extraction Model to Detect Drug and ADE relations We are releasing new re_ade_conversational model that can extract relations between DRUG and ADE entities from conversational texts and tag the relations as is_related and not_related. See Models Hub Page for more details. Example : ... re_model = RelationExtractionModel().pretrained(&quot;re_ade_conversational&quot;, &quot;en&quot;, &#39;clinical/models&#39;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setRelationPairs([&quot;ade-drug&quot;, &quot;drug-ade&quot;]) ... sample_text = &quot;E19.32 day 20 rivaroxaban diary. still residual aches and pains; only had 4 paracetamol today.&quot; Results : |--|-|-||-| | chunk1 | entitiy1 | chunk2 | entity2 | relation | |--|-|-||-| | residual aches and pains | ADE | rivaroxaban | DRUG | is_related | | residual aches and pains | ADE | paracetamol | DRUG | not_related | |--|-|-||-| New Module for Converting Annotation Lab (ALAB) Exports Into Suitable Formats for Training New Models We have a new sparknlp_jsl.alab module with functions for converting ALAB JSON exports into suitable formats for training NER, Assertion and Relation Extraction models. Example : from sparknlp_jsl.alab import get_conll_data, get_assertion_data, get_relation_extraction_data get_conll_data(spark=spark, input_json_path=&quot;alab_demo.json&quot;, output_name=&quot;conll_demo&quot;) assertion_df = get_assertion_data(spark=spark, input_json_path = &#39;alab_demo.json&#39;, assertion_labels = [&#39;ABSENT&#39;], relevant_ner_labels = [&#39;PROBLEM&#39;, &#39;TREATMENT&#39;]) relation_df = get_relation_extraction_data(spark=spark, input_json_path=&#39;alab_demo.json&#39;) These functions contain over 10 arguments each which give you all the flexibility you need to convert your annotations to trainable formats. These include parameters controlling tokenization, ground truth selections, negative annotations, negative annotation weights, task exclusions, and many more. To find out how to make best use of these functions, head over to this repository. Updated De-identification Pretrained Pipelines We have updated de-identification pretrained pipelines to provide better performance than ever before. This includes an update to the clinical_deidentification pretrained pipeline and a new light-weight version clinical_deidentification_slim. Example : from sparknlp.pretrained import PretrainedPipeline deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification&quot;, &quot;en&quot;, &quot;clinical/models&quot;) slim_deid_pipeline = PretrainedPipeline(&quot;clinical_deidentification_slim&quot;, &quot;en&quot;, &quot;clinical/models&quot;) sample_text = &quot;Name : Hendrickson, Ora, Record date: 2093-01-13, # 719435&quot; Results : Name : &lt;PATIENT&gt;, Record date: &lt;DATE&gt;, &lt;MEDICALRECORD&gt; Name : [**************], Record date: [********], [****] Name : ****, Record date: ****, **** Name : Alexia Mcgill, Record date: 2093-02-19, Y138038 New setBlackList() Parameter in ChunkFilterer() Annotator We are releasing a new setBlackList() parameter in the ChunkFilterer() annotator. ChunkFilterer() lets through every chunk except those that match the list of phrases or regex rules in the setBlackList() parameter. Example : ... chunk_filterer = ChunkFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;) .setOutputCol(&quot;chunk_filtered&quot;) .setCriteria(&quot;isin&quot;) .setBlackList([&#39;severe fever&#39;, &#39;severe cough&#39;]) ... example_text= &quot;&quot;&quot;Patient with severe fever, severe cough, sore throat, stomach pain, and a headache.&quot;&quot;&quot; Results : +-++ |ner_chunk |chunk_filtered | +-++ |[severe fever, severe cough, sore throat, stomach pain, a headache]|[sore throat, stomach pain, a headache]| +-++ New Doc2ChunkInternal() Annotator We are releasing a Doc2ChunkInternal() annotator. This is a licensed version of the open source Doc2Chunk() annotator. You can now customize the tokenization step within Doc2Chunk(). This will be quite handy when it comes to training custom assertion models. Example : ... doc2ChunkInternal = Doc2ChunkInternal() .setInputCols(&quot;document&quot;,&quot;token&quot;) .setStartCol(&quot;start&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;doc2chunkInternal&quot;) ... df= spark.createDataFrame([ [&quot;The mass measures 4 x 3.5cm in size more.&quot;,8,&quot;size&quot;], [&quot;The mass measures 4 x 3.5cm in size more.&quot;,9,&quot;size&quot;]]).toDF(&quot;sentence&quot;,&quot;start&quot;, &quot;target&quot;) Results : +--+--++--+--+ | sentence|start|target| doc2chunkInternal| doc2chunk| +--+--++--+--+ |The mass measures 4 x 3.5cm in size more.| 8| size|[{chunk, 31, 34, size, {sentence -&gt; 0, chunk -&gt; 0}, []}]|[{chunk, 31, 34, size, {sentence -&gt; 0, chunk -&gt; 0}, []}] | |The mass measures 4 x 3.5cm in size more.| 9| size| []|[{chunk, 31, 34, size, {sentence -&gt; 0, chunk -&gt; 0}, []}] | +--+--++--+--+ Listing Pretrained Clinical Models and Pipelines with One-Liner We have new returnPrivatePipelines() and returnPrivateModels() features under InternalResourceDownloader package to return licensed models and pretrained pipelines as a list. Example : from sparknlp_jsl.pretrained import InternalResourceDownloader # pipelines = InternalResourceDownloader.returnPrivatePipelines() assertion_models = InternalResourceDownloader.returnPrivateModels(&quot;AssertionDLModel&quot;) Results : [[&#39;assertion_ml&#39;, &#39;en&#39;, &#39;2.0.2&#39;], [&#39;assertion_dl&#39;, &#39;en&#39;, &#39;2.0.2&#39;], [&#39;assertion_dl_healthcare&#39;, &#39;en&#39;, &#39;2.7.2&#39;], [&#39;assertion_dl_biobert&#39;, &#39;en&#39;, &#39;2.7.2&#39;], [&#39;assertion_dl&#39;, &#39;en&#39;, &#39;2.7.2&#39;], [&#39;assertion_dl_radiology&#39;, &#39;en&#39;, &#39;2.7.4&#39;], [&#39;assertion_jsl_large&#39;, &#39;en&#39;, &#39;3.1.2&#39;], [&#39;assertion_jsl&#39;, &#39;en&#39;, &#39;3.1.2&#39;], [&#39;assertion_dl_scope_L10R10&#39;, &#39;en&#39;, &#39;3.4.2&#39;], [&#39;assertion_dl_biobert_scope_L10R10&#39;, &#39;en&#39;, &#39;3.4.2&#39;], [&#39;assertion_oncology_treatment_binary_wip&#39;, &#39;en&#39;, &#39;3.5.0&#39;]] Bug Fixes ZeroShotRelationExtractionModel: Fixed the issue that blocks the use of this annotator. AnnotationToolJsonReader: Fixed the issue with custom pipeline usage in this annotator. RelationExtractionApproach: Fixed issues related to training logs and inference. New and Updated Notebooks Clinical Named Entity Recognition Notebook: Added new getPrivateModel() feature Clinical Entity Resolvers Notebook: Added an example of reseolver pretrained pipelines Pretrained Clinical Pipelines Notebook: Pipeline list updated and examples of resolver pretrained pipelines were added Chunk Mapping Notebook: New mapper models added into model list All certification notebooks updated with v4.0.0. List of Recently Updated and Added Models and Pretrained Pipelines bert_token_classifier_ner_anatem bert_token_classifier_ner_bc2gm_gene bert_token_classifier_ner_bc4chemd_chemicals bert_token_classifier_ner_bc5cdr_chemicals bert_token_classifier_ner_bc5cdr_disease bert_token_classifier_ner_jnlpba_cellular bert_token_classifier_ner_linnaeus_species bert_token_classifier_ner_ncbi_disease bert_token_classifier_ner_species bert_sequence_classifier_ade_augmented bert_sequence_classifier_health_mandates_stance_tweet bert_sequence_classifier_health_mandates_premise_tweet bert_sequence_classifier_treatement_changes_sentiment_tweet bert_sequence_classifier_drug_reviews_webmd bert_sequence_classifier_self_reported_age_tweet bert_sequence_classifier_self_reported_symptoms_tweet =&gt; es bert_sequence_classifier_self_reported_vaccine_status_tweet bert_sequence_classifier_self_reported_partner_violence_tweet bert_sequence_classifier_exact_age_reddit bert_sequence_classifier_self_reported_stress_tweet bert_token_classifier_disease_mentions_tweet =&gt; es bert_token_classifier_ner_ade_tweet_binary bert_token_classifier_ner_pathogen clinical_deidentification clinical_deidentification_slim umls_clinical_drugs_mapper umls_clinical_findings_mapper umls_disease_syndrome_mapper umls_major_concepts_mapper umls_drug_substance_mapper umls_drug_resolver_pipeline umls_clinical_findings_resolver_pipeline umls_disease_syndrome_resolver_pipeline umls_major_concepts_resolver_pipeline umls_drug_substance_resolver_pipeline classifierdl_health_mentions bert_sequence_classifier_health_mentions ner_medication_pipeline bert_sequence_classifier_vaccine_sentiment classifierdl_vaccine_sentiment bert_sequence_classifier_stressor re_ade_conversational medication_resolver_pipeline Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_0_2",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_0_2"
  },
  "235": {
    "id": "235",
    "title": "Spark NLP release notes 4.0.2",
    "content": "4.0.2 Release date: 12-09-2022 Overview We are glad to announce that Spark OCR 4.0.2 has been released! This release comes with new features, fixes and more!. New Features VisualDocumentClassifierV2 is now trainable! Continuing with the effort to make all the most useful models easily trainable, we added training capabilities to this annotator. Added support for Simplified Chinese. Added new ‘PdfToForm’ annotator, capable of extracting forms from digital PDFs. This is different from previously introduced VisualDocumentNER annotator in that this new annotator works only on digital documents, as opposite to the scanned forms handled by VisualDocumentNER. PdfToForm is complementary to VisualDocumentNER. Improvements Support for multi-frame dicom has been added. Added the missing load()​ method in ImageToTextV2. New Notebooks We added two new notebooks for VisualDocumentClassifierV2, a preprocessing notebook, useful when you’re dealing with large datasets, and a fine-tuning notebook. We added a new sample notebook showing how to extract forms from digital PDF documents. We added a new sample notebook explaining how to use Simplified Chinese OCR. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_4_0_2",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_4_0_2"
  },
  "236": {
    "id": "236",
    "title": "Spark NLP for Healthcare Release Notes 4.1.0",
    "content": "4.1.0 Highlights Zero-Shot NER model to extract entities with no training dataset 7 new clinical NER models in Spanish 8 new clinical classification models in English and German related to public health topics (depression, covid sentiment, health mentions) New pretrained chunk mapper model (drug_ade_mapper) to map drugs with their corresponding adverse drug events A new pretrained resolver pipeline (medication_resolver_pipeline) to extract medications and resolve their adverse reactions (ADE), RxNorm, UMLS, NDC, SNOMED CT codes and action/treatments in clinical text with a single line of code. Updated NER profiling pretrained pipelines with new NER models to allow running 64 clinical NER models at once Core improvements and bug fixes New and updated notebooks 20+ new clinical models and pipelines added &amp; updated in total Zero-Shot NER model to Extract Entities With No Training Dataset We are releasing the first of its kind Zero-Shot NER model that can detect any named entities without using any annotated dataset to train a model. It allows extracting entities by crafting appropriate prompts to query any RoBERTa Question Answering model. See Models Hub Page for more details. Example : ... zero_shot_ner = ZeroShotNerModel.pretrained(&quot;zero_shot_ner_roberta&quot;, &quot;en&quot;, &quot;clincial/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;zero_shot_ner&quot;) .setEntityDefinitions( { &quot;PROBLEM&quot;: [&quot;What is the disease?&quot;, &quot;What is his symptom?&quot;, &quot;What is her disease?&quot;, &quot;What is his disease?&quot;, &quot;What is the problem?&quot; ,&quot;What does a patient suffer&quot;, &#39;What was the reason that the patient is admitted to the clinic?&#39;], &quot;DRUG&quot;: [&quot;Which drug?&quot;, &quot;Which is the drug?&quot;, &quot;What is the drug?&quot;, &quot;Which drug does he use?&quot;, &quot;Which drug does she use?&quot;, &quot;Which drug do I use?&quot;, &quot;Which drug is prescribed for a symptom?&quot;], &quot;ADMISSION_DATE&quot;: [&quot;When did patient admitted to a clinic?&quot;], &quot;PATIENT_AGE&quot;: [&quot;How old is the patient?&quot;,&#39;What is the age of the patient?&#39;] }) ... sample_text = [&quot;The doctor pescribed Majezik for my severe headache.&quot;, &quot;The patient was admitted to the hospital for his colon cancer.&quot;, &quot;27 years old patient was admitted to clinic on Sep 1st by Dr. X for a right-sided pleural effusion for thoracentesis.&quot;] Results : ++--+-+ | chunk| ner_label|confidence| ++--+-+ | Majezik| DRUG|0.64671576| | severe headache| PROBLEM| 0.5526346| | colon cancer| PROBLEM| 0.8898498| | 27 years old| PATIENT_AGE| 0.6943085| | Sep 1st|ADMISSION_DATE|0.95646095| |a right-sided pleural effusion for thoracentesis| PROBLEM|0.50026613| ++--+-+ 7 New Clinical NER Models in Spanish We are releasing 4 new MedicalNerModel and 3 new MedicalBertForTokenClassifier NER models in Spanish. model name description predicted entities ner_negation_uncertainty This model detects relevant entities from Spanish medical texts NEG UNC USCO NSCO disease_mentions_tweet This model detects disease mentions in Spanish tweets ENFERMEDAD ner_clinical_trials_abstracts This model detects relevant entities from Spanish clinical trial abstracts CHEM DISO PROC ner_pharmacology This model detects pharmacological entities from Spanish medical texts PROTEINAS NORMALIZABLES bert_token_classifier_ner_clinical_trials_abstracts This model detects relevant entities from Spanish clinical trial abstracts CHEM DISO PROC bert_token_classifier_negation_uncertainty This model detects relevant entities from Spanish medical texts NEG NSCO UNC USCO bert_token_classifier_pharmacology This model detects pharmacological entities from Spanish medical texts PROTEINAS NORMALIZABLES Example : ... ner = MedicalNerModel.pretrained(&#39;ner_clinical_trials_abstracts&#39;, &quot;es&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) example_text= &quot;&quot;&quot;&quot;Efecto de la suplementación con ácido fólico sobre los niveles de homocisteína total en pacientes en hemodiálisis. La hiperhomocisteinemia es un marcador de riesgo independiente de morbimortalidad cardiovascular. Hemos prospectivamente reducir los niveles de homocisteína total (tHcy) mediante suplemento con ácido fólico y vitamina B6 (pp), valorando su posible correlación con dosis de diálisis, función residual y parámetros nutricionales.&quot;&quot;&quot;&quot; Results : +--++ |chunk |ner_label| +--++ |suplementación |PROC | |ácido fólico |CHEM | |niveles de homocisteína |PROC | |hemodiálisis |PROC | |hiperhomocisteinemia |DISO | |niveles de homocisteína total|PROC | |tHcy |PROC | |ácido fólico |CHEM | |vitamina B6 |CHEM | |pp |CHEM | |diálisis |PROC | |función residual |PROC | +--++ 8 New Clinical Classification Models in English and German Related to Public Health Topics (Depression, Covid Sentiment, Health Mentions) We are releasing 8 new MedicalBertForSequenceClassification models to classify text from social media data in English and German related to public health topics (depression, covid sentiment, health mentions) model name description predicted entities bert_sequence_classifier_depression_binary This model classifies whether a social media text expresses depression or not. no-depression depression bert_sequence_classifier_health_mentions_gbert_large This GBERT-large based model classifies public health mentions in German social media text. non-health health-related bert_sequence_classifier_health_mentions_medbert This German-MedBERT based model classifies public health mentions in German social media text. non-health health-related bert_sequence_classifier_health_mentions_gbert This GBERT-large based model classifies public health mentions in German social media text. non-health health-related bert_sequence_classifier_health_mentions_bert This bert-base-german based model classifies public health mentions in German social media text. non-health health-related bert_sequence_classifier_depression_twitter This PHS-BERT based model classifies whether tweets contain depressive text or not. depression no-depression bert_sequence_classifier_depression This PHS-BERT based model classifies depression level of social media text into three levels. no-depression minimum high-depression bert_sequence_classifier_covid_sentiment This BioBERT based sentiment analysis model classifies whether a tweet contains positive, negative, or neutral sentiments about COVID-19 pandemic. neutral positive negative Example : ... sequenceClassifier = MedicalBertForSequenceClassification.pretrained(&quot;bert_sequence_classifier_depression_twitter&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;,&quot;token&quot;]) .setOutputCol(&quot;class&quot;) example_text = [&quot;Do what makes you happy, be with who makes you smile, laugh as much as you breathe, and love as long as you live!&quot;, &quot;Everything is a lie, everyone is fake, I&#39;m so tired of living&quot;] Results : +++ |text |result | +--++ |Do what makes you happy, be with who makes you smile, laugh as much as you breathe, and love as long as you live!|[no-depression]| |Everything is a lie, everyone is fake, I am so tired of living. |[depression] | +--++ New Pretrained Chunk Mapper Model (drug_ade_mapper) to Map Drugs With Their Corresponding Adverse Drug Events We are releasing new drug_ade_mapper pretrained chunk mapper model to map drugs with their corresponding adverse drug events. See Models Hub Page for more details. Example : ... chunkMapper = ChunkMapperModel.pretrained(&quot;drug_ade_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRels([&quot;ADE&quot;]) ... sample_text = &quot;The patient was prescribed 1000 mg fish oil and multivitamins. She was discharged on zopiclone and ambrisentan.&quot; Results : +-++-+ |ner_chunk |ade_mappings|all_relations | +-++-+ |1000 mg fish oil|Dizziness |Myocardial infarction:::Nausea | |multivitamins |Erythema |Acne:::Dry skin:::Skin burning sensation:::Inappropriate schedule of product administration| |zopiclone |Vomiting |Malaise:::Drug interaction:::Asthenia:::Hyponatraemia | |ambrisentan |Dyspnoea |Therapy interrupted:::Death:::Dizziness:::Drug ineffective | +-++-+ A New Pretrained Resolver Pipeline (medication_resolver_pipeline) to Extract Medications and Resolve Their Adverse Reactions (ADE), RxNorm, UMLS, NDC, SNOMED CT Codes and Action/Treatments in Clinical Text. We are releasing the medication_resolver_pipeline pretrained pipeline to extract medications and resolve their adverse reactions (ADE), RxNorm, UMLS, NDC, SNOMED CT codes and action/treatments in clinical text with a single line of code. Also, you can use medication_resolver_transform_pipeline to use transform method of Spark. See Models Hub Page for more details. Example : from sparknlp.pretrained import PretrainedPipeline sample_text = &quot;&quot;&quot;The patient was prescribed Amlodopine Vallarta 10-320mg, Eviplera. The other patient is given Lescol 40 MG and Everolimus 1.5 mg tablet.&quot;&quot;&quot; med_pipeline = PretrainedPipeline(&quot;medication_resolver_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) med_pipeline.annotate(sample_text) med_transform_pipeline = PretrainedPipeline(&quot;medication_resolver_transform_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) med_transform_pipeline.transform(spark.createDataFrame([[sample_text]]).toDF(&quot;text&quot;)) Results : | chunk | ner_label | ADE | RxNorm | Action | Treatment | UMLS | SNOMED_CT | NDC_Product | NDC_Package | |:--|:|:-|:|:|:-|:|:|:--|:--| | Amlodopine Vallarta 10-320mg | DRUG | Gynaecomastia | 722131 | NONE | NONE | C1949334 | 425838008 | 00093-7693 | 00093-7693-56 | | Eviplera | DRUG | Anxiety | 217010 | Inhibitory Bone Resorption | Osteoporosis | C0720318 | NONE | NONE | NONE | | Lescol 40 MG | DRUG | NONE | 103919 | Hypocholesterolemic | Heterozygous Familial Hypercholesterolemia | C0353573 | NONE | 00078-0234 | 00078-0234-05 | | Everolimus 1.5 mg tablet | DRUG | Acute myocardial infarction | 2056895 | NONE | NONE | C4723581 | NONE | 00054-0604 | 00054-0604-21 | Updated NER Profiling Pretrained Pipelines With New NER Models to Allow Running 64 Clinical NER Models at Once We have upadated ner_profiling_clinical and ner_profiling_biobert pretrained pipelines with the new NER models. When you run these pipelines over your text, now you will end up with the predictions coming out of 64 clinical NER models in ner_profiling_clinical and 22 clinical NER models in ner_profiling_biobert results. You can check ner_profiling_clinical and ner_profiling_biobert Models Hub pages for more details and the NER model lists that these pipelines include. Core Improvements and Bug Fixes Updated HCC module (from sparknlp_jsl.functions import profile) with the new changes in HCC score calculation functions. AnnotationToolJsonReader, NerDLMetrics and StructuredDeidentification: These annotators can be used on Spark 3.0 now. NerDLMetrics: Added case_sensitive parameter and case sensitivity issue in tokens is solved. Added drop_o parameter to computeMetricsFromDF method and dropO parameter in NerDLMetrics class is deprecated. MedicalNerModel: Inconsistent NER model results between different versions issue is solved. AssertionDLModel: Unindexed chunks will be ignored by the AssertionDLModel instead of raising an exception. ContextualParserApproach: These two issues are solved when using ruleScope: &quot;document&quot; configuration: Wrong index computations of chunks after matching sub-tokens. Including sub-token matches even though completeMatchRegex: &quot;true&quot;. New and Updated Notebooks We have a new Zero-Shot Clinical NER Notebook to show how to use zero-shot NER model. We have updated Medicare Risk Adjustment Score Calculation Notebook with the new changes in HCC score calculation functions. We have updated these notebooks with the new updates in NER profiling pretrained pipelines: Clinical Named Entity Recognition Model Notebook Pretrained Clinical Pipelines Notebook Pretrained NER Profiling Pipelines Notebook We have updated Clinical Assertion Model Notebook according to the bug fix in the training section. We moved all Azure/AWS/Databricks notebooks to products folder in spark-nlp-worksop repo. 20+ New Clinical Models and Pipelines Added &amp; Updated in Total zero_shot_ner_roberta medication_resolver_pipeline medication_resolver_transform_pipeline ner_profiling_clinical ner_profiling_biobert drug_ade_mapper ner_negation_uncertainty disease_mentions_tweet ner_clinical_trials_abstracts ner_pharmacology bert_token_classifier_ner_clinical_trials_abstracts bert_token_classifier_negation_uncertainty bert_token_classifier_pharmacology bert_sequence_classifier_depression_binary bert_sequence_classifier_health_mentions_gbert_large bert_sequence_classifier_health_mentions_medbert bert_sequence_classifier_health_mentions_gbert bert_sequence_classifier_health_mentions_bert bert_sequence_classifier_depression_twitter bert_sequence_classifier_depression bert_sequence_classifier_covid_sentiment Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_1_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_1_0"
  },
  "237": {
    "id": "237",
    "title": "Annotation Lab Release Notes 4.1.0",
    "content": "4.1.0 Release date: 30-09-2022 Here are the highlights of this release: Highlights Updated login page. This release of Annotation Lab has an updated Login View. Unlike a plain old form, we have an aesthetically pleasing Login Page with a section highlighting the key features of Annotation Lab. Now the Sign-In page highlights the new features we add to the Annotation Lab with animated GIFs. Project Dashboard. The Projects dashboard has a new structure with visually pleasing project cards. For each project, details like description, tasks count, groups, team members, etc. are now available on the main dashboard so users can quickly identify the projects they need to work on, without navigating to the Project Details page. Projects can also be sorted by name or date of creation. Categorize Projects with Groups. Projects can be organized in custom groups, and each project card will inherit the group color so that the users can visually distinguish the projects easily in a large cluster of projects. Also, the new color picker for the group is much more user-friendly and customizable, unlike the random color generator in the previous versions of Annotation Lab. Project Filters. The filters associated with the Projects dashboard are clear, simple, and precise to make the users more productive and efficient while working with a large number of projects. Project Creation Wizard. A project creation wizard is now available and will guide users through each step of the project creation and configuration. Two navigation buttons Back and Next were added to the Team page. The Back button navigates to the Project’s Details page and the Next button to navigates to the Configuration page. Optimized Task page. The newly redesigned task page incorporates all the Task Related operations that a user needs to perform, such as Import, Export, Labeling, Pre-Annotation, etc., in a single page without having to navigate between different pages. Support for multiple comments. Previously a comment could be pinned to a task from the Task List Page where anyone could leave a note for peer contributors. With this release, multiple comments can be added to any task. The users can have a to and fro communication in the comment section resulting in the improved efficiency of the annotation process. New Import page. The new Import Page contains detailed information on the supported file formats with sample files attached to them. Users can refer to the samples and create their files/tasks to import with minimum help. New Export page. The new Export page simplifies the experience while exporting annotations in different formats. The Annotation page. The annotation page has been reorganized and optimized as annotators spend most of their time on this page. The Side Column now separates Annotation, Versions, and Progress into separate tabs. The Regions/Labels UI is migrated into a collapsible structure that inherits the Label color defined in the project configuration to make it easy for users to identify annotations in case of a large number of Regions or Labels. The role switcher is now more visible on the upper right side, and the choice is persisted when navigating to other pages of the same project. New Train page. The Train page is now part of the Project Menu, for improved accessibility. It has been revised to improve the experience and guide users on each step. Users can now follow a step-wise wizard view or a synthesis view for initiating the training of a model. During the training, a progress bar is shown to give users basic information on the status of the training process. New Models HUB page. This version comes with brightened and improved Models HUB page. The cards for models, embeddings, and rules are visually pleasing and highlight their source. The displayed information is much more compact and easy to read. The cards are visually separable just by looking at the colors and the card types. New License page. The License Page now has a tabbed view. The first tab allows importing of the JSL license in the preferred method. The second tab displays the already existing license on a full page with corresponding details and actions. The page provides detailed instructions on how to import licenses and how to get a trial license. New Users page. The Users page is redesigned to make the operations regarding users’ information more time efficient and less confusing. The Personal Info, Role, and Credential sections are merged into a single page so that users do not have to click around to add/update. New Analytics Request page. The Swagger and Secrets Page have been merged into one single API Integration page. The users can find everything needed on that page without having to click around for the needed information regarding the APIs. New Clusters page. The Servers page has been redesigned and renamed into the Clusters page. The page now shows more details like License type/scope and Server usage of all the spawned instances at a given time. A license information banner is now available on the Clusters and License pages. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_4_1_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_4_1_0"
  },
  "238": {
    "id": "238",
    "title": "Spark NLP release notes 4.1.0",
    "content": "4.1.0 Release date: 22-09-2022 Overview We are glad to announce that Spark OCR 4.1.0 has been released! This release comes with new features, enhancements, fixes and more!. New Features DicomSplitter: new annotator that helps to distribute and split Dicom files into multiple frames. It supports multiple strategies, similar to our PdfToImage annotator. It enables parallel processing of different frames and keeps memory utilization bounded. For big datasets, or memory constrained environments, it enables Streaming Mode to process frames 1-by-1, resulting in very low memory requirements. DicomToImageV2: new annotator that supports loading images from Dicom files/frames, without loading Dicom files into memory. Targeted to datasets containing big Dicom files. This is an example on how to use the two above mentioned annotators to process images, coming from your big Dicom files in a memory constrained setting, splitter = DicomSplitter() splitter.setInputCol(&quot;path&quot;) splitter.setOutputCol(&quot;frames&quot;) splitter.setSplitNumBatch(2) splitter.setPartitionNum(2) dicom = DicomToImageV2() dicom.setInputCols([&quot;path&quot;, &quot;frames&quot;]) dicom.setOutputCol(&quot;image&quot;) pipeline = PipelineModel(stages=[ splitter, dicom ]) New image pre-processing annotators: ImageHomogenizeLight, ImageRemoveBackground, ImageEnhanceContrast, ImageRemoveGlare. For examples on how to use them, and their amazing results check this notebook: SparkOcrImagePreprocessing.ipynb. Improvements VisualDocumentClassifierV2 training has been improved for more efficient memory utilization. Library dependencies have been updated to remove security vulnerabilities. Bug Fixes The infamous “ImportError: No module named resource” bug that was affecting Windows users has been fixed. Some issues while loading images using AlabReader have been fixed. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_4_1_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_4_1_0"
  },
  "239": {
    "id": "239",
    "title": "Spark NLP for Healthcare Release Notes 4.2.0",
    "content": "4.2.0 Highlights Introducing 46 new Oncology specific pretrained models (12 NER, 12 BERT-based token classification, 14 relation extraction, 8 assertion status models) Brand new NerQuestionGenerator annotator for automated prompt generation for a QA-based Zero-Shot NER model Updated ALAB (Annotation Lab) module becoming a fullfledged suite to manage activities on ALAB via its API remotely New pretrained assertion status detection model (assertion_jsl_augmented) to classify the negativity &amp; assertion scope of medical concepts New chunk mapper models and pretrained pipeline to map entities (phrases) to their corresponding ICD-9, ICD-10-CM and RxNorm codes New ICD-9-CM sentence entity resolver model and pretrained pipeline New shifting days feature in DeIdentification by using the new DocumentHashCoder annotator Updated NER model finder pretrained pipeline to help users find the most appropriate NER model for their use case in one-liner Medicare risk adjustment score calculation module updated to support different version and year combinations Core improvements and bug fixes New and updated notebooks 50+ new clinical models and pipelines added &amp; updated in total Introducing 46 New Oncology Specific Pretrained Models (12 NER, 12 BERT-Based Token Classification, 14 Relation Extraction, 8 Assertion Status Models) These models will be the first versions (wip - work in progress) of Oncology models. See Oncology Model Notebook for examples. New Oncological NER and BERT-Based Token Classification Models We have 12 new oncological NER and their BERT-based token classification models. NER model name (MedicalNerModel) BERT-Based model name (MedicalBertForTokenClassifier) description predicted entities ner_oncology_therapy_wip bert_token_classifier_ner_oncology_therapy_wip This model extracts entities related to cancer therapies, including posology entities and response to treatment, using granular labels. Response_To_Treatment, Line_Of_Therapy, Cancer_Surgery, Radiotherapy, Immunotherapy, Targeted_Therapy, Hormonal_Therapy, Chemotherapy, Unspecific_Therapy, Route, Duration, Cycle_Count, Dosage, Frequency, Cycle_Number, Cycle_Day, Radiation_Dose ner_oncology_diagnosis_wip bert_token_classifier_ner_oncology_diagnosis_wip This model extracts entities related to cancer diagnosis, including the presence of metastasis. Grade, Staging, Tumor_Size, Adenopathy, Pathology_Result, Histological_Type, Metastasis, Cancer_Score, Cancer_Dx, Invasion, Tumor_Finding, Performance_Status ner_oncology_wip bert_token_classifier_ner_oncology_wip This model extracts more than 40 oncology-related entities. Histological_Type, Direction, Staging, Cancer_Score, Imaging_Test, Cycle_Number, Tumor_Finding, Site_Lymph_Node, Invasion, Response_To_Treatment, Smoking_Status, Tumor_Size, Cycle_Count, Adenopathy, Age, Biomarker_Result, Unspecific_Therapy, Site_Breast, Chemotherapy, Targeted_Therapy, Radiotherapy, Performance_Status, Pathology_Test, Site_Other_Body_Part, Cancer_Surgery, Line_Of_Therapy, Pathology_Result, Hormonal_Therapy, Site_Bone, Biomarker, Immunotherapy, Cycle_Day, Frequency, Route, Duration, Death_Entity, Metastasis, Site_Liver, Cancer_Dx, Grade, Date, Site_Lung, Site_Brain, Relative_Date, Race_Ethnicity, Gender, Oncogene, Dosage, Radiation_Dose ner_oncology_tnm_wip bert_token_classifier_ner_oncology_tnm_wip This model extracts mentions related to TNM staging. Lymph_Node, Staging, Lymph_Node_Modifier, Tumor_Description, Tumor, Metastasis, Cancer_Dx ner_oncology_anatomy_general_wip bert_token_classifier_ner_oncology_anatomy_general_wip This model extracts anatomical entities. Anatomical_Site, Direction ner_oncology_demographics_wip bert_token_classifier_ner_oncology_demographics_wip This model extracts demographic information, including smoking status. Age, Gender, Smoking_Status, Race_Ethnicity ner_oncology_test_wip bert_token_classifier_ner_oncology_test_wip This model extracts mentions of oncology-related tests. Oncogene, Biomarker, Biomarker_Result, Imaging_Test, Pathology_Test ner_oncology_unspecific_posology_wip bert_token_classifier_ner_oncology_unspecific_posology_wip This model extracts any mention of cancer therapies and posology information using general labels Cancer_Therapy, Posology_Information ner_oncology_anatomy_granular_wip bert_token_classifier_ner_oncology_anatomy_granular_wip This model extracts anatomical entities using granular labels. Direction, Site_Lymph_Node, Site_Breast, Site_Other_Body_Part, Site_Bone, Site_Liver, Site_Lung, Site_Brain ner_oncology_response_to_treatment_wip bert_token_classifier_ner_oncology_response_to_treatment_wip This model extracts entities related to the patient’s response to cancer treatment. Response_To_Treatment, Size_Trend, Line_Of_Therapy ner_oncology_biomarker_wip bert_token_classifier_ner_oncology_biomarker_wip This model extracts biomarkers and their results. Biomarker, Biomarker_Result ner_oncology_posology_wip bert_token_classifier_ner_oncology_posology_wip This model extracts oncology specific posology information and cancer therapies. Cycle_Number, Cycle_Count, Radiotherapy, Cancer_Surgery, Cycle_Day, Frequency, Route, Cancer_Therapy, Duration, Dosage, Radiation_Dose F1 Scores: label f1 label f1 label f1 label f1 label f1 Adenopathy 0.73 Cycle_Day 0.83 Histological_Type 0.71 Posology_Information 0.88 Site_Lymph_Node 0.91 Age 0.97 Cycle_Number 0.79 Hormonal_Therapy 0.90 Race_Ethnicity 0.86 Smoking_Status 0.82 Anatomical_Site 0.83 Date 0.97 Imaging_Test 0.90 Radiation_Dose 0.87 Staging 0.85 Biomarker 0.89 Death_Entity 0.82 Invasion 0.80 Radiotherapy 0.90 Targeted_Therapy 0.87 Biomarker_Result 0.82 Direction 0.82 Line_Of_Therapy 0.91 Relative_Date 0.79 Tumor 0.91 Cancer_Dx 0.92 Dosage 0.91 Lymph_Node 0.86 Route 0.84 Tumor_Description 0.81 Cancer_Surgery 0.85 Duration 0.77 Lymph_Node_Modifier 0.75 Site_Bone 0.80 Tumor_Finding 0.92 Cancer_Therapy 0.90 Frequency 0.88 Metastasis 0.95 Site_Brain 0.78 Tumor_Size 0.88 Chemotherapy 0.90 Gender 0.99 Oncogene 0.77 Site_Breast 0.88     Cycle_Count 0.81 Grade 0.81 Pathology_Test 0.79 Site_Lung 0.79     NER Model Example: ... medical_ner = MedicalNerModel.pretrained(&quot;ner_oncology_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) ... sample_text = &quot;The had previously undergone a left mastectomy and an axillary lymph node dissection for a left breast cancer twenty years ago. The tumor was positive for ER. Postoperatively, radiotherapy was administered to her breast.&quot; BERT-Based Token Classification Model Example: ... tokenClassifier = MedicalBertForTokenClassifier.pretrained(&quot;bert_token_classifier_ner_oncology_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;ner&quot;) .setCaseSensitive(True) ... sample_text = &quot;The had previously undergone a left mastectomy and an axillary lymph node dissection for a left breast cancer twenty years ago. The tumor was positive for ER. Postoperatively, radiotherapy was administered to her breast.&quot; Results: +++ |chunk |ner_label | +++ |left |Direction | |mastectomy |Cancer_Surgery | |axillary lymph node dissection|Cancer_Surgery | |left |Direction | |breast cancer |Cancer_Dx | |twenty years ago |Relative_Date | |tumor |Tumor_Finding | |positive |Biomarker_Result | |ER |Biomarker | |radiotherapy |Radiotherapy | |her |Gender | |breast |Site_Breast | +++ New Oncological Assertion Status Models We have 8 new oncological assertion status detection models. model name description predicted entities assertion_oncology_wip This model identifies the assertion status of different oncology-related entities. Medical_History, Family_History, Possible, Hypothetical_Or_Absent assertion_oncology_problem_wip This assertion model identifies the status of Cancer_Dx extractions and other problem entities. Present, Possible, Hypothetical, Absent, Family assertion_oncology_treatment_wip This model identifies the assertion status of treatments mentioned in text. Present, Planned, Past, Hypothetical, Absent assertion_oncology_response_to_treatment_wip This assertion model identifies if the response to treatment mentioned in text actually happened, or if it mentioned as something absent or hypothetical. Present_Or_Past, Hypothetical_Or_Absent assertion_oncology_test_binary_wip This assertion model identifies if a test mentioned in text actually was used, or if it mentioned as something absent or hypothetical. Present_Or_Past, Hypothetical_Or_Absent assertion_oncology_smoking_status_wip This assertion model is used to classify the smoking status of the patient. Absent, Past, Present assertion_oncology_family_history_wip This assertion model identifies if an entity refers to a family member. Family_History, Other assertion_oncology_demographic_binary_wip This assertion model identifies if the demographic entities refer to the patient or to someone else. Patient, Someone_Else Example: ... assertion = AssertionDLModel.pretrained(&quot;assertion_oncology_problem_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &#39;ner_chunk&#39;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) ... sample_text = &quot;Considering the findings, the patient may have a breast cancer. There are no signs of metastasis. Family history positive for breast cancer in her maternal grandmother.&quot; Results: +-+-++ | chunk| ner_label|assertion| +-+-++ |breast cancer| Cancer_Dx| Possible| | metastasis|Metastasis| Absent| |breast cancer| Cancer_Dx| Family| +-+-++ New Oncological Relation Extraction Models We are releasing 7 new RelationExtractionModel and 7 new RelationExtractionDLModel models to extract relations between various oncological concepts. model name description predicted entities re_oncology_size_wip This model links Tumor_Size extractions to their corresponding Tumor_Finding extractions. is_size_of, O re_oncology_biomarker_result_wip This model links Biomarker and Oncogene extractions to their corresponding Biomarker_Result extractions. is_finding_of, O re_oncology_granular_wip This model can be identified four relation types is_size_of, is_finding_of, is_date_of, is_location_of, O re_oncology_location_wip This model links extractions from anatomical entities (such as Site_Breast or Site_Lung) to other clinical entities (such as Tumor_Finding or Cancer_Surgery). is_location_of, O re_oncology_temporal_wip This model links Date and Relative_Date extractions to clinical entities such as Test or Cancer_Dx. is_date_of, O re_oncology_test_result_wip This model links test extractions to their corresponding results. is_finding_of, O re_oncology_wip This model link between dates and other clinical entities, between tumor mentions and their size, between anatomical entities and other clinical entities, and between tests and their results. is_related_to, O redl_oncology_size_biobert_wip This model links Tumor_Size extractions to their corresponding Tumor_Finding extractions. is_size_of, O redl_oncology_biomarker_result_biobert_wip This model links Biomarker and Oncogene extractions to their corresponding Biomarker_Result extractions. is_finding_of, O redl_oncology_location_biobert_wip This model links extractions from anatomical entities (such as Site_Breast or Site_Lung) to other clinical entities (such as Tumor_Finding or Cancer_Surgery). is_location_of, O redl_oncology_temporal_biobert_wip This model links Date and Relative_Date extractions to clinical entities such as Test or Cancer_Dx. is_date_of, O redl_oncology_test_result_biobert_wip This model links test extractions to their corresponding results. is_finding_of, O redl_oncology_biobert_wip This model identifies relations between dates and other clinical entities, between tumor mentions and their size, between anatomical entities and other clinical entities, and between tests and their results. is_related_to redl_oncology_granular_biobert_wip This model can be identified four relation types is_date_of, is_finding_of, is_location_of, is_size_of, O F1 Scores and Samples: label F1 Score sample_text results is_finding_of 0.95 “Immunohistochemistry was negative for thyroid transcription factor-1 and napsin A.” negative - thyroid transcription factor-1, negative - napsin is_date_of 0.81 “A mastectomy was performed two months ago.” mastectomy-two months ago is_location_of 0.92 “In April 2011, she first noticed a lump in her right breast.” lump - breast is_size_of 0.86 “The patient presented a 2 cm mass in her left breast.” 2 cm - mass is_related_to 0.87 A mastectomy was performed two months ago.” mastectomy - two months ago Example: ... re_model = RelationExtractionModel.pretrained(&quot;re_oncology_size_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunk&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setRelationPairs([&quot;Tumor_Finding-Tumor_Size&quot;, &quot;Tumor_Size-Tumor_Finding&quot;]) .setMaxSyntacticDistance(10) ... sample_text = &quot;The patient presented a 2 cm mass in her left breast, and the tumor in her other breast was 3 cm long.&quot; Results: +-+-++-++-+ | relation| entity1|chunk1| entity2|chunk2|confidence| +-+-++-++-+ |is_size_of| Tumor_Size| 2 cm|Tumor_Finding| mass| 0.8532705| |is_size_of|Tumor_Finding| tumor| Tumor_Size| 3 cm| 0.8156226| +-+-++-++-+ Brand New NerQuestionGenerator Annotator For Automated Prompt Generation For A QA-based Zero-Shot NER Model. This annotators helps you build questions on the fly using 2 entities from different labels (preferably a subject and a verb). For example, let’s suppose you have an NER model, able to detect PATIENTand ADMISSION in the following text: John Smith was admitted Sep 3rd to Mayo Clinic PATIENT: John Smith ADMISSION: was admitted You can add the following annotator to construct questions using PATIENT and ADMISSION: # setEntities1 says which entity from NER goes first in the question # setEntities2 says which entity from NER goes second in the question # setQuestionMark to True adds a &#39;?&#39; at the end of the sentence (after entity 2) # To sum up, the pattern is [QUESTIONPRONOUN] [ENTITY1] [ENTITY2] [QUESTIONMARK] qagenerator = NerQuestionGenerator() .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;question&quot;) .setQuestionMark(True) .setQuestionPronoun(&quot;When&quot;) .setStrategyType(&quot;Paired&quot;) .setEntities1([&quot;PATIENT&quot;]) .setEntities2([&quot;ADMISSION&quot;]) In the column question you will find: When John Smith was admitted?. Likewise you could have Where or any other question pronoun you may need. You can use those questions in a QuestionAnsweringModel or ZeroShotNER (any model which requires a question as an input. Let’s see the case of QA. qa = BertForQuestionAnswering.pretrained(&quot;bert_qa_spanbert_finetuned_squadv1&quot;,&quot;en&quot;) .setInputCols([&quot;question&quot;, &quot;document&quot;]) .setOutputCol(&quot;answer&quot;) .setCaseSensitive(True) The result will be: +--+--+ |question |answer | +--+--+ |[{document, 0, 25, When John Smith was admitted ? ...}] |[{chunk, 0, 8, Sep 3rd ...}] | +--+--+ Strategies: Paired: First chunk of Entity 1 will be grouped with first chunk of Entity 2, second with second, third with third, etc (one-vs-one) Combined: A more flexible strategy to be used in case the number of chukns in Entity 1 is not aligned with the number of chunks in Entityt 2. The first chunk from Entity 1 will be grouped with all chunks in Entity 2, the second chunk in Entity 1 with again be grouped with all the chunks in Entity 2, etc (one-vs-all). Updated ALAB (Annotation Lab) Module Becoming a Fullfledged Suite to Manage Activities on ALAB Via Its API Remotely We are release a new module for interacting with Annotation Lab with minimal code. Users can now create/edit/delete projects and their tasks. Also, they can upload preannotations, and export annotations and generate training data for various models. Complete documentation and tutorial is available at Spark NLP Workshop. Following is a comprehensive list of supported tasks: Getting details of all projects in the Annotation Lab instance. Creating New Projects. Deleting Projects. Setting &amp; editing configuration of projects. Accessing/getting configuration of any existing project. Upload tasks to a project. Deleting tasks of a project. Generating Preannotations for a project using custom Spark NLP pipelines. Uploading Preannotations to a project. Generating dataset for training Classification models. Generating dataset for training NER models. Generating dataset for training Assertion models. Generating dataset for training Relation Extraction models. Using Annotation Lab Module: from sparknlp_jsl.alab import AnnotationLab alab = AnnotationLab() alab.set_credentials(username=username, password=password, client_secret=client_secret, annotationlab_url=annotationlab_url) # create a new project alab.create_project(&#39;alab_demo&#39;) # assign ner labels to the project alab.set_project_config(&#39;alab_demo&#39;, ner_labels=[&#39;Age&#39;, &#39;Gender&#39;]) # upload tasks alab.upload_tasks(&#39;alab_demo&#39;, task_list=[txt1, txt2...]) # export tasks alab.get_annotations(&#39;alab_demo&#39;) New Pretrained Assertion Status Detection Model (assertion_jsl_augmented) to Classify The Negativity &amp; Assertion Scope of Medical Concepts We are releasing new assertion_jsl_augmented model to classify the assertion status of the clinical entities with Present, Absent, Possible, Planned, Past, Family, Hypothetical and SomeoneElse labels. See Models Hub Page for more details. Example: ... clinical_assertion = AssertionDLModel.pretrained(&quot;assertion_jsl_augmented&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;ner_chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;assertion&quot;) ... sample_text = &quot;&quot;&quot;Patient had a headache for the last 2 weeks, and appears anxious when she walks fast. No alopecia noted. She denies pain. Her father is paralyzed and it is a stressor for her. She was bullied by her boss and got antidepressant. We prescribed sleeping pills for her current insomnia&quot;&quot;&quot; Results: +--+--++-+--++ |ner_chunk |begin|end|ner_label |sentence_id|assertion| +--+--++-+--++ |headache |14 |21 |Symptom |0 |Past | |anxious |57 |63 |Symptom |0 |Possible | |alopecia |89 |96 |Disease_Syndrome_Disorder|1 |Absent | |pain |116 |119|Symptom |2 |Absent | |paralyzed |136 |144|Symptom |3 |Family | |antidepressant|212 |225|Drug_Ingredient |4 |Past | |sleeping pills|242 |255|Drug_Ingredient |5 |Planned | |insomnia |273 |280|Symptom |5 |Present | +--+--++-+--++ New Chunk Mapper models and Pretrained Pipeline to map entities (phrases) to their corresponding ICD-9, ICD-10-CM and RxNorm codes We are releasing 4 new chunk mapper models that can map entities to their corresponding ICD-9, ICD-10-CM and RxNorm codes. model name description rxnorm_normalized_mapper Mapping drug entities (phrases) with the corresponding RxNorm codes and normalized resolutions. icd9_mapper Mapping entities with their corresponding ICD-9-CM codes. icd10_icd9_mapper Mapping ICD-10-CM codes with their corresponding ICD-9-CM codes. icd9_icd10_mapper Mapping ICD-9-CM codes with their corresponding ICD-10-CM codes. icd10_icd9_mapping (Pipeline) This pretrained pipeline maps ICD-10-CM codes to ICD-9-CM codes without using any text data. Model Example: ... chunkerMapper = ChunkMapperModel.pretrained(&quot;rxnorm_normalized_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRels([&quot;rxnorm_code&quot;, &quot;normalized_name&quot;]) ... sample_text = &quot;The patient was given Zyrtec 10 MG, Adapin 10 MG Oral Capsule, Septi-Soothe 0.5 Topical Spray&quot; Results: ++--+--+ |ner_chunk |rxnorm_code|normalized_name | ++--+--+ |Zyrtec 10 MG |1011483 |cetirizine hydrochloride 10 MG [Zyrtec] | |Adapin 10 MG Oral Capsule |1000050 |doxepin hydrochloride 10 MG Oral Capsule [Adapin] | |Septi-Soothe 0.5 Topical Spray|1000046 |chlorhexidine diacetate 0.5 MG/ML Topical Spray [Septi-Soothe]| ++--+--+ Pipeline Example: from sparknlp.pretrained import PretrainedPipeline pipeline = PretrainedPipeline( &quot;icd10_icd9_mapping&quot;,&quot;en&quot;,&quot;clinical/models&quot;) pipeline.annotate(&quot;Z833 A0100 A000&quot;) Results: | icd10_code | icd9_code | |:--|:-| | Z833 - A0100 - A000 | V180 - 0020 - 0010 | New ICD-9-CM Sentence Entity Resolver Model and Pretrained Pipeline sbiobertresolve_icd9 : This model maps extracted medical entities to their corresponding ICD-9-CM codes using sbiobert_base_cased_mli Sentence Bert Embeddings. Example: ... icd10_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_icd9&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) ... sample_text = &quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus, associated with an acute hepatitis, and obesity with a body mass index (BMI) of 33.5 kg/m2.&quot; Results: +-+-+++-+ | ner_chunk| entity|icd9_code| resolution| all_codes| +-+-+++-+ | gestational diabetes mellitus|PROBLEM| V12.21|[Personal history of gestational diabetes, Ne...|[V12.21, 775.1, 249, 250, 249.7, 249.71, 249.9, 249.61,...| |subsequent type two diabetes mellitus|PROBLEM| 249|[Secondary diabetes mellitus, Diabetes mellit...|[249, 250, 249.9, 249.7, 775.1, 249.6, 249.8, V12.21, 2...| | an acute hepatitis|PROBLEM| 571.1|[Acute alcoholic hepatitis, Viral hepatitis, ...|[571.1, 070, 571.42, 902.22, 279.51, 571.4, 091.62, 572...| | obesity|PROBLEM| 278.0|[Overweight and obesity, Morbid obesity, Over...|[278.0, 278.01, 278.02, V77.8, 278, 278.00, 272.2, 783....| | a body mass index|PROBLEM| V85|[Body mass index [BMI], Human bite, Localized...|[V85, E928.3, 278.1, 993, E008.4, V61.5, 747.63, V85.5,...| +-+-+++-+ icd9_resolver_pipeline : This pretrained pipeline maps entities with their corresponding ICD-9-CM codes. You’ll just feed your text and it will return the corresponding ICD-9-CM codes. Example: from sparknlp.pretrained import PretrainedPipeline resolver_pipeline = PretrainedPipeline(&quot;icd9_resolver_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) sample_text = &quot;&quot;&quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years and anisakiasis. Also, it was reported that fetal and neonatal hemorrhage&quot;&quot;&quot; result = resolver_pipeline.fullAnnotate(sample_text) Results: +--+++ |chunk |ner_chunk|icd9_code| +--+++ |gestational diabetes mellitus|PROBLEM |V12.21 | |anisakiasis |PROBLEM |127.1 | |fetal and neonatal hemorrhage|PROBLEM |772 | +--+++ New Shifting Days Feature in Deidentification by Using the New DocumentHashCoder Annotator Now we can shift dates in the documents rather than obfuscating randomly. We have a new DocumentHashCoder() annotator to determine shifting days. This annotator gets the hash of the specified column and creates a new document column containing day shift information. And then, the DeIdentification annotator deidentifies this new doc. We can use the seed parameter to hash consistently. Example: documentHasher = DocumentHashCoder() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;document2&quot;) .setPatientIdColumn(&quot;patientID&quot;) .setRangeDays(100) .setNewDateShift(&quot;shift_days&quot;) .setSeed(100) de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;document2&quot;]) .setOutputCol(&quot;deid_text&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateDate(True) .setDateTag(&quot;DATE&quot;) .setLanguage(&quot;en&quot;) .setObfuscateRefSource(&#39;faker&#39;) .setUseShifDays(True) Results: output.select(&#39;patientID&#39;,&#39;text&#39;, &#39;deid_text.result&#39;).show(truncate = False) ++-++ |patientID|text |result | ++-++ |A001 |Chris Brown was discharged on 10/02/2022|[Glorious Mc was discharged on 27/03/2022] | |A001 |Mark White was discharged on 10/04/2022 |[Kimberlee Bair was discharged on 25/05/2022]| |A003 |John was discharged on 15/03/2022 |[Monia Richmond was discharged on 17/05/2022]| |A003 |John Moore was discharged on 15/12/2022 |[Veleta Pollard was discharged on 16/02/2023]| ++-++ Instead of shifting days according to ID column, we can specify shifting values with another column. Example: documentHasher = DocumentHashCoder() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;document2&quot;) .setDateShiftColumn(&quot;dateshift&quot;) de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;document2&quot;]) .setOutputCol(&quot;deid_text&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateDate(True) .setDateTag(&quot;DATE&quot;) .setLanguage(&quot;en&quot;) .setObfuscateRefSource(&#39;faker&#39;) .setUseShifDays(True) Results: +-+++ |text |dateshift|result | +-+++ |Chris Brown was discharged on 10/02/2022|10 |[Levorn Powers was discharged on 20/02/2022] | |Mark White was discharged on 10/04/2022 |10 |[Hall Jointer was discharged on 20/04/2022] | |John was discharged on 15/03/2022 |30 |[Jared Gains was discharged on 14/04/2022] | |John Moore was discharged on 15/12/2022 |30 |[Frederic Seitz was discharged on 14/01/2023]| +-+++ You can check Clinical Deidentification Notebook for more examples. Updated NER Model Finder Pretrained Pipeline to Help Users Find The Most Appropriate NER Model For Their Use Case In One-Liner We have updated ner_model_finder pretrained pipeline and sbertresolve_ner_model_finder resolver model with 70 clinical NER models and their labels. See Models Hub Page for more details and the Pretrained Clinical Pipelines Notebook for the examples. Support Different Version and Year Combinations on Medicare Risk Adjustment Score Calculation Module Now, you can calculate CMS-HCC risk score with different version and year combinations by importing one of the following function calculate the score. - profileV2217 - profileV2318 - profileV2417 - profileV2218 - profileV2319 - profileV2418 - profileV2219 - profileV2419 - profileV2220 - profileV2420 - profileV2221 - profileV2421 - profileV2222 - profileV2422 from sparknlp_jsl.functions import profileV24Y20 See the notebook for more details. Core Improvements and Bug Fixes ContextualParserApproach: New parameter completeContextMatch. This parameter let the user define whether to do an exact match of prefix and suffix. Deidentification: Enhanced default regex rules in French deidentification for DATE entity extraction. ZeroShotRelationExtractionModel: Fixed the issue that setting some parameters together and no need to setRelationalCategories after downloading the model. New and Updated Notebooks New MedicalBertForSequenceClassification Notebook to show how to use MedicalBertForSequenceClassification models. New ALAB Module Notebook to show all features of ALAB Module. New Oncology Models Notebook to show the examples of the new Oncology models. Updated Medicare Risk Adjustment Score Calculation Notebook with the new changes in HCC score calculation functions. Updated Clinical DeIdentification Notebook by adding how not to deidentify a part of an entity section and showing examples of shifting days feature with the new DocumentHashCoder. Updated Pretrained Clinical Pipelines Notebook with the updated ner_model_finder results. 50+ New Clinical Models and Pipelines Added &amp; Updated in Total assertion_jsl_augmented rxnorm_normalized_mapper ner_model_finder sbertresolve_ner_model_finder sbiobertresolve_icd9 icd9_resolver_pipeline rxnorm_normalized_mapper icd9_mapper icd10_icd9_mapper icd9_icd10_mapper icd10_icd9_mapping bert_qa_spanbert_finetuned_squadv1 ner_oncology_therapy_wip ner_oncology_diagnosis_wip ner_oncology_wip ner_oncology_tnm_wip ner_oncology_anatomy_general_wip ner_oncology_demographics_wip ner_oncology_test_wip ner_oncology_unspecific_posology_wip ner_oncology_anatomy_granular_wip ner_oncology_response_to_treatment_wip ner_oncology_biomarker_wip ner_oncology_posology_wip bert_token_classifier_ner_oncology_therapy_wip bert_token_classifier_ner_oncology_diagnosis_wip bert_token_classifier_ner_oncology_wip bert_token_classifier_ner_oncology_tnm_wip bert_token_classifier_ner_oncology_anatomy_general_wip bert_token_classifier_ner_oncology_demographics_wip bert_token_classifier_ner_oncology_test_wip bert_token_classifier_ner_oncology_unspecific_posology_wip bert_token_classifier_ner_oncology_anatomy_granular_wip bert_token_classifier_ner_oncology_response_to_treatment_wip bert_token_classifier_ner_oncology_biomarker_wip bert_token_classifier_ner_oncology_posology_wip assertion_oncology_wip assertion_oncology_problem_wip assertion_oncology_treatment_wip assertion_oncology_response_to_treatment_wip assertion_oncology_test_binary_wip assertion_oncology_smoking_status_wip assertion_oncology_family_history_wip assertion_oncology_demographic_binary_wip re_oncology_size_wip re_oncology_biomarker_result_wip re_oncology_granular_wip re_oncology_location_wip re_oncology_temporal_wip re_oncology_test_result_wip re_oncology_wip redl_oncology_size_biobert_wip redl_oncology_biomarker_result_biobert_wip redl_oncology_location_biobert_wip redl_oncology_temporal_biobert_wip redl_oncology_test_result_biobert_wip redl_oncology_biobert_wip redl_oncology_granular_biobert_wip Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_2_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_2_0"
  },
  "240": {
    "id": "240",
    "title": "Annotation Lab Release Notes 4.2.0",
    "content": "4.2.0 Release date: 02-11-2022 Annotation Lab 4.2.0 supports projects combining models trained with multiple embeddings for preannotation as well as predefined Demo projects that can be imported with the click of a button for easy experimentations and features testing. The Project Configuration page now has a new “View” step to configure the layout of the Labeling page. The release also includes stabilization and fixes bugs reported by our user community. Here are the highlights of this release: Highlights Projects can reuse and combine models trained with different embeddings for pre-annotation. Now, it is easily possible to use models with different embeddings and deploy them as part of the same pre-annotation server. In the customize configuration page all the added models and their embeddings are listed. The list makes it easier for the user to delete the labels of a specific model. Demo Projects can be imported for experiments. To allow users access and experiment with already configured and populated projects we have added the option to import predefined Demo projects. This is for helping users understand the various features offered by the Annotation Lab. The user can import demo projects from the Import Project window, by clicking on the Import Demo Project option. Visual Update of the Annotation Screen Layout from the View Tab. A new tab - “View” - has been added to the project setup wizard after the “Content Type” selection tab. This gives users the ability to set different layouts based on their needs and preferences. Support for Granular License Scopes. This versions brings support for more granular license scopes such as Healthcare: Inference, Healthcare: Training, OCR: Inference or OCR: Training. This is in line with the latest developments of the John Snow Labs licenses. Easy Reuse and Editing of Pre-annotations. For an improved usability, when pre-annotations are available for a task, those will be shown by default when accessing the labeling screen. Users can filter them based on the confidence score and the either accept the visible annotations as a new submitted completion or start editing those as part of a new completion. Easy Export of Large Visual NER Projects. From version 4.2.0 users will be able to export large NER/ Visual NER projects with a size bigger than 500 MB. Smaller Project Tiles on the Projects Dashboard. The size of a project tile was compacted in this version in order to increase the number of project cards that could be displayed on the screen at one time. Confusion Matrix in Training Logs for NER projects. With the addition of confusion matrix it will be easier to understand the performance of the model and judge whether the model is underfitting or overfitting. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_4_2_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_4_2_0"
  },
  "241": {
    "id": "241",
    "title": "Spark NLP release notes 4.2.0",
    "content": "4.2.0 Release date: 31-10-2022 We are glad to announce that Spark OCR 4.2.0 has been released. This is mostly a compatibility release to ensure compatibility of Spark OCR against Spark NLP 4.2.1, and Spark NLP Healthcare 4.2.1. Improvements Improved memory consumption and performance in the training of Visual NER models. New Features PdfToForm new param: useFullyQualifiedName, added capability to return fully qualified key names. New or Updated Notebooks SparkOcrProcessMultiplepageScannedPDF.ipynb has been added to show how to serve a multi-page document processing pipeline. SparkOcrDigitalFormRecognition.ipynb has been updated to show utilization of useFullyQualifiedName parameter. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_4_2_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_4_2_0"
  },
  "242": {
    "id": "242",
    "title": "Spark NLP for Healthcare Release Notes 4.2.1",
    "content": "4.2.1 Highlights Creating new chunks with NerConverterInternal by merging chunks by skipping stopwords in between. Adding relation direction to RelationExtraction models to make the relations direction-aware. Using proper regional date formats in the DeIdentification module. Being able to play with different date formats in DateNormalizer output. New Replacer annotator to replace chunks with their normalized versions (`DateNormalizer’) in documents. New ModelTracer helper class to generate and add model UID and timestamps of the stages in a pipeline Added entity source and labels to the AssertionFilterer metadata New chunk mapper and sentence entity resolver models and a pipeline for CVX Updated clinical NER models with new labels New Certification Training notebooks for the johnsnowlabs library New and updated notebooks 6 new clinical models and pipelines added &amp; updated in total Creating New Chunks with NerConverterInternal by Merging Chunks by Skipping Stopwords in Between. NerConverterInternal’s new setIgnoreStopWords parameter allows merging between chunks with the same label, ignoring stopwords and punctuations. txt = &quot;&quot;&quot; The qualified manufacturers for this starting material are: Alpha Chemicals Pvt LTD 17, R K Industry House, Walbhat Rd, Goregaon – 400063 Mumbai, Maharashtra, India Beta Chemical Co., Ltd Huan Cheng Xi Lu 3111hao Hai Guan Da Ting Shanghai, China &quot;&quot;&quot; Example for default: NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_deid&quot;]) .setOutputCol(&quot;chunk_deid&quot;) .setGreedyMode(True) .setWhiteList([&#39;LOCATION&#39;]) Results: | chunks | entities | begin | end | |:-|:|:|-:| | R K Industry House | LOCATION | 90 | 107 | | Walbhat | LOCATION | 110 | 116 | | Mumbai | LOCATION | 141 | 146 | | Maharashtra | LOCATION | 149 | 159 | | India | LOCATION | 162 | 166 | | Huan Cheng Xi Lu 3111hao | LOCATION | 191 | 214 | | Shanghai | LOCATION | 234 | 241 | | China | LOCATION | 244 | 248 | Example for setting setIgnoreStopWords parameter: NerConverterInternal() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_deid&quot;]) .setOutputCol(&quot;chunk_deid&quot;) .setGreedyMode(True) .setWhiteList([&#39;LOCATION&#39;]) .setIgnoreStopWords([&#39; n&#39;, &#39;,&#39;, &quot;and&quot;, &#39;or&#39;, &#39;.&#39;]) Results: | chunks | entities | begin | end | |:|:|:|-:| | R K Industry House Walbhat | LOCATION | 90 | 116 | | Mumbai Maharashtra India | LOCATION | 141 | 166 | | Huan Cheng Xi Lu 3111hao | LOCATION | 191 | 214 | | Shanghai China | LOCATION | 234 | 248 | Adding Relation Direction to RelationExtraction Models to Make the Relations Direction-aware. We have a new setRelationDirectionCol parameter that is used during training with a new separate column that specified relationship directions. The column should contain one of the following values: rightwards: The first entity in the text is also the first argument of the relation (as well as the second entity in the text is the second argument). In other words, the relation arguments are ordered left to right in the text. leftwards: The first entity in the text is the second argument of the relation (and the second entity in the text is the first argument). both: Order doesn’t matter (relation is symmetric). In our test cases, it was observed that the accuracy increased significantly when we just add setRelationDirectionCol parameter by keeping the other parameter as they are. Example: +--+++--+-+-+ | chunk1| label1| label2| chunk2| rel| rel_dir| +--+++--+-+-+ |expected long ter...|treatment|treatment| a picc line| O| both| | light-headedness| problem| problem| diaphoresis| PIP|rightwards| | po pain medications|treatment| problem| his pain|TrAP| leftwards| |bilateral pleural...| problem| problem|increased work of...| PIP|rightwards| | her urine output| test| problem| decreased|TeRP|rightwards| |his psychiatric i...| problem| problem|his neurologic in...| PIP|rightwards| | white blood cells| test| test| red blood cells| O| both| | chloride| test| test| bun| O| both| | further work-up| test| problem|his neurologic co...|TeCP|rightwards| | four liters|treatment| test| blood pressure| O| both| +--+++--+-+-+ re_approach_with_dir = RelationExtractionApproach() .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setLabelColumn(&quot;rel&quot;) ... .setRelationDirectionCol(&quot;rel_dir&quot;) Using Proper Regional date Formats in DeIdentification Module You can specify the format for date entities that will be shifted to the new date or converted to a year. de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;dei_id&quot;) .setRegion(&#39;us&#39;) # &#39;eu&#39; for Europe Being Able to Play With Different Date Formats in DateNormalizer Output Now we can customize the normalized date formats in the output of DateNormalizer by using the new setOutputDateformat parameter. There are two options to do that; us for MM/DD/YYYY, eu for DD/MM/YYYY formats. Example: date_normalizer_us = DateNormalizer() .setInputCols(&#39;date_chunk&#39;) .setOutputCol(&#39;normalized_date_us&#39;) .setOutputDateformat(&#39;us&#39;) date_normalizer_eu = DateNormalizer() .setInputCols(&#39;date_chunk&#39;) .setOutputCol(&#39;normalized_date_eu&#39;) .setOutputDateformat(&#39;eu&#39;) sample_text = [&#39;She was last seen in the clinic on Jan 30, 2018, by Dr. Y.&#39;, &#39;Chris Brown was discharged on 12Mar2021&#39;, &#39;We reviewed the pathology obtained on 13.04.1999.&#39;] Results: +-++++ |text |date_chunk |normalized_date_eu|normalized_date_us| +-++++ |She was last seen in the clinic on Jan 30, 2018, by Dr. Y.|Jan 30, 2018|30/01/2018 |01/30/2018 | |Chris Brown was discharged on 12Mar2021 |12Mar2021 |12/03/2021 |03/20/2021 | |We reviewed the pathology obtained on 13.04.1999. |13.04.1999 |13/04/1999 |04/13/1999 | +-++++ New Replacer Annotator To Replace Chunks With Their Normalized Versions (DateNormalizer) In Documents We have a new Replacer annotator that returns the original document by replacing it with the normalized version of the original chunks. Example: date_normalizer = DateNormalizer() .setInputCols(&#39;date_chunk&#39;) .setOutputCol(&#39;normalized_date&#39;) replacer = Replacer() .setInputCols([&quot;normalized_date&quot;,&quot;document&quot;]) .setOutputCol(&quot;replaced_document&quot;) sample_text = [&#39;She was last seen in the clinic on Jan 30, 2018, by Dr. Y.&#39;, &#39;Chris Brown was discharged on 12Mar2021&#39;, &#39;We reviewed the pathology obtained on 13.04.1999.&#39;] Results: +-++--+ |text |normalized_date|replaced_document | +-++--+ |She was last seen in the clinic on Jan 30, 2018, by Dr. Y.|2018/01/30 |She was last seen in the clinic on 2018/01/30, by Dr. Y.| |Chris Brown was discharged on 12Mar2021 |2021/03/12 |Chris Brown was discharged on 2021/03/12 | |We reviewed the pathology obtained on 13.04.1999. |1999/04/13 |We reviewed the pathology obtained on 1999/04/13. | +-++--+ New ModelTracer Helper Class to Generate and Add Model UID and Timestamps of the Stages in a Pipeline ModelTracer allows to track the UIDs and timestamps of each stage of a pipeline. Example: from sparknlp_jsl.modelTracer import ModelTracer ... pipeline = Pipeline( stages=[ documentAssembler, tokenizer, tokenClassifier, ]) df = pipeline.fit(data).transform(data) result = ModelTracer().addUidCols(pipeline = pipeline, df = df) result.show(truncate=False) Results: +-+--+--++-+--+-+ |text|document|token|ner|documentassembler_model_uid |tokenizer_model_uid |bert_for_token_classification_model_uid | +-+--+--++-+--+-+ |... |... |... |...|{uid -&gt; DocumentAssembler_a666efd1d789, timestamp -&gt; 2022-10-21_11:34}|{uid -&gt; Tokenizer_01fbad79f069, timestamp -&gt; 2022-10-21_11:34}|{uid -&gt; BERT_FOR_TOKEN_CLASSIFICATION_675a6a750b89, timestamp -&gt; 2022-10-21_11:34}| +-+--+--++-+--+-+ Added Entity Source and Labels to the AssertionFilterer Metadata Now the AssertionFilterer annotator returns the entity source and assertion labels in the metadata. Example: assertionFilterer = AssertionFilterer() .setInputCols([&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;]) .setOutputCol(&quot;filtered&quot;) .setCriteria(&quot;assertion&quot;) .setWhiteList([&quot;Absent&quot;]) text = &quot;Patient has a headache for the last 2 weeks, no alopecia noted.&quot; Results: # before v4.2.1 +--+ |filtered | +--+ |[{chunk, 48, 55, alopecia, {entity -&gt; PROBLEM, sentence -&gt; 0, chunk -&gt; 1, confidence -&gt; 0.9988}, []}]| +--+ # v4.2.1 ++ |filtered | ++ |[{chunk, 48, 55, alopecia, {chunk -&gt; 1, confidence -&gt; 0.9987, ner_source -&gt; ner_chunk, assertion -&gt; Absent, entity -&gt; PROBLEM, sentence -&gt; 0}, []}]| ++ New Chunk Mapper and Sentence Entity Resolver Models And A Pipeline for CVX We are releasing 2 new chunk mapper models to map entities to their corresponding CVX codes, vaccine names and CPT codes. There are 3 types of vaccine names mapped; short_name, full_name and trade_name model name description cvx_name_mapper Mapping vaccine products to their corresponding CVX codes, vaccine names and CPT codes. cvx_code_mapper Mapping CVX codes to their corresponding vaccine names and CPT codes. Example: chunkerMapper = ChunkMapperModel .pretrained(&quot;cvx_name_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRels([&quot;cvx_code&quot;, &quot;short_name&quot;, &quot;full_name&quot;, &quot;trade_name&quot;, &quot;cpt_code&quot;]) data = spark.createDataFrame([[&#39;DTaP&#39;], [&#39;MYCOBAX&#39;], [&#39;cholera, live attenuated&#39;]]).toDF(&#39;text&#39;) Results: +--+--+--+-++--+ |chunk |cvx_code|short_name |full_name |trade_name |cpt_code| +--+--+--+-++--+ |[DTaP] |[20] |[DTaP] |[diphtheria, tetanus toxoids and acellular pertussis vaccine]|[ACEL-IMUNE]|[90700] | |[MYCOBAX] |[19] |[BCG] |[Bacillus Calmette-Guerin vaccine] |[MYCOBAX] |[90585] | |[cholera, live attenuated]|[174] |[cholera, live attenuated]|[cholera, live attenuated] |[VAXCHORA] |[90625] | +--+--+--+-++--+ sbiobertresolve_cvx: This sentence entity resolver model maps vaccine entities to CVX codes using sbiobert_base_cased_mli Sentence Bert Embeddings. Additionally, this model returns status of the vaccine (Active/Inactive/Pending/Non-US) in all_k_aux_labels column. Example: cvx_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_cvx&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;cvx_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) result = light_model.fullAnnotate([&quot;Sinovac&quot;, &quot;Moderna&quot;, &quot;BIOTHRAX&quot;]) Results: +-+--+-+--+ |ner_chunk |cvx_code|resolved_text |Status | +-+--+-+--+ |Sinovac |511 |COVID-19 IV Non-US Vaccine (CoronaVac, Sinovac) |Non-US | |Moderna |227 |COVID-19, mRNA, LNP-S, PF, pediatric 50 mcg/0.5 mL dose|Inactive| |BIOTHRAX |24 |anthrax |Active | +-+--+-+--+ cvx_resolver_pipeline: This pretrained pipeline maps entities with their corresponding CVX codes. Example: from sparknlp.pretrained import PretrainedPipeline resolver_pipeline = PretrainedPipeline(&quot;cvx_resolver_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) text= &quot;The patient has a history of influenza vaccine, tetanus and DTaP&quot; result = resolver_pipeline.fullAnnotate(text) Results: +--++--+ |chunk |ner_chunk|cvx_code| +--++--+ |influenza vaccine|Vaccine |160 | |tetanus |Vaccine |35 | |DTaP |Vaccine |20 | +--++--+ Updated Clinical NER Models With New Labels ner_jsl and ner_covid_trials models were updated with the new label called “Vaccine_Name”. Example: ... jsl_ner = MedicalNerModel.pretrained(&quot;ner_jsl&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;jsl_ner&quot;) ... sample_text= &quot;&quot;&quot;The patient is a 21-day-old Caucasian male here for 2 days, there is no side effect observed after the influenza vaccine&quot;&quot;&quot; Results: |chunks | begin | end | entities | ||--:|:|:| |21-day-old | 18 | 27 | Age | |Caucasian | 29 | 37 | Race_Ethnicity | |male | 39 | 42 | Gender | |for 2 days | 49 | 58 | Duration | |influenza vaccine | 100 | 116 | Vaccine_Name | New Certification Training Notebooks for the johnsnowlabs Library Now we have 46 new Healtcare Certification Training notebooks for the users who want to use the new johnsnowlabs library. New and Updated Notebooks New Coreference Resolution notebook to find other references of clinical entities in a document. Updated Clinical Name Entity Recognition Model notebook with the new feature setIgnoreStopWords parameter and ModelTracer module. Updated Clinical Assertion Model notebook with the new changes in AssertionFilterer improvement. Updated Clinical Deidentification notebook with the new setRegion parameter in DeIdentification. Updated Clinical Relation Extraction notebook with the new setRelationDirectionCol parameter in RelationExtractionApproach. Updated Date Normalizer notebook with the new setOutputDateformat parameter in DateNormalizer and Replacer annotator. Updated 25 Certification Training Public notebooks and 47 Certification Training Healthcare notebooks with the latest updates in the libraries. Updated 6 Databricks Public notebooks and 14 Databricks Healthcare notebooks with the latest updates in the libraries and 4 new Databricks notebooks created. 6 New Clinical Models and Pipelines Added &amp; Updated in Total cvx_code_mapper cvx_name_mapper sbiobertresolve_cvx cvx_resolver_pipeline ner_jsl ner_covid_trials Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_2_1",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_2_1"
  },
  "243": {
    "id": "243",
    "title": "Visual NLP(Spark OCR) release notes 4.2.1",
    "content": "4.2.1 Release date: 11-28-2022 We’re glad to announce that Spark-OCR 4.2.1 has been released! This release is almost completely about LightPipelines. LightPipeline added to Spark-OCR Originally introduced by Spark-NLP, this has been one of the most celebrated features by our users. In a nutshell, LightPipelines allow you switching your pipeline from distributed processing to local mode, in a single line of code. Also, results are much easier to post-process as they come in plain Python data structures. Now, LightPipelines are available in Spark-OCR as well! This is an initial implementation only covering three of our most popular annotators: ImageToText, PdfToImage, and BinaryToImage. Although not all the annotators from Spark-OCR are included in this initial release, a number of interesting features are being delivered: Latency has been dramatically reduced for small input dataset sizes. Interoperability with Spark-NLP and Spark-NLP healthcare: you can mix any NLP annotator with supported OCR annotators on the same LightPipeline. Following is a chart comparing performance of different techniques on batches of different page counts: 8, 16, 24, 32, 40, 48, and 80 pages. For the 8 pages case, on the left side of the chart, LightPipelines average 1.25s per page vs. 4s per page that were scored by a similar Pytesseract implementation. That makes LightPipelines a great candidate to achieve low latency on small sized batches, while still leveraging parallelism. Korean Support You can start using Korean language by just passing the ‘KOR’ option to ImageToText, ... # Run OCR ocr = ImageToText() # Set Korean language ocr.setLanguage(Language.KOR) # Download model from JSL S3 ocr.setDownloadModelData(True) Bug Fixes AlabReader has been updated to handle the new structure present in Annotation Lab’s exported annotations. New Notebooks Check how to use LightPipelines in this notebook: SparkOcrLightPipelines.ipynb Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_4_2_1",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_4_2_1"
  },
  "244": {
    "id": "244",
    "title": "Spark NLP for Healthcare Release Notes 4.2.2",
    "content": "4.2.2 Highlights Fine-tuning Relation Extraction models with your data Added Romanian support in deidentification annotator for data obfuscation New SDOH (Social Determinants of Health) ner model Improved oncology models and 4 pretrained pipelines New chunk mapper models to map entities (phrases) to their corresponding ICD-10-CM codes as well as clinical abbreviations to their definitions New ICD-10-PCS sentence entity resolver model and ICD-10-CM resolver pipeline New utility &amp; helper modules documentation page New and updated notebooks 22 new clinical models and pipelines added &amp; updated in total Fine-Tuning Relation Extraction Models With Your Data Instead of starting from scratch when training a new Relation Extraction model, you can train a new model by adding your new data to the pretrained model. There are two new params in RelationExtractionApproach which allows you to initialize your model with the data from the pretrained model: setPretrainedModelPath: This parameter allows you to point the training process to an existing model. setОverrideExistingLabels: This parameter overrides the existing labels in the original model that are assigned the same output nodes in the new model. Default is True, when it is set to False the RelationExtractionApproach uses the existing labels and if it finds new ones it tries to assign them to unused output nodes. Example: reApproach_finetune = RelationExtractionApproach() .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setLabelColumn(&quot;rel&quot;) ... .setFromEntity(&quot;begin1i&quot;, &quot;end1i&quot;, &quot;label1&quot;) .setToEntity(&quot;begin2i&quot;, &quot;end2i&quot;, &quot;label2&quot;) .setPretrainedModelPath(&quot;existing_RE_MODEL_path&quot;) .setOverrideExistingLabels(False) You can check Resume RelationExtractionApproach Training Notebook for more examples. Added Romanian Support in Deidentification Annotator For Data Obfuscation Deidentification annotator is now able to obfuscate entities (coming from a deid NER model) with fake data in Romanian language. Example: deid_obfuscated_faker = DeIdentification() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_chunk&quot;]) .setOutputCol(&quot;obfuscated&quot;) .setMode(&quot;obfuscate&quot;) .setLanguage(&#39;ro&#39;) .setObfuscateDate(True) .setObfuscateRefSource(&#39;faker&#39;) text = &quot;&quot;&quot;Nume si Prenume : BUREAN MARIA, Varsta: 77 ,Spitalul Pentru Ochi de Deal, Drumul Oprea Nr. 972 Vaslui&quot;&quot;&quot; Result: Sentence Masked with entity Masked with Chars Masked with Fixed Chars Obfuscated Nume si Prenume : BUREAN MARIA, Varsta: 77 ,Spitalul Pentru Ochi de Deal, Drumul Oprea Nr. 972 Vaslui Nume si Prenume : &lt; PATIENT&gt;, Varsta: &lt; AGE&gt; ,&lt; HOSPITAL&gt;, &lt; STREET&gt; &lt; CITY&gt; Nume si Prenume : ****, Varsta: ** ,********, ****** ** Nume si Prenume : **, Varsta: ** , **, ** ** Nume si Prenume : Claudia Crumble, Varsta: 18 ,LOS ANGELES AMBULATORY CARE CENTER, 706 north parrish avenue Piscataway New SDOH (Social Determinants of Health) NER Model Social Determinants of Health(SDOH) are the socioeconomic factors under which people live, learn, work, worship, and play that determine their health outcomes.The World Health Organization also provides a definition of social determinants of health. Social determinants of health as the conditions in which people are born, grow, live, work and age. These circumstances are shaped by the distribution of money, power, and resources at global, national, and local levels. Social determinants of health (SDOH) have a major impact on people’s health, well-being, and quality of life. SDOH include lots of factors, also contribute to wide health disparities and inequities. In this project We have tried to define well these factors. The goal of this project is to train models for natural language processing focused on extracting terminology related to social determinants of health from various kinds of biomedical documents. This first model is Named Entity Recognition (NER) task. The project is still ongoing and will mature over time and the number of sdoh factors (entities) will also be enriched. It will include other tasks as well. Example: ner_model = MedicalNerModel.pretrained(&quot;sdoh_slim_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) text = &quot;&quot;&quot; Mother states that he does smoke, there is a family hx of alcohol on both maternal and paternal sides of the family, maternal grandfather who died of alcohol related complications and paternal grandmother with severe alcoholism. Pts own drinking began at age 16, living in LA, had a DUI at age 17 after totaling a new car that his mother bought for him, he was married. &quot;&quot;&quot; Result: +-+-+ | token| ner_label| +-+-+ | Mother| B-Family_Member| | he| B-Gender| | smoke| B-Smoking| | alcohol| B-Alcohol| | maternal| B-Family_Member| | paternal| B-Family_Member| | maternal| B-Family_Member| | grandfather| B-Family_Member| | alcohol| B-Alcohol| | paternal| B-Family_Member| | grandmother| B-Family_Member| | severe| B-Alcohol| | alcoholism| I-Alcohol| | drinking| B-Alcohol| | age| B-Age| | 16| I-Age| | LA|B-Geographic_Entity| | age| B-Age| | 17| I-Age| | his| B-Gender| | mother| B-Family_Member| | him| B-Gender| | he| B-Gender| | married| B-Marital_Status| +-+-+ Improved Oncology NER Models And 4 New Pretrained Pipelines We are releasing the improved version of Oncological NER models (_wip) and 4 new pretrained oncological pipelines which are able to detect assertion status and relations between the extracted oncological entities. NER model name (MedicalNerModel) description predicted entities ner_oncology_anatomy_general Extracting anatomical entities. Anatomical_Site, Direction ner_oncology_anatomy_granular Extracting anatomical entities using granular labels. Direction, Site_Lymph_Node, Site_Breast, Site_Other_Body_Part, Site_Bone, Site_Liver, Site_Lung, Site_Brain ner_oncology_biomarker Extracting biomarkers and their results. Biomarker, Biomarker_Result ner_oncology_demographics Extracting demographic information, including smoking status. Age, Gender, Smoking_Status, Race_Ethnicity ner_oncology_diagnosis Extracting entities related to cancer diagnosis, including the presence of metastasis. Grade, Staging, Tumor_Size, Adenopathy, Pathology_Result, Histological_Type, Metastasis, Cancer_Score, Cancer_Dx, Invasion, Tumor_Finding, Performance_Status ner_oncology Extracting more than 40 oncology-related entities. Histological_Type, Direction, Staging, Cancer_Score, Imaging_Test, Cycle_Number, Tumor_Finding, Site_Lymph_Node, Invasion, Response_To_Treatment, Smoking_Status, Tumor_Size, Cycle_Count, Adenopathy, Age, Biomarker_Result, Unspecific_Therapy, Site_Breast, Chemotherapy, Targeted_Therapy, Radiotherapy, Performance_Status, Pathology_Test, Site_Other_Body_Part, Cancer_Surgery, Line_Of_Therapy, Pathology_Result, Hormonal_Therapy, Site_Bone, Biomarker, Immunotherapy, Cycle_Day, Frequency, Route, Duration, Death_Entity, Metastasis, Site_Liver, Cancer_Dx, Grade, Date, Site_Lung, Site_Brain, Relative_Date, Race_Ethnicity, Gender, Oncogene, Dosage, Radiation_Dose ner_oncology_posology This model extracts oncology specific posology information and cancer therapies. Cycle_Number, Cycle_Count, Radiotherapy, Cancer_Surgery, Cycle_Day, Frequency, Route, Cancer_Therapy, Duration, Dosage, Radiation_Dose ner_oncology_unspecific_posology Extracting any mention of cancer therapies and posology information using general labels Cancer_Therapy, Posology_Information ner_oncology_response_to_treatment_wip Extracting entities related to the patient’s response to cancer treatment. Response_To_Treatment, Size_Trend, Line_Of_Therapy ner_oncology_therapy Extracting entities related to cancer therapies, including posology entities and response to treatment, using granular labels. Response_To_Treatment, Line_Of_Therapy, Cancer_Surgery, Radiotherapy, Immunotherapy, Targeted_Therapy, Hormonal_Therapy, Chemotherapy, Unspecific_Therapy, Route, Duration, Cycle_Count, Dosage, Frequency, Cycle_Number, Cycle_Day, Radiation_Dose ner_oncology_test Extracting mentions of oncology-related tests. Oncogene, Biomarker, Biomarker_Result, Imaging_Test, Pathology_Test ner_oncology_tnm Extracting mentions related to TNM staging. Lymph_Node, Staging, Lymph_Node_Modifier, Tumor_Description, Tumor, Metastasis, Cancer_Dx Oncological Pipeline (PretrainedPipeline) Description oncology_general_pipeline Includes Named-Entity Recognition, Assertion Status and Relation Extraction models to extract information from oncology texts. This pipeline extracts diagnoses, treatments, tests, anatomical references and demographic entities. oncology_biomarker_pipeline Includes Named-Entity Recognition, Assertion Status and Relation Extraction models to extract information from oncology texts. This pipeline focuses on entities related to biomarkers oncology_diagnosis_pipeline Includes Named-Entity Recognition, Assertion Status, Relation Extraction and Entity Resolution models to extract information from oncology texts. This pipeline focuses on entities related to oncological diagnosis. oncology_therapy_pipeline Includes Named-Entity Recognition and Assertion Status models to extract information from oncology texts. This pipeline focuses on entities related to therapies. Example: from sparknlp.pretrained import PretrainedPipeline pipeline = PretrainedPipeline(&quot;oncology_general_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) text = &quot;The patient underwent a left mastectomy for a left breast cancer two months ago. The tumor is positive for ER and PR.&quot; Result: **** ner_oncology_wip results **** | chunk | ner_label | |:|:--| | left | Direction | | mastectomy | Cancer_Surgery | | left | Direction | | breast cancer | Cancer_Dx | | two months ago | Relative_Date | | tumor | Tumor_Finding | | positive | Biomarker_Result | | ER | Biomarker | | PR | Biomarker | **** assertion_oncology_wip results **** | chunk | ner_label | assertion | |:--|:|:| | mastectomy | Cancer_Surgery | Past | | breast cancer | Cancer_Dx | Present | | tumor | Tumor_Finding | Present | | ER | Biomarker | Present | | PR | Biomarker | Present | **** re_oncology_wip results **** | chunk1 | entity1 | chunk2 | entity2 | relation | |:--|:--|:|:--|:--| | mastectomy | Cancer_Surgery | two months ago | Relative_Date | is_related_to | | breast cancer | Cancer_Dx | two months ago | Relative_Date | is_related_to | | tumor | Tumor_Finding | ER | Biomarker | O | | tumor | Tumor_Finding | PR | Biomarker | O | | positive | Biomarker_Result | ER | Biomarker | is_related_to | | positive | Biomarker_Result | PR | Biomarker | is_related_to | New Chunk Mapper Models to Map Entities (phrases) to Their Corresponding ICD-10-CM Codes As Well As Clinical Abbreviations to Their Definitions We have 2 new chunk mapper models: abbreviation_mapper_augmented is an augmented version of the existing abbreviation_mapper model. It maps abbreviations and acronyms of medical regulatory activities to their definitions. icd10cm_mapper maps entities to corresponding ICD-10-CM codes. Example: chunkerMapper = ChunkMapperModel .pretrained(&quot;icd10cm_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRels([&quot;icd10cm_code&quot;]) text = &quot;&quot;&quot;A 35-year-old male with a history of primary leiomyosarcoma of neck, gestational diabetes mellitus diagnosed eight years prior to presentation and presented with a one-week history of polydipsia, poor appetite, and vomiting.&quot;&quot;&quot; Result: ++-++ |ner_chunk |entity |icd10cm_code| ++-++ |primary leiomyosarcoma of neck|PROBLEM|C49.0 | |gestational diabetes mellitus |PROBLEM|O24.919 | |polydipsia |PROBLEM|R63.1 | |poor appetite |PROBLEM|R63.0 | |vomiting |PROBLEM|R11.10 | ++-++ New ICD-10-PCS Sentence Entity Resolver Model and ICD-10-CM Resolver Pipeline We are releasing new ICD-10-PCS resolver model and ICD-10-CM resolver pipeline: sbiobertresolve_icd10pcs_augmented model maps extracted medical entities to ICD-10-PCS codes using sbiobert_base_cased_mli sentence bert embeddings. It trained on the augmented version of the dataset which is used in previous ICD-10-PCS resolver model. Example: icd10pcs_resolver = SentenceEntityResolverModel .pretrained(&quot;sbiobertresolve_icd10pcs_augmented&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;, &quot;sbert_embeddings&quot;]) .setOutputCol(&quot;resolution&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) text = &quot;Given the severity of her abdominal examination and her persistence of her symptoms, it is detected that need for laparoscopic appendectomy and possible open appendectomy as well as pyeloplasty. We recommend performing a mediastinoscopy&quot; Result: +-++-++--+ | ner_chunk| entity|icd10pcs_code| resolutions| all_codes| +-++-++--+ | abdominal examination| Test| 2W63XZZ|[traction of abdominal wall [trac...|[2W63XZZ, BW40ZZZ...| |laparoscopic appendectomy|Procedure| 0DTJ8ZZ|[resection of appendix, endo [res...|[0DTJ8ZZ, 0DT84ZZ...| | open appendectomy|Procedure| 0DBJ0ZZ|[excision of appendix, open appro...|[0DBJ0ZZ, 0DTJ0ZZ...| | pyeloplasty|Procedure| 0TS84ZZ|[reposition bilateral ureters, pe...|[0TS84ZZ, 0TS74ZZ...| | mediastinoscopy|Procedure| BB1CZZZ|[fluoroscopy of mediastinum [fluo...|[BB1CZZZ, 0WJC4ZZ...| +-++-++--+ icd10cm_resolver_pipeline pretrained pipeline maps entities with their corresponding ICD-10-CM codes. You’ll just feed your text and it will return the corresponding ICD-10-CM codes. Example: from sparknlp.pretrained import PretrainedPipeline resolver_pipeline = PretrainedPipeline(&quot;icd10cm_resolver_pipeline&quot;, &quot;en&quot;, &quot;clinical/models&quot;) text = &quot;A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years and anisakiasis. Also, it was reported that fetal and neonatal hemorrhage&quot; Result: +--+++ |chunk |ner_chunk|icd10cm_code| +--+++ |gestational diabetes mellitus|PROBLEM |O24.919 | |anisakiasis |PROBLEM |B81.0 | |fetal and neonatal hemorrhage|PROBLEM |P545 | +--+++ New Utility &amp; Helper Modules Documentation Page We have a new utility &amp; helper modules documentation page that you can find the documentations of Spark NLP for Healthcare modules with examples. New and Updated Notebooks New Resume RelationExtractionApproach Training notebook train a model already trained on a different dataset. Updated Clinical Deidentification notebook with day shifting feature in DeIdentification. Updated Clinical Multi Language Deidentification notebook with new Romanian obfuscation and faker improvement. Updated Adverse Drug Event ADE NER and Classifier notebook with the new models and improvement. 22 New Clinical Models and Pipelines Added &amp; Updated in Total abbreviation_mapper_augmented icd10cm_mapper sbiobertresolve_icd10pcs_augmented icd10cm_resolver_pipeline oncology_biomarker_pipeline oncology_diagnosis_pipeline oncology_therapy_pipeline oncology_general_pipeline ner_oncology_anatomy_general ner_oncology_anatomy_granular ner_oncology_biomarker ner_oncology_demographics ner_oncology_diagnosis ner_oncology ner_oncology_posology ner_oncology_response_to_treatment ner_oncology_test ner_oncology_therapy ner_oncology_tnm ner_oncology_unspecific_posology sdoh_slim_wip t5_base_pubmedqa For all Spark NLP for healthcare models, please check: Models Hub Page Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_2_2",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_2_2"
  },
  "245": {
    "id": "245",
    "title": "Spark NLP for Healthcare Release Notes 4.2.3",
    "content": "4.2.3 Highlights 3 new chunk mapper models to mapping Drugs and Diseases from the KEGG Database as well as mapping abbreviations to their categories New utility &amp; helper Relation Extraction modules to handle preprocess New utility &amp; helper OCR modules to handle annotate New utility &amp; helper NER log parser Adding flexibility chunk merger prioritization Core improvements and bug fixes New and updated notebooks 3 new clinical models and pipelines added &amp; updated in total 3 New Hhunk Mapper Models to Mapping Drugs and Diseases from the KEGG Database as well as Mapping Abbreviations to Their Categories kegg_disease_mapper: This pretrained model maps diseases with their corresponding category, description, icd10_code, icd11_code, mesh_code, and hierarchical brite_code. This model was trained with the data from the KEGG database. Example: chunkerMapper = ChunkMapperModel.pretrained(&quot;kegg_disease_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRels([&quot;description&quot;, &quot;category&quot;, &quot;icd10_code&quot;, &quot;icd11_code&quot;, &quot;mesh_code&quot;, &quot;brite_code&quot;]) text= &quot;A 55-year-old female with a history of myopia, kniest dysplasia and prostate cancer. She was on glipizide , and dapagliflozin for congenital nephrogenic diabetes insipidus.&quot; Result: +--+--+--+-+-++--+ | ner_chunk| description| category|icd10_code|icd11_code|mesh_code| brite_code| +--+--+--+-+-++--+ | myopia|Myopia is the most common ocular disorder world...| Nervous system disease| H52.1| 9D00.0| D009216| 08402,08403| | kniest dysplasia|Kniest dysplasia is an autosomal dominant chond...|Congenital malformation| Q77.7| LD24.3| C537207| 08402,08403| | prostate cancer|Prostate cancer constitutes a major health prob...| Cancer| C61| 2C82| NONE|08402,08403,08442,08441| |congenital nephrogenic diabetes insipidus|Nephrogenic diabetes insipidus (NDI) is charact...| Urinary system disease| N25.1| GB90.4A| D018500| 08402,08403| +--+--+--+-+-++--+ kegg_drug_mapper: This pretrained model maps drugs with their corresponding efficacy, molecular_weight as well as CAS, PubChem, ChEBI, LigandBox, NIKKAJI, PDB-CCD codes. This model was trained with the data from the KEGG database. Example: chunkerMapper = ChunkMapperModel.pretrained(&quot;kegg_drug_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRels([&quot;efficacy&quot;, &quot;molecular_weight&quot;, &quot;CAS&quot;, &quot;PubChem&quot;, &quot;ChEBI&quot;, &quot;LigandBox&quot;, &quot;NIKKAJI&quot;, &quot;PDB-CCD&quot;]) text= &quot;She is given OxyContin, folic acid, levothyroxine, Norvasc, aspirin, Neurontin&quot; Result: +-+--+-+-+--+-+++-+ | ner_chunk| efficacy|molecular_weight| CAS| PubChem| ChEBI|LigandBox| NIKKAJI|PDB-CCD| +-+--+-+-+--+-+++-+ | OxyContin| Analgesic (narcotic), Opioid receptor agonist| 351.8246| 124-90-3| 7847912.0| 7859.0| D00847|J281.239H| NONE| | folic acid|Anti-anemic, Hematopoietic, Supplement (folic a...| 441.3975| 59-30-3| 7847138.0|27470.0| D00070| J1.392G| FOL| |levothyroxine| Replenisher (thyroid hormone)| 776.87| 51-48-9|9.6024815E7|18332.0| D08125| J4.118A| T44| | Norvasc|Antihypertensive, Vasodilator, Calcium channel ...| 408.8759|88150-42-9|5.1091781E7| 2668.0| D07450| J33.383B| NONE| | aspirin|Analgesic, Anti-inflammatory, Antipyretic, Anti...| 180.1574| 50-78-2| 7847177.0|15365.0| D00109| J2.300K| AIN| | Neurontin| Anticonvulsant, Antiepileptic| 171.2368|60142-96-3| 7847398.0|42797.0| D00332| J39.388F| GBN| +-+--+-+-+--+-+++-+ abbreviation_category_mapper: This pretrained model maps abbreviations and acronyms of medical regulatory activities with their definitions and categories. Predicted categories: general, problem, test, treatment, medical_condition, clinical_dept, drug, nursing, internal_organ_or_component, hospital_unit, drug_frequency, employment, procedure. Example: chunkerMapper = ChunkMapperModel.pretrained(&quot;abbreviation_category_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;abbr_ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRels([&quot;definition&quot;, &quot;category&quot;]) text = [&quot;&quot;&quot;Gravid with estimated fetal weight of 6-6/12 pounds. LABORATORY DATA: Laboratory tests include a CBC which is normal. VDRL: Nonreactive HIV: Negative. One-Hour Glucose: 117. Group B strep has not been done as yet.&quot;&quot;&quot;] Result: | chunk | category | definition | |:--|:|:| | CBC | general | complete blood count | | VDRL | clinical_dept | Venereal Disease Research Laboratories | | HIV | medical_condition | Human immunodeficiency virus | New Utility &amp; Helper Relation Extraction Modules to Handle Preprocess This process is standard and training column should be same in all RE trainings. We can simplify this process with helper class. With proposed changes it can be done as follows: Example: from sparknlp_jsl.training import REDatasetHelper # map entity columns to dataset columns column_map = { &quot;begin1&quot;: &quot;firstCharEnt1&quot;, &quot;end1&quot;: &quot;lastCharEnt1&quot;, &quot;begin2&quot;: &quot;firstCharEnt2&quot;, &quot;end2&quot;: &quot;lastCharEnt2&quot;, &quot;chunk1&quot;: &quot;chunk1&quot;, &quot;chunk2&quot;: &quot;chunk2&quot;, &quot;label1&quot;: &quot;label1&quot;, &quot;label2&quot;: &quot;label2&quot; } # apply preprocess function to dataframe data = REDatasetHelper(data).create_annotation_column( column_map, ner_column_name=&quot;train_ner_chunks&quot; # optional, default train_ner_chunks ) New Utility &amp; Helper OCR Modules to Handle Annotations This modeule can generates an annotated PDF file using input PDF files. style: PDF file proccess style that has 3 options; black_band: Black bands over the chunks detected by NER pipeline. bounding_box: Colorful bounding boxes around the chunks detected by NER pipeline. Each color represents a different NER label. highlight: Colorful highlights over the chunks detected by NER pipeline. Each color represents a different NER label. You can check Spark OCR Utility Module notebook for more examples. Example: from sparknlp_jsl.utils.ocr_nlp_processor import ocr_entity_processor path=&#39;/*.pdf&#39; box = &quot;bounding_box&quot; ocr_entity_processor(spark=spark,file_path=path,ner_pipeline = nlp_model,chunk_col = &quot;merged_chunk&quot;, black_list = [&quot;AGE&quot;, &quot;DATA&quot;, &quot;PATIENT&quot;], style = box, save_dir = &quot;colored_box&quot;,label= True, label_color = &quot;red&quot;,color_chart_path = &quot;label_colors.png&quot;, display_result=True) box = &quot;highlight&quot; ocr_entity_processor(spark=spark,file_path=path, ner_pipeline = nlp_model, chunk_col = &quot;merged_chunk&quot;, black_list = [&quot;AGE&quot;, &quot;DATE&quot;, &quot;PATIENT&quot;], style = box, save_dir = &quot;colored_box&quot;, label= True, label_color = &quot;red&quot;, color_chart_path = &quot;label_colors.png&quot;, display_result=True) box = &quot;black_band&quot; ocr_entity_processor(spark=spark,file_path=path, ner_pipeline = nlp_modelchunk_col = &quot;merged_chunk&quot;, style = box, save_dir = &quot;black_band&quot;,label= True, label_color = &quot;red&quot;, display_result = True) Results: Bounding box with labels and black list Highlight with labels and black_list black_band with labels New Utility &amp; Helper NER Log Parser ner_utils: This new module is used after NER training to calculate mertic chunkbase and plot training logs. Example: nerTagger = NerDLApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) ... .setOutputLogsPath(&#39;ner_logs&#39;) ner_pipeline = Pipeline(stages=[glove_embeddings, graph_builder, nerTagger]) ner_model = ner_pipeline.fit(training_data) evaluate: if verbose, returns overall performance, as well as performance per chunk type; otherwise, simply returns overall precision, recall, f1 scores Example: from sparknlp_jsl.utils.ner_utils import evaluate metrics = evaluate(preds_df[&#39;ground_truth&#39;].values, preds_df[&#39;prediction&#39;].values) Result: processed 14133 tokens with 1758 phrases; found: 1779 phrases; correct: 1475. accuracy: 83.45%; (non-O) accuracy: 96.67%; precision: 82.91%; recall: 83.90%; FB1: 83.40 LOC: precision: 91.41%; recall: 85.69%; FB1: 88.46 524 MISC: precision: 78.15%; recall: 62.11%; FB1: 69.21 151 ORG: precision: 61.86%; recall: 74.93%; FB1: 67.77 430 PER: precision: 90.80%; recall: 93.58%; FB1: 92.17 674 loss_plot: Plots the figure of loss vs epochs Example: from sparknlp_jsl.utils.ner_utils import loss_plot loss_plot(&#39;./ner_logs/&#39;+log_files[0]) Results: get_charts : Plots the figures of metrics ( precision, recall, f1) vs epochs Example: from sparknlp_jsl.utils.ner_utils import get_charts get_charts(&#39;./ner_logs/&#39;+log_files[0]) Results: Adding Flexibility Chunk Merger Prioritization orderingFeatures: Array of strings specifying the ordering features to use for overlapping entities. Possible values are ChunkBegin, ChunkLength, ChunkPrecedence, ChunkConfidence selectionStrategy: Whether to select annotations sequentially based on annotation order Sequential or using any other available strategy, currently only DiverseLonger are available. defaultConfidence: When ChunkConfidence ordering feature is included and a given annotation does not have any confidence the value of this param will be used. chunkPrecedence: When ChunkPrecedence ordering feature is used this param contains the comma separated fields in metadata that drive prioritization of overlapping annotations. When used by itself (empty chunkPrecedenceValuePrioritization) annotations will be prioritized based on number of metadata fields present. When used together with chunkPrecedenceValuePrioritization param it will prioritize based on the order of its values. chunkPrecedenceValuePrioritization: When ChunkPrecedence ordering feature is used this param contains an Array of comma separated values representing the desired order of prioritization for the VALUES in the metadata fields included from chunkPrecedence. Example: text = &quot;&quot;&quot;A 63 years old man presents to the hospital with a history of recurrent infections that include cellulitis, pneumonias, and upper respiratory tract infections...&quot;&quot;&quot; +-+ |ner_deid_chunk | +-+ |[{chunk, 2, 3, 63, {entity -&gt; AGE, sentence -&gt; 0, chunk -&gt; 0, confidence -&gt; 0.9997}}]| +-+ +-+ |jsl_ner_chunk | +-+ |[{chunk, 2, 13, 63 years old, {entity -&gt; Age, sentence -&gt; 0, chunk -&gt; 0, confidence -&gt; 0.85873336}}]| +-+ Merging overlapped chunks by considering their lenght If we set setOrderingFeatures([&quot;ChunkLength&quot;]) and setSelectionStrategy(&quot;DiverseLonger&quot;) parameters, the longest chunk will be prioritized in case of overlapping. Example: chunk_merger = ChunkMergeApproach() .setInputCols(&#39;ner_deid_chunk&#39;, &quot;jsl_ner_chunk&quot;) .setOutputCol(&#39;merged_ner_chunk&#39;) .setOrderingFeatures([&quot;ChunkLength&quot;]) .setSelectionStrategy(&quot;DiverseLonger&quot;) Results: |begin|end| chunk| entity| +--++-++ | 2| 13| 63 years old| Age| | 15| 17| man| Gender| | 35| 42| hospital| Clinical_Dept| Merging overlapped chunks by considering custom values that we set setChunkPrecedence() parameter contains an Array of comma separated values representing the desired order of prioritization for the VALUES in the metadata fields included from setOrderingFeatures([&quot;chunkPrecedence&quot;]). Example: chunk_merger = ChunkMergeApproach() .setInputCols(&#39;ner_deid_chunk&#39;, &quot;jsl_ner_chunk&quot;) .setOutputCol(&#39;merged_ner_chunk&#39;) .setMergeOverlapping(True) .setOrderingFeatures([&quot;ChunkPrecedence&quot;]) .setChunkPrecedence(&#39;ner_deid_chunk,AGE&#39;) # .setChunkPrecedenceValuePrioritization([&quot;ner_deid_chunk,AGE&quot;, &quot;jsl_ner_chunk,Age&quot;]) Results: |begin|end| chunk| entity| +--++-++ | 2| 3| 63| AGE| | 15| 17| man| Gender| | 35| 42| hospital| Clinical_Dept| You can check NER Chunk Merger notebook for more examples. Core improvements and bug fixes AssertionDL IncludeConfidence() parameters default value set by True Fixed NaN outputs in RelationExtraction Fixed loadSavedModel method that we use for importing transformers into Spark NLP Fixed replacer with setUseReplacement(True) parameter Added overall confidence score to MedicalNerModel when setIncludeAllConfidenceScore is True Fixed in InternalResourceDownloader showAvailableAnnotators New and Updated Notebooks New Spark OCR Utility Module notebook to help handle OCR process. Updated Clinical Entity Resolvers notebook with Assertion Filterer example. Updated NER Chunk Merger notebook with flexibility chunk merger prioritization example. Updated Clinical Relation Extraction notebook with new REDatasetHelper module. Updated ALab Module SparkNLP JSL notebook with new updates. 3 New Clinical Models and Pipelines Added &amp; Updated in Total kegg_disease_mapper kegg_drug_mapper abbreviation_category_mapper For all Spark NLP for healthcare models, please check: Models Hub Page Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_2_3",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_2_3"
  },
  "246": {
    "id": "246",
    "title": "Spark NLP for Healthcare Release Notes 4.2.4",
    "content": "4.2.4 Highlights New chunk mapper model for matching drugs by categories as well as other brands and names 4 new NER and classification models for Social Determinant of Health Allow fuzzy matching in the ChunkMapper annotator New NameChunkObfuscatorApproach annotator to obfuscate doctor and patient names using a custom external list (consistent name obfuscation) New AssertionChunkConverter annotator to prepare assertion model training dataset from chunk indices New training_log_parser module to parse NER and Assertion Status Detection model training log files Obfuscation of age entities by age groups in Deidentification Controlling the behaviour of unnormalized dates while shifting the days in Deidentification (setUnnormalizedDateMode parameter) Setting default day, months or years for partial dates via DateNormalizer Setting label case sensitivity in AssertionFilterer getClasses method for Zero Shot NER and Zero Shot Relation Extraction models Setting max syntactic distance parameter in RelationExtractionApproach Generic Relation Extraction Model (generic_re) to extract relations between any named entities using syntactic distances Core improvements and bug fixes New and updated notebooks New and updated demos MEDICAL QUESTION ANSWERING SMOKING STATUS MENTAL HEALTH DEPRESSION 5 new clinical models and pipelines added &amp; updated in total New Chunk Mapper Model For Matching Drugs by Categories As Well As Other Brands and Names We have a new drug_category_mapper chunk mapper model that maps drugs to their categories, other brands and names. It has two categories called main category and subcategory. Example: chunkerMapper = ChunkMapperModel.pretrained(&quot;drug_category_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRels([&quot;main_category&quot;, &quot;sub_category&quot;, &quot;other_name&quot;]) sample_text= &quot;She is given OxyContin, folic acid, levothyroxine, Norvasc, aspirin, Neurontin.&quot; Result: +-++--+--+ | ner_chunk| main_category| sub_category|other_names| +-++--+--+ | OxyContin| Pain Management| Opioid Analgesics| Oxaydo| | folic acid| Nutritionals| Vitamins, Water-Soluble| Folvite| |levothyroxine|Metabolic &amp; Endocrine| Thyroid Products| Levo T| | Norvasc| Cardiovascular| Antianginal Agents| Katerzia| | aspirin| Cardiovascular|Antiplatelet Agents, Cardiovascular| ASA| | Neurontin| Neurologics| GABA Analogs| Gralise| +-++--+--+ 4 New NER and Classification Models for Social Determinant of Health We are releasing 4 new NER and Classification models for Social Determinant of Health. ner_sdoh_mentions: Detecting Social Determinants of Health mentions in clinical notes. Predicted entities: sdoh_community, sdoh_economics, sdoh_education, sdoh_environment, behavior_tobacco, behavior_alcohol, behavior_drug. Example: ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_mentions&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) text = &quot;&quot;&quot;Mr. John Smith is a pleasant, cooperative gentleman with a long standing history (20 years) of diverticulitis. He is married and has 3 children. He works in a bank. He denies any alcohol or intravenous drug use. He has been smoking for many years.&quot;&quot;&quot; Result: +-+-+ |chunk |ner_label | +-+-+ |married |sdoh_community | |children |sdoh_community | |works |sdoh_economics | |alcohol |behavior_alcohol| |intravenous drug|behavior_drug | |smoking |behavior_tobacco| +-+-+ MedicalBertForSequenceClassification models that can be used in Social Determinant of Health related classification tasks: model name description predicted entities bert_sequence_classifier_sdoh_community_absent_status Classifies the clinical texts related to the loss of social support such as a family member or friend in the clinical documents. A discharge summary was classified True for Community-Absent if the discharge summary had passages related to the loss of social support and False if such passages were not found in the discharge summary. True False bert_sequence_classifier_sdoh_community_present_status Classifies the clinical texts related to social support such as a family member or friend in the clinical documents. A discharge summary was classified True for Community-Present if the discharge summary had passages related to active social support and False if such passages were not found in the discharge summary. True False bert_sequence_classifier_sdoh_environment_status Classifies the clinical texts related to environment situation such as any indication of housing, homeless or no related passage. A discharge summary was classified as True for the SDOH Environment if there was any indication of housing, False if the patient was homeless and None if there was no related passage. True False None Example: sequenceClassifier = MedicalBertForSequenceClassification.pretrained(&quot;bert_sequence_classifier_sdoh_community_present_status&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;document&quot;,&quot;token&quot;]) .setOutputCol(&quot;class&quot;) sample_text = [&quot;Right inguinal hernia repair in childhood Cervical discectomy 3 years ago Umbilical hernia repair 2137. Retired schoolteacher, now substitutes. Lives with wife in location 1439. Has a 27 yo son and a 25 yo daughter. Name (NI) past or present smoking hx, no EtOH.&quot;, &quot;Atrial Septal Defect with Right Atrial Thrombus Pulmonary Hypertension Obesity, Obstructive Sleep Apnea. Denies tobacco and ETOH. Works as cafeteria worker.&quot;] Result: +-+-+ | text| result| +-+-+ |Right inguinal hernia repair in childhood Cervical discectomy 3 years ago Umbilical hernia repair...| [True]| |Atrial Septal Defect with Right Atrial Thrombus Pulmonary Hypertension Obesity, Obstructive Sleep...|[False]| +-+-+ Allow Fuzzy Matching in the ChunkMapper Annotator There are multiple options to achieve fuzzy matching using the ChunkMapper annotation: Partial Token NGram Fingerprinting: Useful to combine two frequent usecases; when there are noisy non informative tokens at the beginning / end of the chunk and the order of the chunk is not absolutely relevant. i.e. stomach acute pain –&gt; acute pain stomach ; metformin 100 mg –&gt; metformin. Char NGram Fingerprinting: Useful in usecases that involve typos or different spacing patterns for chunks. i.e. head ache / ache head –&gt; headache ; metformini / metformoni / metformni –&gt; metformin Fuzzy Distance (Slow): Useful when the mapping can be defined in terms of edit distance thresholds using functions like char based like Levenshtein, Hamming, LongestCommonSubsequence or token based like Cosine, Jaccard. The mapping logic will be run in the previous order also ordering by longest key inside each option as an intuitive way to minimize false positives. Basic Mapper Example: cm = ChunkMapperApproach() .setInputCols([&quot;ner_chunk&quot;]) .setLowerCase(True) .setRels([&quot;action&quot;, &quot;treatment&quot;]) text = &quot;&quot;&quot;The patient was given Lusa Warfarina 5mg and amlodipine 10 MG. The patient was given Aspaginaspa, coumadin 5 mg, coumadin, and he has metamorfin&quot;&quot;&quot; # Since mappers only match one-to-one | ner_chunk | fixed_chunk | action | treatment | |:-|:|:-|:-| | Aspaginaspa | nan | nan | nan | | Lusa Warfarina 5mg | nan | nan | nan | | amlodipine 10 | nan | nan | nan | | coumadin | coumadin | Coagulation Inhibitor | hypertension | | coumadin 5 mg | nan | nan | nan | | metamorfin | nan | nan | nan | Since mappers only match one-to-one, we see that only 1 chunk has action and teatment in the table above. Token Fingerprinting Example: cm = ChunkMapperApproach() .setInputCols([&quot;ner_chunk&quot;]) .setLowerCase(True) .setRels([&quot;action&quot;, &quot;treatment&quot;]) .setAllowMultiTokenChunk(True) .setEnableTokenFingerprintMatching(True) .setMinTokenNgramFingerprint(1) .setMaxTokenNgramFingerprint(3) .setMaxTokenNgramDroppingCharsRatio(0.5) Result: | ner_chunk | fixed_chunk | action | treatment | |:--|:-|:--|:-| | Aspaginaspa | nan | nan | nan | | Lusa Warfarina 5mg | Warfarina lusa | Analgesic | diabetes | | amlodipine 10 | amlodipine | Calcium Ions Inhibitor | hypertension | | coumadin | coumadin | Coagulation Inhibitor | hypertension | | coumadin 5 mg | coumadin | Coagulation Inhibitor | hypertension | | metamorfin | nan | nan | nan | Token and Char Fingerprinting Example: cm = ChunkMapperApproach() .setInputCols([&quot;ner_chunk&quot;]) .setLowerCase(True) .setRels([&quot;action&quot;, &quot;treatment&quot;]) .setAllowMultiTokenChunk(True) .setEnableTokenFingerprintMatching(True) .setMinTokenNgramFingerprint(1) .setMaxTokenNgramFingerprint(3) .setMaxTokenNgramDroppingCharsRatio(0.5) .setEnableCharFingerprintMatching(True) .setMinCharNgramFingerprint(1) .setMaxCharNgramFingerprint(3) Result: | ner_chunk | fixed_chunk | action | treatment | |:--|:|:|:-| | Aspaginaspa | aspagin | Cycooxygenase Inhibitor | arthritis | | Lusa Warfarina 5mg | Warfarina lusa | Analgesic | diabetes | | amlodipine 10 | amlodipine | Calcium Ions Inhibitor | hypertension | | coumadin | coumadin | Coagulation Inhibitor | hypertension | | coumadin 5 mg | coumadin | Coagulation Inhibitor | hypertension | | metamorfin | nan | nan | nan | Token and Char Fingerprinting With Fuzzy Distance Calculation Example: cm = ChunkMapperApproach() .setInputCols([&quot;ner_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setDictionary(&quot;mappings.json&quot;) .setLowerCase(True) .setRels([&quot;action&quot;]) .setAllowMultiTokenChunk(True) .setEnableTokenFingerprintMatching(True) .setMinTokenNgramFingerprint(1) .setMaxTokenNgramFingerprint(3) .setMaxTokenNgramDroppingCharsRatio(0.5) .setEnableCharFingerprintMatching(True) .setMinCharNgramFingerprint(1) .setMaxCharNgramFingerprint(3) .setEnableFuzzyMatching(True) .setFuzzyMatchingDistanceThresholds(0.31) Result: | ner_chunk | fixed_chunk | action | treatment | |:-|:|:|:-| | Aspaginaspa | aspagin | Cycooxygenase Inhibitor | arthritis | | Lusa Warfarina 5mg | Warfarina lusa | Analgesic | diabetes | | amlodipine 10 | amlodipine | Calcium Ions Inhibitor | hypertension | | coumadin | coumadin | Coagulation Inhibitor | hypertension | | coumadin 5 mg | coumadin | Coagulation Inhibitor | hypertension | | metamorfin | metformin | hypoglycemic | diabetes | You can check Chunk_Mapping notebook for more examples. New NameChunkObfuscatorApproach Annotator to Obfuscate Doctor and Patient Names Using a Custom External List (consistent name obfuscation) We have a new NameChunkObfuscatorApproach annotator that can be used in deidentification tasks for replacing doctor and patient names with fake names using a reference document. Example: names = &quot;&quot;&quot;Mitchell#NAME Jackson#NAME Leonard#NAME Bowman#NAME Fitzpatrick#NAME Melody#NAME&quot;&quot;&quot; with open(&#39;names_test.txt&#39;, &#39;w&#39;) as file: file.write(names) nameChunkObfuscator = NameChunkObfuscatorApproach() .setInputCols(&quot;ner_chunk&quot;) .setOutputCol(&quot;replacement&quot;) .setRefFileFormat(&quot;csv&quot;) .setObfuscateRefFile(&quot;names_test.txt&quot;) .setRefSep(&quot;#&quot;) text = &#39;&#39;&#39;John Davies is a 62 y.o. patient admitted. Mr. Davies was seen by attending physician Dr. Lorand and was scheduled for emergency assessment. &#39;&#39;&#39; Result: Original text : John Davies is a 62 y.o. patient admitted. Mr. Davies was seen by attending physician Dr. Lorand and was scheduled for emergency assessment. Obfuscated text : Fitzpatrick is a &lt;AGE&gt; y.o. patient admitted. Mr. Bowman was seen by attending physician Dr. Melody and was scheduled for emergency assessment. You can check Clinical DeIdentification notebook for more examples. New AssertionChunkConverter Annotator to Prepare Assertion Model Training Dataset From Chunk Indices In some cases, there may be issues while creating the chunk column by using token indices and losing some data while training and testing the assertion status model if there are issues in these token indices. So we developed a new AssertionChunkConverter annotator that takes begin and end indices of the chunks as input and creates an extended chunk column with metadata that can be used for assertion status detection model training. Example: ... converter = AssertionChunkConverter() .setInputCols(&quot;tokens&quot;) .setChunkTextCol(&quot;target&quot;) .setChunkBeginCol(&quot;char_begin&quot;) .setChunkEndCol(&quot;char_end&quot;) .setOutputTokenBeginCol(&quot;token_begin&quot;) .setOutputTokenEndCol(&quot;token_end&quot;) .setOutputCol(&quot;chunk&quot;) sample_data = spark.createDataFrame([[&quot;An angiography showed bleeding in two vessels off of the Minnie supplying the sigmoid that were succesfully embolized.&quot;, &quot;Minnie&quot;, 57, 63], [&quot;After discussing this with his PCP, Leon was clear that the patient had had recurrent DVTs and ultimately a PE and his PCP felt strongly that he required long-term anticoagulation &quot;, &quot;PCP&quot;, 31, 34]]) .toDF(&quot;text&quot;, &quot;target&quot;, &quot;char_begin&quot;, &quot;char_end&quot;) Result: ++-+--+--++--+++-+ |target|char_begin|char_end|token_begin|token_end|tokens[token_begin].result|tokens[token_end].result|target|chunk | ++-+--+--++--+++-+ |Minnie|57 |62 |10 |10 |Minnie |Minnie |Minnie|[{chunk, 57, 63, Minnie, {sentence -&gt; 0}, []}]| |PCP |31 |34 |5 |5 |PCP |PCP |PCP |[{chunk, 31, 33, PCP, {sentence -&gt; 0}, []}] | ++-+--+--++--+++-+ New training_log_parser Module to Parse Training Log Files of NER And Assertion Status Detection Models We are releasing a new training_log_parser module that helps to parse NER and Assertion Status Detection model training log files using a single module. Here are the methods and their descriptions:   Description ner_log_parser assertion_log_parser How to import You can import this module for NER and Assertion as shown here from sparknlp_jsl.utils.training_log_parser import ner_log_parser from sparknlp_jsl.utils.training_log_parser import assertion_log_parser get_charts Plots the figures of metrics ( precision, recall, f1) vs epochs ner_log_parser.get_charts(log_file, threshold) assertion_log_parser.get_charts(log_file, labels, threshold) loss_plot Plots the figures of validation and test loss values vs epochs. ner_log_parser.loss_plot(path) assertion_log_parser.loss_plot(path) get_best_f1_scores Returns the best Micro and Macro F1 Scores on test set ner_log_parser.get_best_f1_scores(path) assertion_log_parser.get_best_f1_scores(path) parse_logfile Returns the parsed log file in pandas dataframe format with the order of label-score dataframe, epoch-metrics dataframe and graph file used in tranining. ner_log_parser.parse_logfile(path) assertion_log_parser.parse_logfile(path, labels) evaluate if verbose, returns overall performance, as well as performance per chunk type; otherwise, simply returns overall precision, recall, f1 scores. Ground truth and predictions should be provided in pandas dataframe. ner_log_parser.evaluate(preds_df[&#39;ground_truth&#39;].values, preds_df[&#39;prediction&#39;].values) - Import from sparknlp_jsl.utils.training_log_parser import ner_log_parser, assertion_log_parser ner_parser = ner_log_parser() assertion_parser = assertion_log_parser() Example for NER loss_plot method: ner_parser.loss_plot(&#39;NER_training_log_file.log&#39;) Result: Example for NER evaluate method: metrics = ner_parser.evaluate(preds_df[&#39;ground_truth&#39;].values, preds_df[&#39;prediction&#39;].values) Result: Example for Assertion get_best_f1_scores method: assertion_parser.get_best_f1_scores(&#39;Assertion_training_log_file.log&#39;, [&#39;Absent&#39;, &#39;Present&#39;]) Result: Obfuscation of Age Entities by Age Groups in Deidentification We have a new setAgeRanges() parameter in Deidentification annotator that provides the ability to set a custom range for obfuscation of AGE entities by another age within that age group (range). Default age groups list is [1, 4, 12, 20, 40, 60] and users can set any range. Infant = 0-1 year. Toddler = 2-4 yrs. Child = 5-12 yrs. Teen = 13-19 yrs. Adult = 20-39 yrs. Middle Age Adult = 40-59 yrs. Senior Adult = 60+ Example: deidentification = DeIdentification() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;age_chunk&quot;]) .setOutputCol(&quot;obfuscation&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateDate(True) .setObfuscateRefSource(&quot;faker&quot;) .setAgeRanges([1, 4, 12, 20, 40, 60, 80]) Result: +--++--+ |text |age_chunk|obfuscation | +--++--+ |1 year old baby |1 |2 year old baby | |4 year old kids |4 |6 year old kids | |A 15 year old female with |15 |A 12 year old female with | |Record date: 2093-01-13, Age: 25|25 |Record date: 2093-03-01, Age: 30| |Patient is 45 years-old |45 |Patient is 44 years-old | |He is 65 years-old male |65 |He is 75 years-old male | +--++--+ Controlling the behaviour of unnormalized dates while shifting the days in Deidentification (setUnnormalizedDateMode parameter) Two alternatives can be used when deidentification in unnormalized date formats, these are mask and obfuscation. setUnnormalizedDateMode(&#39;mask&#39;) parameter is used to mask the DATE entities that can not be normalized. setUnnormalizedDateMode(&#39;obfuscate&#39;) parameter is used to obfuscate the DATE entities that can not be normalized. Example: de_identification = DeIdentification() .setInputCols([&quot;ner_chunk&quot;, &quot;token&quot;, &quot;document2&quot;]) .setOutputCol(&quot;deid_text&quot;) .setMode(&quot;obfuscate&quot;) ... .setUnnormalizedDateMode(&quot;mask&quot;) # or obfuscation Result: +--++++ |text |dateshift| mask | obfuscation| +--++++ |04/19/2018 |-5 | 04/14/2018 | 04/14/2018 | |04-19-2018 |-2 | 04-17-2018 | 04-17-2018 | |19 Apr 2018|10 | &lt;DATE&gt; | 10-10-1975 | |04-19-18 |20 | &lt;DATE&gt; | 03-23-2001 | +--++++ Setting Default Day, Months or Years for Partial Dates via DateNormalizer We have 3 new parameters to make DateNormalizer more flexible with date replacing. If any of the day, month and year information is missing in the date format, the following default values will be added. setDefaultReplacementDay: default value is 15 setDefaultReplacementMonth: default value is July or 6 setDefaultReplacementYear: default value is 2020 Example: date_normalizer_us = DateNormalizer() .setInputCols(&#39;date_chunk&#39;) .setOutputCol(&#39;normalized_date_us&#39;) .setOutputDateformat(&#39;us&#39;) .setDefaultReplacementDay(&quot;15&quot;) .setDefaultReplacementMonth(&quot;6&quot;) .setDefaultReplacementYear(&quot;2020&quot;) Result: ++++ |text |date_chunk |normalized_date_us| ++++ |08/02/2018 |08/02/2018 |08/02/2018 | |3 April 2020|3 April 2020|04/03/2020 | |03/2021 |03/2021 |03/15/2021 | |05 Jan |05 Jan |01/05/2020 | |01/05 |01/05 |01/05/2020 | |2022 |2022 |06/15/2022 | ++++ You can check Date Normalizer notebook for more examples Setting Label Case Sensitivity in AssertionFilterer We have case sensitive filtering flexibility for labels by setting new setCaseSensitive(True) in AssertionFilterer annotator. Example: assertion_filterer = AssertionFilterer() .setInputCols(&quot;sentence&quot;,&quot;ner_chunk&quot;,&quot;assertion&quot;) .setOutputCol(&quot;assertion_filtered&quot;) .setCaseSensitive(False) .setWhiteList([&quot;ABsent&quot;]) sample_text = &quot;The patient was admitted 2 weeks ago with a headache. No alopecia was noted.&quot; Result: | chunks | entities | assertion | confidence | | -- | - | | - | | Alopecia | Disease_Syndrome_Disorder | Absent | 1 | getClasses Method to Zero Shot NER and Zero Shot Relation Extraction Models The predicted entities of ZeroShotNerModel and ZeroShotRelationExtractionModels can be extracted with getClasses methods just like NER annotators. Example: zero_shot_ner = ZeroShotNerModel.pretrained(&quot;zero_shot_ner_roberta&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setEntityDefinitions({ &quot;PROBLEM&quot;: [&quot;What is the disease?&quot;, &quot;What is the problem?&quot; ,&quot;What does a patient suffer&quot;], &quot;DRUG&quot;: [&quot;Which drug?&quot;, &quot;Which is the drug?&quot;, &quot;What is the drug?&quot;], &quot;ADMISSION_DATE&quot;: [&quot;When did patient admitted to a clinic?&quot;], &quot;PATIENT_AGE&quot;: [&quot;How old is the patient?&quot;,&#39;What is the gae of the patient?&#39;] }) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;zero_shot_ner&quot;) zero_shot_ner.getClasses() Result: [&#39;DRUG&#39;, &#39;PATIENT_AGE&#39;, &#39;ADMISSION_DATE&#39;, &#39;PROBLEM&#39;] Setting Max Syntactic Distance Flexibility In RelationExtractionApproach Now we are able to set maximal syntactic distance as threshold in RelationExtractionApproach while training relation extraction models. reApproach = RelationExtractionApproach() .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;train_ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setLabelColumn(&quot;rel&quot;) ... .setMaxSyntacticDistance(10) Generic Relation Extraction Model (generic_re) to extract relations between any named entities using syntactic distances We already have more than 80 relation extraction (RE) models that can extract relations between certain named entities. Nevertheless, there are some rare entities or cases that you may not find the right RE or the one you find may not work as expected due to nature of your dataset. In order to ease this burden, we are releasing a generic RE model (generic_re) that can be used between any named entities using the syntactic distances, POS tags and dependency tree between the entities. You can tune this model by using the setMaxSyntacticDistance param. Example: reModel = RelationExtractionModel() .pretrained(&quot;generic_re&quot;) .setInputCols([&quot;embeddings&quot;, &quot;pos_tags&quot;, &quot;ner_chunks&quot;, &quot;dependencies&quot;]) .setOutputCol(&quot;relations&quot;) .setRelationPairs([&quot;Biomarker-Biomarker_Result&quot;, &quot;Biomarker_Result-Biomarker&quot;, &quot;Oncogene-Biomarker_Result&quot;, &quot;Biomarker_Result-Oncogene&quot;, &quot;Pathology_Test-Pathology_Result&quot;, &quot;Pathology_Result-Pathology_Test&quot;]) .setMaxSyntacticDistance(4) text = &quot;&quot;&quot;Pathology showed tumor cells, which were positive for estrogen and progesterone receptors.&quot;&quot;&quot; Result: |sentence |entity1_begin |entity1_end | chunk1 | entity1 |entity2_begin |entity2_end | chunk2 | entity2 | relation |confidence| |--:|-:|--:|:-|:--|-:|--:|:--|:--|:--|-| | 0 | 1 | 9 | Pathology | Pathology_Test | 18 | 28 | tumor cells | Pathology_Result | Pathology_Test-Pathology_Result | 1| | 0 | 42 | 49 | positive | Biomarker_Result | 55 | 62 | estrogen | Biomarker | Biomarker_Result-Biomarker | 1| | 0 | 42 | 49 | positive | Biomarker_Result | 68 | 89 | progesterone receptors | Biomarker | Biomarker_Result-Biomarker | 1| Core improvements and bug fixes Fixed obfuscated addresses capitalized word style Added more patterns for Date Obfuscation Improve speed of get_conll_data() method in alab module Fixed serialization Issue with MLFlow ContextualParser Renamed TFGraphBuilder.setIsMedical to TFGraphBuilder.setIsLicensed New and Updated Notebooks Updated ZeroShot Clinical NER Notebook with getClasses method for zero shot NER models. Updated Clinical Assertion Notebook with AssertionChunkConverter, AssertionFilterer and TFGraphBuilder.setIsLicensed examples. Updated Clinical Entity Resolvers Notebook with AssertionFilterer example. Updated Clinical DeIdentification Notebook with setUnnormalizedDateMode and NameChunkObfuscatorApproach example. Updated ZeroShot Clinical Relation Extraction Notebook with getClasses and setMaxSyntacticDistance method for Relation Extraction models. Updated Date Normalizer notebook with DateNormalizer for dynamic date replace values. Updated Chunk Mapping notebook with fuzzy matching flexibility examples. New and Updated Demos MEDICAL QUESTION ANSWERING SMOKING STATUS MENTAL HEALTH DEPRESSION 5 New Clinical Models and Pipelines Added &amp; Updated in Total drug_category_mapper ner_sdoh_mentions bert_sequence_classifier_sdoh_community_absent_status bert_sequence_classifier_sdoh_community_present_status bert_sequence_classifier_sdoh_environment_status For all Spark NLP for healthcare models, please check: Models Hub Page Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_2_4",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_2_4"
  },
  "247": {
    "id": "247",
    "title": "Visual NLP(Spark OCR) release notes 4.2.4",
    "content": "4.2.4 We are glad to announce that Spark OCR 4.2.4⚡has been released!! This release includes new optimized ImageToTextV2 models, more support on annotators in LightPipelines, a new PdfToHocr annotator, enhancements, and more! New Features New annotators supported in LightPipelines: PdfToText and most Image transformations. Check sample notebook for details. Handling of PDFs with broken headers: some PDFs may contain incorrect header information causing the pipelines to fail to process them, now PDF processing annotators support handling these documents. New Annotators New ImageToTextV2 Transformers based OCR annotator, Intended to become a full replacement of original ImageToTextV2. Speed ups of up to 2x compared to original model. It doesn’t require GPU, it works with CPU only environments. Preliminary experiments show similar character error rate compared to original model. Optimized versions take less space(about a half) and are faster to store and download. Full JVM implementation. Limitations: currently the new ImageToTextV2 doesn’t support Hocr output. To start using it, follow this example, ... from sparkocr.optimized import ImageToTextV2 ocr = ImageToTextV2.pretrained(&quot;ocr_base_printed_v2_opt&quot;, &quot;en&quot;, &quot;clinical/ocr&quot;) New PdfToHocr: this new annotator allows to produce HOCR output from digital PDFs. This is not only useful for integrating into existing annotators that already consume HOCR, but for new pipelines that will be released in the future. Stay tuned for new releases. New Models ocr_base_printed_v2 ocr_base_handwritten_v2 ocr_base_printed_v2_opt (quantized version) ocr_base_handwritten_v2_opt (quantized version) New Notebooks New supported transformers in LightPipelines in action, SparkOcrLightPipelinesPdf.ipynb PdfToHocr, SparkOCRPdfToHocr.ipynb This release is compatible with Spark NLP 4.2.4, and Spark NLP for Healthcare 4.2.3. Versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_4_2_4",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_4_2_4"
  },
  "248": {
    "id": "248",
    "title": "Spark NLP for Healthcare Release Notes 4.2.8",
    "content": "4.2.8 Highlights 4 new clinical named entity recognition models (3 oncology, 1 others) 5 new Social Determenant of Health text classification models New DocumentMLClassifierApproach annotator for training text classification models using SVM and Logistic Regression using TfIdf New Resolution2Chunk annotator to map entity resolver outputs (terminology codes) to other clinical terminologies New DocMapperModel annotator allows to use any mapper model in DOCUMENT type Option to return Deidentification output as a single document Inter-Annotator Agreement (IAA) metrics module that works with NLP Lab seamlessly Assertion dataset preparation module now supports chunk start and end indices, rather than token indices Added ner_source in the ChunkConverter metadata Core improvements and bug fixes Added chunk confidence score in the RelationExtractionModel metadata Added confidence score in the DocumentLogRegClassifierApproach metadata Fixed non-deterministic Relation Extraction DL Models (30+ models updated in the model hub) Fixed incompatible PretrainedPipelines with PySpark v3.2.x and v3.3.x Fixed ZIP label issue on faker mode with setZipCodeTag parameter in Deidentification Fixed obfuscated numbers have the same number of chars as the original ones Fixed name obfuscation hashes in Deidentification for romanian language Fixed LightPipeline validation parameter for internal annotators LightPipeline support for GenericClassifier (FeatureAssembler) New and updated notebooks New Clinical Text Classification with Spark_NLP Notebook New Clinical Text Classification with DocumentMLClassifier Notebook Updated ALAB Notebook New and updated demos SOCIAL DETERMINANT demo 9 new clinical models and pipelines added &amp; updated in total 4 New Clinical Named Entity Recognition Models (3 Oncology, 1 Others) We are releasing 3 new oncological NER models that were trained by using embeddings_healthcare_100d embeddings model. model name description predicted entities ner_oncology_anatomy_general_healthcare Extracts anatomical entities using an unspecific label Anatomical_Site Direction ner_oncology_biomarker_healthcare Extracts mentions of biomarkers and biomarker results in oncological texts. Biomarker_Result Biomarker ner_oncology_unspecific_posology_healthcare Extracts mentions of treatments and posology information using unspecific labels (low granularity). Posology_Information Cancer_Therapy Example: ... word_embeddings = WordEmbeddingsModel() .pretrained(&quot;embeddings_healthcare_100d&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner = MedicalNerModel .pretrained(&quot;ner_oncology_anatomy_general_healthcare&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) text = &quot;The patient presented a mass in her left breast, and a possible metastasis in her lungs and in her liver.&quot; Result: ++-+ |chunk |ner_label | ++-+ |left |Direction | |breast |Anatomical_Site | |lungs |Anatomical_Site | |liver |Anatomical_Site | ++-+ We are releasing new oncological NER models that used for model training is provided by European Clinical Case Corpus (E3C), a project aimed at offering a freely available multilingual corpus of semantically annotated clinical narratives. Example: ... ner = MedicalNerModel.pretrained(&#39;ner_eu_clinical_case&#39;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) text = &quot;&quot;&quot;A 3-year-old boy with autistic disorder on hospital of pediatric ward A at university hospital. He has no family history of illness or autistic spectrum disorder.&quot;&quot;&quot; Result: +++ |chunk |ner_label | +++ |A 3-year-old boy |patient | |autistic disorder |clinical_condition| |He |patient | |illness |clinical_event | |autistic spectrum disorder |clinical_condition| +++ 5 New Social Determinant of Health Text Classification Models We are releasing 5 new models that can be used in Social Determinant of Health related classification tasks. model name description predicted entities genericclassifier_sdoh_alcohol_usage_sbiobert_cased_mli This model is intended for detecting alcohol use in clinical notes and trained by using GenericClassifierApproach annotator. Present Past Never None genericclassifier_sdoh_alcohol_usage_binary_sbiobert_cased_mli This model is intended for detecting alcohol use in clinical notes and trained by using GenericClassifierApproach annotator. Present Never None genericclassifier_sdoh_tobacco_usage_sbiobert_cased_mli This model is intended for detecting tobacco use in clinical notes and trained by using GenericClassifierApproach annotator Present Past Never None genericclassifier_sdoh_economics_binary_sbiobert_cased_mli This model classifies related to social economics status in the clinical documents and trained by using GenericClassifierApproach annotator. True False genericclassifier_sdoh_substance_usage_binary_sbiobert_cased_mli This model is intended for detecting substance use in clinical notes and trained by using GenericClassifierApproach annotator. Present None Example: ... features_asm = FeaturesAssembler() .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCol(&quot;features&quot;) generic_classifier_tobacco = GenericClassifierModel.pretrained(&quot;genericclassifier_sdoh_tobacco_usage_sbiobert_cased_mli&quot;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;class_tobacco&quot;) generic_classifier_alcohol = GenericClassifierModel.pretrained(&quot;genericclassifier_sdoh_alcohol_usage_sbiobert_cased_mli&quot;, &#39;en&#39;, &#39;clinical/models&#39;) .setInputCols([&quot;features&quot;]) .setOutputCol(&quot;class_alcohol&quot;) text = [&quot;Retired schoolteacher, now substitutes. Lives with wife in location 1439. Has a 27 yo son and a 25 yo daughter. He uses alcohol and cigarettes&quot;, &quot;The patient quit smoking approximately two years ago with an approximately a 40 pack year history, mostly cigar use.&quot;, &quot;The patient denies any history of smoking or alcohol abuse. She lives with her one daughter.&quot;, &quot;She was previously employed as a hairdresser, though says she hasnt worked in 4 years. Not reported by patient, but there is apparently a history of alochol abuse.&quot; ] Result: +-+++ | text| tobacco| alcohol| +-+++ |Retired schoolteacher, now substitutes. Lives with wife in location 1439. Has a 27 yo son and a 2...|[Present]|[Present]| |The patient quit smoking approximately two years ago with an approximately a 40 pack year history...| [Past]| [None]| | The patient denies any history of smoking or alcohol abuse. She lives with her one daughter.| [Never]| [Never]| |She was previously employed as a hairdresser, though says she hasnt worked in 4 years. Not report...| [None]| [Past]| +-+++ New DocumentMLClassifierApproach Annotator For Training Text Classification Models Using SVM And Logistic Regression Using TfIdf We have a new DocumentMLClassifierApproach that can be used for training text classification models with Logistic Regression and SVM algorithms. Training data requires “text” and their “label” columns only and the trained model will be a DocumentMLClassifierModel(). Input types: TOKEN Output type: CATEGORY Parameters Description labels array to output the label in the original form. labelCol column with the value result we are trying to predict. maxIter maximum number of iterations. tol convergence tolerance after each iteration. fitIntercept whether to fit an intercept term, default is true. maxTokenNgram the max number of tokens for Ngrams minTokenNgram the min number of tokens for Ngrams vectorizationModelPath specify the vectorization model if it has been already trained. classificationModelPath specify the classification model if it has been already trained. classificationModelClass specify the SparkML classification class; possible values are logreg, svm Example: ... classifier_svm= DocumentMLClassifierApproach() .setInputCols(&quot;token&quot;) .setLabelCol(&quot;category&quot;) .setOutputCol(&quot;prediction&quot;) .setMaxTokenNgram(1) .setClassificationModelClass(&quot;svm&quot;) #or &quot;logreg&quot; model_svm = Pipeline(stages=[document, token, classifier_svm]).fit(trainingData) text = [ [&quot;This 1-year-old child had a gastrostomy placed due to feeding difficulties.&quot;], [&quot;He is a pleasant young man who has a diagnosis of bulbar cerebral palsy and hypotonia.&quot;], [&quot;The patient is a 45-year-old female whose symptoms are pain in the left shoulder and some neck pain.&quot;], [&quot;The patient is a 61-year-old female with history of recurrent uroseptic stones.&quot;] ] Result: +-+-+ |text |prediction | +-+-+ |He is a pleasant young man who has a diagnosis of bulbar cerebral palsy and hypotonia. |Neurology | |This 1-year-old child had a gastrostomy placed due to feeding difficulties. |Gastroenterology| |The patient is a 61-year-old female with history of recurrent uroseptic stones. |Urology | |The patient is a 45-year-old female whose symptoms are pain in the left shoulder and some neck pain.|Orthopedic | +-+-+ Option To Return Deidentification Output As a Single Document We can return Deidentification() output as a single document by setting new setOutputAsDocument as True. If it is False, the outputs will be list of sentences as it is used to be. Example: deid_obfuscated = DeIdentification() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner_chunk_subentity&quot;]) .setOutputCol(&quot;obfuscated&quot;) .setMode(&quot;obfuscate&quot;) .setObfuscateDate(True) .setObfuscateRefFile(&#39;obfuscate.txt&#39;) .setObfuscateRefSource(&quot;file&quot;) .setUnnormalizedDateMode(&quot;obfuscate&quot;) .setOutputAsDocument(True) # or False for sentence level result text =&#39;&#39;&#39; Record date : 2093-01-13 , David Hale , M.D . , Name : Hendrickson , Ora MR # 7194334 Date : 01/13/93 . Patient : Oliveira, 25 years-old , Record date : 2079-11-09 . Cocke County Baptist Hospital . 0295 Keats Street &#39;&#39;&#39; Result of .setOutputAsDocument(True): &#39;obfuscated&#39;: [&#39;Record date : 2093-01-14 , Beer-Karge , M.D . , Name : Hasan Jacobi Jäckel MR # &lt;MEDICALRECORD&gt; Date : 01-31-1991 . Patient : Herr Anselm Trüb, 51 years-old , Record date : 2080-01-08 . Klinik St. Hedwig . &lt;MEDICALRECORD&gt; Keats Street&#39;] Result of .setOutputAsDocument(False): &#39;obfuscated&#39;: [&#39;Record date : 2093-02-19 , Kaul , M.D . , Name : Frauke Oestrovsky MR # &lt;MEDICALRECORD&gt; Date : 05-08-1971 .&#39;, &#39;Patient : Lars Bloch, 33 years-old , Record date : 2079-11-11 .&#39;, &#39;University Hospital of Düsseldorf . &lt;MEDICALRECORD&gt; Keats Street&#39;] New Resolution2Chunk Annotator To Map Entity Resolver Outputs (terminology codes) To Other Clinical Terminologies We have a new Resolution2Chunk annotator that maps the entity resolver outputs to other clinical terminologies. Example: icd_resolver = SentenceEntityResolverModel.pretrained(&quot;sbiobertresolve_icd10cm_augmented_billable_hcc&quot;,&quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCol(&quot;icd10cm_code&quot;) .setDistanceFunction(&quot;EUCLIDEAN&quot;) resolver2chunk = Resolution2Chunk() .setInputCols([&quot;icd10cm_code&quot;]) .setOutputCol(&quot;resolver2chunk&quot;) chunkerMapper = ChunkMapperModel.pretrained(&quot;icd10cm_snomed_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;resolver2chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRels([&quot;snomed_code&quot;]) sample_text = &quot;&quot;&quot;Diabetes Mellitus&quot;&quot;&quot; Result: +--+--++--+ |text |ner_chunk |icd10cm_code|snomed_code| +--+--++--+ |Diabetes Mellitus|Diabetes Mellitus|E109 |170756003 | +--+--++--+ New DocMapperModel Annotator Allows To Use With Any Mapper Model In DOCUMENT Type Any ChunkMapperModel can be used with this new annotator called DocMapperModel and as its name suggests, it is used to map short strings via DocumentAssembler without using any other annotator between to convert strings to Chunk type that ChunkMapperModel expects. Example: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) model = DocMapperModel.pretrained(&quot;drug_brandname_ndc_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;mappings&quot;) sample_text = &quot;ZYVOX&quot; Result: | Brand_Name | Strenth_NDC | |:-|:-| | ZYVOX | 600 mg/300mL | 0009-4992 | Inter-Annotator Agreement (IAA) metrics module that works with NLP Lab seamlessly We added a new get_IAA_metrics() method to ALAB module. This method allows you to compare and evaluate the annotations in the seed corpus that all annotators annotated the same documents at the begining of an annotation project. It returns all the results in CSV files. Here are the parameters; spark : SparkSession. conll_dir (str): path to the folder that conll files in. annotator_names (list): list of annotator names. set_ref_annotator (str): reference annotator name. If present, all comparisons made with respect to it, if it is None all annotators will be compared by each other. Default is None. return_NerDLMetrics (boolean): If True, we get the full_chunk and - partial_chunk_per_token IAA metrics by using NerDLMetrics. If False, we get the chunk based metrics using evaluate method of training_log_parser module and the token based metrics using classification reports, then write the results in “eval_metric_files” folder. Default is False. save_dir (str): path to save the token based results dataframes, default is “results_token_based”. For more details and examples, please check ALAB Notebook. Example: alab.get_IAA_metrics(spark, conll_dir = path_to_conll_folder, annotator_names = [&quot;annotator_1&quot;,&quot;annotator_2&quot;,&quot;annotator_3&quot;,&quot;annotator_4&quot;], set_ref_annotator = &quot;annotator_1&quot;, return_NerDLMetrics = False, save_dir = &quot;./token_based_results&quot;) Assertion dataset preparation module now supports chunk start and end indices, rather than token indices. Here are the new features in get_assertion_data(); Now it returns the char_begin and char_end indices of the chunks. These columns can be used in AssertionDLApproach() annotator instead of token_begin and token_end columns for training an Assertion Status Detection model. Added included_task_ids parameter that allows you to prepare the assertion model training dataframe with only the included tasks. Default is None. Added seed parameter that allows you to get the same training dataframe at each time when you set unannotated_label_strategy. Default is None. For more details and examples, please check ALAB Notebook. Added ner_source in the ChunkConverter Metadata We added ner_source in the metadata of ChunkConverter output. In this way, the sources of the chunks can be seen if there are multiple components that have the same NER label in the same pipeline. Example: ... age_contextual_parser = ContextualParserApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;age_cp&quot;) .setJsonPath(&quot;age.json&quot;) .setCaseSensitive(False) .setPrefixAndSuffixMatch(False) chunks_age = ChunkConverter() .setInputCols(&quot;age_cp&quot;) .setOutputCol(&quot;age_chunk&quot;) ... sample_text = &quot;&quot;&quot;The patient is a 28 years old female with a history of gestational diabetes mellitus was diagnosed in April 2002 in County Baptist Hospital .&quot;&quot;&quot; Result: [Annotation(chunk, 17, 18, 28, {&#39;tokenIndex&#39;: &#39;4&#39;, &#39;entity&#39;: &#39;Age&#39;, &#39;field&#39;: &#39;Age&#39;, &#39;ner_source&#39;: &#39;age_chunk&#39;, &#39;chunk&#39;: &#39;0&#39;, &#39;normalized&#39;: &#39;&#39;, &#39;sentence&#39;: &#39;0&#39;, &#39;confidenceValue&#39;: &#39;0.74&#39;})] Core Improvements and Bug Fixes Added chunk confidence score in the RelationExtractionModel metadata Added confidence score in the DocumentLogRegClassifierApproach metadata Fixed non-deterministic Relation Extraction DL Models (30+ models updated in the model hub) Fixed incompatible PretrainedPipelines with PySpark v3.2.x and v3.3.x Fixed ZIP label issue on faker mode with setZipCodeTag parameter in Deidentification Fixed obfuscated numbers have the same number of chars as the original ones Fixed name obfuscation hashes in Deidentification for romanian language Fixed LightPipeline validation parameter for internal annotators LightPipeline support for GenericClassifier (FeatureAssembler) New and Updated Notebooks New Clinical Text Classification with Spark_NLP Notebook show how can use medical text with ClassifierDL, MultiClassifierDL, GenericClassifier, and DocumentLogRegClassifier New Clinical Text Classification with DocumentMLClassifier Notebook show how can use medical text with DocumentMLClassifier Updated ALAB Notebook with the changes in get_assertion_data() and the new get_IAA_metrics() method. New and Updated Demos SOCIAL DETERMINANT demo 9 New Clinical Models and Pipelines Added &amp; Updated in Total ner_oncology_anatomy_general_healthcare ner_oncology_biomarker_healthcare ner_oncology_unspecific_posology_healthcare ner_eu_clinical_case genericclassifier_sdoh_economics_binary_sbiobert_cased_mli genericclassifier_sdoh_substance_usage_binary_sbiobert_cased_mli genericclassifier_sdoh_tobacco_usage_sbiobert_cased_mli genericclassifier_sdoh_alcohol_usage_sbiobert_cased_mli genericclassifier_sdoh_alcohol_usage_binary_sbiobert_cased_mli For all Spark NLP for Healthcare models, please check: Models Hub Page Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_2_8",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_2_8"
  },
  "249": {
    "id": "249",
    "title": "Spark NLP for Healthcare Release Notes 4.3.0",
    "content": "4.3.0 Highlights 12 new clinical models and pipelines added &amp; updated (8 new clinical named entity recognition models including 4 social determinants of health models) New Chunk Mapper model for mapping RxNorm codes to drug brand names New text classification annotators (architectures) for training text classification models using SVM and Logistic Regression with sentence embeddings One-liner clinical deidentification module Certification_Training notebooks (written in johnsnowlabs library) moved to parent workshop folder Different validation split per epoch in MedicalNerApproach Core improvements and bug fixes New read_conll method for reading conll files as Conll.readDataset does but it returns pandas dataframe with document(task) ids. Updated documentation Allow using FeatureAssembler in pretrained pipelines. Fixed RelationExtractionModel running in LightPipeline Fixed get_conll_data method issue New and updated notebooks New Clinical Deidentification Utility Module Notebook. Updated Clinical_Named_Entity_Recognition_Model with Conll.readDataset examples. Updated Clinical Text Classification with Spark NLP with new GenericLogRegClassifierApproach and GenericSVMClassifierApproach examples. New and updated demos SOCIAL DETERMINANT NER demo SOCIAL DETERMINANT CLASSIFICATION demo SOCIAL DETERMINANT GENERIC CLASSIFICATION demo 13 new clinical models and pipelines added &amp; updated in total 12 New Clinical Models And Pipelines Added &amp; Updated (8 New Clinical Named Entity Recognition Models Including 4 Social Determinants of Health Models) We are releasing 4 new SDOH NER models that were trained by using embeddings_clinical embeddings model. model name description predicted entities ner_sdoh_wip Extracts terminology related to Social Determinants of Health from various kinds of biomedical documents. Other_SDoH_Keywords Education Population_Group Quality_Of_Life Housing Substance_Frequency Smoking Eating_Disorder Obesity Healthcare_Institution Financial_Status Age Chidhood_Event Exercise Communicable_Disease Hypertension Other_Disease Violence_Or_Abuse Spiritual_Beliefs Employment Social_Exclusion Access_To_Care Marital_Status Diet Social_Support Disability Mental_Health Alcohol Insurance_Status Substance_Quantity Hyperlipidemia Family_Member Legal_Issues Race_Ethnicity Gender Geographic_Entity Sexual_Orientation Transportation Sexual_Activity Language Substance_Use ner_sdoh_social_environment_wip Extracts social environment terminologies related to Social Determinants of Health from various kinds of biomedical documents. Social_Support Chidhood_Event Social_Exclusion Violence_Abuse_Legal ner_sdoh_demographics_wip Extracts demographic information related to Social Determinants of Health from various kinds of biomedical documents. Family_Member Age Gender Geographic_Entity Race_Ethnicity Language Spiritual_Beliefs ner_sdoh_income_social_status_wip Extracts income and social status information related to Social Determinants of Health from various kinds of biomedical documents. Education Marital_Status Financial_Status Population_Group Employment Example: ... clinical_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner_model = MedicalNerModel.pretrained(&quot;ner_sdoh_wip&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) sample_texts =&quot;Smith is a 55 years old, divorced Mexcian American woman with financial problems. She speaks spanish. She lives in an apartment. She has been struggling with diabetes for the past 10 years and has recently been experiencing frequent hospitalizations due to uncontrolled blood sugar levels. Smith works as a cleaning assistant and does not have access to health insurance or paid sick leave. She has a son student at college. Pt with likely long-standing depression. She is aware she needs rehab. Pt reprots having her catholic faith as a means of support as well. She has long history of etoh abuse, beginning in her teens. She reports she has been a daily drinker for 30 years, most recently drinking beer daily. She smokes a pack of cigarettes a day. She had DUI back in April and was due to be in court this week.&quot; Result: ++--++-+ |chunk |begin|end|ner_label | ++--++-+ |55 years old |11 |22 |Age | |divorced |25 |32 |Marital_Status | |Mexcian American |34 |49 |Race_Ethnicity | |financial problems|62 |79 |Financial_Status | |spanish |93 |99 |Language | |apartment |118 |126|Housing | |diabetes |158 |165|Other_Disease | |cleaning assistant|307 |324|Employment | |health insurance |354 |369|Insurance_Status | |son |401 |403|Family_Member | |student |405 |411|Education | |college |416 |422|Education | |depression |454 |463|Mental_Health | |rehab |489 |493|Access_To_Care | |catholic faith |518 |531|Spiritual_Beliefs | |support |547 |553|Social_Support | |etoh abuse |589 |598|Alcohol | |teens |618 |622|Age | |drinker |658 |664|Alcohol | |drinking beer |694 |706|Alcohol | |daily |708 |712|Substance_Frequency| |smokes |719 |724|Smoking | |a pack |726 |731|Substance_Quantity | |cigarettes |736 |745|Smoking | |a day |747 |751|Substance_Frequency| |DUI |762 |764|Legal_Issues | ++--++-+ We are releasing 8 new NER models which are trained by European Clinical Case Corpus (E3C), a project aimed at offering a freely available multilingual corpus of semantically annotated clinical narratives. ner_eu_clinical_case: This model extracts 6 different clinical entities based on medical taxonomies. ner_eu_clinical_condition: This model extracts one entity – clinical / medical conditions. model name lang predicted entities ner_eu_clinical_case es clinical_condition clinical_event bodypart units_measurements patient date_time ner_eu_clinical_case fr clinical_condition clinical_event bodypart units_measurements patient date_time ner_eu_clinical_case eu clinical_condition clinical_event bodypart units_measurements patient date_time ner_eu_clinical_condition en clinical_condition ner_eu_clinical_condition es clinical_condition ner_eu_clinical_condition eu clinical_condition ner_eu_clinical_condition fr clinical_condition ner_eu_clinical_condition it clinical_condition Example: word_embeddings = WordEmbeddingsModel.pretrained(&quot;w2v_cc_300d&quot;,&quot;es&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) ner = MedicalNerModel.pretrained(&quot;ner_eu_clinical_case&quot;, &quot;es&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) sample_text = &quot;&quot;&quot;Paciente de 59 años que refiere dificultad para caminar desde hace un mes aproximadamente. Presenta debilidad y dolor en los miembros inferiores, que mejora tras detenerse, acompañándose en ocasiones de lumbalgia no irradiada. En la exploración neurológica presenta habla hipofónica, facial centrado. Debido a la mala perfusión secundaria a la sepsis aparecieron lesiones necróticas en extremidades superiores y principalmente inferiores distales. Motilidad ocular interna y externa normal.&quot;&quot;&quot; Result: +++ |chunk |ner_label | +++ |Paciente de 59 años |patient | |refiere |clinical_event | |dificultad para caminar |clinical_event | |hace un mes aproximadamente|date_time | |debilidad |clinical_event | |dolor |clinical_event | |los miembros inferiores |bodypart | |mejora |clinical_event | |detenerse |clinical_event | |lumbalgia |clinical_event | |irradiada |clinical_event | |exploración |clinical_event | |habla |clinical_event | |hipofónica |clinical_event | |perfusión |clinical_event | |sepsis |clinical_event | |lesiones |clinical_event | |extremidades superiores |bodypart | |inferiores distales |bodypart | |Motilidad |clinical_event | |normal |units_measurements| +++ New Chunk Mapper Model for Mapping RxNorm Codes to Drug Brand Names We are releasing rxnorm_drug_brandname_mapper pretrained model that maps RxNorm and RxNorm Extension codes with their corresponding drug brand names. It returns 2 types of brand names called rxnorm_brandname and rxnorm_extension_brandname for the corresponding RxNorm or RxNorm Extension code. Example: ... chunkerMapper = ChunkMapperModel.pretrained(&quot;rxnorm_drug_brandname_mapper&quot;, &quot;en&quot;, &quot;clinical/models&quot;) .setInputCols([&quot;rxnorm_chunk&quot;]) .setOutputCol(&quot;mappings&quot;) .setRels([&quot;rxnorm_brandname&quot;, &quot;rxnorm_extension_brandname&quot;]) sample_text= [&#39;metformin&#39;, &#39;advil&#39;] Result: +--+-+--+--+ | drug_name|rxnorm_result| mapping_result| relation | +--+-+--+--+ | metformin| 6809|Actoplus Met (metformin):::Avandamet (metformin...| rxnorm_brandname| | metformin| 6809|A FORMIN (metformin):::ABERIN MAX (metformin)::...|rxnorm_extension_brandname| | advil| 153010| Advil (Advil)| rxnorm_brandname| | advil| 153010| NONE|rxnorm_extension_brandname| +--+-+--+--+ New Text Classification Annotators (Architectures) For Training Text Classification Models Using SVM and Logistic Regression With Sentence Embeddings We have a new text classification architecture called GenericLogRegClassifierApproach that implements a multinomial Logistic Regression with sentence embeddings. This is a single layer neural network with the logistic function at the output. The input to the model is FeatureVector (from any sentence embeddings) and the output is Category annotations with labels and corresponding confidence scores. Training data requires “text” and their “label” columns only and the trained model will be a GenericLogRegClassifierModel(). We have another text classification architecture called GenericSVMClassifierApproach that implements SVM (Support Vector Machine) classification. The input to the model is FeatureVector (from any sentence embeddings) and the output is Category annotations with labels and corresponding confidence scores. Taining data requires “text” and their “label” columns only and the trained model will be a GenericSVMClassifierModel(). Input types: FEATURE_VECTOR Output type: CATEGORY Example: features_asm = sparknlp_jsl.base.FeaturesAssembler() .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCol(&quot;feature_vector&quot;) gcf_graph_builder = sparknlp_jsl.annotators.TFGraphBuilder() .setModelName(&quot;logreg_classifier&quot;) .setInputCols([&quot;feature_vector&quot;]) .setLabelColumn(&quot;label&quot;) .setGraphFolder(&quot;/tmp/&quot;) .setGraphFile(&quot;log_reg_graph.pb&quot;) log_reg_approach = sparknlp_jsl.annotators.GenericLogRegClassifierApproach() .setLabelColumn(&quot;label&quot;) .setInputCols([&quot;feature_vector&quot;]) .setOutputCol(&quot;prediction&quot;) .setModelFile(f&quot;/tmp/log_reg_graph.pb&quot;) .setEpochsNumber(10) .setBatchSize(1) .setLearningRate(0.001) One-Liner Clinical Deidentification Module Spark NLP for Healthcare provides functionality to apply Deidentification using one-liner module called Deid. The Deid module is a tool for deidentifying Protected Health Information (PHI) from data in a file path. It can be used with or without ant Spark NLP NER pipelines. It can apply deidentification and obfuscation on different columns at the same time. It returns the deidentification &amp; obfuscation results as a spark dataframe as well as a csv or json file saved locally. The module also includes functionality for applying Structured Deidentification task to data from a file path. The function, deidentify(), can be used with a custom pipeline or without defining any custom pipeline. structured_deidentifier() function can be used for the Structured Deidentification task. Please see this notebook for the detailed usage and explanation of all parameters. Check here for the documentation of the module. Deidentification with a custom pipeline Example: from sparknlp_jsl import Deid deid_implementor= Deid( # required: Spark session with spark-nlp-jsl jar spark ) res= deid_implementor.deidentify( # required: The path of the input file. Default is None. File type must be &#39;csv&#39; or &#39;json&#39;. input_file_path=&quot;data.csv&quot;, #optional: The path of the output file. Default is &#39;deidentified.csv&#39;. File type must be &#39;csv&#39; or &#39;json&#39;. output_file_path=&quot;deidentified.csv&quot;, #optional: The separator of the input csv file. Default is &quot; t&quot;. separator=&quot;,&quot;, #optional: A custom pipeline model to be used for deidentification. If not specified, the default is None. custom_pipeline=nlpModel, #optional: Fields to be deidentified and their deidentification modes, by default {&quot;text&quot;: &quot;mask&quot;} fields={&quot;text_column_1&quot;: &quot;text_column_1_deidentified&quot;, &quot;text_column_2&quot;: &quot;text_column_2_deidentified&quot;}, #optional: The masking policy. Default is &quot;entity_labels&quot;. masking_policy=&quot;fixed_length_chars&quot;, #optional: The fixed mask length. Default is 4. fixed_mask_length=4) Result: ++-+-+-+-+ | ID| text_column_1| text_column_1_deidentified| text_column_2| text_column_2_deidentified| ++-+-+-+-+ | 0|Record date : 2093-01-13 , David Hale , M.D . , Name : Hendrickson ...|Record date : ** , ** , M.D . , Name : ** MR .|Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-...|Date : 10-16-1991 PCP : Alveda Castles , 26 years-old , Record date...| ++-+-+-+-+ Deidentification with no custom pipeline Example: from sparknlp_jsl import Deid deid_implementor= Deid( # required: Spark session with spark-nlp-jsl jar spark ) res= deid_implementor.deidentify( # required: The path of the input file. Default is None. File type must be &#39;csv&#39; or &#39;json&#39;. input_file_path=&quot;data.csv&quot;, #optional: The path of the output file. Default is &#39;deidentified.csv&#39;. File type must be &#39;csv&#39; or &#39;json&#39;. output_file_path=&quot;deidentified.csv&quot;, #optional: The separator of the input csv file. Default is &quot; t&quot;. separator=&quot;,&quot;, #optional: Fields to be deidentified and their deidentification modes, by default {&quot;text&quot;: &quot;mask&quot;} fields={&quot;text&quot;: &quot;mask&quot;}, #optional: The masking policy. Default is &quot;entity_labels&quot;. masking_policy=&quot;entity_labels&quot;) Result: ++-+-+ | ID| text_original| text_deid| ++-+-+ | 0| &quot;| &quot;| | 1|Record date : 2093-01-13 , David Hale , M.D . , Name : Hendrickson ...|Record date : &lt;DATE&gt; , &lt;DOCTOR&gt; , M.D . , Name : &lt;PATIENT&gt; , MR # &lt;...| | 2| &quot;| &quot;| ++-+-+ Structured Deidentification Example: from sparknlp_jsl import Deid deid_implementor= Deid( # required: Spark session with spark-nlp-jsl jar spark ) res= deid_implementor.structured_deidentifier( #required: The path of the input file. Default is None. File type must be &#39;csv&#39; or &#39;json&#39;. input_file_path=&quot;data.csv&quot;, #optional: The path of the output file. Default is &#39;deidentified.csv&#39;. File type must be &#39;csv&#39; or &#39;json&#39;. output_file_path=&quot;deidentified.csv&quot;, #optional: The separator of the input csv file. Default is &quot; t&quot;. separator=&quot;,&quot;, #optional: A dictionary that contains the column names and the tags that should be used for deidentification. Default is {&quot;NAME&quot;:&quot;PATIENT&quot;,&quot;AGE&quot;:&quot;AGE&quot;} columns_dict= {&quot;NAME&quot;: &quot;ID&quot;, &quot;DOB&quot;: &quot;DATE&quot;}, #optional: The seed value for the random number generator. Default is {&quot;NAME&quot;: 23, &quot;AGE&quot;: 23} columns_seed= {&quot;NAME&quot;: 23, &quot;DOB&quot;: 23}, #optional: The source of the reference file. Default is faker. ref_source=&quot;faker&quot;, #optional: The number of days to be shifted. Default is None shift_days=5) Result: +-++--++-+ | NAME| DOB| ADDRESS|SBP| TEL| +-++--++-+ |[N2649912]|[18/02/1977]| 711 Nulla St.|140| 673 431234| | [W466004]|[28/02/1977]| 1 Green Avenue.|140|+23 (673) 431234| | [M403810]|[16/04/1900]|Calle del Liberta...|100| 912 345623| +-++--++-+ Different Validation Split Per Epoch In MedicalNerApproach The validation splits in MedicalNerApproach used to be static and same for every epoch. Now we can control with behaviour with a new parameter called setRandomValidationSplitPerEpoch(bool) and allow users to set random validation splits per epoch. Certification_Training Notebooks (Written In Johnsnowlabs Library) Moved to Parent Workshop Folder re-organize and re-locate open-source-nlp folder re-organize and re-locate healthcare-nlp folder Core Improvements and Bug Fixes New read_conll method for reading conll files as Conll.readDataset does but it returns dataframe with document(task) ids. Updated documentation Allow using FeatureAssembler in pretrained pipelines. Fixed RelationExtractionModel running in LightPipeline Fixed get_conll_data method issue New and Updated Notebooks New Clinical Deidentification Utility Module Notebook. Updated Clinical_Named_Entity_Recognition_Model with Conll.readDataset examples. Updated Clinical Text Classification with Spark NLP with new GenericLogRegClassifierApproach and GenericSVMClassifierApproach examples. New and Updated Demos SOCIAL DETERMINANT NER demo SOCIAL DETERMINANT CLASSIFICATION demo SOCIAL DETERMINANT GENERIC CLASSIFICATION demo 12 New Clinical Models and Pipelines Added &amp; Updated in Total ner_eu_clinical_case-&gt; es ner_eu_clinical_case-&gt; fr ner_eu_clinical_case-&gt; eu ner_eu_clinical_condition-&gt; en ner_eu_clinical_condition-&gt; es ner_eu_clinical_condition-&gt; fr ner_eu_clinical_condition-&gt; eu ner_eu_clinical_condition-&gt; it ner_sdoh_demographics_wip ner_sdoh_income_social_status_wip ner_sdoh_social_environment_wip ner_sdoh_wip rxnorm_drug_brandname_mapper For all Spark NLP for Healthcare models, please check: Models Hub Page Versions Version Version Version 4.3.0 4.2.8 4.2.4 4.2.3 4.2.2 4.2.1 4.2.0 4.1.0 4.0.2 4.0.0 3.5.3 3.5.2 3.5.1 3.5.0 3.4.2 3.4.1 3.4.0 3.3.4 3.3.2 3.3.1 3.3.0 3.2.3 3.2.2 3.2.1 3.2.0 3.1.3 3.1.2 3.1.1 3.1.0 3.0.3 3.0.2 3.0.1 3.0.0 2.7.6 2.7.5 2.7.4 2.7.3 2.7.2 2.7.1 2.7.0 2.6.2 2.6.0 2.5.5 2.5.3 2.5.2 2.5.0 2.4.6 2.4.5 2.4.2 2.4.1 2.4.0",
    "url": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_3_0",
    "relUrl": "/docs/en/spark_nlp_healthcare_versions/release_notes_4_3_0"
  },
  "250": {
    "id": "250",
    "title": "Annotation Lab Release Notes 4.3.0",
    "content": "4.3.0 Release date: 25-11-2022 Annotation Lab 4.3.0 adds support for Finance and Legal NLP Libraries, Finance and Legal License Scopes, and access to pre-trained Visual NER models on the Models Hub. It also allows easy task import directly from S3 and keeps the complete history of training logs. The release also includes stabilization and fixes for several issues reported by our user community. Here are the highlights of this release: Highlights Support for Finance NLP and Legal NLP. Annotation Labs now includes a full-fledged integration with two new NLP libraries: Finance NLP and Legal NLP. Pretrained models for the Finance and Legal verticals are now available on the Models Hub page, covering tasks such as Entity Recognition, Assertion Status, and Text Classification. Searching models on Models Hub. A new filter named Edition was added to the Models Hub. It includes all supported NLP editions: Healthcare, Opensource, Legal, Finance, and Visual. It will ease search for models specific to an Edition, which can then easily be downloaded and used within Annotation Lab projects. Support for Finance and Legal Licenses. Annotation Lab now supports import of licenses with legal and/or finance scopes. It can be uploaded from the Licenses page. Similar to Healthcare and Visual licenses, they unlock access to optimized annotators, models, embeddings, and rules. Pre-annotations using Finance and Legal models. Finance and Legal models downloaded from the Models Hub can be used for pre-annotation in NER, assertion status, and classification projects. Train Finance and Legal models. Two new options: Legal and Finance libraries were added for selection when training a new NER model in Annotation Lab. The new options are only available when at least one valid license with the corresponding scope is added to the License page. Import tasks from S3. Annotation Lab now supports importing tasks/documents stored on Amazon S3. In the Import Page, a new section was added which allows users to define S3 connection details. All documents in the specified path will then be imported as tasks in the current project. Project level history of the Trained Models. It is now possible to keep track of all previous training activities executed for a project. When pressing the History button from the Train page, users are presented with a list of all trainings triggered for the current project. Easier page navigation. Users can now right-click on the available links and select “Open in new tab” to open the link in a new tab without losing the current work context. Optimized user editing UI. All the checkboxes on the Users Edit page now have the same style. The “UserAdmins” group was renamed to “Admins” and the description of groups is more detailed and easier to understand. Also, a new error message is shown when an invalid email address is used. Improved page navigation for Visual NER projects. For Visual NER projects, users can jump to a specific page in any multi-page task instead of passing through all pages to reach a target section of a PDF document. Visual configuration options for Visual NER project. Users are now able to add custom labels and choices in the project configuration from the Visual tab for Visual NER projects as well as for the text projects. Visual NER Models available on the Models Hub page. Visual NER models can now be filtered, downloaded from the NLP Models Hub, and used for pre-annotating image-based documents. Lower CPU and Memory resources allocated to the license server. In this version, the resources allocated to the license server were decreased to CPU: 1000m (1 core) and Memory: 1GB. Simplify Training and Pre-annotation configurations. Now the user only need to adjust “Memory limit” and “CPU limit” in the Infrastructure page. “Spark Drive Memory” is calculated as 85% of Memory Limit where are “Spark Kryo Buff Max” and “Spark Driver Max Result Size” are constants with values “2000 MB” and “4096 MB” respectively. Auto-close user settings. The user settings menu is closed automatically when a user clicks on any other settings options. Preserve task filters. From version 4.3.0, all defined filters in the task page remain preserved when the user navigates back and forth between the labeling page and the task page. Optimized Alert Messages. All the alert notification shows clear errors, warnings, information, and success messages. Zoom in/out features in Visual NER projects with Sticky Left Column view. In various views of Visual NER, zoom-controlling features are now available by default. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_4_3_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_4_3_0"
  },
  "251": {
    "id": "251",
    "title": "Visual NLP(Spark OCR) release notes 4.3.0",
    "content": "4.3.0 Release date: 2023-01-13 We are glad to announce that Spark OCR 4.3.0 has been released!! This big release comes with improvements in Dicom Processing, Visual Question Answering, new Table Extraction annotators, and much more!. New Features PositionFinder now works in LightPipelines. New annotator HocrToTextTable to work together with PdfToHocr that allows table extraction from digital PDFs. This allows to extract tables using a mixed pipeline in which tables are detected using visual features, but the text is pulled directly from the digital layer of the PDF yielding near to perfect results, and removing OCR overhead. New Dicom Processing improvements, Added support of Dicom documents to BinaryFile Datasource: this allows to write Dicom documents from Spark Dataframes to all data storages supported by Spark, in batch and streaming mode. Added possibility to specify name of the files in BinaryFile Datasource: now we can store images, PDFs, Dicom files directly using Spark capabilities with names of our choice, overcoming the limitation imposed by Spark of naming files according to partitions. Added DicomToMetadata Transformer: it allows to extract metadata from the Dicom documents. This allows to analyze Dicom metadata using Spark capabilities. For example, collect statistic about color schema, number of frames, compression of the images. This is useful for estimating needed resources and time before starting to process a big dataset. Added DicomToImageV3 based on Pydicom with better support of different color schemas. Added support YBR_FULL_422, YBR_FULL images. Also fixed handling pixel data with different pixel size for RGB and Monochrome images. Added support for compression after update pixel data in DicomDrawRegions. This reduces size of output Dicom files by applying JPEGBaseline8Bit compression to the pixel data. Added support for different color schemas in DicomDrawRegions. Added support YBR_FULL_422, YBR_FULL images. Added support for coordinates with rotated bounding box in DicomDrawRegions for compatibility with ImageTextDetectorV2. Fixed ImageTextDetectorV2 for images without text. New Donut based VisualQuestionAnswering annotator. Supports two modes of operation: it can receive an array of questions in the same row as the input image; in this way, each input image can be queried by an arbitrary set of user-defined questions, and also questions can be defined globally outside the Dataframe. This will cause that all images will be queried by the same set of questions. Running time is about a half the time per question when compared to the open-source version. Optimized model is smaller(about a half) of the original open-source version, making it easier to download and distribute in a cluster. Two models available: docvqa_donut_base and docvqa_donut_base_opt(quantized). LightPipelines support. Bug Fixes Empty tables now handled properly in ImageCellsToTextTable. Pretrained models for VisualDocumentNerV21 are now accessible. New/updated Notebooks SparkOcrVisualQuestionAnswering.ipynb, this notebook shows examples on how to use Donut based visual question answering in Spark-OCR. SparkOCRPdfToTable.ipynb, this notebook shows how PdfToHocr and HocrToTextTable can be put together to do table extraction without OCR, by just relying on the digital layer of text in the PDF. Still, existent well tested table detection models, continue to be used for finding the tables. SparkOcrImageTableRecognitionWHOCR.ipynb, this notebook shows table detection, and the HocrToTextTable in action. Compared to previous implementations, now the OCR method is external, and it can be replaced by different implementations(even handwritten!). This release is compatible with Spark NLP for Healthcare 4.2.4, and Spark NLP 4.2.4. Previous versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_4_3_0",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_4_3_0"
  },
  "252": {
    "id": "252",
    "title": "Visual NLP(Spark OCR) release notes 4.3.1",
    "content": "4.3.1 Release date: 17-02-2023 We’re glad to announce that Visual NLP 😎 4.3.1 has been released. Highlights ImageTextCleaner &amp; ImageTableDetector have improved memory consumption. New Annotators supported in LightPipelines. Table extraction from Digital PDFs pipeline now entirely supported as a LightPipeline. ImageTextCleaner &amp; ImageTableDetector improved memory consumption ImageTextCleaner &amp; ImageTableDetector improved memory consumption: we reduced about 30% the memory consumption for this annotator making it more memory friendly and enabling running on memory constrained environments like Colab. New Annotators supported in LightPipelines Now the following annotators are supported in LightPipelines, PdfToHocr, HocrTokenizer, ImageTableDetector, ImageScaler, HocrToTextTable, Table extraction from Digital PDFs pipeline now entirely supported as a LightPipeline. Our Table Extraction from digital PDFs pipeline now supports running as a LightPipeline, check the updated notebook: SparkOCRPdfToTable.ipynb This release is compatible with Spark NLP for Healthcare 4.3.0, and Spark NLP 4.3.0. Previous versions 4.3.0 4.2.4 4.2.1 4.1.0 4.0.2 4.0.0 3.14.0 3.13.0 3.12.0 3.11.0 3.10.0 3.9.1 3.9.0 3.8.0 3.7.0 3.6.0 3.5.0 3.4.0 3.3.0 3.2.0 3.1.0 3.0.0 1.11.0 1.10.0 1.9.0 1.8.0 1.7.0 1.6.0 1.5.0 1.4.0 1.3.0 1.2.0 1.1.2 1.1.1 1.1.0 1.0.0",
    "url": "/docs/en/spark_ocr_versions/release_notes_4_3_1",
    "relUrl": "/docs/en/spark_ocr_versions/release_notes_4_3_1"
  },
  "253": {
    "id": "253",
    "title": "Annotation Lab Release Notes 4.4.0",
    "content": "4.4.0 Release date: 05-12-2022 Annotation Lab 4.4.0 brings performance matrix and benchmarking information for NER and classification models - both imported from NLP Models Hub and/or trained locally. Furthermore, with this release, tasks can be explicitly assigned to Project Owners for annotators and/or reviewers. The release also includes several improvements and fixes for issues reported by our users. Here are the highlights of this release: Highlights Benchmarking information for Classification models. Benchmarking information is now available for Classification models. It includes the confusion matrix in the training logs and is also available on the models on the Models page, which is accessible by clicking on the benchmarking icon. Task Assignment for Project Owners. Project Owners can now be explicitly assigned as annotators and/or reviewers for tasks. It is useful when working in a small team and when the Project Owners are also involved in the annotation process. A new option “Only Assigned” checkbox is now available on the labeling page that allows Project Owners to filter the tasks explicitly assigned to them when clicking the Next button. New Role available on the Team Page. On the Team Setup page, the project creator is now clearly identified by the “Owner” role. Rules Available in the Finance and Legal Editions. Rules can now be deployed and used for pre-annotation using the Legal and Finance licenses. UX Improvement for Completion. The action icons are now available on the completions, and users can directly execute the appropriate action without having to select the completion first. IAA chart improvements. NER labels and Assertion Status labels are now handled separately in the IAA charts on the Analytics page. The filter for selecting the label type is added on the respective charts. Import tasks with title field. Users can now import the tasks with title information pre-defined in the JSON/CSV. The title field was also added to the sample task file that can be downloaded from the Import page. Rename Models Hub page. The name Models HUB on the left navigation panel has been changed to Hub. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_4_4_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_4_4_0"
  },
  "254": {
    "id": "254",
    "title": "Annotation Lab Release Notes 4.4.1",
    "content": "4.4.1 Release date: 07-12-2022 Annotation Lab 4.4.1 hotfix has beed released and it includes few features, enhancements, and bug fixes. Here are the highlights of this release: Highlights Users can now delete the relations using the backspace key (on windows) or delete key (on mac) or using the delete action icon on Relations widget. Unsupported Legal and Finance models are now hidden on the Models Hub Issue when deploying pre-annotation server for some assertion models have been fixed. The “Only Assigned” checkbox state is preserved when user moves to the next task. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_4_4_1",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_4_4_1"
  },
  "255": {
    "id": "255",
    "title": "Annotation Lab Release Notes 4.5.0",
    "content": "4.5.0 Release date: 01-01-2023 Over the last year, Annotation Lab has grown to be much more than a document annotation tool. It became a full-fledged AI system, capable of testing pre-trained models and rules, applying them to new datasets, training, and tuning models, and exporting them to be deployed in production. All those features together with the new Playground concept presented in the current release notes contributed to the transformation of the Annotation Lab into the NLP Lab. A new Playground feature is released as part of the NLP Lab’s Hub that allows users to quickly test any model and/or rule on a snippet of text without the need to create a project and import tasks. NLP Lab also supports the training of Legal and Finance models and Model evaluation for classification projects. As always the release includes some stabilization and bug fixes for issues reported by our user community. Below are the details of what has been included in this release. NLP Lab’s Playground NLP Lab introduces the Playground feature where users can directly deploy and test models and/or rules. In previous versions, the pre-annotation servers could only be deployed from within a given project. With the addition of the Playground, models can easily be deployed and tested on a sample text without going through the project setup wizard. Any model or rule can now be selected and deployed for testing by clicking on the “Open in Playground” button. Rules are deployable in the Playground from the rules page. When a particular rule is deployed in the playground, the user can also change the parameters of the rules on the right side of the page. After saving the changes users need to click on the “Deploy” button to refresh the results of the pre-annotation on the provided text. Deployment of models and rules is supported by floating and air-gapped licenses. Healthcare, Legal, and Finance models require a license with their respective scopes to be deployed in Playground. Unlike pre-annotation servers, only one playground can be deployed at any given time. Export trained models to the S3 bucket With this release, users can easily export trained models to a given s3 bucket. This feature is available on the Available Models page under the Hub tab. Users need to enter the S3 bucket path, S3 access key, and S3 secret key to upload the model to the S3 bucket. Support Training of Finance and Legal Models With this release, users can perform training of Legal and Finance models depending on the available license(s). When training a new model in the NLP Lab, users have the option to select what library to use. Two options were available up until now: Open source and Healthcare. This release adds two new options: Legal and Finance. This helps differentiate the library used for training the models. The new options are only available when at least one valid license with the corresponding scope is added to the License page. Improvements Keyword-based Search at task level Finding tokens on the Visual NER project was restricted to only one page, and searching for keywords from the labeling page on a text-based project was not available. NLP Lab supports task-level keyword-based searches. The keyword-based search feature will work for text and Visual NER projects alike. The search will work on all paginated pages. It is also possible to navigate between search results, even if that result is located on another page. Important Previously this feature was implemented with the help of tag in the Visual NER project configurations. With the implementation of search at task level, the previous search tag should be removed from existing visual NER projects. Config to be removed from all existing Visual NER project: &lt;Search name=&quot;search&quot; toName=&quot;image&quot; placeholder=&quot;Search&quot;/&gt; Chunk-based Search in Visual NER tasks In previous versions, users could only run token-based searches at page level. The search feature did not support searching a collection of tokens as a single chunk. With this release, users can find a chunk of tokens in the Visual NER task. Model Evaluation for Classification Projects Up until now, the Annotation Lab only supported test and model evaluation for the NER-based projects. From this version on, NLP Lab supports test and model evaluation for Classification project as well. Evaluation results can now be downloaded if needed. Hide and Unhide Regions in NER project In this version, we support the hide/show annotated token regions feature in the text-based project in the same way as it was available in the Visual NER project. Ground Truth can only be set/unset by the owner of the completion With this version, we have improved the feature to set/unset ground truth for a completion submitted by an annotator. Now, for the Manager/Project Owner/Reviewer, the button to set/unset ground truth is disabled. The ground truth can only be updated by the annotator who submitted the completion or is unset when a submitted completion is rejected by a reviewer. Finite Zoom Out Level in Visual NER tasks Previously, users could zoom in and zoom out again on images while working with the Visual NER project, but the user could not get what the last stage of zoom-out was. Now, when the user zooms out of the image if it is the last phase then the zoom-out button will automatically be disabled so the user knows where to stop zooming out next. Taxonomy Location Customizable from the Project Configuration There are many different views available for each project template. This diversity can be confusing for users. For eliminating this complexity, the View tab was removed from the project configuration page and replaced by an “orientation” option that can be directly applied to the project configuration. Orientation will decide, where the taxonomy (labels, choices, text, images, etc.) will be located on the labeling screen i.e. placed at the top, bottom or next to the annotation screen. Pre-annotation CPU requirement message in Visual NER projects By default, the pre-annotation server uses 2 CPUs. For Visual NER pre-annotation, it is likely that 2 CPUs are not enough. Now a friendly message is shown during the deployment of Visual NER pre-annotation if the CPU count is less than or equal to 2. Bug Fixes Expanding the text on the Labelling page visually does not expand the labeling area Previously, expanding the text area on the labeling page did not make any changes in the text expansion. This issue has been fixed. Now, expanding the text will change the text area to full-screen mode. Revoking granted analytics request do not update the revoked section Earlier, when an analytics request was revoked, the corresponding entry was not shown in the revoked section. We have fixed this issue. With NLP Lab 4.5.0, the revoked entries are available in the revoked section. Also, when an analytics request is revoked, in the revoked section, two new actions, Accept and Delete, are available. Show Confidence score in Regions option is not working properly for non-Visual NER tasks For none Visual NER tasks, enabling/disabling “Show Confidence score in Regions” from Layout did not change the UI. The changes only appear when the page was reloaded or when the Versions tab was clicked. This issue has been fixed in this version. Username validation is missing when creating a new user With this version, the issue related to the missing validation of the username when creating a new user has been fixed. Issues with role selection on Teams page When a user was added to the project team as a new team member, the recently added user name was still visible in the search bar. This issue has been fixed. Clicking on the eye icon to hide a labeled region removes the region from the Annotations widget Previously, when a user clicked on the eye icon to hide a label, the labeled region was removed from the Annotations widget. Furthermore, the color of the label was also changed in the panel. This issue has been fixed. Deployed legal and finance models servers are not associated with their respective licenses In the previous version, when a Legal and Finance model server was deployed, the respective licenses were not associated with their deployed server. The availability of the Legal and Finance license was checked when the models were deployed. Version 4.5.0 fixes this bug. Model Evaluation cannot be triggered using an air-gapped healthcare license with scope training/inference The issue of triggering Model Evaluation using an air-gapped healthcare license with the training/inference scope has been fixed. When user enabled “Allow user for custom selection of regions”, token values are missing in JSON export Earlier, when the user annotates tokens while enabling “Allow user for custom selection of regions” and exports the completion. The token values were missing from the JSON export. In this version, the issue is fixed, and all the token fields and values are available in the JSON Pre-annotation server with pending status is not removed when the user deletes the server from the cluster page Deleting the pre-annotation server with status pending from the cluster page did not delete the pod from Kubernetes and created multiple pre-annotation pods. This issue has been fixed. Project export with space in the name is allowed to be imported In the earlier version, the users could import previously exported projects with space in the project’s name. Though the project was listed on the projects page, the project could not be deleted. Also, the user was unable to perform any operations on the project. The “Only Assigned” checkbox overlaps the review dialog box The overlap between the “Only Assigned” checkbox and the review dialog box was fixed. Open-source Models cannot be downloaded in the NLP Lab without a license Previously open-source models could not be downloaded from the NLP models hub when there was no license uploaded. This issue has been fixed. Now all open-source licenses are downloadable without any issue. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_4_5_0",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_4_5_0"
  },
  "256": {
    "id": "256",
    "title": "Annotation Lab Release Notes 4.5.1",
    "content": "4.5.1 Release date: 05-01-2023 This release includes some stabilization and bug fixes for issues reported by our user community. Below are the details of what has been included in this release. Improvement Name of available models should be visible completely in the Predefined Labels tab Bug Fixes Finance models cannot be downloaded to NLP Lab with a floating license from Models Hub Trained visual NER model is not listed in the predefined labels section on the configuration page Error due to circular dependency of logger Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_4_5_1",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_4_5_1"
  },
  "257": {
    "id": "257",
    "title": "Annotation Lab Release Notes 4.6.2",
    "content": "4.6.2 Release date: 21-01-2023 NLP Lab 4.6.2 comes with support for zero-shot learning via prompts. Prompt engineering is a very recent but rapidly growing discipline that aims to guide language models such as GPT-3 to generate specific and desired outputs, such as answering a question or writing a coherent story. This version of the NLP Lab, adds support for the creation and use of prompts for entities and relations identification within text documents. The goal of prompt engineering in this context is designing and crafting some questions, which are fed into a question-answering model together with some input text. The purpose is to guide the language model to generate specific and desired outputs, such as identifying entities or relations within the input text. This release offers features such as creation and editing of prompts, a dedicated section for prompts management and sharing inside the resources Hub, an optimized configuration page allowing mixing models, prompts, and rules into the same project, and support for quick prompts deployments and testing to the Playground. Prompts on the Hub The resources Hub has a new page dedicated to prompts. It allows users to easily discover and explore the existing prompts or create new prompts for identifying entities or relations. Currently, NLP Lab supports prompts for Healthcare, Finance, and Legal domains applied using pre-trained question-answering language models published on the NLP Models Hub and available to download in one click. The main advantage behind the use of prompts in entity or relation recognition is the ease of definition. Non-technical domain experts can easily create prompts, test and edit them on the playground on custom text snippets and, when ready, deploy them for pre-annotation as part of larger NLP projects. Together with rules, prompts are very handy in situations where no pre-trained models exist, for the target entities and domains. With rules and prompts the annotators never start their projects from scratch but can capitalize on the power of zero-shot models and rules to help them pre-annotate the simple entities and relations and speed up the annotation process. As such the NLP Lab ensures fewer manual annotations are required from any given task. Creating NER Prompts NER prompts, can be used to identify entities in natural language text documents. Those can be created based on healthcare, finance, and legal zero-shot models selectable from the “Domain” dropdown. For one prompt, the user adds one or more questions for which the answer represents the target entity to annotate. Creating Relation Prompts Prompts can also be used to identify relations between entities for healthcare, finance, and legal domains. The domain-specific zero-shot model to use for detecting relation can be selected from the “Domain” dropdown. The relation prompts are defined by a pair of entities related by a predicate. The entities can be selected from the available dropdowns listing all entities available in the current NLP Lab (included in available NER models or rules) for the specified domain. A simplified configuration wizard allows the reuse of models, rules, and prompts The project configuration page was simplified by grouping into one page all available resources that can be reused for pre-annotation: models, rules, and prompts. Users can easily mix and match the relevant resources and add them to their configuration. Note: One project configuration can only reuse the prompts defined by one single zero-shot model. Prompts created based on multiple zero-shot models (e.g. finance or legal or healthcare) cannot be mixed into the same project because of high resource consumption. Furthermore, all prompts require a license with a scope that matches the domain of the prompt. Experiment with prompts in Playground NLP Lab’s Playground supports the deployment and testing of prompts. Users can quickly test the results of applying a prompt on custom text, can easily edit the prompt, save it, and deploy it right away to see the change in the pre-annotation results. Zero-Shot Models available in the NLP Models Hub NLP Models Hub now lists the newly released zero-shot models that are used to define prompts. These models need to be downloaded to NLP Lab instance before prompts can be created. A valid license must be available for the models to be downloaded to NLP Lab. Bug Fixes Error while deploying classification model to the playground Previously, deploying the classification model to the playground had some issues which have been fixed in this version. **Information on the model’s details not visible completely on the playground ** In this version, we have fixed an issue related to the visibility of the information for Edition, Uploaded by, and Source inside the Models Detail accordion. Now, the UI can handle long model names on the playground page. Undo and Reset buttons are not working With release 4.6.2, issues regarding undo/redo buttons in the labeling page for annotated tokens have been fixed. Now, the Undo and Redo button works as expected. Finance and Legal models cannot be downloaded to NLP Lab with a floating license from Models Hub Earlier, users were not able to download the Finance and Legal model from the NLP Models HUB page using floating licenses. This issue has been fixed. Now, legal and finance models are downloadable in the NLP lab using a floating license. Pre-annotation server cannot be deployed for Visual NER This version also fixes the issue of failing to deploy the pre-annotation server for Visual NER models. Draft saved is seen for submitted completion Previously, in the NER task when the user clicked on regions of a previously submitted completion and viewed the versions submitted by the users, a draft was saved. A draft should not be created and saved for submitted completions. This issue was fixed in 4.6.2. Training fails for NER when embedding_clinical is used and the license type is open-source Earlier it was not possible to train a NER model with the open-source library using embeddings_clinical. This issue has been fixed. Hence users can now train open-sourced models with embeddings_clinical. UI goes blank for the Visual NER project when an annotation is saved and the next button is clicked In the previous version, annotators were not served the next task after clicking the Next button. A blank page with a console error was seen. Now the next task is served in the Visual NER project without any error. Pre-annotation server cannot be deployed for RE model There was an issue with the deployment of trained NER models with a relation extraction model. This issue has been fixed in this version. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_4_6_2",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_4_6_2"
  },
  "258": {
    "id": "258",
    "title": "Annotation Lab Release Notes 4.6.3",
    "content": "4.6.3 Release date: 31-01-2023 NLP Lab v4.6.3 is available which includes improvements for Playground and Prompt Engineering features introduced in v4.5 and v4.6. Here are some of them: Prompt (relation) using 2 different NER models is possible Ability to add long texts with new lines in the playground Issue when finance models are directly deployed to playground from the Hub page is fixed Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_4_6_3",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_4_6_3"
  },
  "259": {
    "id": "259",
    "title": "Annotation Lab Release Notes 4.6.5",
    "content": "4.6.5 Release date: 08-02-2023 NLP Lab v4.6.5, which includes significant optimizations and bugfixes for Project Analytics and the Prompt Engineering feature. The following are some of the key updates included in this release: The issue with the all_extracted_chunks chart not updating in the analytics page has now been resolved. The performance of project analytics operations has been improved, allowing for faster calculation of results. Limits have been added to the prompt description and prompt questions, ensuring that the text does not crash the UI. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_4_6_5",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_4_6_5"
  },
  "260": {
    "id": "260",
    "title": "Annotation Lab Release Notes 4.7.1",
    "content": "4.7.1 Release date: 22-02-2023 The latest version of NLP Lab, version 4.7.1, brings several enhancements that are worth highlighting. One of the most notable improvements is in relation prompts. NLP Lab now offers support for combining NER models, prompts and rules when defining relation prompts. The playground feature in NLP Lab has also received some noteworthy upgrades in version 4.7.1. The “playground” environment was initially added to facilitate experiments with different NLP models, tweak prompts and rules, and explore the potential of language models in a safe, sandboxed environment. The improvements made to the playground in this version are expected to enhance the overall user experience, and to make the environment faster and more responsive. In addition to these improvements, the latest version of NLP Lab has extended support for importing large task archives. This means that users can now work with bigger datasets more efficiently, which will undoubtedly save them time and effort. Below are the specifics of the additions included in this release: Improvements in Prompts Build Relation Prompts using NER Models, Prompts and Rules In previous version, relation prompts could be defined based on NER models and rules. In this release, NLP Lab allows for NER prompts to be reused when defining relation prompts. To include a NER prompt within a relation prompt, users need to navigate to the Questions section of the Relation Prompt creation page and search for the prompt to reuse. Once the NER prompt has been selected, users can start defining the question patterns. For example, users could create prompts that identify the relationship between people and the organizations they work for, or prompts that identify the relationship between a place and its geographic coordinates. The ability to incorporate NER prompts into relation prompts is a significant advancement in prompts engineering, and it opens up new possibilities for more sophisticated and accurate natural language processing. Improvements in Playground Direct Navigation to Active Playground Sessions Navigating between multiple projects to and from the playground experiments can be necessary, especially when you want to revisit a previously edited prompt or rule. This is why NLP Lab Playground now allow users to navigate to any active Playground session without having to redeploy the server. This feature enables users to check how their resources (models, rules and prompts) behave at project level, compare the preannotation results with ground truth, and quickly get back to experiments for modifying prompts or rules without losing progress or spending time on new deployments. This feature makes experimenting with NLP prompts and rules in a playground more efficient, streamlined, and productive. Automatic Deployment of Updated Rules/Prompts Another benefit of experimenting with NLP prompts and rules in the playground is the immediate feedback that you receive. When you make changes to the parameters of your rules or to the questions in your prompts, the updates are deployed instantly. Manually deploying the server is not necessary any more for changes made to Rules/Prompts to be reflected in the preannotation results. Once the changes are saved, by simply clicking on the Test button, updated results are presented. This allows you to experiment with a range of variables and see how each one affects the correctness and completeness of the results. The real-time feedback and immediate deployment of changes in the playground make it a powerful tool for pushing the boundaries of what is possible with language processing. Playground Server Destroyed after 5 Minutes of Inactivity When active, the NLP playground consumes resources from your server. For this reason, NLP Lab defines an idle time limit of 5 minutes after which the playground is automatically destroyed. This is done to ensure that the server resources are not being wasted on idle sessions. When the server is destroyed, a message is displayed, so users are aware that the session has ended. Users can view information regarding the reason for the Playground’s termination, and have the option to restart by pressing the Restart button. Playground Servers use Light Pipelines The replacement of regular preannotation pipelines with Light Pipelines has a significant impact on the performance of the NLP playground. Light pipelines allow for faster initial deployment, quicker pipeline update and fast processing of text data, resulting in overall quicker results in the UI. Direct Access to Model Details Page on the Playground Another useful feature of NLP Lab Playground is the ability to quickly and easily access information on the models being used. This information can be invaluable for users who are trying to gain a deeper understanding of the model’s inner workings and capabilities. In particular, by click on the model’s name it is now possible to navigate to the NLP Models hub page. This page provides users with additional details about the model, including its training data, architecture, and performance metrics. By exploring this information, users can gain a better understanding of the model’s strengths and weaknesses, and use this knowledge to make more informed decisions on how good the model is for the data they need to annotate. Improvements in Task Import Support for Large Document Import One of the challenges when working on big annotation projects is dealing with large size tasks, especially when uploading them to the platform. This is particularly problematic for files/archives larger than 20 MB, which can often lead to timeouts and failed uploads. To address this issue, NLP Lab has implemented chunk file uploading on the task import page. Chunk file uploading is a method that breaks large files into smaller, more manageable chunks. This process makes the uploading of large files smoother and more reliable, as it reduces the risk of timeouts and failed uploads. This is especially important for NLP practitioners who work with large datasets, as it allows them to upload and process their data more quickly and effectively. Versions Version Version Version 4.7.1 4.6.5 4.6.3 4.6.2 4.5.1 4.5.0 4.4.1 4.4.0 4.3.0 4.2.0 4.1.0 3.5.0 3.4.1 3.4.0 3.3.1 3.3.0 3.2.0 3.1.1 3.1.0 3.0.1 3.0.0 2.8.0 2.7.2 2.7.1 2.7.0 2.6.0 2.5.0 2.4.0 2.3.0 2.2.2 2.1.0 2.0.1",
    "url": "/docs/en/alab/annotation_labs_releases/release_notes_4_7_1",
    "relUrl": "/docs/en/alab/annotation_labs_releases/release_notes_4_7_1"
  },
  "261": {
    "id": "261",
    "title": "Resolve Entities to Terminology Codes - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/resolve_entities_codes",
    "relUrl": "/resolve_entities_codes"
  },
  "262": {
    "id": "262",
    "title": "Risk and Factors - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/risk_factors",
    "relUrl": "/risk_factors"
  },
  "263": {
    "id": "263",
    "title": "Rules",
    "content": "Rule based annotation is supported by Healthcare NLP, Finance NLP, and Legal NLP via the ContextualParser Annotator. Annotation Lab supports creating and using rules in a NER project using any one of these libraries with the presence of valid license. Users in the Admins group can see and edit the available rules on the Rules page under the Models Hub menu. Users can create new rules using the + Add Rules button. Users can also import and export the rules. There are two types of rules supported: Regex Based: Users can define a regex that will be used to label all possible hit chunks and label them as the target entity. For example, for labeling height entity the following regex can be used [0-7]&#39;((0?[0-9])|(1(0|1)))&#39;&#39;. All hits found in the task’s text content that match the regex are pre-annotated as height. Dictionary-Based: Users can define and upload a CSV dictionary of keywords that cover the list of chunks that should be annotated as a target entity. For example, for the label female, all occurrences of strings woman, lady, and girl within the text content of a given task will be pre-annotated as female. After adding a rule, the Project Owner or Manager can add the rule to the configuration of the project where they want to use it. This can be done from the Rules screen of the Project Configuration step on the Project Setup page. A valid Healthcare, Finance or Legal NLP license is required to deploy rules as a pre-annotation server after completing the project configuration step. The user is notified every time a rule in use is edited with the message “Redeploy preannotation server to apply these changes” on the Edit Rule form. Import and Export Rules Annotation Lab allows importing and exporting Rules from the Rules page. Import Rules Users can import rules from the Rules page. The rules can be both dictionary based or regex based. The rules can be imported in the following formats: JSON file or content. Zip archive of JSON file/s. Export Rules To export any rule, the user need to select the available rules and click on Export Rules button. Rules are then downloaded as a zip file. The zip file contains the JSON file for each rule. These exported rules can again be imported to Annotation Lab. The following blog posts explain how to create and use rules for jump starting your annotation projects: Using Rules to Jump Start Text Annotation Projects Using Rules and Pretrained Models in Text Annotation Projects Training and tuning models based on Rule-based annotation of text documents",
    "url": "/docs/en/alab/rules",
    "relUrl": "/docs/en/alab/rules"
  },
  "264": {
    "id": "264",
    "title": "Security and Privacy",
    "content": "We understand and take the security issues as the highest priority. On every release, all our artifacts and images ran through a series of security testing - Static Code analysis, Pen Test, Images Vulnerabilities Test, AWS AMI Scan Test. Every identified critical issue is remediated, code gets refactored to pass our standard Static Code Analysis. Role-based access Role-based access control is available for all Annotation Lab deployments. By default, all projects are private to the user who created them – the project owner. If necessary, project owners can add other users to the project and define their role(s) among annotator, reviewer, manager. The three roles supported by Annotation Lab offer different levels of task and feature visibility. Annotators can only see tasks assigned to them and their own completions. Reviewers can see the work of annotators who created completions for the tasks assigned to them. Annotators and reviewers do not have access to task import or annotation export nor to the Models Hub page. Managers have higher level of access. They can see all tasks content, can assign work to annotators and reviewers, can import tasks, export annotations, see completions created by team members or download models. When creating the annotation team, make sure the appropriate role is assigned to each team member according to the Need-To-Know Basis. Screen capture is not disabled, and given the high adoption of mobile technologies, team members can easily take pictures of the data. This is why, when dealing with sensitive documents, it is advisable to conduct periodical HIPPA/GDPR training with the annotation team to avoid data breaches. Data sharing Annotation Lab runs locally - all computation and model training run inside the boundaries of your deployment environment. The content related to any tasks within your projects is NOT SHARED with anyone. The Annotation Lab does not call home. Access to internet is used ONLY when downloading models from the NLP Models Hub. Document processing - OCR, pre-annotation, training, fine-tuning- runs entirely on your environment. Secure user access to Annotation Lab Access to Annotation Lab is restricted to users who are given access by an admin or project manager. Each user has an account; when created, passwords are enforced to best practice security policy. Annotation Lab keeps track of who has access to the defined projects and their actions regarding completions creation, cloning, submission, and starring. See User Management Page for more details. API access to Annotation Lab Access to Annotation Lab REST API requires an access token that is specific to a user account. To obtain your access token please follow the steps illustrated here. Complete project audit trail Annotation Lab keeps trail for all created completions. It is not possible for annotators or reviewers to delete any completions and only managers and project owners are able to remove tasks. Application development cycle The Annotation Lab development cycle currently includes static code analysis; everything is assembled as docker images whom are being scanned for vulnerabilities before being published. We are currently implementing web vulnerability scanning.",
    "url": "/docs/en/alab/security",
    "relUrl": "/docs/en/alab/security"
  },
  "265": {
    "id": "265",
    "title": "Serving Spark NLP&#58 MLFlow on Databricks",
    "content": "This is the first article of the “Serving Spark NLP via API” series, showcasing how to serve Spark NLP using Databricks Jobs and MLFlow Serve APIs. You can find two more approaches (first, using FastAPI and second, using SynapseML) in the Spark NLP for Healthcare documentation page. Background Spark NLP is a Natural Language Understanding Library built on top of Apache Spark, leveranging Spark MLLib pipelines, that allows you to run NLP models at scale, including SOTA Transformers. Therefore, it’s the only production-ready NLP platform that allows you to go from a simple PoC on 1 driver node, to scale to multiple nodes in a cluster, to process big amounts of data, in a matter of minutes. Before starting, if you want to know more about all the advantages of using Spark NLP (as the ability to work at scale on air-gapped environments, for instance) we recommend you to take a look at the following resources: John Snow Labs webpage; The official technical documentation of Spark NLP; Spark NLP channel on Medium; Motivation Spark NLP is server-agnostic, what means it does not come with an integrated API server, but offers a lot of options to serve NLP models using Rest APIs. There is a wide range of possibilities to add a web server and serve Spark NLP pipelines using RestAPI, and in this series of articles we are only describing some of them. Let’s have an overview of how to use Databricks Jobs API and MLFlow Serve as an example for that purpose. Databricks Jobs and MLFlow Serve APIs About Databricks Databricks is an enterprise software company founded by the creators of Apache Spark. The company has also created MLflow, the Serialization and Experiment tracking library you can use (inside or outside databricks), as described in the section “Experiment Tracking”. Databricks develops a web-based platform for working with Spark, that provides automated cluster management and IPython-style notebooks. Their infrastructured is provided for training and production purposes, and is integrated in cloud platforms as Azure and AWS. Spark NLP is a proud partner of Databricks and we offer a seamless integration with them — see Install on Databricks. All Spark NLP capabilities run in Databricks, including MLFlow serialization and Experiment tracking, what can be used for serving Spark NLP for production purposes. About MLFlow MLFlow is a serialization and Experiment Tracking platform, which also natively suports Spark NLP. We have a documentation entry about MLFlow in the “Experiment Tracking” section. It’s highly recommended that you take a look before moving forward in this document, since we will use some of the concepts explained there. We will use MLFlow serialization to serve our Spark NLP models. Strengths Easily configurable and scalable clusters in Databricks Seamless integration of Spark NLP and Databricks for automatically creating Spark NLP clusters (check Install on Databricks URL) Integration with MLFlow, experiment tracking, etc. Configure your training and serving environments separately. Use your serving environment for inference and scale it as you need. Weaknesses This approach does not allow you to customize your endpoints, it uses Databricks JOBS API ones Requires some time and expertise in Databricks to configure everything properly Creating a cluster in Databricks As mentioned before, Spark NLP offers a seamless integration with Databricks. To create a cluster, please follow the instructions in Install on Databricks. That cluster can be then replicated (cloned) for production purposes later on. Configuring Databricks for serving Spark NLP on MLFlow In Databricks Runtime Version, select any Standard runtime, not ML ones… These add their version of MLFlow, and some incompatibilities may arise. For this example, we have used 8.3 (includes Apache Spark 3.1.1, Scala 2.12) The cluster instantiated is prepared to use Spark NLP, but to make it production-ready using MLFlow, we need to add the MLFlow jar, in addition to the Spark NLP jar, as shown in the “Experiment Tracking” section. In that case, we did it adding both jars… (&quot;spark.jars.packages&quot;:&quot; com.johnsnowlabs.nlp:spark-nlp_2.12:[YOUR_SPARKNLP_VERSION],org.mlflow:mlflow-spark:1.21.0&quot;) …into the SparkSession. However, in Databricks, you don’t instantiate programmatically a session, but you configure it in the Compute screen, selecting your Spark NLP cluster, and then going to Configuration -&gt; Advanced Options -&gt; Spark -&gt; Spark Config, as shown in the following image: In addition to Spark Config, we need to add the Spark NLP and MLFlow libraries to the Cluster. You can do that by going to Libraries inside your cluster. Make sure you have spark-nlp and mlflow. If not, you can install them either using PyPI or Maven artifacts. In the image below you can see the PyPI alternative: TIP: You can also use the Libraries section to add the jars (using Maven Coordinates) instead of setting them in the Spark Config, as showed before. Creating a notebook You are ready to create a notebook in Databricks and attach it to the recently created cluster. To do that, go to Create --&gt; Notebook, and select the cluster you want in the dropdown above your notebook. Make sure you have selected the cluster with the right Spark NLP + MLFlow configuration. To check everything is ok, run the following lines: To check the session is running: spark To check jars are in the session: spark.sparkContext.getConf().get(&#39;spark.jars.packages&#39;) You should see the following output from the last line (versions may differ depending on which ones you used to configure your cluster) Out[2]: &#39;com.johnsnowlabs.nlp:spark-nlp_2.12:[YOUR_SPARKNLP_VERSION],org.mlflow:mlflow-spark:1.21.0&#39; Logging the experiment in Databricks using MLFlow As explained in the “Experiment Tracking” section, MLFlow can log Spark MLLib / NLP Pipelines as experiments, to carry out runs on them, track versions, etc. MLFlow is natively integrated in Databricks, so we can leverage the mlflow.spark.log_model() function of the Spark flavour of MLFlow, to start tracking our Spark NLP pipelines. Let’s first import our libraries: import mlflow import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import pandas as pd from sparknlp.training import CoNLL import pyspark from pyspark.sql import SparkSession Then, create a Lemmatization pipeline: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = LemmatizerModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;prediction&quot;) # It&#39;s mandatory to call it prediction pipeline = Pipeline(stages=[ documentAssembler, tokenizer, lemmatizer ]) p_model = pipeline.fit( spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) ) IMPORTANT: Last output column of the last component in the pipeline should be called prediction. Finally, let’s log the experiment. In the Experiment Tracking section, we used the pip_requirements parameter in the log_model() function to set the required libraries: But we mentioned using conda is also available. Let’s use conda in this example: conda_env = { &#39;channels&#39;: [&#39;conda-forge&#39;], &#39;dependencies&#39;: [ &#39;python=3.8.8&#39;, { &quot;pip&quot;: [ &#39;pyspark==3.1.1&#39;, &#39;mlflow==1.21.0&#39;, &#39;spark-nlp==[YOUR_SPARKNLP_VERSION]&#39; ] } ], &#39;name&#39;: &#39;mlflow-env&#39; } With this conda environment, we are ready to log our pipeline: mlflow.spark.log_model(p_model, &quot;lemmatizer&quot;, conda_env=conda_env) You should see an output similar to this one: (6) Spark Jobs (1) MLflow run *Logged 1 run to an experiment in MLflow. Learn more* Experiment UI On the top right corner of your notebook, you will see the Experiment widget, and inside, as shown in the image below. You can also access Experiments UI if you switch your environment from “Data Science &amp; Engineering” to “Machine Learning”, on the left panel… Once in the experiment UI, you will see the following screen, where your experiments are tracked. If you click on the Start Time cell of your experiment, you will reach the registered MLFlow run. On the left panel you will see the MLFlow model and some other artifacts, as the conda.yml and pip_requirements.txt that manage the dependencies of your models. On the right panel, you will see two snippets, about how to call to the model for inference internally from Databricks. Snippet for calling with a Pandas Dataframe: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; Load model as a Spark UDF. loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model) Predict on a Spark DataFrame. columns = list(df.columns) df.withColumn(&#39;predictions&#39;, loaded_model(*columns)).collect() Snippet for calling with a Spark Dataframe. We won’t include it in this documentation because that snippet does not include SPark NLP specificities. To make it work, the correct snippet should be: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model) ### Predict on a Spark DataFrame. res_spark = loaded_model.predict(df_1_spark.rdd) IMPORTANT: You will only get the last column (prediction) results, which is a list of Rows of Annotation Types. To convert the result list into a Spark Dataframe, use the following schema: import pyspark.sql.types as T import pyspark.sql.functions as f annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) And then, get the results (for example, in res_spark) and apply the schema: spark_res = spark.createDataFrame(res_pandas[0], schema=annotationType) Calling the experiment for production purposes using MLFlow Rest API Instead of choosing a Batch Inference, you can select REST API. This will lead you to another screen, when the model will be loaded for production purposes in an independent cluster. Once deployed, you will be able to: Check the endpoint URL to consume the model externally; Test the endpoint writing a json (in our example, ‘text’ is our first input col of the pipeline, so it shoud look similar to: {&quot;text&quot;: &quot;This is a test of how the lemmatizer works&quot;} You can see the response in the same screen. Check what is the Python code or cURL command to do that very same thing programatically. By just using that Python code, you can already consume it for production purposes from any external web app. IMPORTANT: As per 17/02/2022, there is an issue being studied by Databricks team, regarding the creation on the fly of job clusters to serve MLFlow models that require configuring the Spark Session with specific jars. This will be fixed in later versions of Databricks. In the meantime, the way to go is using Databricks Jobs API. Calling the experiment for production purposes using Databricks Asynchronous Jobs API Creating the notebook for the inference job And last, but not least, another approach to consume models for production purposes. the Jobs API. Databricks has its own API for managing jobs, that allows you to instantiate any notebook or script as a job, run it, stop it, and manage all the life cycle. And you can configure the cluster where this job will run before hand, what prevents having the issue described in point 3. To do that: Create a new production cluster, as described before, cloning you training environment but adapting it to your needs for production purposes. Make sure the Spark Config is right, as described at the beginning of this documentation. Create a new notebook. Always check that the jars are in the session: spark.sparkContext.getConf().get(&#39;spark.jars.packages&#39;) Out[2]: &#39;com.johnsnowlabs.nlp:spark-nlp_2.12:[YOUR_SPARKNLP_VERSION],org.mlflow:mlflow-spark:1.21.0&#39; Add the Spark NLP imports. import mlflow import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import pandas as pd from sparknlp.training import CoNLL import pyspark from pyspark.sql import SparkSession import pyspark.sql.types as T import pyspark.sql.functions as f import json Let’s define that an input param called text will be sent in the request. Let’s get the text from that parameter using dbutils. input = &quot;&quot; try: input = dbutils.widgets.get(&quot;text&quot;) print(&#39;&quot;text&quot; input found: &#39; + input) except: print(&#39;Unable to run: dbutils.widgets.get(&quot;text&quot;). Setting it to NOT_SET&#39;) input = &quot;NOT_SET&quot; Right now, the input text will be in input var. You can trigger an exception or set the input to some default value if the parameter does not come in the request. Let’s create a Spark Dataframe with the input df = spark.createDataFrame([[input]]).toDF(&#39;text&#39;) And now, we just need to use the snippet for Spark Dataframe to consume MLFlow models, described above: import mlflow import pyspark.sql.types as T import pyspark.sql.functions as f logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model) Predict on a Spark DataFrame. res_spark = loaded_model.predict(df_1_spark.rdd) annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) spark_res = spark.createDataFrame(res_spark[0], schema=annotationType) Let’s transform our lemmatized tokens from the Dataframe into a list of strings: lemmas = spark_res.select(&quot;result&quot;).collect() txt_results = [x[&#39;result&#39;] for x in lemmas] And finally, let’s use again dbutils to tell Databricks to spin off the run and return an exit parameter: the list of token strings. dbutils.notebook.exit(json.dumps({ &quot;status&quot;: &quot;OK&quot;, &quot;results&quot;: txt_results })) Configuring the job Last, but not least. We need to precreate the job, so that we run it from the API. We could do that using the API as well, but we will show you how to do it using the UI. On the left panel, go to Jobs and then Create Job. In the jobs screen, you will see you job created. It’s not running, it’s prepared to be called on demand, programatically or in the interface, with a text input param. Let’s see how to do that: Running the job In the jobs screen, if you click on the job, you will enter the Job screen, and be able to set your text input parameter and run the job manually. You can use this for testing purposes, but the interesting part is calling it externally, using the Databricks Jobs API. Using the Databricks Jobs API, from for example, Postman. POST HTTP request URL: https://[your_databricks_instance]/api/2.1/jobs/run-now Authorization: [use Bearer Token. You can get it from Databricks, Settings, User Settings, Generate New Token.] Body: { &quot;job_id&quot;: [job_id, check it in the Jobs screen], &quot;notebook_params&quot;: {&quot;text&quot;: &quot;This is an example of how well the lemmatizer works&quot;} } As it’s an asynchronous call, it will return the number a number of run, but no results. You will need to query for results using the number of the run and the following url https://[your_databricks_instance]/2.1/jobs/runs/get-output You will get a big json, but the most relevant info, the output, will be up to the end: Results (list of lemmatized words) {&quot;notebook_output&quot;: { &quot;status&quot;: &quot;OK&quot;, &quot;results&quot;: [&quot;This&quot;, &quot;is&quot;, &quot;a&quot;, &quot;example&quot;, &quot;of&quot;, &quot;how&quot;, &quot;lemmatizer&quot;, &quot;work&quot;] }} The notebook will be prepared in the job, but idle, until you call it programatically, what will instantiate a run. Check the Jobs API for more information about what you can do with it and how to adapt it to your solutions for production purposes. Do you want to know more? Check how to productionize Spark NLP in our official documentation here Visit John Snow Labs and Spark NLP Technical Documentation websites Follow us on Medium: Spark NLP and Veysel Kocaman Write to support@johnsnowlabs.com for any additional request you may have",
    "url": "/docs/en/serving_spark_nlp_via_api_databricks_mlflow",
    "relUrl": "/docs/en/serving_spark_nlp_via_api_databricks_mlflow"
  },
  "266": {
    "id": "266",
    "title": "Social Determinant - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/social_determinant",
    "relUrl": "/social_determinant"
  },
  "267": {
    "id": "267",
    "title": "Spark NLP",
    "content": "Requirements &amp; Setup Spark NLP is built on top of Apache Spark 3.x. For using Spark NLP you need: Java 8 and 11 Apache Spark 3.3.x, 3.2.x, 3.1.x, 3.0.x GPU (optional): Spark NLP 4.3.1 is built with TensorFlow 2.7.1 and the following NVIDIA® software are only required for GPU support: NVIDIA® GPU drivers version 450.80.02 or higher CUDA® Toolkit 11.2 cuDNN SDK 8.1.0 It is recommended to have basic knowledge of the framework and a working environment before using Spark NLP. Please refer to Spark documentation to get started with Spark. Install Spark NLP in Python Scala and Java Databricks EMR Join our Slack channel Join our channel, to ask for help and share your feedback. Developers and users can help each other getting started here. Spark NLP Slack Spark NLP in Action Make sure to check out our demos built by Streamlit to showcase Spark NLP in action: Spark NLP Demo Spark NLP Examples If you prefer learning by example, check this repository: Spark NLP Examples It is full of fresh examples and even a docker container if you want to skip installation. Below, you can follow into a more theoretical and thorough quick start guide. Where to go next If you need more detailed information about how to install Spark NLP you can check the Installation page Detailed information about Spark NLP concepts, annotators and more may be found HERE",
    "url": "/docs/en/spark-nlp",
    "relUrl": "/docs/en/spark-nlp"
  },
  "268": {
    "id": "268",
    "title": "Summarize & Paraphrase - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/summarize_paraphrase",
    "relUrl": "/summarize_paraphrase"
  },
  "269": {
    "id": "269",
    "title": "Enterprise Spark NLP",
    "content": "{% include programmingLanguageSelectScalaPythonNLU.html %} {% include programmingLanguageSelectPythons.html %} ... pos = PerceptronModel.pretrained(&quot;pos_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;token&quot;,&quot;sentence&quot;]) .setOutputCol(&quot;pos&quot;) pos_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, pos]) light_pipeline = LightPipeline(pos_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;))) result = light_pipeline.fullAnnotate(&quot;&quot;&quot;He was given boluses of MS04 with some effect, he has since been placed on a PCA - he take 80mg of oxycontin at home, his PCA dose is ~ 2 the morphine dose of the oxycontin, he has also received ativan for anxiety.&quot;&quot;&quot;) ... pos = PerceptronModel.pretrained(&quot;pos_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;token&quot;,&quot;sentence&quot;]) .setOutputCol(&quot;pos&quot;) pos_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, pos]) light_pipeline = LightPipeline(pos_pipeline.fit(spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;))) result = light_pipeline.fullAnnotate(&quot;&quot;&quot;He was given boluses of MS04 with some effect, he has since been placed on a PCA - he take 80mg of oxycontin at home, his PCA dose is ~ 2 the morphine dose of the oxycontin, he has also received ativan for anxiety.&quot;&quot;&quot;) val pos = PerceptronModel.pretrained(&quot;pos_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols(&quot;token&quot;,&quot;sentence&quot;) .setOutputCol(&quot;pos&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, sentence_detector, tokenizer, pos)) val data = Seq(&quot;He was given boluses of MS04 with some effect, he has since been placed on a PCA - he take 80mg of oxycontin at home, his PCA dose is ~ 2 the morphine dose of the oxycontin, he has also received ativan for anxiety.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) import nlu nlu.load(&quot;en.pos.clinical&quot;).predict(&quot;&quot;&quot;He was given boluses of MS04 with some effect, he has since been placed on a PCA - he take 80mg of oxycontin at home, his PCA dose is ~ 2 the morphine dose of the oxycontin, he has also received ativan for anxiety.&quot;&quot;&quot;)",
    "url": "/docs/en/tab_example",
    "relUrl": "/docs/en/tab_example"
  },
  "270": {
    "id": "270",
    "title": "Tasks",
    "content": "The Tasks screen shows a list of all documents that have been imported into the current project. Under each task you can see meta data about the task: the time of import, the user who imported the task and the annotators and reviewers assigned to the task. Task Assignment Project Owners/Managers can assign tasks to annotator(s) and reviewer(s) in order to better plan/distribute project work. Annotators and Reviewers can only view tasks that are assigned to them which means there is no chance of accidental work overlap. For assigning a task to an annotator, from the task page select one or more tasks and from the Assign dropdown choose an annotator. You can only assign a task to annotators that have already been added to the project team. For adding an annotator to the project team, select your project and navigate to the Setup &gt; Team menu item. On the Add Team Member page, search for the user you want to add, select the role you want to assign to him/her and click on Add To Team button. Project Owners can also be explicitly assigned as annotators and/or reviewers for tasks. It is useful when working in a small team and when the Project Owners are also involved in the annotation process. A new option Only Assigned checkbox is now available on the labeling page that allows Project Owners to filter the tasks explicitly assigned to them when clicking the Next button. NOTE: When upgrading from an older version of the Annotation Lab, the annotators will no longer have access to the tasks they worked on unless they are assigned to those explicitely by the admin user who created the project. Once they are assigned, they can resume work and no information is lost. Task Status At high level, each task can have one of the following statuses: Incomplete, when none of the assigned annotators has started working on the task. In Progress, when at least one of the assigned annotators has submitted at least one completion for this task. Submitted, when all annotators which were assigned to the task have submitted a completion which is set as ground truth (starred). Reviewed, in the case there is a reviewer assigned to the task, and the reviewer has reviewed and accepted the submited completion. To Correct, in the case the assigned reviewer has rejected the completion created by the Annotator. The status of a task varies according to the type of account the logged in user has (his/her visibility over the project) and according to the tasks that have been assigned to him/her. For Project Owner, Manager and Reviewer On the Analytics page and Tasks page, the Project Owner/Manager/Reviewer will see the general overview of the projects which will take into consideration the task level statuses as follows: Incomplete - Assigned annotators have not started working on this task In Progress - At least one annotator still has not starred (marked as ground truth) one submitted completion Submitted - All annotators that are assigned to the task have starred (marked as ground truth) one submitted completion Reviewed - Reviewer has approved all starred submitted completions for the task For Annotators On the Annotator’s Task page, the task status will be shown with regards to the context of the logged-in Annotator’s work. As such, if the same task is assigned to two annotators then: if annotator1 is still working and not submitted the task, then he/she will see task status as In-progress if annotator2 submits the task from his/her side then he/she will see task status as Submitted The following statuses are available on the Annotator’s view. Incomplete – Current logged-in annotator has not started working on this task. In Progress - At least one saved/submitted completions exist, but there is no starred submitted completion. Submitted - Annotator has at least one starred submitted completion. Reviewed - Reviewer has approved the starred submitted completion for the task. To Correct - Reviewer has rejected the submitted work. In this case, the star is removed from the reviewed completion. The annotator should start working on the task and resubmit. Note: The status of a task is maintained/available only for the annotators assigned to the task. When multiple Annotators are assigned to a task, the reviewer will see the task as submitted when all annotators submit and star their completions. Otherwise, if one of the assigned Annotators has not submitted or has not starred one completion, then the Reviewer will see the task as In Progress. Task Filters As normally annotation projects involve a large number of tasks, the Task page includes filtering and sorting options which will help the user identify the tasks he/she needs faster. Tasks can be sorted by time of import ascending or descending. Tasks can be filtered by the assigned tags, by the user who imported the task and by the status. There is also a search functionality which will identify the tasks having a given string on their name. The number of tasks visible on the screeen is customizable by selecting the predefined values from the Tasks per page drop-down. Task Search by Text, Label and Choice Annotation Lab offers advanced search features that help users identify the tasks they need based on the text or based on the annotations defined so far. Currently supported search queries are: text: patient -&gt; returns all tasks which contain the string “patient”; label: ABC -&gt; returns all tasks that have at least one completion containing a chunk with label ABC; label: ABC=DEF -&gt; returns all tasks that have at least one completion containing the text DEF labeled as ABC; choice: Sport -&gt; returns all tasks that have at least one completion which classified the task as Sport; choice: Sport,Politics -&gt; returns all tasks that have at least one completion containing multiple choices Sport and Politics. Search functionality is case insensitive, thus the following queries label: ABC=DEF , label: Abc=Def or label: abc=def are considered equivalent. Example: Consider a project with 3 tasks which are annotated as below: Search-query “label:LOC” will list as results Task 1 and Task 3. Search-query “label:WORK_OF_ART” will list as result Task 1 and Task 2. Search-query “label:PERSON=Leonardo” will list as result Task 1. Comments Comments can be added to each task by any team member. This is done by clicking the View comments link present on the rightmost side of each Task in the Tasks List page. It is important to notice that these comments are visible to everyone who can view the particular task.",
    "url": "/docs/en/alab/tasks",
    "relUrl": "/docs/en/alab/tasks"
  },
  "271": {
    "id": "271",
    "title": "Terminology",
    "content": "Concept Definition Project A project in Annotation Lab resembles a set of tasks that need to be annotated and/or reviewed by users in order to extract structured data and/or to train a DL model.Think of it as a factory assembly line for producing labels. For jumpstarting annotations on a project preannotations generated by existing models and/or rules can be used. Project configuration Specifies the type of documents that will be annotated as well as the labels, classes and relations which will be used for annotation. A project configuration can also reuse existing labels, classes and relations from pre-trained models or rules. Model In the context of the Annotation Lab use, the term model refers to a DL model build using John Snow Labs NLP libraries. Predictions Annotations automatically generated by Spark NLP models or user defined Spark NLP rules. Completions A series of annotations manually created or copied from automatic predictions and edited/validated by human annotators. Task A document that needs to be annotated by an annotator with or without the use of preannotation.",
    "url": "/docs/en/alab/terminology",
    "relUrl": "/docs/en/alab/terminology"
  },
  "272": {
    "id": "272",
    "title": "Test Project Configuration",
    "content": "Annotation Lab offer testing features for projects that reuse existing models/rules. In other words, if a project’s configuration references one or several (pre)trained models/rules it is possible to check how efficient those are when applied on custom data. The Test Configuration feature is available on the Train page, accessible from the Project Menu. During the training, a progress bar is shown on the top of the train page to show the status of the testing. Note This feature is available for Project Owners or Managers. The Test Configuration feature applies to project tasks with status submitted or reviewed, and which are tagged as Test. After the evaluaton is completed, the resulting logs can be downloaded to view the performance metrics. Note: Model evaluation can only be triggered in the presence of a valid Healthcare, Finance or/and Legal NLP license.",
    "url": "/docs/en/alab/test_project_configuration",
    "relUrl": "/docs/en/alab/test_project_configuration"
  },
  "273": {
    "id": "273",
    "title": "Text Summarization - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/text_summarization",
    "relUrl": "/text_summarization"
  },
  "274": {
    "id": "274",
    "title": "Third Party Projects",
    "content": "There are third party projects that can integrate with Spark NLP. These packages need to be installed separately to be used. If you’d like to integrate your application with Spark NLP, please send us a message! Logging Comet Comet is a meta machine learning platform designed to help AI practitioners and teams build reliable machine learning models for real-world applications by streamlining the machine learning model lifecycle. By leveraging Comet, users can track, compare, explain and reproduce their machine learning experiments. Comet can easily integrated into the Spark NLP workflow with the a dedicated logging class CometLogger to log training and evaluation metrics, pipeline parameters and NER visualization made with sparknlp-display. For more information see the User Guide and for more examples see the Spark NLP Examples. Python API: CometLogger Show Example # Metrics while training an annotator can be logged with for example: import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.logging.comet import CometLogger spark = sparknlp.start() OUTPUT_LOG_PATH = &quot;./run&quot; logger = CometLogger() document = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) embds = ( UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) ) multiClassifier = ( MultiClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;labels&quot;) .setBatchSize(128) .setLr(1e-3) .setThreshold(0.5) .setShufflePerEpoch(False) .setEnableOutputLogs(True) .setOutputLogsPath(OUTPUT_LOG_PATH) .setMaxEpochs(1) ) logger.monitor(logdir=OUTPUT_LOG_PATH, model=multiClassifier) trainDataset = spark.createDataFrame( [(&quot;Nice.&quot;, [&quot;positive&quot;]), (&quot;That&#39;s bad.&quot;, [&quot;negative&quot;])], schema=[&quot;text&quot;, &quot;labels&quot;], ) pipeline = Pipeline(stages=[document, embds, multiClassifier]) pipeline.fit(trainDataset) logger.end() # If you are using a jupyter notebook, it is possible to display the live web # interface with logger.experiment.display(tab=&#39;charts&#39;) MLflow Spark NLP uses Spark MLlib Pipelines, what are natively supported by MLFlow. MLFlow is, as stated in their official webpage, an open source platform for the machine learning lifecycle, that includes: Mlflow Tracking: Record and query experiments: code, data, config, and results MLflow Projects: Package data science code in a format to reproduce runs on any platform MLflow Models: Deploy machine learning models in diverse serving environments Model Registry: Store, annotate, discover, and manage models in a central repository For more information, please see the complete guide at Experiment Tracking.",
    "url": "/docs/en/third-party-projects",
    "relUrl": "/docs/en/third-party-projects"
  },
  "275": {
    "id": "275",
    "title": "Annotation Settings",
    "content": "Optimize view for large taxonomy For projects that include a large number of labels, we have created a way to optimize the taxonomy display so that users can quickly find the label they are searching for. To obtain the above display please use the following configuration: &lt;View&gt; &lt;Filter name=&quot;fl&quot; toName=&quot;label&quot; hotkey=&quot;shift+f&quot; minlength=&quot;1&quot; /&gt; &lt;View style=&quot; background:white; height: 100px; overflow-y:scroll; resize:vertical; position:sticky; top:0;&quot; &gt; &lt;Labels name=&quot;label&quot; toName=&quot;text&quot;&gt; &lt;Label value=&quot;Person&quot; background=&quot;red&quot;&gt;&lt;/Label&gt; &lt;Label value=&quot;Organization&quot; background=&quot;darkorange&quot;&gt;&lt;/Label&gt; &lt;/Labels&gt; &lt;/View&gt; &lt;View style=&quot; resize:vertical; margin-top:10px; max-height:400px; overflow-y:scroll;&quot; &gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;&gt;&lt;/Text&gt; &lt;/View&gt; &lt;/View&gt;",
    "url": "/docs/en/alab/tips",
    "relUrl": "/docs/en/alab/tips"
  },
  "276": {
    "id": "276",
    "title": "Training",
    "content": "Training Datasets These are classes to load common datasets to train annotators for tasks such as part-of-speech tagging, named entity recognition, spell checking and more. {% include_relative training_entries/pos.md %} {% include_relative training_entries/conll.md %} {% include_relative training_entries/conllu.md %} {% include_relative training_entries/pubtator.md %} Spell Checkers Dataset (Corpus) In order to train a Norvig or Symmetric Spell Checkers, we need to get corpus data as a spark dataframe. We can read a plain text file and transforms it to a spark dataset. Example: {% include programmingLanguageSelectScalaPython.html %} train_corpus = spark.read .text(&quot;./sherlockholmes.txt&quot;) .withColumnRenamed(&quot;value&quot;, &quot;text&quot;) val trainCorpus = spark.read .text(&quot;./sherlockholmes.txt&quot;) .select(trainCorpus.col(&quot;value&quot;).as(&quot;text&quot;)) Text Processing These are annotators that can be trained to process text for tasks such as dependency parsing, lemmatisation, part-of-speech tagging, sentence detection and word segmentation. {% include_relative training_entries/DependencyParserApproach.md %} {% include_relative training_entries/Lemmatizer.md %} {% include_relative training_entries/PerceptronApproach.md %} {% include_relative training_entries/SentenceDetectorDLApproach.md %} {% include_relative training_entries/TypedDependencyParser.md %} {% include_relative training_entries/WordSegmenterApproach.md %} Spell Checkers These are annotators that can be trained to correct text. {% include_relative training_entries/ContextSpellCheckerApproach.md %} {% include_relative training_entries/NorvigSweeting.md %} {% include_relative training_entries/SymmetricDelete.md %} Token Classification These are annotators that can be trained to recognize named entities in text. {% include_relative training_entries/NerCrfApproach.md %} {% include_relative training_entries/NerDLApproach.md %} Text Classification These are annotators that can be trained to classify text into different classes, such as sentiment. {% include_relative training_entries/ClassifierDLApproach.md %} {% include_relative training_entries/MultiClassifierDLApproach.md %} {% include_relative training_entries/SentimentDLApproach.md %} {% include_relative training_entries/ViveknSentimentApproach.md %} Text Representation These are annotators that can be trained to turn text into a numerical representation. {% include_relative training_entries/Doc2VecApproach.md %} {% include_relative training_entries/Word2VecApproach.md %} External Trainable Models These are annotators that are trained in an external library, which are then loaded into Spark NLP. {% include_relative training_entries/AlbertForTokenClassification.md %} {% include_relative training_entries/BertForSequenceClassification.md %} {% include_relative training_entries/BertForTokenClassification.md %} {% include_relative training_entries/DistilBertForSequenceClassification.md %} {% include_relative training_entries/DistilBertForTokenClassification.md %} {% include_relative training_entries/RoBertaForTokenClassification.md %} {% include_relative training_entries/XlmRoBertaForTokenClassification.md %} TensorFlow Graphs NER DL uses Char CNNs - BiLSTM - CRF Neural Network architecture. Spark NLP defines this architecture through a Tensorflow graph, which requires the following parameters: Tags Embeddings Dimension Number of Chars Spark NLP infers these values from the training dataset used in NerDLApproach annotator and tries to load the graph embedded on spark-nlp package. Currently, Spark NLP has graphs for the most common combination of tags, embeddings, and number of chars values: Tags Embeddings Dimension 10 100 10 200 10 300 10 768 10 1024 25 300 All of these graphs use an LSTM of size 128 and number of chars 100 In case, your train dataset has a different number of tags, embeddings dimension, number of chars and LSTM size combinations shown in the table above, NerDLApproach will raise an IllegalArgumentException exception during runtime with the message below: Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Check https://nlp.johnsnowlabs.com/docs/en/graph for instructions to generate the required graph. To overcome this exception message we have to follow these steps: Clone spark-nlp github repo Run python file create_models with number of tags, embeddings dimension and number of char values mentioned on your exception message error. cd spark-nlp/python/tensorflow export PYTHONPATH=lib/ner python create_models.py [number_of_tags] [embeddings_dimension] [number_of_chars] [output_path] This will generate a graph on the directory defined on `output_path argument. Retry training with NerDLApproach annotator but this time use the parameter setGraphFolder with the path of your graph. Note: Make sure that you have Python 3 and Tensorflow 1.15.0 installed on your system since create_models requires those versions to generate the graph successfully. Note: We also have a notebook in the same directory if you prefer Jupyter notebook to cerate your custom graph.",
    "url": "/docs/en/training",
    "relUrl": "/docs/en/training"
  },
  "277": {
    "id": "277",
    "title": "Train New Model",
    "content": "A Project Owner or a Manager can use the completed tasks (completions) from a project to train a new Spark NLP model. The training feature can be found on the train page, accessible from the Project Menu. The training process can be triggered via a three step wizard that guides users and offers useful hints. Users can also opt for a synthesis view for initiating the training of a model. During the training, a progress bar is shown to give users basic information on the status of the training process. Deploy a new training job Users can perform multiple training jobs at the same time, depending on the available resources/license(s). Users can opt to create new training jobs independently from already running training/pre-annotation/OCR jobs. If resources/licenses are available when pressing the Train Model button a new training server is launched. The running servers can be seen by visiting the Clusters page. Named Entity Recognition For training a good Named Entity Recognition (NER) model, a relevant number of annotations must exist for all labels included in the project configuration. The recommendation is to have minimum 40-50 examples for each entity. Once this requirement is met, for training a new model users need to navigate to the Train page for the current project and follow some very simple steps: Select the type of model to train - Open source/Healthcare/Finance/Legal - and the embeddings to use; Define the training parameters and the train/test data split; Optionally turn on the Active Learning feature; Click the Train Model button. When triggering the training, users are prompted to choose either to immediately deploy models or just do training. If immediate deployment is chosen, then the Labeling config is updated according to the name of the new model. Notice how the name of the original model used for preannotations is replaced with the name of the new model in the configuration below. Information on the overall training progress is shown in the page. User can get indications on the success or failure of the training as well as check the live training logs (by pressing the Show Logs button). Once the training is finished, it is possible to download the training logs by clicking on the download logs icon of the recently trained NER model which includes information like training parameters and TF graph used along with precision, recall, f1 score, etc. This information is also accessible by clicking on the benchmarking icon available on the models on the Models page. Starting from version 4.3.0, it is possible to keep track of all previous training activities executed for a project. When pressing the History button from the Train page, users are presented with a list of all trainings triggered for the current project. Each training event is characterized by the source (manual, active learning), data used for training, date of event, and status. Training logs can also be downloaded for each training event. Training parameters In Annotation Lab, for mixed projects containing multiple types of annotations in a single project like classifications, NER, and assertion status, if multiple trainings were triggered at the same time using the same system resources and Spark NLP resources, the training component could fail because of resource limitations. In order to improve the usability of the system, dropdown options can be used to choose which type of training to run next. The project Owner or Manager of a project can scroll down to Training Settings and choose the training type. The drop-down gives a list of possible training types for that particular project based on its actual configuration. A second drop-down lists available embeddings which can be used for training the model. It is possible to tune the most common training parameters (Number of Epochs, Learning rate, Decay, Dropout, and Batch) by editing their values in Training Parameters. Test/Train data for a model can be randomly selected based on the Validation Split value or can be set using Test/Train tags. The later option is very useful when conducting experiments that require testing and training data to be the same on each run. It is also possible to train a model by using a sublist of tasks with predefined tags. This is done by specifying the targeted Tags on the Training Parameters (last option). Annotation Lab also includes additional filtering options for the training dataset based on the status of completions, either all submitted completions can be used for training or only the reviewed ones. Custom Training Script If users want to change the default Training script present within the Annotation Lab, they can upload their own training pipeline. In the Train Page, project owners can upload the training scripts. At the moment we are supporting custom training script just for NER projects. Selection of Completions During the annotation project lifetime, normally not all tasks/completions are ready to be used as a training dataset. This is why the training process selects completions based on their status: Filter tasks by tags (if defined in Training Parameters widget, otherwise all tasks are considered) For completed tasks, completions to be taken into account are also selected based on the following criteria: If a task has a completion accepted by a reviewer this is selected for training and all others are ignored; Completions rejected by a Reviewer are not used for training; If no reviewer is assigned to a task that has multiple submitted completions the completion to use for training purpose is the one created by the user with the highest priority. Assertion Status NER configurations for the healthcare domain are often mixed with Assertion Status labels. In this case, Annotation Lab offers support for training both types of models in one go. After the training is complete, the models will be listed in the Pretrained Labels section of the Project Configuration. Information such as the source of the model and time of training will be displayed as well. Once the model(s) has been trained, the project configuration will be automatically updated to reference the new model for prediction. Notice below, for the Assertion Status Label tag the addition of model attribute to indicate which model will be used for task pre-annotation for this label. &lt;Label value=&quot;Absent&quot; assertion=&quot;true&quot; model=&quot;assertion_jsl_annotation_manual.model&quot;/&gt; &lt;Label value=&quot;Past&quot; assertion=&quot;true&quot; model=&quot;assertion_jsl_annotation_manual.model&quot;/&gt; It is not possible to mark a label as an Assertion Status label and use a NER model to predict it. A validation error is shown in the Interface Preview in case an invalid Assertion model is used. The Annotation Lab only allows the use of one single Assertion Status model in the same project. Classification Annotation Lab supports two types of classification training: Single Choice Classification and Multi-Choice Classification. For doing so, it uses three important attributes of the Choices tag to drive the Classification Models training and pre-annotation. Those are name, choice and train. Attribute name The attribute name allows the naming of the different choices present in the project configuration, and thus the training of separate models based on the same project annotations. For example, in the sample configuration illustrated below, the name=”age” attribute, tells the system to only consider age-related classification information when training an Age Classifier. The value specified by the name attribute is also used to name the resulting Classification model (classification_age_annotation_manual). Attribute choice The choice attribute specifies the type of model that will be trained: multiple or single. For example, in the Labeling Config below, Age and Gender are Single Choice Classification categories while the Smoking Status is Multi-Choice Classification. Depending upon the value of this attribute, the respective model will be trained as a Single Choice Classifier or Multi-Choice Classifier. &lt;View&gt; &lt;View style=&quot;overflow: auto;&quot;&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;/&gt; &lt;/View&gt; &lt;Header value=&quot;Smoking Status&quot;/&gt; &lt;Choices name=&quot;smokingstatus&quot; toName=&quot;text&quot; choice=&quot;multiple&quot; showInLine=&quot;true&quot;&gt; &lt;Choice value=&quot;Smoker&quot;/&gt; &lt;Choice value=&quot;Past Smoker&quot;/&gt; &lt;Choice value=&quot;Nonsmoker&quot;/&gt; &lt;/Choices&gt; &lt;Header value=&quot;Age&quot;/&gt; &lt;Choices name=&quot;age&quot; toName=&quot;text&quot; choice=&quot;single&quot; showInLine=&quot;true&quot;&gt; &lt;Choice value=&quot;Child (less than 18y)&quot; hotkey=&quot;c&quot;/&gt; &lt;Choice value=&quot;Adult (19-50y)&quot; hotkey=&quot;a&quot;/&gt; &lt;Choice value=&quot;Aged (50+y)&quot; hotkey=&quot;o&quot;/&gt; &lt;/Choices&gt; &lt;Header value=&quot;Gender&quot;/&gt; &lt;Choices name=&quot;gender&quot; toName=&quot;text&quot; choice=&quot;single&quot; showInLine=&quot;true&quot;&gt; &lt;Choice value=&quot;Female&quot; hotkey=&quot;f&quot;/&gt; &lt;Choice value=&quot;Male&quot; hotkey=&quot;m&quot;/&gt; &lt;/Choices&gt; &lt;/View&gt; Attribute train Annotation Lab restricts the training of two or more Classification Models at the same time. If there are multiple Classification categories in a project (like the one above), only the category whose name comes first in alphabetical order will be trained by default. In the above example, based on the value of the name attribute, we conclude that the Age classifier model is trained. The model to be trained can also be specified by setting the train=”true” attribute for the targeted Choices tag (like the one defined in Gender category below). &lt;View&gt; &lt;View style=&quot;overflow: auto;&quot;&gt; &lt;Text name=&quot;text&quot; value=&quot;$text&quot;/&gt; &lt;/View&gt; &lt;Header value=&quot;Smoking Status&quot;/&gt; &lt;Choices name=&quot;smokingstatus&quot; toName=&quot;text&quot; choice=&quot;multiple&quot; showInLine=&quot;true&quot;&gt; ... &lt;/Choices&gt; &lt;Header value=&quot;Age&quot;/&gt; &lt;Choices name=&quot;age&quot; toName=&quot;text&quot; choice=&quot;single&quot; showInLine=&quot;true&quot;&gt; ... &lt;/Choices&gt; &lt;Header value=&quot;Gender&quot;/&gt; &lt;Choices name=&quot;gender&quot; train=&quot;true&quot; toName=&quot;text&quot; choice=&quot;single&quot; showInLine=&quot;true&quot;&gt; ... &lt;/Choices&gt; &lt;/View&gt; The trained classification models are available to reuse in any project and can be added on step 3 of the Project Configuration wizard. The classification models trained using Annotation Lab also have attached benchmarking information. The training logs include the confusion matrix, helpful in understanding the performance of the model and in checking if the model is underfitting or overfitting. The confusion matrix is also available on the models tiles on the Models page, and is accessible by clicking on the benchmarking icon. Visual NER Training Annotation Lab offers the ability to train Visual NER models, apply active learning for automatic model training, and preannotate image-based tasks with existing models in order to accelerate annotation work. Model Training The training feature for Visual NER projects can be activated from the Setup page via the “Train Now” button (See 1). From the Training Settings sections, users can tune the training parameters (e.g. Epoch, Batch) and choose the tasks to use for training the Visual NER model (See 3). Information on the training progress is shown in the top right corner of the Model Training tab (See 2). Users can check detailed information regarding the success or failure of the last training. Training Failure can occur because of: Insufficient number of completions Poor quality of completions Insufficient CPU and Memory Wrong training parameters When triggering the training, users can choose to immediately deploy the model or just train it without deploying. If immediate deployment is chosen, then the labeling config is updated with references to the new model so that it will be used for preannotations. License Requirements Visual NER annotation, training and preannotation features are dependent on the presence of a Visual NLP license. Licenses with scope ocr: inference and ocr: training are required for preannotation and training respectively. Training Server Specification The minimal required training configuration is 64 GB RAM, 16 Core CPU for Visual NER Training. Mixed Projects If a project is set up to include Classification, Named Entity Recognition and Assertion Status labels and the three kinds of annotations are present in the training data, it is possible to train three models: one for Named Entity Recognition, one for Assertion Status, and one for Classification at the same time. The training logs from all three trainings can be downloaded at once by clicking the download button present in the Training section of the Setup Page. The newly trained models will be added to the Spark NLP pipeline config. Support for European Languagues Users can download English, German, Spanish, Portuguese, Italian, Danish and Romanian pretrained models from the NLP Models Hub and use them for pre-annotation. Annotation Lab also offers support for training/tuning models in the above languages.",
    "url": "/docs/en/alab/training_configurations",
    "relUrl": "/docs/en/alab/training_configurations"
  },
  "278": {
    "id": "278",
    "title": "Training Parameters",
    "content": "Annotation Lab supports the Transfer Learning feature offered by Spark NLP for Healthcare 3.1.2. This feature is available for project manages and project owners, but only if a valid Healthcare NLP license is loaded into the Annotation Lab. In this case, the feature can be activated for any project by navigating to the Train page. It requires the presence of a base model trained with MedicalNERModel. If a MedicalNER model is available on the Models Hub section of the Annotation Lab, it can be chosen as a starting point of the training process. This means the base model will be Fine Tuned with the new training data. When Fine Tuning is enabled, the same embeddings used for training the base model will be used to train the new model. Those need to be available on the Models Hub section as well. If present, embeddings will be automatically selected, otherwise users must go to the Models Hub page and download or upload them.",
    "url": "/docs/en/alab/transfer_learning",
    "relUrl": "/docs/en/alab/transfer_learning"
  },
  "279": {
    "id": "279",
    "title": "Transformers",
    "content": "{% assign parent_path = “en/transformer_entries” %} {% for file in site.static_files %} {% if file.path contains parent_path %} {% assign file_name = file.path | remove: parent_path | remove: “/” | prepend: “transformer_entries/” %} {% include_relative {{ file_name }} %} {% endif %} {% endfor %} Import Transformers into Spark NLP Overview We have extended support for HuggingFace 🤗 and TF Hub exported models since 3.1.0 to equivalent Spark NLP 🚀 annotators. Starting this release, you can easily use the saved_model feature in HuggingFace within a few lines of codes and import any BERT, DistilBERT, CamemBERT, RoBERTa, DeBERTa, XLM-RoBERTa, Longformer, BertForTokenClassification, DistilBertForTokenClassification, AlbertForTokenClassification, RoBertaForTokenClassification, DeBertaForTokenClassification, XlmRoBertaForTokenClassification, XlnetForTokenClassification, LongformerForTokenClassification, CamemBertForTokenClassification, CamemBertForSequenceClassification, CamemBertForQuestionAnswering, BertForSequenceClassification, DistilBertForSequenceClassification, AlbertForSequenceClassification, RoBertaForSequenceClassification, DeBertaForSequenceClassification, XlmRoBertaForSequenceClassification, XlnetForSequenceClassification, LongformerForSequenceClassification, AlbertForQuestionAnswering, BertForQuestionAnswering, DeBertaForQuestionAnswering, DistilBertForQuestionAnswering, LongformerForQuestionAnswering, RoBertaForQuestionAnswering, XlmRoBertaForQuestionAnswering, TapasForQuestionAnswering, Vision Transformers (ViT), HubertForCTC, and SwinForImageClassification models to Spark NLP. We will work on the remaining annotators and extend this support to the rest with each release 😊 Compatibility Spark NLP: The equivalent annotator in Spark NLP TF Hub: Models from TF Hub HuggingFace: Models from HuggingFace Model Architecture: Which architecture is compatible with that annotator Flags: Fully supported ✅ Partially supported (requires workarounds) ✔️ Under development ❎ Not supported ❌ Spark NLP TF Hub HuggingFace Model Architecture BertEmbeddings ✅ ✅ BERT - Small BERT - ELECTRA BertSentenceEmbeddings ✅ ✅ BERT - Small BERT - ELECTRA DistilBertEmbeddings   ✅ DistilBERT CamemBertEmbeddings   ✅ CamemBERT RoBertaEmbeddings   ✅ RoBERTa - DistilRoBERTa DeBertaEmbeddings   ✅ DeBERTa-v2 - DeBERTa-v3 XlmRoBertaEmbeddings   ✅ XLM-RoBERTa AlbertEmbeddings ✅ ✅ ALBERT XlnetEmbeddings   ✅ XLNet LongformerEmbeddings   ✅ Longformer ElmoEmbeddings ❎     UniversalSentenceEncoder ❎     BertForTokenClassification   ✅ TFBertForTokenClassification DistilBertForTokenClassification   ✅ TFDistilBertForTokenClassification AlbertForTokenClassification   ✅ TFAlbertForTokenClassification RoBertaForTokenClassification   ✅ TFRobertaForTokenClassification DeBertaForTokenClassification   ✅ TFDebertaV2ForTokenClassification XlmRoBertaForTokenClassification   ✅ TFXLMRobertaForTokenClassification XlnetForTokenClassification   ✅ TFXLNetForTokenClassificationet LongformerForTokenClassification   ✅ TFLongformerForTokenClassification CamemBertForTokenClassification   ✅ TFCamemBertForTokenClassification CamemBertForSequenceClassification   ✅ TFCamemBertForSequenceClassification CamemBertForQuestionAnswering   ✅ TFCamembertForQuestionAnswering BertForSequenceClassification   ✅ TFBertForSequenceClassification DistilBertForSequenceClassification   ✅ TFDistilBertForSequenceClassification AlbertForSequenceClassification   ✅ TFAlbertForSequenceClassification RoBertaForSequenceClassification   ✅ TFRobertaForSequenceClassification DeBertaForSequenceClassification   ✅ TFDebertaV2ForSequenceClassification XlmRoBertaForSequenceClassification   ✅ TFXLMRobertaForSequenceClassification XlnetForSequenceClassification   ✅ TFXLNetForSequenceClassification LongformerForSequenceClassification   ✅ TFLongformerForSequenceClassification AlbertForQuestionAnswering   ✅ TFAlbertForQuestionAnswering BertForQuestionAnswering   ✅ TFBertForQuestionAnswering DeBertaForQuestionAnswering   ✅ TFDebertaV2ForQuestionAnswering DistilBertForQuestionAnswering   ✅ TFDistilBertForQuestionAnswering LongformerForQuestionAnswering   ✅ TFLongformerForQuestionAnswering RoBertaForQuestionAnswering   ✅ TFRobertaForQuestionAnswering XlmRoBertaForQuestionAnswering   ✅ TFXLMRobertaForQuestionAnswering TapasForQuestionAnswering   ❎ TFTapasForQuestionAnswering ViTForImageClassification ❌ ✅ TFViTForImageClassification Automatic Speech Recognition (Wav2Vec2ForCTC)   ❎ TFWav2Vec2ForCTC SwinForImageClassification   ❎ TFSwinForImageClassification HubertForCTC   ❎ TFHubertForCTC T5Transformer   ❌   MarianTransformer   ❌   OpenAI GPT2   ❌   Example Notebooks HuggingFace to Spark NLP Spark NLP HuggingFace Notebooks Colab BertEmbeddings HuggingFace in Spark NLP - BERT BertSentenceEmbeddings HuggingFace in Spark NLP - BERT Sentence DistilBertEmbeddings HuggingFace in Spark NLP - DistilBERT CamemBertEmbeddings HuggingFace in Spark NLP - CamemBERT RoBertaEmbeddings HuggingFace in Spark NLP - RoBERTa DeBertaEmbeddings HuggingFace in Spark NLP - DeBERTa XlmRoBertaEmbeddings HuggingFace in Spark NLP - XLM-RoBERTa AlbertEmbeddings HuggingFace in Spark NLP - ALBERT XlnetEmbeddings HuggingFace in Spark NLP - XLNet LongformerEmbeddings HuggingFace in Spark NLP - Longformer BertForTokenClassification HuggingFace in Spark NLP - BertForTokenClassification DistilBertForTokenClassification HuggingFace in Spark NLP - DistilBertForTokenClassification AlbertForTokenClassification HuggingFace in Spark NLP - AlbertForTokenClassification RoBertaForTokenClassification HuggingFace in Spark NLP - RoBertaForTokenClassification XlmRoBertaForTokenClassification HuggingFace in Spark NLP - XlmRoBertaForTokenClassification CamemBertForTokenClassification HuggingFace in Spark NLP - CamemBertForTokenClassification CamemBertForSequenceClassification HuggingFace in Spark NLP - CamemBertForSequenceClassification CamemBertForQuestionAnswering HuggingFace in Spark NLP - CamemBertForQuestionAnswering BertForSequenceClassification HuggingFace in Spark NLP - BertForSequenceClassification DistilBertForSequenceClassification HuggingFace in Spark NLP - DistilBertForSequenceClassification AlbertForSequenceClassification HuggingFace in Spark NLP - AlbertForSequenceClassification RoBertaForSequenceClassification HuggingFace in Spark NLP - RoBertaForSequenceClassification XlmRoBertaForSequenceClassification HuggingFace in Spark NLP - XlmRoBertaForSequenceClassification XlnetForSequenceClassification HuggingFace in Spark NLP - XlnetForSequenceClassification LongformerForSequenceClassification HuggingFace in Spark NLP - LongformerForSequenceClassification AlbertForQuestionAnswering HuggingFace in Spark NLP - AlbertForQuestionAnswering BertForQuestionAnswering HuggingFace in Spark NLP - BertForQuestionAnswering DeBertaForQuestionAnswering HuggingFace in Spark NLP - DeBertaForQuestionAnswering DistilBertForQuestionAnswering HuggingFace in Spark NLP - DistilBertForQuestionAnswering LongformerForQuestionAnswering HuggingFace in Spark NLP - LongformerForQuestionAnswering RoBertaForQuestionAnswering HuggingFace in Spark NLP - RoBertaForQuestionAnswering XlmRobertaForQuestionAnswering HuggingFace in Spark NLP - XlmRobertaForQuestionAnswering ViTForImageClassification HuggingFace in Spark NLP - ViTForImageClassification TF Hub to Spark NLP Spark NLP TF Hub Notebooks Colab BertEmbeddings TF Hub in Spark NLP - BERT BertSentenceEmbeddings TF Hub in Spark NLP - BERT Sentence AlbertEmbeddings TF Hub in Spark NLP - ALBERT",
    "url": "/docs/en/transformers",
    "relUrl": "/docs/en/transformers"
  },
  "280": {
    "id": "280",
    "title": "FAQ",
    "content": "Useful knowledge basebase for troubleshooting some of the common issues and tips for customizing the Annotation Lab set up and configurations. 1. How to deploy multiple preannotation/training servers in parallel? By default the Annotation Lab installation is configured to use only one model server. If you want to allow the deployment of multiple model servers (e.g. up to 3), open the annotationlab-upgrader.sh script located under the artifacts folder of your Annotation Lab installation directory. Update the below configuration properties in the annotaionlab-upgrader.sh script for deploying upto 3 model servers. --set airflow.model_server.count=3 --set model_server.count=3 Save the file and re-run this script for the changes to take effect. 2. How can I access the API documentation? API documentation is included in the Annotation Lab setup. So you will need to first set up Annotation Lab. Only admin user can view the API documentation available under Settings &gt; API Integration. 3. Can I upload/download tasks/data using API? Yes, it is possible to perform both the upload and download operations using API. There is import and export API for those operations. You can get more details about it from the API documentation. 4. Can the user who created a project/task be assigned annotation/review tasks? The project owner has by default all permissions (annotator, reviewer, manager). So we do not need to explicitly assign the annotator or reviewer role to the owner for the tasks. 5. Can I download the swagger API documentation? No. At present you can only access the API documentation directly from the API integration page under Settings &gt; API Integration. 6. How to uninstall Kubernetes during faulty install and re-install Annotation Lab? If you have access to backend CLI then you can follow the steps below to fix faulty installation issue. Go to /usr/local/bin cd /usr/local/bin Run the uninstall script ./k3s-uninstall.sh Re-run the installer script from the project folder ./k3s-installer.sh Run the annotation lab installer ./annotationlab-installer.sh This will take some time and produce the output below: NAME STATUS ROLES AGE VERSION ip-172-31-91-230 Ready control-plane,master 3m38s v1.22.4+k3s1 Image is up to date for sha256:18481c1d051558c1e2e3620ba4ddf15cf4734fe35dc45fbf8065752925753c9d Image is up to date for sha256:a5b6ca180ebba94863ac9310ebcfacaaa64aca9efaa3b1f07ff4fad90ff76f68 Image is up to date for sha256:55208fe5388a7974bc4e3d63cfe20b2f097a79e99e9d10916752c3f8da560aa6 Image is up to date for sha256:a566a53e9ae7171faac1ce58db1d48cf029fbeb6cbf28cd53fd9651d5039429c Image is up to date for sha256:09ad16bd0d3fb577cbfdbbdc754484f707b528997d64e431cba19ef7d97ed785 NAME: annotationlab LAST DEPLOYED: Thu Sep 22 14:16:10 2022 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: ############################################################################# Thank you for installing annotationlab. Please run the following commands to get the credentials. export KEYCLOAK_CLIENT_SECRET_KEY=$(kubectl get secret annotationlab-secret --template={{.data.KEYCLOAK_CLIENT_SECRET_KEY}} | base64 --decode; echo) export PG_PASSWORD=$(kubectl get secrets annotationlab-postgresql -o yaml | grep &#39; postgresql-password:&#39; | cut -d &#39; &#39; -f 4 | base64 -d; echo) export PG_KEYCLOAK_PASSWORD=$(kubectl get secrets annotationlab-keyclo-postgres -o yaml | grep &#39; postgresql-password:&#39; | cut -d &#39; &#39; -f 4 | base64 -d; echo) export ADMIN_PASSWORD=$(kubectl get secret annotationlab-keyclo-admincreds --template={{.data.password}} | base64 --decode; echo) #############################################################################",
    "url": "/docs/en/alab/troubleshooting",
    "relUrl": "/docs/en/alab/troubleshooting"
  },
  "281": {
    "id": "281",
    "title": "Video Tutorials",
    "content": "{%- include extensions/youtube.html id=&#39;ycrJX_UMA6I&#39; -%}Programmatic labeling in Annotation Lab. Suvrat Joshi - October, 2022 {%- include extensions/youtube.html id=&#39;tzEwzT_HmXM&#39; -%}How to create a NER project in Annotation Lab. Suvrat Joshi - September, 2022 {%- include extensions/youtube.html id=&#39;jgUylZlz3uA&#39; -%}End-to-End No-Code Development of NER model for Text with Annotation Lab. Dia Trambitas - April, 2022 {%- include extensions/youtube.html id=&#39;JDDmTF6ir9k&#39; -%}End-to-End No-Code Development of Visual NER Models for PDFs and Images. Dia Trambitas - April, 2022 Add a new user. Ida Lucente - January, 2021 Update password from User Profile. Ida Lucente - January, 2021 Collect the client secret. Ida Lucente - January, 2021 Setup 2FA. Ida Lucente - January, 2021 API usage example. Ida Lucente - January, 2021",
    "url": "/docs/en/alab/tutorials",
    "relUrl": "/docs/en/alab/tutorials"
  },
  "282": {
    "id": "282",
    "title": "Understand Entities in Context - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/understand_financial_entities_context",
    "relUrl": "/understand_financial_entities_context"
  },
  "283": {
    "id": "283",
    "title": "Understand Entities in Context - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/understand_legal_entities_context",
    "relUrl": "/understand_legal_entities_context"
  },
  "284": {
    "id": "284",
    "title": "User Management",
    "content": "Basic user management features are present in the Annotation Lab. The user with the admin privilege can add or remove other users from the system or can edit user information if necessary. This feature is available by selecting the Users option under the Settings menu from the navigation panel. All user accounts created on the Annotation Lab can be seen on the Users page. The table shows the username, first name, last name, and email address of all created user accounts. A user with the admin privilege can edit or delete that information, add a user to a group or change the user’s password. User Details Annotation Lab stores basic information for each user. Such as the First Name, Last Name, and Email. It is editable from the Details section by any user with admin privilege. User Groups Currently, two user groups are available: Annotators and Admins. By default, a new user gets added to the Annotators group. It means the user will not have access to any admin features, such as user management or other settings. To add a user to the admin group, a user with admin privilege needs to navigate to the Users page, click on the concerned username or select the Edit option from the More Actions icon, then go to the Group section and check the Admins checkbox. Reset User Credentials A user with the admin privilege can change the login credentials for another user by navigating to the Credentials section of the edit user page and defining a new (temporary) password. For extra protection, the user with the admin privilege can enforce the password change on the next login. SAML Integration AnnotationLab supports Security Assertion Markup Language (SAML). To login to AnnotationLab using SAML, follow the steps below. SAML Server Setup Run the following command to setup a sample SAML server in a Docker environment: docker run --rm --name mysamlserver -p 8081:8080 -p 8443:8443 -e SIMPLESAMLPHP_SP_ENTITY_ID=http://{IP}/auth/realms/master -e SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://{IP}/auth/realms/master/broker/saml/endpoint --network annotationlab kristophjunge/test-saml-idp SAML Configuration Follow the steps described below to setup a SAML connection. Goto AnnotationLab Keyclock console and navigate to Identity Providers under Configure on the left-side menu. Choose SAML v2.0 from Add Provider drop-down menu and a configuration page should appear. Provide values for Alias(e.g: saml) and Display Name(e.g: SAML). The value for Display Name will be seen in the login page. Now, set the value of the following attributes as shown below: Enabled: On Store Tokens: On First Login Flow : first broker login Sync Mode: force Under SAML Config specify values for the following parameters as provided by SAML sever: Service Provider Entity ID Single Sign-On Service URL Single Logout Service URL Choose a Principal Type(e.g: Attribute[Name]) and add value to Principal Attribute(e.g. email) according to the data provided by SAML server Click on the Save button to save the changes. Identity Provider Mapper An Identity Provider Mapper must be defined for importing SAML data provided by the External Identity Provider (IDP) and using it for authenticating into Annotation Lab. This allows user profile and other user information to be imported and made available into Annotation Lab. On Identity Providers &gt; SAML page click on the Mappers tab located next to the Settings tab and follow the steps below: Click on Create. This should open a form to add a new Identity Provider Mapper Set the value for the following attributes: Name(e.g: uma_protection mapper) Sync Mode Override: inherit Mapper Type: Hardcoded Role Click on the Select Role button and under the Client Roles menu put annotationlab. Now, select uma_protection and click on Select client role. annotationlab.uma_protection should be the value displayed for Role Save the changes Default Group Default groups are used for assigning group membership automatically whenever any new user is created. Add Annotators as the default group using the following steps: Goto Groups, on the left side panel under Manages Select the Default Groups tab Under Available Groups select Annotators and then click on the Add button Now, Annotators should be listed under Default Groups. Login to Annotation Lab Goto the Annotation Lab’s login dashboard and click on the display name which was set earlier(e.g: SAML). This is displayed under Or sign in with. Login with the data provided by the SAML server here: The user account information is updated and the user is redirected to Annotation Lab and presented with the Project dashboard. NOTES: Users added as an IDP will be available in the Users tab on the left side under Manages",
    "url": "/docs/en/alab/user_management",
    "relUrl": "/docs/en/alab/user_management"
  },
  "285": {
    "id": "285",
    "title": "Utility & Helper Modules",
    "content": "ALAB (Annotation Lab) Interface Module Spark NLP for Healthcare provides functionality to interact with the Annotation Lab using easy-to-use functions.Annotation Lab is a tool for multi-modal data annotation. It allows annotation teams to efficiently collaborate to generate training data for ML models and/or to validate automatic annotations generated by those. ALAB Intreacting Module provides using ALAB programmatically. A detailed usage examples can be found at Complete ALAB Module SparkNLP JSL, and Python’s documentation in the Python API. Following are the functionalities supported by the module: Generating a CoNLL formatted file from the annotation JSON for training an NER model. Generating a csv/excel formatted file from the annotation JSON for training classification, assertion, and relation extraction models. Build preannotation JSON file using Spark NLP pipelines, saving it as a JSON and uploading preannotations to a project. Interacting with the ALAB instance, and setting up projects of ALAB. Getting the list of all projects in the ALAB instance. Creating New Projects. Deleting Projects. Setting &amp; editing configuration of projects. Accessing/getting configuration of any existing project. Upload tasks to a project. Deleting tasks of a project. Start Module # import the module from sparknlp_jsl.alab import AnnotationLab alab = AnnotationLab() Generate Data for Traing a Classification Model alab.get_classification_data( # required: path to Annotation Lab JSON export input_json_path=&#39;alab_demo.json&#39;, # optional: set to True to select ground truth completions, False to select latest completions, # defaults to False # ground_truth=False) Converting The Json Export into a Conll Format Suitable for Training an Ner Model alab.get_conll_data( # required: Spark session with spark-nlp-jsl jar spark=spark, # required: path to Annotation Lab JSON export input_json_path=&quot;alab_demo.json&quot;, # required: name of the CoNLL file to save output_name=&quot;conll_demo&quot;, # optional: path for CoNLL file saving directory, defaults to &#39;exported_conll&#39; # save_dir=&quot;exported_conll&quot;, # optional: set to True to select ground truth completions, False to select latest completions, # defaults to False # ground_truth=False, # optional: labels to exclude from CoNLL; these are all assertion labels and irrelevant NER labels, # defaults to empty list # excluded_labels=[&#39;ABSENT&#39;], # optional: set a pattern to use regex tokenizer, defaults to regular tokenizer if pattern not defined # regex_pattern=&quot; s+|(?=[-.:;*+,$&amp;% [ ]])|(?&lt;=[-.:;*+,$&amp;% [ ]])&quot; # optional: list of Annotation Lab task titles to exclude from CoNLL, defaults to empty list # excluded_task_ids = [2, 3] # optional: list of Annotation Lab task titles to exclude from CoNLL, defaults to None # excluded_task_titles = [&#39;Note 1&#39;]) Converting The JSON Export into a Dataframe Suitable for Training an Assertion Model alab.get_assertion_data( # required: SparkSession with spark-nlp-jsl jar spark=spark, # required: path to Annotation Lab JSON export input_json_path = &#39;alab_demo.json&#39;, # required: annotated assertion labels to train on assertion_labels = [&#39;ABSENT&#39;], # required: relevant NER labels that are assigned assertion labels relevant_ner_labels = [&#39;PROBLEM&#39;, &#39;TREATMENT&#39;], # optional: set to True to select ground truth completions, False to select latest completions, # defaults to False # ground_truth = False, # optional: assertion label to assign to entities that have no assertion labels, defaults to None # unannotated_label = &#39;PRESENT&#39;, # optional: set a pattern to use regex tokenizer, defaults to regular tokenizer if pattern not defined # regex_pattern = &quot; s+|(?=[-.:;*+,$&amp;% [ ]])|(?&lt;=[-.:;*+,$&amp;% [ ]])&quot;, # optional: set the strategy to control the number of occurrences of the unannotated assertion label # in the output dataframe, options are &#39;weighted&#39; or &#39;counts&#39;, &#39;weighted&#39; allows to sample using a # fraction, &#39;counts&#39; allows to sample using absolute counts, defaults to None # unannotated_label_strategy = None, # optional: dictionary in the format {&#39;ENTITY_LABEL&#39;: sample_weight_or_counts} to control the number of # occurrences of the unannotated assertion label in the output dataframe, where &#39;ENTITY_LABEL&#39; are the # NER labels that are assigned the unannotated assertion label, and sample_weight_or_counts should be # between 0 and 1 if `unannotated_label_strategy` is &#39;weighted&#39; or between 0 and the max number of # occurrences of that NER label if `unannotated_label_strategy` is &#39;counts&#39; # unannotated_label_strategy_dict = {&#39;PROBLEM&#39;: 0.5, &#39;TREATMENT&#39;: 0.5}, # optional: list of Annotation Lab task IDs to exclude from output dataframe, defaults to None # excluded_task_ids = [2, 3] # optional: list of Annotation Lab task titles to exclude from output dataframe, defaults to None # excluded_task_titles = [&#39;Note 1&#39;]) Converting The JSON Export into a Dataframe Suitable for Training a Relation Extraction Model alab.get_relation_extraction_data( # required: Spark session with spark-nlp-jsl jar spark=spark, # required: path to Annotation Lab JSON export input_json_path=&#39;alab_demo.json&#39;, # optional: set to True to select ground truth completions, False to select latest completions, # defaults to False ground_truth=True, # optional: set to True to assign a relation label between entities where no relation was annotated, # defaults to False negative_relations=True, # optional: all assertion labels that were annotated in the Annotation Lab, defaults to None assertion_labels=[&#39;ABSENT&#39;], # optional: plausible pairs of entities for relations, separated by a &#39;-&#39;, use the same casing as the # annotations, include only one relation direction, defaults to all possible pairs of annotated entities relation_pairs=[&#39;DATE-PROBLEM&#39;,&#39;TREATMENT-PROBLEM&#39;,&#39;TEST-PROBLEM&#39;], # optional: set the strategy to control the number of occurrences of the negative relation label # in the output dataframe, options are &#39;weighted&#39; or &#39;counts&#39;, &#39;weighted&#39; allows to sample using a # fraction, &#39;counts&#39; allows to sample using absolute counts, defaults to None negative_relation_strategy=&#39;weighted&#39;, # optional: dictionary in the format {&#39;ENTITY1-ENTITY2&#39;: sample_weight_or_counts} to control the number of # occurrences of negative relations in the output dataframe for each entity pair, where &#39;ENTITY1-ENTITY2&#39; # represent the pairs of entities for relations separated by a `-` (include only one relation direction), # and sample_weight_or_counts should be between 0 and 1 if `negative_relation_strategy` is &#39;weighted&#39; or # between 0 and the max number of occurrences of negative relations if `negative_relation_strategy` is # &#39;counts&#39;, defaults to None negative_relation_strategy_dict = {&#39;DATE-PROBLEM&#39;: 0.1, &#39;TREATMENT-PROBLEM&#39;: 0.5, &#39;TEST-PROBLEM&#39;: 0.2}, # optional: list of Annotation Lab task IDs to exclude from output dataframe, defaults to None # excluded_task_ids = [2, 3] # optional: list of Annotation Lab task titles to exclude from output dataframe, defaults to None # excluded_task_titles = [&#39;Note 1&#39;]) Geneate JSON Containing Pre-annotations Using a Spark NLP Pipeline pre_annotations, summary = alab.generate_preannotations( # required: list of results. all_results = results, # requied: output column name of &#39;DocumentAssembler&#39; stage - to get original document string. document_column = &#39;document&#39;, # required: column name(s) of ner model(s). Note: multiple NER models can be used, but make sure their results don&#39;t overrlap. # Or use &#39;ChunkMergeApproach&#39; to combine results from multiple NER models. ner_columns = [&#39;ner_chunk&#39;], # optional: column name(s) of assertion model(s). Note: multiple assertion models can be used, but make sure their results don&#39;t overrlap. # assertion_columns = [&#39;assertion_res&#39;], # optional: column name(s) of relation extraction model(s). Note: multiple relation extraction models can be used, but make sure their results don&#39;t overrlap. # relations_columns = [&#39;relations_clinical&#39;, &#39;relations_pos&#39;], # optional: This can be defined to identify which pipeline/user/model was used to get predictions. # Default: &#39;model&#39; # user_name = &#39;model&#39;, # optional: Option to assign custom titles to tasks. By default, tasks will be titled as &#39;task_#&#39; # titles_list = [], # optional: If there are already tasks in project, then this id offset can be used to make sure default titles &#39;task_#&#39; do not overlap. # While upload a batch after the first one, this can be set to number of tasks currently present in the project # This number would be added to each tasks&#39;s ID and title. # id_offset=0) Interacting with Annotation Lab alab = AnnotationLab() username=&#39;&#39; password=&#39;&#39; client_secret=&#39;&#39; annotationlab_url=&#39;&#39; alab.set_credentials( # required: username username=username, # required: password password=password, # required: secret for you alab instance (every alab installation has a different secret) client_secret=client_secret, # required: http(s) url for you annotation lab annotationlab_url=annotationlab_url) Get All Visible Projects alab.get_all_projects() Create a New Project alab.create_project( # required: unique name of project project_name = &#39;alab_demo&#39;, # optional: other details about project. Default: Empty string project_description=&#39;&#39;, # optional: Sampling option of tasks. Default: random project_sampling=&#39;&#39;, # optional: Annotation Guidelines of project project_instruction=&#39;&#39;) Delete a Project alab.delete_project( # required: unique name of project project_name = &#39;alab_demo&#39;, # optional: confirmation for deletion. Default: False - will ask for confirmation. If set to true, will delete directly. confirm=False) Upload Tasks to a Project alab.upload_tasks( # required: name of project to upload tasks to project_name=&#39;alab_demo&#39;, # required: list of examples / tasks as string (One string is one task). task_list=task_list, # optional: Option to assign custom titles to tasks. By default, tasks will be titled as &#39;task_#&#39; title_list = [], # optional: If there are already tasks in project, then this id offset can be used to make sure default titles &#39;task_#&#39; do not overlap. # While upload a batch after the first one, this can be set to number of tasks currently present in the project # This number would be added to each tasks&#39;s ID and title. id_offset=0) Delete Tasks from a Project alab.delete_tasks( # required: name of project to upload tasks to project_name=&#39;alab_demo&#39;, # required: list of ids of tasks. # note: you can get task ids from the above step. Look for &#39;task_ids&#39; key. task_ids=[1, 2], # optional: confirmation for deletion. Default: False - will ask for confirmation. If set to true, will delete directly. confirm=False) Upload Pre-annotations to Annotation Lab alab.upload_preannotations( # required: name of project to upload annotations to project_name = &#39;alab_demo&#39;, # required: preannotation JSON preannotations = pre_annotations) Deidentification Module Spark NLP for Healthcare provides functionality to apply Deidentification using easy-to-use module named Deid. The Deid module is a tool for deidentifying Personal Health Information from data in a file path. It can be used with custom SparkNLP NER pipelines or without any pipeline specified. It returns the deidentification results as a pyspark dataframe as well as a csv or json file. The module also includes functionality for applying Structured Deidentification task to data from a file path. The function, deidentify(), can be used with a custom pipeline or without defining any custom pipeline. structured_deidentifier() function can be used for the Structured Deidentification task. Apply Deidentification With a Custom Pipeline from sparknlp_jsl import Deid deid_implementor= Deid( # required: Spark session with spark-nlp-jsl jar spark ) res= deid_implementor.deidentify( # required: The path of the input file. Default is None. File type must be &#39;csv&#39; or &#39;json&#39;. input_file_path=&quot;data.csv&quot;, #optional: The path of the output file. Default is &#39;deidentified.csv&#39;. File type must be &#39;csv&#39; or &#39;json&#39;. output_file_path=&quot;deidentified.csv&quot;, #optional: The separator of the input csv file. Default is &quot; t&quot;. separator=&quot;,&quot;, #optional: A custom pipeline model to be used for deidentification. If not specified, the default is None. custom_pipeline=nlpModel, #optional: Fields to be deidentified and their deidentification modes, by default {&quot;text&quot;: &quot;mask&quot;} fields={&quot;text&quot;: &quot;mask&quot;, &quot;text_1&quot;: &quot;obfuscate&quot;}, #optional: The masking policy. Default is &quot;entity_labels&quot;. masking_policy=&quot;fixed_length_chars&quot;, #optional: The fixed mask length. Default is 4. fixed_mask_length=4, #optional: The final chunk column name of the custom pipeline that will be deidentified, if specified. Default is &quot;ner_chunk&quot;. ner_chunk=&quot;ner_chunk&quot;, #optional: The corresponding document column name of the custom pipeline, if specified. Default is &quot;document&quot; document=&quot;document&quot;, #optional: The corresponding sentence column name of the custom pipeline, if specified. Default is &quot;sentence&quot; sentence=&quot;sentence&quot;, #optional: The corresponding token column name of the custom pipeline, if specified. Default is &quot;token&quot; token=&quot;token&quot;, #optional: The source of the reference file for obfuscation. Default is &quot;faker&quot;. #obfuscate_ref_source=&quot;both&quot;, #optional: The path of the reference file for obfuscation. Default is None. #obfuscate_ref_file_path=&quot;obfuscation.txt&quot;, #optional: Obfuscate date. Default is True. #obfuscate_date=True, #optional: The document hash coder column name. Default is &quot;documentHash&quot;. #documentHashCoder_col_name= &quot;documentHash&quot; #optional: ID column name. Default is &quot;id&quot;. #id_column_name= &quot;ID&quot; #optional: Date shift column name. Default is &quot;date_shift&quot;. #date_shift_column_name= &quot;date_shift&quot; #optional: The date tag. Default is &quot;DATE&quot;. #date_tag=&quot;DATE&quot; #optional: Language. Default is &quot;en&quot; #language=&quot;en&quot; #optional: Region. Default is &quot;us&quot; #region=&quot;us&quot; #optional: Age group obfuscation. Default is False. #age_group_obfuscation=True #optional: Age ranges for obfuscation. Default is [1, 4, 12, 20, 40, 60, 80]. #age_ranges=[1, 4, 12, 20, 40, 60, 80] #optional: Shift days. Default is False. #shift_days=False #optional: The number of days to shift. Default is None. #number_of_days=5 #optional: Use unnormalized date. Default is False. #unnormalized_date=True #optional: The unnormalized mode. Default is &quot;mask&quot;. #unnormalized_mode=&quot;obfuscate&quot; ) ++-+-+-+-+ | ID| text| text_deidentified| text_1| text_1_deidentified| ++-+-+-+-+ | 0|Record date : 2093-01-13 , David Hale , M.D . , Name : Hendrickson ...|Record date : ** , ** , M.D . , Name : ** MR .|Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-...|Date : 10-16-1991 PCP : Alveda Castles , 26 years-old , Record date...| ++-+-+-+-+ Apply Deidentification With No Custom Pipeline from sparknlp_jsl import Deid deid_implementor= Deid( # required: Spark session with spark-nlp-jsl jar spark ) res= deid_implementor.deidentify( # required: The path of the input file. Default is None. File type must be &#39;csv&#39; or &#39;json&#39;. input_file_path=&quot;data.csv&quot;, #optional: The path of the output file. Default is &#39;deidentified.csv&#39;. File type must be &#39;csv&#39; or &#39;json&#39;. output_file_path=&quot;deidentified.csv&quot;, #optional: The separator of the input csv file. Default is &quot; t&quot;. separator=&quot;,&quot;, #optional: Fields to be deidentified and their deidentification modes, by default {&quot;text&quot;: &quot;mask&quot;} fields={&quot;text&quot;: &quot;mask&quot;}, #optional: The masking policy. Default is &quot;entity_labels&quot;. masking_policy=&quot;entity_labels&quot;, #optional: Age group obfuscation. Default is False. #age_group_obfuscation=True #optional: Age ranges for obfuscation. Default is [1, 4, 12, 20, 40, 60, 80]. #age_ranges=[1, 4, 12, 20, 40, 60, 80] #optional: Shift days. Default is False. #shift_days=False #optional: The number of days to shift. Default is None. #number_of_days=5 #optional: Use unnormalized date. Default is False. #unnormalized_date=True #optional: The unnormalized mode. Default is &quot;mask&quot;. #unnormalized_mode=&quot;obfuscate&quot; ) ++-+-+ | ID| text_original| text_deid| ++-+-+ | 0| &quot;| &quot;| | 1|Record date : 2093-01-13 , David Hale , M.D . , Name : Hendrickson ...|Record date : &lt;DATE&gt; , &lt;DOCTOR&gt; , M.D . , Name : &lt;PATIENT&gt; , MR # &lt;...| | 2| &quot;| &quot;| ++-+-+ Apply Structured Deidentification from sparknlp_jsl import Deid deid_implementor= Deid( # required: Spark session with spark-nlp-jsl jar spark ) res= deid_implementor.structured_deidentifier( #required: The path of the input file. Default is None. File type must be &#39;csv&#39; or &#39;json&#39;. input_file_path=&quot;data.csv&quot;, #optional: The path of the output file. Default is &#39;deidentified.csv&#39;. File type must be &#39;csv&#39; or &#39;json&#39;. output_file_path=&quot;deidentified.csv&quot;, #optional: The separator of the input csv file. Default is &quot; t&quot;. separator=&quot;,&quot;, #optional: A dictionary that contains the column names and the tags that should be used for deidentification. Default is {&quot;NAME&quot;:&quot;PATIENT&quot;,&quot;AGE&quot;:&quot;AGE&quot;} columns_dict= {&quot;NAME&quot;: &quot;ID&quot;, &quot;DOB&quot;: &quot;DATE&quot;}, #optional: The seed value for the random number generator. Default is {&quot;NAME&quot;: 23, &quot;AGE&quot;: 23} columns_seed= {&quot;NAME&quot;: 23, &quot;DOB&quot;: 23}, #optional: The source of the reference file. Default is faker. ref_source=&quot;faker&quot;, #optional: The number of days to be shifted. Default is None shift_days=5, #optional: The path of the reference file for obfuscation. Default is None. #obfuscateRefFile: &quot;obfuscator_unique_ref_test.txt&quot;, #optional: A list of date formats. Default is [&quot;dd/MM/yyyy&quot;, &quot;dd-MM-yyyy&quot;, &quot;d/M/yyyy&quot;, &quot;dd-MM-yyyy&quot;, &quot;d-M-yyyy&quot;] #date_formats=[&quot;dd/MM/yyyy&quot;, &quot;dd-MM-yyyy&quot;] ) +-++--++-+ | NAME| DOB| ADDRESS|SBP| TEL| +-++--++-+ |[N2649912]|[18/02/1977]| 711 Nulla St.|140| 673 431234| | [W466004]|[28/02/1977]| 1 Green Avenue.|140|+23 (673) 431234| | [M403810]|[16/04/1900]|Calle del Liberta...|100| 912 345623| +-++--++-+ Compatibility This module helps to find appropriate model versions depending your distribution of John Snow Labs products. By searching our vast repository of models available at NLP Model Hub, we can return a JSON-like file with the models’s information (using method .findVersion()) or print the models that match a given query (using method .showVersion()). To use it, simply run the following: from johnsnowlabs import medical # Or: from sparknlp_jsl.compatibility import Compatibility compatibility = medical.Compatibility() # Returns a list of dict objects found_models = compatibility.findVersion(&#39;ner_clinical&#39;) To tabulate and visualize all retrieved models, you can: import pandas as pd models_df = pd. | | name | sparkVersion | version | language | date | readyToUse | |:|:-|:|:-|:--|:|:-| | 0 | ner_clinical_noncontrib | 2.4 | 2.3.0 | en | 2019-11-14T17:07:35.434 | true | | 1 | ner_clinical_large | 2.4 | 2.5.0 | en | 2020-05-21T00:35:02.624 | true | | 2 | ner_clinical | 3 | 3.0.0 | en | 2021-01-27T12:52:59.087 | true | | 3 | ner_clinical_large_en | 3 | 3.0.0 | en | 2021-03-31T12:32:55.357 | true | | 4 | ner_clinical | 3 | 3.0.0 | en | 2021-03-31T16:33:39.368 | true | | 5 | ner_clinical_large | 3 | 3.0.0 | en | 2021-03-31T15:55:14.650 | true | | 6 | ner_clinical_biobert | 3 | 3.0.0 | en | 2021-04-01T07:06:52.919 | true | | 7 | ner_clinical | 2.3 | 3.0.0 | en | 2021-03-31T16:33:39.368 | true | | 8 | ner_clinical_biobert | 2.3 | 3.0.0 | en | 2021-04-01T07:06:52.919 | true | | 9 | ner_clinical | 2.3 | 3.0.0 | en | 2021-01-27T12:52:59.087 | true | | 10 | ner_clinical | 2.3 | 3.0.0 | en | 2021-03-31T16:33:39.368 | true | | 11 | ner_clinical_large | 2.3 | 3.0.0 | en | 2021-03-31T15:55:14.650 | true | | 12 | bert_token_classifier_ner_clinical | 2.4 | 3.2.0 | en | 2021-08-28T15:51:44.492 | true | | 13 | bert_token_classifier_ner_clinical | 2.4 | 3.3.4 | en | 2022-01-06T12:42:21.908 | true | | 14 | bert_token_classifier_ner_clinical_pipeline | 3 | 3.4.1 | en | 2022-03-15T12:08:50.209 | true | | 15 | bert_token_classifier_ner_clinical_pipeline | 2.4 | 3.4.1 | en | 2022-03-15T12:56:42.874 | true | | 16 | ner_clinical_biobert_pipeline | 3 | 3.4.1 | en | 2022-03-21T15:06:54.361 | true | | 17 | ner_clinical_large_pipeline | 3 | 3.4.1 | en | 2022-03-21T14:29:11.545 | true | | 18 | ner_clinical_pipeline | 3 | 3.4.1 | en | 2022-03-21T14:32:59.531 | true | | 19 | bert_token_classifier_ner_clinical_pipeline | 3 | 3.4.1 | en | 2022-03-21T18:51:36.583 | true | | 20 | ner_clinical_trials_abstracts | 3 | 3.5.3 | en | 2022-06-22T15:26:56.789 | true | | 21 | ner_clinical_trials_abstracts_pipeline | 3 | 3.5.3 | en | 2022-06-27T07:07:17.828 | true | | 22 | bert_token_classifier_ner_clinical_trials_abstracts | 3 | 3.5.3 | en | 2022-06-29T04:10:29.985 | true | | 23 | ner_clinical_bert | 3 | 4.0.0 | ro | 2022-06-30T21:36:31.573 | true | | 24 | ner_clinical | 3 | 4.0.0 | ro | 2022-07-01T14:55:02.322 | true | | 25 | ner_clinical_bert | 3 | 4.0.2 | ro | 2022-08-12T09:12:00.992 | true | | 26 | bert_token_classifier_ner_clinical_trials_abstracts | 3 | 4.0.2 | es | 2022-08-11T14:45:17.151 | true | | 27 | ner_clinical_trials_abstracts | 3 | 4.0.2 | es | 2022-08-12T21:19:27.613 | true | | 28 | ner_clinical_bert | 3 | 4.2.2 | ro | 2022-11-22T13:33:53.852 | true | Or simply run the showVersion() method instead: compatibility.showVersion(&#39;ner_clinical&#39;) +--+++ | Pipeline/Model | lang | version | +--+++ | ner_clinical_noncontrib | en | 2.3.0 | | ner_clinical_large | en | 2.5.0 | | ner_clinical | en | 3.0.0 | | ner_clinical_large_en | en | 3.0.0 | | ner_clinical | en | 3.0.0 | | ner_clinical_large | en | 3.0.0 | | ner_clinical_biobert | en | 3.0.0 | | ner_clinical | en | 3.0.0 | | ner_clinical_biobert | en | 3.0.0 | | ner_clinical | en | 3.0.0 | | ner_clinical | en | 3.0.0 | | ner_clinical_large | en | 3.0.0 | | bert_token_classifier_ner_clinical | en | 3.2.0 | | bert_token_classifier_ner_clinical | en | 3.3.4 | | bert_token_classifier_ner_clinical_pipeline | en | 3.4.1 | | bert_token_classifier_ner_clinical_pipeline | en | 3.4.1 | | ner_clinical_biobert_pipeline | en | 3.4.1 | | ner_clinical_large_pipeline | en | 3.4.1 | | ner_clinical_pipeline | en | 3.4.1 | | bert_token_classifier_ner_clinical_pipeline | en | 3.4.1 | | ner_clinical_trials_abstracts | en | 3.5.3 | | ner_clinical_trials_abstracts_pipeline | en | 3.5.3 | | bert_token_classifier_ner_clinical_trials_abstracts | en | 3.5.3 | | ner_clinical_bert | ro | 4.0.0 | | ner_clinical | ro | 4.0.0 | | ner_clinical_bert | ro | 4.0.2 | | bert_token_classifier_ner_clinical_trials_abstracts | es | 4.0.2 | | ner_clinical_trials_abstracts | es | 4.0.2 | | ner_clinical_bert | ro | 4.2.2 | +--+++ InternalResourceDownloader This module has extended functinalities to list and download models from John Snow Labs repositories. It is an auxiliary module for finding and downloading different models for studies and analysis. As with the Compatibility module, InternalResourceDownloader is also capable of displaying the available models. The difference is that this module can filter the results based on the Python’s class name of the annotator, while Compatibility searches for models’ name. Displaying available models To display the pipelines or models, you can use the .showPrivateModels(), .showPrivatePipelines(), .returnPrivateModels(), or .returnPrivatePipelines() methods, which return the results in a list or print the results directly. For example, to list all models with class MedicalNerModel, just run (some results were ommited for brevity): medical_ner_models = medical.InternalResourceDownloader.returnPrivateModels(&quot;MedicalNerModel&quot;) medical_ner_models[0] [&#39;nerdl_tumour_demo&#39;, &#39;en&#39;, &#39;1.7.3&#39;] medical.InternalResourceDownloader.showPrivateModels(&quot;MedicalNerModel&quot;) +-+++ | Model | lang | version | +-+++ | ner_deid_subentity_bert | ro | 4.0.0 | | ner_deid_subentity | ro | 4.0.0 | | ner_pathogen | en | 4.0.0 | | ner_clinical_bert | ro | 4.0.0 | | ner_clinical | ro | 4.0.0 | | ner_ade_binary | en | 4.0.0 | | ner_living_species_300 | es | 4.0.0 | | ner_clinical_bert | ro | 4.0.2 | | ner_clinical_trials_abstracts | es | 4.0.2 | | ner_pharmacology | es | 4.0.2 | | ner_negation_uncertainty | es | 4.0.2 | | disease_mentions_tweet | es | 4.0.2 | | ner_deid_generic_bert | ro | 4.0.2 | | ner_oncology_unspecific_posology_wip | en | 4.0.0 | | ner_oncology_wip | en | 4.0.0 | | ner_oncology_therapy_wip | en | 4.0.0 | | ner_oncology_posology_wip | en | 4.0.0 | | ner_oncology_anatomy_general_wip | en | 4.0.0 | | ner_oncology_tnm_wip | en | 4.0.0 | | ner_oncology_demographics_wip | en | 4.0.0 | | ner_oncology_biomarker_wip | en | 4.0.0 | | ner_oncology_anatomy_granular_wip | en | 4.0.0 | | ner_oncology_test_wip | en | 4.0.0 | | ner_oncology_diagnosis_wip | en | 4.0.0 | | ner_oncology_response_to_treatment_wip | en | 4.0.0 | | ner_jsl | en | 4.2.0 | | ner_covid_trials | en | 4.2.0 | | ner_oncology_unspecific_posology | en | 4.0.0 | | ner_oncology | en | 4.0.0 | | ner_oncology_tnm | en | 4.0.0 | | ner_oncology_anatomy_general | en | 4.0.0 | | ner_oncology_therapy | en | 4.0.0 | | ner_oncology_test | en | 4.0.0 | | ner_oncology_diagnosis | en | 4.0.0 | | ner_oncology_demographics | en | 4.0.0 | | ner_oncology_anatomy_granular | en | 4.0.0 | | ner_oncology_response_to_treatment | en | 4.0.0 | | ner_oncology_posology | en | 4.0.0 | | ner_oncology_biomarker | en | 4.0.0 | | ner_sdoh_slim_wip | en | 4.2.1 | | ner_clinical_bert | ro | 4.2.2 | | ner_living_species_300 | es | 4.2.2 | | ner_deid_generic_bert | ro | 4.2.2 | | ner_oncology_biomarker | en | 4.2.2 | | ner_oncology_response_to_treatment | en | 4.2.2 | | ner_oncology_demographics | en | 4.2.2 | | ner_oncology_therapy | en | 4.2.2 | | ner_oncology | en | 4.2.2 | | ner_oncology_anatomy_granular | en | 4.2.2 | | ner_oncology_anatomy_general | en | 4.2.2 | | ner_oncology_diagnosis | en | 4.2.2 | | ner_oncology_tnm | en | 4.2.2 | | ner_oncology_posology | en | 4.2.2 | | ner_oncology_unspecific_posology | en | 4.2.2 | | ner_oncology_test | en | 4.2.2 | +-+++ ModelTracer This module adds information on the data to help track uids and timestamps of each stage of the pipeline. Given the following pipeline for Medical NER: # Annotator that transforms a text column from dataframe into an Annotation ready for NLP documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLModel.pretrained(&quot;sentence_detector_dl_healthcare&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) # Tokenizer splits words in a relevant format for NLP tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) # Clinical word embeddings trained on PubMED dataset word_embeddings = WordEmbeddingsModel.pretrained(&quot;embeddings_clinical&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # NER model trained on i2b2 (sampled from MIMIC) dataset clinical_ner = MedicalNerModel.pretrained(&quot;ner_clinical_large&quot;,&quot;en&quot;,&quot;clinical/models&quot;) .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) .setLabelCasing(&quot;upper&quot;) #decide if we want to return the tags in upper or lower case ner_converter = NerConverter() .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;]) .setOutputCol(&quot;ner_chunk&quot;) nlpPipeline = Pipeline( stages=[ documentAssembler, sentenceDetector, tokenizer, word_embeddings, clinical_ner, ner_converter ]) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = nlpPipeline.fit(empty_data) To add the UID and timestamp of each pipeline step, simply use from sparknlp_jsl.modelTracer import ModelTracer df = model.transform(empty_data) tracer_result = ModelTracer().addUidCols(pipeline = nlpPipeline, df = df) tracer_result.show(truncate=False) +-++--+--+-+++-+-+--+--+--+--+ |text|document |sentence|token|embeddings|ner|ner_chunk|documentassembler_model_uid |sentencedetectordlmodel_model_uid |tokenizer_model_uid |word_embeddings_model_model_uid |medicalnermodel_model_uid |nerconverter_model_uid | +-++--+--+-+++-+-+--+--+--+--+ | |[{document, 0, -1, , {sentence -&gt; 0}, []}]|[] |[] |[] |[] |[] |{uid -&gt; DocumentAssembler_3e110f5ce3dc, timestamp -&gt; 2022-10-21_22:58}|{uid -&gt; SentenceDetectorDLModel_6bafc4746ea5, timestamp -&gt; 2022-10-21_22:58}|{uid -&gt; Tokenizer_bd74fe5f5860, timestamp -&gt; 2022-10-21_22:58}|{uid -&gt; WORD_EMBEDDINGS_MODEL_9004b1d00302, timestamp -&gt; 2022-10-21_22:58}|{uid -&gt; MedicalNerModel_1a8637089929, timestamp -&gt; 2022-10-21_22:58}|{uid -&gt; NerConverter_643c903e9161, timestamp -&gt; 2022-10-21_22:58}| +-++--+--+-+++-+-+--+--+--+--+",
    "url": "/docs/en/utility_helper_modules",
    "relUrl": "/docs/en/utility_helper_modules"
  },
  "286": {
    "id": "286",
    "title": "Version Compatibility",
    "content": "Healthcare NLP Spark OCR Spark NLP 3.0.0 3.4.0 3.0.0 3.0.1 3.4.0 3.0.1 3.0.2 3.4.0 3.0.2 3.0.3 3.4.0 3.0.3 3.1.0 3.4.1 3.1.0 3.1.1 3.4.1 3.1.1 3.1.2 3.4.1 3.1.2 3.1.3 3.5.0 3.1.3 3.1.3 3.6.0 3.1.3 3.2.0 3.7.0 3.2.0 3.2.1 3.8.0 3.2.1 3.2.2 3.8.0 3.2.2 3.2.3 3.8.0 3.2.3 3.3.0 3.8.0 3.3.0 3.3.1 3.9.0 3.3.1 3.3.2 3.9.0 3.3.2 3.3.4 3.10.0 3.3.4 3.4.0 3.11.0 3.4.0 3.4.1 3.11.0 3.4.1 3.4.2 3.11.0 3.4.2 3.5.0 3.11.0 3.4.2 3.5.1 3.12.0 3.4.2 3.5.2 3.13.0 3.4.4 3.5.2 3.14.0 3.4.4 4.0.0 4.0.0 4.0.0 4.1.0 4.1.0 4.1.0 4.2.1 4.2.0 4.2.1 4.2.3 4.2.4 4.2.4 4.2.4 4.3.0 4.2.4",
    "url": "/docs/en/version_compatibility",
    "relUrl": "/docs/en/version_compatibility"
  },
  "287": {
    "id": "287",
    "title": "Video Tutorials",
    "content": "{%- include extensions/youtube.html id=&#39;isxffn4Tcds&#39; -%}How to Install NLP Server on Azure {%- include extensions/youtube.html id=&#39;YZFhsZZD6QM&#39; -%}How to Import a License in the NLP Server",
    "url": "/docs/en/nlp_server/video_tutorials",
    "relUrl": "/docs/en/nlp_server/video_tutorials"
  },
  "288": {
    "id": "288",
    "title": "Visual Document Understanding - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/visual_document_understanding",
    "relUrl": "/visual_document_understanding"
  },
  "289": {
    "id": "289",
    "title": "Visual NER",
    "content": "Annotating text included in image documents (e.g. scanned documents) is a common use case in many verticals but comes with several challenges. With the new Visual NER Labeling config, we aim to ease the work of annotators by allowing them to simply select text from an image and assign the corresponding label to it. This feature is powered by Spark OCR 3.5.0; thus a valid Spark OCR license is required to get access to it. Here is how this can be used: Upload a valid Spark OCR license. See how to do this here. Create a new project, specify a name for your project, add team members if necessary, and from the list of predefined templates (Default Project Configs) choose “Visual NER Labeling”. Update the configuration if necessary. This might be useful if you want to use other labels than the currently defined ones. Click the save button. While saving the project, a confirmation dialog is displayed to let you know that the Spark OCR pipeline for Visual NER is being deployed. Import the tasks you want to annotate (images). Start annotating text on top of the image by clicking on the text tokens or by drawing bounding boxes on top of chunks or image areas. Export annotations in your preferred format. The entire process is illustrated below: Support for multi-page PDF documents When a valid Saprk OCR license is available, Annotation Lab offers support for multi-page PDF annotation. The complete flow of import, annotation, and export for multi-page PDF files is currently supported. Users have two options for importing a new PDF file into the Visual NER project Import PDF file from local storage; Add a link to the PDF file in the file attribute. After import, the task becomes available on the Tasks Page. The title of the new task is the name of the imported file. On the labeling page, the PDF file is displayed with pagination so that annotators can annotate on the PDF document one page at a time. OCR and Visual NER servers Just like (preannotation servers)[], Annotation Lab 3.0.0 also supports the deployment of multiple OCR servers. If a user has uploaded a Spark OCR license, be it airgap or floating, OCR inference is enabled. To create a Visual NER project, users have to deploy at least one OCR server. Any OCR server can perform preannotation. To select the OCR server, users have to go to the Import page, toggle the OCR option and from the popup, choose one of the available OCR servers. In no suitable OCR server is available, one can be created by choosing the “Create Server” option. Visual NER Training And Preannotation With release 3.4.0 came support for Visual NER Automated Preannotation and Model Training. Visual NER Training Support Version 3.4.0 of the Annotation Lab offers the ability to train Visual NER models, apply active learning for automatic model training, and preannotate image-based tasks with existing models in order to accelerate annotation work. License Requirements Visual NER annotation, training and preannotation features are dependent on the presence of a Spark OCR license. Floating or airgap licenses with scope ocr: inference and ocr: training are required for preannotation and training respectively. Model Training The training feature for Visual NER projects can be activated from the Setup page via the “Train Now” button (See 1). From the Training Settings sections, users can tune the training parameters (e.g. Epoch, Batch) and choose the tasks to use for training the Visual NER model (See 3). Information on the training progress is shown in the top right corner of the Model Training tab (See 2). Users can check detailed information regarding the success or failure of the last training. Training Failure can occur because of: Insufficient number of completions Poor quality of completions Insufficient CPU and Memory Wrong training parameters When triggering the training, users can choose to immediately deploy the model or just train it without deploying. If immediate deployment is chosen, then the labeling config is updated with references to the new model so that it will be used for preannotations. Training Server Specification The minimal required training configuration is 64 GB RAM, 16 Core CPU for Visual NER Training. Visual NER Preannotation For running preannotation on one or several tasks, the Project Owner or the Manager must select the target tasks and can click on the Preannotate button from the upper right side of the Tasks Page. This will display a popup with information regarding the last deployment including the list of models deployed and the labels they predict. Known Limitations: When bulk preannotation is run on a lot of tasks, the preannotation can fail due to memory issues. Preannotation currently works at token level, and does not merge all tokens of a chunk into one entity. Preannotation Server Specification The minimal required training configuration is 32 GB RAM, 2 Core CPU for Visual NER Model.",
    "url": "/en/alab/visual_ner.html",
    "relUrl": "/en/alab/visual_ner.html"
  },
  "290": {
    "id": "290",
    "title": "Wiki",
    "content": "This page is created for sharing some tips and tricks for the Spark NLP library. You can find valuable information under the related highlights. Miscellaneous Loading the Same Model Into the Same Annotator More Than One Time There is 1 instance of a model when we use .pretrained or .load. So when we try to use the same model with the same annotator more than one time with different parameters, it fails. We cannot have more than 1 model per annotator in the memory. You can load 10 different NerModel, but not the same model twice with different parameters, it will just load once and reuse it the other times. It’s not possible to duplicate the annotator/model unless the model is different. (each model creates a unique id). You can only load 1 model per annotator, once that happens that model with all its parameters stays in the memory. So if you want to load the very same model on the very same annotator in another pipeline, whether you use .transform, or LightPipeline, it will take the already loaded model from the memory. So if the first one has different inputCol/outputCol then the second pipeline just can’t find the input/output or if the parameters are different in the second pipeline you may not see the desired outcome. So the lesson here is, if you want to use the same model in different places, you must make sure they all have the same parameters. This behavior is the same for LP and .transform. LightPipeline LightPipeline does not check the storageRef of resolver models. This feature will make LP so complicated and also slower. So, the resolver models can work with an embeddings model that is not trained with in LightPipeline, but they return irrelevant results. ChunkMergeApproach Chunk Prioritization in ChunkMergeApproach ChunkMergeApproach() has some prioritizing rules while merging chunks that come from entity extractors (NER models, ContextualParser, TextMatcher, RegexMatcher, etc.): In case of the extracted chunks are same in the all given entity extractors, ChunkMergeApproach prioritizes the leftmost chunk output. Example: When we use ner_posology and ner_clinical models together, and if there is insulin in the clinical text, merger will behave like this: chunk_merger = ChunkMergeApproach() .setInputCols([&quot;ner_posology_chunk&quot;, &quot;ner_clinical_chunk&quot;]) .setOutputCol(&quot;merger_output&quot;) ... &gt;&gt; ner_posology_chunk: insulin -&gt; DRUG &gt;&gt; ner_clinical_chunk: insulin -&gt; TREATMENT &gt;&gt; merger_output: insulin -&gt; DRUG In the event of chunk names being different but some of them are overlapped, ChunkMergeApproach prioritizes the longest chunk even though it is not in the leftmost. Example: If we use ner_posology and ner_posology_greedy models together in the same pipeline and merge their results on a clinical text that has “… bactrim for 14 days …”, merger result will be as shown below: chunk_merger = ChunkMergeApproach() .setInputCols([&quot;ner_posology_chunk&quot;, &quot;ner_posology_greedy_chunk&quot;]) .setOutputCol(&quot;merger_output&quot;) ... &gt;&gt; ner_posology_chunk: bactrim -&gt; DRUG &gt;&gt; ner_posology_greedy_chunk: bactrim for 14 days -&gt; DRUG &gt;&gt; merger_output: bactrim for 14 days -&gt; DRUG Confidence scores don’t have any effect on prioritization. Sentence Entity Resolver Confidence vs Cosine Distance Calculation of Resolvers Let’s assume we have the 10 closest candidates (close meaning lower cosine distance) in our results. The confidence score is calculated with Softmax (vector to vector function). The vector is the full input and the output is also a full vector, it is not a function that is calculated item by item. Each item in the output depends on all the distances. So, what you are expecting is not “expected”. If you get two distances 0.1 and 0.1, Softmax would return 0.5 and 0.5 for each. But if you have 0.1 and 10 distances, Softmax would be 1 and 0. You can have a low distance (chunks are very similar semantically) but low confidence if there are many other chunks also very similar. And sometimes you can have high confidence but high distance, meaning there is only one chunk “close” to your target but not so close. In general, we can see less distance and less confidence but not perfect linear relationships. We can say that using the distance is a better parameter to judge the “goodness” of the resolution than the confidence. So, We recommend that you consider the cosine distance.",
    "url": "/docs/en/wiki",
    "relUrl": "/docs/en/wiki"
  },
  "291": {
    "id": "291",
    "title": "Workflows",
    "content": "When a team of people collaborate on a large annotation project, the work can be organized into multi-step workflows for an easier management of each team member’s responsabilities. This is also necessary when the project has strict requirements such as the same document must be labeled by multiple annotators; the annotations must be checked by a senior annotator. The default workflow supported by the Annotation Lab involves task assignment to one or multiple Annotators and to maximum one Reviewer. In the majority of projects having one annotator working on a task and then one reviewer checking the work done by the annotator is sufficient. NOTE: In NER projects, we recommend that in the early stages of a project, a batch of 50 - 100 content rich tasks should be assigned to all annotators for checking the Inter Annotator Agreement (IAA). This is a best practice to follow in order to quickly identify the difference in annotations as a complementary way to ensure high agreement and completeness across team. NOTE: When multiple annotators are assigned to a task, multiple ground truth completions will be created for that task. The way Annotation Lab prioritises the ground truth completion used for model training and CONLL export is via the priority assigned for each user in the Team (see Project Configuration). When more complex workflows need to be implemented, this is possible using the task tagging functionality provided by the Annotation Lab. Tags can be used for splitting work across the team but also for differentiating between first-level annotators and second-level reviewers. To add a tag, select a task and press Tags &gt; Add More. Tasks can be filtered by tags, making it easier to identify, for example, which documents are completed and which ones need to be reviewed.",
    "url": "/docs/en/alab/workflow",
    "relUrl": "/docs/en/alab/workflow"
  }
  
}
