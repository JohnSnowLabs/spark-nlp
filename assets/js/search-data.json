{
  "0": {
    "id": "0",
    "title": "404",
    "content": "404 Page not found :(",
    "url": "/404.html",
    "relUrl": "/404.html"
  },
  "1": {
    "id": "1",
    "title": "GPU vs CPU benchmark",
    "content": "This section includes benchmarks for different Approach() (training classes), comparing their performance when running in m5.8xlarge CPU vs a Tesla V100 SXM2 GPU, as described in the Machine Specs section below. Different benchmarks, as well as their takeaways and some conclusions of how to get the best of GPU, are included as well, to guide you in the process of getting the best performance out of Spark NLP on GPU. Each major release comes with big improvements, so please, make sure you use at least that version to fully levearge Spark NLP capabilities on GPU. Machine specs CPU An AWS m5.8xlarge machine was used for the CPU benchmarking. This machine consists of 32 vCPUs and 128 GB of RAM, as you can check in the official specification webpage available here GPU A Tesla V100 SXM2 GPU with 32GB of memory was used to calculate the GPU benchmarking. Versions The benchmarking was carried out with the following Spark NLP versions: Spark version: 3.0.2 Hadoop version: 3.2.0 SparkNLP version: 3.3.4 Spark nodes: 1 Benchmark on classifierDLApproach() This experiment consisted of training a Deep Learning Binary Classifier (Question vs Statement classes) at sentence-level, using a fully connected CNN and Bert Sentence Embeddings. Only 1 Spark node was usd for the training. We used the Spark NLP class ClassifierDL and it’s method Approach() as described in the documentation. The pipeline looks as follows: Dataset The size of the dataset was relatively small (200K), consisting of: Training (rows): 162250 Test (rows): 40301 Training params Different batch sizes were tested to demonstrate how GPU performance improves with bigger batches compared to CPU, for a constant number of epochs and learning rate. Epochs: 10 Learning rate: 0.003 Batch sizes: 32, 64, 256, 1024 Results Even for this average-sized dataset, we can observe that GPU is able to beat the CPU machine by a 76% in both training and inference times. Training times depending on batch (in minutes) Batch size CPU GPU 32 66 16.1 64 65 15.3 256 64 14.5 1024 64 14 Inference times (in minutes) The average inference time remained more or less constant regardless the batch size: CPU: 8.7 min GPU: 2 min Performance metrics A weighted F1-score of 0.88 was achieved, with a 0.90 score for question detection and 0.83 for statements. Benchmark on NerDLApproach() This experiment consisted of training a Name Entity Recognition model (token-level), using our class NerDLApproach(), using Bert Word Embeddings and a Char-CNN-BiLSTM Neural Network. Only 1 Spark node was used for the training. We used the Spark NLP class NerDL and it’s method Approach() as described in the documentation. The pipeline looks as follows: Dataset The size of the dataset was small (17K), consisting of: Training (rows): 14041 Test (rows): 3250 Training params Different batch sizes were tested to demonstrate how GPU performance improves with bigger batches compared to CPU, for a constant number of epochs and learning rate. Epochs: 10 Learning rate: 0.003 Batch sizes: 32, 64, 256, 512, 1024, 2048 Results Even for this small dataset, we can observe that GPU is able to beat the CPU machine by a 62% in training time and a 68% in inference times. It’s important to mention that the batch size is very relevant when using GPU, since CPU scales much worse with bigger batch sizes than GPU. Training times depending on batch (in minutes) Batch size CPU GPU 32 9.5 10 64 8.1 6.5 256 6.9 3.5 512 6.7 3 1024 6.5 2.5 2048 6.5 2.5 Inference times (in minutes) Although CPU times in inference remain more or less constant regardless the batch sizes, GPU time experiment good improvements the bigger the batch size is. CPU times: ~29 min Batch size GPU 32 10 64 6.5 256 3.5 512 3 1024 2.5 2048 2.5 Performance metrics A macro F1-score of about 0.92 (0.90 in micro) was achieved, with the following charts extracted from the NERDLApproach() logs: Inference benchmark on BertSentenceEmbeddings() This experiment consisted of benchmarking the improvement obtained in inference by using GPU on BertSentenceEmbeddings(). We used the Spark NLP class BertSentenceEmbeddings() described in the Transformers documentation. The pipeline contains only two components and looks as follows: Dataset The size of the dataset was bigger than the previous ones, with 417735 rows for inference. Results We have observed in previous experiments, using BertSentenceEmbeddings (classifierDL) and also BertEmbeddings (NerDL) how GPU improved both training and inference times. In this case, we observe again big improvements in inference, what is already pointing that one of the main reasons of why GPU improves so much over CPU is the better management of Embeddings (word, sentence level) and bigger batch sizes. Batch sizes: 32, 64, 256, 1024 Inference times depending on batch (in minutes) Batch size CPU GPU 32 80 9.9 64 77 9.8 256 63 9.4 1024 62 9.1 Takeaways: How to get the best of the GPU You will experiment big GPU improvements in the following cases: Embeddings and Transformers are used in your pipeline. Take into consideration that GPU will performance very well in Embeddings / Transformer components, but other components of your pipeline may not leverage as well GPU capabilities; Bigger batch sizes get the best of GPU, while CPU does not scale with bigger batch sizes; Bigger dataset sizes get the best of GPU, while may be a bottleneck while running in CPU and lead to performance drops; MultiGPU training Right now, we don’t support multigpu training (1 model in different GPUs in parallel), but you can train different models in different GPU. Where to look for more information about Training Please, take a look at the Spark NLP and feel free to reach us out in case you want to maximize the performance on your GPU.",
    "url": "/docs/en/CPUvsGPUbenchmark",
    "relUrl": "/docs/en/CPUvsGPUbenchmark"
  },
  "2": {
    "id": "2",
    "title": "Spark NLP - Advanced Settings",
    "content": "SparkNLP Properties You can change the following Spark NLP configurations via Spark Configuration: Property Name Default Meaning spark.jsl.settings.pretrained.cache_folder ~/cache_pretrained The location to download and extract pretrained Models and Pipelines. By default, it will be in User’s Home directory under cache_pretrained directory spark.jsl.settings.storage.cluster_tmp_dir hadoop.tmp.dir The location to use on a cluster for temporarily files such as unpacking indexes for WordEmbeddings. By default, this locations is the location of hadoop.tmp.dir set via Hadoop configuration for Apache Spark. NOTE: S3 is not supported and it must be local, HDFS, or DBFS spark.jsl.settings.annotator.log_folder ~/annotator_logs The location to save logs from annotators during training such as NerDLApproach, ClassifierDLApproach, SentimentDLApproach, MultiClassifierDLApproach, etc. By default, it will be in User’s Home directory under annotator_logs directory spark.jsl.settings.aws.credentials.access_key_id None Your AWS access key to use your S3 bucket to store log files of training models or access tensorflow graphs used in NerDLApproach spark.jsl.settings.aws.credentials.secret_access_key None Your AWS secret access key to use your S3 bucket to store log files of training models or access tensorflow graphs used in NerDLApproach spark.jsl.settings.aws.credentials.session_token None Your AWS MFA session token to use your S3 bucket to store log files of training models or access tensorflow graphs used in NerDLApproach spark.jsl.settings.aws.s3_bucket None Your AWS S3 bucket to store log files of training models or access tensorflow graphs used in NerDLApproach spark.jsl.settings.aws.region None Your AWS region to use your S3 bucket to store log files of training models or access tensorflow graphs used in NerDLApproach spark.jsl.settings.onnx.gpuDeviceId 0 Constructs CUDA execution provider options for the specified non-negative device id. spark.jsl.settings.onnx.intraOpNumThreads 6 Sets the size of the CPU thread pool used for executing a single graph, if executing on a CPU. spark.jsl.settings.onnx.optimizationLevel ALL_OPT Sets the optimization level of this options object, overriding the old setting. spark.jsl.settings.onnx.executionMode SEQUENTIAL Sets the execution mode of this options object, overriding the old setting. How to set Spark NLP Configuration SparkSession: You can use .config() during SparkSession creation to set Spark NLP configurations. from pyspark.sql import SparkSession spark = SparkSession.builder .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;16G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000m&quot;) .config(&quot;spark.jsl.settings.pretrained.cache_folder&quot;, &quot;sample_data/pretrained&quot;) .config(&quot;spark.jsl.settings.storage.cluster_tmp_dir&quot;, &quot;sample_data/storage&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2&quot;) .getOrCreate() spark-shell: spark-shell --driver-memory 16g --conf spark.driver.maxResultSize=0 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryoserializer.buffer.max=2000M --conf spark.jsl.settings.pretrained.cache_folder=&quot;sample_data/pretrained&quot; --conf spark.jsl.settings.storage.cluster_tmp_dir=&quot;sample_data/storage&quot; --packages com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 pyspark: pyspark --driver-memory 16g --conf spark.driver.maxResultSize=0 --conf spark.serializer=org.apache.spark.serializer.KryoSerializer --conf spark.kryoserializer.buffer.max=2000M --conf spark.jsl.settings.pretrained.cache_folder=&quot;sample_data/pretrained&quot; --conf spark.jsl.settings.storage.cluster_tmp_dir=&quot;sample_data/storage&quot; --packages com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 Databricks: On a new cluster or existing one you need to add the following to the Advanced Options -&gt; Spark tab: spark.kryoserializer.buffer.max 2000M spark.serializer org.apache.spark.serializer.KryoSerializer spark.jsl.settings.pretrained.cache_folder dbfs:/PATH_TO_CACHE spark.jsl.settings.storage.cluster_tmp_dir dbfs:/PATH_TO_STORAGE spark.jsl.settings.annotator.log_folder dbfs:/PATH_TO_LOGS NOTE: If this is an existing cluster, after adding new configs or changing existing properties you need to restart it. Additional Configuration for Databricks When running Email Reader feature sparknlp.read().email(&quot;./email-files&quot;) on Databricks, it is necessary to include the following Spark configurations to avoid dependency conflicts: spark.driver.userClassPathFirst true spark.executor.userClassPathFirst true These configurations are required because the Databricks runtime environment includes a bundled version of the com.sun.mail:jakarta.mail library, which conflicts with jakarta.activation. By setting these properties, the application ensures that the user-provided libraries take precedence over those bundled in the Databricks environment, resolving the dependency conflict. S3 Integration Logging: To configure S3 path for logging while training models. We need to set up AWS credentials as well as an S3 path spark.conf.set(&quot;spark.jsl.settings.annotator.log_folder&quot;, &quot;s3://my/s3/path/logs&quot;) spark.conf.set(&quot;spark.jsl.settings.aws.credentials.access_key_id&quot;, &quot;MY_KEY_ID&quot;) spark.conf.set(&quot;spark.jsl.settings.aws.credentials.secret_access_key&quot;, &quot;MY_SECRET_ACCESS_KEY&quot;) spark.conf.set(&quot;spark.jsl.settings.aws.s3_bucket&quot;, &quot;my.bucket&quot;) spark.conf.set(&quot;spark.jsl.settings.aws.region&quot;, &quot;my-region&quot;) Now you can check the log on your S3 path defined in spark.jsl.settings.annotator.log_folder property. Make sure to use the prefix s3://, otherwise it will use the default configuration. Tensorflow Graphs: To reference S3 location for downloading graphs. We need to set up AWS credentials spark.conf.set(&quot;spark.jsl.settings.aws.credentials.access_key_id&quot;, &quot;MY_KEY_ID&quot;) spark.conf.set(&quot;spark.jsl.settings.aws.credentials.secret_access_key&quot;, &quot;MY_SECRET_ACCESS_KEY&quot;) spark.conf.set(&quot;spark.jsl.settings.aws.region&quot;, &quot;my-region&quot;) MFA Configuration: In case your AWS account is configured with MFA. You will need first to get temporal credentials and add session token to the configuration as shown in the examples below For logging: spark.conf.set(&quot;spark.jsl.settings.aws.credentials.session_token&quot;, &quot;MY_TOKEN&quot;) An example of a bash script that gets temporal AWS credentials can be found here This script requires three arguments: ./aws_tmp_credentials.sh iam_user duration serial_number",
    "url": "/docs/en/advanced_settings",
    "relUrl": "/docs/en/advanced_settings"
  },
  "3": {
    "id": "3",
    "title": "Analyze Non-English Text & Documents - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/analyze_non_english_text_documents",
    "relUrl": "/analyze_non_english_text_documents"
  },
  "4": {
    "id": "4",
    "title": "Analyze Spelling & Grammar - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/analyze_spelling_grammar",
    "relUrl": "/analyze_spelling_grammar"
  },
  "5": {
    "id": "5",
    "title": "Spark NLP - Annotators",
    "content": "How to read this section All annotators in Spark NLP share a common interface, this is: Annotation: Annotation(annotatorType, begin, end, result, meta-data, embeddings) AnnotatorType: some annotators share a type. This is not only figurative, but also tells about the structure of the metadata map in the Annotation. This is the one referred in the input and output of annotators. Inputs: Represents how many and which annotator types are expected in setInputCols(). These are column names of output of other annotators in the DataFrames. Output Represents the type of the output in the column setOutputCol(). There are two types of Annotators: Approach: AnnotatorApproach extend Estimators, which are meant to be trained through fit() Model: AnnotatorModel extend from Transformers, which are meant to transform DataFrames through transform() Model suffix is explicitly stated when the annotator is the result of a training process. Some annotators, such as Tokenizer are transformers, but do not contain the word Model since they are not trained annotators. Model annotators have a pretrained() on it’s static object, to retrieve the public pre-trained version of a model. pretrained(name, language, extra_location) -&gt; by default, pre-trained will bring a default model, sometimes we offer more than one model, in this case, you may have to use name, language or extra location to download them. Available Annotators Annotator Description Version AutoGGUFEmbeddings Annotator that uses the llama.cpp library to generate text embeddings with large language models. Opensource AutoGGUFModel Annotator that uses the llama.cpp library to generate text completions with large language models. Opensource AutoGGUFReranker Annotator that uses the llama.cpp library to rerank text documents based on their relevance to a given query using GGUF-format reranking models. Opensource AutoGGUFVisionModel Multimodal annotator that uses the llama.cpp library to generate text completions with large language models. Opensource BGEEmbeddings Sentence embeddings using BGE. Opensource BigTextMatcher Annotator to match exact phrases (by token) provided in a file against a Document. Opensource Chunk2Doc Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result. Opensource ChunkEmbeddings This annotator utilizes WordEmbeddings, BertEmbeddings etc. to generate chunk embeddings from either Chunker, NGramGenerator, or NerConverter outputs. Opensource ChunkTokenizer Tokenizes and flattens extracted NER chunks. Opensource Chunker This annotator matches a pattern of part-of-speech tags in order to return meaningful phrases from document. Opensource ClassifierDL ClassifierDL for generic Multi-class Text Classification. Opensource ContextSpellChecker Implements a deep-learning based Noisy Channel Model Spell Algorithm. Opensource Date2Chunk Converts DATE type Annotations to CHUNK type. Opensource DateMatcher Matches standard date formats into a provided format. Opensource DependencyParser Unlabeled parser that finds a grammatical relation between two words in a sentence. Opensource Doc2Chunk Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol. Opensource Doc2Vec Word2Vec model that creates vector representations of words in a text corpus. Opensource DocumentAssembler Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. Opensource DocumentCharacterTextSplitter Annotator which splits large documents into chunks of roughly given size. Opensource DocumentNormalizer Annotator which normalizes raw text from tagged text, e.g. scraped web pages or xml documents, from document type columns into Sentence. Opensource DocumentSimilarityRanker Annotator that uses LSH techniques present in Spark ML lib to execute approximate nearest neighbors search on top of sentence embeddings. Opensource DocumentTokenSplitter Annotator that splits large documents into smaller documents based on the number of tokens in the text. Opensource EntityRuler Fits an Annotator to match exact strings or regex patterns provided in a file against a Document and assigns them an named entity. Opensource EmbeddingsFinisher Extracts embeddings from Annotations into a more easily usable form. Opensource Finisher Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. Opensource GraphExtraction Extracts a dependency graph between entities. Opensource GraphFinisher Helper class to convert the knowledge graph from GraphExtraction into a generic format, such as RDF. Opensource ImageAssembler Prepares images read by Spark into a format that is processable by Spark NLP. Opensource LanguageDetectorDL Language Identification and Detection by using CNN and RNN architectures in TensorFlow. Opensource Lemmatizer Finds lemmas out of words with the objective of returning a base dictionary word. Opensource MultiClassifierDL Multi-label Text Classification. Opensource MultiDateMatcher Matches standard date formats into a provided format. Opensource MultiDocumentAssembler Prepares data into a format that is processable by Spark NLP. Opensource NGramGenerator A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). Opensource NerConverter Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. Opensource NerCrf Extracts Named Entities based on a CRF Model. Opensource NerDL This Named Entity recognition annotator is a generic NER model based on Neural Networks. Opensource NerOverwriter Overwrites entities of specified strings. Opensource Normalizer Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary. Opensource NorvigSweeting Spellchecker Retrieves tokens and makes corrections automatically if not found in an English dictionary. Opensource POSTagger (Part of speech tagger) Averaged Perceptron model to tag words part-of-speech. Opensource PromptAssembler Assembles a sequence of messages into a single string using a template. Opensource RecursiveTokenizer Tokenizes raw text recursively based on a handful of definable rules. Opensource RegexMatcher Uses rules to match a set of regular expressions and associate them with a provided identifier. Opensource RegexTokenizer A tokenizer that splits text by a regex pattern. Opensource SentenceDetector Annotator that detects sentence boundaries using regular expressions. Opensource SentenceDetectorDL Detects sentence boundaries using a deep learning approach. Opensource SentenceEmbeddings Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols). Opensource SentimentDL Annotator for multi-class sentiment analysis. Opensource SentimentDetector Rule based sentiment detector, which calculates a score based on predefined keywords. Opensource Stemmer Returns hard-stems out of words with the objective of retrieving the meaningful part of the word. Opensource StopWordsCleaner This annotator takes a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences. Opensource SymmetricDelete Spellchecker Symmetric Delete spelling correction algorithm. Opensource TextMatcher Matches exact phrases (by token) provided in a file against a Document. Opensource Token2Chunk Converts TOKEN type Annotations to CHUNK type. Opensource TokenAssembler This transformer reconstructs a DOCUMENT type annotation from tokens, usually after these have been normalized, lemmatized, normalized, spell checked, etc, in order to use this document annotation in further annotators. Opensource Tokenizer Tokenizes raw text into word pieces, tokens. Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. Opensource TypedDependencyParser Labeled parser that finds a grammatical relation between two words in a sentence. Opensource ViveknSentiment Sentiment analyser inspired by the algorithm by Vivek Narayanan. Opensource WordEmbeddings Word Embeddings lookup annotator that maps tokens to vectors. Opensource Word2Vec Word2Vec model that creates vector representations of words in a text corpus. Opensource WordSegmenter Tokenizes non-english or non-whitespace separated texts. Opensource YakeKeywordExtraction Unsupervised, Corpus-Independent, Domain and Language-Independent and Single-Document keyword extraction. Opensource Available Transformers Additionally, these transformers are available. Transformer Description Version AlbertEmbeddings ALBERT: A Lite BERT for Self-supervised Learning of Language Representations Opensource AlbertForQuestionAnswering AlbertForQuestionAnswering can load ALBERT Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource AlbertForTokenClassification AlbertForTokenClassification can load ALBERT Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource AlbertForSequenceClassification AlbertForSequenceClassification can load ALBERT Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource BartForZeroShotClassification BartForZeroShotClassification using a ModelForSequenceClassification trained on NLI (natural language inference) tasks. Opensource BartTransformer BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension Transformer Opensource BertForQuestionAnswering BertForQuestionAnswering can load Bert Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource BertForSequenceClassification Bert Models with sequence classification/regression head on top. Opensource BertForTokenClassification BertForTokenClassification can load Bert Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource BertForZeroShotClassification BertForZeroShotClassification using a ModelForSequenceClassification trained on NLI (natural language inference) tasks. Opensource BertSentenceEmbeddings Sentence-level embeddings using BERT. BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture. Opensource CamemBertEmbeddings CamemBert is based on Facebook’s RoBERTa model released in 2019. Opensource CamemBertForQuestionAnswering CamemBertForQuestionAnswering can load CamemBERT Models with a span classification head on top for extractive question-answering tasks like SQuAD Opensource CamemBertForSequenceClassification amemBertForSequenceClassification can load CamemBERT Models with sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for multi-class document classification tasks. Opensource CamemBertForTokenClassification CamemBertForTokenClassification can load CamemBERT Models with a token classification head on top Opensource CLIPForZeroShotClassification Zero Shot Image Classifier based on CLIP Opensource ConvNextForImageClassification ConvNextForImageClassification is an image classifier based on ConvNet models Opensource DeBertaEmbeddings DeBERTa builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in RoBERTa. Opensource DeBertaForQuestionAnswering DeBertaForQuestionAnswering can load DeBERTa Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource DeBertaForSequenceClassification DeBertaForSequenceClassification can load DeBerta v2 &amp; v3 Models with sequence classification/regression head on top. Opensource DeBertaForTokenClassification DeBertaForTokenClassification can load DeBERTA Models v2 and v3 with a token classification head on top. Opensource DistilBertEmbeddings DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. Opensource DistilBertForQuestionAnswering DistilBertForQuestionAnswering can load DistilBert Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource DistilBertForSequenceClassification DistilBertForSequenceClassification can load DistilBERT Models with sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for multi-class document classification tasks. Opensource DistilBertForTokenClassification DistilBertForTokenClassification can load DistilBERT Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource DistilBertForZeroShotClassification DistilBertForZeroShotClassification using a ModelForSequenceClassification trained on NLI (natural language inference) tasks. Opensource E5Embeddings Sentence embeddings using E5, an instruction-finetuned text embedding model that can generate text embeddings tailored to any task. Opensource MiniLMEmbeddings Sentence embeddings using MiniLM, a lightweight and efficient sentence embedding model that can generate text embeddings for various NLP tasks. Opensource ElmoEmbeddings Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark. Opensource GPT2Transformer GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. Opensource HubertForCTC Hubert Model with a language modeling head on top for Connectionist Temporal Classification (CTC). Opensource InstructorEmbeddings Sentence embeddings using INSTRUCTOR. Opensource LongformerEmbeddings Longformer is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. Opensource LongformerForQuestionAnswering LongformerForQuestionAnswering can load Longformer Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource LongformerForSequenceClassification LongformerForSequenceClassification can load Longformer Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource LongformerForTokenClassification LongformerForTokenClassification can load Longformer Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource MarianTransformer Marian is an efficient, free Neural Machine Translation framework written in pure C++ with minimal dependencies. Opensource MPNetEmbeddings Sentence embeddings using MPNet. Opensource MPNetForQuestionAnswering MPNet Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource MPNetForSequenceClassification MPNet Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource OpenAICompletion Transformer that makes a request for OpenAI Completion API for each executor. Opensource RoBertaEmbeddings RoBERTa: A Robustly Optimized BERT Pretraining Approach Opensource RoBertaForQuestionAnswering RoBertaForQuestionAnswering can load RoBERTa Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource RoBertaForSequenceClassification RoBertaForSequenceClassification can load RoBERTa Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource RoBertaForTokenClassification RoBertaForTokenClassification can load RoBERTa Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource RoBertaForZeroShotClassification RoBertaForZeroShotClassification using a ModelForSequenceClassification trained on NLI (natural language inference) tasks. Opensource RoBertaSentenceEmbeddings Sentence-level embeddings using RoBERTa. Opensource SpanBertCoref A coreference resolution model based on SpanBert. Opensource SwinForImageClassification SwinImageClassification is an image classifier based on Swin. Opensource T5Transformer T5 reconsiders all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Opensource TapasForQuestionAnswering TapasForQuestionAnswering is an implementation of TaPas - a BERT-based model specifically designed for answering questions about tabular data. Opensource UAEEmbeddings Sentence embeddings using Universal AnglE Embedding (UAE). Opensource UniversalSentenceEncoder The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks. Opensource VisionEncoderDecoderForImageCaptioning VisionEncoderDecoder model that converts images into text captions. Opensource ViTForImageClassification Vision Transformer (ViT) for image classification. Opensource Wav2Vec2ForCTC Wav2Vec2 Model with a language modeling head on top for Connectionist Temporal Classification (CTC). Opensource WhisperForCTC Whisper Model with a language modeling head on top for Connectionist Temporal Classification (CTC). Opensource XlmRoBertaEmbeddings XlmRoBerta is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl Opensource XlmRoBertaForQuestionAnswering XlmRoBertaForQuestionAnswering can load XLM-RoBERTa Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource XlmRoBertaForSequenceClassification XlmRoBertaForSequenceClassification can load XLM-RoBERTa Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource XlmRoBertaForTokenClassification XlmRoBertaForTokenClassification can load XLM-RoBERTa Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource XlmRoBertaForZeroShotClassification XlmRoBertaForZeroShotClassification using a ModelForSequenceClassification trained on NLI (natural language inference) tasks. Opensource XlmRoBertaSentenceEmbeddings Sentence-level embeddings using XLM-RoBERTa. Opensource XlnetEmbeddings XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Opensource XlnetForTokenClassification XlnetForTokenClassification can load XLNet Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource XlnetForSequenceClassification XlnetForSequenceClassification can load XLNet Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource ZeroShotNer ZeroShotNerModel implements zero shot named entity recognition by utilizing RoBERTa transformer models fine tuned on a question answering task. Opensource AutoGGUFEmbeddings Annotator that uses the llama.cpp library to generate text embeddings with large language models. The type of embedding pooling can be set with the setPoolingType method. The default is &quot;MEAN&quot;. The available options are &quot;NONE&quot;, &quot;MEAN&quot;, &quot;CLS&quot;, and &quot;LAST&quot;. If the parameters are not set, the annotator will default to use the parameters provided by the model. Pretrained models can be loaded with pretrained of the companion object: val autoGGUFEmbeddings = AutoGGUFEmbeddings.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;nomic-embed-text-v1.5.Q8_0.gguf&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the AutoGGUFEmbeddingsTest and the example notebook. Note: To use GPU inference with this annotator, make sure to use the Spark NLP GPU package and set the number of GPU layers with the setNGpuLayers method. When using larger models, we recommend adjusting GPU usage with setNCtx and setNGpuLayers according to your hardware to avoid out-of-memory errors. Input Annotator Types: DOCUMENT Output Annotator Type: SENTENCE_EMBEDDINGS Python API: AutoGGUFEmbeddings Scala API: AutoGGUFEmbeddings Source: AutoGGUFEmbeddings Show Example PythonScala &gt;&gt;&gt; import sparknlp &gt;&gt;&gt; from sparknlp.base import * &gt;&gt;&gt; from sparknlp.annotator import * &gt;&gt;&gt; from pyspark.ml import Pipeline &gt;&gt;&gt; document = DocumentAssembler() ... .setInputCol(&quot;text&quot;) ... .setOutputCol(&quot;document&quot;) &gt;&gt;&gt; autoGGUFEmbeddings = AutoGGUFEmbeddings.pretrained() ... .setInputCols([&quot;document&quot;]) ... .setOutputCol(&quot;completions&quot;) ... .setBatchSize(4) ... .setNGpuLayers(99) ... .setPoolingType(&quot;MEAN&quot;) &gt;&gt;&gt; pipeline = Pipeline().setStages([document, autoGGUFEmbeddings]) &gt;&gt;&gt; data = spark.createDataFrame([[&quot;The moons of Jupiter are 77 in total, with 79 confirmed natural satellites and 2 man-made ones.&quot;]]).toDF(&quot;text&quot;) &gt;&gt;&gt; result = pipeline.fit(data).transform(data) &gt;&gt;&gt; result.select(&quot;completions&quot;).show() +--+ | embeddings| +--+ |[[-0.034486726, 0.07770534, -0.15982522, -0.017873349, 0.013914132, 0.0365736...| +--+ import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline import spark.implicits._ val document = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val autoGGUFEmbeddings = AutoGGUFEmbeddings .pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;embeddings&quot;) .setBatchSize(4) .setPoolingType(&quot;MEAN&quot;) val pipeline = new Pipeline().setStages(Array(document, autoGGUFEmbeddings)) val data = Seq( &quot;The moons of Jupiter are 77 in total, with 79 confirmed natural satellites and 2 man-made ones.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;embeddings.embeddings&quot;).show(1, truncate=80) +--+ | embeddings| +--+ |[[-0.034486726, 0.07770534, -0.15982522, -0.017873349, 0.013914132, 0.0365736...| +--+ AutoGGUFModel Annotator that uses the llama.cpp library to generate text completions with large language models. For settable parameters, and their explanations, see HasLlamaCppProperties and refer to the llama.cpp documentation of server.cpp for more information. If the parameters are not set, the annotator will default to use the parameters provided by the model. Pretrained models can be loaded with pretrained of the companion object: val autoGGUFModel = AutoGGUFModel.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;completions&quot;) The default model is &quot;phi3.5_mini_4k_instruct_q4_gguf&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the AutoGGUFModelTest and the example notebook. Note: To use GPU inference with this annotator, make sure to use the Spark NLP GPU package and set the number of GPU layers with the setNGpuLayers method. When using larger models, we recommend adjusting GPU usage with setNCtx and setNGpuLayers according to your hardware to avoid out-of-memory errors. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: AutoGGUFModel Scala API: AutoGGUFModel Source: AutoGGUFModel Show Example PythonScala &gt;&gt;&gt; import sparknlp &gt;&gt;&gt; from sparknlp.base import * &gt;&gt;&gt; from sparknlp.annotator import * &gt;&gt;&gt; from pyspark.ml import Pipeline &gt;&gt;&gt; document = DocumentAssembler() ... .setInputCol(&quot;text&quot;) ... .setOutputCol(&quot;document&quot;) &gt;&gt;&gt; autoGGUFModel = AutoGGUFModel.pretrained() ... .setInputCols([&quot;document&quot;]) ... .setOutputCol(&quot;completions&quot;) ... .setBatchSize(4) ... .setNPredict(20) ... .setNGpuLayers(99) ... .setTemperature(0.4) ... .setTopK(40) ... .setTopP(0.9) ... .setPenalizeNl(True) &gt;&gt;&gt; pipeline = Pipeline().setStages([document, autoGGUFModel]) &gt;&gt;&gt; data = spark.createDataFrame([[&quot;Hello, I am a&quot;]]).toDF(&quot;text&quot;) &gt;&gt;&gt; result = pipeline.fit(data).transform(data) &gt;&gt;&gt; result.select(&quot;completions&quot;).show(truncate = False) +--+ |completions | +--+ |[{document, 0, 78, new user. I am currently working on a project and I need to create a list of , {prompt -&gt; Hello, I am a}, []}]| +--+ import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline import spark.implicits._ val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val autoGGUFModel = AutoGGUFModel .pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;completions&quot;) .setBatchSize(4) .setNPredict(20) .setNGpuLayers(99) .setTemperature(0.4f) .setTopK(40) .setTopP(0.9f) .setPenalizeNl(true) val pipeline = new Pipeline().setStages(Array(document, autoGGUFModel)) val data = Seq(&quot;Hello, I am a&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;completions&quot;).show(truncate = false) +--+ |completions | +--+ |[{document, 0, 78, new user. I am currently working on a project and I need to create a list of , {prompt -&gt; Hello, I am a}, []}]| +--+ `markdown AutoGGUFReranker Annotator that uses the llama.cpp library to rerank text documents based on their relevance to a given query using GGUF-format reranking models. This annotator is specifically designed for text reranking tasks, where multiple documents or text passages are ranked according to their relevance to a query. It uses specialized reranking models in GGUF format that output relevance scores for each input document. The reranker takes a query (set via setQuery) and a list of documents, then returns the same documents with added metadata containing relevance scores. The documents are processed in batches and each receives a relevance_score in its metadata indicating how relevant it is to the provided query. For settable parameters, and their explanations, see HasLlamaCppInferenceProperties, HasLlamaCppModelProperties and refer to the llama.cpp documentation of server.cpp for more information. If the parameters are not set, the annotator will default to use the parameters provided by the model. Pretrained models can be loaded with pretrained of the companion object: val reranker = AutoGGUFReranker.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;reranked_documents&quot;) .setQuery(&quot;A man is eating pasta.&quot;) The default model is &quot;bge-reranker-v2-m3-Q4_K_M&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the AutoGGUFRerankerTest and the example notebook. Note: This annotator is designed for reranking tasks and requires setting a query using setQuery. The query represents the search intent against which documents will be ranked. Each input document receives a relevance score in the output metadata. To use GPU inference with this annotator, make sure to use the Spark NLP GPU package and set the number of GPU layers with the setNGpuLayers method. When using larger models, we recommend adjusting GPU usage with setNCtx and setNGpuLayers according to your hardware to avoid out-of-memory errors. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: AutoGGUFReranker Scala API: AutoGGUFReranker Source: AutoGGUFReranker Show Example PythonScala &gt;&gt;&gt; import sparknlp &gt;&gt;&gt; from sparknlp.base import * &gt;&gt;&gt; from sparknlp.annotator import * &gt;&gt;&gt; from pyspark.ml import Pipeline &gt;&gt;&gt; document = DocumentAssembler() ... .setInputCol(&quot;text&quot;) ... .setOutputCol(&quot;document&quot;) &gt;&gt;&gt; reranker = AutoGGUFReranker.pretrained() ... .setInputCols([&quot;document&quot;]) ... .setOutputCol(&quot;reranked_documents&quot;) ... .setBatchSize(4) ... .setQuery(&quot;A man is eating pasta.&quot;) ... .setNGpuLayers(99) &gt;&gt;&gt; pipeline = Pipeline().setStages([document, reranker]) &gt;&gt;&gt; data = spark.createDataFrame([ ... [&quot;A man is eating food.&quot;], ... [&quot;A man is eating a piece of bread.&quot;], ... [&quot;The girl is carrying a baby.&quot;], ... [&quot;A man is riding a horse.&quot;] ... ]).toDF(&quot;text&quot;) &gt;&gt;&gt; result = pipeline.fit(data).transform(data) &gt;&gt;&gt; result.select(&quot;reranked_documents&quot;).show(truncate = False) +-+ |reranked_documents | +-+ |[{document, 0, 20, A man is eating food., {query -&gt; A man is eating pasta., relevance_...}]| |[{document, 0, 31, A man is eating a piece of bread., {query -&gt; A man is eating pasta.,...}]| |[{document, 0, 27, The girl is carrying a baby., {query -&gt; A man is eating pasta., rel...}]| |[{document, 0, 22, A man is riding a horse., {query -&gt; A man is eating pasta., relevan...}]| +-+ import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline import spark.implicits._ val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val reranker = AutoGGUFReranker .pretrained(&quot;bge-reranker-v2-m3-Q4_K_M&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;reranked_documents&quot;) .setBatchSize(4) .setQuery(&quot;A man is eating pasta.&quot;) .setNGpuLayers(99) val pipeline = new Pipeline().setStages(Array(document, reranker)) val data = Seq( &quot;A man is eating food.&quot;, &quot;A man is eating a piece of bread.&quot;, &quot;The girl is carrying a baby.&quot;, &quot;A man is riding a horse.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;reranked_documents&quot;).show(truncate = false) +-+ |reranked_documents | +-+ |[{document, 0, 20, A man is eating food., {query -&gt; A man is eating pasta., relevance_...}]| |[{document, 0, 31, A man is eating a piece of bread., {query -&gt; A man is eating pasta.,...}]| |[{document, 0, 27, The girl is carrying a baby., {query -&gt; A man is eating pasta., rel...}]| |[{document, 0, 22, A man is riding a horse., {query -&gt; A man is eating pasta., relevan...}]| +-+ &lt;div class=&quot;h3-box tabs-python-scala-box&quot; markdown=&quot;1&quot;&gt; ## AutoGGUFVisionModel Multimodal annotator that uses the llama.cpp library to generate text completions with large language models. It supports ingesting images for captioning. At the moment only CLIP based models are supported. For settable parameters, and their explanations, see HasLlamaCppInferenceProperties, HasLlamaCppModelProperties and refer to the llama.cpp documentation of [server.cpp](https://github.com/ggerganov/llama.cpp/tree/7d5e8777ae1d21af99d4f95be10db4870720da91/examples/server) for more information. If the parameters are not set, the annotator will default to use the parameters provided by the model. This annotator expects a column of annotator type AnnotationImage for the image and Annotation for the caption. Note that the image bytes in the image annotation need to be raw image bytes without preprocessing. We provide the helper function ImageAssembler.loadImagesAsBytes to load the image bytes from a directory. Pretrained models can be loaded with `pretrained` of the companion object: scala val autoGGUFVisionModel = AutoGGUFVisionModel.pretrained() .setInputCols(&quot;image&quot;, &quot;document&quot;) .setOutputCol(&quot;completions&quot;) The default model is &quot;llava_v1.5_7b_Q4_0_gguf&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the AutoGGUFVisionModelTest and the example notebook. Note: To use GPU inference with this annotator, make sure to use the Spark NLP GPU package and set the number of GPU layers with the setNGpuLayers method. When using larger models, we recommend adjusting GPU usage with setNCtx and setNGpuLayers according to your hardware to avoid out-of-memory errors. Input Annotator Types: IMAGE, DOCUMENT Output Annotator Type: DOCUMENT Python API: AutoGGUFVisionModel Scala API: AutoGGUFVisionModel Source: AutoGGUFVisionModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline from pyspark.sql.functions import lit documentAssembler = DocumentAssembler() .setInputCol(&quot;caption&quot;) .setOutputCol(&quot;caption_document&quot;) imageAssembler = ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) imagesPath = &quot;src/test/resources/image/&quot; data = ImageAssembler .loadImagesAsBytes(spark, imagesPath) .withColumn(&quot;caption&quot;, lit(&quot;Caption this image.&quot;)) # Add a caption to each image. nPredict = 40 model = AutoGGUFVisionModel.pretrained() .setInputCols([&quot;caption_document&quot;, &quot;image_assembler&quot;]) .setOutputCol(&quot;completions&quot;) .setBatchSize(4) .setNGpuLayers(99) .setNCtx(4096) .setMinKeep(0) .setMinP(0.05) .setNPredict(nPredict) .setNProbs(0) .setPenalizeNl(False) .setRepeatLastN(256) .setRepeatPenalty(1.18) .setStopStrings([&quot;&lt;/s&gt;&quot;, &quot;Llama:&quot;, &quot;User:&quot;]) .setTemperature(0.05) .setTfsZ(1) .setTypicalP(1) .setTopK(40) .setTopP(0.95) pipeline = Pipeline().setStages([documentAssembler, imageAssembler, model]) pipeline.fit(data).transform(data) .selectExpr(&quot;reverse(split(image.origin, &#39;/&#39;))[0] as image_name&quot;, &quot;completions.result&quot;) .show(truncate = False) +--+-+ |image_name |result | +--+-+ |palace.JPEG |[ The image depicts a large, ornate room with high ceilings and beautifully decorated walls. There are several chairs placed throughout the space, some of which have cushions] | |egyptian_cat.jpeg|[ The image features two cats lying on a pink surface, possibly a bed or sofa. One cat is positioned towards the left side of the scene and appears to be sleeping while holding] | |hippopotamus.JPEG|[ A large brown hippo is swimming in a body of water, possibly an aquarium. The hippo appears to be enjoying its time in the water and seems relaxed as it floats] | |hen.JPEG |[ The image features a large chicken standing next to several baby chickens. In total, there are five birds in the scene: one adult and four young ones. They appear to be gathered together] | |ostrich.JPEG |[ The image features a large, long-necked bird standing in the grass. It appears to be an ostrich or similar species with its head held high and looking around. In addition to] | |junco.JPEG |[ A small bird with a black head and white chest is standing on the snow. It appears to be looking at something, possibly food or another animal in its vicinity. The scene takes place out] | |bluetick.jpg |[ A dog with a red collar is sitting on the floor, looking at something. The dog appears to be staring into the distance or focusing its attention on an object in front of it.] | |chihuahua.jpg |[ A small brown dog wearing a sweater is sitting on the floor. The dog appears to be looking at something, possibly its owner or another animal in the room. It seems comfortable and relaxed]| |tractor.JPEG |[ A man is sitting in the driver&#39;s seat of a green tractor, which has yellow wheels and tires. The tractor appears to be parked on top of an empty field with] | |ox.JPEG |[ A large bull with horns is standing in a grassy field.] | +--+-+ import com.johnsnowlabs.nlp.ImageAssembler import com.johnsnowlabs.nlp.annotator._ import com.johnsnowlabs.nlp.base._ import org.apache.spark.ml.Pipeline import org.apache.spark.sql.DataFrame import org.apache.spark.sql.functions.lit val documentAssembler = new DocumentAssembler() .setInputCol(&quot;caption&quot;) .setOutputCol(&quot;caption_document&quot;) val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) val imagesPath = &quot;src/test/resources/image/&quot; val data: DataFrame = ImageAssembler .loadImagesAsBytes(ResourceHelper.spark, imagesPath) .withColumn(&quot;caption&quot;, lit(&quot;Caption this image.&quot;)) // Add a caption to each image. val nPredict = 40 val model = AutoGGUFVisionModel.pretrained() .setInputCols(&quot;caption_document&quot;, &quot;image_assembler&quot;) .setOutputCol(&quot;completions&quot;) .setBatchSize(4) .setNGpuLayers(99) .setNCtx(4096) .setMinKeep(0) .setMinP(0.05f) .setNPredict(nPredict) .setNProbs(0) .setPenalizeNl(false) .setRepeatLastN(256) .setRepeatPenalty(1.18f) .setStopStrings(Array(&quot;&lt;/s&gt;&quot;, &quot;Llama:&quot;, &quot;User:&quot;)) .setTemperature(0.05f) .setTfsZ(1) .setTypicalP(1) .setTopK(40) .setTopP(0.95f) val pipeline = new Pipeline().setStages(Array(documentAssembler, imageAssembler, model)) pipeline .fit(data) .transform(data) .selectExpr(&quot;reverse(split(image.origin, &#39;/&#39;))[0] as image_name&quot;, &quot;completions.result&quot;) .show(truncate = false) +--+-+ |image_name |result | +--+-+ |palace.JPEG |[ The image depicts a large, ornate room with high ceilings and beautifully decorated walls. There are several chairs placed throughout the space, some of which have cushions] | |egyptian_cat.jpeg|[ The image features two cats lying on a pink surface, possibly a bed or sofa. One cat is positioned towards the left side of the scene and appears to be sleeping while holding] | |hippopotamus.JPEG|[ A large brown hippo is swimming in a body of water, possibly an aquarium. The hippo appears to be enjoying its time in the water and seems relaxed as it floats] | |hen.JPEG |[ The image features a large chicken standing next to several baby chickens. In total, there are five birds in the scene: one adult and four young ones. They appear to be gathered together] | |ostrich.JPEG |[ The image features a large, long-necked bird standing in the grass. It appears to be an ostrich or similar species with its head held high and looking around. In addition to] | |junco.JPEG |[ A small bird with a black head and white chest is standing on the snow. It appears to be looking at something, possibly food or another animal in its vicinity. The scene takes place out] | |bluetick.jpg |[ A dog with a red collar is sitting on the floor, looking at something. The dog appears to be staring into the distance or focusing its attention on an object in front of it.] | |chihuahua.jpg |[ A small brown dog wearing a sweater is sitting on the floor. The dog appears to be looking at something, possibly its owner or another animal in the room. It seems comfortable and relaxed]| |tractor.JPEG |[ A man is sitting in the driver&#39;s seat of a green tractor, which has yellow wheels and tires. The tractor appears to be parked on top of an empty field with] | |ox.JPEG |[ A large bull with horns is standing in a grassy field.] | +--+-+ &lt;/div&gt; BGEEmbeddings Sentence embeddings using BGE. BGE, or BAAI General Embeddings, a model that can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. Note that this annotator is only supported for Spark Versions 3.4 and up. Pretrained models can be loaded with pretrained of the companion object: val embeddings = BGEEmbeddings.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;bge_base&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see BGEEmbeddingsTestSpec. Sources : C-Pack: Packaged Resources To Advance General Chinese Embedding BGE Github Repository Paper abstract We introduce C-Pack, a package of resources that significantly advance the field of general Chinese embeddings. C-Pack includes three critical resources. 1) C-MTEB is a comprehensive benchmark for Chinese text embeddings covering 6 tasks and 35 datasets. 2) C-MTP is a massive text embedding dataset curated from labeled and unlabeled Chinese corpora for training embedding models. 3) C-TEM is a family of embedding models covering multiple sizes. Our models outperform all prior Chinese text embeddings on C-MTEB by up to +10% upon the time of the release. We also integrate and optimize the entire suite of training methods for C-TEM. Along with our resources on general Chinese embedding, we release our data and models for English text embeddings. The English models achieve stateof-the-art performance on the MTEB benchmark; meanwhile, our released English data is 2 times larger than the Chinese data. All these resources are made publicly available at https://github.com/FlagOpen/FlagEmbedding. Input Annotator Types: DOCUMENT Output Annotator Type: SENTENCE_EMBEDDINGS Python API: BGEEmbeddings Scala API: BGEEmbeddings Source: BGEEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) embeddings = BGEEmbeddings.pretrained() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;bge_embeddings&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;bge_embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) pipeline = Pipeline().setStages([ documentAssembler, embeddings, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;query: how much protein should a female eat&quot;, &quot;passage: As a general guideline, the CDC&#39;s average requirement of protein for women ages 19 to 70 is 46 grams per day.&quot; + &quot;But, as you can see from this chart, you&#39;ll need to increase that if you&#39;re expecting or training for a&quot; + &quot;marathon. Check out the chart below to see how much protein you should be eating each day.&quot;, ]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80) +--+ | result| +--+ |[[8.0190285E-4, -0.005974853, -0.072875895, 0.007944068, 0.026059335, -0.0080...| |[[0.050514214, 0.010061974, -0.04340176, -0.020937217, 0.05170225, 0.01157857...| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.embeddings.BGEEmbeddings import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = BGEEmbeddings.pretrained(&quot;bge_base&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;) .setOutputCol(&quot;bge_embeddings&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;bge_embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, embeddings, embeddingsFinisher )) val data = Seq(&quot;query: how much protein should a female eat&quot;, &quot;passage: As a general guideline, the CDC&#39;s average requirement of protein for women ages 19 to 70 is 46 grams per day.&quot; + But, as you can see from this chart, you&#39;ll need to increase that if you&#39;re expecting or training for a&quot; + marathon. Check out the chart below to see how much protein you should be eating each day.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[[8.0190285E-4, -0.005974853, -0.072875895, 0.007944068, 0.026059335, -0.0080...| |[[0.050514214, 0.010061974, -0.04340176, -0.020937217, 0.05170225, 0.01157857...| +--+ BigTextMatcher ApproachModel Annotator to match exact phrases (by token) provided in a file against a Document. A text file of predefined phrases must be provided with setStoragePath. In contrast to the normal TextMatcher, the BigTextMatcher is designed for large corpora. For extended examples of usage, see the BigTextMatcherTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: BigTextMatcher Scala API: BigTextMatcher Source: BigTextMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the entities file is of the form # # ... # dolore magna aliqua # lorem ipsum dolor. sit # laborum # ... # # where each line represents an entity phrase to be extracted. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) data = spark.createDataFrame([[&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;]]).toDF(&quot;text&quot;) entityExtractor = BigTextMatcher() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setStoragePath(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(False) pipeline = Pipeline().setStages([documentAssembler, tokenizer, entityExtractor]) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity)&quot;).show(truncate=False) +--+ |col | +--+ |[chunk, 6, 24, dolore magna aliqua, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 53, 59, laborum, [sentence -&gt; 0, chunk -&gt; 1], []] | +--+ // In this example, the entities file is of the form // // ... // dolore magna aliqua // lorem ipsum dolor. sit // laborum // ... // // where each line represents an entity phrase to be extracted. import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.BigTextMatcher import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val data = Seq(&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;).toDF(&quot;text&quot;) val entityExtractor = new BigTextMatcher() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setStoragePath(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(false) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer, entityExtractor)) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity)&quot;).show(false) +--+ |col | +--+ |[chunk, 6, 24, dolore magna aliqua, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 53, 59, laborum, [sentence -&gt; 0, chunk -&gt; 1], []] | +--+ Instantiated model of the BigTextMatcher. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: BigTextMatcherModel Scala API: BigTextMatcherModel Source: BigTextMatcherModel Chunk2Doc Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result. Input Annotator Types: CHUNK Output Annotator Type: DOCUMENT Python API: Chunk2Doc Scala API: Chunk2Doc Source: Chunk2Doc Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline # Location entities are extracted and converted back into `DOCUMENT` type for further processing data = spark.createDataFrame([[1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;]]).toDF(&quot;id&quot;, &quot;text&quot;) # Extracts Named Entities amongst other things pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) chunkToDoc = Chunk2Doc().setInputCols(&quot;entities&quot;).setOutputCol(&quot;chunkConverted&quot;) explainResult = pipeline.transform(data) result = chunkToDoc.transform(explainResult) result.selectExpr(&quot;explode(chunkConverted)&quot;).show(truncate=False) ++ |col | ++ |[document, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []] | |[document, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]| ++ // Location entities are extracted and converted back into `DOCUMENT` type for further processing import spark.implicits._ import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.Chunk2Doc val data = Seq((1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;)).toDF(&quot;id&quot;, &quot;text&quot;) // Extracts Named Entities amongst other things val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) val chunkToDoc = new Chunk2Doc().setInputCols(&quot;entities&quot;).setOutputCol(&quot;chunkConverted&quot;) val explainResult = pipeline.transform(data) val result = chunkToDoc.transform(explainResult) result.selectExpr(&quot;explode(chunkConverted)&quot;).show(false) ++ |col | ++ |[document, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []] | |[document, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]| ++ ChunkEmbeddings This annotator utilizes WordEmbeddings, BertEmbeddings etc. to generate chunk embeddings from either Chunker, NGramGenerator, or NerConverter outputs. For extended examples of usage, see the Examples and the ChunkEmbeddingsTestSpec. Input Annotator Types: CHUNK, WORD_EMBEDDINGS Output Annotator Type: WORD_EMBEDDINGS Python API: ChunkEmbeddings Scala API: ChunkEmbeddings Source: ChunkEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Extract the Embeddings from the NGrams documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) nGrams = NGramGenerator() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;chunk&quot;) .setN(2) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) # Convert the NGram chunks into Word Embeddings chunkEmbeddings = ChunkEmbeddings() .setInputCols([&quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;chunk_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) pipeline = Pipeline() .setStages([ documentAssembler, sentence, tokenizer, nGrams, embeddings, chunkEmbeddings ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk_embeddings) as result&quot;) .select(&quot;result.annotatorType&quot;, &quot;result.result&quot;, &quot;result.embeddings&quot;) .show(5, 80) ++-+--+ | annotatorType| result| embeddings| ++-+--+ |word_embeddings| This is|[-0.55661, 0.42829502, 0.86661, -0.409785, 0.06316501, 0.120775, -0.0732005, ...| |word_embeddings| is a|[-0.40674996, 0.22938299, 0.50597, -0.288195, 0.555655, 0.465145, 0.140118, 0...| |word_embeddings|a sentence|[0.17417, 0.095253006, -0.0530925, -0.218465, 0.714395, 0.79860497, 0.0129999...| |word_embeddings|sentence .|[0.139705, 0.177955, 0.1887775, -0.45545, 0.20030999, 0.461557, -0.07891501, ...| ++-+--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.{NGramGenerator, Tokenizer} import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings import org.apache.spark.ml.Pipeline // Extract the Embeddings from the NGrams val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val nGrams = new NGramGenerator() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;chunk&quot;) .setN(2) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) // Convert the NGram chunks into Word Embeddings val chunkEmbeddings = new ChunkEmbeddings() .setInputCols(&quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;chunk_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentence, tokenizer, nGrams, embeddings, chunkEmbeddings )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk_embeddings) as result&quot;) .select(&quot;result.annotatorType&quot;, &quot;result.result&quot;, &quot;result.embeddings&quot;) .show(5, 80) ++-+--+ | annotatorType| result| embeddings| ++-+--+ |word_embeddings| This is|[-0.55661, 0.42829502, 0.86661, -0.409785, 0.06316501, 0.120775, -0.0732005, ...| |word_embeddings| is a|[-0.40674996, 0.22938299, 0.50597, -0.288195, 0.555655, 0.465145, 0.140118, 0...| |word_embeddings|a sentence|[0.17417, 0.095253006, -0.0530925, -0.218465, 0.714395, 0.79860497, 0.0129999...| |word_embeddings|sentence .|[0.139705, 0.177955, 0.1887775, -0.45545, 0.20030999, 0.461557, -0.07891501, ...| ++-+--+ ChunkTokenizer ApproachModel Tokenizes and flattens extracted NER chunks. The ChunkTokenizer will split the extracted NER CHUNK type Annotations and will create TOKEN type Annotations. The result is then flattened, resulting in a single array. For extended examples of usage, see the ChunkTokenizerTestSpec. Input Annotator Types: CHUNK Output Annotator Type: TOKEN Python API: ChunkTokenizer Scala API: ChunkTokenizer Source: ChunkTokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) entityExtractor = TextMatcher() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setEntities(&quot;src/test/resources/entity-extractor/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) chunkTokenizer = ChunkTokenizer() .setInputCols([&quot;entity&quot;]) .setOutputCol(&quot;chunk_token&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, entityExtractor, chunkTokenizer ]) data = spark.createDataFrame([[ &quot;Hello world, my name is Michael, I am an artist and I work at Benezar&quot;, &quot;Robert, an engineer from Farendell, graduated last year. The other one, Lucas, graduated last week.&quot; ]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;entity.result as entity&quot; , &quot;chunk_token.result as chunk_token&quot;).show(truncate=False) +--++ |entity |chunk_token | +--++ |[world, Michael, work at Benezar] |[world, Michael, work, at, Benezar] | |[engineer from Farendell, last year, last week]|[engineer, from, Farendell, last, year, last, week]| +--++ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotators.{ChunkTokenizer, TextMatcher, Tokenizer} import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;src/test/resources/entity-extractor/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) val chunkTokenizer = new ChunkTokenizer() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;chunk_token&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, entityExtractor, chunkTokenizer )) val data = Seq( &quot;Hello world, my name is Michael, I am an artist and I work at Benezar&quot;, &quot;Robert, an engineer from Farendell, graduated last year. The other one, Lucas, graduated last week.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;entity.result as entity&quot; , &quot;chunk_token.result as chunk_token&quot;).show(false) +--++ |entity |chunk_token | +--++ |[world, Michael, work at Benezar] |[world, Michael, work, at, Benezar] | |[engineer from Farendell, last year, last week]|[engineer, from, Farendell, last, year, last, week]| +--++ Instantiated model of the ChunkTokenizer. For usage and examples see the documentation of the main class. Input Annotator Types: CHUNK Output Annotator Type: TOKEN Python API: ChunkTokenizerModel Scala API: ChunkTokenizerModel Source: ChunkTokenizerModel Chunker This annotator matches a pattern of part-of-speech tags in order to return meaningful phrases from document. Extracted part-of-speech tags are mapped onto the sentence, which can then be parsed by regular expressions. The part-of-speech tags are wrapped by angle brackets &lt;&gt; to be easily distinguishable in the text itself. This example sentence will result in the form: &quot;Peter Pipers employees are picking pecks of pickled peppers.&quot; &quot;&lt;NNP&gt;&lt;NNP&gt;&lt;NNS&gt;&lt;VBP&gt;&lt;VBG&gt;&lt;NNS&gt;&lt;IN&gt;&lt;JJ&gt;&lt;NNS&gt;&lt;.&gt;&quot; To then extract these tags, regexParsers need to be set with e.g.: val chunker = new Chunker() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;)) When defining the regular expressions, tags enclosed in angle brackets are treated as groups, so here specifically &quot;&lt;NNP&gt;+&quot; means 1 or more nouns in succession. Additional patterns can also be set with addRegexParsers. For more extended examples see the Examples) and the ChunkerTestSpec. Input Annotator Types: DOCUMENT, POS Output Annotator Type: CHUNK Python API: Chunker Scala API: Chunker Source: Chunker Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) POSTag = PerceptronModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) chunker = Chunker() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;]) pipeline = Pipeline() .setStages([ documentAssembler, sentence, tokenizer, POSTag, chunker ]) data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(truncate=False) +-+ |result | +-+ |[chunk, 0, 11, Peter Pipers, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 13, 21, employees, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 35, 39, pecks, [sentence -&gt; 0, chunk -&gt; 2], []] | |[chunk, 52, 58, peppers, [sentence -&gt; 0, chunk -&gt; 3], []] | +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotators.{Chunker, Tokenizer} import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val POSTag = PerceptronModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val chunker = new Chunker() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;)) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentence, tokenizer, POSTag, chunker )) val data = Seq(&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(false) +-+ |result | +-+ |[chunk, 0, 11, Peter Pipers, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 13, 21, employees, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 35, 39, pecks, [sentence -&gt; 0, chunk -&gt; 2], []] | |[chunk, 52, 58, peppers, [sentence -&gt; 0, chunk -&gt; 3], []] | +-+ ClassifierDL ApproachModel Trains a ClassifierDL for generic Multi-class Text Classification. ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 100 classes. For instantiated/pretrained models, see ClassifierDLModel. Setting a test dataset to monitor model metrics can be done with .setTestDataset. The method expects a path to a parquet file containing a dataframe that has the same required columns as the training dataframe. The pre-processing steps for the training dataframe should also be applied to the test dataframe. The following example will show how to create the test dataset: val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val preProcessingPipeline = new Pipeline().setStages(Array(documentAssembler, embeddings)) val Array(train, test) = data.randomSplit(Array(0.8, 0.2)) preProcessingPipeline .fit(test) .transform(test) .write .mode(&quot;overwrite&quot;) .parquet(&quot;test_data&quot;) val classifier = new ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setTestDataset(&quot;test_data&quot;) For extended examples of usage, see the Examples [1] [2] and the ClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Note: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. UniversalSentenceEncoder, BertSentenceEmbeddings, or SentenceEmbeddings can be used for the inputCol Python API: ClassifierDLApproach Scala API: ClassifierDLApproach Source: ClassifierDLApproach Show Example PythonScala # In this example, the training data `&quot;sentiment.csv&quot;` has the form of # # text,label # This movie is the best movie I have wached ever! In my opinion this movie can win an award.,0 # This was a terrible movie! The acting was bad really bad!,1 # ... # # Then traning can be done like so: import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline smallCorpus = spark.read.option(&quot;header&quot;,&quot;True&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) docClassifier = ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(5e-3) .setDropout(0.5) pipeline = Pipeline() .setStages( [ documentAssembler, useEmbeddings, docClassifier ] ) pipelineModel = pipeline.fit(smallCorpus) // In this example, the training data `&quot;sentiment.csv&quot;` has the form of // // text,label // This movie is the best movie I have wached ever! In my opinion this movie can win an award.,0 // This was a terrible movie! The acting was bad really bad!,1 // ... // // Then traning can be done like so: import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach import org.apache.spark.ml.Pipeline val smallCorpus = spark.read.option(&quot;header&quot;,&quot;true&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val docClassifier = new ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(5e-3f) .setDropout(0.5f) val pipeline = new Pipeline() .setStages( Array( documentAssembler, useEmbeddings, docClassifier ) ) val pipelineModel = pipeline.fit(smallCorpus) ClassifierDL for generic Multi-class Text Classification. ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 100 classes. This is the instantiated model of the ClassifierDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val classifierDL = ClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;classification&quot;) The default model is &quot;classifierdl_use_trec6&quot;, if no name is provided. It uses embeddings from the UniversalSentenceEncoder and is trained on the TREC-6 dataset. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the ClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: ClassifierDLModel Scala API: ClassifierDLModel Source: ClassifierDLModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) sarcasmDL = ClassifierDLModel.pretrained(&quot;classifierdl_use_sarcasm&quot;) .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sarcasm&quot;) pipeline = Pipeline() .setStages([ documentAssembler, sentence, useEmbeddings, sarcasmDL ]) data = spark.createDataFrame([ [&quot;I&#39;m ready!&quot;], [&quot;If I could put into words how much I love waking up at 6 am on Mondays I would.&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(arrays_zip(sentence, sarcasm)) as out&quot;) .selectExpr(&quot;out.sentence.result as sentence&quot;, &quot;out.sarcasm.result as sarcasm&quot;) .show(truncate=False) +-+-+ |sentence |sarcasm| +-+-+ |I&#39;m ready! |normal | |If I could put into words how much I love waking up at 6 am on Mondays I would.|sarcasm| +-+-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLModel import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val sarcasmDL = ClassifierDLModel.pretrained(&quot;classifierdl_use_sarcasm&quot;) .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sarcasm&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentence, useEmbeddings, sarcasmDL )) val data = Seq( &quot;I&#39;m ready!&quot;, &quot;If I could put into words how much I love waking up at 6 am on Mondays I would.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(arrays_zip(sentence, sarcasm)) as out&quot;) .selectExpr(&quot;out.sentence.result as sentence&quot;, &quot;out.sarcasm.result as sarcasm&quot;) .show(false) +-+-+ |sentence |sarcasm| +-+-+ |I&#39;m ready! |normal | |If I could put into words how much I love waking up at 6 am on Mondays I would.|sarcasm| +-+-+ ContextSpellChecker ApproachModel Trains a deep-learning based Noisy Channel Model Spell Algorithm. Correction candidates are extracted combining context information and word information. For instantiated/pretrained models, see ContextSpellCheckerModel. Spell Checking is a sequence to sequence mapping problem. Given an input sequence, potentially containing a certain number of errors, ContextSpellChecker will rank correction sequences according to three things: Different correction candidates for each word — word level. The surrounding text of each word, i.e. it’s context — sentence level. The relative cost of different correction candidates according to the edit operations at the character level it requires — subword level. For an in-depth explanation of the module see the article Applying Context Aware Spell Checking in Spark NLP. For extended examples of usage, see the article Training a Contextual Spell Checker for Italian Language, the Examples and the ContextSpellCheckerTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: ContextSpellCheckerApproach Scala API: ContextSpellCheckerApproach Source: ContextSpellCheckerApproach Show Example PythonScala # For this example, we use the first Sherlock Holmes book as the training dataset. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) spellChecker = ContextSpellCheckerApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;corrected&quot;) .setWordMaxDistance(3) .setBatchSize(24) .setEpochs(8) .setLanguageModelClasses(1650) # dependant on vocabulary size # .addVocabClass(&quot;_NAME_&quot;, names) # Extra classes for correction could be added like this pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker ]) path = &quot;sherlockholmes.txt&quot; dataset = spark.read.text(path) .toDF(&quot;text&quot;) pipelineModel = pipeline.fit(dataset) // For this example, we use the first Sherlock Holmes book as the training dataset. import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val spellChecker = new ContextSpellCheckerApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;corrected&quot;) .setWordMaxDistance(3) .setBatchSize(24) .setEpochs(8) .setLanguageModelClasses(1650) // dependant on vocabulary size // .addVocabClass(&quot;_NAME_&quot;, names) // Extra classes for correction could be added like this val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker )) val path = &quot;src/test/resources/spell/sherlockholmes.txt&quot; val dataset = spark.sparkContext.textFile(path) .toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(dataset) Implements a deep-learning based Noisy Channel Model Spell Algorithm. Correction candidates are extracted combining context information and word information. Spell Checking is a sequence to sequence mapping problem. Given an input sequence, potentially containing a certain number of errors, ContextSpellChecker will rank correction sequences according to three things: Different correction candidates for each word — word level. The surrounding text of each word, i.e. it’s context — sentence level. The relative cost of different correction candidates according to the edit operations at the character level it requires — subword level. For an in-depth explanation of the module see the article Applying Context Aware Spell Checking in Spark NLP. This is the instantiated model of the ContextSpellCheckerApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val spellChecker = ContextSpellCheckerModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;checked&quot;) The default model is &quot;spellcheck_dl&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the ContextSpellCheckerTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: ContextSpellCheckerModel Scala API: ContextSpellCheckerModel Source: ContextSpellCheckerModel Date2Chunk Converts DATE type Annotations to CHUNK type. This can be useful if the following annotators after DateMatcher and MultiDateMatcher require CHUNK types. The entity name in the metadata can be changed with setEntityName. Input Annotator Types: DATE Output Annotator Type: CHUNK Python API: Date2Chunk Scala API: Date2Chunk Source: Date2Chunk Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) date = DateMatcher() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;date&quot;) date2Chunk = Date2Chunk() .setInputCols([&quot;date&quot;]) .setOutputCol(&quot;date_chunk&quot;) pipeline = Pipeline().setStages([ documentAssembler, date, date2Chunk ]) data = spark.createDataFrame([[&quot;Omicron is a new variant of COVID-19, which the World Health Organization designated a variant of concern on Nov. 26, 2021/26/11.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;date_chunk&quot;).show(1, truncate=False) +-+ |date_chunk | +-+ |[{chunk, 118, 121, 2021/01/01, {sentence -&gt; 0}, []}]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val inputFormats = Array(&quot;yyyy&quot;, &quot;yyyy/dd/MM&quot;, &quot;MM/yyyy&quot;, &quot;yyyy&quot;) val outputFormat = &quot;yyyy/MM/dd&quot; val date = new DateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) val date2Chunk = new Date2Chunk() .setInputCols(&quot;date&quot;) .setOutputCol(&quot;date_chunk&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, date, date2Chunk )) val data = Seq( &quot;&quot;&quot;Omicron is a new variant of COVID-19, which the World Health Organization designated a variant of concern on Nov. 26, 2021/26/11.&quot;&quot;&quot;, &quot;&quot;&quot;Neighbouring Austria has already locked down its population this week for at until 2021/10/12, becoming the first to reimpose such restrictions.&quot;&quot;&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.transform(data).select(&quot;date_chunk&quot;).show(false) -+ date_chunk | -+ [{chunk, 118, 121, 2021/01/01, {sentence -&gt; 0}, []}]| [{chunk, 83, 86, 2021/01/01, {sentence -&gt; 0}, []}] | -+ DateMatcher Matches standard date formats into a provided format. Reads from different forms of date and time expressions and converts them to a provided date format. Extracts only one date per document. Use with sentence detector to find matches in each sentence. To extract multiple dates from a document, please use the MultiDateMatcher. Reads the following kind of dates: &quot;1978-01-28&quot;, &quot;1984/04/02,1/02/1980&quot;, &quot;2/28/79&quot;, &quot;The 31st of April in the year 2008&quot;, &quot;Fri, 21 Nov 1997&quot;, &quot;Jan 21, ‘97&quot;, &quot;Sun&quot;, &quot;Nov 21&quot;, &quot;jan 1st&quot;, &quot;next thursday&quot;, &quot;last wednesday&quot;, &quot;today&quot;, &quot;tomorrow&quot;, &quot;yesterday&quot;, &quot;next week&quot;, &quot;next month&quot;, &quot;next year&quot;, &quot;day after&quot;, &quot;the day before&quot;, &quot;0600h&quot;, &quot;06:00 hours&quot;, &quot;6pm&quot;, &quot;5:30 a.m.&quot;, &quot;at 5&quot;, &quot;12:59&quot;, &quot;23:59&quot;, &quot;1988/11/23 6pm&quot;, &quot;next week at 7.30&quot;, &quot;5 am tomorrow&quot; For example &quot;The 31st of April in the year 2008&quot; will be converted into 2008/04/31. Pretrained pipelines are available for this module, see Pipelines. For extended examples of usage, see the Examples and the DateMatcherTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DATE Python API: DateMatcher Scala API: DateMatcher Source: DateMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) date = DateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) pipeline = Pipeline().setStages([ documentAssembler, date ]) data = spark.createDataFrame([[&quot;Fri, 21 Nov 1997&quot;], [&quot;next week at 7.30&quot;], [&quot;see you a day after&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;date&quot;).show(truncate=False) +-+ |date | +-+ |[[date, 5, 15, 1997/11/21, [sentence -&gt; 0], []]] | |[[date, 0, 8, 2020/01/18, [sentence -&gt; 0], []]] | |[[date, 10, 18, 2020/01/12, [sentence -&gt; 0], []]]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.DateMatcher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val date = new DateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, date )) val data = Seq(&quot;Fri, 21 Nov 1997&quot;, &quot;next week at 7.30&quot;, &quot;see you a day after&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;date&quot;).show(false) +-+ |date | +-+ |[[date, 5, 15, 1997/11/21, [sentence -&gt; 0], []]] | |[[date, 0, 8, 2020/01/18, [sentence -&gt; 0], []]] | |[[date, 10, 18, 2020/01/12, [sentence -&gt; 0], []]]| +-+ DependencyParser ApproachModel Trains an unlabeled parser that finds a grammatical relations between two words in a sentence. For instantiated/pretrained models, see DependencyParserModel. Dependency parser provides information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The required training data can be set in two different ways (only one can be chosen for a particular model): Dependency treebank in the Penn Treebank format set with setDependencyTreeBank Dataset in the CoNLL-U format set with setConllU Apart from that, no additional training data is needed. See DependencyParserApproachTestSpec for further reference on how to use this API. Input Annotator Types: DOCUMENT, POS, TOKEN Output Annotator Type: DEPENDENCY Python API: DependencyParserApproach Scala API: DependencyParserApproach Source: DependencyParserApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) dependencyParserApproach = DependencyParserApproach() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) .setDependencyTreeBank(&quot;src/test/resources/parser/unlabeled/dependency_treebank&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, posTagger, dependencyParserApproach ]) # Additional training data is not needed, the dependency parser relies on the dependency tree bank / CoNLL-U only. emptyDataSet = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(emptyDataSet) import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val dependencyParserApproach = new DependencyParserApproach() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) .setDependencyTreeBank(&quot;src/test/resources/parser/unlabeled/dependency_treebank&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, posTagger, dependencyParserApproach )) // Additional training data is not needed, the dependency parser relies on the dependency tree bank / CoNLL-U only. val emptyDataSet = Seq.empty[String].toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(emptyDataSet) Unlabeled parser that finds a grammatical relation between two words in a sentence. Dependency parser provides information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. This is the instantiated model of the DependencyParserApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val dependencyParserApproach = DependencyParserModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) The default model is &quot;dependency_conllu&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the DependencyParserApproachTestSpec. Input Annotator Types: [String]DOCUMENT, POS, TOKEN Output Annotator Type: DEPENDENCY Python API: DependencyParserModel Scala API: DependencyParserModel Source: DependencyParserModel Doc2Chunk Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol. Chunk text must be contained within input DOCUMENT. May be either StringType or ArrayType[StringType] (using setIsArray). Useful for annotators that require a CHUNK type input. Input Annotator Types: DOCUMENT Output Annotator Type: CHUNK Python API: Doc2Chunk Scala API: Doc2Chunk Source: Doc2Chunk Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) chunkAssembler = Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) .setIsArray(True) data = spark.createDataFrame([[ &quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;, [&quot;Spark NLP&quot;, &quot;text processing library&quot;, &quot;natural language processing&quot;] ]]).toDF(&quot;text&quot;, &quot;target&quot;) pipeline = Pipeline().setStages([documentAssembler, chunkAssembler]).fit(data) result = pipeline.transform(data) result.selectExpr(&quot;chunk.result&quot;, &quot;chunk.annotatorType&quot;).show(truncate=False) +--++ |result |annotatorType | +--++ |[Spark NLP, text processing library, natural language processing]|[chunk, chunk, chunk]| +--++ import spark.implicits._ import com.johnsnowlabs.nlp.{Doc2Chunk, DocumentAssembler} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val chunkAssembler = new Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) .setIsArray(true) val data = Seq( (&quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;, Seq(&quot;Spark NLP&quot;, &quot;text processing library&quot;, &quot;natural language processing&quot;)) ).toDF(&quot;text&quot;, &quot;target&quot;) val pipeline = new Pipeline().setStages(Array(documentAssembler, chunkAssembler)).fit(data) val result = pipeline.transform(data) result.selectExpr(&quot;chunk.result&quot;, &quot;chunk.annotatorType&quot;).show(false) +--++ |result |annotatorType | +--++ |[Spark NLP, text processing library, natural language processing]|[chunk, chunk, chunk]| +--++ Doc2Vec ApproachModel Trains a Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. For instantiated/pretrained models, see Doc2VecModel. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: SENTENCE_EMBEDDINGS Python API: Doc2VecApproach Scala API: Doc2VecApproach Source: Doc2VecApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Doc2VecApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings ]) path = &quot;sherlockholmes.txt&quot; dataset = spark.read.text(path).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(dataset) import spark.implicits._ import com.johnsnowlabs.nlp.annotator.{Tokenizer, Doc2VecApproach} import com.johnsnowlabs.nlp.base.DocumentAssembler import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = new Doc2VecApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings )) val path = &quot;src/test/resources/spell/sherlockholmes.txt&quot; val dataset = spark.sparkContext.textFile(path) .toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(dataset) Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. This is the instantiated model of the Doc2VecApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val embeddings = Doc2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;doc2vec_gigaword_300&quot;, if no name is provided. For available pretrained models please see the Models Hub. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: SENTENCE_EMBEDDINGS Python API: Doc2VecModel Scala API: Doc2VecModel Source: Doc2VecModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Doc2VecModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, embeddings, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Tokenizer, Doc2VecModel} import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = Doc2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsFinisher )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ DocumentAssembler Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. The DocumentAssembler reads String columns. Additionally, setCleanupMode can be used to pre-process the text (Default: disabled). For possible options please refer the parameters section. For more extended examples on document pre-processing see the Examples. Input Annotator Types: NONE Output Annotator Type: DOCUMENT Python API: DocumentAssembler Scala API: DocumentAssembler Source: DocumentAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library.&quot;]]).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) result = documentAssembler.transform(data) result.select(&quot;document&quot;).show(truncate=False) +-+ |document | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document&quot;).printSchema root |-- document: array (nullable = True) | |-- element: struct (containsNull = True) | | |-- annotatorType: string (nullable = True) | | |-- begin: integer (nullable = False) | | |-- end: integer (nullable = False) | | |-- result: string (nullable = True) | | |-- metadata: map (nullable = True) | | | |-- key: string | | | |-- value: string (valueContainsNull = True) | | |-- embeddings: array (nullable = True) | | | |-- element: float (containsNull = False) import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler val data = Seq(&quot;Spark NLP is an open-source text processing library.&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val result = documentAssembler.transform(data) result.select(&quot;document&quot;).show(false) +-+ |document | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document&quot;).printSchema root |-- document: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- begin: integer (nullable = false) | | |-- end: integer (nullable = false) | | |-- result: string (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | | |-- embeddings: array (nullable = true) | | | |-- element: float (containsNull = false) DocumentCharacterTextSplitter Annotator which splits large documents into chunks of roughly given size. DocumentCharacterTextSplitter takes a list of separators. It takes the separators in order and splits subtexts if they are over the chunk length, considering optional overlap of the chunks. For example, given chunk size 20 and overlap 5: &quot;He was, I take it, the most perfect reasoning and observing machine that the world has seen.&quot; [&quot;He was, I take it,&quot;, &quot;it, the most&quot;, &quot;most perfect&quot;, &quot;reasoning and&quot;, &quot;and observing&quot;, &quot;machine that the&quot;, &quot;the world has seen.&quot;] Additionally, you can set custom patterns with setSplitPatterns whether patterns should be interpreted as regex with setPatternsAreRegex whether to keep the separators with setKeepSeparators whether to trim whitespaces with setTrimWhitespace whether to explode the splits to individual rows with setExplodeSplits For extended examples of usage, see the DocumentCharacterTextSplitterTest. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: DocumentCharacterTextSplitter Scala API: DocumentCharacterTextSplitter Source: DocumentCharacterTextSplitter Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline textDF = spark.read.text( &quot;sherlockholmes.txt&quot;, wholetext=True ).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;) textSplitter = DocumentCharacterTextSplitter() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;splits&quot;) .setChunkSize(20000) .setChunkOverlap(200) .setExplodeSplits(True) pipeline = Pipeline().setStages([documentAssembler, textSplitter]) result = pipeline.fit(textDF).transform(textDF) result.selectExpr( &quot;splits.result&quot;, &quot;splits[0].begin&quot;, &quot;splits[0].end&quot;, &quot;splits[0].end - splits[0].begin as length&quot;) .show(8, truncate = 80) +--++-++ | result|splits[0].begin|splits[0].end|length| +--++-++ |[ Project Gutenberg&#39;s The Adventures of Sherlock Holmes, by Arthur Conan Doyl...| 0| 19994| 19994| |[&quot;And Mademoiselle&#39;s address?&quot; he asked. n n&quot;Is Briony Lodge, Serpentine Aven...| 19798| 39395| 19597| |[&quot;How did that help you?&quot; n n&quot;It was all-important. When a woman thinks that ...| 39371| 59242| 19871| |[&quot;&#39;But,&#39; said I, &#39;there would be millions of red-headed men who nwould apply....| 59166| 77833| 18667| |[My friend was an enthusiastic musician, being himself not only a nvery capab...| 77835| 97769| 19934| |[&quot;And yet I am not convinced of it,&quot; I answered. &quot;The cases which ncome to li...| 97771| 117248| 19477| |[&quot;Well, she had a slate-coloured, broad-brimmed straw hat, with a nfeather of...| 117250| 137242| 19992| |[&quot;That sounds a little paradoxical.&quot; n n&quot;But it is profoundly True. Singulari...| 137244| 157171| 19927| +--++-++ import com.johnsnowlabs.nlp.annotator._ import com.johnsnowlabs.nlp.DocumentAssembler import org.apache.spark.ml.Pipeline val textDF = spark.read .option(&quot;wholetext&quot;, &quot;true&quot;) .text(&quot;src/test/resources/spell/sherlockholmes.txt&quot;) .toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;) val textSplitter = new DocumentCharacterTextSplitter() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;splits&quot;) .setChunkSize(20000) .setChunkOverlap(200) .setExplodeSplits(true) val pipeline = new Pipeline().setStages(Array(documentAssembler, textSplitter)) val result = pipeline.fit(textDF).transform(textDF) result .selectExpr( &quot;splits.result&quot;, &quot;splits[0].begin&quot;, &quot;splits[0].end&quot;, &quot;splits[0].end - splits[0].begin as length&quot;) .show(8, truncate = 80) +--++-++ | result|splits[0].begin|splits[0].end|length| +--++-++ |[ Project Gutenberg&#39;s The Adventures of Sherlock Holmes, by Arthur Conan Doyl...| 0| 19994| 19994| |[&quot;And Mademoiselle&#39;s address?&quot; he asked. n n&quot;Is Briony Lodge, Serpentine Aven...| 19798| 39395| 19597| |[&quot;How did that help you?&quot; n n&quot;It was all-important. When a woman thinks that ...| 39371| 59242| 19871| |[&quot;&#39;But,&#39; said I, &#39;there would be millions of red-headed men who nwould apply....| 59166| 77833| 18667| |[My friend was an enthusiastic musician, being himself not only a nvery capab...| 77835| 97769| 19934| |[&quot;And yet I am not convinced of it,&quot; I answered. &quot;The cases which ncome to li...| 97771| 117248| 19477| |[&quot;Well, she had a slate-coloured, broad-brimmed straw hat, with a nfeather of...| 117250| 137242| 19992| |[&quot;That sounds a little paradoxical.&quot; n n&quot;But it is profoundly true. Singulari...| 137244| 157171| 19927| +--++-++ DocumentNormalizer Annotator which normalizes raw text from tagged text, e.g. scraped web pages or xml documents, from document type columns into Sentence. Removes all dirty characters from text following one or more input regex patterns. Can apply not wanted character removal with a specific policy. Can apply lower case normalization. For extended examples of usage, see the Examples. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: DocumentNormalizer Scala API: DocumentNormalizer Source: DocumentNormalizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) cleanUpPatterns = [&quot;&lt;[^&gt;]&gt;&quot;] documentNormalizer = DocumentNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;normalizedDocument&quot;) .setAction(&quot;clean&quot;) .setPatterns(cleanUpPatterns) .setReplacement(&quot; &quot;) .setPolicy(&quot;pretty_all&quot;) .setLowercase(True) pipeline = Pipeline().setStages([ documentAssembler, documentNormalizer ]) text = &quot;&quot;&quot; &lt;div id=&quot;theworldsgreatest&quot; class=&#39;my-right my-hide-small my-wide toptext&#39; style=&quot;font-family:&#39;Segoe UI&#39;,Arial,sans-serif&quot;&gt; THE WORLD&#39;S LARGEST WEB DEVELOPER SITE &lt;h1 style=&quot;font-size:300%;&quot;&gt;THE WORLD&#39;S LARGEST WEB DEVELOPER SITE&lt;/h1&gt; &lt;p style=&quot;font-size:160%;&quot;&gt;Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&quot;&quot;&quot; data = spark.createDataFrame([[text]]).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(data) result = pipelineModel.transform(data) result.selectExpr(&quot;normalizedDocument.result&quot;).show(truncate=False) +--+ |result | +--+ |[ the world&#39;s largest web developer site the world&#39;s largest web developer site lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum..]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.DocumentNormalizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val cleanUpPatterns = Array(&quot;&lt;[^&gt;]&gt;&quot;) val documentNormalizer = new DocumentNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;normalizedDocument&quot;) .setAction(&quot;clean&quot;) .setPatterns(cleanUpPatterns) .setReplacement(&quot; &quot;) .setPolicy(&quot;pretty_all&quot;) .setLowercase(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, documentNormalizer )) val text = &quot;&quot;&quot; &lt;div id=&quot;theworldsgreatest&quot; class=&#39;my-right my-hide-small my-wide toptext&#39; style=&quot;font-family:&#39;Segoe UI&#39;,Arial,sans-serif&quot;&gt; THE WORLD&#39;S LARGEST WEB DEVELOPER SITE &lt;h1 style=&quot;font-size:300%;&quot;&gt;THE WORLD&#39;S LARGEST WEB DEVELOPER SITE&lt;/h1&gt; &lt;p style=&quot;font-size:160%;&quot;&gt;Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&quot;&quot;&quot; val data = Seq(text).toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(data) val result = pipelineModel.transform(data) result.selectExpr(&quot;normalizedDocument.result&quot;).show(truncate=false) +--+ |result | +--+ |[ the world&#39;s largest web developer site the world&#39;s largest web developer site lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum..]| +--+ DocumentSimilarityRanker ApproachModel Annotator that uses LSH techniques present in Spark ML lib to execute approximate nearest neighbors search on top of sentence embeddings. It aims to capture the semantic meaning of a document in a dense, continuous vector space and return it to the ranker search. For instantiated/pretrained models, see DocumentSimilarityRankerModel. For extended examples of usage, see the jupyter notebook Document Similarity Ranker for Spark NLP. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: DOC_SIMILARITY_RANKINGS Python API: DocumentSimilarityRankerApproach Scala API: DocumentSimilarityRankerApproach Source: DocumentSimilarityRankerApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline from sparknlp.annotator.similarity.document_similarity_ranker import * document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence_embeddings = E5Embeddings.pretrained() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) document_similarity_ranker = DocumentSimilarityRankerApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;doc_similarity_rankings&quot;) .setSimilarityMethod(&quot;brp&quot;) .setNumberOfNeighbours(1) .setBucketLength(2.0) .setNumHashTables(3) .setVisibleDistances(True) .setIdentityRanking(False) document_similarity_ranker_finisher = DocumentSimilarityRankerFinisher() .setInputCols(&quot;doc_similarity_rankings&quot;) .setOutputCols( &quot;finished_doc_similarity_rankings_id&quot;, &quot;finished_doc_similarity_rankings_neighbors&quot;) .setExtractNearestNeighbor(True) pipeline = Pipeline(stages=[ document_assembler, sentence_embeddings, document_similarity_ranker, document_similarity_ranker_finisher ]) docSimRankerPipeline = pipeline.fit(data).transform(data) ( docSimRankerPipeline .select( &quot;finished_doc_similarity_rankings_id&quot;, &quot;finished_doc_similarity_rankings_neighbors&quot; ).show(10, False) ) +--++ |finished_doc_similarity_rankings_id|finished_doc_similarity_rankings_neighbors| +--++ |1510101612 |[(1634839239,0.12448559591306324)] | |1634839239 |[(1510101612,0.12448559591306324)] | |-612640902 |[(1274183715,0.1220122862046063)] | |1274183715 |[(-612640902,0.1220122862046063)] | |-1320876223 |[(1293373212,0.17848855164122393)] | |1293373212 |[(-1320876223,0.17848855164122393)] | |-1548374770 |[(-1719102856,0.23297156732534166)] | |-1719102856 |[(-1548374770,0.23297156732534166)] | +--++ import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import com.johnsnowlabs.nlp.annotators.similarity.DocumentSimilarityRankerApproach import com.johnsnowlabs.nlp.finisher.DocumentSimilarityRankerFinisher import org.apache.spark.ml.Pipeline import spark.implicits._ val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceEmbeddings = RoBertaSentenceEmbeddings .pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val documentSimilarityRanker = new DocumentSimilarityRankerApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;doc_similarity_rankings&quot;) .setSimilarityMethod(&quot;brp&quot;) .setNumberOfNeighbours(1) .setBucketLength(2.0) .setNumHashTables(3) .setVisibleDistances(true) .setIdentityRanking(false) val documentSimilarityRankerFinisher = new DocumentSimilarityRankerFinisher() .setInputCols(&quot;doc_similarity_rankings&quot;) .setOutputCols( &quot;finished_doc_similarity_rankings_id&quot;, &quot;finished_doc_similarity_rankings_neighbors&quot;) .setExtractNearestNeighbor(true) // Let&#39;s use a dataset where we can visually control similarity // Documents are coupled, as 1-2, 3-4, 5-6, 7-8 and they were create to be similar on purpose val data = Seq( &quot;First document, this is my first sentence. This is my second sentence.&quot;, &quot;Second document, this is my second sentence. This is my second sentence.&quot;, &quot;Third document, climate change is arguably one of the most pressing problems of our time.&quot;, &quot;Fourth document, climate change is definitely one of the most pressing problems of our time.&quot;, &quot;Fifth document, Florence in Italy, is among the most beautiful cities in Europe.&quot;, &quot;Sixth document, Florence in Italy, is a very beautiful city in Europe like Lyon in France.&quot;, &quot;Seventh document, the French Riviera is the Mediterranean coastline of the southeast corner of France.&quot;, &quot;Eighth document, the warmest place in France is the French Riviera coast in Southern France.&quot;) .toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages( Array( documentAssembler, sentenceEmbeddings, documentSimilarityRanker, documentSimilarityRankerFinisher)) val result = pipeline.fit(data).transform(data) result .select(&quot;finished_doc_similarity_rankings_id&quot;, &quot;finished_doc_similarity_rankings_neighbors&quot;) .show(10, truncate = false) +--++ |finished_doc_similarity_rankings_id|finished_doc_similarity_rankings_neighbors| +--++ |1510101612 |[(1634839239,0.12448559591306324)] | |1634839239 |[(1510101612,0.12448559591306324)] | |-612640902 |[(1274183715,0.1220122862046063)] | |1274183715 |[(-612640902,0.1220122862046063)] | |-1320876223 |[(1293373212,0.17848855164122393)] | |1293373212 |[(-1320876223,0.17848855164122393)] | |-1548374770 |[(-1719102856,0.23297156732534166)] | |-1719102856 |[(-1548374770,0.23297156732534166)] | +--++ Instantiated model of the DocumentSimilarityRankerApproach. For usage and examples see the documentation of the main class. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: DOC_SIMILARITY_RANKINGS Python API: DocumentSimilarityRankerModel Scala API: DocumentSimilarityRankerModel Source: DocumentSimilarityRankerModel DocumentTokenSplitter Annotator that splits large documents into smaller documents based on the number of tokens in the text. Currently, DocumentTokenSplitter splits the text by whitespaces to create the tokens. The number of these tokens will then be used as a measure of the text length. In the future, other tokenization techniques will be supported. For example, given 3 tokens and overlap 1: &quot;He was, I take it, the most perfect reasoning and observing machine that the world has seen.&quot; [&quot;He was, I&quot;, &quot;I take it,&quot;, &quot;it, the most&quot;, &quot;most perfect reasoning&quot;, &quot;reasoning and observing&quot;, &quot;observing machine that&quot;, &quot;that the world&quot;, &quot;world has seen.&quot;] Additionally, you can set whether to trim whitespaces with setTrimWhitespace whether to explode the splits to individual rows with setExplodeSplits For extended examples of usage, see the DocumentTokenSplitterTest. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: DocumentTokenSplitter Scala API: DocumentTokenSplitter Source: DocumentTokenSplitter Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline textDF = spark.read.text( &quot;sherlockholmes.txt&quot;, wholetext=True ).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;) textSplitter = DocumentTokenSplitter() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;splits&quot;) .setNumTokens(512) .setTokenOverlap(10) .setExplodeSplits(True) pipeline = Pipeline().setStages([documentAssembler, textSplitter]) result = pipeline.fit(textDF).transform(textDF) result.selectExpr( &quot;splits.result as result&quot;, &quot;splits[0].begin as begin&quot;, &quot;splits[0].end as end&quot;, &quot;splits[0].end - splits[0].begin as length&quot;, &quot;splits[0].metadata.numTokens as tokens&quot;) .show(8, truncate = 80) +--+--+--+++ | result|begin| end|length|tokens| +--+--+--+++ |[ Project Gutenberg&#39;s The Adventures of Sherlock Holmes, by Arthur Conan Doyl...| 0| 3018| 3018| 512| |[study of crime, and occupied his nimmense faculties and extraordinary powers...| 2950| 5707| 2757| 512| |[but as I have changed my clothes I can&#39;t imagine how you ndeduce it. As to M...| 5659| 8483| 2824| 512| |[quarters received. Be in your chamber then at that hour, and do nnot take it...| 8427|11241| 2814| 512| |[a pity nto miss it.&quot; n n&quot;But your client--&quot; n n&quot;Never mind him. I may want y...|11188|13970| 2782| 512| |[person who employs me wishes his agent to be unknown to nyou, and I may conf...|13918|16898| 2980| 512| |[letters back.&quot; n n&quot;Precisely so. But how--&quot; n n&quot;Was there a secret marriage?...|16836|19744| 2908| 512| |[seven hundred in nnotes,&quot; he said. n nHolmes scribbled a receipt upon a shee...|19683|22551| 2868| 512| +--+--+--+++ import com.johnsnowlabs.nlp.annotator._ import com.johnsnowlabs.nlp.DocumentAssembler import org.apache.spark.ml.Pipeline val textDF = spark.read .option(&quot;wholetext&quot;, &quot;true&quot;) .text(&quot;src/test/resources/spell/sherlockholmes.txt&quot;) .toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;) val textSplitter = new DocumentTokenSplitter() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;splits&quot;) .setNumTokens(512) .setTokenOverlap(10) .setExplodeSplits(true) val pipeline = new Pipeline().setStages(Array(documentAssembler, textSplitter)) val result = pipeline.fit(textDF).transform(textDF) result .selectExpr( &quot;splits.result as result&quot;, &quot;splits[0].begin as begin&quot;, &quot;splits[0].end as end&quot;, &quot;splits[0].end - splits[0].begin as length&quot;, &quot;splits[0].metadata.numTokens as tokens&quot;) .show(8, truncate = 80) +--+--+--+++ | result|begin| end|length|tokens| +--+--+--+++ |[ Project Gutenberg&#39;s The Adventures of Sherlock Holmes, by Arthur Conan Doyl...| 0| 3018| 3018| 512| |[study of crime, and occupied his nimmense faculties and extraordinary powers...| 2950| 5707| 2757| 512| |[but as I have changed my clothes I can&#39;t imagine how you ndeduce it. As to M...| 5659| 8483| 2824| 512| |[quarters received. Be in your chamber then at that hour, and do nnot take it...| 8427|11241| 2814| 512| |[a pity nto miss it.&quot; n n&quot;But your client--&quot; n n&quot;Never mind him. I may want y...|11188|13970| 2782| 512| |[person who employs me wishes his agent to be unknown to nyou, and I may conf...|13918|16898| 2980| 512| |[letters back.&quot; n n&quot;Precisely so. But how--&quot; n n&quot;Was there a secret marriage?...|16836|19744| 2908| 512| |[seven hundred in nnotes,&quot; he said. n nHolmes scribbled a receipt upon a shee...|19683|22551| 2868| 512| +--+--+--+++ EmbeddingsFinisher Extracts embeddings from Annotations into a more easily usable form. This is useful for example: WordEmbeddings, BertEmbeddings, SentenceEmbeddings and ChunkEmbeddings. By using EmbeddingsFinisher you can easily transform your embeddings into array of floats or vectors which are compatible with Spark ML functions such as LDA, K-mean, Random Forest classifier or any other functions that require featureCol. For more extended examples see the Examples. Input Annotator Types: EMBEDDINGS Output Annotator Type: NONE Python API: EmbeddingsFinisher Scala API: EmbeddingsFinisher Source: EmbeddingsFinisher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) stopwordsCleaner = StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) gloveEmbeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;cleanTokens&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) embeddingsFinisher = EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_sentence_embeddings&quot;) .setOutputAsVector(True) .setCleanAnnotations(False) data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library.&quot;]]) .toDF(&quot;text&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, normalizer, stopwordsCleaner, gloveEmbeddings, embeddingsFinisher ]).fit(data) result = pipeline.transform(data) resultWithSize = result.selectExpr(&quot;explode(finished_sentence_embeddings) as embeddings&quot;) resultWithSize.show(5, 80) +--+ | embeddings| +--+ |[0.1619900017976761,0.045552998781204224,-0.03229299932718277,-0.685609996318...| |[-0.42416998744010925,1.1378999948501587,-0.5717899799346924,-0.5078899860382...| |[0.08621499687433243,-0.15772999823093414,-0.06067200005054474,0.395359992980...| |[-0.4970499873161316,0.7164199948310852,0.40119001269340515,-0.05761000141501...| |[-0.08170200139284134,0.7159299850463867,-0.20677000284194946,0.0295659992843...| +--+ import spark.implicits._ import org.apache.spark.ml.Pipeline import com.johnsnowlabs.nlp.{DocumentAssembler, EmbeddingsFinisher} import com.johnsnowlabs.nlp.annotator.{Normalizer, StopWordsCleaner, Tokenizer, WordEmbeddingsModel} val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) val stopwordsCleaner = new StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val gloveEmbeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;cleanTokens&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_sentence_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) val data = Seq(&quot;Spark NLP is an open-source text processing library.&quot;) .toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, normalizer, stopwordsCleaner, gloveEmbeddings, embeddingsFinisher )).fit(data) val result = pipeline.transform(data) val resultWithSize = result.selectExpr(&quot;explode(finished_sentence_embeddings)&quot;) .map { row =&gt; val vector = row.getAs[org.apache.spark.ml.linalg.DenseVector](0) (vector.size, vector) }.toDF(&quot;size&quot;, &quot;vector&quot;) resultWithSize.show(5, 80) +-+--+ |size| vector| +-+--+ | 100|[0.1619900017976761,0.045552998781204224,-0.03229299932718277,-0.685609996318...| | 100|[-0.42416998744010925,1.1378999948501587,-0.5717899799346924,-0.5078899860382...| | 100|[0.08621499687433243,-0.15772999823093414,-0.06067200005054474,0.395359992980...| | 100|[-0.4970499873161316,0.7164199948310852,0.40119001269340515,-0.05761000141501...| | 100|[-0.08170200139284134,0.7159299850463867,-0.20677000284194946,0.0295659992843...| +-+--+ EntityRuler ApproachModel Fits an Annotator to match exact strings or regex patterns provided in a file against a Document and assigns them an named entity. The definitions can contain any number of named entities. There are multiple ways and formats to set the extraction resource. It is possible to set it either as a “JSON”, “JSONL” or “CSV” file. A path to the file needs to be provided to setPatternsResource. The file format needs to be set as the “format” field in the option parameter map and depending on the file type, additional parameters might need to be set. If the file is in a JSON format, then the rule definitions need to be given in a list with the fields “id”, “label” and “patterns”: [ { &quot;id&quot;: &quot;person-regex&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot; w+ s w+&quot;, &quot; w+- w+&quot;] }, { &quot;id&quot;: &quot;locations-words&quot;, &quot;label&quot;: &quot;LOCATION&quot;, &quot;patterns&quot;: [&quot;Winterfell&quot;] } ] The same fields also apply to a file in the JSONL format: {&quot;id&quot;: &quot;names-with-j&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Jon&quot;, &quot;John&quot;, &quot;John Snow&quot;]} {&quot;id&quot;: &quot;names-with-s&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Stark&quot;, &quot;Snow&quot;]} {&quot;id&quot;: &quot;names-with-e&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Eddard&quot;, &quot;Eddard Stark&quot;]} In order to use a CSV file, an additional parameter “delimiter” needs to be set. In this case, the delimiter might be set by using .setPatternsResource(&quot;patterns.csv&quot;, ReadAs.TEXT, Map(&quot;format&quot;-&gt;&quot;csv&quot;, &quot;delimiter&quot; -&gt; &quot; |&quot;)) PERSON|Jon PERSON|John PERSON|John Snow LOCATION|Winterfell Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: EntityRulerApproach Scala API: EntityRulerApproach Source: EntityRulerApproach Show Example PythonScala # In this example, the entities file as the form of # # PERSON|Jon # PERSON|John # PERSON|John Snow # LOCATION|Winterfell # # where each line represents an entity and the associated string delimited by &quot;|&quot;. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.common import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) entityRuler = EntityRulerApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;entities&quot;) .setPatternsResource( &quot;patterns.csv&quot;, ReadAs.TEXT, {&quot;format&quot;: &quot;csv&quot;, &quot;delimiter&quot;: &quot; |&quot;} ) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, entityRuler ]) data = spark.createDataFrame([[&quot;Jon Snow wants to be lord of Winterfell.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(entities)&quot;).show(truncate=False) +--+ |col | +--+ |[chunk, 0, 2, Jon, [entity -&gt; PERSON, sentence -&gt; 0], []] | |[chunk, 29, 38, Winterfell, [entity -&gt; LOCATION, sentence -&gt; 0], []]| +--+ // In this example, the entities file as the form of // // PERSON|Jon // PERSON|John // PERSON|John Snow // LOCATION|Winterfell // // where each line represents an entity and the associated string delimited by &quot;|&quot;. import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.er.EntityRulerApproach import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val entityRuler = new EntityRulerApproach() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;entities&quot;) .setPatternsResource( &quot;src/test/resources/entity-ruler/patterns.csv&quot;, ReadAs.TEXT, {&quot;format&quot;: &quot;csv&quot;, &quot;delimiter&quot;: &quot;|&quot;)} ) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, entityRuler )) val data = Seq(&quot;Jon Snow wants to be lord of Winterfell.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(entities)&quot;).show(false) +--+ |col | +--+ |[chunk, 0, 2, Jon, [entity -&gt; PERSON, sentence -&gt; 0], []] | |[chunk, 29, 38, Winterfell, [entity -&gt; LOCATION, sentence -&gt; 0], []]| +--+ Instantiated model of the EntityRulerApproach. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: EntityRulerModel Scala API: EntityRulerModel Source: EntityRulerModel Finisher Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. The Finisher outputs annotation(s) values into String. For more extended examples on document pre-processing see the Examples. Input Annotator Types: ANY Output Annotator Type: NONE Python API: Finisher Scala API: Finisher Source: Finisher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline data = spark.createDataFrame([[1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;]]).toDF(&quot;id&quot;, &quot;text&quot;) # Extracts Named Entities amongst other things pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) finisher = Finisher().setInputCols(&quot;entities&quot;).setOutputCols(&quot;output&quot;) explainResult = pipeline.transform(data) explainResult.selectExpr(&quot;explode(entities)&quot;).show(truncate=False) ++ |entities | ++ |[[chunk, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []], [chunk, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]]| ++ result = finisher.transform(explainResult) result.select(&quot;output&quot;).show(truncate=False) +-+ |output | +-+ |[New York, New Jersey]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.Finisher val data = Seq((1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;)).toDF(&quot;id&quot;, &quot;text&quot;) // Extracts Named Entities amongst other things val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) val finisher = new Finisher().setInputCols(&quot;entities&quot;).setOutputCols(&quot;output&quot;) val explainResult = pipeline.transform(data) explainResult.selectExpr(&quot;explode(entities)&quot;).show(false) ++ |entities | ++ |[[chunk, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []], [chunk, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]]| ++ val result = finisher.transform(explainResult) result.select(&quot;output&quot;).show(false) +-+ |output | +-+ |[New York, New Jersey]| +-+ GraphExtraction Extracts a dependency graph between entities. The GraphExtraction class takes e.g. extracted entities from a NerDLModel and creates a dependency tree which describes how the entities relate to each other. For that a triple store format is used. Nodes represent the entities and the edges represent the relations between those entities. The graph can then be used to find relevant relationships between words. Both the DependencyParserModel and TypedDependencyParserModel need to be present in the pipeline. There are two ways to set them: Both Annotators are present in the pipeline already. The dependencies are taken implicitly from these two Annotators. Setting setMergeEntities to true will download the default pretrained models for those two Annotators automatically. The specific models can also be set with setDependencyParserModel and setTypedDependencyParserModel: val graph_extraction = new GraphExtraction() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;graph&quot;) .setRelationshipTypes(Array(&quot;prefer-LOC&quot;)) .setMergeEntities(true) //.setDependencyParserModel(Array(&quot;dependency_conllu&quot;, &quot;en&quot;, &quot;public/models&quot;)) //.setTypedDependencyParserModel(Array(&quot;dependency_typed_conllu&quot;, &quot;en&quot;, &quot;public/models&quot;)) To transform the resulting graph into a more generic form such as RDF, see the GraphFinisher. Input Annotator Types: DOCUMENT, TOKEN, NAMED_ENTITY Output Annotator Type: NODE Python API: GraphExtraction Scala API: GraphExtraction Source: GraphExtraction Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) nerTagger = NerDLModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) posTagger = PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) dependencyParser = DependencyParserModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency&quot;) typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols([&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency_type&quot;) graph_extraction = GraphExtraction() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;graph&quot;) .setRelationshipTypes([&quot;prefer-LOC&quot;]) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, embeddings, nerTagger, posTagger, dependencyParser, typedDependencyParser, graph_extraction ]) data = spark.createDataFrame([[&quot;You and John prefer the morning flight through Denver&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;graph&quot;).show(truncate=False) +--+ |graph | +--+ |13, 18, prefer, [relationship -&gt; prefer,LOC, path1 -&gt; prefer,nsubj,morning,flat,flight,flat,Denver], []| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel import com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserModel import org.apache.spark.ml.Pipeline import com.johnsnowlabs.nlp.annotators.GraphExtraction val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val nerTagger = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val dependencyParser = DependencyParserModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) val typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols(&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency_type&quot;) val graph_extraction = new GraphExtraction() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;graph&quot;) .setRelationshipTypes(Array(&quot;prefer-LOC&quot;)) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger, posTagger, dependencyParser, typedDependencyParser, graph_extraction )) val data = Seq(&quot;You and John prefer the morning flight through Denver&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;graph&quot;).show(false) +--+ |graph | +--+ |[[node, 13, 18, prefer, [relationship -&gt; prefer,LOC, path1 -&gt; prefer,nsubj,morning,flat,flight,flat,Denver], []]]| +--+ GraphFinisher Helper class to convert the knowledge graph from GraphExtraction into a generic format, such as RDF. Input Annotator Types: NONE Output Annotator Type: NONE Python API: GraphFinisher Scala API: GraphFinisher Source: GraphFinisher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # This is a continuation of the example of # GraphExtraction. To see how the graph is extracted, see the # documentation of that class. graphFinisher = GraphFinisher() .setInputCol(&quot;graph&quot;) .setOutputCol(&quot;graph_finished&quot;) .setOutputAs[False] finishedResult = graphFinisher.transform(result) finishedResult.select(&quot;text&quot;, &quot;graph_finished&quot;).show(truncate=False) +--+--+ |text |graph_finished | +--+--+ |You and John prefer the morning flight through Denver|(morning,flat,flight), (flight,flat,Denver)| +--+--+ // This is a continuation of the example of // [[com.johnsnowlabs.nlp.annotators.GraphExtraction GraphExtraction]]. To see how the graph is extracted, see the // documentation of that class. import com.johnsnowlabs.nlp.GraphFinisher val graphFinisher = new GraphFinisher() .setInputCol(&quot;graph&quot;) .setOutputCol(&quot;graph_finished&quot;) .setOutputAsArray(false) val finishedResult = graphFinisher.transform(result) finishedResult.select(&quot;text&quot;, &quot;graph_finished&quot;).show(false) +--+--+ |text |graph_finished | +--+--+ |You and John prefer the morning flight through Denver|[[(prefer,nsubj,morning), (morning,flat,flight), (flight,flat,Denver)]]| +--+--+ ImageAssembler Prepares images read by Spark into a format that is processable by Spark NLP. This component is needed to process images. Input Annotator Types: NONE Output Annotator Type: IMAGE Python API: ImageAssembler Scala API: ImageAssembler Source: ImageAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from pyspark.ml import Pipeline data = spark.read.format(&quot;image&quot;).load(&quot;./tmp/images/&quot;).toDF(&quot;image&quot;) imageAssembler = ImageAssembler().setInputCol(&quot;image&quot;).setOutputCol(&quot;image_assembler&quot;) result = imageAssembler.transform(data) result.select(&quot;image_assembler&quot;).show() result.select(&quot;image_assembler&quot;).printSchema() root |-- image_assembler: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- origin: string (nullable = true) | | |-- height: integer (nullable = true) | | |-- width: integer (nullable = true) | | |-- nChannels: integer (nullable = true) | | |-- mode: integer (nullable = true) | | |-- result: binary (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) import com.johnsnowlabs.nlp.ImageAssembler import org.apache.spark.ml.Pipeline val imageDF: DataFrame = spark.read .format(&quot;image&quot;) .option(&quot;dropInvalid&quot;, value = true) .load(&quot;src/test/resources/image/&quot;) val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) val pipeline = new Pipeline().setStages(Array(imageAssembler)) val pipelineDF = pipeline.fit(imageDF).transform(imageDF) pipelineDF.printSchema() root |-- image_assembler: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- origin: string (nullable = true) | | |-- height: integer (nullable = false) | | |-- width: integer (nullable = false) | | |-- nChannels: integer (nullable = false) | | |-- mode: integer (nullable = false) | | |-- result: binary (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) LanguageDetectorDL Language Identification and Detection by using CNN and RNN architectures in TensorFlow. LanguageDetectorDL is an annotator that detects the language of documents or sentences depending on the inputCols. The models are trained on large datasets such as Wikipedia and Tatoeba. Depending on the language (how similar the characters are), the LanguageDetectorDL works best with text longer than 140 characters. The output is a language code in Wiki Code style. Pretrained models can be loaded with pretrained of the companion object: Val languageDetector = LanguageDetectorDL.pretrained() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;language&quot;) The default model is &quot;ld_wiki_tatoeba_cnn_21&quot;, default language is &quot;xx&quot; (meaning multi-lingual), if no values are provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples And the LanguageDetectorDLTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: LANGUAGE Python API: LanguageDetectorDL Scala API: LanguageDetectorDL Source: LanguageDetectorDL Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) languageDetector = LanguageDetectorDL.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;language&quot;) pipeline = Pipeline() .setStages([ documentAssembler, languageDetector ]) data = spark.createDataFrame([ [&quot;Spark NLP is an open-source text processing library for advanced natural language processing for the Python, Java and Scala programming languages.&quot;], [&quot;Spark NLP est une bibliothèque de traitement de texte open source pour le traitement avancé du langage naturel pour les langages de programmation Python, Java et Scala.&quot;], [&quot;Spark NLP ist eine Open-Source-Textverarbeitungsbibliothek für fortgeschrittene natürliche Sprachverarbeitung für die Programmiersprachen Python, Java und Scala.&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;language.result&quot;).show(truncate=False) ++ |result| ++ |[en] | |[fr] | |[de] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.ld.dl.LanguageDetectorDL import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val languageDetector = LanguageDetectorDL.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;language&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, languageDetector )) val data = Seq( &quot;Spark NLP is an open-source text processing library for advanced natural language processing for the Python, Java and Scala programming languages.&quot;, &quot;Spark NLP est une bibliothèque de traitement de texte open source pour le traitement avancé du langage naturel pour les langages de programmation Python, Java et Scala.&quot;, &quot;Spark NLP ist eine Open-Source-Textverarbeitungsbibliothek für fortgeschrittene natürliche Sprachverarbeitung für die Programmiersprachen Python, Java und Scala.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;language.result&quot;).show(false) ++ |result| ++ |[en] | |[fr] | |[de] | ++ Lemmatizer ApproachModel Class to find lemmas out of words with the objective of returning a base dictionary word. Retrieves the significant part of a word. A dictionary of predefined lemmas must be provided with setDictionary. The dictionary can be set as a delimited text file. Pretrained models can be loaded with LemmatizerModel.pretrained. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: Lemmatizer Scala API: Lemmatizer Source: Lemmatizer Show Example PythonScala # In this example, the lemma dictionary `lemmas_small.txt` has the form of # # ... # pick -&gt; pick picks picking picked # peck -&gt; peck pecking pecked pecks # pickle -&gt; pickle pickles pickled pickling # pepper -&gt; pepper peppers peppered peppering # ... # # where each key is delimited by `-&gt;` and values are delimited by ` t` import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = Lemmatizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) pipeline = Pipeline() .setStages([ documentAssembler, sentenceDetector, tokenizer, lemmatizer ]) data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;lemma.result&quot;).show(truncate=False) ++ |result | ++ |[Peter, Pipers, employees, are, pick, peck, of, pickle, pepper, .]| ++ // In this example, the lemma dictionary `lemmas_small.txt` has the form of // // ... // pick -&gt; pick picks picking picked // peck -&gt; peck pecking pecked pecks // pickle -&gt; pickle pickles pickled pickling // pepper -&gt; pepper peppers peppered peppering // ... // // where each key is delimited by `-&gt;` and values are delimited by ` t` import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.Lemmatizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val lemmatizer = new Lemmatizer() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentenceDetector, tokenizer, lemmatizer )) val data = Seq(&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;lemma.result&quot;).show(false) ++ |result | ++ |[Peter, Pipers, employees, are, pick, peck, of, pickle, pepper, .]| ++ Instantiated Model of the Lemmatizer. For usage and examples, please see the documentation of that class. For available pretrained models please see the Models Hub. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: LemmatizerModel Scala API: LemmatizerModel Source: LemmatizerModel MultiClassifierDL ApproachModel Trains a MultiClassifierDL for Multi-label Text Classification. MultiClassifierDL uses a Bidirectional GRU with a convolutional model that we have built inside TensorFlow and supports up to 100 classes. For instantiated/pretrained models, see MultiClassifierDLModel. The input to MultiClassifierDL are Sentence Embeddings such as the state-of-the-art UniversalSentenceEncoder, BertSentenceEmbeddings or SentenceEmbeddings. In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to. Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y). Setting a test dataset to monitor model metrics can be done with .setTestDataset. The method expects a path to a parquet file containing a dataframe that has the same required columns as the training dataframe. The pre-processing steps for the training dataframe should also be applied to the test dataframe. The following example will show how to create the test dataset: val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val preProcessingPipeline = new Pipeline().setStages(Array(documentAssembler, embeddings)) val Array(train, test) = data.randomSplit(Array(0.8, 0.2)) preProcessingPipeline .fit(test) .transform(test) .write .mode(&quot;overwrite&quot;) .parquet(&quot;test_data&quot;) val multiClassifier = new MultiClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setTestDataset(&quot;test_data&quot;) For extended examples of usage, see the Examples and the MultiClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Note: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. UniversalSentenceEncoder, BertSentenceEmbeddings, SentenceEmbeddings or other sentence based embeddings can be used for the inputCol Python API: MultiClassifierDLApproach Scala API: MultiClassifierDLApproach Source: MultiClassifierDLApproach Show Example PythonScala # In this example, the training data has the form # # +-+--+--+ # | id| text| labels| # +-+--+--+ # |ed58abb40640f983|PN NewsYou mean ... | [toxic]| # |a1237f726b5f5d89|Dude. Place the ...| [obscene, insult]| # |24b0d6c8733c2abe|Thanks - thanks ...| [insult]| # |8c4478fb239bcfc0|&quot; Gee, 5 minutes ...|[toxic, obscene, ...| # +-+--+--+ import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Process training data to create text with associated array of labels trainDataset.printSchema() # root # |-- id: string (nullable = true) # |-- text: string (nullable = true) # |-- labels: array (nullable = true) # | |-- element: string (containsNull = true) # Then create pipeline for training documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) .setCleanupMode(&quot;shrink&quot;) embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;embeddings&quot;) docClassifier = MultiClassifierDLApproach() .setInputCols(&quot;embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;labels&quot;) .setBatchSize(128) .setMaxEpochs(10) .setLr(1e-3) .setThreshold(0.5) .setValidationSplit(0.1) pipeline = Pipeline() .setStages( [ documentAssembler, embeddings, docClassifier ] ) pipelineModel = pipeline.fit(trainDataset) // In this example, the training data has the form (Note: labels can be arbitrary) // // mr,ref // &quot;name[Alimentum], area[city centre], familyFriendly[no], near[Burger King]&quot;,Alimentum is an adult establish found in the city centre area near Burger King. // &quot;name[Alimentum], area[city centre], familyFriendly[yes]&quot;,Alimentum is a family-friendly place in the city centre. // ... // // It needs some pre-processing first, so the labels are of type `Array[String]`. This can be done like so: import spark.implicits._ import com.johnsnowlabs.nlp.annotators.classifier.dl.MultiClassifierDLApproach import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import org.apache.spark.ml.Pipeline import org.apache.spark.sql.functions.{col, udf} // Process training data to create text with associated array of labels def splitAndTrim = udf { labels: String =&gt; labels.split(&quot;, &quot;).map(x=&gt;x.trim) } val smallCorpus = spark.read .option(&quot;header&quot;, true) .option(&quot;inferSchema&quot;, true) .option(&quot;mode&quot;, &quot;DROPMALFORMED&quot;) .csv(&quot;src/test/resources/classifier/e2e.csv&quot;) .withColumn(&quot;labels&quot;, splitAndTrim(col(&quot;mr&quot;))) .withColumn(&quot;text&quot;, col(&quot;ref&quot;)) .drop(&quot;mr&quot;) smallCorpus.printSchema() // root // |-- ref: string (nullable = true) // |-- labels: array (nullable = true) // | |-- element: string (containsNull = true) // Then create pipeline for training val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) .setCleanupMode(&quot;shrink&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;embeddings&quot;) val docClassifier = new MultiClassifierDLApproach() .setInputCols(&quot;embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;labels&quot;) .setBatchSize(128) .setMaxEpochs(10) .setLr(1e-3f) .setThreshold(0.5f) .setValidationSplit(0.1f) val pipeline = new Pipeline() .setStages( Array( documentAssembler, embeddings, docClassifier ) ) val pipelineModel = pipeline.fit(smallCorpus) MultiClassifierDL for Multi-label Text Classification. MultiClassifierDL Bidirectional GRU with Convolution model we have built inside TensorFlow and supports up to 100 classes. The input to MultiClassifierDL are Sentence Embeddings such as state-of-the-art UniversalSentenceEncoder, BertSentenceEmbeddings or SentenceEmbeddings. This is the instantiated model of the MultiClassifierDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val multiClassifier = MultiClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;categories&quot;) The default model is &quot;multiclassifierdl_use_toxic&quot;, if no name is provided. It uses embeddings from the UniversalSentenceEncoder and classifies toxic comments. The data is based on the Jigsaw Toxic Comment Classification Challenge. For available pretrained models please see the Models Hub. In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to. Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y). For extended examples of usage, see the Examples and the MultiClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: MultiClassifierDLModel Scala API: MultiClassifierDLModel Source: MultiClassifierDLModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) multiClassifierDl = MultiClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;classifications&quot;) pipeline = Pipeline() .setStages([ documentAssembler, useEmbeddings, multiClassifierDl ]) data = spark.createDataFrame([ [&quot;This is pretty good stuff!&quot;], [&quot;Wtf kind of crap is this&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;text&quot;, &quot;classifications.result&quot;).show(truncate=False) +--+-+ |text |result | +--+-+ |This is pretty good stuff!|[] | |Wtf kind of crap is this |[toxic, obscene]| +--+-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.classifier.dl.MultiClassifierDLModel import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val multiClassifierDl = MultiClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;classifications&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, useEmbeddings, multiClassifierDl )) val data = Seq( &quot;This is pretty good stuff!&quot;, &quot;Wtf kind of crap is this&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;text&quot;, &quot;classifications.result&quot;).show(false) +--+-+ |text |result | +--+-+ |This is pretty good stuff!|[] | |Wtf kind of crap is this |[toxic, obscene]| +--+-+ MultiDateMatcher Matches standard date formats into a provided format. Reads the following kind of dates: &quot;1978-01-28&quot;, &quot;1984/04/02,1/02/1980&quot;, &quot;2/28/79&quot;, &quot;The 31st of April in the year 2008&quot;, &quot;Fri, 21 Nov 1997&quot;, &quot;Jan 21, ‘97&quot;, &quot;Sun&quot;, &quot;Nov 21&quot;, &quot;jan 1st&quot;, &quot;next thursday&quot;, &quot;last wednesday&quot;, &quot;today&quot;, &quot;tomorrow&quot;, &quot;yesterday&quot;, &quot;next week&quot;, &quot;next month&quot;, &quot;next year&quot;, &quot;day after&quot;, &quot;the day before&quot;, &quot;0600h&quot;, &quot;06:00 hours&quot;, &quot;6pm&quot;, &quot;5:30 a.m.&quot;, &quot;at 5&quot;, &quot;12:59&quot;, &quot;23:59&quot;, &quot;1988/11/23 6pm&quot;, &quot;next week at 7.30&quot;, &quot;5 am tomorrow&quot; For example &quot;The 31st of April in the year 2008&quot; will be converted into 2008/04/31. For extended examples of usage, see the Examples and the MultiDateMatcherTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DATE Python API: MultiDateMatcher Scala API: MultiDateMatcher Source: MultiDateMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) date = MultiDateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) pipeline = Pipeline().setStages([ documentAssembler, date ]) data = spark.createDataFrame([[&quot;I saw him yesterday and he told me that he will visit us next week&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(date) as dates&quot;).show(truncate=False) +--+ |dates | +--+ |[date, 57, 65, 2020/01/18, [sentence -&gt; 0], []]| |[date, 10, 18, 2020/01/10, [sentence -&gt; 0], []]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.MultiDateMatcher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val date = new MultiDateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, date )) val data = Seq(&quot;I saw him yesterday and he told me that he will visit us next week&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(date) as dates&quot;).show(false) +--+ |dates | +--+ |[date, 57, 65, 2020/01/18, [sentence -&gt; 0], []]| |[date, 10, 18, 2020/01/10, [sentence -&gt; 0], []]| +--+ MultiDocumentAssembler Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. The MultiDocumentAssembler can read either a String column or an Array[String]. Additionally, MultiDocumentAssembler.setCleanupMode can be used to pre-process the text (Default: disabled). For possible options please refer the parameters section. For more extended examples on document pre-processing see the Examples. Input Annotator Types: NONE Output Annotator Type: DOCUMENT Python API: MultiDocumentAssembler Scala API: MultiDocumentAssembler Source: MultiDocumentAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from pyspark.ml import Pipeline data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library.&quot;], [&quot;Spark NLP is a state-of-the-art Natural Language Processing library built on top of Apache Spark&quot;]]).toDF(&quot;text&quot;, &quot;text2&quot;) documentAssembler = MultiDocumentAssembler().setInputCols([&quot;text&quot;, &quot;text2&quot;]).setOutputCols([&quot;document1&quot;, &quot;document2&quot;]) result = documentAssembler.transform(data) result.select(&quot;document1&quot;).show(truncate=False) +-+ |document1 | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document1&quot;).printSchema() root |-- document: array (nullable = True) | |-- element: struct (containsNull = True) | | |-- annotatorType: string (nullable = True) | | |-- begin: integer (nullable = False) | | |-- end: integer (nullable = False) | | |-- result: string (nullable = True) | | |-- metadata: map (nullable = True) | | | |-- key: string | | | |-- value: string (valueContainsNull = True) | | |-- embeddings: array (nullable = True) | | | |-- element: float (containsNull = False) import spark.implicits._ import com.johnsnowlabs.nlp.MultiDocumentAssembler val data = Seq(&quot;Spark NLP is an open-source text processing library.&quot;).toDF(&quot;text&quot;) val multiDocumentAssembler = new MultiDocumentAssembler().setInputCols(&quot;text&quot;).setOutputCols(&quot;document&quot;) val result = multiDocumentAssembler.transform(data) result.select(&quot;document&quot;).show(false) +-+ |document | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document&quot;).printSchema root |-- document: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- begin: integer (nullable = false) | | |-- end: integer (nullable = false) | | |-- result: string (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | | |-- embeddings: array (nullable = true) | | | |-- element: float (containsNull = false) NGramGenerator A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words. When the input is empty, an empty array is returned. When the input array length is less than n (number of elements per n-gram), no n-grams are returned. For more extended examples see the Examples and the NGramGeneratorTestSpec. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: NGramGenerator Scala API: NGramGenerator Source: NGramGenerator Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) nGrams = NGramGenerator() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;ngrams&quot;) .setN(2) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, nGrams ]) data = spark.createDataFrame([[&quot;This is my sentence.&quot;]]).toDF(&quot;text&quot;) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(ngrams) as result&quot;).show(truncate=False) ++ |result | ++ |[chunk, 0, 6, This is, [sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 5, 9, is my, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 8, 18, my sentence, [sentence -&gt; 0, chunk -&gt; 2], []]| |[chunk, 11, 19, sentence ., [sentence -&gt; 0, chunk -&gt; 3], []]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.NGramGenerator import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val nGrams = new NGramGenerator() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;ngrams&quot;) .setN(2) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, nGrams )) val data = Seq(&quot;This is my sentence.&quot;).toDF(&quot;text&quot;) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(ngrams) as result&quot;).show(false) ++ |result | ++ |[chunk, 0, 6, This is, [sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 5, 9, is my, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 8, 18, my sentence, [sentence -&gt; 0, chunk -&gt; 2], []]| |[chunk, 11, 19, sentence ., [sentence -&gt; 0, chunk -&gt; 3], []]| ++ NerConverter Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. Results in CHUNK Annotation type. NER chunks can then be filtered by setting a whitelist with setWhiteList. Chunks with no associated entity (tagged “O”) are filtered. See also Inside–outside–beginning (tagging) for more information. Input Annotator Types: DOCUMENT, TOKEN, NAMED_ENTITY Output Annotator Type: CHUNK Python API: NerConverter Scala API: NerConverter Source: NerConverter Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # This is a continuation of the example of the NerDLModel. See that class # on how to extract the entities. # The output of the NerDLModel follows the Annotator schema and can be converted like so: # # result.selectExpr(&quot;explode(ner)&quot;).show(truncate=False) # +-+ # |col | # +-+ # |[named_entity, 0, 2, B-ORG, [word -&gt; U.N], []] | # |[named_entity, 3, 3, O, [word -&gt; .], []] | # |[named_entity, 5, 12, O, [word -&gt; official], []] | # |[named_entity, 14, 18, B-PER, [word -&gt; Ekeus], []] | # |[named_entity, 20, 24, O, [word -&gt; heads], []] | # |[named_entity, 26, 28, O, [word -&gt; for], []] | # |[named_entity, 30, 36, B-LOC, [word -&gt; Baghdad], []]| # |[named_entity, 37, 37, O, [word -&gt; .], []] | # +-+ # # After the converter is used: converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;entities&quot;) converter.transform(result).selectExpr(&quot;explode(entities)&quot;).show(truncate=False) ++ |col | ++ |[chunk, 0, 2, U.N, [entity -&gt; ORG, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 14, 18, Ekeus, [entity -&gt; PER, sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 30, 36, Baghdad, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 2], []]| ++ // This is a continuation of the example of the [[com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel NerDLModel]]. See that class // on how to extract the entities. // The output of the NerDLModel follows the Annotator schema and can be converted like so: // // result.selectExpr(&quot;explode(ner)&quot;).show(false) // +-+ // |col | // +-+ // |[named_entity, 0, 2, B-ORG, [word -&gt; U.N], []] | // |[named_entity, 3, 3, O, [word -&gt; .], []] | // |[named_entity, 5, 12, O, [word -&gt; official], []] | // |[named_entity, 14, 18, B-PER, [word -&gt; Ekeus], []] | // |[named_entity, 20, 24, O, [word -&gt; heads], []] | // |[named_entity, 26, 28, O, [word -&gt; for], []] | // |[named_entity, 30, 36, B-LOC, [word -&gt; Baghdad], []]| // |[named_entity, 37, 37, O, [word -&gt; .], []] | // +-+ // // After the converter is used: val converter = new NerConverter() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) .setPreservePosition(false) converter.transform(result).selectExpr(&quot;explode(entities)&quot;).show(false) ++ |col | ++ |[chunk, 0, 2, U.N, [entity -&gt; ORG, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 14, 18, Ekeus, [entity -&gt; PER, sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 30, 36, Baghdad, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 2], []]| ++ NerCrf ApproachModel Algorithm for training a Named Entity Recognition Model For instantiated/pretrained models, see NerCrfModel. This Named Entity recognition annotator allows for a generic model to be trained by utilizing a CRF machine learning algorithm. The training data should be a labeled Spark Dataset, e.g. CoNLL 2003 IOB with Annotation type columns. The data should have columns of type DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS and an additional label column of annotator type NAMED_ENTITY. Excluding the label, this can be done with for example a SentenceDetector, a Tokenizer and a PerceptronModel and a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). Optionally the user can provide an entity dictionary file with setExternalFeatures for better accuracy. For extended examples of usage, see the Examples and the NerCrfApproachTestSpec. Input Annotator Types: DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerCrfApproach Scala API: NerCrfApproach Source: NerCrfApproach Show Example PythonScala # This CoNLL dataset already includes the sentence, token, pos and label column with their respective annotator types. # If a custom dataset is used, these need to be defined. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) nerTagger = NerCrfApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setMinEpochs(1) .setMaxEpochs(3) .setC0(34) .setL2(3.0) .setOutputCol(&quot;ner&quot;) pipeline = Pipeline().setStages([ documentAssembler, embeddings, nerTagger ]) conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) // This CoNLL dataset already includes the sentence, token, pos and label column with their respective annotator types. // If a custom dataset is used, these need to be defined. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotator.NerCrfApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val nerTagger = new NerCrfApproach() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;) .setLabelColumn(&quot;label&quot;) .setMinEpochs(1) .setMaxEpochs(3) .setC0(34) .setL2(3.0) .setOutputCol(&quot;ner&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, embeddings, nerTagger )) val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) Extracts Named Entities based on a CRF Model. This Named Entity recognition annotator allows for a generic model to be trained by utilizing a CRF machine learning algorithm. The data should have columns of type DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS. These can be extracted with for example a SentenceDetector, a Tokenizer and a PerceptronModel This is the instantiated model of the NerCrfApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val nerTagger = NerCrfModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;, &quot;pos&quot;) .setOutputCol(&quot;ner&quot; The default model is &quot;ner_crf&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples. Input Annotator Types: DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerCrfModel Scala API: NerCrfModel Source: NerCrfModel NerDL ApproachModel This Named Entity recognition annotator allows to train generic NER model based on Neural Networks. The architecture of the neural network is a Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. For instantiated/pretrained models, see NerDLModel. The training data should be a labeled Spark Dataset, in the format of CoNLL 2003 IOB with Annotation type columns. The data should have columns of type DOCUMENT, TOKEN, WORD_EMBEDDINGS and an additional label column of annotator type NAMED_ENTITY. Excluding the label, this can be done with for example a SentenceDetector, a Tokenizer and a PerceptronModel and a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). Setting a test dataset to monitor model metrics can be done with .setTestDataset. The method expects a path to a parquet file containing a dataframe that has the same required columns as the training dataframe. The pre-processing steps for the training dataframe should also be applied to the test dataframe. The following example will show how to create the test dataset with a CoNLL dataset: val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = WordEmbeddingsModel .pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val preProcessingPipeline = new Pipeline().setStages(Array(documentAssembler, embeddings)) val conll = CoNLL() val Array(train, test) = conll .readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) .randomSplit(Array(0.8, 0.2)) preProcessingPipeline .fit(test) .transform(test) .write .mode(&quot;overwrite&quot;) .parquet(&quot;test_data&quot;) val nerTagger = new NerDLApproach() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setTestDataset(&quot;test_data&quot;) For extended examples of usage, see the Examples and the NerDLSpec. Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerDLApproach Scala API: NerDLApproach Source: NerDLApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline # First extract the prerequisites for the NerDLApproach documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = BertEmbeddings.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Then the training can start nerTagger = NerDLApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(1) .setRandomSeed(0) .setVerbose(0) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, embeddings, nerTagger ]) # We use the text and labels from the CoNLL dataset conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.BertEmbeddings import com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline // First extract the prerequisites for the NerDLApproach val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger = new NerDLApproach() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(1) .setRandomSeed(0) .setVerbose(0) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) // We use the text and labels from the CoNLL dataset val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) This Named Entity recognition annotator is a generic NER model based on Neural Networks. Neural Network architecture is Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. This is the instantiated model of the NerDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val nerModel = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) The default model is &quot;ner_dl&quot;, if no name is provided. For available pretrained models please see the Models Hub. Additionally, pretrained pipelines are available for this module, see Pipelines. Note that some pretrained models require specific types of embeddings, depending on which they were trained on. For example, the default model &quot;ner_dl&quot; requires the WordEmbeddings &quot;glove_100d&quot;. For extended examples of usage, see the Examples and the NerDLSpec. Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerDLModel Scala API: NerDLModel Source: NerDLModel NerOverwriter Overwrites entities of specified strings. The input for this Annotator have to be entities that are already extracted, Annotator type NAMED_ENTITY. The strings specified with setStopWords will have new entities assigned to, specified with setNewResult. Input Annotator Types: NAMED_ENTITY Output Annotator Type: NAMED_ENTITY Python API: NerOverwriter Scala API: NerOverwriter Source: NerOverwriter Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # First extract the prerequisite Entities documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;bert&quot;) nerTagger = NerDLModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;bert&quot;]) .setOutputCol(&quot;ner&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, embeddings, nerTagger ]) data = spark.createDataFrame([[&quot;Spark NLP Crosses Five Million Downloads, John Snow Labs Announces.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(ner)&quot;).show(truncate=False) # ++ # |col | # ++ # |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | # |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | # |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | # |[named_entity, 18, 21, O, [word -&gt; Five], []] | # |[named_entity, 23, 29, O, [word -&gt; Million], []] | # |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | # |[named_entity, 40, 40, O, [word -&gt; ,], []] | # |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | # |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | # |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | # |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []]| # |[named_entity, 66, 66, O, [word -&gt; .], []] | # ++ # The recognized entities can then be overwritten nerOverwriter = NerOverwriter() .setInputCols([&quot;ner&quot;]) .setOutputCol(&quot;ner_overwritten&quot;) .setStopWords([&quot;Million&quot;]) .setNewResult(&quot;B-CARDINAL&quot;) nerOverwriter.transform(result).selectExpr(&quot;explode(ner_overwritten)&quot;).show(truncate=False) ++ |col | ++ |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | |[named_entity, 18, 21, O, [word -&gt; Five], []] | |[named_entity, 23, 29, B-CARDINAL, [word -&gt; Million], []]| |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | |[named_entity, 40, 40, O, [word -&gt; ,], []] | |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []] | |[named_entity, 66, 66, O, [word -&gt; .], []] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel import com.johnsnowlabs.nlp.annotators.ner.NerOverwriter import org.apache.spark.ml.Pipeline // First extract the prerequisite Entities val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;bert&quot;) val nerTagger = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;bert&quot;) .setOutputCol(&quot;ner&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) val data = Seq(&quot;Spark NLP Crosses Five Million Downloads, John Snow Labs Announces.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(ner)&quot;).show(false) / ++ |col | ++ |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | |[named_entity, 18, 21, O, [word -&gt; Five], []] | |[named_entity, 23, 29, O, [word -&gt; Million], []] | |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | |[named_entity, 40, 40, O, [word -&gt; ,], []] | |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []]| |[named_entity, 66, 66, O, [word -&gt; .], []] | ++ / // The recognized entities can then be overwritten val nerOverwriter = new NerOverwriter() .setInputCols(&quot;ner&quot;) .setOutputCol(&quot;ner_overwritten&quot;) .setStopWords(Array(&quot;Million&quot;)) .setNewResult(&quot;B-CARDINAL&quot;) nerOverwriter.transform(result).selectExpr(&quot;explode(ner_overwritten)&quot;).show(false) ++ |col | ++ |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | |[named_entity, 18, 21, O, [word -&gt; Five], []] | |[named_entity, 23, 29, B-CARDINAL, [word -&gt; Million], []]| |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | |[named_entity, 40, 40, O, [word -&gt; ,], []] | |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []] | |[named_entity, 66, 66, O, [word -&gt; .], []] | ++ Normalizer ApproachModel Annotator that cleans out tokens. Requires stems, hence tokens. Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary For extended examples of usage, see the Examples. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: Normalizer Scala API: Normalizer Source: Normalizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) .setLowercase(True) .setCleanupPatterns([&quot;&quot;&quot;[^ w d s]&quot;&quot;&quot;]) # remove punctuations (keep alphanumeric chars) # if we don&#39;t set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z]) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, normalizer ]) data = spark.createDataFrame([[&quot;John and Peter are brothers. However they don&#39;t support each other that much.&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;normalized.result&quot;).show(truncate = False) +-+ |result | +-+ |[john, and, peter, are, brothers, however, they, dont, support, each, other, that, much]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Normalizer, Tokenizer} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) .setLowercase(true) .setCleanupPatterns(Array(&quot;&quot;&quot;[^ w d s]&quot;&quot;&quot;)) // remove punctuations (keep alphanumeric chars) // if we don&#39;t set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z]) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, normalizer )) val data = Seq(&quot;John and Peter are brothers. However they don&#39;t support each other that much.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;normalized.result&quot;).show(truncate = false) +-+ |result | +-+ |[john, and, peter, are, brothers, however, they, dont, support, each, other, that, much]| +-+ Instantiated Model of the Normalizer. For usage and examples, please see the documentation of that class. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: NormalizerModel Scala API: NormalizerModel Source: NormalizerModel NorvigSweeting Spellchecker ApproachModel Trains annotator, that retrieves tokens and makes corrections automatically if not found in an English dictionary. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster (than the standard approach with deletes + transposes + replaces + inserts) and language independent. A dictionary of correct spellings must be provided with setDictionary as a text file, where each word is parsed by a regex pattern. Inspired by Norvig model and SymSpell. For instantiated/pretrained models, see NorvigSweetingModel. For extended examples of usage, see the NorvigSweetingTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: NorvigSweetingApproach Scala API: NorvigSweetingApproach Source: NorvigSweetingApproach Show Example PythonScala # In this example, the dictionary `&quot;words.txt&quot;` has the form of # # ... # gummy # gummic # gummier # gummiest # gummiferous # ... # # This dictionary is then set to be the basis of the spell checker. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) spellChecker = NorvigSweetingApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker ]) pipelineModel = pipeline.fit(trainingData) // In this example, the dictionary `&quot;words.txt&quot;` has the form of // // ... // gummy // gummic // gummier // gummiest // gummiferous // ... // // This dictionary is then set to be the basis of the spell checker. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.spell.norvig.NorvigSweetingApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val spellChecker = new NorvigSweetingApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker )) val pipelineModel = pipeline.fit(trainingData) This annotator retrieves tokens and makes corrections automatically if not found in an English dictionary. Inspired by Norvig model and SymSpell. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster (than the standard approach with deletes + transposes + replaces + inserts) and language independent. This is the instantiated model of the NorvigSweetingApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val spellChecker = NorvigSweetingModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) .setDoubleVariants(true) The default model is &quot;spellcheck_norvig&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of see the NorvigSweetingTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: NorvigSweetingModel Scala API: NorvigSweetingModel Source: NorvigSweetingModel POSTagger (Part of speech tagger) ApproachModel Trains an averaged Perceptron model to tag words part-of-speech. Sets a POS tag to each word within a sentence. For pretrained models please see the PerceptronModel. The training data needs to be in a Spark DataFrame, where the column needs to consist of Annotations of type POS. The Annotation needs to have member result set to the POS tag and have a &quot;word&quot; mapping to its word inside of member metadata. This DataFrame for training can easily created by the helper class POS. POS().readDataset(spark, datasetPath).selectExpr(&quot;explode(tags) as tags&quot;).show(false) ++ |tags | ++ |[pos, 0, 5, NNP, [word -&gt; Pierre], []] | |[pos, 7, 12, NNP, [word -&gt; Vinken], []] | |[pos, 14, 14, ,, [word -&gt; ,], []] | |[pos, 31, 34, MD, [word -&gt; will], []] | |[pos, 36, 39, VB, [word -&gt; join], []] | |[pos, 41, 43, DT, [word -&gt; the], []] | |[pos, 45, 49, NN, [word -&gt; board], []] | ... For extended examples of usage, see the Examples and PerceptronApproach tests. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: POS Python API: PerceptronApproach Scala API: PerceptronApproach Source: PerceptronApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) datasetPath = &quot;src/test/resources/anc-pos-corpus-small/test-training.txt&quot; trainingPerceptronDF = POS().readDataset(spark, datasetPath) trainedPos = PerceptronApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) .setPosColumn(&quot;tags&quot;) .fit(trainingPerceptronDF) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, trainedPos ]) data = spark.createDataFrame([[&quot;To be or not to be, is this the question?&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;pos.result&quot;).show(truncate=False) +--+ |result | +--+ |[NNP, NNP, CD, JJ, NNP, NNP, ,, MD, VB, DT, CD, .]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.training.POS import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val datasetPath = &quot;src/test/resources/anc-pos-corpus-small/test-training.txt&quot; val trainingPerceptronDF = POS().readDataset(spark, datasetPath) val trainedPos = new PerceptronApproach() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) .setPosColumn(&quot;tags&quot;) .fit(trainingPerceptronDF) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, trainedPos )) val data = Seq(&quot;To be or not to be, is this the question?&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;pos.result&quot;).show(false) +--+ |result | +--+ |[NNP, NNP, CD, JJ, NNP, NNP, ,, MD, VB, DT, CD, .]| +--+ Averaged Perceptron model to tag words part-of-speech. Sets a POS tag to each word within a sentence. This is the instantiated model of the PerceptronApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) The default model is &quot;pos_anc&quot;, if no name is provided. For available pretrained models please see the Models Hub. Additionally, pretrained pipelines are available for this module, see Pipelines. For extended examples of usage, see the Examples. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: POS Python API: PerceptronModel Scala API: PerceptronModel Source: PerceptronModel PromptAssembler Assembles a sequence of messages into a single string using a template. These strings can then be used as prompts for large language models. This annotator expects an array of two-tuples as the type of the input column (one array of tuples per row). The first element of the tuples should be the role and the second element is the text of the message. Possible roles are “system”, “user” and “assistant”. An assistant header can be added to the end of the generated string by using setAddAssistant(true). At the moment, this annotator uses llama.cpp as a backend to parse and apply the templates. llama.cpp uses basic pattern matching to determine the type of the template, then applies a basic version of the template to the messages. This means that more advanced templates are not supported. For an extended example see the example notebook. Input Annotator Types: NONE Output Annotator Type: DOCUMENT Python API: PromptAssembler Scala API: PromptAssembler Source: PromptAssembler Show Example PythonScala from sparknlp.base import * messages = [ [ (&quot;system&quot;, &quot;You are a helpful assistant.&quot;), (&quot;assistant&quot;, &quot;Hello there, how can I help you?&quot;), (&quot;user&quot;, &quot;I need help with organizing my room.&quot;), ] ] df = spark.createDataFrame([messages]).toDF(&quot;messages&quot;) # llama3.1 template = ( &quot;{{- bos_token }} {%- if custom_tools is defined %} {%- set tools = custom_tools %} {%- &quot; &quot;endif %} {%- if not tools_in_user_message is defined %} {%- set tools_in_user_message = true %} {%- &quot; &#39;endif %} {%- if not date_string is defined %} {%- set date_string = &quot;26 Jul 2024&quot; %} {%- endif %} &#39; &quot;{%- if not tools is defined %} {%- set tools = none %} {%- endif %} {#- This block extracts the &quot; &quot;system message, so we can slot it into the right place. #} {%- if messages[0][&#39;role&#39;] == &#39;system&#39; %}&quot; &quot; {%- set system_message = messages[0][&#39;content&#39;]|trim %} {%- set messages = messages[1:] %} {%- else&quot; &#39; %} {%- set system_message = &quot;&quot; %} {%- endif %} {#- System message + builtin tools #} {{- &#39; &#39;&quot;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt; n n&quot; }} {%- if builtin_tools is defined or tools is &#39; &#39;not none %} {{- &quot;Environment: ipython n&quot; }} {%- endif %} {%- if builtin_tools is defined %} {{- &#39; &#39;&quot;Tools: &quot; + builtin_tools | reject( &#39;equalto &#39;, &#39;code_interpreter &#39;) | join(&quot;, &quot;) + &quot; n n&quot;}} &#39; &#39;{%- endif %} {{- &quot;Cutting Knowledge Date: December 2023 n&quot; }} {{- &quot;Today Date: &quot; + date_string &#39; &#39;+ &quot; n n&quot; }} {%- if tools is not none and not tools_in_user_message %} {{- &quot;You have access to &#39; &#39;the following functions. To call a function, please respond with JSON for a function call.&quot; }} {{- &#39; &#39; &#39;Respond in the format {&quot;name&quot;: function name, &quot;parameters&quot;: dictionary of argument name and its&#39; &#39; value}. &#39; }} {{- &quot;Do not use variables. n n&quot; }} {%- for t in tools %} {{- t | tojson(indent=4) &#39; &#39;}} {{- &quot; n n&quot; }} {%- endfor %} {%- endif %} {{- system_message }} {{- &quot;&lt;|eot_id|&gt;&quot; }} {#- &#39; &quot;Custom tools are passed in a user message with some extra guidance #} {%- if tools_in_user_message &quot; &quot;and not tools is none %} {#- Extract the first user message so we can plug it in here #} {%- if &quot; &quot;messages | length != 0 %} {%- set first_user_message = messages[0][&#39;content&#39;]|trim %} {%- set &quot; &#39;messages = messages[1:] %} {%- else %} {{- raise_exception(&quot;Cannot put tools in the first user &#39; &quot;message when there&#39;s no first user message! &quot;) }} {%- endif %} {{- &quot; &quot;&#39;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; n n&#39; -}} {{- &quot;Given the following functions, please &quot; &#39;respond with a JSON for a function call &quot; }} {{- &quot;with its proper arguments that best answers the &#39; &#39;given prompt. n n&quot; }} {{- &#39;Respond in the format {&quot;name&quot;: function name, &quot;parameters&quot;: &#39; &#39;dictionary of argument name and its value}. &#39; }} {{- &quot;Do not use variables. n n&quot; }} {%- for t in &#39; &#39;tools %} {{- t | tojson(indent=4) }} {{- &quot; n n&quot; }} {%- endfor %} {{- first_user_message + &#39; &quot; &quot;&lt;|eot_id|&gt; &quot;}} {%- endif %} {%- for message in messages %} {%- if not (message.role == &#39;ipython&#39; &quot; &quot;or message.role == &#39;tool&#39; or &#39;tool_calls&#39; in message) %} {{- &#39;&lt;|start_header_id|&gt;&#39; + message[&#39;role&#39;]&quot; &quot; + &#39;&lt;|end_header_id|&gt; n n&#39;+ message[&#39;content&#39;] | trim + &#39;&lt;|eot_id|&gt;&#39; }} {%- elif &#39;tool_calls&#39; in &quot; &#39;message %} {%- if not message.tool_calls|length == 1 %} {{- raise_exception(&quot;This model only &#39; &#39;supports single tool-calls at once!&quot;) }} {%- endif %} {%- set tool_call = message.tool_calls[0]&#39; &quot;.function %} {%- if builtin_tools is defined and tool_call.name in builtin_tools %} {{- &quot; &quot;&#39;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; n n&#39; -}} {{- &quot;&lt;|python_tag|&gt; &quot; + tool_call.name + &quot; &#39;&quot;.call(&quot; }} {%- for arg_name, arg_val in tool_call.arguments | items %} {{- arg_name + &#39;=&quot; &#39; + &#39; &#39;arg_val + &#39;&quot; &#39; }} {%- if not loop.last %} {{- &quot;, &quot; }} {%- endif %} {%- endfor %} {{- &quot;)&quot; }} {%- &#39; &quot;else %} {{- &#39;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; n n&#39; -}} {{- &#39;{ &quot;name &quot;: &quot;&#39; + &quot; &#39;tool_call.name + &#39;&quot;, &#39; }} {{- &#39;&quot;parameters&quot;: &#39; }} {{- tool_call.arguments | tojson }} {{- &quot;}&quot; &#39; &quot;}} {%- endif %} {%- if builtin_tools is defined %} {#- This means we&#39;re in ipython mode #} {{- &quot; &#39;&quot;&lt;|eom_id|&gt;&quot; }} {%- else %} {{- &quot;&lt;|eot_id|&gt;&quot; }} {%- endif %} {%- elif message.role == &quot;tool&quot; &#39; &#39;or message.role == &quot;ipython&quot; %} {{- &quot;&lt;|start_header_id|&gt;ipython&lt;|end_header_id|&gt; n n&quot; }} {%- &#39; &quot;if message.content is mapping or message.content is iterable %} {{- message.content | tojson }} {%- &quot; &#39;else %} {{- message.content }} {%- endif %} {{- &quot;&lt;|eot_id|&gt;&quot; }} {%- endif %} {%- endfor %} {%- if &#39; &quot;add_generation_prompt %} {{- &#39;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; n n&#39; }} {%- endif %} &quot; ) prompt_assembler = ( PromptAssembler() .setInputCol(&quot;messages&quot;) .setOutputCol(&quot;prompt&quot;) .setChatTemplate(template) ) prompt_assembler.transform(df).select(&quot;prompt.result&quot;).show(truncate=False) +-+ |result | +-+ |[&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt; n nYou are a helpful assistant.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; n nHello there, how can I help you?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; n nI need help with organizing my room.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; n n]| +-+ // Batches (whole conversations) of arrays of messages val data: Seq[Seq[(String, String)]] = Seq( Seq( (&quot;system&quot;, &quot;You are a helpful assistant.&quot;), (&quot;assistant&quot;, &quot;Hello there, how can I help you?&quot;), (&quot;user&quot;, &quot;I need help with organizing my room.&quot;))) val dataDF = data.toDF(&quot;messages&quot;) // llama3.1 val template = &quot;{{- bos_token }} {%- if custom_tools is defined %} {%- set tools = custom_tools %} {%- &quot; + &quot;endif %} {%- if not tools_in_user_message is defined %} {%- set tools_in_user_message = true %} {%- &quot; + &quot;endif %} {%- if not date_string is defined %} {%- set date_string = &quot;26 Jul 2024 &quot; %} {%- endif %} &quot; + &quot;{%- if not tools is defined %} {%- set tools = none %} {%- endif %} {#- This block extracts the &quot; + &quot;system message, so we can slot it into the right place. #} {%- if messages[0][&#39;role&#39;] == &#39;system&#39; %}&quot; + &quot; {%- set system_message = messages[0][&#39;content&#39;]|trim %} {%- set messages = messages[1:] %} {%- else&quot; + &quot; %} {%- set system_message = &quot; &quot; %} {%- endif %} {#- System message + builtin tools #} {{- &quot; + &quot; &quot;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt; n n &quot; }} {%- if builtin_tools is defined or tools is &quot; + &quot;not none %} {{- &quot;Environment: ipython n &quot; }} {%- endif %} {%- if builtin_tools is defined %} {{- &quot; + &quot; &quot;Tools: &quot; + builtin_tools | reject(&#39;equalto&#39;, &#39;code_interpreter&#39;) | join( &quot;, &quot;) + &quot; n n &quot;}} &quot; + &quot;{%- endif %} {{- &quot;Cutting Knowledge Date: December 2023 n &quot; }} {{- &quot;Today Date: &quot; + date_string &quot; + &quot;+ &quot; n n &quot; }} {%- if tools is not none and not tools_in_user_message %} {{- &quot;You have access to &quot; + &quot;the following functions. To call a function, please respond with JSON for a function call. &quot; }} {{- &quot; + &quot;&#39;Respond in the format { &quot;name &quot;: function name, &quot;parameters &quot;: dictionary of argument name and its&quot; + &quot; value}.&#39; }} {{- &quot;Do not use variables. n n &quot; }} {%- for t in tools %} {{- t | tojson(indent=4) &quot; + &quot;}} {{- &quot; n n &quot; }} {%- endfor %} {%- endif %} {{- system_message }} {{- &quot;&lt;|eot_id|&gt; &quot; }} {#- &quot; + &quot;Custom tools are passed in a user message with some extra guidance #} {%- if tools_in_user_message &quot; + &quot;and not tools is none %} {#- Extract the first user message so we can plug it in here #} {%- if &quot; + &quot;messages | length != 0 %} {%- set first_user_message = messages[0][&#39;content&#39;]|trim %} {%- set &quot; + &quot;messages = messages[1:] %} {%- else %} {{- raise_exception( &quot;Cannot put tools in the first user &quot; + &quot;message when there&#39;s no first user message! &quot;) }} {%- endif %} {{- &quot; + &quot;&#39;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; n n&#39; -}} {{- &quot;Given the following functions, please &quot; + &quot;respond with a JSON for a function call &quot; }} {{- &quot;with its proper arguments that best answers the &quot; + &quot;given prompt. n n &quot; }} {{- &#39;Respond in the format { &quot;name &quot;: function name, &quot;parameters &quot;: &quot; + &quot;dictionary of argument name and its value}.&#39; }} {{- &quot;Do not use variables. n n &quot; }} {%- for t in &quot; + &quot;tools %} {{- t | tojson(indent=4) }} {{- &quot; n n &quot; }} {%- endfor %} {{- first_user_message + &quot; + &quot; &quot;&lt;|eot_id|&gt; &quot;}} {%- endif %} {%- for message in messages %} {%- if not (message.role == &#39;ipython&#39; &quot; + &quot;or message.role == &#39;tool&#39; or &#39;tool_calls&#39; in message) %} {{- &#39;&lt;|start_header_id|&gt;&#39; + message[&#39;role&#39;]&quot; + &quot; + &#39;&lt;|end_header_id|&gt; n n&#39;+ message[&#39;content&#39;] | trim + &#39;&lt;|eot_id|&gt;&#39; }} {%- elif &#39;tool_calls&#39; in &quot; + &quot;message %} {%- if not message.tool_calls|length == 1 %} {{- raise_exception( &quot;This model only &quot; + &quot;supports single tool-calls at once! &quot;) }} {%- endif %} {%- set tool_call = message.tool_calls[0]&quot; + &quot;.function %} {%- if builtin_tools is defined and tool_call.name in builtin_tools %} {{- &quot; + &quot;&#39;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; n n&#39; -}} {{- &quot;&lt;|python_tag|&gt; &quot; + tool_call.name + &quot; + &quot; &quot;.call( &quot; }} {%- for arg_name, arg_val in tool_call.arguments | items %} {{- arg_name + &#39;= &quot;&#39; + &quot; + &quot;arg_val + &#39; &quot;&#39; }} {%- if not loop.last %} {{- &quot;, &quot; }} {%- endif %} {%- endfor %} {{- &quot;) &quot; }} {%- &quot; + &quot;else %} {{- &#39;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; n n&#39; -}} {{- &#39;{ &quot;name &quot;: &quot;&#39; + &quot; + &quot;tool_call.name + &#39; &quot;, &#39; }} {{- &#39; &quot;parameters &quot;: &#39; }} {{- tool_call.arguments | tojson }} {{- &quot;} &quot; &quot; + &quot;}} {%- endif %} {%- if builtin_tools is defined %} {#- This means we&#39;re in ipython mode #} {{- &quot; + &quot; &quot;&lt;|eom_id|&gt; &quot; }} {%- else %} {{- &quot;&lt;|eot_id|&gt; &quot; }} {%- endif %} {%- elif message.role == &quot;tool &quot; &quot; + &quot;or message.role == &quot;ipython &quot; %} {{- &quot;&lt;|start_header_id|&gt;ipython&lt;|end_header_id|&gt; n n &quot; }} {%- &quot; + &quot;if message.content is mapping or message.content is iterable %} {{- message.content | tojson }} {%- &quot; + &quot;else %} {{- message.content }} {%- endif %} {{- &quot;&lt;|eot_id|&gt; &quot; }} {%- endif %} {%- endfor %} {%- if &quot; + &quot;add_generation_prompt %} {{- &#39;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; n n&#39; }} {%- endif %} &quot; val promptAssembler = new PromptAssembler() .setInputCol(&quot;messages&quot;) .setOutputCol(&quot;prompt&quot;) .setChatTemplate(template) promptAssembler.transform(dataDF).select(&quot;prompt.result&quot;).show(truncate = false) +-+ |result | +-+ |[&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt; n nYou are a helpful assistant.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; n nHello there, how can I help you?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt; n nI need help with organizing my room.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt; n n]| +-+ RecursiveTokenizer ApproachModel Tokenizes raw text recursively based on a handful of definable rules. Unlike the Tokenizer, the RecursiveTokenizer operates based on these array string parameters only: prefixes: Strings that will be split when found at the beginning of token. suffixes: Strings that will be split when found at the end of token. infixes: Strings that will be split when found at the middle of token. whitelist: Whitelist of strings not to split For extended examples of usage, see the Examples and the TokenizerTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: RecursiveTokenizer Scala API: RecursiveTokenizer Source: RecursiveTokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = RecursiveTokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer ]) data = spark.createDataFrame([[&quot;One, after the Other, (and) again. PO, QAM,&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;token.result&quot;).show(truncate=False) ++ |result | ++ |[One, ,, after, the, Other, ,, (, and, ), again, ., PO, ,, QAM, ,]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.RecursiveTokenizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new RecursiveTokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer )) val data = Seq(&quot;One, after the Other, (and) again. PO, QAM,&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;token.result&quot;).show(false) ++ |result | ++ |[One, ,, after, the, Other, ,, (, and, ), again, ., PO, ,, QAM, ,]| ++ Instantiated model of the RecursiveTokenizer. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: RecursiveTokenizerModel Scala API: RecursiveTokenizerModel Source: RecursiveTokenizerModel RegexMatcher ApproachModel Uses rules to match a set of regular expressions and associate them with a provided identifier. A rule consists of a regex pattern and an identifier, delimited by a character of choice. An example could be &quot; d{4} / d d / d d,date&quot; which will match strings like &quot;1970/01/01&quot; to the identifier &quot;date&quot;. Rules must be provided by either setRules (followed by setDelimiter) or an external file. To use an external file, a dictionary of predefined regular expressions must be provided with setExternalRules. The dictionary can be set as a delimited text file. Pretrained pipelines are available for this module, see Pipelines. For extended examples of usage, see the Examples and the RegexMatcherTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: CHUNK Python API: RegexMatcher Scala API: RegexMatcher Source: RegexMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the `rules.txt` has the form of # # the s w+, followed by &#39;the&#39; # ceremonies, ceremony # # where each regex is separated by the identifier by `&quot;,&quot;` documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentence = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) regexMatcher = RegexMatcher() .setExternalRules(&quot;src/test/resources/regex-matcher/rules.txt&quot;, &quot;,&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;regex&quot;) .setStrategy(&quot;MATCH_ALL&quot;) pipeline = Pipeline().setStages([documentAssembler, sentence, regexMatcher]) data = spark.createDataFrame([[ &quot;My first sentence with the first rule. This is my second sentence with ceremonies rule.&quot; ]]).toDF(&quot;text&quot;) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(regex) as result&quot;).show(truncate=False) +--+ |result | +--+ |[chunk, 23, 31, the first, [identifier -&gt; followed by &#39;the&#39;, sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 71, 80, ceremonies, [identifier -&gt; ceremony, sentence -&gt; 1, chunk -&gt; 0], []] | +--+ // In this example, the `rules.txt` has the form of // // the s w+, followed by &#39;the&#39; // ceremonies, ceremony // // where each regex is separated by the identifier by `&quot;,&quot;` import ResourceHelper.spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.RegexMatcher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val regexMatcher = new RegexMatcher() .setExternalRules(&quot;src/test/resources/regex-matcher/rules.txt&quot;, &quot;,&quot;) .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;regex&quot;) .setStrategy(&quot;MATCH_ALL&quot;) val pipeline = new Pipeline().setStages(Array(documentAssembler, sentence, regexMatcher)) val data = Seq( &quot;My first sentence with the first rule. This is my second sentence with ceremonies rule.&quot; ).toDF(&quot;text&quot;) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(regex) as result&quot;).show(false) +--+ |result | +--+ |[chunk, 23, 31, the first, [identifier -&gt; followed by &#39;the&#39;, sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 71, 80, ceremonies, [identifier -&gt; ceremony, sentence -&gt; 1, chunk -&gt; 0], []] | +--+ Instantiated model of the RegexMatcher. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT Output Annotator Type: CHUNK Python API: RegexMatcherModel Scala API: RegexMatcherModel Source: RegexMatcherModel RegexTokenizer A tokenizer that splits text by a regex pattern. The pattern needs to be set with setPattern and this sets the delimiting pattern or how the tokens should be split. By default this pattern is s+ which means that tokens should be split by 1 or more whitespace characters. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: RegexTokenizer Scala API: RegexTokenizer Source: RegexTokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) regexTokenizer = RegexTokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;regexToken&quot;) .setToLowercase(True) .setPattern(&quot; s+&quot;) pipeline = Pipeline().setStages([ documentAssembler, regexTokenizer ]) data = spark.createDataFrame([[&quot;This is my first sentence. nThis is my second.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;regexToken.result&quot;).show(truncate=False) +-+ |result | +-+ |[this, is, my, first, sentence., this, is, my, second.]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.RegexTokenizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val regexTokenizer = new RegexTokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;regexToken&quot;) .setToLowercase(true) .setPattern(&quot; s+&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, regexTokenizer )) val data = Seq(&quot;This is my first sentence. nThis is my second.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;regexToken.result&quot;).show(false) +-+ |result | +-+ |[this, is, my, first, sentence., this, is, my, second.]| +-+ SentenceDetector Annotator that detects sentence boundaries using regular expressions. The following characters are checked as sentence boundaries: Lists (“(i), (ii)”, “(a), (b)”, “1., 2.”) Numbers Abbreviations Punctuations Multiple Periods Geo-Locations/Coordinates (“N°. 1026.253.553.”) Ellipsis (“…”) In-between punctuations Quotation marks Exclamation Points Basic Breakers (“.”, “;”) For the explicit regular expressions used for detection, refer to source of PragmaticContentFormatter. To add additional custom bounds, the parameter customBounds can be set with an array: val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) .setCustomBounds(Array(&quot; n n&quot;)) If only the custom bounds should be used, then the parameter useCustomBoundsOnly should be set to true. Each extracted sentence can be returned in an Array or exploded to separate rows, if explodeSentences is set to true. For extended examples of usage, see the Examples. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: SentenceDetector Scala API: SentenceDetector Source: SentenceDetector Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setCustomBounds([&quot; n n&quot;]) pipeline = Pipeline().setStages([ documentAssembler, sentence ]) data = spark.createDataFrame([[&quot;This is my first sentence. This my second. How about a third?&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(sentence) as sentences&quot;).show(truncate=False) ++ |sentences | ++ |[document, 0, 25, This is my first sentence., [sentence -&gt; 0], []]| |[document, 27, 41, This my second., [sentence -&gt; 1], []] | |[document, 43, 60, How about a third?, [sentence -&gt; 2], []] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) .setCustomBounds(Array(&quot; n n&quot;)) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence )) val data = Seq(&quot;This is my first sentence. This my second. How about a third?&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(sentence) as sentences&quot;).show(false) ++ |sentences | ++ |[document, 0, 25, This is my first sentence., [sentence -&gt; 0], []]| |[document, 27, 41, This my second., [sentence -&gt; 1], []] | |[document, 43, 60, How about a third?, [sentence -&gt; 2], []] | ++ SentenceDetectorDL ApproachModel Trains an annotator that detects sentence boundaries using a deep learning approach. For pretrained models see SentenceDetectorDLModel. Currently, only the CNN model is supported for training, but in the future the architecture of the model can be set with setModelArchitecture. The default model &quot;cnn&quot; is based on the paper Deep-EOS: General-Purpose Neural Networks for Sentence Boundary Detection (2020, Stefan Schweter, Sajawel Ahmed) using a CNN architecture. We also modified the original implementation a little bit to cover broken sentences and some impossible end of line chars. Each extracted sentence can be returned in an Array or exploded to separate rows, if explodeSentences is set to true. For extended examples of usage, see the Examples and the SentenceDetectorDLSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: SentenceDetectorDLApproach Scala API: SentenceDetectorDLApproach Source: SentenceDetectorDLApproach Show Example PythonScala # The training process needs data, where each data point is a sentence. # In this example the `train.txt` file has the form of # # ... # Slightly more moderate language would make our present situation – namely the lack of progress – a little easier. # His political successors now have great responsibilities to history and to the heritage of values bequeathed to them by Nelson Mandela. # ... # # where each line is one sentence. # Training can then be started like so: import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline trainingData = spark.read.text(&quot;train.txt&quot;).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLApproach() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) .setEpochsNumber(100) pipeline = Pipeline().setStages([documentAssembler, sentenceDetector]) model = pipeline.fit(trainingData) // The training process needs data, where each data point is a sentence. // In this example the `train.txt` file has the form of // // ... // Slightly more moderate language would make our present situation – namely the lack of progress – a little easier. // His political successors now have great responsibilities to history and to the heritage of values bequeathed to them by Nelson Mandela. // ... // // where each line is one sentence. // Training can then be started like so: import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLApproach import org.apache.spark.ml.Pipeline val trainingData = spark.read.text(&quot;train.txt&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetectorDLApproach() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentences&quot;) .setEpochsNumber(100) val pipeline = new Pipeline().setStages(Array(documentAssembler, sentenceDetector)) val model = pipeline.fit(trainingData) Annotator that detects sentence boundaries using a deep learning approach. Instantiated Model of the SentenceDetectorDLApproach. Detects sentence boundaries using a deep learning approach. Pretrained models can be loaded with pretrained of the companion object: val sentenceDL = SentenceDetectorDLModel.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentencesDL&quot;) The default model is &quot;sentence_detector_dl&quot;, if no name is provided. For available pretrained models please see the Models Hub. Each extracted sentence can be returned in an Array or exploded to separate rows, if explodeSentences is set to true. For extended examples of usage, see the Examples and the SentenceDetectorDLSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: SentenceDetectorDLModel Scala API: SentenceDetectorDLModel Source: SentenceDetectorDLModel SentenceEmbeddings Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols). This can be configured with setPoolingStrategy, which either be &quot;AVERAGE&quot; or &quot;SUM&quot;. For more extended examples see the Examples. and the SentenceEmbeddingsTestSpec. TIP: Here is how you can explode and convert these embeddings into Vectors or what’s known as Feature column so it can be used in Spark ML regression or clustering functions: PythonScala from org.apache.spark.ml.linal import Vector, Vectors from pyspark.sql.functions import udf # Let&#39;s create a UDF to take array of embeddings and output Vectors @udf(Vector) def convertToVectorUDF(matrix): return Vectors.dense(matrix.toArray.map(_.toDouble)) # Now let&#39;s explode the sentence_embeddings column and have a new feature column for Spark ML pipelineDF.select(explode(&quot;sentence_embeddings.embeddings&quot;).as(&quot;sentence_embedding&quot;)) .withColumn(&quot;features&quot;, convertToVectorUDF(&quot;sentence_embedding&quot;)) import org.apache.spark.ml.linalg.{Vector, Vectors} // Let&#39;s create a UDF to take array of embeddings and output Vectors val convertToVectorUDF = udf((matrix : Seq[Float]) =&gt; { Vectors.dense(matrix.toArray.map(_.toDouble)) }) // Now let&#39;s explode the sentence_embeddings column and have a new feature column for Spark ML pipelineDF.select(explode($&quot;sentence_embeddings.embeddings&quot;).as(&quot;sentence_embedding&quot;)) .withColumn(&quot;features&quot;, convertToVectorUDF($&quot;sentence_embedding&quot;)) Input Annotator Types: DOCUMENT, WORD_EMBEDDINGS Output Annotator Type: SENTENCE_EMBEDDINGS Note: If you choose document as your input for Tokenizer, WordEmbeddings/BertEmbeddings, and SentenceEmbeddings then it averages/sums all the embeddings into one array of embeddings. However, if you choose sentence as inputCols then for each sentence SentenceEmbeddings generates one array of embeddings. Python API: SentenceEmbeddings Scala API: SentenceEmbeddings Source: SentenceEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsSentence = SentenceEmbeddings() .setInputCols([&quot;document&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) .setCleanAnnotations(False) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings, embeddingsSentence, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80) +--+ | result| +--+ |[-0.22093398869037628,0.25130119919776917,0.41810303926467896,-0.380883991718...| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsSentence = new SentenceEmbeddings() .setInputCols(Array(&quot;document&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;sentence_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsSentence, embeddingsFinisher )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80) +--+ | result| +--+ |[-0.22093398869037628,0.25130119919776917,0.41810303926467896,-0.380883991718...| +--+ SentimentDL ApproachModel Trains a SentimentDL, an annotator for multi-class sentiment analysis. In natural language processing, sentiment analysis is the task of classifying the affective state or subjective view of a text. A common example is if either a product review or tweet can be interpreted positively or negatively. For the instantiated/pretrained models, see SentimentDLModel. Notes: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. So positive sentiment can be expressed as either &quot;positive&quot; or 0, negative sentiment as &quot;negative&quot; or 1. UniversalSentenceEncoder, BertSentenceEmbeddings, SentenceEmbeddings or other sentence based embeddings can be used Setting a test dataset to monitor model metrics can be done with .setTestDataset. The method expects a path to a parquet file containing a dataframe that has the same required columns as the training dataframe. The pre-processing steps for the training dataframe should also be applied to the test dataframe. The following example will show how to create the test dataset: val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val preProcessingPipeline = new Pipeline().setStages(Array(documentAssembler, embeddings)) val Array(train, test) = data.randomSplit(Array(0.8, 0.2)) preProcessingPipeline .fit(test) .transform(test) .write .mode(&quot;overwrite&quot;) .parquet(&quot;test_data&quot;) val classifier = new SentimentDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sentiment&quot;) .setLabelColumn(&quot;label&quot;) .setTestDataset(&quot;test_data&quot;) For extended examples of usage, see the Examples and the SentimentDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: SentimentDLApproach Scala API: SentimentDLApproach Source: SentimentDLApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, `sentiment.csv` is in the form # # text,label # This movie is the best movie I have watched ever! In my opinion this movie can win an award.,0 # This was a terrible movie! The acting was bad really bad!,1 # # The model can then be trained with smallCorpus = spark.read.option(&quot;header&quot;, &quot;True&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) docClassifier = SentimentDLApproach() .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCol(&quot;sentiment&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(32) .setMaxEpochs(1) .setLr(5e-3) .setDropout(0.5) pipeline = Pipeline() .setStages( [ documentAssembler, useEmbeddings, docClassifier ] ) pipelineModel = pipeline.fit(smallCorpus) // In this example, `sentiment.csv` is in the form // // text,label // This movie is the best movie I have watched ever! In my opinion this movie can win an award.,0 // This was a terrible movie! The acting was bad really bad!,1 // // The model can then be trained with import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.UniversalSentenceEncoder import com.johnsnowlabs.nlp.annotators.classifier.dl.{SentimentDLApproach, SentimentDLModel} import org.apache.spark.ml.Pipeline val smallCorpus = spark.read.option(&quot;header&quot;, &quot;true&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val docClassifier = new SentimentDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sentiment&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(32) .setMaxEpochs(1) .setLr(5e-3f) .setDropout(0.5f) val pipeline = new Pipeline() .setStages( Array( documentAssembler, useEmbeddings, docClassifier ) ) val pipelineModel = pipeline.fit(smallCorpus) SentimentDL, an annotator for multi-class sentiment analysis. In natural language processing, sentiment analysis is the task of classifying the affective state or subjective view of a text. A common example is if either a product review or tweet can be interpreted positively or negatively. This is the instantiated model of the SentimentDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val sentiment = SentimentDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sentiment&quot;) The default model is &quot;sentimentdl_use_imdb&quot;, if no name is provided. It is english sentiment analysis trained on the IMDB dataset. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the SentimentDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: SentimentDLModel Scala API: SentimentDLModel Source: SentimentDLModel SentimentDetector ApproachModel Trains a rule based sentiment detector, which calculates a score based on predefined keywords. A dictionary of predefined sentiment keywords must be provided with setDictionary, where each line is a word delimited to its class (either positive or negative). The dictionary can be set as a delimited text file. By default, the sentiment score will be assigned labels &quot;positive&quot; if the score is &gt;= 0, else &quot;negative&quot;. To retrieve the raw sentiment scores, enableScore needs to be set to true. For extended examples of usage, see the Examples and the SentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: SentimentDetector Scala API: SentimentDetector Source: SentimentDetector Show Example PythonScala # In this example, the dictionary `default-sentiment-dict.txt` has the form of # # ... # cool,positive # superb,positive # bad,negative # uninspired,negative # ... # # where each sentiment keyword is delimited by `&quot;,&quot;`. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = Lemmatizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) sentimentDetector = SentimentDetector() .setInputCols([&quot;lemma&quot;, &quot;document&quot;]) .setOutputCol(&quot;sentimentScore&quot;) .setDictionary(&quot;default-sentiment-dict.txt&quot;, &quot;,&quot;, ReadAs.TEXT) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, lemmatizer, sentimentDetector, ]) data = spark.createDataFrame([ [&quot;The staff of the restaurant is nice&quot;], [&quot;I recommend others to avoid because it is too expensive&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;sentimentScore.result&quot;).show(truncate=False) +-+ # ++ for enableScore set to True |result | # |result| +-+ # ++ |[positive]| # |[1.0] | |[negative]| # |[-2.0]| +-+ # ++ // In this example, the dictionary `default-sentiment-dict.txt` has the form of // // ... // cool,positive // superb,positive // bad,negative // uninspired,negative // ... // // where each sentiment keyword is delimited by `&quot;,&quot;`. import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotators.Lemmatizer import com.johnsnowlabs.nlp.annotators.sda.pragmatic.SentimentDetector import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val lemmatizer = new Lemmatizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) val sentimentDetector = new SentimentDetector() .setInputCols(&quot;lemma&quot;, &quot;document&quot;) .setOutputCol(&quot;sentimentScore&quot;) .setDictionary(&quot;src/test/resources/sentiment-corpus/default-sentiment-dict.txt&quot;, &quot;,&quot;, ReadAs.TEXT) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, lemmatizer, sentimentDetector, )) val data = Seq( &quot;The staff of the restaurant is nice&quot;, &quot;I recommend others to avoid because it is too expensive&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;sentimentScore.result&quot;).show(false) +-+ // ++ for enableScore set to true |result | // |result| +-+ // ++ |[positive]| // |[1.0] | |[negative]| // |[-2.0]| +-+ // ++ Rule based sentiment detector, which calculates a score based on predefined keywords. This is the instantiated model of the SentimentDetector. For training your own model, please see the documentation of that class. A dictionary of predefined sentiment keywords must be provided with setDictionary, where each line is a word delimited to its class (either positive or negative). The dictionary can be set as a delimited text file. By default, the sentiment score will be assigned labels &quot;positive&quot; if the score is &gt;= 0, else &quot;negative&quot;. To retrieve the raw sentiment scores, enableScore needs to be set to true. For extended examples of usage, see the Examples and the SentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: SentimentDetectorModel Scala API: SentimentDetectorModel Source: SentimentDetectorModel Stemmer Returns hard-stems out of words with the objective of retrieving the meaningful part of the word. For extended examples of usage, see the Examples. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: Stemmer Scala API: Stemmer Source: Stemmer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) stemmer = Stemmer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;stem&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, stemmer ]) data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;stem.result&quot;).show(truncate = False) +-+ |result | +-+ |[peter, piper, employe, ar, pick, peck, of, pickl, pepper, .]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Stemmer, Tokenizer} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val stemmer = new Stemmer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;stem&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, stemmer )) val data = Seq(&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;stem.result&quot;).show(truncate = false) +-+ |result | +-+ |[peter, piper, employe, ar, pick, peck, of, pickl, pepper, .]| +-+ StopWordsCleaner This annotator takes a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences. By default, it uses stop words from MLlibs StopWordsRemover. Stop words can also be defined by explicitly setting them with setStopWords(value: Array[String]) or loaded from pretrained models using pretrained of its companion object. val stopWords = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) // will load the default pretrained model `&quot;stopwords_en&quot;`. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and StopWordsCleanerTestSpec. NOTE: If you need to setStopWords from a text file, you can first read and convert it into an array of string as follows. PythonScala # your stop words text file, each line is one stop word stopwords = sc.textFile(&quot;/tmp/stopwords/english.txt&quot;).collect() # simply use it in StopWordsCleaner stopWordsCleaner = StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setStopWords(stopwords) .setCaseSensitive(False) # or you can use pretrained models for StopWordsCleaner stopWordsCleaner = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) // your stop words text file, each line is one stop word val stopwords = sc.textFile(&quot;/tmp/stopwords/english.txt&quot;).collect() // simply use it in StopWordsCleaner val stopWordsCleaner = new StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setStopWords(stopwords) .setCaseSensitive(false) // or you can use pretrained models for StopWordsCleaner val stopWordsCleaner = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: StopWordsCleaner Scala API: StopWordsCleaner Source: StopWordsCleaner Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) stopWords = StopWordsCleaner() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, stopWords ]) data = spark.createDataFrame([ [&quot;This is my first sentence. This is my second.&quot;], [&quot;This is my third sentence. This is my forth.&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;cleanTokens.result&quot;).show(truncate=False) +-+ |result | +-+ |[first, sentence, ., second, .]| |[third, sentence, ., forth, .] | +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.StopWordsCleaner import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val stopWords = new StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, stopWords )) val data = Seq( &quot;This is my first sentence. This is my second.&quot;, &quot;This is my third sentence. This is my forth.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;cleanTokens.result&quot;).show(false) +-+ |result | +-+ |[first, sentence, ., second, .]| |[third, sentence, ., forth, .] | +-+ SymmetricDelete Spellchecker ApproachModel Trains a Symmetric Delete spelling correction algorithm. Retrieves tokens and utilizes distance metrics to compute possible derived words. Inspired by SymSpell. For instantiated/pretrained models, see SymmetricDeleteModel. See SymmetricDeleteModelTestSpec for further reference. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: SymmetricDeleteApproach Scala API: SymmetricDeleteApproach Source: SymmetricDeleteApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the dictionary `&quot;words.txt&quot;` has the form of # # ... # gummy # gummic # gummier # gummiest # gummiferous # ... # # This dictionary is then set to be the basis of the spell checker. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) spellChecker = SymmetricDeleteApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker ]) pipelineModel = pipeline.fit(trainingData) // In this example, the dictionary `&quot;words.txt&quot;` has the form of // // ... // gummy // gummic // gummier // gummiest // gummiferous // ... // // This dictionary is then set to be the basis of the spell checker. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val spellChecker = new SymmetricDeleteApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker )) val pipelineModel = pipeline.fit(trainingData) Symmetric Delete spelling correction algorithm. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster (than the standard approach with deletes + transposes + replaces + inserts) and language independent. Inspired by SymSpell. Pretrained models can be loaded with pretrained of the companion object: val spell = SymmetricDeleteModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) The default model is &quot;spellcheck_sd&quot;, if no name is provided. For available pretrained models please see the Models Hub. See SymmetricDeleteModelTestSpec for further reference. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: SymmetricDeleteModel Scala API: SymmetricDeleteModel Source: SymmetricDeleteModel TextMatcher ApproachModel Annotator to match exact phrases (by token) provided in a file against a Document. A text file of predefined phrases must be provided with setEntities. For extended examples of usage, see the Examples and the TextMatcherTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: TextMatcher Scala API: TextMatcher Source: TextMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the entities file is of the form # # ... # dolore magna aliqua # lorem ipsum dolor. sit # laborum # ... # # where each line represents an entity phrase to be extracted. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) data = spark.createDataFrame([[&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;]]).toDF(&quot;text&quot;) entityExtractor = TextMatcher() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setEntities(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(False) pipeline = Pipeline().setStages([documentAssembler, tokenizer, entityExtractor]) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity) as result&quot;).show(truncate=False) ++ |result | ++ |[chunk, 6, 24, dolore magna aliqua, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 27, 48, Lorem ipsum dolor. sit, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 1], []]| |[chunk, 53, 59, laborum, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 2], []] | ++ // In this example, the entities file is of the form // // ... // dolore magna aliqua // lorem ipsum dolor. sit // laborum // ... // // where each line represents an entity phrase to be extracted. import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.TextMatcher import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val data = Seq(&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;).toDF(&quot;text&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setEntities(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(false) .setTokenizer(tokenizer.fit(data)) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer, entityExtractor)) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity) as result&quot;).show(false) ++ |result | ++ |[chunk, 6, 24, dolore magna aliqua, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 27, 48, Lorem ipsum dolor. sit, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 1], []]| |[chunk, 53, 59, laborum, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 2], []] | ++ Instantiated model of the TextMatcher. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: TextMatcherModel Scala API: TextMatcherModel Source: TextMatcherModel Token2Chunk Converts TOKEN type Annotations to CHUNK type. This can be useful if a entities have been already extracted as TOKEN and following annotators require CHUNK types. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: Token2Chunk Scala API: Token2Chunk Source: Token2Chunk Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) token2chunk = Token2Chunk() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;chunk&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, token2chunk ]) data = spark.createDataFrame([[&quot;One Two Three Four&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(truncate=False) ++ |result | ++ |[chunk, 0, 2, One, [sentence -&gt; 0], []] | |[chunk, 4, 6, Two, [sentence -&gt; 0], []] | |[chunk, 8, 12, Three, [sentence -&gt; 0], []]| |[chunk, 14, 17, Four, [sentence -&gt; 0], []]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.{Token2Chunk, Tokenizer} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val token2chunk = new Token2Chunk() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;chunk&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, token2chunk )) val data = Seq(&quot;One Two Three Four&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(false) ++ |result | ++ |[chunk, 0, 2, One, [sentence -&gt; 0], []] | |[chunk, 4, 6, Two, [sentence -&gt; 0], []] | |[chunk, 8, 12, Three, [sentence -&gt; 0], []]| |[chunk, 14, 17, Four, [sentence -&gt; 0], []]| ++ TokenAssembler This transformer reconstructs a DOCUMENT type annotation from tokens, usually after these have been normalized, lemmatized, normalized, spell checked, etc, in order to use this document annotation in further annotators. Requires DOCUMENT and TOKEN type annotations as input. For more extended examples on document pre-processing see the Examples. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: DOCUMENT Python API: TokenAssembler Scala API: TokenAssembler Source: TokenAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # First, the text is tokenized and cleaned documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) .setLowercase(False) stopwordsCleaner = StopWordsCleaner() .setInputCols([&quot;normalized&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) # Then the TokenAssembler turns the cleaned tokens into a `DOCUMENT` type structure. tokenAssembler = TokenAssembler() .setInputCols([&quot;sentences&quot;, &quot;cleanTokens&quot;]) .setOutputCol(&quot;cleanText&quot;) data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;]]) .toDF(&quot;text&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, normalizer, stopwordsCleaner, tokenAssembler ]).fit(data) result = pipeline.transform(data) result.select(&quot;cleanText&quot;).show(truncate=False) ++ |cleanText | ++ |0, 80, Spark NLP opensource text processing library advanced natural language processing, [sentence -&gt; 0], []| ++ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.{Normalizer, StopWordsCleaner} import com.johnsnowlabs.nlp.TokenAssembler import org.apache.spark.ml.Pipeline // First, the text is tokenized and cleaned val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) .setLowercase(false) val stopwordsCleaner = new StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) // Then the TokenAssembler turns the cleaned tokens into a `DOCUMENT` type structure. val tokenAssembler = new TokenAssembler() .setInputCols(&quot;sentences&quot;, &quot;cleanTokens&quot;) .setOutputCol(&quot;cleanText&quot;) val data = Seq(&quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;) .toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, normalizer, stopwordsCleaner, tokenAssembler )).fit(data) val result = pipeline.transform(data) result.select(&quot;cleanText&quot;).show(false) ++ |cleanText | ++ |[[document, 0, 80, Spark NLP opensource text processing library advanced natural language processing, [sentence -&gt; 0], []]]| ++ Tokenizer ApproachModel Tokenizes raw text in document type columns into TokenizedSentence . This class represents a non fitted tokenizer. Fitting it will cause the internal RuleFactory to construct the rules for tokenizing from the input configuration. Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. For extended examples of usage see the Examples and Tokenizer test class Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Note: All these APIs receive regular expressions so please make sure that you escape special characters according to Java conventions. Python API: Tokenizer Scala API: Tokenizer Source: Tokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline data = spark.createDataFrame([[&quot;I&#39;d like to say we didn&#39;t expect that. Jane&#39;s boyfriend.&quot;]]).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) tokenizer = Tokenizer().setInputCols([&quot;document&quot;]).setOutputCol(&quot;token&quot;).fit(data) pipeline = Pipeline().setStages([documentAssembler, tokenizer]).fit(data) result = pipeline.transform(data) result.selectExpr(&quot;token.result&quot;).show(truncate=False) +--+ |output | +--+ |[I&#39;d, like, to, say, we, didn&#39;t, expect, that, ., Jane&#39;s, boyfriend, .]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import org.apache.spark.ml.Pipeline val data = Seq(&quot;I&#39;d like to say we didn&#39;t expect that. Jane&#39;s boyfriend.&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer().setInputCols(&quot;document&quot;).setOutputCol(&quot;token&quot;).fit(data) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer)).fit(data) val result = pipeline.transform(data) result.selectExpr(&quot;token.result&quot;).show(false) +--+ |output | +--+ |[I&#39;d, like, to, say, we, didn&#39;t, expect, that, ., Jane&#39;s, boyfriend, .]| +--+ Tokenizes raw text into word pieces, tokens. Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. This class represents an already fitted Tokenizer model. See the main class Tokenizer for more examples of usage. Input Annotator Types: DOCUMENT //A Tokenizer could require only for now a SentenceDetector annotator Output Annotator Type: TOKEN Python API: TokenizerModel Scala API: TokenizerModel Source: TokenizerModel TypedDependencyParser ApproachModel Labeled parser that finds a grammatical relation between two words in a sentence. Its input is either a CoNLL2009 or ConllU dataset. For instantiated/pretrained models, see TypedDependencyParserModel. Dependency parsers provide information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The parser requires the dependant tokens beforehand with e.g. DependencyParser. The required training data can be set in two different ways (only one can be chosen for a particular model): Dataset in the CoNLL 2009 format set with setConll2009 Dataset in the CoNLL-U format set with setConllU Apart from that, no additional training data is needed. See TypedDependencyParserApproachTestSpec for further reference on this API. Input Annotator Types: TOKEN, POS, DEPENDENCY Output Annotator Type: LABELED_DEPENDENCY Python API: TypedDependencyParserApproach Scala API: TypedDependencyParserApproach Source: TypedDependencyParserApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) posTagger = PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) dependencyParser = DependencyParserModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency&quot;) typedDependencyParser = TypedDependencyParserApproach() .setInputCols([&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency_type&quot;) .setConllU(&quot;src/test/resources/parser/labeled/train_small.conllu.txt&quot;) .setNumberOfIterations(1) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, posTagger, dependencyParser, typedDependencyParser ]) # Additional training data is not needed, the dependency parser relies on CoNLL-U only. emptyDataSet = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(emptyDataSet) import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel import com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val dependencyParser = DependencyParserModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) val typedDependencyParser = new TypedDependencyParserApproach() .setInputCols(&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency_type&quot;) .setConllU(&quot;src/test/resources/parser/labeled/train_small.conllu.txt&quot;) .setNumberOfIterations(1) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, posTagger, dependencyParser, typedDependencyParser )) // Additional training data is not needed, the dependency parser relies on CoNLL-U only. val emptyDataSet = Seq.empty[String].toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(emptyDataSet) Labeled parser that finds a grammatical relation between two words in a sentence. Its input is either a CoNLL2009 or ConllU dataset. Dependency parsers provide information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The parser requires the dependant tokens beforehand with e.g. DependencyParser. Pretrained models can be loaded with pretrained of the companion object: val typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols(&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency_type&quot;) The default model is &quot;dependency_typed_conllu&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the TypedDependencyModelTestSpec. Input Annotator Types: TOKEN, POS, DEPENDENCY Output Annotator Type: LABELED_DEPENDENCY Python API: TypedDependencyParserModel Scala API: TypedDependencyParserModel Source: TypedDependencyParserModel ViveknSentiment ApproachModel Trains a sentiment analyser inspired by the algorithm by Vivek Narayanan https://github.com/vivekn/sentiment/. The algorithm is based on the paper “Fast and accurate sentiment classification using an enhanced Naive Bayes model”. The analyzer requires sentence boundaries to give a score in context. Tokenization is needed to make sure tokens are within bounds. Transitivity requirements are also required. The training data needs to consist of a column for normalized text and a label column (either &quot;positive&quot; or &quot;negative&quot;). For extended examples of usage, see the Examples and the ViveknSentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: ViveknSentimentApproach Scala API: ViveknSentimentApproach Source: ViveknSentimentApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) token = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normal&quot;) vivekn = ViveknSentimentApproach() .setInputCols([&quot;document&quot;, &quot;normal&quot;]) .setSentimentCol(&quot;train_sentiment&quot;) .setOutputCol(&quot;result_sentiment&quot;) finisher = Finisher() .setInputCols([&quot;result_sentiment&quot;]) .setOutputCols(&quot;final_sentiment&quot;) pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher]) training = spark.createDataFrame([ (&quot;I really liked this movie!&quot;, &quot;positive&quot;), (&quot;The cast was horrible&quot;, &quot;negative&quot;), (&quot;Never going to watch this again or recommend it to anyone&quot;, &quot;negative&quot;), (&quot;It&#39;s a waste of time&quot;, &quot;negative&quot;), (&quot;I loved the protagonist&quot;, &quot;positive&quot;), (&quot;The music was really really good&quot;, &quot;positive&quot;) ]).toDF(&quot;text&quot;, &quot;train_sentiment&quot;) pipelineModel = pipeline.fit(training) data = spark.createDataFrame([ [&quot;I recommend this movie&quot;], [&quot;Dont waste your time!!!&quot;] ]).toDF(&quot;text&quot;) result = pipelineModel.transform(data) result.select(&quot;final_sentiment&quot;).show(truncate=False) ++ |final_sentiment| ++ |[positive] | |[negative] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.Normalizer import com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentApproach import com.johnsnowlabs.nlp.Finisher import org.apache.spark.ml.Pipeline val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val token = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normal&quot;) val vivekn = new ViveknSentimentApproach() .setInputCols(&quot;document&quot;, &quot;normal&quot;) .setSentimentCol(&quot;train_sentiment&quot;) .setOutputCol(&quot;result_sentiment&quot;) val finisher = new Finisher() .setInputCols(&quot;result_sentiment&quot;) .setOutputCols(&quot;final_sentiment&quot;) val pipeline = new Pipeline().setStages(Array(document, token, normalizer, vivekn, finisher)) val training = Seq( (&quot;I really liked this movie!&quot;, &quot;positive&quot;), (&quot;The cast was horrible&quot;, &quot;negative&quot;), (&quot;Never going to watch this again or recommend it to anyone&quot;, &quot;negative&quot;), (&quot;It&#39;s a waste of time&quot;, &quot;negative&quot;), (&quot;I loved the protagonist&quot;, &quot;positive&quot;), (&quot;The music was really really good&quot;, &quot;positive&quot;) ).toDF(&quot;text&quot;, &quot;train_sentiment&quot;) val pipelineModel = pipeline.fit(training) val data = Seq( &quot;I recommend this movie&quot;, &quot;Dont waste your time!!!&quot; ).toDF(&quot;text&quot;) val result = pipelineModel.transform(data) result.select(&quot;final_sentiment&quot;).show(false) ++ |final_sentiment| ++ |[positive] | |[negative] | ++ Sentiment analyser inspired by the algorithm by Vivek Narayanan https://github.com/vivekn/sentiment/. The algorithm is based on the paper “Fast and accurate sentiment classification using an enhanced Naive Bayes model”. This is the instantiated model of the ViveknSentimentApproach. For training your own model, please see the documentation of that class. The analyzer requires sentence boundaries to give a score in context. Tokenization is needed to make sure tokens are within bounds. Transitivity requirements are also required. For extended examples of usage, see the Examples and the ViveknSentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: ViveknSentimentModel Scala API: ViveknSentimentModel Source: ViveknSentimentModel Word2Vec ApproachModel Trains a Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. For instantiated/pretrained models, see Word2VecModel. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: Word2VecApproach Scala API: Word2VecApproach Source: Word2VecApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Word2VecApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings ]) path = &quot;sherlockholmes.txt&quot; dataset = spark.read.text(path).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(dataset) import spark.implicits._ import com.johnsnowlabs.nlp.annotator.{Tokenizer, Word2VecApproach} import com.johnsnowlabs.nlp.base.DocumentAssembler import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = new Word2VecApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings )) val path = &quot;src/test/resources/spell/sherlockholmes.txt&quot; val dataset = spark.sparkContext.textFile(path) .toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(dataset) Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. This is the instantiated model of the Word2VecApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val embeddings = Word2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;word2vec_gigaword_300&quot;, if no name is provided. For available pretrained models please see the Models Hub. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: Word2VecModel Scala API: Word2VecModel Source: Word2VecModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Word2VecModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, embeddings, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Tokenizer, Word2VecModel} import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = Word2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsFinisher )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ WordEmbeddings ApproachModel Word Embeddings lookup annotator that maps tokens to vectors. For instantiated/pretrained models, see WordEmbeddingsModel. A custom token lookup dictionary for embeddings can be set with setStoragePath. Each line of the provided file needs to have a token, followed by their vector representation, delimited by a spaces. ... are 0.39658191506190343 0.630968081620067 0.5393722253731201 0.8428180123359783 were 0.7535235923631415 0.9699218875629833 0.10397182122983872 0.11833962569383116 stress 0.0492683418305907 0.9415954572751959 0.47624463167525755 0.16790967216778263 induced 0.1535748762292387 0.33498936903209897 0.9235178224122094 0.1158772920395934 ... If a token is not found in the dictionary, then the result will be a zero vector of the same dimension. Statistics about the rate of converted tokens, can be retrieved with[WordEmbeddingsModel.withCoverageColumn and WordEmbeddingsModel.overallCoverage. For extended examples of usage, see the Examples and the WordEmbeddingsTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: WordEmbeddings Scala API: WordEmbeddings Source: WordEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the file `random_embeddings_dim4.txt` has the form of the content above. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddings() .setStoragePath(&quot;src/test/resources/random_embeddings_dim4.txt&quot;, ReadAs.TEXT) .setStorageRef(&quot;glove_4d&quot;) .setDimension(4) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) .setCleanAnnotations(False) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;The patient was diagnosed with diabetes.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(truncate=False) +-+ |result | +-+ |[0.9439099431037903,0.4707513153553009,0.806300163269043,0.16176554560661316] | |[0.7966810464859009,0.5551124811172485,0.8861005902290344,0.28284206986427307] | |[0.025029370561242104,0.35177749395370483,0.052506182342767715,0.1887107789516449]| |[0.08617766946554184,0.8399239182472229,0.5395117998123169,0.7864698767662048] | |[0.6599600911140442,0.16109347343444824,0.6041093468666077,0.8913561105728149] | |[0.5955275893211365,0.01899011991918087,0.4397728443145752,0.8911281824111938] | |[0.9840458631515503,0.7599489092826843,0.9417727589607239,0.8624503016471863] | +-+ // In this example, the file `random_embeddings_dim4.txt` has the form of the content above. import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.embeddings.WordEmbeddings import com.johnsnowlabs.nlp.util.io.ReadAs import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = new WordEmbeddings() .setStoragePath(&quot;src/test/resources/random_embeddings_dim4.txt&quot;, ReadAs.TEXT) .setStorageRef(&quot;glove_4d&quot;) .setDimension(4) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsFinisher )) val data = Seq(&quot;The patient was diagnosed with diabetes.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(false) +-+ |result | +-+ |[0.9439099431037903,0.4707513153553009,0.806300163269043,0.16176554560661316] | |[0.7966810464859009,0.5551124811172485,0.8861005902290344,0.28284206986427307] | |[0.025029370561242104,0.35177749395370483,0.052506182342767715,0.1887107789516449]| |[0.08617766946554184,0.8399239182472229,0.5395117998123169,0.7864698767662048] | |[0.6599600911140442,0.16109347343444824,0.6041093468666077,0.8913561105728149] | |[0.5955275893211365,0.01899011991918087,0.4397728443145752,0.8911281824111938] | |[0.9840458631515503,0.7599489092826843,0.9417727589607239,0.8624503016471863] | +-+ Word Embeddings lookup annotator that maps tokens to vectors This is the instantiated model of WordEmbeddings. Pretrained models can be loaded with pretrained of the companion object: val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;glove_100d&quot;, if no name is provided. For available pretrained models please see the Models Hub. There are also two convenient functions to retrieve the embeddings coverage with respect to the transformed dataset: withCoverageColumn(dataset, embeddingsCol, outputCol): Adds a custom column with word coverage stats for the embedded field: (coveredWords, totalWords, coveragePercentage). This creates a new column with statistics for each row. val wordsCoverage = WordEmbeddingsModel.withCoverageColumn(resultDF, &quot;embeddings&quot;, &quot;cov_embeddings&quot;) wordsCoverage.select(&quot;text&quot;,&quot;cov_embeddings&quot;).show(false) +-+--+ |text |cov_embeddings| +-+--+ |This is a sentence.|[5, 5, 1.0] | +-+--+ overallCoverage(dataset, embeddingsCol): Calculates overall word coverage for the whole data in the embedded field. This returns a single coverage object considering all rows in the field. val wordsOverallCoverage = WordEmbeddingsModel.overallCoverage(wordsCoverage,&quot;embeddings&quot;).percentage 1.0 For extended examples of usage, see the Examples and the WordEmbeddingsTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: WordEmbeddingsModel Scala API: WordEmbeddingsModel Source: WordEmbeddingsModel WordSegmenter ApproachModel Trains a WordSegmenter which tokenizes non-english or non-whitespace separated texts. Many languages are not whitespace separated and their sentences are a concatenation of many symbols, like Korean, Japanese or Chinese. Without understanding the language, splitting the words into their corresponding tokens is impossible. The WordSegmenter is trained to understand these languages and split them into semantically correct parts. This annotator is based on the paper Chinese Word Segmentation as Character Tagging [1]. Word segmentation is treated as a tagging problem. Each character is be tagged as on of four different labels: LL (left boundary), RR (right boundary), MM (middle) and LR (word by itself). The label depends on the position of the word in the sentence. LL tagged words will combine with the word on the right. Likewise, RR tagged words combine with words on the left. MM tagged words are treated as the middle of the word and combine with either side. LR tagged words are words by themselves. Example (from [1], Example 3(a) (raw), 3(b) (tagged), 3(c) (translation)): 上海 计划 到 本 世纪 末 实现 人均 国内 生产 总值 五千 美元 上/LL 海/RR 计/LL 划/RR 到/LR 本/LR 世/LL 纪/RR 末/LR 实/LL 现/RR 人/LL 均/RR 国/LL 内/RR 生/LL 产/RR 总/LL 值/RR 五/LL 千/RR 美/LL 元/RR Shanghai plans to reach the goal of 5,000 dollars in per capita GDP by the end of the century. For instantiated/pretrained models, see WordSegmenterModel. To train your own model, a training dataset consisting of Part-Of-Speech tags is required. The data has to be loaded into a dataframe, where the column is an Annotation of type &quot;POS&quot;. This can be set with setPosColumn. Tip: The helper class POS might be useful to read training data into data frames. For extended examples of usage, see the Examples and the WordSegmenterTest. References: [1] Xue, Nianwen. “Chinese Word Segmentation as Character Tagging.” International Journal of Computational Linguistics &amp; Chinese Language Processing, Volume 8, Number 1, February 2003: Special Issue on Word Formation and Chinese Language Processing, 2003, pp. 29-48. ACLWeb, https://aclanthology.org/O03-4002. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: WordSegmenterApproach Scala API: WordSegmenterApproach Source: WordSegmenterApproach Show Example PythonScala # In this example, `&quot;chinese_train.utf8&quot;` is in the form of # # 十|LL 四|RR 不|LL 是|RR 四|LL 十|RR # # and is loaded with the `POS` class to create a dataframe of `&quot;POS&quot;` type Annotations. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) wordSegmenter = WordSegmenterApproach() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) .setPosColumn(&quot;tags&quot;) .setNIterations(5) pipeline = Pipeline().setStages([ documentAssembler, wordSegmenter ]) trainingDataSet = POS().readDataset( spark, &quot;src/test/resources/word-segmenter/chinese_train.utf8&quot; ) pipelineModel = pipeline.fit(trainingDataSet) // In this example, `&quot;chinese_train.utf8&quot;` is in the form of // // 十|LL 四|RR 不|LL 是|RR 四|LL 十|RR // // and is loaded with the `POS` class to create a dataframe of `&quot;POS&quot;` type Annotations. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.ws.WordSegmenterApproach import com.johnsnowlabs.nlp.training.POS import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val wordSegmenter = new WordSegmenterApproach() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) .setPosColumn(&quot;tags&quot;) .setNIterations(5) val pipeline = new Pipeline().setStages(Array( documentAssembler, wordSegmenter )) val trainingDataSet = POS().readDataset( ResourceHelper.spark, &quot;src/test/resources/word-segmenter/chinese_train.utf8&quot; ) val pipelineModel = pipeline.fit(trainingDataSet) WordSegmenter which tokenizes non-english or non-whitespace separated texts. Many languages are not whitespace separated and their sentences are a concatenation of many symbols, like Korean, Japanese or Chinese. Without understanding the language, splitting the words into their corresponding tokens is impossible. The WordSegmenter is trained to understand these languages and plit them into semantically correct parts. This annotator is based on the paper Chinese Word Segmentation as Character Tagging. Word segmentation is treated as a tagging problem. Each character is be tagged as on of four different labels: LL (left boundary), RR (right boundary), MM (middle) and LR (word by itself). The label depends on the position of the word in the sentence. LL tagged words will combine with the word on the right. Likewise, RR tagged words combine with words on the left. MM tagged words are treated as the middle of the word and combine with either side. LR tagged words are words by themselves. Example (from [1], Example 3(a) (raw), 3(b) (tagged), 3(c) (translation)): 上海 计划 到 本 世纪 末 实现 人均 国内 生产 总值 五千 美元 上/LL 海/RR 计/LL 划/RR 到/LR 本/LR 世/LL 纪/RR 末/LR 实/LL 现/RR 人/LL 均/RR 国/LL 内/RR 生/LL 产/RR 总/LL 值/RR 五/LL 千/RR 美/LL 元/RR Shanghai plans to reach the goal of 5,000 dollars in per capita GDP by the end of the century. This is the instantiated model of the WordSegmenterApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val wordSegmenter = WordSegmenterModel.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;words_segmented&quot;) The default model is &quot;wordseg_pku&quot;, default language is &quot;zh&quot;, if no values are provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the WordSegmenterTest. References: [1] Xue, Nianwen. “Chinese Word Segmentation as Character Tagging.” International Journal of Computational Linguistics &amp; Chinese Language Processing, Volume 8, Number 1, February 2003: Special Issue on Word Formation and Chinese Language Processing, 2003, pp. 29-48. ACLWeb, https://aclanthology.org/O03-4002. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: WordSegmenterModel Scala API: WordSegmenterModel Source: WordSegmenterModel YakeKeywordExtraction Yake is an Unsupervised, Corpus-Independent, Domain and Language-Independent and Single-Document keyword extraction algorithm. Extracting keywords from texts has become a challenge for individuals and organizations as the information grows in complexity and size. The need to automate this task so that text can be processed in a timely and adequate manner has led to the emergence of automatic keyword extraction tools. Yake is a novel feature-based system for multi-lingual keyword extraction, which supports texts of different sizes, domain or languages. Unlike other approaches, Yake does not rely on dictionaries nor thesauri, neither is trained against any corpora. Instead, it follows an unsupervised approach which builds upon features extracted from the text, making it thus applicable to documents written in different languages without the need for further knowledge. This can be beneficial for a large number of tasks and a plethora of situations where access to training corpora is either limited or restricted. The algorithm makes use of the position of a sentence and token. Therefore, to use the annotator, the text should be first sent through a Sentence Boundary Detector and then a tokenizer. Note that each keyword will be given a keyword score greater than 0 (The lower the score better the keyword). Therefore to filter the keywords, an upper bound for the score can be set with setThreshold. For extended examples of usage, see the Examples and the YakeTestSpec. Sources : Campos, R., Mangaravite, V., Pasquali, A., Jatowt, A., Jorge, A., Nunes, C. and Jatowt, A. (2020). YAKE! Keyword Extraction from Single Documents using Multiple Local Features. In Information Sciences Journal. Elsevier, Vol 509, pp 257-289 Paper abstract: As the amount of generated information grows, reading and summarizing texts of large collections turns into a challenging task. Many documents do not come with descriptive terms, thus requiring humans to generate keywords on-the-fly. The need to automate this kind of task demands the development of keyword extraction systems with the ability to automatically identify keywords within the text. One approach is to resort to machine-learning algorithms. These, however, depend on large annotated text corpora, which are not always available. An alternative solution is to consider an unsupervised approach. In this article, we describe YAKE!, a light-weight unsupervised automatic keyword extraction method which rests on statistical text features extracted from single documents to select the most relevant keywords of a text. Our system does not need to be trained on a particular set of documents, nor does it depend on dictionaries, external corpora, text size, language, or domain. To demonstrate the merits and significance of YAKE!, we compare it against ten state-of-the-art unsupervised approaches and one supervised method. Experimental results carried out on top of twenty datasets show that YAKE! significantly outperforms other unsupervised methods on texts of different sizes, languages, and domains. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: YakeKeywordExtraction Scala API: YakeKeywordExtraction Source: YakeKeywordExtraction Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) token = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) .setContextChars([&quot;(&quot;, &quot;]&quot;, &quot;?&quot;, &quot;!&quot;, &quot;.&quot;, &quot;,&quot;]) keywords = YakeKeywordExtraction() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;keywords&quot;) .setThreshold(0.6) .setMinNGrams(2) .setNKeywords(10) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, token, keywords ]) data = spark.createDataFrame([[ &quot;Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud Next conference in San Francisco this week, the official announcement could come as early as tomorrow. Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. Google itself declined &#39;to comment on rumors&#39;. Kaggle, which has about half a million data scientists on its platform, was founded by Goldbloom and Ben Hamner in 2010. The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, it has managed to stay well ahead of them by focusing on its specific niche. The service is basically the de facto home for running data science and machine learning competitions. With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow and other projects). Kaggle has a bit of a history with Google, too, but that&#39;s pretty recent. Earlier this month, Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google will keep the service running - likely under its current name. While the acquisition is probably more about Kaggle&#39;s community than technology, Kaggle did build some interesting tools for hosting its competition and &#39;kernels&#39;, too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can share this code on the platform (the company previously called them &#39;scripts&#39;). Like similar competition-centric sites, Kaggle also runs a job board, too. It&#39;s unclear what Google will do with that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it&#39;s $12.75) since its launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, NaRavikant, Google chie economist Hal Varian, Khosla Ventures and Yuri Milner&quot; ]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) # combine the result and score (contained in keywords.metadata) scores = result .selectExpr(&quot;explode(arrays_zip(keywords.result, keywords.metadata)) as resultTuples&quot;) .selectExpr(&quot;resultTuples[&#39;0&#39;] as keyword&quot;, &quot;resultTuples[&#39;1&#39;].score as score&quot;) # Order ascending, as lower scores means higher importance scores.orderBy(&quot;score&quot;).show(5, truncate = False) ++-+ |keyword |score | ++-+ |google cloud |0.32051516486864573| |google cloud platform|0.37786450577630676| |ceo anthony goldbloom|0.39922830978423146| |san francisco |0.40224744669493756| |anthony goldbloom |0.41584827825302534| ++-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{SentenceDetector, Tokenizer} import com.johnsnowlabs.nlp.annotators.keyword.yake.YakeKeywordExtraction import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val token = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) .setContextChars(Array(&quot;(&quot;, &quot;)&quot;, &quot;?&quot;, &quot;!&quot;, &quot;.&quot;, &quot;,&quot;)) val keywords = new YakeKeywordExtraction() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;keywords&quot;) .setThreshold(0.6f) .setMinNGrams(2) .setNKeywords(10) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, token, keywords )) val data = Seq( &quot;Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud Next conference in San Francisco this week, the official announcement could come as early as tomorrow. Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. Google itself declined &#39;to comment on rumors&#39;. Kaggle, which has about half a million data scientists on its platform, was founded by Goldbloom and Ben Hamner in 2010. The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, it has managed to stay well ahead of them by focusing on its specific niche. The service is basically the de facto home for running data science and machine learning competitions. With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow and other projects). Kaggle has a bit of a history with Google, too, but that&#39;s pretty recent. Earlier this month, Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google will keep the service running - likely under its current name. While the acquisition is probably more about Kaggle&#39;s community than technology, Kaggle did build some interesting tools for hosting its competition and &#39;kernels&#39;, too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can share this code on the platform (the company previously called them &#39;scripts&#39;). Like similar competition-centric sites, Kaggle also runs a job board, too. It&#39;s unclear what Google will do with that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it&#39;s $12.75) since its launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, Naval Ravikant, Google chief economist Hal Varian, Khosla Ventures and Yuri Milner&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) // combine the result and score (contained in keywords.metadata) val scores = result .selectExpr(&quot;explode(arrays_zip(keywords.result, keywords.metadata)) as resultTuples&quot;) .select($&quot;resultTuples.0&quot; as &quot;keyword&quot;, $&quot;resultTuples.1.score&quot;) // Order ascending, as lower scores means higher importance scores.orderBy(&quot;score&quot;).show(5, truncate = false) ++-+ |keyword |score | ++-+ |google cloud |0.32051516486864573| |google cloud platform|0.37786450577630676| |ceo anthony goldbloom|0.39922830978423146| |san francisco |0.40224744669493756| |anthony goldbloom |0.41584827825302534| ++-+",
    "url": "/docs/en/annotators",
    "relUrl": "/docs/en/annotators"
  },
  "6": {
    "id": "6",
    "title": "Automatic Speech Recognition",
    "content": "Automatic Speech Recognition (ASR) is the technology that enables computers to recognize and process human speech into text. ASR plays a vital role in numerous applications, from voice-activated assistants to transcription services, making it an essential part of modern natural language processing (NLP) solutions. Spark NLP provides powerful tools for implementing ASR systems effectively. In this context, ASR involves converting spoken language into text by analyzing audio signals. Common use cases include: Voice Assistants: Enabling devices like smartphones and smart speakers to understand and respond to user commands. Transcription Services: Automatically converting audio recordings from meetings, interviews, or lectures into written text. Accessibility: Helping individuals with disabilities interact with technology through voice commands. By leveraging ASR, organizations can enhance user experience, improve accessibility, and streamline workflows that involve audio data. Picking a Model When selecting a model for Automatic Speech Recognition, it’s essential to evaluate several factors to ensure optimal performance for your specific use case. Begin by analyzing the nature of your audio data, considering the accent, language, and quality of the recordings. Determine if your task requires real-time transcription or if batch processing is sufficient, as some models excel in specific scenarios. Next, assess the model complexity; simpler models may suffice for straightforward tasks, while more sophisticated models are better suited for nuanced speech recognition. Consider the availability of diverse audio data for training, as larger datasets can significantly enhance model performance. Define key performance metrics (e.g., word error rate, accuracy) to guide your choice, and ensure the model’s interpretability meets your requirements. Finally, account for resource constraints, as advanced models typically demand more memory and processing power. To explore and select from a variety of models, visit Spark NLP Models, where you can find models tailored for different ASR tasks and languages. Recommended Models for Automatic Speech Recognition Tasks General Speech Recognition: Use models like asr_wav2vec2_large_xlsr_53_english_by_jonatasgrosman for general-purpose transcription. Multilingual Support: For applications requiring support for multiple languages, consider using models like asr_wav2vec2_large_xlsr_53_portuguese_by_jonatasgrosman from the Wav2Vec2ForCTC transformer. By thoughtfully considering these factors and using the right models, you can enhance your ASR applications significantly. How to use PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Step 1: Assemble the raw audio content into a suitable format audioAssembler = AudioAssembler() .setInputCol(&quot;audio_content&quot;) .setOutputCol(&quot;audio_assembler&quot;) # Step 2: Load a pre-trained Wav2Vec2 model for automatic speech recognition (ASR) speechToText = Wav2Vec2ForCTC .pretrained() .setInputCols([&quot;audio_assembler&quot;]) .setOutputCol(&quot;text&quot;) # Step 3: Define the pipeline with audio assembler and speech-to-text model pipeline = Pipeline().setStages([audioAssembler, speechToText]) # Step 4: Create a DataFrame containing the raw audio content (as floats) processedAudioFloats = spark.createDataFrame([[rawFloats]]).toDF(&quot;audio_content&quot;) # Step 5: Fit the pipeline and transform the audio data result = pipeline.fit(processedAudioFloats).transform(processedAudioFloats) # Step 6: Display the transcribed text from the audio result.select(&quot;text.result&quot;).show(truncate = False) ++ |result | ++ |[MISTER QUILTER IS THE APOSTLE OF THE MIDLE CLASES AND WE ARE GLAD TO WELCOME HIS GOSPEL ]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotators._ import com.johnsnowlabs.nlp.annotators.audio.Wav2Vec2ForCTC import org.apache.spark.ml.Pipeline // Step 1: Assemble the raw audio content into a suitable format val audioAssembler: AudioAssembler = new AudioAssembler() .setInputCol(&quot;audio_content&quot;) .setOutputCol(&quot;audio_assembler&quot;) // Step 2: Load a pre-trained Wav2Vec2 model for automatic speech recognition (ASR) val speechToText: Wav2Vec2ForCTC = Wav2Vec2ForCTC .pretrained() .setInputCols(&quot;audio_assembler&quot;) .setOutputCol(&quot;text&quot;) // Step 3: Define the pipeline with audio assembler and speech-to-text model val pipeline: Pipeline = new Pipeline().setStages(Array(audioAssembler, speechToText)) // Step 4: Load raw audio floats from a CSV file val bufferedSource = scala.io.Source.fromFile(&quot;src/test/resources/audio/csv/audio_floats.csv&quot;) // Step 5: Extract raw audio floats from CSV and convert to an array of floats val rawFloats = bufferedSource .getLines() .map(_.split(&quot;,&quot;).head.trim.toFloat) .toArray bufferedSource.close // Step 6: Create a DataFrame with raw audio content (as floats) val processedAudioFloats = Seq(rawFloats).toDF(&quot;audio_content&quot;) // Step 7: Fit the pipeline and transform the audio data val result = pipeline.fit(processedAudioFloats).transform(processedAudioFloats) // Step 8: Display the transcribed text from the audio result.select(&quot;text.result&quot;).show(truncate = false) ++ |result | ++ |[MISTER QUILTER IS THE APOSTLE OF THE MIDLE CLASES AND WE ARE GLAD TO WELCOME HIS GOSPEL ]| ++ Try Real-Time Demos! If you want to see the outputs of ASR models in real time, visit our interactive demos: Wav2Vec2ForCTC – Try this powerful model for real-time speech-to-text from raw audio. WhisperForCTC – Test speech recognition in multiple languages and noisy environments. HubertForCTC – Experience quick and accurate voice command recognition. Useful Resources Want to dive deeper into Automatic Speech Recognition with Spark NLP? Here are somText Preprocessinge curated resources to help you get started and explore further: Articles and Guides Converting Speech to Text with Spark NLP and Python Simplify Your Speech Recognition Workflow with SparkNLP Vision Transformers and Automatic Speech Recognition in Spark NLP Notebooks Automatic Speech Recognition in Spark NLP Speech Recognition Transformers in Spark NLP",
    "url": "/docs/en/tasks/automatic_speech_recognition",
    "relUrl": "/docs/en/tasks/automatic_speech_recognition"
  },
  "7": {
    "id": "7",
    "title": "Helper functions",
    "content": "Spark NLP Annotation functions The functions presented here help users manipulate annotations, by providing both UDFs and dataframe utilities to deal with them more easily Python In python, the functions are straight forward and have both UDF and Dataframe applications map_annotations(f, output_type: DataType) UDF that applies f(). Requires output DataType from pyspark.sql.types map_annotations_strict(f) UDF that apples an f() method that returns a list of Annotations map_annotations_col(dataframe: DataFrame, f, column: str, output_column: str, annotatyon_type: str, output_type: DataType = Annotation.arrayType()) applies f() to column from dataframe map_annotations_cols(dataframe: DataFrame, f, columns: str, output_column: str, annotatyon_type: str, output_type: DataType = Annotation.arrayType()) applies f() to columns from dataframe filter_by_annotations_col(dataframe, f, column) applies a boolean filter f() to column from dataframe explode_annotations_col(dataframe: DataFrame, column, output_column) explodes annotation column from dataframe Scala In Scala, importing inner functions brings implicits that allow these functions to be applied directly on top of the dataframe mapAnnotations(function: Seq[Annotation] =&gt; T, outputType: DataType) mapAnnotationsStrict(function: Seq[Annotation] =&gt; Seq[Annotation]) mapAnnotationsCol[T: TypeTag](column: String, outputCol: String,annotatorType: String, function: Seq[Annotation] =&gt; T) mapAnnotationsCol[T: TypeTag](cols: Seq[String], outputCol: String,annotatorType: String, function: Seq[Annotation] =&gt; T) eachAnnotationsCol[T: TypeTag](column: String, function: Seq[Annotation] =&gt; Unit) def explodeAnnotationsCol[T: TypeTag](column: String, outputCol: String) Imports: PythonScala from sparknlp.functions import * from sparknlp.annotation import Annotation import com.johnsnowlabs.nlp.functions._ import com.johnsnowlabs.nlp.Annotation Examples: Complete usage examples can be seen here: https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/234-release-candidate/jupyter/annotation/english/spark-nlp-basics/spark-nlp-basics-functions.ipynb PythonScala def my_annoation_map_function(annotations): return list(map(lambda a: Annotation( &#39;my_own_type&#39;, a.begin, a.end, a.result, {&#39;my_key&#39;: &#39;custom_annotation_data&#39;}, []), annotations)) result.select( map_annotations(my_annoation_map_function, Annotation.arrayType())(&#39;token&#39;) ).toDF(&quot;my output&quot;).show(truncate=False) val modified = data.mapAnnotationsCol(&quot;pos&quot;, &quot;mod_pos&quot;,&quot;pos&quot; ,(_: Seq[Annotation]) =&gt; { &quot;hello world&quot; })",
    "url": "/docs/en/auxiliary",
    "relUrl": "/docs/en/auxiliary"
  },
  "8": {
    "id": "8",
    "title": "Classify Documents - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/classify_documents",
    "relUrl": "/classify_documents"
  },
  "9": {
    "id": "9",
    "title": "Spark NLP - General Concepts",
    "content": "Concepts Spark ML provides a set of Machine Learning applications that can be build using two main components: Estimators and Transformers. The Estimators have a method called fit() which secures and trains a piece of data to such application. The Transformer is generally the result of a fitting process and applies changes to the the target dataset. These components have been embedded to be applicable to Spark NLP. Pipelines are a mechanism for combining multiple estimators and transformers in a single workflow. They allow multiple chained transformations along a Machine Learning task. For more information please refer to Spark ML library. Annotation The basic result of a Spark NLP operation is an annotation. It’s structure includes: annotatorType: the type of annotator that generated the current annotation begin: the begin of the matched content relative to raw-text end: the end of the matched content relative to raw-text result: the main output of the annotation metadata: content of matched result and additional information embeddings: (new in 2.0) contains vector mappings if required This object is automatically generated by annotators after a transform process. No manual work is required. However, it is important to clearly understand the structure of an annotation to be able too efficiently use it. Annotators Annotators are the spearhead of NLP functions in Spark NLP. There are two forms of annotators: Annotator Approaches: are those who represent a Spark ML Estimator and require a training stage. They have a function called fit(data) which trains a model based on some data. They produce the second type of annotator which is an annotator model or transformer. Annotator Models: are spark models or transformers, meaning they have a transform(data) function. This function takes as input a dataframe to which it adds a new column containing the result of the current annotation. All transformers are additive, meaning they append to current data, never replace or delete previous information. Both forms of annotators can be included in a Pipeline. All annotators included in a Pipeline will be automatically executed in the defined order and will transform the data accordingly. A Pipeline is turned into a PipelineModel after the fit() stage. The Pipeline can be saved to disk and re-loaded at any time. Common Functions setInputCols(column_names): Takes a list of column names of annotations required by this annotator. Those are generated by the annotators which precede the current annotator in the pipeline. setOutputCol(column_name): Defines the name of the column containing the result of the current annotator. Use this name as an input for other annotators down the pipeline requiring the outputs generated by the current annotator. Quickly annotate some text You can run these examples using Python or Scala. The easiest way to run the python examples is by starting a pyspark jupyter notebook including the spark-nlp package: $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.7 -y $ conda activate sparknlp # spark-nlp by default is based on pyspark 3.x $ pip install spark-nlp==6.1.2 pyspark==3.3.1 jupyter $ jupyter notebook Explain Document ML Spark NLP offers a variety of pretrained pipelines that will help you get started, and get a sense of how the library works. We are constantly working on improving the available content. You can checkout a demo application of the Explain Document ML pipeline here: View Demo Downloading and using a pretrained pipeline Explain Document ML (explain_document_ml) is a pretrained pipeline that does a little bit of everything NLP related. Let’s try it out in scala. Note that the first time you run the below code it might take longer since it downloads the pretrained pipeline from our servers! PythonScala import sparknlp spark = sparknlp.start() from sparknlp.pretrained import PretrainedPipeline explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) annotations = explain_document_pipeline.annotate(&quot;We are very happy about SparkNLP&quot;) print(annotations) # OUTPUT: # { # &#39;stem&#39;: [&#39;we&#39;, &#39;ar&#39;, &#39;veri&#39;, &#39;happi&#39;, &#39;about&#39;, &#39;sparknlp&#39;], # &#39;checked&#39;: [&#39;We&#39;, &#39;are&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], # &#39;lemma&#39;: [&#39;We&#39;, &#39;be&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], # &#39;document&#39;: [&#39;We are very happy about SparkNLP&#39;], # &#39;pos&#39;: [&#39;PRP&#39;, &#39;VBP&#39;, &#39;RB&#39;, &#39;JJ&#39;, &#39;IN&#39;, &#39;NNP&#39;], # &#39;token&#39;: [&#39;We&#39;, &#39;are&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], # &#39;sentence&#39;: [&#39;We are very happy about SparkNLP&#39;] # } import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) // OUTPUT: // explain_document_ml download started this may take some time. // Approximate size to download 9.4 MB // Download done! Loading the resource. // explain_document_pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_ml,en,public/models) val annotations = explainDocumentPipeline.annotate(&quot;We are very happy about SparkNLP&quot;) println(annotations) // OUTPUT: // Map( // stem -&gt; List(we, ar, veri, happi, about, sparknlp), // checked -&gt; List(We, are, very, happy, about, SparkNLP), // lemma -&gt; List(We, be, very, happy, about, SparkNLP), // document -&gt; List(We are very happy about SparkNLP), // pos -&gt; ArrayBuffer(PRP, VBP, RB, JJ, IN, NNP), // token -&gt; List(We, are, very, happy, about, SparkNLP), // sentence -&gt; List(We are very happy about SparkNLP) ) As you can see the explain_document_ml is able to annotate any “document” providing as output a list of stems, check-spelling, lemmas, part of speech tags, tokens and sentence boundary detection and all this “out-of-the-box”!. Using a pretrained pipeline with spark dataframes You can also use the pipeline with a spark dataframe. You just need to create first a spark dataframe with a column named “text” that will work as the input for the pipeline and then use the .transform() method to run the pipeline over that dataframe and store the outputs of the different components in a spark dataframe. Remember than when starting jupyter notebook from pyspark or when running the spark-shell for scala, a Spark Session is started in the background by default within the namespace ‘scala’. PythonScala import sparknlp spark = sparknlp.start() sentences = [ [&#39;Hello, this is an example sentence&#39;], [&#39;And this is a second sentence.&#39;] ] # spark is the Spark Session automatically started by pyspark. data = spark.createDataFrame(sentences).toDF(&quot;text&quot;) # Download the pretrained pipeline from Johnsnowlab&#39;s servers explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) OUTPUT: explain_document_ml download started this may take some time. Approx size to download 9.4 MB [OK!] val data = Seq( &quot;Hello, this is an example sentence&quot;, &quot;And this is a second sentence&quot;) .toDF(&quot;text&quot;) data.show(truncate=false) // OUTPUT: ++ |text | ++ |Hello, this is an example set | |And this is a second sentence.| ++ val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) val annotations_df = explainDocumentPipeline.transform(data) annotations_df.show() // OUTPUT: +--+--+--+--+--+--+--+--+ | text| document| sentence| token| checked| lemma| stem| pos| +--+--+--+--+--+--+--+--+ |Hello, this is an...|[[document, 0, 33...|[[document, 0, 33...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, he...|[[pos, 0, 4, UH, ...| |And this is a sec...|[[document, 0, 29...|[[document, 0, 29...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, an...|[[pos, 0, 2, CC, ...| +--+--+--+--+--+--+--+--+ Manipulating pipelines The output of the previous DataFrame was in terms of Annotation objects. This output is not really comfortable to deal with, as you can see by running the code: PythonScala annotations_df.select(&quot;token&quot;).show(truncate=False) # OUTPUT: +--+ |token | +--+ |[[token, 0, 4, Hello, [sentence -&gt; 0], [], []], [token, 5, 5, ,, [sentence -&gt; 0], [], []], [token, 7, 10, this, [sentence -&gt; 0], [], []], [token, 12, 13, is, [sentence -&gt; 0], [], []], [token, 15, 16, an, [sentence -&gt; 0], [], []], [token, 18, 24, example, [sentence -&gt; 0], [], []], [token, 26, 33, sentence, [sentence -&gt; 0], [], []]]| |[[token, 0, 2, And, [sentence -&gt; 0], [], []], [token, 4, 7, this, [sentence -&gt; 0], [], []], [token, 9, 10, is, [sentence -&gt; 0], [], []], [token, 12, 12, a, [sentence -&gt; 0], [], []], [token, 14, 19, second, [sentence -&gt; 0], [], []], [token, 21, 28, sentence, [sentence -&gt; 0], [], []], [token, 29, 29, ., [sentence -&gt; 0], [], []]] | +--+ annotations_df.select(&quot;token&quot;).show(truncate=false) // OUTPUT: +--+ |token | +--+ |[[token, 0, 4, Hello, [sentence -&gt; 0], [], []], [token, 5, 5, ,, [sentence -&gt; 0], [], []], [token, 7, 10, this, [sentence -&gt; 0], [], []], [token, 12, 13, is, [sentence -&gt; 0], [], []], [token, 15, 16, an, [sentence -&gt; 0], [], []], [token, 18, 24, example, [sentence -&gt; 0], [], []], [token, 26, 33, sentence, [sentence -&gt; 0], [], []]]| |[[token, 0, 2, And, [sentence -&gt; 0], [], []], [token, 4, 7, this, [sentence -&gt; 0], [], []], [token, 9, 10, is, [sentence -&gt; 0], [], []], [token, 12, 12, a, [sentence -&gt; 0], [], []], [token, 14, 19, second, [sentence -&gt; 0], [], []], [token, 21, 28, sentence, [sentence -&gt; 0], [], []], [token, 29, 29, ., [sentence -&gt; 0], [], []]] | +--+ What if we want to deal with just the resulting annotations? We can use the Finisher annotator, retrieve the Explain Document ML pipeline, and add them together in a Spark ML Pipeline. Remember that pretrained pipelines expect the input column to be named “text”. PythonScala from sparknlp import Finisher from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline finisher = Finisher().setInputCols([&quot;token&quot;, &quot;lemmas&quot;, &quot;pos&quot;]) explain_pipeline_model = PretrainedPipeline(&quot;explain_document_ml&quot;).model pipeline = Pipeline() .setStages([ explain_pipeline_model, finisher ]) sentences = [ [&#39;Hello, this is an example sentence&#39;], [&#39;And this is a second sentence.&#39;] ] data = spark.createDataFrame(sentences).toDF(&quot;text&quot;) model = pipeline.fit(data) annotations_finished_df = model.transform(data) annotations_finished_df.select(&#39;finished_token&#39;).show(truncate=False) # OUTPUT: +-+ |finished_token | +-+ |[Hello, ,, this, is, an, example, sentence]| |[And, this, is, a, second, sentence, .] | +-+ import com.johnsnowlabs.nlp.Finisher import org.apache.spark.ml.Pipeline val finisher = new Finisher().setInputCols(&quot;token&quot;, &quot;lemma&quot;, &quot;pos&quot;) val explainPipelineModel = PretrainedPipeline(&quot;explain_document_ml&quot;).model val pipeline = new Pipeline(). setStages(Array( explainPipelineModel, finisher )) val data = Seq( &quot;Hello, this is an example sentence&quot;, &quot;And this is a second sentence&quot;) .toDF(&quot;text&quot;) val model = pipeline.fit(data) val annotations_df = model.transform(data) annotations_df.select(&quot;finished_token&quot;).show(truncate=false) // OUTPUT: +-+ |finished_token | +-+ |[Hello, ,, this, is, an, example, sentence]| |[And, this, is, a, second, sentence, .] | +-+ Setup your own pipeline Annotator types Every annotator has a type. Those annotators that share a type, can be used interchangeably, meaning you could use any of them when needed. For example, when a token type annotator is required by another annotator, such as a sentiment analysis annotator, you can either provide a normalized token or a lemma, as both are of type token. Necessary imports Since version 1.5.0 we are making necessary imports easy to reach, base._ will include general Spark NLP transformers and concepts, while annotator._ will include all annotators that we currently provide. We also need Spark ML pipelines. PythonScala from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline DocumentAssembler: Getting data in In order to get through the NLP process, we need to get raw data annotated. There is a special transformer that does this for us: the DocumentAssembler, it creates the first annotation of type Document which may be used by annotators down the road. PythonScala documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val documentAssembler = new DocumentAssembler(). setInputCol(&quot;text&quot;). setOutputCol(&quot;document&quot;) Sentence detection and tokenization In this quick example, we now proceed to identify the sentences in the input document. SentenceDetector requires a Document annotation, which is provided by the DocumentAssembler output, and it’s itself a Document type token. The Tokenizer requires a Document annotation type. That means it works both with DocumentAssembler or SentenceDetector output. In the following example we use the sentence output. PythonScala sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;Sentence&quot;) regexTokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) val sentenceDetector = new SentenceDetector(). setInputCols(Array(&quot;document&quot;)). setOutputCol(&quot;sentence&quot;) val regexTokenizer = new Tokenizer(). setInputCols(Array(&quot;sentence&quot;)). setOutputCol(&quot;token&quot;) Spark NLP also includes another special transformer, called Finisher to show tokens in a human language. finisher = Finisher() .setInputCols([&quot;token&quot;]) .setCleanAnnotations(False) val finisher = new Finisher(). setInputCols(&quot;token&quot;). setCleanAnnotations(false) Finisher: Getting data out At the end of each pipeline or any stage that was done by Spark NLP, you may want to get results out whether onto another pipeline or simply write them on disk. The Finisher annotator helps you to clean the metadata (if it’s set to true) and output the results into an array: PythonScala finisher = Finisher() .setInputCols([&quot;token&quot;]) .setIncludeMetadata(True) val finisher = new Finisher() .setInputCols(&quot;token&quot;) .setIncludeMetadata(true) If you need to have a flattened DataFrame (each sub-array in a new column) from any annotations other than struct type columns, you can use explode function from Spark SQL. You can also use Apache Spark functions (SQL) to manipulate the output DataFrame in any way you need. Here we combine the tokens and NER results together: import pyspark.sql.functions as F df.withColumn(&quot;tmp&quot;, F.explode(&quot;chunk&quot;)).select(&quot;tmp.*&quot;) finisher.withColumn(&quot;newCol&quot;, explode(arrays_zip($&quot;finished_token&quot;, $&quot;finished_ner&quot;))) import org.apache.spark.sql.functions._ df.withColumn(&quot;tmp&quot;, explode(col(&quot;chunk&quot;))).select(&quot;tmp.*&quot;) Using Spark ML Pipeline Now we want to put all this together and retrieve the results, we use a Pipeline for this. We use the same data in fit() that we will use in transform since none of the pipeline stages have a training stage. PythonScala pipeline = Pipeline() .setStages([ documentAssembler, sentenceDetector, regexTokenizer, finisher ]) OUTPUT: +-+ |finished_token | +-+ |[hello, ,, this, is, an, example, sentence]| +-+ val pipeline = new Pipeline(). setStages(Array( documentAssembler, sentenceDetector, regexTokenizer, finisher )) val data = Seq(&quot;hello, this is an example sentence&quot;).toDF(&quot;text&quot;) val annotations = pipeline. fit(data). transform(data).toDF(&quot;text&quot;)) annotations.select(&quot;finished_token&quot;).show(truncate=false) OUTPUT: +-+ |finished_token | +-+ |[hello, ,, this, is, an, example, sentence]| +-+ Using Spark NLP’s LightPipeline LightPipeline is a Spark NLP specific Pipeline class equivalent to Spark ML Pipeline. The difference is that it’s execution does not hold to Spark principles, instead it computes everything locally (but in parallel) in order to achieve fast results when dealing with small amounts of data. This means, we do not input a Spark Dataframe, but a string or an Array of strings instead, to be annotated. To create Light Pipelines, you need to input an already trained (fit) Spark ML Pipeline. It’s transform() stage is converted into annotate() instead. PythonScala from sparknlp.base import LightPipeline explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) lightPipeline = LightPipeline(explain_document_pipeline.model) OUTPUT: explain_document_ml download started this may take some time. Approx size to download 9.4 MB [OK!] lightPipeline.annotate(&quot;Hello world, please annotate my text&quot;) OUTPUT: {&#39;stem&#39;: [&#39;hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;pleas&#39;, &#39;annot&#39;, &#39;my&#39;, &#39;text&#39;], &#39;checked&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;my&#39;, &#39;text&#39;], &#39;lemma&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;i&#39;, &#39;text&#39;], &#39;document&#39;: [&#39;Hello world, please annotate my text&#39;], &#39;pos&#39;: [&#39;UH&#39;, &#39;NN&#39;, &#39;,&#39;, &#39;VB&#39;, &#39;NN&#39;, &#39;PRP$&#39;, &#39;NN&#39;], &#39;token&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;my&#39;, &#39;text&#39;], &#39;sentence&#39;: [&#39;Hello world, please annotate my text&#39;]} import com.johnsnowlabs.nlp.base._ val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) val lightPipeline = new LightPipeline(explainDocumentPipeline.model) lightPipeline.annotate(&quot;Hello world, please annotate my text&quot;) OUTPUT: Map[String,Seq[String]] = Map( stem -&gt; List(hello, world, ,, pleas, annot, my, text), checked -&gt; List(Hello, world, ,, please, annotate, my, tex), lemma -&gt; List(Hello, world, ,, please, annotate, i, text), document -&gt; List(Hello world, please annotate my text), pos -&gt; ArrayBuffer(UH, NN, ,, VB, NN, PRP$, NN), token -&gt; List(Hello, world, ,, please, annotate, my, text), sentence -&gt; List(Hello world, please annotate my text) ) Training annotators Training methodology Training your own annotators is a key concept when dealing with real life scenarios. Any of the annotators provided above, such as pretrained pipelines and models, can be applied out-of-the-box to a specific use case, but better results are obtained when they are fine-tuned to your specific use-case. Dealing with real life problems ofter requires training your own models. In Spark NLP, we support three ways of training a custom annotator: Train from a dataset. Most annotators are capable of training from a dataset passed to fit() method just as Spark ML does. Annotators that use the suffix Approach are such trainable annotators. Training from fit() is the standard behavior in Spark ML. Annotators have different schema requirements for training. Check the reference to see what are the requirements of each annotators. Training from an external source: Some of our annotators train from an external file or folder passed to the annotator as a param. You will see such ones as setCorpus() or setDictionary() param setter methods, allowing you to configure the input to use. You can set Spark NLP to read them as Spark datasets or LINE_BY_LINE which is usually faster for small files. Last but not least, some of our annotators are Deep Learning based. These models may be trained with the standard AnnotatorApproach API just like any other annotator. For more advanced users, we also allow importing your own graphs or even training from Python and converting them into an AnnotatorModel. Spark NLP Imports base includes general Spark NLP transformers and concepts, annotator includes all annotators that we currently provide, embeddings includes word embedding annotators. Example: PythonScala from sparknlp.base import * from sparknlp.annotator import * from sparknlp.embeddings import * import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ Spark ML Pipelines SparkML Pipelines are a uniform structure that helps creating and tuning practical machine learning pipelines. Spark NLP integrates with them seamlessly so it is important to have this concept handy. Once a Pipeline is trained with fit(), it becomes a PipelineModel Example: PythonScala from pyspark.ml import Pipeline pipeline = Pipeline().setStages([...]) import org.apache.spark.ml.Pipeline new Pipeline().setStages(Array(...)) LightPipeline LightPipelines are Spark ML pipelines converted into a single machine but multithreaded task, becoming more than 10x times faster for smaller amounts of data (small is relative, but 50k sentences is roughly a good maximum). To use them, simply plug in a trained (fitted) pipeline. Example: PythonScala from sparknlp.base import LightPipeline LightPipeline(someTrainedPipeline).annotate(someStringOrArray) import com.johnsnowlabs.nlp.LightPipeline new LightPipeline(somePipelineModel).annotate(someStringOrArray)) Functions: annotate(string or string[]): returns dictionary list of annotation results fullAnnotate(string or string[]): returns dictionary list of entire annotations content For more details please refer to Using Spark NLP’s LightPipelines. RecursivePipeline Recursive pipelines are SparkNLP specific pipelines that allow a Spark ML Pipeline to know about itself on every Pipeline Stage task, allowing annotators to utilize this same pipeline against external resources to process them in the same way the user decides. Only some of our annotators take advantage of this. RecursivePipeline behaves exactly the same as normal Spark ML pipelines, so they can be used with the same intention. Example: PythonScala from sparknlp.annotator import * recursivePipeline = RecursivePipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, lemmatizer, finisher ]) import com.johnsnowlabs.nlp.RecursivePipeline val recursivePipeline = new RecursivePipeline() .setStages(Array( documentAssembler, sentenceDetector, tokenizer, lemmatizer, finisher )) Params and Features Annotator parameters SparkML uses ML Params to store pipeline parameter maps. In SparkNLP, we also use Features, which are a way to store parameter maps that are larger than just a string or a boolean. These features are serialized as either Parquet or RDD objects, allowing much faster and scalable annotator information. Features are also broadcasted among executors for better performance.",
    "url": "/docs/en/concepts",
    "relUrl": "/docs/en/concepts"
  },
  "10": {
    "id": "10",
    "title": "Contribute",
    "content": "Refer to our GitHub page to take a look at the GH Issues, as the project is yet small. You can create in there your own issues to either work on them yourself or simply propose them. Feel free to clone the repository locally and submit pull requests so we can review them and work together. feedback, ideas and bug reports testing and development training and testing nlp corpora documentation and research Help is always welcome, for any further questions, contact nlp@johnsnowlabs.com. Your own annotator model Creating your first annotator transformer should not be hard, here are a few guidelines to get you started. Lets assume we want a wrapper annotator, which puts a character surrounding tokens provided by a Tokenizer WordWrapper uid is utilized for transformer serialization, AnnotatorModel[MyAnnotator] will contain the common annotator logic We need to use standard constructor for java and python compatibility class WordWrapper(override val uid: String) extends AnnotatorModel[WordWrapper] { def this() = this(Identifiable.randomUID(&quot;WORD_WRAPPER&quot;)) } Annotator attributes This annotator is not flexible if we don’t provide parameters import com.johnsnowlabs.nlp.AnnotatorType._ override val annotatorType: AnnotatorType = TOKEN override val requiredAnnotatorTypes: Array[AnnotatorType] = Array[AnnotatorType](TOKEN) Annotator parameters This annotator is not flexible if we don’t provide parameters protected val character: Param[String] = new Param(this, &quot;character&quot;, &quot;this is the character used to wrap a token&quot;) def setCharacter(value: String): this.type = set(pattern, value) def getCharacter: String = $(pattern) setDefault(character, &quot;@&quot;) Annotator logic Here is how we act, annotations will automatically provide our required annotations We generally use annotatorType for metadata keys override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = { annotations.map(annotation =&gt; { Annotation( annotatorType, annotation.begin, annotation.end, Map(annotatorType -&gt; $(character) + annotation.result + $(character)) }) }",
    "url": "/contribute",
    "relUrl": "/contribute"
  },
  "11": {
    "id": "11",
    "title": "Databricks Solution Accelerators",
    "content": "",
    "url": "/databricks_solution_accelerators",
    "relUrl": "/databricks_solution_accelerators"
  },
  "12": {
    "id": "12",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/demos",
    "relUrl": "/demos"
  },
  "13": {
    "id": "13",
    "title": "Dependency Parsing",
    "content": "Dependency Parsing is a syntactic analysis task that focuses on the grammatical structure of sentences. It identifies the dependencies between words, showcasing how they relate in terms of grammar. Spark NLP provides advanced dependency parsing models that can accurately analyze sentence structures, enabling various applications in natural language processing. Dependency parsing models process input sentences and generate a structured representation of word relationships. Common use cases include: Grammatical Analysis: Understanding the grammatical structure of sentences for better comprehension. Information Extraction: Identifying key relationships and entities in sentences for tasks like knowledge graph construction. By using Spark NLP dependency parsing models, you can build efficient systems to analyze and understand sentence structures accurately. Picking a Model When selecting a dependency parsing model, consider factors such as the language of the text and the complexity of sentence structures. Some models may be optimized for specific languages or types of text. Evaluate whether you need detailed syntactic parsing or a more general analysis based on your application. Explore the available dependency parsing models at Spark NLP Models to find the one that best fits your requirements. Recommended Models for Dependency Parsing Tasks General Dependency Parsing: Consider models such as dependency_conllu_en_3_0 for analyzing English sentences. You can also explore language-specific models tailored for non-English languages. Choosing the appropriate model ensures you produce accurate syntactic structures that suit your specific language and use case. How to use PythonScala from sparknlp.annotator import * from sparknlp.base import * from pyspark.ml import Pipeline # Document Assembler: Converts raw text into a document format suitable for processing documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) # Sentence Detector: Splits text into individual sentences sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) # Tokenizer: Breaks sentences into tokens (words) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) # Part-of-Speech Tagger: Tags each token with its respective POS (pretrained model) posTagger = PerceptronModel.pretrained() .setInputCols([&quot;token&quot;, &quot;sentence&quot;]) .setOutputCol(&quot;pos&quot;) # Dependency Parser: Analyzes the grammatical structure of a sentence dependencyParser = DependencyParserModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency&quot;) # Typed Dependency Parser: Assigns typed labels to the dependencies typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols([&quot;token&quot;, &quot;pos&quot;, &quot;dependency&quot;]) .setOutputCol(&quot;labdep&quot;) # Create a pipeline that includes all the stages pipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, posTagger, dependencyParser, typedDependencyParser ]) # Sample input data (a DataFrame with one text example) data = {&quot;text&quot;: [&quot;Dependencies represent relationships between words in a sentence.&quot;]} df = spark.createDataFrame(data) # Run the pipeline on the input data result = pipeline.fit(df).transform(df) # Show the dependency parsing results result.select(&quot;dependency.result&quot;).show(truncate=False) ++ |result | ++ |[ROOT, Dependencies, represents, words, relationships, Sentence, Sentence, words]| ++ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline import spark.implicits._ // Document Assembler: Converts raw text into a document format for NLP processing val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) // Sentence Detector: Splits the input text into individual sentences val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) // Tokenizer: Breaks sentences into individual tokens (words) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) // Part-of-Speech Tagger: Tags each token with its respective part of speech (pretrained model) val posTagger = PerceptronModel.pretrained() .setInputCols(Array(&quot;token&quot;, &quot;sentence&quot;)) .setOutputCol(&quot;pos&quot;) // Dependency Parser: Analyzes the grammatical structure of the sentence val dependencyParser = DependencyParserModel.pretrained() .setInputCols(Array(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;)) .setOutputCol(&quot;dependency&quot;) // Typed Dependency Parser: Assigns typed labels to the dependencies val typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols(Array(&quot;token&quot;, &quot;pos&quot;, &quot;dependency&quot;)) .setOutputCol(&quot;labdep&quot;) // Create a pipeline that includes all stages val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, posTagger, dependencyParser, typedDependencyParser )) // Sample input data (a DataFrame with one text example) val df = Seq(&quot;Dependencies represent relationships between words in a Sentence&quot;).toDF(&quot;text&quot;) // Run the pipeline on the input data val result = pipeline.fit(df).transform(df) // Show the dependency parsing results result.select(&quot;dependency.result&quot;).show(truncate = false) ++ |result | ++ |[ROOT, Dependencies, represents, words, relationships, Sentence, Sentence, words]| ++ Try Real-Time Demos! If you want to see the outputs of dependency parsing models in real time, visit our interactive demos: Grammar Analysis &amp; Dependency Parsing – An interactive demo to visualize dependencies in sentences. Useful Resources Want to dive deeper into dependency parsing with Spark NLP? Here are some curated resources to help you get started and explore further: Articles and Guides Mastering Dependency Parsing with Spark NLP and Python Notebooks Extract Part of speech tags and perform dependency parsing on a text Typed Dependency Parsing with NLU.",
    "url": "/docs/en/tasks/dependency_parsing",
    "relUrl": "/docs/en/tasks/dependency_parsing"
  },
  "14": {
    "id": "14",
    "title": "Detect Sentiment & Emotion - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/detect_sentiment_emotion",
    "relUrl": "/detect_sentiment_emotion"
  },
  "15": {
    "id": "15",
    "title": "Spark NLP - Developers Guideline",
    "content": "Spark NLP is an open-source library and everyone’s contribution is welcome! In this section we provide a guide on how to setup your environment using IntelliJ IDEA for a smoother start. You can also check our video tutorials available on our YouTube channel: https://www.youtube.com/johnsnowlabs Setting up the Environment Import to IntelliJ IDEA Setup Spark NLP development environment. This section will cover library set up for IntelliJ IDEA. Before you begin, make sure what you have Java and Spark installed in your system. We suggest that you have installed jdk 8 and Apache Spark 2.4.x. To check installation run: java -version and spark-submit --version Next step is to open IntelliJ IDEA. On the Welcome to IntelliJ IDEA screen you will see ability to Check out from Version Controle. Log in into your github account in pop up. After select from a list Spark NLP repo url: https://github.com/JohnSnowLabs/spark-nlp and press clone button. If you don’t see url in the list, clone or fork repo first to your Github account and try again. When the repo cloned IDE will detect SBT file with dependencies. Click Yes to start import from sbt. In the Import from sbt pop up make sure you have JDK 8 detected. Click Ok to proceed and download required resources. If you already had dependences installed you may see the pop up Not empty folder, click Ok to ignore it and reload resources. IntelliJ IDEA will be open and it will start syncing SBT project. It make take some time, you will see the progress in the build output panel in the bottom of the screen. To see the project panel in the left press Alt+1. Next step is to install Python plugin to the IntelliJ IDEA. To do this, open File -&gt; Settings -&gt; Plugins, type Python in the search and select Python plugin by JetBrains. Install this plugin by clicking Install button. After this steps you can check project structure in the File -&gt; Project Structure -&gt; Modules. Make sure what you have spark-nlp and spark-nlp-build folders and no errors in the exported dependencies. In the Project settings check what project SDK is set to 1.8 and in Platform Settings -&gt; SDK&#39;s you have Java installation as well as Python installation. If you don’t see Python installed in the SDK&#39;s tab click + button, add Python SDK with new virtual environment in the project folder with Python 3.x. Compiling, assembly and unit testing Run tests in Scala Click Add configuration in the Top right corner. In the pop up click on the + and look for sbt task. In the Name field put Test. In the Tasks field write down test. After you can disable checkbox in Use sbt shell to have more custom configurations. In the VM parameters increase the memory by changing -Xmx1024M to -Xmx10G and click Ok. If everything was set up correctly you suhould see unabled green button Run ‘Test’ in the top right. Click on it to start running the tests. This algorithm will Run all tests under spark-nlp/src/test/scala/com.johnsnowlabs/ Copy tasks After you created task, click Edit configuration. Select target task and instead of + button you can click copy in the same menu. It will recreate all settings from parent task and create a new task. You can do it for Scala or for Python tasks. Run individual tests Open test file you want to run. For example, spark-nlp/src/test/scala/com.johnsnowlabs/nlp/FinisherTestSpec.scala. Right click on the class name and select Copy reference. It will copy to you buffer classpath - com.johnsnowlabs.nlp.FinisherTestSpec. Copy existing Scala task and Name it as FinisherTest. In the Tasks field write down &quot;testOnly *classpath*&quot; -&gt; &quot;testOnly com.johnsnowlabs.nlp.FinisherTestSpec&quot; and click Ok to save individual scala test run configuration. Press play button to run individual test. Debugging tests To run tests in debug mode click Debug button (next to play button). In this mode task will stop at the given break points. Run tests in Python To run Python test, first you need to configure project structure. Go to File -&gt; Project Settings -&gt; Modules, click on the + button and select New Module. In the pop up choose Python on left menu, select Python SDK from created virtual environment and click Next. Enter python in the Module name and click Finish. After you need to add Spark dependencies. Select created Python module and click on the + button in the Dependencies part. Choose Jars or directories… and find the find installation path of spark (usually the folder name is spark-2.4.5-bin-hadoop2.7). In the Spark folder go to the python/libs and select pyspark.zip to the project. Do the same for another file in the same folder - py4j-0.10.7-src.zip. All available tests are in spark-nlp/python/run-tests.py. Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and look for Python. In the Script path locate file spark-nlp/python/run-tests.py. Also you need to add SPARK_HOME environment variable to the project. Choose Environment variables and add new variable SPARK_HOME. Insert installation path of spark to the Value field. Click Ok to save and close pop up and click Ok to confirm new task creation. Before running the tests we need to install requered python dependencies in the new virtual environment. Select in the bottom menu Terminal and activate your environment with command source venv/bin/activate after install packages by running pip install pyspark==3.3.1 numpy Compiling jar Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and select sbt task. In the Name field put AssemblyCopy. In the Tasks field write down assemblyAndCopy. After you can disable checkbox in Use sbt shell to have more custom configurations. In the VM parameters increase the memory by changing -Xmx1024M to -Xmx6G and click Ok. You can find created jar in the folder spark-nlp/python/lib/sparknlp.jar Note: Assembly command creates a fat jars, that includes all dependencies within Compiling pypi, whl Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and select sbt task. In the Name field put AssemblyAndCopyForPyPi. In the Tasks field write down assemblyAndCopyForPyPi. Then you go to spark-nlp/python/ directory and run: python setup.py sdist bdist_wheel You can find created whl and tar.gz in the folder spark-nlp/python/dist/. Use this files to install spark-nlp locally: pip install spark_nlp-2.x.x-py3-none-any.whl",
    "url": "/docs/en/developers",
    "relUrl": "/docs/en/developers"
  },
  "16": {
    "id": "16",
    "title": "Spark NLP - Spark NLP Display",
    "content": "Getting started Spark NLP Display is an open-source python library for visualizing the annotations generated with Spark NLP. It currently offers out-of-the-box suport for the following types of annotations: Dependency Parser Named Entity Recognition Entity Resolution Relation Extraction Assertion Status The ability to quickly visualize the entities/relations/assertion statuses, etc. generated using Spark NLP is a very useful feature for speeding up the development process as well as for understanding the obtained results. Getting all of this in a one liner is extremelly convenient especially when running Jupyter notebooks which offers full support for html visualizations. The visualisation classes work with the outputs returned by both Pipeline.transform() function and LightPipeline.fullAnnotate(). Install Spark NLP Display You can install the Spark NLP Display library via pip by using: pip install spark-nlp-display A complete guideline on how to use the Spark NLP Display library is available here. Visualize a dependency tree For visualizing a dependency trees generated with DependencyParserApproach you can use the following code. from sparknlp_display import DependencyParserVisualizer dependency_vis = DependencyParserVisualizer() dependency_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe. pos_col = &#39;pos&#39;, #specify the pos column dependency_col = &#39;dependency&#39;, #specify the dependency column dependency_type_col = &#39;dependency_type&#39; #specify the dependency type column ) The following image gives an example of html output that is obtained for a test sentence: Visualize extracted named entities The NerVisualizer highlights the named entities that are identified by Spark NLP and also displays their labels as decorations on top of the analyzed text. The colors assigned to the predicted labels can be configured to fit the particular needs of the application. from sparknlp_display import NerVisualizer ner_vis = NerVisualizer() ner_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe label_col=&#39;entities&#39;, #specify the entity column document_col=&#39;document&#39; #specify the document column (default: &#39;document&#39;) labels=[&#39;PER&#39;] #only allow these labels to be displayed. (default: [] - all labels will be displayed) ) ## To set custom label colors: ner_vis.set_label_colors({&#39;LOC&#39;:&#39;#800080&#39;, &#39;PER&#39;:&#39;#77b5fe&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences: Visualize relations The RelationExtractionVisualizer can be used to visualize the relations predicted by Spark NLP. The two entities involved in a relation will be highlighted and their label will be displayed. Also a directed and labeled arc(line) will be used to connect the two entities. from sparknlp_display import RelationExtractionVisualizer re_vis = RelationExtractionVisualizer() re_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe relation_col = &#39;relations&#39;, #specify relations column document_col = &#39;document&#39;, #specify document column show_relations=True #display relation names on arrows (default: True) ) The following image gives an example of html output that is obtained for a couple of test sentences: Visualize assertion status The AssertionVisualizer is a special type of NerVisualizer that also displays on top of the labeled entities the assertion status that was infered by a Spark NLP model. from sparknlp_display import AssertionVisualizer assertion_vis = AssertionVisualizer() assertion_vis.display(pipeline_result[0], label_col = &#39;entities&#39;, #specify the ner result column assertion_col = &#39;assertion&#39; #specify assertion column document_col = &#39;document&#39; #specify the document column (default: &#39;document&#39;) ) ## To set custom label colors: assertion_vis.set_label_colors({&#39;TREATMENT&#39;:&#39;#008080&#39;, &#39;problem&#39;:&#39;#800080&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences: Visualize entity resolution Entity resolution refers to the normalization of named entities predicted by Spark NLP with respect to standard terminologies such as ICD-10, SNOMED, RxNorm etc. You can read more about the available entity resolvers here. The EntityResolverVisualizer will automatically display on top of the NER label the standard code (ICD10 CM, PCS, ICDO; CPT) that corresponds to that entity as well as the short description of the code. If no resolution code could be identified a regular NER-type of visualization will be displayed. from sparknlp_display import EntityResolverVisualizer er_vis = EntityResolverVisualizer() er_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe label_col=&#39;entities&#39;, #specify the ner result column resolution_col = &#39;resolution&#39; document_col=&#39;document&#39; #specify the document column (default: &#39;document&#39;) ) ## To set custom label colors: er_vis.set_label_colors({&#39;TREATMENT&#39;:&#39;#800080&#39;, &#39;PROBLEM&#39;:&#39;#77b5fe&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences:",
    "url": "/docs/en/display",
    "relUrl": "/docs/en/display"
  },
  "17": {
    "id": "17",
    "title": "John Snow Labs - NLP Documentation",
    "content": "",
    "url": "/docs",
    "relUrl": "/docs"
  },
  "18": {
    "id": "18",
    "title": "East Asian Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/east_asian_languages",
    "relUrl": "/east_asian_languages"
  },
  "19": {
    "id": "19",
    "title": "Enhance Low-Quality Images - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/enhance_low_quality_images",
    "relUrl": "/enhance_low_quality_images"
  },
  "20": {
    "id": "20",
    "title": "European Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/european_languages",
    "relUrl": "/european_languages"
  },
  "21": {
    "id": "21",
    "title": "Spark NLP - Examples",
    "content": "Showcasing notebooks and codes of how to use Spark NLP in Python and Scala. Python Setup $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.7 -y $ conda activate sparknlp $ pip install spark-nlp==6.1.2 pyspark==3.3.1 Google Colab Notebook Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account. Run the following code in Google Colab notebook and start using spark-nlp right away. # This is only to setup PySpark and Spark NLP on Colab !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash This script comes with the two options to define pyspark and spark-nlp versions via options: # -p is for pyspark # -s is for spark-nlp # by default they are set to the latest !bash colab.sh -p 3.2.3 -s 6.1.2 Spark NLP quick start on Google Colab is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines. &lt;div class=”h3-box” markdown=”1” Kaggle Kernel Run the following code in Kaggle Kernel and start using spark-nlp right away. # Let&#39;s setup Kaggle for Spark NLP and PySpark !wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash &lt;/div&gt;&lt;div class=&quot;h3-box&quot; markdown=&quot;1&quot;&gt; Notebooks Tutorials and articles Jupyter Notebooks &lt;/div&gt;",
    "url": "/docs/en/examples",
    "relUrl": "/docs/en/examples"
  },
  "22": {
    "id": "22",
    "title": "Extract handwritten texts - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/extract_handwritten_texts",
    "relUrl": "/extract_handwritten_texts"
  },
  "23": {
    "id": "23",
    "title": "Extract Tables - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/extract_tables",
    "relUrl": "/extract_tables"
  },
  "24": {
    "id": "24",
    "title": "Extract Text from Documents - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/extract_text_from_documents",
    "relUrl": "/extract_text_from_documents"
  },
  "25": {
    "id": "25",
    "title": "Spark NLP - FAQ",
    "content": "How to use Spark NLP? To use Spark NLP in Python, follow these steps: Installation: pip install spark-nlp if you don’t have PySpark you should also install the following dependencies: pip install pyspark numpy Initialize SparkSession with Spark NLP: import sparknlp spark = sparknlp.start() Use Annotators: Spark NLP offers a variety of annotators (e.g., Tokenizer, SentenceDetector, Lemmatizer). To use them, first create the appropriate pipeline. Example using a Tokenizer: from sparknlp.base import DocumentAssembler from sparknlp.annotator import Tokenizer documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) tokenizer = Tokenizer().setInputCols([&quot;document&quot;]).setOutputCol(&quot;token&quot;) pipeline = Pipeline(stages=[documentAssembler, tokenizer]) Transform Data: Once you have a pipeline, you can transform your data. result = pipeline.fit(data).transform(data) Explore and Utilize Models: Spark NLP offers pre-trained models for tasks like Named Entity Recognition (NER), sentiment analysis, and more. You can easily plug these into your pipeline and customize as needed. Further Reading: Dive deeper into the official documentation for more detailed examples, a complete list of annotators and models, and best practices for building NLP pipelines. Is Spark NLP free? Short answer: 100%! Free forever inculding any commercial use. Longer answer: Yes, Spark NLP is an open-source library and can be used freely. It’s released under the Apache License 2.0. Users can use, modify, and distribute it without incurring costs. What is the difference between spaCy and Spark NLP? Both spaCy and Spark NLP are popular libraries for Natural Language Processing, but Spark NLP shines when it comes to scalability and distributed processing. Here are some key differences between the two: Scalability &amp; Distributed Processing: Spark NLP: Built on top of Apache Spark, it’s designed for distributed processing and handling large datasets at scale. This makes it especially suitable for big data processing tasks that need to run on a cluster. spaCy: Designed for processing data on a single machine and it’s not natively built for distributed computing. Language Models &amp; Pretrained Pipelines: Spark NLP: Offers over 18,000 diverse pre-trained models and pipelines for over 235 languages, making it easy to get started on various NLP tasks. It also makes it easy to import your custom models from Hugging Face in TensorFlow and ONNX formats. Spark NLP also offeres a large number of state-of-the-art Large Language Models (LLMs) like BERT, RoBERTa, ALBERT, T5, OpenAI Whisper, and many more for Text Embeddings (useful for RAG), Named Entity Recognition, Text Classification, Answering, Automatic Speech Recognition, and more. These models can be used out of the box or fine-tuned on your own data. spaCy: Provides support for multiple languages with its models and supports tasks like tokenization, named entity recognition, and dependency parsing out of the box. However, spaCy doesn’t have any Models Hub and the number of offered models out of the box is limited. Licensing &amp; Versions: Spark NLP: The core library is open-source under the Apache License 2.0, making it free for both academic and commercial use. spaCy: Open-source and released under the MIT license. What are the Spark NLP models? Spark NLP provides a range of models to tackle various NLP tasks. These models are often pre-trained on large datasets and can be fine-tuned or used directly for inference. Some of the primary categories and examples of Spark NLP models include: Named Entity Recognition (NER): Pre-trained models for recognizing entities such as persons, organizations, locations, etc. Specialized models for sectors like healthcare to detect medical entities. Text Classification: Models for tasks like sentiment analysis, topic classification, and more. Word Embeddings: Word2Vec, GloVe, and BERT embeddings. Models to generate embeddings for words or sentences, useful in many downstream tasks. Language Models: Models like BERT, ALBERT, and ELECTRA are available pre-trained and can be fine-tuned for specific tasks. Dependency Parsing: Models that analyze the grammatical structure of a sentence and determine relationships between words. Spell Checking and Correction: Models that can detect and correct spelling mistakes in the text. Sentence Embeddings: Models to generate vector representations for entire sentences, such as Universal Sentence Encoder. Translation and Language Detection: Models to detect the language of a given text or translate text between languages. Text Matching: Models that can be used for tasks like textual similarity, paraphrase detection, etc. Pretrained Pipelines: Ready-to-use pipelines that combine multiple models and annotators for common tasks, allowing users to quickly start processing text without building a custom pipeline. For the latest list of models, detailed documentation, and instructions on how to use them, visiting the Official Spark NLP Models Hub would be beneficial. What are the main functions of Spark NLP? Spark NLP offers a comprehensive suite of functionalities tailored for natural language processing tasks via large language models. Some of the main functions and capabilities include: Text Tokenization: Segmenting text into words, phrases, or other meaningful elements called tokens. Named Entity Recognition (NER): Identifying and classifying named entities in text, such as names of people, places, organizations, dates, etc. Document Classification: Categorizing documents or chunks of text into predefined classes or topics. Sentiment Analysis: Determining the sentiment or emotion expressed in a piece of text, typically as positive, negative, or neutral. Dependency Parsing: Analyzing the grammatical structure of a sentence to establish relationships between words. Lemmatization and Stemming: Reducing words to their base or root form. For example, “running” becomes “run”. Spell Checking and Correction: Identifying and correcting spelling mistakes in text. Word and Sentence Embeddings: Transforming words or sentences into numerical vectors, useful for many machine learning tasks. Language Detection and Translation: Detecting the language of a given text and translating text between different languages. Text Matching and Similarity: Calculating the similarity between pieces of text or determining if texts are duplicates or paraphrases of one another. Chunking: Extracting short, meaningful phrases from the text. Stop Words Removal: Removing commonly used words that don’t carry significant meaning in most contexts (e.g., “and”, “the”, “is”). Normalization: Converting text into a standard or regular form, such as converting all letters to lowercase or removing special characters. Pre-trained Pipelines: Ready-to-use workflows combining multiple functions, allowing users to process text without creating a custom sequence of operations. Customizable Workflows: Building custom pipelines by chaining different annotators and models to create a tailored NLP processing sequence. Spark NLP is designed to be highly scalable and can handle large-scale text processing tasks efficiently by leveraging the distributed processing capabilities of Apache Spark. To fully grasp the breadth of functions and learn how to use them, users are encouraged to explore the official Spark NLP documentation. Where can I get prebuilt versions of Spark NLP? Prebuilt versions of Spark NLP can be obtained through multiple channels, depending on your development environment and platform: PyPI (for Python Users): You can install Spark NLP using pip, the Python package installer. pip install spark-nlp Maven Central (for Java/Scala Users): If you are using Maven, you can add the following dependency to your pom.xml: &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;LATEST_VERSION&lt;/version&gt; &lt;/dependency&gt; Make sure to replace LATEST_VERSION with the desired version of Spark NLP. Spark Packages: For those using the spark-shell, pyspark, or spark-submit, you can include Spark NLP directly via Spark Packages: --packages com.johnsnowlabs.nlp:spark-nlp_2.12:LATEST_VERSION Pre-trained Models &amp; Pipelines: Apart from the library itself, Spark NLP provides a range of pre-trained models and pipelines. These can be found on the Spark NLP Model Hub. Always make sure to consult the official documentation or the GitHub repository for the latest instructions and versions available.",
    "url": "/docs/en/faq",
    "relUrl": "/docs/en/faq"
  },
  "26": {
    "id": "26",
    "title": "Spark NLP - Features",
    "content": "Text Preprocessing Tokenization Trainable Word Segmentation Stop Words Removal Token Normalizer Document Normalizer Document &amp; Text Splitter Stemmer Lemmatizer NGrams Regex Matching Text Matching Spell Checker (ML and DL models) Parsing and Analysis Chunking Date Matcher Sentence Detector Deep Sentence Detector (Deep learning) Dependency parsing (Labeled/unlabeled) SpanBertCorefModel (Coreference Resolution) Part-of-speech tagging Named entity recognition (Deep learning) Unsupervised keywords extraction Language Detection &amp; Identification (up to 375 languages) Sentiment and Classification Sentiment Detection (ML models) Multi-class &amp; Multi-label Sentiment analysis (Deep learning) Multi-class Text Classification (Deep learning) Zero-Shot NER Model Zero-Shot Text Classification by Transformers (ZSL) Embeddings Word Embeddings (GloVe and Word2Vec) Doc2Vec (based on Word2Vec) BERT Embeddings (TF Hub &amp; HuggingFace models) DistilBERT Embeddings (HuggingFace models) CamemBERT Embeddings (HuggingFace models) RoBERTa Embeddings (HuggingFace models) DeBERTa Embeddings (HuggingFace v2 &amp; v3 models) XLM-RoBERTa Embeddings (HuggingFace models) Longformer Embeddings (HuggingFace models) ALBERT Embeddings (TF Hub &amp; HuggingFace models) XLNet Embeddings ELMO Embeddings (TF Hub models) Universal Sentence Encoder (TF Hub models) BERT Sentence Embeddings (TF Hub &amp; HuggingFace models) RoBerta Sentence Embeddings (HuggingFace models) XLM-RoBerta Sentence Embeddings (HuggingFace models) INSTRUCTOR Embeddings (HuggingFace models) E5 Embeddings (HuggingFace models) MPNet Embeddings (HuggingFace models) UAE Embeddings (HuggingFace models) OpenAI Embeddings Sentence &amp; Chunk Embeddings Classification and Question Answering Models BERT for Token &amp; Sequence Classification &amp; Question Answering DistilBERT for Token &amp; Sequence Classification &amp; Question Answering CamemBERT for Token &amp; Sequence Classification &amp; Question Answering ALBERT for Token &amp; Sequence Classification &amp; Question Answering RoBERTa for Token &amp; Sequence Classification &amp; Question Answering DeBERTa for Token &amp; Sequence Classification &amp; Question Answering XLM-RoBERTa for Token &amp; Sequence Classification &amp; Question Answering Longformer for Token &amp; Sequence Classification &amp; Question Answering MPnet for Token &amp; Sequence Classification &amp; Question Answering XLNet for Token &amp; Sequence Classification Machine Translation and Generation Neural Machine Translation (MarianMT) Many-to-Many multilingual translation model (Facebook M2M100) Table Question Answering (TAPAS) Text-To-Text Transfer Transformer (Google T5) Generative Pre-trained Transformer 2 (OpenAI GPT2) Seq2Seq for NLG, Translation, and Comprehension (Facebook BART) Chat and Conversational LLMs (Facebook Llama-2) Image and Speech Vision Transformer (Google ViT) Swin Image Classification (Microsoft Swin Transformer) ConvNext Image Classification (Facebook ConvNext) Vision Encoder Decoder for image-to-text like captioning Zero-Shot Image Classification by OpenAI’s CLIP Automatic Speech Recognition (Wav2Vec2) Automatic Speech Recognition (HuBERT) Automatic Speech Recognition (OpenAI Whisper) Integration and Interoperability Easy ONNX, OpenVINO, and TensorFlow integrations Full integration with Spark ML functions GPU Support Pre-trained Models +31000 pre-trained models in +200 languages! +6000 pre-trained pipelines in +200 languages! Please check out our Models Hub for the full list of pre-trained models with examples, demo, benchmark, and more Multi-lingual Support Multi-lingual NER models: Arabic, Bengali, Chinese, Danish, Dutch, English, Finnish, French, German, Hebrew, Italian, Japanese, Korean, Norwegian, Persian, Polish, Portuguese, Russian, Spanish, Swedish, Urdu, and more.",
    "url": "/docs/en/features",
    "relUrl": "/docs/en/features"
  },
  "27": {
    "id": "27",
    "title": "Genes, Variants, Phenotypes - Biomedical NLP Demos & Notebooks",
    "content": "",
    "url": "/genes_variants_phenotypes",
    "relUrl": "/genes_variants_phenotypes"
  },
  "28": {
    "id": "28",
    "title": "German - Medical NLP Demos & Notebooks",
    "content": "",
    "url": "/german",
    "relUrl": "/german"
  },
  "29": {
    "id": "29",
    "title": "Spark NLP - Tensorflow Graph",
    "content": "NER DL uses Char CNNs - BiLSTM - CRF Neural Network architecture. Spark NLP defines this architecture through a Tensorflow graph, which requires the following parameters: Tags Embeddings Dimension Number of Chars Spark NLP infers these values from the training dataset used in NerDLApproach annotator and tries to load the graph embedded on spark-nlp package. Currently, Spark NLP has graphs for the most common combination of tags, embeddings, and number of chars values: Tags Embeddings Dimension 10 100 10 200 10 300 10 768 10 1024 25 300 All of these graphs use an LSTM of size 128 and number of chars 100 In case, your train dataset has a different number of tags, embeddings dimension, number of chars and LSTM size combinations shown in the table above, NerDLApproach will raise an IllegalArgumentException exception during runtime with the message below: Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Check https://sparknlp.org/docs/en/graph for instructions to generate the required graph. To overcome this exception message we have to follow these steps: Clone spark-nlp github repo Run python file create_models with number of tags, embeddings dimension and number of char values mentioned on your exception message error. cd spark-nlp/python/tensorflow export PYTHONPATH=lib/ner python ner/create_models.py [number_of_tags] [embeddings_dimension] [number_of_chars] [output_path] This will generate a graph on the directory defined on `output_path argument. Retry training with NerDLApproach annotator but this time use the parameter setGraphFolder with the path of your graph. Note: Make sure that you have Python 3 and Tensorflow 1.15.0 installed on your system since create_models requires those versions to generate the graph successfully. Note: We also have a notebook in the same directory if you prefer Jupyter notebook to cerate your custom graph (create_models.ipynb).",
    "url": "/docs/en/graph",
    "relUrl": "/docs/en/graph"
  },
  "30": {
    "id": "30",
    "title": "Spark NLP - Hardware Acceleration",
    "content": "Spark NLP is a production-ready and fully-featured NLP library that runs natively on Apache Spark. It is already faster on a single machine than other popular NLP libraries let alone in a cluster with multiple machines. In addition, we are constantly optimizing our codes to make them even faster while using fewer resources (memory/CPU). For instance, Spark NLP 4.0 comes with massive optimizations for GPU and modern CPUs for most of our Transformer-based annotators. That said, some downstream tasks such as Language Models (Transformer models like BERT) or text and token classifiers use Deep Learning via the TensorFlow engine. Therefore, there are ways to optimize them even more by using newer hardware, especially those with accelerations. The following benchmarks have been done by using a single Dell Server with the following specs: GPU: Tesla P100 PCIe 12GB - CUDA Version: 11.3 - Driver Version: 465.19.01 CPU: Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz - 40 Cores Memory: 80G GPU Perhaps the best and the easiest way in Spark NLP to massively improve a DL-based task(s) is to use GPU. Spark NLP comes with a zero-code change feature to run seamlessly on both CPU and GPU by simply enabling GPU via sparknlp.start(gpu=True) or using directly the Maven package that is for GPU spark-nlp-gpu. (more details) Since the new Transformer models such as BERT for Word and Sentence embeddings are the most computationally available downstream tasks in Spark NLP, we will show a benchmark for inference (prediction) to compare CPU (without any accelerations) to GPU: Model on GPU Spark NLP 3.4.3 vs. 4.0.0 RoBERTa base +560%(6.6x) RoBERTa Large +332%(4.3x) Albert Base +587%(6.9x) Albert Large +332%(4.3x) DistilBERT +659%(7.6x) XLM-RoBERTa Base +638%(7.4x) XLM-RoBERTa Large +365%(4.7x) XLNet Base +449%(5.5x) XLNet Large +267%(3.7x) DeBERTa Base +713%(8.1x) DeBERTa Large +477%(5.8x) Longformer Base +52%(1.5x) Spark NLP 6.1.2 is built with TensorFlow 2.7.1 and the following NVIDIA® software are only required for GPU support: NVIDIA® GPU drivers version 450.80.02 or higher CUDA® Toolkit 11.2 cuDNN SDK 8.1.0 CPU The oneAPI Deep Neural Network Library (oneDNN) optimizations are now available in Spark NLP 4.0.0 which uses TensorFlow 2.7.1. You can enable those CPU optimizations by setting the environment variable TF_ENABLE_ONEDNN_OPTS=1. Intel has been collaborating with Google to optimize its performance on Intel Xeon processor-based platforms using Intel oneAPI Deep Neural Network (oneDNN), an open-source, cross-platform performance library for DL applications. TensorFlow optimizations are enabled via oneDNN to accelerate key performance-intensive operations such as convolution, matrix multiplication, and batch normalization. This feature is experimental as it has to be enabled manually and benchmarked manually to see whether or not your pipeline can benefit from oneDNN accelerations. That being said, it does not always result in accelerating your annotators as it highly depends on the hardware and the NLP tasks. Similar to GPU, if the task is not computational it won’t change the result and it may even slow down the inferences. NOTE: Always have a baseline benchmark without having oneDNN enabled so you can compare it with oneDNN. In addition, always make sure you repeat the same steps if you are moving to another hardware (CPU). Here we compare the last release of Spark NLP 3.4.3 on CPU (normal) with Spark NLP 4.0.0 on CPU with oneDNN enabled. We chose some of the most computational downstream tasks in Spark NLP as they are usually required in the pipeline for other tasks such as NER or text classification): Model on CPU 3.4.x vs. 4.0.0 with oneDNN BERT Base +47% BERT Large +42% RoBERTa Base +51% RoBERTa Large +61% Albert Base +83% Albert Large +58% DistilBERT +80% XLM-RoBERTa Base +82% XLM-RoBERTa Large +72% XLNet Base +50% XLNet Large +27% DeBERTa Base +59% DeBERTa Large +56% CamemBERT Base +97% CamemBERT Large +65% Longformer Base +63% In future TensorFlow releases, the oneDNN will be enabled by default (starting TF 2.9) as this feature becomes more stable and more generic for almost all TF ops. Maximize TensorFlow* Performance on CPU: Considerations and Recommendations for Inference Workloads GPU vs. CPU Webinar: Speed Optimization &amp; Benchmarks in Spark NLP 3: Making the Most of Modern Hardware",
    "url": "/docs/en/hardware_acceleration",
    "relUrl": "/docs/en/hardware_acceleration"
  },
  "31": {
    "id": "31",
    "title": "Identify & Translate Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/identify_translate_languages",
    "relUrl": "/identify_translate_languages"
  },
  "32": {
    "id": "32",
    "title": "Image Captioning",
    "content": "Image Captioning is the process of generating descriptive text for an image based on its visual content. This task is crucial in computer vision and has a variety of applications, such as enhancing accessibility for visually impaired individuals, improving image search, and enriching multimedia content. Spark NLP integrates image captioning with other NLP and vision-based tasks, enabling efficient and scalable caption generation within the same framework. By utilizing image captioning models, we can produce natural language descriptions that capture the key elements and context of images. Common use cases include: Social Media: Automatically generating captions for user-uploaded images. E-Commerce: Generating product descriptions based on visual attributes. Accessibility: Describing visual content for the visually impaired. Search Engines: Improving search results by associating images with relevant text. Picking a Model When selecting a model for image captioning, it’s important to consider the image complexity and the quality of captions required. For example, some tasks may need simple, high-level descriptions (e.g., “a person riding a bike”), while others might require more detailed, context-rich captions (e.g., “a young man riding a mountain bike on a sunny day”). Additionally, assess the performance metrics such as BLEU score or ROUGE score for evaluating the quality of generated captions. Ensure that the model is well-suited to your specific dataset, whether it consists of simple images like products or more complex images like natural scenes. Explore pre-trained image captioning models in the Spark NLP Models Hub for a variety of datasets and tasks. Recommended Models for Image Captioning VisionEncoderDecoder For Image Captioning: This model can be used for generating descriptive captions based on images. It utilizes a transformer-based architecture, providing high-quality captions for various types of images. Check out the pre-trained model image-captioning-vit-gpt2. How to use PythonScala # Import necessary libraries from Spark NLP import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Load image data into a DataFrame, discarding any invalid images imageDF = spark.read .format(&quot;image&quot;) .option(&quot;dropInvalid&quot;, value=True) .load(&quot;src/test/resources/image/&quot;) # Create an ImageAssembler to prepare image data for processing imageAssembler = ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) # Initialize the VisionEncoderDecoder model for image captioning imageCaptioning = VisionEncoderDecoderForImageCaptioning .pretrained() # Load a pre-trained model for image captioning .setBeamSize(2) .setDoSample(False) .setInputCols([&quot;image_assembler&quot;]) .setOutputCol(&quot;caption&quot;) # Create a pipeline that includes the image assembler and image captioning stages pipeline = Pipeline().setStages([imageAssembler, imageCaptioning]) # Fit the pipeline on the image DataFrame and transform the data pipelineDF = pipeline.fit(imageDF).transform(imageDF) # Select and display the image file name and the generated captions pipelineDF .selectExpr(&quot;reverse(split(image.origin, &#39;/&#39;))[0] as image_name&quot;, &quot;caption.result&quot;) .show(truncate=False) +--++ |image_name |result | +--++ |palace.JPEG |[a large room filled with furniture and a large window] | |egyptian_cat.jpeg|[a cat laying on a couch next to another cat] | |hippopotamus.JPEG|[a brown bear in a body of water] | |hen.JPEG |[a flock of chickens standing next to each other] | |ostrich.JPEG |[a large bird standing on top of a lush green field] | |junco.JPEG |[a small bird standing on a wet ground] | |bluetick.jpg |[a small dog standing on a wooden floor] | |chihuahua.jpg |[a small brown dog wearing a blue sweater] | |tractor.JPEG |[a man is standing in a field with a tractor] | |ox.JPEG |[a large brown cow standing on top of a lush green field]| +--++ // Import necessary libraries from Spark NLP import com.johnsnowlabs.nlp.annotator._ import com.johnsnowlabs.nlp.ImageAssembler import org.apache.spark.ml.Pipeline // Load image data into a DataFrame, discarding invalid images val imageDF: DataFrame = spark.read .format(&quot;image&quot;) .option(&quot;dropInvalid&quot;, value = true) .load(&quot;src/test/resources/image/&quot;) // Image Assembler: Prepares image data for processing val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) // Initialize image captioning model val imageCaptioning = VisionEncoderDecoderForImageCaptioning .pretrained() .setBeamSize(2) .setDoSample(false) .setInputCols(&quot;image_assembler&quot;) .setOutputCol(&quot;caption&quot;) // Create and fit the pipeline val pipeline = new Pipeline().setStages(Array(imageAssembler, imageCaptioning)) val pipelineDF = pipeline.fit(imageDF).transform(imageDF) // Display image names and generated captions pipelineDF .selectExpr(&quot;reverse(split(image.origin, &#39;/&#39;))[0] as image_name&quot;, &quot;caption.result&quot;) .show(truncate = false) +--++ |image_name |result | +--++ |palace.JPEG |[a large room filled with furniture and a large window] | |egyptian_cat.jpeg|[a cat laying on a couch next to another cat] | |hippopotamus.JPEG|[a brown bear in a body of water] | |hen.JPEG |[a flock of chickens standing next to each other] | |ostrich.JPEG |[a large bird standing on top of a lush green field] | |junco.JPEG |[a small bird standing on a wet ground] | |bluetick.jpg |[a small dog standing on a wooden floor] | |chihuahua.jpg |[a small brown dog wearing a blue sweater] | |tractor.JPEG |[a man is standing in a field with a tractor] | |ox.JPEG |[a large brown cow standing on top of a lush green field]| +--++ Try Real-Time Demos! Explore real-time image captioning outputs with our interactive demos: VisionEncoderDecoder For Image Captioning Useful Resources To dive deeper into image captioning using Spark NLP, check out these useful resources: Notebooks Vision Encoder-Decoder for Image Captioning",
    "url": "/docs/en/tasks/image_captioning",
    "relUrl": "/docs/en/tasks/image_captioning"
  },
  "33": {
    "id": "33",
    "title": "Image Classification",
    "content": "Image classification is the process of assigning a label or category to an image based on its visual content. This task is fundamental in the field of computer vision and has numerous applications, from facial recognition to product classification in e-commerce. Spark NLP provides tools that make it easier to integrate image classification into your data pipelines, allowing for scalable, efficient image processing within the same framework. By using image classification models, we can analyze and classify images into predefined categories based on patterns and features in the image data. Some common use cases include: Classifying product images into categories like clothing, electronics, furniture, etc. Recognizing objects in images, such as identifying animals, vehicles, or various types of landscapes. Detecting facial expressions and other human features for tasks like emotion analysis or identity verification. Picking a Model When selecting a model for image classification, it’s essential to consider several factors that ensure optimal performance for your specific use case. Start by evaluating the type of images you are working with, such as grayscale vs. colored, high-resolution vs. low-resolution, or simple vs. complex visual patterns. Determine whether your task requires binary classification (e.g., cat vs. dog) or multiclass classification (e.g., classifying various animal species), as the right model choice depends on the complexity of the task. Next, assess the computational power available to you. Complex models such as CNNs (Convolutional Neural Networks) can be resource-intensive but deliver highly accurate results. Simpler models may be sufficient for less demanding tasks. Ensure the model’s performance metrics (accuracy, precision, recall) align with your project goals, and consider the interpretability of the model—more advanced models may be less interpretable but offer greater accuracy. Explore a wide variety of image classification models on the Spark NLP Models, where you can find pre-trained models suited for different tasks and datasets. Recommended Models for Specific Image Classification Tasks Object Detection: For detecting objects in images, models such as image_classifier_vit_base_patch16_224 can be used to detect objects across multiple categories. Facial Expression Recognition: Models like image_classifier_swin_swin_large_patch4_window12_384 are great for tasks that involve recognizing facial emotions. Scene Classification: To classify scenes into categories like urban, rural, or forest, models like image_classifier_vit_base_patch16_224 can be applied effectively. By carefully considering your data, task requirements, and available resources, you can make an informed decision and leverage the best models for your image classification needs. How to use PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Load image data into a DataFrame, discarding any invalid images imageDF = spark.read .format(&quot;image&quot;) .option(&quot;dropInvalid&quot;, value=True) .load(&quot;src/test/resources/image/&quot;) # Image Assembler: Prepares image data for processing imageAssembler = ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) # ViTForImageClassification: Pretrained Vision Transformer model for image classification imageClassifier = ViTForImageClassification .pretrained() .setInputCols([&quot;image_assembler&quot;]) .setOutputCol(&quot;class&quot;) # Create a pipeline with image assembler and classifier stages pipeline = Pipeline().setStages([imageAssembler, imageClassifier]) # Fit the pipeline on the image DataFrame and transform the data pipelineDF = pipeline.fit(imageDF).transform(imageDF) # Select and display the image file name and the classification result pipelineDF .selectExpr(&quot;reverse(split(image.origin, &#39;/&#39;))[0] as image_name&quot;, &quot;class.result&quot;) .show(truncate=False) +--+-+ |image_name |result | +--+-+ |palace.JPEG |[palace] | |egyptian_cat.jpeg|[Egyptian cat] | |hippopotamus.JPEG|[hippopotamus, hippo, river horse, Hippopotamus amphibius]| |hen.JPEG |[hen] | |ostrich.JPEG |[ostrich, Struthio camelus] | |junco.JPEG |[junco, snowbird] | |bluetick.jpg |[bluetick] | |chihuahua.jpg |[Chihuahua] | |tractor.JPEG |[tractor] | |ox.JPEG |[ox] | +--+-+ import com.johnsnowlabs.nlp.annotator._ import com.johnsnowlabs.nlp.ImageAssembler import org.apache.spark.ml.Pipeline // Load image data into a DataFrame, discarding invalid images val imageDF: DataFrame = spark.read .format(&quot;image&quot;) .option(&quot;dropInvalid&quot;, value = true) .load(&quot;src/test/resources/image/&quot;) // Image Assembler: Prepares image data for further processing val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) // Pretrained ViT model for image classification val imageClassifier = ViTForImageClassification .pretrained() .setInputCols(&quot;image_assembler&quot;) .setOutputCol(&quot;class&quot;) // Create a pipeline with the image assembler and classifier stages val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier)) // Fit the pipeline on the image DataFrame and apply transformations val pipelineDF = pipeline.fit(imageDF).transform(imageDF) // Select and display the image name and the classification result pipelineDF .selectExpr(&quot;reverse(split(image.origin, &#39;/&#39;))[0] as image_name&quot;, &quot;class.result&quot;) .show(truncate = false) +--+-+ |image_name |result | +--+-+ |palace.JPEG |[palace] | |egyptian_cat.jpeg|[Egyptian cat] | |hippopotamus.JPEG|[hippopotamus, hippo, river horse, Hippopotamus amphibius]| |hen.JPEG |[hen] | |ostrich.JPEG |[ostrich, Struthio camelus] | |junco.JPEG |[junco, snowbird] | |bluetick.jpg |[bluetick] | |chihuahua.jpg |[Chihuahua] | |tractor.JPEG |[tractor] | |ox.JPEG |[ox] | +--+-+ Try Real-Time Demos! If you want to explore real-time image classification outputs, visit our interactive demos: Swin For Image Classification VisionEncoderDecoder For Image Captioning Object Detection &amp; Scene Classification ConvNext For Image Classification Useful Resources To dive deeper into image classification using Spark NLP, check out these useful resources: Notebooks Image Classification Notebooks in SparkNLP ViT for Image Classification with Transformers",
    "url": "/docs/en/tasks/image_classification",
    "relUrl": "/docs/en/tasks/image_classification"
  },
  "34": {
    "id": "34",
    "title": "Spark NLP 🚀 <span>State of the Art Natural Language Processing</span>",
    "content": "",
    "url": "/",
    "relUrl": "/"
  },
  "35": {
    "id": "35",
    "title": "Infer Meaning & Intent - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/infer_meaning_intent",
    "relUrl": "/infer_meaning_intent"
  },
  "36": {
    "id": "36",
    "title": "Spark NLP - Installation",
    "content": "Spark NLP Cheatsheet # Install Spark NLP from PyPI pip install spark-nlp==6.1.2 # Install Spark NLP from Anaconda/Conda conda install -c johnsnowlabs spark-nlp # Load Spark NLP with Spark Shell spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 # Load Spark NLP with PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 # Load Spark NLP with Spark Submit spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 # Load Spark NLP as external JAR after compiling and building Spark NLP by `sbt assembly` spark-shell --jars spark-nlp-assembly-6.1.2.jar GPU (optional): Spark NLP 6.1.2 is built with ONNX 1.17.0 and TensorFlow 2.7.1 deep learning engines. The minimum following NVIDIA® software are only required for GPU support: NVIDIA® GPU drivers version 450.80.02 or higher CUDA® Toolkit 11.2 cuDNN SDK 8.1.0 Python Spark NLP supports Python 3.7.x and above depending on your major PySpark version. NOTE: Since Spark version 3.2, Python 3.6 is deprecated. If you are using this python version, consider sticking to lower versions of Spark. Quick Install Let’s create a new Conda environment to manage all the dependencies there. You can use Python Virtual Environment if you prefer or not have any environment. $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.8 -y $ conda activate sparknlp $ pip install spark-nlp==6.1.2 pyspark==3.3.1 Of course you will need to have jupyter installed in your system: pip install jupyter Now you should be ready to create a jupyter notebook running from terminal: jupyter notebook Start Spark NLP Session from Python Spark session for Spark NLP can be created (or retrieved) by using sparknlp.start(): import sparknlp spark = sparknlp.start() If you need to manually start SparkSession because you have other configurations and sparknlp.start() is not including them, you can manually start the SparkSession with: spark = SparkSession.builder .appName(&quot;Spark NLP&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;16G&quot;) .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2&quot;) .getOrCreate() If using local jars, you can use spark.jars instead for comma-delimited jar files. For cluster setups, of course, you’ll have to put the jars in a reachable location for all driver and executor nodes. Python without explicit Pyspark installation Pip/Conda If you installed pyspark through pip/conda, you can install spark-nlp through the same channel. Pip: pip install spark-nlp==6.1.2 Conda: conda install -c johnsnowlabs spark-nlp PyPI spark-nlp package / Anaconda spark-nlp package Then you’ll have to create a SparkSession either from Spark NLP: import sparknlp spark = sparknlp.start() Quick example: import sparknlp from sparknlp.pretrained import PretrainedPipeline # create or get Spark Session spark = sparknlp.start() sparknlp.version() spark.version # download, load and annotate a text by pre-trained pipeline pipeline = PretrainedPipeline(&#39;recognize_entities_dl&#39;, &#39;en&#39;) result = pipeline.annotate(&#39;The Mona Lisa is a 16th century oil painting created by Leonardo&#39;) Scala and Java To use Spark NLP you need the following requirements: Java 8 and 11 Apache Spark 3.5.x, 3.4.x, 3.3.x, 3.2.x, 3.1.x, 3.0.x Maven spark-nlp on Apache Spark 3.0.x, 3.1.x, 3.2.x, 3.3.x, and 3.4.x The spark-nlp has been published to the Maven Repository. &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;6.1.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;6.1.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-silicon: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-silicon --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-silicon_2.12&lt;/artifactId&gt; &lt;version&gt;6.1.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-aarch64: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-aarch64 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-aarch64_2.12&lt;/artifactId&gt; &lt;version&gt;6.1.2&lt;/version&gt; &lt;/dependency&gt; SBT spark-nlp on Apache Spark 3.0.x, 3.1.x, 3.2.x, 3.3.x, and 3.4.x // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp&quot; % &quot;6.1.2&quot; spark-nlp-gpu: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-gpu&quot; % &quot;6.1.2&quot; spark-nlp-silicon: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-silicon libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-silicon&quot; % &quot;6.1.2&quot; spark-nlp-aarch64: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-aarch64 libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-aarch64&quot; % &quot;6.1.2&quot; Maven Central: https://mvnrepository.com/artifact/com.johnsnowlabs.nlp If you are interested, there is a simple SBT project for Spark NLP to guide you on how to use it in your projects Spark NLP SBT Starter Command line Spark NLP supports all major releases of Apache Spark 3.0.x, Apache Spark 3.1.x, Apache Spark 3.2.x, Apache Spark 3.3.x, Apache Spark 3.4.x, and Apache Spark 3.5.x This steps require internet connection. Apache Spark 3.x (3.0.x, 3.1.x, 3.2.x, 3.3.x, 3.4.x, and 3.5.x - Scala 2.12) # CPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 The spark-nlp has been published to the Maven Repository. # GPU spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:6.1.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:6.1.2 spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:6.1.2 The spark-nlp-gpu has been published to the Maven Repository. # AArch64 spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-aarch64_2.12:6.1.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-aarch64_2.12:6.1.2 spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-aarch64_2.12:6.1.2 The spark-nlp-aarch64 has been published to the Maven Repository. # Apple Silicon spark-shell --packages com.johnsnowlabs.nlp:spark-nlp-silicon_2.12:6.1.2 pyspark --packages com.johnsnowlabs.nlp:spark-nlp-silicon_2.12:6.1.2 spark-submit --packages com.johnsnowlabs.nlp:spark-nlp-silicon_2.12:6.1.2 The spark-nlp-silicon has been published to the Maven Repository. NOTE: In case you are using large pretrained models like UniversalSentenceEncoder, you need to have the following set in your SparkSession: spark-shell --driver-memory 16g --conf spark.kryoserializer.buffer.max=2000M --packages com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 Installation for Apple Silicon Macs Starting from version 4.0.0, Spark NLP has experimental support for Apple Silicon Macs. Make sure the following prerequisites are met: An Apple Silicon compatible Java version needs to be installed. We recommend Amazon Corretto Java 11, which can be easily installed with SDKMAN!. To check if the installed Java environment is running natively on arm64, you can run the following command: johnsnow@m1mac ~ % realpath $(which java) | file -f - /Users/johnsnow/.sdkman/candidates/java/11.0.27-amzn/bin/java: Mach-O 64-bit executable arm64 Note the executable type arm64. If it says anything else (e.g. universal binary, x86_64 or arm64e) it might not work. The environment variable JAVA_HOME should also be set to this java version. You can check this by running echo $JAVA_HOME in your terminal. If it is not set, you can set it by adding export JAVA_HOME=$(/usr/libexec/java_home) to your ~/.zshrc file. If you are planning to use Annotators or Pipelines that use the RocksDB library (for example WordEmbeddings, TextMatcher or explain_document_dl_en Pipeline respectively) with spark-submit, then a workaround is required to get it working. See Apple Silicon RocksDB workaround for spark-submit with Spark version &gt;= 3.2.0. Scala and Java Installation for Apple Silicon Adding Spark NLP to your Scala or Java project is easy: Simply change to dependency coordinates to spark-nlp-silicon and add the dependency to your project. How to do this is mentioned above: Scala And Java So for example for Spark NLP with Apache Spark 3.0.x and 3.1.x you will end up with maven coordinates like these: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-silicon --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-silicon_2.12&lt;/artifactId&gt; &lt;version&gt;6.1.2&lt;/version&gt; &lt;/dependency&gt; or in case of sbt: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-silicon&quot; % &quot;6.1.2&quot; If everything went well, you can now start Spark NLP with the apple_silicon flag set to true: import com.johnsnowlabs.nlp.SparkNLP val spark = SparkNLP.start(apple_silicon = true) Python for Apple Silicon First, make sure you have a recent Python 3 installation. johnsnow@m1mac ~ % python3 --version Python 3.9.13 Then we can install the dependency as described in the Python section. It is also recommended to use a virtual environment for this. If everything went well, you can now start Spark NLP with the apple_silicon flag set to True: import sparknlp spark = sparknlp.start(apple_silicon=True) Apple Silicon RocksDB workaround for spark-submit with Spark version &gt;= 3.2.0 Starting from Spark version 3.2.0, Spark includes their own version of the RocksDB dependency. Unfortunately, this is an older version of RocksDB does not include the necessary binaries for Apple Silicon. To work around this issue, the default packaged RocksDB jar has to be removed from the Spark distribution. For example, if you downloaded Spark version 3.2.0 from the official archives, you will find the following folders in the directory of Spark: $ ls bin conf data examples jars kubernetes LICENSE licenses NOTICE python R README.md RELEASE sbin yarn To check for the RocksDB jar, you can run $ ls jars | grep rocksdb rocksdbjni-6.20.3.jar to find the jar you have to remove. After removing the jar, the pipelines should work as expected. Installation for Linux Aarch64 Systems Starting from version 4.1.0, Spark NLP supports Linux systems running on an aarch64 processor architecture. The necessary dependencies have been built on Ubuntu 16.04, so a recent system with an environment of at least that will be needed. Check the Python section and the Scala And Java section on to install Spark NLP for your system. Starting Spark NLP Spark NLP needs to be started with the aarch64 flag set to true: For Scala: import com.johnsnowlabs.nlp.SparkNLP val spark = SparkNLP.start(aarch64 = true) For Python: import sparknlp spark = sparknlp.start(aarch64=True) Google Colab Notebook Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account. Run the following code in Google Colab notebook and start using spark-nlp right away. # This is only to setup PySpark and Spark NLP on Colab !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash This script comes with the two options to define pyspark and spark-nlp versions via options: # -p is for pyspark # -s is for spark-nlp # by default they are set to the latest !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.4.0 -s 6.1.2 Spark NLP quick start on Google Colab is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines. Kaggle Kernel Run the following code in Kaggle Kernel and start using spark-nlp right away. # Let&#39;s setup Kaggle for Spark NLP and PySpark !wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash Spark NLP quick start on Kaggle Kernel is a live demo on Kaggle Kernel that performs named entity recognitions by using Spark NLP pretrained pipeline. Apache Zeppelin Use either one of the following options Add the following Maven Coordinates to the interpreter’s library list com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 Add a path to pre-built jar from here in the interpreter’s library list making sure the jar is available to driver path Python in Zeppelin Apart from the previous step, install the python module through pip pip install spark-nlp==6.1.2 Or you can install spark-nlp from inside Zeppelin by using Conda: python.conda install -c johnsnowlabs spark-nlp Configure Zeppelin properly, use cells with %spark.pyspark or any interpreter name you chose. Finally, in Zeppelin interpreter settings, make sure you set properly zeppelin.python to the python you want to use and install the pip library with (e.g. python3). An alternative option would be to set SPARK_SUBMIT_OPTIONS (zeppelin-env.sh) and make sure --packages is there as shown earlier since it includes both scala and python side installation. Jupyter Notebook Recommended: The easiest way to get this done on Linux and macOS is to simply install spark-nlp and pyspark PyPI packages and launch the Jupyter from the same Python environment: $ conda create -n sparknlp python=3.8 -y $ conda activate sparknlp # spark-nlp by default is based on pyspark 3.x $ pip install spark-nlp==6.1.2 pyspark==3.3.1 jupyter $ jupyter notebook Then you can use python3 kernel to run your code with creating SparkSession via spark = sparknlp.start(). Optional: If you are in different operating systems and require to make Jupyter Notebook run by using pyspark, you can follow these steps: export SPARK_HOME=/path/to/your/spark/folder export PYSPARK_PYTHON=python3 export PYSPARK_DRIVER_PYTHON=jupyter export PYSPARK_DRIVER_PYTHON_OPTS=notebook pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 Alternatively, you can mix in using --jars option for pyspark + pip install spark-nlp If not using pyspark at all, you’ll have to run the instructions pointed here Databricks Cluster Install Spark NLP on Databricks Create a cluster if you don’t have one already On a new cluster or existing one you need to add the following to the Advanced Options -&gt; Spark tab: spark.kryoserializer.buffer.max 2000M spark.serializer org.apache.spark.serializer.KryoSerializer In Libraries tab inside your cluster you need to follow these steps: 3.1. Install New -&gt; PyPI -&gt; spark-nlp==6.1.2 -&gt; Install 3.2. Install New -&gt; Maven -&gt; Coordinates -&gt; com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 -&gt; Install Now you can attach your notebook to the cluster and use Spark NLP! NOTE: Databricks’ runtimes support different Apache Spark major releases. Please make sure you choose the correct Spark NLP Maven package name (Maven Coordinate) for your runtime from our Packages Cheatsheet ONNX GPU Inference on Databricks To run infer ONNX models with GPU on Databricks clusters, we need to perform some additional setup steps. ONNX requires CUDA 12 and cuDNN 9 to be installed. Therefore, we need to use Databricks runtimes starting from version 15, as these come with CUDA 12. However, they come with cuDNN 8, which we need to upgrade manually. To do so, we have to add the following script as an init script: #!/bin/bash sudo apt-get update &amp;&amp; sudo apt-get -y install cudnn9-cuda-12 You need to save this script to a shell script file (i.e. upgrade-cudnn9.sh) in your workspace. Afterwards, you need to specify it on your compute resource under the Advanced options section. cuDNN will be upgraded to version 9 on all nodes before Spark is started. Databricks Notebooks You can view all the Databricks notebooks from this address: https://johnsnowlabs.github.io/spark-nlp-workshop/databricks/index.html Note: You can import these notebooks by using their URLs. EMR Cluster To launch EMR clusters with Apache Spark/PySpark and Spark NLP correctly you need to have bootstrap and software configuration. A sample of your bootstrap script #!/bin/bash set -x -e echo -e &#39;export PYSPARK_PYTHON=/usr/bin/python3 export HADOOP_CONF_DIR=/etc/hadoop/conf export SPARK_JARS_DIR=/usr/lib/spark/jars export SPARK_HOME=/usr/lib/spark&#39; &gt;&gt; $HOME/.bashrc &amp;&amp; source $HOME/.bashrc sudo python3 -m pip install awscli boto spark-nlp set +x exit 0 A sample of your software configuration in JSON on S3 (must be public access): [{ &quot;Classification&quot;: &quot;spark-env&quot;, &quot;Configurations&quot;: [{ &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;PYSPARK_PYTHON&quot;: &quot;/usr/bin/python3&quot; } }] }, { &quot;Classification&quot;: &quot;spark-defaults&quot;, &quot;Properties&quot;: { &quot;spark.yarn.stagingDir&quot;: &quot;hdfs:///tmp&quot;, &quot;spark.yarn.preserve.staging.files&quot;: &quot;true&quot;, &quot;spark.kryoserializer.buffer.max&quot;: &quot;2000M&quot;, &quot;spark.serializer&quot;: &quot;org.apache.spark.serializer.KryoSerializer&quot;, &quot;spark.driver.maxResultSize&quot;: &quot;0&quot;, &quot;spark.jars.packages&quot;: &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2&quot; } }] A sample of AWS CLI to launch EMR cluster: aws emr create-cluster --name &quot;Spark NLP 6.1.2&quot; --release-label emr-6.2.0 --applications Name=Hadoop Name=Spark Name=Hive --instance-type m4.4xlarge --instance-count 3 --use-default-roles --log-uri &quot;s3://&lt;S3_BUCKET&gt;/&quot; --bootstrap-actions Path=s3://&lt;S3_BUCKET&gt;/emr-bootstrap.sh,Name=custome --configurations &quot;https://&lt;public_access&gt;/sparknlp-config.json&quot; --ec2-attributes KeyName=&lt;your_ssh_key&gt;,EmrManagedMasterSecurityGroup=&lt;security_group_with_ssh&gt;,EmrManagedSlaveSecurityGroup=&lt;security_group_with_ssh&gt; --profile &lt;aws_profile_credentials&gt; GCP Dataproc Create a cluster if you don’t have one already as follows. At gcloud shell: gcloud services enable dataproc.googleapis.com compute.googleapis.com storage-component.googleapis.com bigquery.googleapis.com bigquerystorage.googleapis.com REGION=&lt;region&gt; BUCKET_NAME=&lt;bucket_name&gt; gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME} REGION=&lt;region&gt; ZONE=&lt;zone&gt; CLUSTER_NAME=&lt;cluster_name&gt; BUCKET_NAME=&lt;bucket_name&gt; You can set image-version, master-machine-type, worker-machine-type, master-boot-disk-size, worker-boot-disk-size, num-workers as your needs. If you use the previous image-version from 2.0, you should also add ANACONDA to optional-components. And, you should enable gateway. Don’t forget to set the maven coordinates for the jar in properties. gcloud dataproc clusters create ${CLUSTER_NAME} --region=${REGION} --zone=${ZONE} --image-version=2.0 --master-machine-type=n1-standard-4 --worker-machine-type=n1-standard-2 --master-boot-disk-size=128GB --worker-boot-disk-size=128GB --num-workers=2 --bucket=${BUCKET_NAME} --optional-components=JUPYTER --enable-component-gateway --metadata &#39;PIP_PACKAGES=spark-nlp spark-nlp-display google-cloud-bigquery google-cloud-storage&#39; --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh --properties spark:spark.serializer=org.apache.spark.serializer.KryoSerializer,spark:spark.driver.maxResultSize=0,spark:spark.kryoserializer.buffer.max=2000M,spark:spark.jars.packages=com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2 On an existing one, you need to install spark-nlp and spark-nlp-display packages from PyPI. Now, you can attach your notebook to the cluster and use the Spark NLP! Apache Spark Support Spark NLP 6.1.2 has been built on top of Apache Spark 3.4 while fully supports Apache Spark 3.0.x, 3.1.x, 3.2.x, 3.3.x, 3.4.x, and 3.5.x Spark NLP Apache Spark 3.5.x Apache Spark 3.4.x Apache Spark 3.3.x Apache Spark 3.2.x Apache Spark 3.1.x Apache Spark 3.0.x Apache Spark 2.4.x Apache Spark 2.3.x 5.4.x YES YES YES YES YES YES NO NO 5.3.x YES YES YES YES YES YES NO NO 5.2.x YES YES YES YES YES YES NO NO 5.1.x Partially YES YES YES YES YES NO NO 5.0.x YES YES YES YES YES YES NO NO 4.4.x YES YES YES YES YES YES NO NO 4.3.x NO NO YES YES YES YES NO NO 4.2.x NO NO YES YES YES YES NO NO 4.1.x NO NO YES YES YES YES NO NO 4.0.x NO NO YES YES YES YES NO NO Find out more about Spark NLP versions from our release notes. Scala and Python Support Spark NLP Python 3.6 Python 3.7 Python 3.8 Python 3.9 Python 3.10 Scala 2.11 Scala 2.12 5.3.x NO YES YES YES YES NO YES 5.2.x NO YES YES YES YES NO YES 5.1.x NO YES YES YES YES NO YES 5.0.x NO YES YES YES YES NO YES 4.4.x NO YES YES YES YES NO YES 4.3.x YES YES YES YES YES NO YES 4.2.x YES YES YES YES YES NO YES 4.1.x YES YES YES YES NO NO YES 4.0.x YES YES YES YES NO NO YES Databricks Support Spark NLP 6.1.2 has been tested and is compatible with the following runtimes: CPU: 9.1 9.1 ML 10.1 10.1 ML 10.2 10.2 ML 10.3 10.3 ML 10.4 10.4 ML 10.5 10.5 ML 11.0 11.0 ML 11.1 11.1 ML 11.2 11.2 ML 11.3 11.3 ML 12.0 12.0 ML 12.1 12.1 ML 12.2 12.2 ML 13.0 13.0 ML 13.1 13.1 ML 13.2 13.2 ML 13.3 13.3 ML 14.0 14.0 ML 14.1 14.1 ML 15.x 15.x ML GPU: 9.1 ML &amp; GPU 10.1 ML &amp; GPU 10.2 ML &amp; GPU 10.3 ML &amp; GPU 10.4 ML &amp; GPU 10.5 ML &amp; GPU 11.0 ML &amp; GPU 11.1 ML &amp; GPU 11.2 ML &amp; GPU 11.3 ML &amp; GPU 12.0 ML &amp; GPU 12.1 ML &amp; GPU 12.2 ML &amp; GPU 13.0 ML &amp; GPU 13.1 ML &amp; GPU 13.2 ML &amp; GPU 13.3 ML &amp; GPU 14.0 ML &amp; GPU 14.1 ML &amp; GPU 15.x ML &amp; GPU EMR Support Spark NLP 6.1.2 has been tested and is compatible with the following EMR releases: emr-6.2.0 emr-6.3.0 emr-6.3.1 emr-6.4.0 emr-6.5.0 emr-6.6.0 emr-6.7.0 emr-6.8.0 emr-6.9.0 emr-6.10.0 emr-6.11.0 emr-6.12.0 emr-6.13.0 emr-6.14.0 Full list of Amazon EMR 6.x releases NOTE: The EMR 6.1.0 and 6.1.1 are not supported. How to create EMR cluster via CLI To lanuch EMR cluster with Apache Spark/PySpark and Spark NLP correctly you need to have bootstrap and software configuration. A sample of your bootstrap script #!/bin/bash set -x -e echo -e &#39;export PYSPARK_PYTHON=/usr/bin/python3 export HADOOP_CONF_DIR=/etc/hadoop/conf export SPARK_JARS_DIR=/usr/lib/spark/jars export SPARK_HOME=/usr/lib/spark&#39; &gt;&gt; $HOME/.bashrc &amp;&amp; source $HOME/.bashrc sudo python3 -m pip install awscli boto spark-nlp set +x exit 0 A sample of your software configuration in JSON on S3 (must be public access): [{ &quot;Classification&quot;: &quot;spark-env&quot;, &quot;Configurations&quot;: [{ &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;PYSPARK_PYTHON&quot;: &quot;/usr/bin/python3&quot; } }] }, { &quot;Classification&quot;: &quot;spark-defaults&quot;, &quot;Properties&quot;: { &quot;spark.yarn.stagingDir&quot;: &quot;hdfs:///tmp&quot;, &quot;spark.yarn.preserve.staging.files&quot;: &quot;true&quot;, &quot;spark.kryoserializer.buffer.max&quot;: &quot;2000M&quot;, &quot;spark.serializer&quot;: &quot;org.apache.spark.serializer.KryoSerializer&quot;, &quot;spark.driver.maxResultSize&quot;: &quot;0&quot;, &quot;spark.jars.packages&quot;: &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2&quot; } } ] A sample of AWS CLI to launch EMR cluster: aws emr create-cluster --name &quot;Spark NLP 6.1.2&quot; --release-label emr-6.2.0 --applications Name=Hadoop Name=Spark Name=Hive --instance-type m4.4xlarge --instance-count 3 --use-default-roles --log-uri &quot;s3://&lt;S3_BUCKET&gt;/&quot; --bootstrap-actions Path=s3://&lt;S3_BUCKET&gt;/emr-bootstrap.sh,Name=custome --configurations &quot;https://&lt;public_access&gt;/sparknlp-config.json&quot; --ec2-attributes KeyName=&lt;your_ssh_key&gt;,EmrManagedMasterSecurityGroup=&lt;security_group_with_ssh&gt;,EmrManagedSlaveSecurityGroup=&lt;security_group_with_ssh&gt; --profile &lt;aws_profile_credentials&gt; GCP Dataproc Support Create a cluster if you don’t have one already as follows. At gcloud shell: gcloud services enable dataproc.googleapis.com compute.googleapis.com storage-component.googleapis.com bigquery.googleapis.com bigquerystorage.googleapis.com REGION=&lt;region&gt; BUCKET_NAME=&lt;bucket_name&gt; gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME} REGION=&lt;region&gt; ZONE=&lt;zone&gt; CLUSTER_NAME=&lt;cluster_name&gt; BUCKET_NAME=&lt;bucket_name&gt; You can set image-version, master-machine-type, worker-machine-type, master-boot-disk-size, worker-boot-disk-size, num-workers as your needs. If you use the previous image-version from 2.0, you should also add ANACONDA to optional-components. And, you should enable gateway. gcloud dataproc clusters create ${CLUSTER_NAME} --region=${REGION} --zone=${ZONE} --image-version=2.0 --master-machine-type=n1-standard-4 --worker-machine-type=n1-standard-2 --master-boot-disk-size=128GB --worker-boot-disk-size=128GB --num-workers=2 --bucket=${BUCKET_NAME} --optional-components=JUPYTER --enable-component-gateway --metadata &#39;PIP_PACKAGES=spark-nlp spark-nlp-display google-cloud-bigquery google-cloud-storage&#39; --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh On an existing one, you need to install spark-nlp and spark-nlp-display packages from PyPI. Now, you can attach your notebook to the cluster and use the Spark NLP! Amazon Linux 2 Support # Update Package List &amp; Install Required Packages sudo yum update sudo yum install -y amazon-linux-extras sudo yum -y install python3-pip # Create Python virtual environment and activate it: python3 -m venv .sparknlp-env source .sparknlp-env/bin/activate Check JAVA version: For Sparknlp versions above 3.x, please use JAVA-11 Checking Java versions installed on your machine: sudo alternatives --config java You can pick the index number (I am using java-8 as default - index 2): If you dont have java-11 or java-8 in you system, you can easily install via: sudo yum install java-1.8.0-openjdk Now, we can start installing the required libraries: pip install pyspark==3.3.1 pip install spark-nlp Docker Support For having Spark NLP, PySpark, Jupyter, and other ML/DL dependencies as a Docker image you can use the following template: #Download base image ubuntu 18.04 FROM ubuntu:18.04 ENV NB_USER jovyan ENV NB_UID 1000 ENV HOME /home/${NB_USER} ENV PYSPARK_PYTHON=python3 ENV PYSPARK_DRIVER_PYTHON=python3 RUN apt-get update &amp;&amp; apt-get install -y tar wget bash rsync gcc libfreetype6-dev libhdf5-serial-dev libpng-dev libzmq3-dev python3 python3-dev python3-pip unzip pkg-config software-properties-common graphviz RUN adduser --disabled-password --gecos &quot;Default user&quot; --uid ${NB_UID} ${NB_USER} # Install OpenJDK-8 RUN apt-get update &amp;&amp; apt-get install -y openjdk-8-jdk &amp;&amp; apt-get install -y ant &amp;&amp; apt-get clean; # Fix certificate issues RUN apt-get update &amp;&amp; apt-get install ca-certificates-java &amp;&amp; apt-get clean &amp;&amp; update-ca-certificates -f; # Setup JAVA_HOME -- useful for docker commandline ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/ RUN export JAVA_HOME RUN echo &quot;export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/&quot; &gt;&gt; ~/.bashrc RUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* RUN pip3 install --upgrade pip # You only need pyspark and spark-nlp paclages to use Spark NLP # The rest of the PyPI packages are here as examples RUN pip3 install --no-cache-dir pyspark spark-nlp==3.2.3 notebook==5.* numpy pandas mlflow Keras scikit-spark scikit-learn scipy matplotlib pydot tensorflow==2.4.1 graphviz # Make sure the contents of our repo are in ${HOME} RUN mkdir -p /home/jovyan/tutorials RUN mkdir -p /home/jovyan/jupyter COPY data ${HOME}/data COPY jupyter ${HOME}/jupyter COPY tutorials ${HOME}/tutorials RUN jupyter notebook --generate-config COPY jupyter_notebook_config.json /home/jovyan/.jupyter/jupyter_notebook_config.json USER root RUN chown -R ${NB_UID} ${HOME} USER ${NB_USER} WORKDIR ${HOME} # Specify the default command to run CMD [&quot;jupyter&quot;, &quot;notebook&quot;, &quot;--ip&quot;, &quot;0.0.0.0&quot;] Finally, use jupyter_notebook_config.json for the password: { &quot;NotebookApp&quot;: { &quot;password&quot;: &quot;sha1:65adaa6ffb9c:36df1c2086ef294276da703667d1b8ff38f92614&quot; } } Windows Support In order to fully take advantage of Spark NLP on Windows (8 or 10), you need to setup/install Apache Spark, Apache Hadoop, Java and a Pyton environment correctly by following the following instructions: https://github.com/JohnSnowLabs/spark-nlp/discussions/1022 How to correctly install Spark NLP on Windows Follow the below steps to set up Spark NLP with Spark 3.2.3: Download Adopt OpenJDK 1.8 Make sure it is 64-bit Make sure you install it in the root of your main drive C: java. During installation after changing the path, select setting Path Download the pre-compiled Hadoop binaries winutils.exe, hadoop.dll and put it in a folder called C: hadoop bin from https://github.com/cdarlint/winutils/tree/master/hadoop-3.2.0/bin Note: The version above is for Spark 3.2.3, which was built for Hadoop 3.2.0. You might have to change the hadoop version in the link, depending on which Spark version you are using. Download Apache Spark 3.2.3 and extract it to C: spark. Set/add environment variables for HADOOP_HOME to C: hadoop and SPARK_HOME to C: spark. Add %HADOOP_HOME% bin and %SPARK_HOME% bin to the PATH environment variable. Install Microsoft Visual C++ 2010 Redistributed Package (x64). Create folders C: tmp and C: tmp hive If you encounter issues with permissions to these folders, you might need to change the permissions by running the following commands: %HADOOP_HOME% bin winutils.exe chmod 777 /tmp/hive %HADOOP_HOME% bin winutils.exe chmod 777 /tmp/ Requisites for PySpark We recommend using conda to manage your Python environment on Windows. Download Miniconda for Python 3.8 See Quick Install on how to set up a conda environment with Spark NLP. The following environment variables need to be set: PYSPARK_PYTHON=python Optionally, if you want to use the Jupyter Notebook runtime of Spark: first install it in the environment with conda install notebook then set PYSPARK_DRIVER_PYTHON=jupyter, PYSPARK_DRIVER_PYTHON_OPTS=notebook The environment variables can either be directly set in windows, or if only the conda env will be used, with conda env config vars set PYSPARK_PYTHON=python. After setting the variable with conda, you need to deactivate and re-activate the environment. Now you can use the downloaded binary by navigating to %SPARK_HOME% bin and running Either create a conda env for python 3.6, install pyspark==3.3.1 spark-nlp numpy and use Jupyter/python console, or in the same conda env you can go to spark bin for pyspark –packages com.johnsnowlabs.nlp:spark-nlp_2.12:6.1.2. Offline Spark NLP library and all the pre-trained models/pipelines can be used entirely offline with no access to the Internet. If you are behind a proxy or a firewall with no access to the Maven repository (to download packages) or/and no access to S3 (to automatically download models and pipelines), you can simply follow the instructions to have Spark NLP without any limitations offline: Instead of using the Maven package, you need to load our Fat JAR Instead of using PretrainedPipeline for pretrained pipelines or the .pretrained() function to download pretrained models, you will need to manually download your pipeline/model from Models Hub, extract it, and load it. Example of SparkSession with Fat JAR to have Spark NLP offline: spark = SparkSession.builder .appName(&quot;Spark NLP&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;,&quot;16G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars&quot;, &quot;/tmp/spark-nlp-assembly-6.1.2.jar&quot;) .getOrCreate() You can download provided Fat JARs from each release notes, please pay attention to pick the one that suits your environment depending on the device (CPU/GPU) and Apache Spark version (3.x) If you are local, you can load the Fat JAR from your local FileSystem, however, if you are in a cluster setup you need to put the Fat JAR on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., hdfs:///tmp/spark-nlp-assembly-6.1.2.jar) Example of using pretrained Models and Pipelines in offline: # instead of using pretrained() for online: # french_pos = PerceptronModel.pretrained(&quot;pos_ud_gsd&quot;, lang=&quot;fr&quot;) # you download this model, extract it, and use .load french_pos = PerceptronModel.load(&quot;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) # example for pipelines # instead of using PretrainedPipeline # pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;) # you download this pipeline, extract it, and use PipelineModel PipelineModel.load(&quot;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&quot;) Since you are downloading and loading models/pipelines manually, this means Spark NLP is not downloading the most recent and compatible models/pipelines for you. Choosing the right model/pipeline is on you If you are local, you can load the model/pipeline from your local FileSystem, however, if you are in a cluster setup you need to put the model/pipeline on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., hdfs:///tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/) Compiled JARs Build from source spark-nlp FAT-JAR for CPU on Apache Spark 3.0.x, 3.1.x, 3.2.x, 3.3.x, 3.4.x, and 3.5.x sbt assembly FAT-JAR for GPU on Apache Spark 3.0.x, 3.1.x, 3.2.x, 3.3.x, 3.4.x, and 3.5.x sbt -Dis_gpu=true assembly FAT-JAR for M! on Apache Spark 3.0.x, 3.1.x, 3.2.x, 3.3.x, 3.4.x, and 3.5.x sbt -Dis_silicon=true assembly Using the jar manually If for some reason you need to use the JAR, you can either download the Fat JARs provided here or download it from Maven Central. To add JARs to spark programs use the --jars option: spark-shell --jars spark-nlp.jar The preferred way to use the library when running spark programs is using the --packages option as specified in the spark-packages section. OpenVINO Spark NLP supports inference and model saving using OpenVINO from version 5.4.2, enabling optimized inference for specific models. OpenVINO is an open-source toolkit for optimizing and deploying deep learning models from cloud to edge. It accelerates deep learning inference across various use cases, such as generative AI, video, audio, and language with models from popular frameworks like PyTorch, TensorFlow, ONNX, and more. For an example on how to use OpenVINO with Spark NLP, see the examples folder. Requirements To run models with OpenVINO, Intel® Threading Building Blocks (Intel® TBB) needs to be available on your system. If not available, you will run into “UnsatisfiedLinkError” exceptions during runtime. For example, to install TBB on Ubuntu we can run sudo apt update &amp;&amp; sudo apt install libtbb-dev",
    "url": "/docs/en/install",
    "relUrl": "/docs/en/install"
  },
  "37": {
    "id": "37",
    "title": "Installation",
    "content": "Deploy using Docker For deploying NLP Server on your instance run the following command. docker run --pull=always -p 5000:5000 johnsnowlabs/nlp-server:latest This will check if the latest docker image is available on your local machine and if not it will automatically download and run it. If you want to keep downloaded models between restarts of the docker image, you can mount a volume. mkdir /var/cache_pretrained chown 1000:1000 /var/cache_pretrained docker run --pull=always -v /var/cache_pretrained:/home/johnsnowlabs/cache_pretrained -p 5000:5000 johnsnowlabs/nlp-server:latest Deploy using AWS Marketplace NLP Server on AWS Marketplace provides one of the fastest and easiest ways to get up and running on Amazon Web Services (AWS). NLP Server is available through AWS Marketplace free of charge. However, to use licensed spells in NLP Server, you need to buy our license from here. You can get NLP Server on AWS Marketplace from this URL. Follow the seven steps instructions or the video tutorial given below to learn how to deploy NLP Server using AWS Marketplace. Make sure you have a valid AWS account and log in to the AWS Marketplace using your credentials. Deploy NLP Server via AWS Marketplace 1.Click on Continue to subscribe button for creating a subscription to the NLP Server product. The software is free of charge. 2.Read the subscription EULA and click on Accept terms button if you want to continue. 3.In a couple of seconds the subscription becomes active. Once it is active you see this screen. 4.Go to AWS Marketplace &gt; Manage subscriptions and click on the Launch new instance button corresponding to the NLP Server subscription. This will redirect you to the following screen. Click on Continue to launch through EC2 button. 5.From the available options select the instance type you want to use for the deployment. Then click on Review and Lauch button. 6.Select an existing key pair or create a new one. This ensures a secured connection to the instance. If you create a new key make sure that you download and safely store it for future usage. Click on the Launch button. 7.While the instance is starting you will see this screen. Then the instance will appear on your EC2 Instances list. The NLP Server can now be accessed via a web browser at http://PUBLIC_EC2_IP . API documentation is also available at http://PUBLIC_EC2_IP/docs Deploy using Azure Marketplace NLP Server on Azure Marketplace provides one of the fastest and easiest ways to get up and running on Microsoft Azure. NLP Server is available through Azure Marketplace free of charge. However, to use licensed spells in NLP Server, you need to buy our license from here. You can get NLP Server on Azure Marketplace from this URL. Follow the video tutorial given below to learn how to deploy NLP Server using Azure Marketplace. Deploy NLP Server using Azure Marketplace",
    "url": "/docs/en/nlp_server/installation",
    "relUrl": "/docs/en/nlp_server/installation"
  },
  "38": {
    "id": "38",
    "title": "Labs, Tests, and Vitals - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/labs_tests_and_vitals",
    "relUrl": "/labs_tests_and_vitals"
  },
  "39": {
    "id": "39",
    "title": "Spark NLP - Tasks",
    "content": "Spark NLP is the central hub for all your State of the Art Natural Language Processing needs. Whether you’re looking for demos, use cases, models, or datasets, you’ll find the resources you need to begin any NLP task right here! Natural Language Processing Text Classification Text classification is the process of automatically categorizing text into predefined labels or categories based on its content. Learn More Token Classification Token classification is the process of assigning labels to individual tokens (words or subwords) in a text, commonly used for tasks like named entity recognition or part-of-speech tagging. Learn More Zero-Shot Classification Zero-shot classification is the process of categorizing text into labels without the model having seen any examples of those labels during training, using general knowledge and context. Learn More Text Generation Text generation is the process of automatically creating coherent and contextually relevant text based on a given input or prompt using machine learning models. Learn More Question Answering Question answering models can retrieve answers from a given text, making them useful for searching documents. Some models can even generate answers independently, without needing any context! Learn More Table Question Answering Table question answering models can extract answers from structured data in tables, making it easy to query and retrieve specific information. Learn More Summarization Summarization models condense long texts into shorter versions, capturing the main ideas and key points while maintaining the overall meaning of the original content. Learn More Translation Translation models automatically convert text from one language to another while preserving the meaning and context of the original content. Learn More Text Preprocessing Text Preprocessing is the task of cleaning and transforming raw text into a format suitable for NLP tasks. This includes steps like tokenization, lowercasing, removing stop words, and stemming or lemmatization to prepare text for analysis. Learn More Dependency Parsing Dependency Parsing is a syntactic analysis method that examines the grammatical structure of a sentence by identifying the dependencies between its words. It illustrates how words relate to each other through a dependency tree or graph, where some words act as “parents” and others as “children.” Learn More Computer Vision Image Classification Image classification models automatically categorize images into predefined labels or classes based on their visual content. Learn More Image Captioning Image captioning models generate descriptive text for images, providing context and details about the visual content they depict. Learn More Zero-Shot Image Classification Zero-shot image classification is the process of categorizing images into labels without the model having seen any examples of those labels during training, using general knowledge and context. Learn More Audio Automatic Speech Recognition Automatic speech recognition (ASR) is the process of converting spoken language into written text. Learn More",
    "url": "/docs/en/tasks/landing_page",
    "relUrl": "/docs/en/tasks/landing_page"
  },
  "40": {
    "id": "40",
    "title": "African Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/languages_africa",
    "relUrl": "/languages_africa"
  },
  "41": {
    "id": "41",
    "title": "Languages of India - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/languages_india",
    "relUrl": "/languages_india"
  },
  "42": {
    "id": "42",
    "title": "Learn",
    "content": "Introductions to Spark NLP Videos State of the Art Natural Language Processing at Scale. David Talby - April 13, 2020 Spark NLP: State of the art natural language processing at scale. David Talby - 4 Jun 2020 What is Spark NLP. John Snow Labs - 30 Jul 2019 Apache Spark NLP Extending Spark ML to Deliver Fast, Scalable, and Unified Natural Language Process. David Talby - 6 May 2019 Natural Language Understanding at Scale with Spark Native NLP, Spark ML &amp;TensorFlow with Alex Thomas. Alex Thomas - 26 Oct 2017 Articles Introducing the Natural Language Processing Library for Apache SparkDavid Talby - October 19, 2017 Improving Clinical Document Understanding on COVID-19 Research with Spark NLPVeysel Kocaman, David Talby - 7 December, 2020 Topic Modelling with PySpark and Spark NLPMaria Obedkova - May 29, 2020 Installing Spark NLP and Spark OCR in air-gapped networks (offline mode)Veysel Kocaman - May 04, 2020 Cleaning and extracting text from HTML/XML documents by using Spark NLPStefano Lori - Jan 13, 2020 A Google Colab Notebook Introducing Spark NLPVeysel Kocaman - September, 2020 State-of-the-art Natural Language Processing at ScaleDavid Talby - April 13, 2020 How to Wrap Your Head Around Spark NLPMustafa Aytuğ Kaya - August 25, 2020 5 Reasons Why Spark NLP Is The Most Widely Used Library In EnterprisesAmbika Choudhury - May 28, 2019 My Experience with SparkNLP Workshop &amp; CertificationAngelina Maria Leigh - August 17, 2020 Out of the box Spark NLP models in actionDia Trambitas - August 14, 2020 Get started with Machine Learning in Java using Spark NLPWill Price - August 27, 2020 SPARK NLP 3: MASSIVE SPEEDUPS &amp; THE LATEST COMPUTE PLATFORMSMaziyar Panahi - March 25, 2021 SPARK NLP 2.7: 720+ NEW MODELS &amp; PIPELINES FOR 192 LANGUAGES!David Talby - January 05, 2021 Python’s NLU Library Videos &quot;Python&#39;s NLU library: 1,000+ Models, 200+ Languages, 1 Line of Code&quot; by: Christian Kasim Loan - 18 June 2021 John Snow Labs NLU: Become a Data Science Superhero with One Line of Python code. Christian Kasim Loan - November, 2020 Articles 1 line to GLOVE Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to XLNET Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to ALBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to COVIDBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to ELECTRA Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to BioBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 Line of Code, 350 + NLP Models with John Snow Labs’ NLU in PythonChristian Kasim Loan - September 21, 2020 Easy sentence similarity with BERT Sentence Embeddings using John Snow Labs NLUChristian Kasim Loan - November 20, 2020 Training Deep Learning NLP Classifier TutorialChristian Kasim Loan - November 20, 2020 1 Python Line for ELMo Word Embeddings and t-SNE plots with John Snow Labs’ NLUChristian Kasim Loan - October 24, 2020 1 line of Python code for BERT, ALBERT, ELMO, ELECTRA, XLNET, GLOVE, Part of Speech with NLU and t-SNEChristian Kasim Loan - September 21, 2020 1 line to BERT Word Embeddings with NLU in PythonChristian Kasim Loan - September 21, 2020 Question answering, intent classification, aspect based ner, and new multilingual models in python’s NLU libraryChristian Kasim Loan - February 12, 2021 Intent and action classification, analyze chinese news and crypto market, 200+ languages &amp; answer questions with NLU 1.1.3Christian Kasim Loan - March 02, 2021 Hindi wordembeddings, bengali named entity recognition, 30+ new models, analyze crypto news with NLU 1.1.2Christian Kasim Loan - February 18, 2021 Named Entity Recognition Videos State-of-the-art Clinical Named Entity Recognition in Spark NLP Workshop - Veysel Kocaman Train your own NerDL. John Snow Labs - 7 Oct 2019 Articles State-of-the-art named entity recognition with BERTVeysel Kocaman - February 26th, 2020 State-of-the-art Named Entity Recognition in Spark NLPVeysel Kocaman Spark NLP in action: intelligent, high-accuracy fact extraction from long financial documentsSaif Addin Ellafi - May 5, 2020 Named Entity Recognition (NER) with BERT in Spark NLPVeysel Kocaman - Mar 4, 2020 Document Classification Videos Spark NLP in Action: Learning to read Life Science research - Saif Addin Ellafi. Saif Addin Ellafi - 1 Aug 2018 State of the art emotion and sentiment analysis with Spark NLP (Data science Salon). Dia Trambitas - December 1, 2020 Articles GloVe, ELMo &amp; BERT. A guide to state-of-the-art text classification using Spark NLP Ryan Burke - March 16, 2021 Distributed Topic Modelling using Spark NLP and Spark MLLib(LDA)Satish Silveri - June 11, 2020 Text Classification in Spark NLP with Bert and Universal Sentence EncodersVeysel Kocaman - April 12, 2020 Classification of Unstructured Documents into the Environmental, Social &amp; Governance (ESG) TaxonomyAlina Petukhova - May, 2020 Using Spark NLP to build a drug discovery knowledge graph for COVID-19Vishnu Vettrivel, Alexander Thomas - October 8, 2020 Build Text Categorization Model with Spark NLPSatish Silveri - Jul 8 2020 Topic Modelling with PySpark and Spark NLPMaria Obedkova - May 29 2020 Spark NLP Tasks &amp; Pipelines Videos Spark NLP Annotators, Annotations and Pipelines. John Snow Labs - 23 Oct 2019 Your first Spark NLP Pipeline. John Snow Labs - 23 Oct 2019 Natural Language Understanding at Scale with Spark NLP | DSS 2020. Veysel Kocaman - December 12, 2020 Articles Cleaning and extracting text from HTML/XML documents by using Spark NLPStefano Lori - January 13 2021 NER model with ELMo Embeddings in 5 minutes in Spark-NLPChristian Kasim Loan - Jule 2020 Applying Context Aware Spell Checking in Spark NLPAlberto Andreotti - May 2020 Spark nlp 2.5 delivers state-of-the-art accuracy for spell checking and sentiment analysisIda Lucente - May 12, 2020 Spark NLP 2.4: More Accurate NER, OCR, and Entity ResolutionIda Lucente - February 14, 2020 Introduction to Spark NLP: Foundations and Basic Components (Part-I)Veysel Kocaman - Sep 29, 2019 Introducing Spark NLP: Why would we need another NLP library (Part-I)Veysel Kocaman - October 22, 2019 Introducing Spark NLP: basic components and underlying technologies (Part-III)Veysel Kocaman - December 2, 2019 Explain document DL – Spark NLP pretrained pipelineVeysel Kocaman - January 15, 2020 Spark NLP Walkthrough, powered by TensorFlowSaif Addin Ellafi - Nov 19, 2018 Natural Language Processing with PySpark and Spark-NLPAllison Honold - Feb 5, 2020 Spark NLP for Healthcare Videos Advancing the State of the Art in Applied Natural Language Processing | Healthcare NLP Summit 2021. David Talby - 21 Apr 2021 How to Apply State-of-the-Art Natural Language Processing in Healthcare. David Talby - 15 Sep 2020 Advanced Natural Language Processing with Apache Spark NLP. David Talby - 20 Aug 2020 Applying State-of-the-art Natural Language Processing for Personalized Healthcare. David Talby - April 13, 2020 State-of-the-art Natural Language Processing at Scale. David Talby - April 13, 2020 Apache SPARK NLP: Extending SPARK ML to Deliver Fast, Scalable &amp; Unified Natural Language Processing. David Talby - June 04, 2018 State of the Art Natural Language Processing at Scale. David Talby - June 04, 2018 Spark NLP in Action: Learning to read Life Science research. Saif Addin Ellafi - May 28, 2018 Natural Language Understanding at Scale with Spark-Native NLP, Spark ML, and TensorFlow. Alexander Thomas - October 14, 2018 Apache Spark NLP for Healthcare: Lessons Learned Building Real-World Healthcare AI Systems. Veysel Kocaman - 9 Jul 2020 SNOMED entity resolver. John Snow Labs - 31 Jul 2020 NLP and its applications in Healthcare. Veysel Kocaman - 17 May 2020 Lessons Learned Building Real-World Healthcare AI Systems. Veysel Kocaman - April 13, 2020 Application of Spark NLP for Development of Multi-Modal Prediction Model from EHR | Healthcare NLP. Sutanay Choudhury - 14 Apr 2021 Best Practices in Improving NLP Accuracy for Clinical Use Cases I Healthcare NLP Summit 2021. Rajesh Chamarthi, Veysel Kocaman - 15 Apr 2021 Articles Contextual Parser: Increased Flexibility Extracting Entities in Spark NLPLuca Martial - Feb 09 2022 Named Entity Recognition for Healthcare with SparkNLP NerDL and NerCRFMaggie Yilmaz - Jul 20 2020 Roche automates knowledge extraction from pathology reports with Spark NLPCase Study Spark NLP in action: Improving patient flow forecastingCase Study Using Spark NLP to Enable Real-World Evidence (RWE) and Clinical Decision Support in OncologyVeysel Kocaman - April 13, 2020 Applying State-of-the-art Natural Language Processing for Personalized HealthcareDavid Talby - April 13, 2020 Automated Mapping of Clinical Entities from Natural Language Text to Medical TerminologiesAndrés Fernández - April 29 2020 Contextual Parser in Spark NLP: Extracting Medical Entities ContextuallyAlina Petukhova - May 28 2020 Deep6 accelerates clinical trial recruitment with Spark NLPCase Study SelectData uses AI to better understand home health patientsCase Study Explain Clinical Document Spark NLP Pretrained PipelineVeysel Kocaman - January 20, 2020 Introducing Spark NLP: State of the art NLP Package (Part-II)Veysel Kocaman - January 20, 2020 Automated Adverse Drug Event (ADE) Detection from Text in Spark NLP with BioBertVeysel Kocaman - Octover 4, 2020 Normalize drug names and dosage units with spark NLPDavid Cecchini - February 23, 2021 Spark NLP for healthcare 2.7.3 with biobert extraction models, higher accuracy, de-identification, new radiology ner model &amp; moreVeysel Kocaman - February 09, 2021 Spark OCR &amp; De-Identification Videos Maximizing Text Recognition Accuracy with Image Transformers in Spark OCR. Mykola Melnyk - June 24, 2020 Accurate de-identification, obfuscation, and editing of scanned medical documents and images. Alina Petukhova - August 19, 2020 Accurate De-Identification of Structured &amp; Unstructured Medical Data at Scale. Julio Bonis - March 18, 2020 Articles A Unified CV, OCR &amp; NLP Model Pipeline for Document Understanding at DocuSignPatrick Beukema, Michael Chertushkin - October 6, 2020 Scaling High-Accuracy Text Extraction from Images using Spark OCR on DatabricksMikola Melnyk - July 2, 2020 Spark NLP at Scale Videos Turbocharging State-of-the-art Natural Language Processing on Ray. David Talby - October 3, 2020 Articles Big Data Analysis of Meetup Events using Spark NLP, Kafka and Vegas VisualizationAndrei Deuşteanu - August 25, 2020 Setup Spark NLP on Databricks in 2 Minutes and get the taste of scalable NLPChristian Kasim Loan - May 25, 2020 Real-time trending topic detection using Spark NLP, Kafka and Vegas VisualizationValentina Crisan - Oct 15, 2020 Mueller Report for Nerds! Spark meets NLP with TensorFlow and BERTMaziyar Panahi - May 1, 2019 Spark in Docker in Kubernetes: A Practical Approach for Scalable NLPJürgen Schmidl - Jan 18 2020 Running Spark NLP in Docker Container for Named Entity Recognition and Other NLP FeaturesYuefeng Zhang - Jun 5 2020 Annotation Lab Videos Accelerating Clinical Data Abstraction and Real-World Data Curation with Active Learning, Dia Trambitas - Apr 15, 2021 MLOPS Veysel &amp; Dia. Dia Trambitas, Veysel Kocaman - July 16, 2020 Best Practices &amp; Tools for Accurate Document Annotation and Data Abstraction. Dia Trambitas - May 27, 2020 Articles John Snow Labs’ data annotator &amp; active learning for human-in-the-loop AI is now included with all subscriptionsIda Lucente - May 26, 2020 Auto NLP: Pretrain, Tune &amp; Deploy State-of-the-art Models Without CodingDia Trambitas - October 6, 2020 Lesson Learned annotating training data for healthcare NLP projectsRebecca Leung, Marianne Mak - October 8, 2020 Task review workflows in the annotation labDia Trambitas - March 08, 2021 The annotation lab 1.1 is here with improvements to speed, accuracy, and productivityIda Lucente - January 20, 2021 Tips and tricks on how to annotate assertion in clinical textsMauro Nievas Offidani - November 24, 2020 Spark NLP Benchmarks Articles Biomedical Named Entity Recognition at ScaleVeysel Kocaman, David Talby - November 12, 2020 NLP Industry Survey Analysis: the industry landscape of natural language use cases in 2020Paco Nathan - October 6, 2020 Comparing the Functionality of Open Source Natural Language Processing LibrariesMaziyar Panahi and David Talby - April 7, 2019 SpaCy or Spark NLP — A Benchmarking ComparisonMustafa Aytuğ Kaya - Aug 27, 2020 Comparing production-grade NLP libraries: Training Spark-NLP and spaCy pipelinesSaif Addin Ellafi - February 28, 2018 Comparing production-grade NLP libraries: Running Spark-NLP and spaCy pipelinesSaif Addin Ellafi - February 28, 2018 Comparing production-grade NLP libraries: Accuracy, performance, and scalabilitySaif Addin Ellafi - February 28, 2018 Spark NLP Awards Articles John Snow Labs is healthcare tech outlook’s 2020 healthcare analytics provider of the yearIda Lucente - July 14, 2020 John Snow Labs wins the 2020 artificial intelligence excellence awardIda Lucente - April 27, 2020 John Snow Labs is named ‘2019 ai platform of the yearIda Lucente - August 14, 2019 Spark NLP is the world’s most widely used nlp library by enterprise practitionersIda Lucente - May 6, 2019 John Snow Labs’ spark nlp wins “most significant open source project” at the strata data awardsIda Lucente April 1 - 2019 John Snow Labs named “artificial intelligence solution provider of the year” by cio reviewIda Lucente - February 7, 2019",
    "url": "/learnold",
    "relUrl": "/learnold"
  },
  "43": {
    "id": "43",
    "title": "The NLP Learning Hub",
    "content": "The Technology Spark NLP Auto NLP The Technology in Action NLP on Databricks Industry Trends No-Code AI Responsible NLP Data Philanthropy Announcements Awards",
    "url": "/learn",
    "relUrl": "/learn"
  },
  "44": {
    "id": "44",
    "title": "Legal Document Splitting - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/long_document_splitting",
    "relUrl": "/long_document_splitting"
  },
  "45": {
    "id": "45",
    "title": "Middle Eastern Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/middle_eastern_languages",
    "relUrl": "/middle_eastern_languages"
  },
  "46": {
    "id": "46",
    "title": "Spark NLP - Experiment Tracking",
    "content": "Serialization and Experiment Tracking with MLFlow (Python) About MLFLow Spark NLP uses Spark MLlib Pipelines, what are natively supported by MLFlow. MLFlow is, as stated in their official webpage, an open source platform for the machine learning lifecycle, that includes: Mlflow Tracking: Record and query experiments: code, data, config, and results MLflow Projects: Package data science code in a format to reproduce runs on any platform MLflow Models: Deploy machine learning models in diverse serving environments Model Registry: Store, annotate, discover, and manage models in a central repository MLFlow is also integrated in Databricks, so you will be able to track your experiments in any Databricks environment, and even use MLFLow Model Registry to serve models for production purposes, using the REST API (see section “Productionizing Spark NLP”). We will be using in this documentation Jupyter Notebook syntax. Available configurations There are several ways of deploying a MLFlow Model Registry: 1) Scenario 1: MLflow on localhost with no Tracking Server: This scenario uses a localhost folder (./mlruns by default) to serialize and store your models, but there is no tracking server available (version tracking will be disabled). 2) Scenario 2: MLflow on localhost with a Tracking Server This scenario uses a localhost folder (./mlruns by default) to serialize and store your mdoels, and a database as a Tracking Sever. It uses SQLAlchemy under the hood, so the following databases are supported: mssql, postgresql, mysql, sqlite. We are going to show how to implement this scenario with a mysql database. 3) Scenario 3: MLflow on remote with a Tracking Server This scenario is a remote version of Scenario 2. It uses a remote S3 bucket to serialize and store your mdoels, and a database as a Tracking Sever. Again, it uses SQLAlchemy for the Tracking Server under the hood, so the following databases are supported: mssql, postgresql, mysql, sqlite. In this case, you can use any service as AWS RDS or Azure SQL Database. Requirements As we said before, we are going to showcase Scenario 2. Since we want to have a Experiment Tracking Server with mysql, we will need to install in our server the requirements for it. !sudo apt-get install -y python-mysqldb mysql-server libmysqlclient-dev Also, let’s install a mysql Python interface library, called pymsql, to access mysql databases. !pip install mysqlclient pymysql We will also need MLFlow (this example was tested with version 1.21.0) !pip install mlflow Finally, make sure you follow the Spark NLP installation, available here Instantiating a MySQL database We are going to use Docker to instantiate a MySQL container with a persistent volume, but you can install it directly on your machine without Docker. To do that, we will need to have installed (feel free to skip this step if you will install MySql without Docker): Docker Docker-compose In our case, I used this docker-compose.yml file to instantiate a mysql database with a persistent volume: version: &#39;3&#39; services: # MySQL mflow_models: container_name: mlflow_models image: mysql:8.0 command: mysqld --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: mlflow_models MYSQL_USER: jsl MYSQL_PASSWORD: passpass MYSQL_ALLOW_EMPTY_PASSWORD: &quot;yes&quot; ports: - &#39;3306:3306&#39; volumes: - &#39;./docker/db/data:/var/lib/mysql&#39; - &#39;./docker/db/my.cnf:/etc/mysql/conf.d/my.cnf&#39; - &#39;./docker/db/sql:/docker-entrypoint-initdb.d&#39; Just by executing the following command in the folder where your docker-compose.yml file is, you will have your MySQL engine, with a mlflow_models database running and prepared for MLFlow Experiment Tracking: !sudo docker-compose up -d . Make sure it’s running using the following command: `!docker ps | grep -o mlflow_models Connection string You will need a connection string that will tell MLFlow (SQLAlchemy) how to reach that database. Connections strings in SQLALchemy have this format: &lt;dialect&gt;+&lt;driver&gt;://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt; In our case, we declare a CONNECTION_STRING var as: CONNECTION_STRING = f&quot;mysql+pymysql://root:root@localhost:3306/mlflow_models&quot; Imports Let’s now import all the libraries we will need. Generic imports import json import os from sklearn.metrics import classification_report import time import mlflow from mlflow.models.signature import infer_signature from urllib.parse import urlparse import pandas as pd import glob Spark NLP imports import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline import pyspark.sql.functions as F from sparknlp.training import CoNLL from pyspark.sql import SparkSession Setting the connection string in MLFLow Now that we have imported mlflow, let’s set the connection string we had prepared before. mlflow.set_tracking_uri(CONNECTION_STRING) mlflow.get_tracking_uri() # This checks if it was set properly Constant with pip_requirements MLFLow requires either a conda_env (conda environment) definition of the requirements of your models, or a pip_requirements list with all pip libraries. We will use this second way, so let’s prepare the list with Spark NLP and MLFlow: PIP_REQUIREMENTS = [f&quot;sparknlp=={sparknlp.version()}&quot;, f&quot;mlflow=={mlflow.__version__}&quot;] PIP_REQUIREMENTS # This checks if it was set properly Training a NERDLApproach() We will be showcasing the serialization and experiment tracking of NERDLApproach(). There is one specific util that is able to parse the log of that approach in order to extract the metrics and charts. Let’s get it. Ner Log Parser Util !wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/utils/ner_image_log_parser.py Now, let’s import the library: import ner_image_log_parser Starting a SparkNLP session It’s important we create a Spark NLP Session using the Session Builder, since we need to specify the jars not only of Spark NLP, but also of MLFlow. def start(): builder = SparkSession.builder .appName(&quot;Spark NLP Licensed&quot;) .master(&quot;local[80]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;256G&quot;) .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.driver.maxResultSize&quot;,&quot;4000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.2,org.mlflow:mlflow-spark:1.21.0&quot;) return builder.getOrCreate() spark = start() Training dataset preparation Let’s download some training and test datasets: !wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.train !wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.testa TRAIN_DATASET = &quot;eng.train&quot; TEST_DATASET = &quot;eng.testa&quot; Let’s read the training dataset: training_data = CoNLL().readDataset(spark, TRAIN_DATASET) training_data.show(3) Let’s get the size: %%time TRAINING_SIZE = training_data.count() TRAINING_SIZE Hyperparameters configuration Let’s configure our hyperparameter values. MODEL_NAME = &#39;&#39; # Add your model name here. Example: clinical_ner EXPERIMENT_NAME = &#39;&#39; # Add your experiment name here. Example: testing_dropout OUTPUT_DIR = f&quot;{MODEL_NAME}_{EXPERIMENT_NAME}_output&quot; # Output folder of all your model artifacts MODEL_DIR = f&quot;model&quot; # Name of the folder where the MLFlow model will be stored MAX_EPOCHS = 10 # Adapt me to your experiment LEARNING_RATE = 0.003 # Adapt me to your experiment BATCH_SIZE = 2048 # Adapt me to your experiment RANDOM_SEED = 0 # Adapt me to your experiment VALIDATION_SPLIT = 0.1 # Adapt me to your experiment Creating the experiment Now, we are ready to instantiate an experiment in MLFlow EXPERIMENT_ID = mlflow.create_experiment(f&quot;{MODEL_NAME}_{EXPERIMENT_NAME}&quot;) Each time you want to test a different thing, change the EXPERIMENT_NAME and rerun the line above to create a new entry in the experiment. By changing the experiment name, a new experiment ID will be generated. Each experiment ID groups all runs in separates folder inside ./mlruns. Pipeline creation document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&#39;document&#39;]) .setOutputCol(&#39;sentence&#39;) token = Tokenizer() .setInputCols([&#39;sentence&#39;]) .setOutputCol(&#39;token&#39;) embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) ner_approach = NerDLApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(MAX_EPOCHS) .setLr(LEARNING_RATE) .setBatchSize(BATCH_SIZE) .setRandomSeed(RANDOM_SEED) .setVerbose(1) .setEnableOutputLogs(True) .setIncludeConfidence(True) .setIncludeAllConfidenceScores(True) .setEvaluationLogExtended(True) .setOutputLogsPath(OUTPUT_DIR) .setValidationSplit(VALIDATION_SPLIT) Let’s create a preprocessing pipeline without the NerDLApproach(): ner_preprocessing_pipeline = Pipeline(stages=[ document, sentence, token, embeddings ]) And a training pipeline with it: ner_training_pipeline = Pipeline(stages = ner_preprocessing_pipeline.getStages() + [ner_approach]) Preparing inference objects Now, let’s prepare the inference as well, since we will train and infer afterwards, and store all the results of training and inference as artifacts in our MLFlow object. Test dataset preparation test_data = CoNLL().readDataset(spark, TEST_DATASET) Setting the names of the inference objects INFERENCE_NAME = &quot;inference.parquet&quot; # This is the name of the results inference on the test dataset, serialized in parquet, CLASSIFICATION_REPORT_LOG_NAME = &quot;classification_report.txt&quot; # Name of the classification report from scikit-learn on Ner Entities PREC_REC_F1_NAME = &quot;precrecf1.jpg&quot; # Name of the precision-recall-f1 file MACRO_MICRO_AVG_NAME = &quot;macromicroavg.jpg&quot; # Name of the macro-micro-average file LOSS_NAME = &quot;loss.jpg&quot; # Name of the loss plot file Now, let’s run the experiment The experiment has already been created before (see “Creating the experiment” section). So we take the ID and start a run. Each time you run execute this cell, you will get a different run for the same experiment. If you want to change the experiment id (and name), go back to “Hyperparameters configuration”. As mentioned before, by changing the experiment name, a new experiment ID will be generated. Each experiment ID groups all runs in separates folder inside ./mlruns. with mlflow.start_run(experiment_id=EXPERIMENT_ID) as run: # Printing RUN and EXPERIMENT ID # ============================== print(f&quot;Model name: {MODEL_NAME}&quot;) RUN_ID = run.info.run_id print(f&quot;Run id: {RUN_ID}&quot;) EXPERIMENT_ID = run.info.experiment_id print(f&quot;Experiment id: {EXPERIMENT_ID}&quot;) # Training the model # ================== print(&quot;Starting training...&quot;) start = time.time() ner_model = ner_training_pipeline.fit(training_data) end = time.time() ELAPSED_SEC_TRAINING = end - start print(&quot;- Finished!&quot;) # Saving the model in TensorFlow (ready to be loaded using NerDLModel.load) # ============================== print(&quot;Saving the model...&quot;) ner_model.stages[-1].write().overwrite().save(f&quot;{OUTPUT_DIR}/{MODEL_DIR}/{MODEL_NAME}&quot;) print(&quot;- Finished!&quot;) # Loading the model (to check everything worked) # ============================== print(&quot;Loading back the model...&quot;) loaded_ner_model = NerDLModel.load(f&quot;{OUTPUT_DIR}/{MODEL_DIR}/{MODEL_NAME}&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) # Creating the inference pipeline with the loaded model # ============================== ner_prediction_pipeline = Pipeline(stages = ner_preprocessing_pipeline.getStages() + [loaded_ner_model]) # Triggering inference # ============================== print(&quot;Starting inference...&quot;) prediction_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) prediction_model = ner_prediction_pipeline.fit(prediction_data) start = time.time() prediction_model.transform(test_data).write.mode(&#39;overwrite&#39;).parquet(f&quot;{OUTPUT_DIR}/{INFERENCE_NAME}&quot;) end = time.time() ELAPSED_SEC_INFERENCE = end - start print(&quot;- Finished!&quot;) # Calculating NER metrics from logs using scikit-learn &#39;classification_report&#39; # ============================== print(&quot;Starting metric calculation...&quot;) predictions = spark.read.parquet(f&quot;{OUTPUT_DIR}/{INFERENCE_NAME}&quot;) preds_df = predictions.select(F.explode(F.arrays_zip(&#39;token.result&#39;,&#39;label.result&#39;,&#39;ner.result&#39;)).alias(&quot;cols&quot;)) .select(F.expr(&quot;cols[&#39;0&#39;]&quot;).alias(&quot;token&quot;), F.expr(&quot;cols[&#39;1&#39;]&quot;).alias(&quot;ground_truth&quot;), F.expr(&quot;cols[&#39;2&#39;]&quot;).alias(&quot;prediction&quot;)).toPandas() preds_df = preds_df.fillna(value=&#39;O&#39;) with open(f&#39;{OUTPUT_DIR}/{CLASSIFICATION_REPORT_LOG_NAME}&#39;, &#39;w&#39;) as f: metrics = classification_report(preds_df[&#39;ground_truth&#39;], preds_df[&#39;prediction&#39;]) f.write(metrics) metrics_dict = classification_report(preds_df[&#39;ground_truth&#39;], preds_df[&#39;prediction&#39;], output_dict=True) print(&quot;- Finished!&quot;) # Printing metrics # ============================== print(f&quot;Training dataset size: {TRAINING_SIZE}&quot;) print(f&quot;Training time (sec): {ELAPSED_SEC_TRAINING}&quot;) print(f&quot;Inference dataset size: {TEST_SIZE}&quot;) print(f&quot;Inference time (sec): {ELAPSED_SEC_INFERENCE}&quot;) print(f&quot;Metrics: n&quot;) print(metrics) # Logging all our params, metrics, charts and artifacts using MLFlow # - log_param: logs a configuration param # - log_artifacts: logs a folder and all its files # - log_artifact: adds a file # - log_metric: logs a metric, what allows you use the MLFlow UI to visually compare results # ============================== print(&quot;Logging params, artifacts, metrics and charts in MLFlow&quot;) mlflow.log_param(&quot;training_size&quot;, TRAINING_SIZE) mlflow.log_param(&quot;training_time&quot;, ELAPSED_SEC_TRAINING) mlflow.log_param(&quot;model_name&quot;, MODEL_NAME) mlflow.log_param(&quot;test_size&quot;, TEST_SIZE) mlflow.log_param(&quot;test_time&quot;, ELAPSED_SEC_INFERENCE) mlflow.log_param(&quot;run_id&quot;, RUN_ID) mlflow.log_param(&quot;max_epochs&quot;, MAX_EPOCHS) mlflow.log_param(&quot;learning_rate&quot;, LEARNING_RATE) mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE) mlflow.log_param(&quot;random_seed&quot;, RANDOM_SEED) mlflow.log_param(&quot;validation_split&quot;, VALIDATION_SPLIT) for file in glob.glob(f&quot;{OUTPUT_DIR}/*.log&quot;): images = {} images.update(ner_image_log_parser.get_charts(file, img_prec_rec_f1_path=f&quot;{OUTPUT_DIR}/{PREC_REC_F1_NAME}&quot;, img_macro_micro_avg_path=f&quot;{OUTPUT_DIR}/{MACRO_MICRO_AVG_NAME}&quot;)) images.update(ner_image_log_parser.loss_plot(file, img_loss_path=f&quot;{OUTPUT_DIR}/{LOSS_NAME}&quot;)) mlflow.log_artifacts(OUTPUT_DIR) mlflow.log_artifact(TRAIN_DATASET) mlflow.log_artifact(TEST_DATASET) for k,v in metrics_dict.items(): if isinstance(v, dict): for kv, vv in v.items(): mlflow.log_metric(f&quot;{k}_{kv}&quot;, vv) else: mlflow.log_metric(k, v) print(&quot;- Finished!&quot;) print(&quot;Logging the model in MLFlow&quot;) # ============================== # Logging the model to be explored in the MLFLow UI tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme # Model registry does not work with file store if tracking_url_type_store != &quot;file&quot;: # Register the model # There are other ways to use the Model Registry, which depends on the use case, # please refer to the doc for more information: # https://mlflow.org/docs/latest/model-registry.html#api-workflow mlflow.spark.log_model(ner_model, f&quot;{MODEL_NAME}_{EXPERIMENT_ID}_{RUN_ID}&quot;, registered_model_name=MODEL_NAME, pip_requirements=PIP_REQUIREMENTS) else: mlflow.spark.log_model(ner_model, f&quot;{MODEL_NAME}_{EXPERIMENT_ID}_{RUN_ID}&quot;, pip_requirements=PIP_REQUIREMENTS) print(&quot;- Finished!&quot;) # Saving the model, in case you want to export it # ============================== print(&quot;Saving the model...&quot;) input_example = predictions.select(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;).limit(1).toPandas() mlflow.spark.save_model(loaded_ner_model, MODEL_NAME, pip_requirements=PIP_REQUIREMENTS, input_example=input_example) print(&quot;- Finished!&quot;) This is an example of the output generated: Model name: NER_base_2048_mlflow Run id: 5f8601fbfc664b3b91c7c61cde31e16d Experiment id: 2 Starting training... - Finished! Saving the model... - Finished! Loading back the model... Starting inference... - Finished! Starting metric calculation... - Finished! Training dataset size: 14041 Training time (sec): 12000.3835768699646 Inference dataset size: 3250 Inference time (sec): 2900.713200330734253 Metrics: precision recall f1-score support B-LOC 0.85 0.82 0.83 1837 B-MISC 0.86 0.83 0.81 922 B-ORG 0.81 0.83 0.82 1341 B-PER 0.86 0.81 0.80 1842 I-LOC 0.80 0.80 0.80 257 I-MISC 0.80 0.80 0.80 346 I-ORG 0.83 0.89 0.80 751 I-PER 0.86 0.83 0.82 1307 O 0.81 0.98 0.84 43792 accuracy 0.87 52395 macro avg 0.88 0.83 0.88 52395 weighted avg 0.84 0.87 0.85 52395 Logging params, artifacts, metrics and charts in MLFlow - Finished! Logging the model in MLFlow Registered model &#39;NER_base_2048_mlflow&#39; already exists. Creating a new version of this model... 2021/11/25 11:51:24 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: NER_base_2048_mlflow, version 2 Created version &#39;2&#39; of model &#39;NER_base_2048_mlflow&#39;. - Finished! Saving the model... - Finished! MLFLow UI to check results Now, we just need to launch the MLFLow UI to see: All the experiments All the runs in each experiment The automatic versioning in the Tracking Server database in MySQL THe MLFlow model, and the TensorFlow version as well The UI for comparing the metrics we set using log_metrics The UI for visualizing the image artifacts we have logged (charts) etc !mlflow ui --backend-store-uri $CONNECTION_STRING Some example screenshots",
    "url": "/docs/en/mlflow",
    "relUrl": "/docs/en/mlflow"
  },
  "47": {
    "id": "47",
    "title": "Available Models and Pipelines",
    "content": "",
    "url": "/models",
    "relUrl": "/models"
  },
  "48": {
    "id": "48",
    "title": "NLP Server",
    "content": "This is a ready to use NLP Server for analyzing text documents using NLU library. Over 4500+ industry grade NLP Models in 300+ Languages are available to use via a simple and intuitive UI, without writing a line of code. For more expert users and more complex tasks, NLP Server also provides a REST API that can be used to process high amounts of data. The models, refered to as spells, are provided by the NLU library and powered by the most widely used NLP library in the industry, Spark NLP. NLP Server is free for everyone to download and use. There is no limitation in the amount of text to analyze. You can setup NLP-Server as a Docker Machine in any enviroment or get it via the AWS Marketplace in just 1 click. Web UI The Web UI is accessible at the following URL: http://localhost:5000/ It allows a very simple and intuitive interaction with the NLP Server. As a first step the user chooses the spell from the first dropdown. All NLU spells are available. Then the user has to provide a text document for analysis. This can be done by either copy/pasting text on the text box, or by uploading a csv/json file. After selecting the grouping option, the user clicks on the Preview button to get the results for the first 10 rows of text. REST API NLP Server includes a REST API which can be used to process any amount of data using NLU. Once you deploy the NLP Server, you can access the API documentation at the following URL http://localhost:5000/docs. Integrate via the Rest API Rest APIs are a popular way to integrate different services into one common platform. NLP Server offers its own API to offer a quick programmatic integration with customers’ services and applications. Bellow is a quick overview of the provided endpoints. More details are provided in the API documentation available http://localhost:5000/docs. Start to analyze Endpoint : /results Method : POST Content-Type (Format) : multipart/form-data Parameters: Spell – the spell that you want to use for this analyze (if you want to run multiple spells you should join them with space character) Data – The data to analyse that can be a single text or an array of strings or files. Grouping – can be choosen from [“document”, “sentence”, “entity”, “word”]. The default value is “” for automatic selection based on spell. Format – The format of the provided input. The default value is “text”. Response: uuid – the unique identifier for the analysis process. Check the status of an analysis process Endpoint : /results/{uuid}/status Method : GET Content-Type (Format) : application/json Response: code – the status code that can be one of “progress”, “success”, “failure”, “broken spell”, “invalid license”, “licensed spell with no license” message – the status message Get the results After ensuring the status of an analysis is “success” you can get the results: Endpoint : /results/{uuid} Method : GET Content-Type (Format) : application/json Parameters: target – if the specified target is “preview” you only get a small part of results. Response: A JSON object that contains the results generated by the spell (each spell has their own specific keys) How to use in Python import requests # Invoke Processing with tokenization spell r = requests.post(f&#39;http://localhost:5000/api/results&#39;,json={&quot;spell&quot;: &quot;tokenize&quot;,&quot;data&quot;: &quot;I love NLU! &lt;3&quot;}) # Use the uuid to get your processed data uuid = r.json()[&#39;uuid&#39;] # Get status of processing r = requests.get(f&#39;http://localhost:5000/api/results/{uuid}/status&#39;).json &gt;&gt;&gt; {&#39;status&#39;: {&#39;code&#39;: &#39;success&#39;, &#39;message&#39;: None}} # Get results r = requests.get(f&#39;http://localhost:5000/api/results/{uuid}&#39;).json() &gt;&gt;&gt; {&#39;sentence&#39;: {&#39;0&#39;: [&#39;I love NLU! &lt;3&#39;]}, &#39;document&#39;: {&#39;0&#39;: &#39;I love NLU! &lt;3&#39;}, &#39;token&#39;: {&#39;0&#39;: [&#39;I&#39;, &#39;love&#39;, &#39;NLU&#39;, &#39;!&#39;, &#39;&lt;3&#39;]}} Import a license key Thanks to the close integration between NLP Server and https://my.JohnSnowLabs.com website, users can easily select and import one of the available licenses to be used on NLP Server. The steps to execute for this are: 1.Click on Login via MYJSL button on the menu bar. 2.In the pop-up window click on the Authorize button. 3.After redirecting back to NLP Server click on the Choose License button. 4.In the modal choose the license that you want to use and then click on the Select button. 5.After the above steps you will see this success alert on the top right of the page. That confirms the import of license completed successfully.",
    "url": "/docs/en/nlp_server/nlp_server",
    "relUrl": "/docs/en/nlp_server/nlp_server"
  },
  "49": {
    "id": "49",
    "title": "Spark NLP - Pipelines",
    "content": "Pipelines and Models Pipelines Quick example: import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;, lang = &quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.5.0 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_dl,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 10 more fields] ++--+--+--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| checked| lemma| stem| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[token, 0, 5, Go...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...| | 2|The Paris metro w...|[[document, 0, 11...|[[token, 0, 2, Th...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 4, 8, Pa...| ++--+--+--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Google, TensorFlow] | |[Donald John Trump, United States]| +-+ */ Showing Available Pipelines There are functions in Spark NLP that will list all the available Pipelines of a particular language for you: import com.johnsnowlabs.nlp.pretrained.ResourceDownloader ResourceDownloader.showPublicPipelines(lang = &quot;en&quot;) /* +--+++ | Pipeline | lang | version | +--+++ | dependency_parse | en | 2.0.2 | | analyze_sentiment_ml | en | 2.0.2 | | check_spelling | en | 2.1.0 | | match_datetime | en | 2.1.0 | ... | explain_document_ml | en | 3.1.3 | +--+++ */ Or if we want to check for a particular version: import com.johnsnowlabs.nlp.pretrained.ResourceDownloader ResourceDownloader.showPublicPipelines(lang = &quot;en&quot;, version = &quot;3.1.0&quot;) /* ++++ | Pipeline | lang | version | ++++ | dependency_parse | en | 2.0.2 | ... | clean_slang | en | 3.0.0 | | clean_pattern | en | 3.0.0 | | check_spelling | en | 3.0.0 | | dependency_parse | en | 3.0.0 | ++++ */ Please check out our Models Hub for the full list of pre-trained pipelines with examples, demos, benchmarks, and more Models **Some selected languages: ** Afrikaans, Arabic, Armenian, Basque, Bengali, Breton, Bulgarian, Catalan, Czech, Dutch, English, Esperanto, Finnish, French, Galician, German, Greek, Hausa, Hebrew, Hindi, Hungarian, Indonesian, Irish, Italian, Japanese, Latin, Latvian, Marathi, Norwegian, Persian, Polish, Portuguese, Romanian, Russian, Slovak, Slovenian, Somali, Southern Sotho, Spanish, Swahili, Swedish, Tswana, Turkish, Ukrainian, Zulu Quick online example: # load NER model trained by deep learning approach and GloVe word embeddings ner_dl = NerDLModel.pretrained(&#39;ner_dl&#39;) # load NER model trained by deep learning approach and BERT word embeddings ner_bert = NerDLModel.pretrained(&#39;ner_dl_bert&#39;) // load French POS tagger model trained by Universal Dependencies val french_pos = PerceptronModel.pretrained(&quot;pos_ud_gsd&quot;, lang = &quot;fr&quot;) // load Italian LemmatizerModel val italian_lemma = LemmatizerModel.pretrained(&quot;lemma_dxc&quot;, lang = &quot;it&quot;) Quick offline example: Loading PerceptronModel annotator model inside Spark NLP Pipeline val french_pos = PerceptronModel.load(&quot;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) Showing Available Models There are functions in Spark NLP that will list all the available Models of a particular Annotator and language for you: import com.johnsnowlabs.nlp.pretrained.ResourceDownloader ResourceDownloader.showPublicModels(annotator = &quot;NerDLModel&quot;, lang = &quot;en&quot;) /* ++++ | Model | lang | version | ++++ | onto_100 | en | 2.1.0 | | onto_300 | en | 2.1.0 | | ner_dl_bert | en | 2.2.0 | | onto_100 | en | 2.4.0 | | ner_conll_elmo | en | 3.2.2 | ++++ */ Or if we want to check for a particular version: import com.johnsnowlabs.nlp.pretrained.ResourceDownloader ResourceDownloader.showPublicModels(annotator = &quot;NerDLModel&quot;, lang = &quot;en&quot;, version = &quot;3.1.0&quot;) /* +-+++ | Model | lang | version | +-+++ | onto_100 | en | 2.1.0 | | ner_aspect_based_sentiment | en | 2.6.2 | | ner_weibo_glove_840B_300d | en | 2.6.2 | | nerdl_atis_840b_300d | en | 2.7.1 | | nerdl_snips_100d | en | 2.7.3 | +-+++ */ And to see a list of available annotators, you can use: import com.johnsnowlabs.nlp.pretrained.ResourceDownloader ResourceDownloader.showAvailableAnnotators() /* AlbertEmbeddings AlbertForTokenClassification AssertionDLModel ... XlmRoBertaSentenceEmbeddings XlnetEmbeddings */ Please check out our Models Hub for the full list of pre-trained models with examples, demo, benchmark, and more",
    "url": "/docs/en/pipelines",
    "relUrl": "/docs/en/pipelines"
  },
  "50": {
    "id": "50",
    "title": "Public Health - Biomedical NLP Demos & Notebooks",
    "content": "",
    "url": "/public_health",
    "relUrl": "/public_health"
  },
  "51": {
    "id": "51",
    "title": "Question Answering - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/question_answering",
    "relUrl": "/question_answering"
  },
  "52": {
    "id": "52",
    "title": "Question Answering",
    "content": "Question Answering (QA) is the task of automatically answering questions posed by humans in natural language. It is a fundamental problem in natural language processing (NLP), playing a vital role in applications such as search engines, virtual assistants, customer support systems, and more. Spark NLP provides state-of-the-art (SOTA) models for QA tasks, enabling accurate and context-aware responses to user queries. QA systems extract relevant information from a given context or knowledge base to answer a question. Depending on the model and input, they can either find exact answers within a text or generate a more comprehensive response. Types of Question Answering Open-Book QA: In this approach, the model has access to external documents, passages, or knowledge sources to extract the answer. The system looks for relevant information within the provided text (e.g., “What is the tallest mountain in the world?” answered using a document about mountains). Closed-Book QA: Here, the model must rely solely on the knowledge it has been trained on, without access to external sources. The answer is generated from the model’s internal knowledge (e.g., answering trivia questions without referring to external material). Common use cases include: Fact-based QA: Answering factoid questions such as “What is the capital of France?” Reading Comprehension: Extracting answers from a provided context, often used in assessments or educational tools. Dialogue-based QA: Supporting interactive systems that maintain context across multiple turns of conversation. By leveraging QA models, organizations can build robust systems that improve user engagement, provide instant information retrieval, and offer customer support in a more intuitive manner. Picking a Model When selecting a model for question answering, consider the following important factors. First, assess the nature of your data (e.g., structured knowledge base vs. unstructured text) and the type of QA needed (open-book or closed-book). Open-book QA requires models that can efficiently search and extract from external sources, while closed-book QA demands models with a large internal knowledge base. Evaluate the complexity of the questions—are they simple factoids or require more reasoning and multi-turn interactions? Metrics such as Exact Match (EM) and F1 score are commonly used to measure model performance in QA tasks. Finally, take into account the computational resources available, as some models, like BERT or T5, may require significant processing power. Explore models tailored for question answering at Spark NLP Models, where you’ll find various options for different QA tasks. Recommended Models for Specific QA Tasks Extractive QA: Use models like distilbert-base-cased-distilled-squad and bert-large-uncased-whole-word-masking-finetuned-squad for extracting answers directly from a provided context. Generative QA (Closed-Book): Consider models such as roberta-base-squad2 or t5_base for generating answers based on internal knowledge without external context. By selecting the appropriate question answering model, you can enhance your ability to deliver accurate and relevant answers tailored to your specific NLP tasks. How to use PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # 1. Document Assembler: Prepares the question and context text for further processing documentAssembler = MultiDocumentAssembler() .setInputCols([&quot;question&quot;, &quot;context&quot;]) .setOutputCol([&quot;document_question&quot;, &quot;document_context&quot;]) # 2. Question Answering Model: Uses a pretrained RoBERTa model for QA spanClassifier = RoBertaForQuestionAnswering.pretrained() .setInputCols([&quot;document_question&quot;, &quot;document_context&quot;]) .setOutputCol(&quot;answer&quot;) .setCaseSensitive(False) # 3. Pipeline: Combines the stages (DocumentAssembler and RoBERTa model) into a pipeline pipeline = Pipeline().setStages([ documentAssembler, spanClassifier ]) # 4. Sample Data: Creating a DataFrame with a question and context data = spark.createDataFrame([[&quot;What&#39;s my name?&quot;, &quot;My name is Clara and I live in Berkeley.&quot;]]).toDF(&quot;question&quot;, &quot;context&quot;) # 5. Running the Pipeline: Fitting the pipeline to the data and generating answers result = pipeline.fit(data).transform(data) # 6. Displaying the Result: The output is the answer to the question extracted from the context result.select(&quot;answer.result&quot;).show(truncate=False) +--+ |result | +--+ |[Clara] | +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline // 1. Document Assembler: Prepares the question and context text for further processing val document = new MultiDocumentAssembler() .setInputCols(&quot;question&quot;, &quot;context&quot;) .setOutputCols(&quot;document_question&quot;, &quot;document_context&quot;) // 2. Question Answering Model: Uses a pretrained RoBERTa model for QA val questionAnswering = RoBertaForQuestionAnswering.pretrained() .setInputCols(Array(&quot;document_question&quot;, &quot;document_context&quot;)) .setOutputCol(&quot;answer&quot;) .setCaseSensitive(true) // 3. Pipeline: Combines the stages (DocumentAssembler and RoBERTa model) into a pipeline val pipeline = new Pipeline().setStages(Array( document, questionAnswering )) // 4. Sample Data: Creating a DataFrame with a question and context val data = Seq(&quot;What&#39;s my name?&quot;, &quot;My name is Clara and I live in Berkeley.&quot;).toDF(&quot;question&quot;, &quot;context&quot;) // 5. Running the Pipeline: Fitting the pipeline to the data and generating answers val result = pipeline.fit(data).transform(data) // 6. Displaying the Result: The output is the answer to the question extracted from the context result.select(&quot;answer.result&quot;).show(false) ++ |result | ++ |[Clara] | ++ Try Real-Time Demos! If you want to see the outputs of question answering models in real time, visit our interactive demos: BERT for Extractive Question Answering – Extract answers directly from provided context using the BERT model. RoBERTa for Question Answering – Use RoBERTa for advanced extractive question answering tasks. T5 for Abstractive Question Answering – Generate abstractive answers using Google’s T5 model. Multihop QA with BERT – Perform complex multihop question answering by reasoning over multiple pieces of text. Useful Resources Want to dive deeper into question answering with Spark NLP? Here are some curated resources to help you get started and explore further: Articles and Guides Empowering NLP with Spark NLP and T5 Model: Text Summarization and Question Answering Question Answering in Visual NLP: A Picture is Worth a Thousand Answers Spark NLP: Unlocking the Power of Question Answering Notebooks Question Answering Transformers in Spark NLP Question Answering and Summarization with T5 Question Answering in Spark NLP T5 Workshop with Spark NLP",
    "url": "/docs/en/tasks/question_answering",
    "relUrl": "/docs/en/tasks/question_answering"
  },
  "53": {
    "id": "53",
    "title": "Spark NLP - Quick Start",
    "content": "Requirements &amp; Setup Spark NLP is built on top of Apache Spark 3.x. For using Spark NLP you need: Java 8 and 11 Apache Spark 3.3.x, 3.2.x, 3.1.x, 3.0.x It is recommended to have basic knowledge of the framework and a working environment before using Spark NLP. Please refer to Spark documentation to get started with Spark. Install Spark NLP in Python Scala and Java Databricks EMR Join our Slack channel Join our channel, to ask for help and share your feedback. Developers and users can help each other getting started here. Spark NLP Slack Spark NLP in Action Make sure to check out our demos built by Streamlit to showcase Spark NLP in action: Spark NLP Demo Spark NLP Examples If you prefer learning by example, check this repository: Spark NLP Examples It is full of fresh examples and even a docker container if you want to skip installation. Below, you can follow into a more theoretical and thorough quick start guide. Where to go next If you need more detailed information about how to install Spark NLP you can check the Installation page Detailed information about Spark NLP concepts, annotators and more may be found HERE",
    "url": "/docs/en/quickstart",
    "relUrl": "/docs/en/quickstart"
  },
  "54": {
    "id": "54",
    "title": "Recognize Entities - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/recognize_entitie",
    "relUrl": "/recognize_entitie"
  },
  "55": {
    "id": "55",
    "title": "Release Notes",
    "content": "0.7.1 Fields Details Name NLP Server Version 0.7.1 Type Patch Release Date 2022-06-17 Overview We are excited to release NLP Server v0.7.1! We are committed to continuously improve the experience for our users and make our product reliable and easy to use. This release focuses on solving a few bugs and improving the stability of the NLP Server. Key Information For smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications. NLP Server is available on both AWS and Azure marketplaces. Bug Fixes Issue when running NER ONTO spell. Issue when running dep spell. Since the spell was broken it is temporarily blacklisted. Document normalizer included the HTML, XML tags to the output even after normalization. Issue when running language translation spells &lt;from_lang&gt;.translate_to.&lt;to_lang&gt;. Upon cancelation of custom model uploading job exception was seen in the logs. Some few UI related issues and abnormalities during operation. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes"
  },
  "56": {
    "id": "56",
    "title": "Spark NLP - release notes",
    "content": "For all official releases please visit GitHub release notes",
    "url": "/docs/en/release_notes",
    "relUrl": "/docs/en/release_notes"
  },
  "57": {
    "id": "57",
    "title": "NLP Server release notes 0.4.0",
    "content": "0.4.0 Highlights This version of NLP Server offers support for licensed models and annotators. Users can now upload a Spark NLP for Healthcare license file and get access to a wide range of additional annotators and transformers. A valid license key also gives access to more than 400 state-of-the-art healthcare models. Those can be used via easy to learn NLU spells or via API calls. NLP Server now supports better handling of large amounts of data to quickly analyze via UI by offering support for uploading CSV files. Support for floating licenses. Users can now take advantage of the floating license flexibility and use those inside of the NLP Server. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_4_0",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_4_0"
  },
  "58": {
    "id": "58",
    "title": "NLP Server release notes 0.5.0",
    "content": "0.5.0 Highlights Support for easy license import from my.johnsnowlabs.com. Visualize annotation results with Spark NLP Display. Examples of results obtained using popular spells on sample texts have been added to the UI. Performance improvement when previewing the annotations. Support for 22 new models for 23 languages including various African and Indian languages as well as Medical Spanish models powered by NLU 3.4.1 Various bug fixes Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_5_0",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_5_0"
  },
  "59": {
    "id": "59",
    "title": "NLP Server release notes 0.6.0",
    "content": "0.6.0 Fields Details Name NLP Server Version 0.6.0 Type Minor Release Date 2022-04-06 Overview We are excited to release NLP Server v0.6.0! This new release comes with exciting new features and improvements that extend and enhance the capabilities of the NLP Server. This release comes with the ability to share the models with the Annotation Lab. This will enable easy access to custom models uploaded to or trained with the Annotation Lab or to pre-trained models downloaded to Annotation Lab from the NLP Models Hub. As such the NLP Server becomes an easy and quick tool for testing our trained models locally on your own infrastructure with zero data sharing. Another important feature we have introduced is the support for Spark OCR spells. Now we can upload images, PDFs, or other documents to the NLP Server and run OCR spells on top of it. The results of the processed documents are also available for export. The release also includes a few improvements to the existing features and some bug fixes. Key Information For a smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications NLP Server is now available on Azure Marketplace as well as on AWS marketplace. Major Features and Improvements Support for custom models trained with the Annotation Lab Models trained with the Annotation Lab are now available as “custom” spells in the NLP Server. Similarly, models manually uploaded to the Annotation Lab, or downloaded from the NLP Models Hub are also made available for use in the NLP Server. This is only supported in a docker setup at present when both tools are deployed in the same machine. Support for Spark OCR spells OCR spells are now supported by NLP Server in the presence of a valid OCR license. Users can upload an image, PDF, or other supported document format and run the OCR spells on it. The processed results are also available for download as a text document. It is also possible to upload multiple files at once for OCR operation. These files can be images, PDFs, word documents, or a zipped file. Other Improvements Now users can chain multiple spells together to analyze the input data. The order of operation on the input data will be in the sequence of the spell chain from left to right. NLP Server now supports more than 5000+ models in 250+ languages powered by NLU. Bug Fixes Not found error seen when running predictions using certain spells. The prediction job runs in an infinite loop when using certain spells. For input data having new line characters JSON exception was seen when processing the output from NLU. Incorrect license information was seen in the license popup. Spell field cleared abruptly when typing the spells. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_6_0",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_6_0"
  },
  "60": {
    "id": "60",
    "title": "NLP Server release notes 0.6.1",
    "content": "0.6.1 Fields Details Name NLP Server Version 0.6.1 Type Patch Release Date 2022-05-06 Overview We are excited to release NLP Server v0.6.1! We are continually committed towards improving the experience for our users and making our product reliable and easy to use. This release focuses on improving the stability of the NLP Server and cleaning up some annoying bugs. To enhance the user experience, the product now provides interactive and informative responses to the users. The improvements and bug fixes are mentioned in their respective sections below. Key Information For smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications. NLP Server is available on both AWS and Azure marketplace. Improvements Support for new models for Lemmatizers, Parts of Speech Taggers, and Word2Vec Embeddings for over 66 languages, with 20 languages being covered for the first time by NLP Server, including ancient and exotic languages like Ancient Greek, Old Russian, Old French and much more. Bug Fixes The prediction job runs in an infinite loop when using certain spells. Now after 3 retries it aborts the process and informs users appropriately. Issue when running lang spell for language classification. The prediction job runs in an infinite loop when incorrect data format is selected for a given input data. The API request for processing spell didn’t work when format parameter was not provided. Now it uses a default value in such case. Users were unable to login to their MYJSL account from NLP Server. Proper response when there is issue in internet connectivity when running spell. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_6_1",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_6_1"
  },
  "61": {
    "id": "61",
    "title": "NLP Server release notes 0.7.0",
    "content": "0.7.0 Fields Details Name NLP Server Version 0.7.0 Type Minor Release Date 2022-06-07 Overview We are excited to release NLP Server v0.7.0! This new release comes with an exciting new feature of table extraction from various file formats. Table extraction feature enables extracting tabular content from the document. This extracted content is available as JSON and hence can again be processed with different spells for further predictions. The various supported files formats are documents (pdf, doc, docx), slides (ppt, pptx), and zipped content containing the mentioned formats. The improvements are mentioned in their respective sections below. Key Information For smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications. NLP Server is available on both AWS and Azure marketplace. Major Features and Improvements Support for Table extraction NLP Server now supports extracting tabular content from various file types. The currently supported file types are documents (pdf, doc, docx), slides (ppt, pptx), and zipped content containing any of the mentioned formats. These extracted contents are available as JSON output from both UI and API that can easily be converted to suitable Data Frames (e.g., pandas DF) for further processing. The output of the table extraction process can also be viewed in the NLP Server UI as a flat table. Currently, if multiple tables are extracted from the document, then only one of the tables selected randomly will be shown as a preview in the UI. However, upon downloading all the extracted tables are exported in separate JSON dumps combined in a single zipped file. For this version, the table extraction on PDF files is successful only if the PDF contains necessary metadata about the table content. Other Improvements Support for over 600 new models, and over 75 new languages including ancient, dead, and extinct languages. Transformer-based embeddings and token classifiers are powered by state-of-the-art CamemBertEmbeddings and DeBertaForTokenClassification based architectures. Added Portuguese De-identification models, NER models for Gene detection, and RxNorm Sentence resolution model for mapping and extracting pharmaceutical actions as well as treatments. JSON payload is now supported in the request body when using create result API. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_7_0",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_7_0"
  },
  "62": {
    "id": "62",
    "title": "NLP Server release notes 0.7.1",
    "content": "0.7.1 Fields Details Name NLP Server Version 0.7.1 Type Patch Release Date 2022-06-17 Overview We are excited to release NLP Server v0.7.1! We are committed to continuously improve the experience for our users and make our product reliable and easy to use. This release focuses on solving a few bugs and improving the stability of the NLP Server. Key Information For smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications. NLP Server is available on both AWS and Azure marketplaces. Bug Fixes Issue when running NER ONTO spell. Issue when running dep spell. Since the spell was broken it is temporarily blacklisted. Document normalizer included the HTML, XML tags to the output even after normalization. Issue when running language translation spells &lt;from_lang&gt;.translate_to.&lt;to_lang&gt;. Upon cancelation of custom model uploading job exception was seen in the logs. Some few UI related issues and abnormalities during operation. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_7_1",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_7_1"
  },
  "63": {
    "id": "63",
    "title": "Resolve Entities to Terminology Codes - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/resolve_entities_codes",
    "relUrl": "/resolve_entities_codes"
  },
  "64": {
    "id": "64",
    "title": "Spark NLP - Serving with MLFlow on Databricks",
    "content": "This is the first article of the “Serving Spark NLP via API” series, showcasing how to serve Spark NLP using Databricks Jobs and MLFlow Serve APIs. Background Spark NLP is a Natural Language Understanding Library built on top of Apache Spark, leveranging Spark MLLib pipelines, that allows you to run NLP models at scale, including SOTA Transformers. Therefore, it’s the only production-ready NLP platform that allows you to go from a simple PoC on 1 driver node, to scale to multiple nodes in a cluster, to process big amounts of data, in a matter of minutes. Before starting, if you want to know more about all the advantages of using Spark NLP (as the ability to work at scale on air-gapped environments, for instance) we recommend you to take a look at the following resources: John Snow Labs webpage; The official technical documentation of Spark NLP; Spark NLP channel on Medium; Motivation Spark NLP is server-agnostic, what means it does not come with an integrated API server, but offers a lot of options to serve NLP models using Rest APIs. There is a wide range of possibilities to add a web server and serve Spark NLP pipelines using RestAPI, and in this series of articles we are only describing some of them. Let’s have an overview of how to use Databricks Jobs API and MLFlow Serve as an example for that purpose. Databricks Jobs and MLFlow Serve APIs About Databricks Databricks is an enterprise software company founded by the creators of Apache Spark. The company has also created MLflow, the Serialization and Experiment tracking library you can use (inside or outside databricks), as described in the section “Experiment Tracking”. Databricks develops a web-based platform for working with Spark, that provides automated cluster management and IPython-style notebooks. Their infrastructured is provided for training and production purposes, and is integrated in cloud platforms as Azure and AWS. Spark NLP is a proud partner of Databricks and we offer a seamless integration with them — see Install on Databricks. All Spark NLP capabilities run in Databricks, including MLFlow serialization and Experiment tracking, what can be used for serving Spark NLP for production purposes. About MLFlow MLFlow is a serialization and Experiment Tracking platform, which also natively suports Spark NLP. We have a documentation entry about MLFlow in the “Experiment Tracking” section. It’s highly recommended that you take a look before moving forward in this document, since we will use some of the concepts explained there. We will use MLFlow serialization to serve our Spark NLP models. Strengths Easily configurable and scalable clusters in Databricks Seamless integration of Spark NLP and Databricks for automatically creating Spark NLP clusters (check Install on Databricks URL) Integration with MLFlow, experiment tracking, etc. Configure your training and serving environments separately. Use your serving environment for inference and scale it as you need. Weaknesses This approach does not allow you to customize your endpoints, it uses Databricks JOBS API ones Requires some time and expertise in Databricks to configure everything properly Creating a cluster in Databricks As mentioned before, Spark NLP offers a seamless integration with Databricks. To create a cluster, please follow the instructions in Install on Databricks. That cluster can be then replicated (cloned) for production purposes later on. Configuring Databricks for serving Spark NLP on MLFlow In Databricks Runtime Version, select any Standard runtime, not ML ones… These add their version of MLFlow, and some incompatibilities may arise. For this example, we have used 8.3 (includes Apache Spark 3.1.1, Scala 2.12) The cluster instantiated is prepared to use Spark NLP, but to make it production-ready using MLFlow, we need to add the MLFlow jar, in addition to the Spark NLP jar, as shown in the “Experiment Tracking” section. In that case, we did it adding both jars… (&quot;spark.jars.packages&quot;:&quot; com.johnsnowlabs.nlp:spark-nlp_2.12:[YOUR_SPARKNLP_VERSION],org.mlflow:mlflow-spark:1.21.0&quot;) …into the SparkSession. However, in Databricks, you don’t instantiate programmatically a session, but you configure it in the Compute screen, selecting your Spark NLP cluster, and then going to Configuration -&gt; Advanced Options -&gt; Spark -&gt; Spark Config, as shown in the following image: In addition to Spark Config, we need to add the Spark NLP and MLFlow libraries to the Cluster. You can do that by going to Libraries inside your cluster. Make sure you have spark-nlp and mlflow. If not, you can install them either using PyPI or Maven artifacts. In the image below you can see the PyPI alternative: TIP: You can also use the Libraries section to add the jars (using Maven Coordinates) instead of setting them in the Spark Config, as showed before. Creating a notebook You are ready to create a notebook in Databricks and attach it to the recently created cluster. To do that, go to Create --&gt; Notebook, and select the cluster you want in the dropdown above your notebook. Make sure you have selected the cluster with the right Spark NLP + MLFlow configuration. To check everything is ok, run the following lines: To check the session is running: spark To check jars are in the session: spark.sparkContext.getConf().get(&#39;spark.jars.packages&#39;) You should see the following output from the last line (versions may differ depending on which ones you used to configure your cluster) Out[2]: &#39;com.johnsnowlabs.nlp:spark-nlp_2.12:[YOUR_SPARKNLP_VERSION],org.mlflow:mlflow-spark:1.21.0&#39; Logging the experiment in Databricks using MLFlow As explained in the “Experiment Tracking” section, MLFlow can log Spark MLLib / NLP Pipelines as experiments, to carry out runs on them, track versions, etc. MLFlow is natively integrated in Databricks, so we can leverage the mlflow.spark.log_model() function of the Spark flavour of MLFlow, to start tracking our Spark NLP pipelines. Let’s first import our libraries: import mlflow import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import pandas as pd from sparknlp.training import CoNLL import pyspark from pyspark.sql import SparkSession Then, create a Lemmatization pipeline: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = LemmatizerModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;prediction&quot;) # It&#39;s mandatory to call it prediction pipeline = Pipeline(stages=[ documentAssembler, tokenizer, lemmatizer ]) p_model = pipeline.fit( spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) ) IMPORTANT: Last output column of the last component in the pipeline should be called prediction. Finally, let’s log the experiment. In the Experiment Tracking section, we used the pip_requirements parameter in the log_model() function to set the required libraries: But we mentioned using conda is also available. Let’s use conda in this example: conda_env = { &#39;channels&#39;: [&#39;conda-forge&#39;], &#39;dependencies&#39;: [ &#39;python=3.8.8&#39;, { &quot;pip&quot;: [ &#39;pyspark==3.1.1&#39;, &#39;mlflow==1.21.0&#39;, &#39;spark-nlp==[YOUR_SPARKNLP_VERSION]&#39; ] } ], &#39;name&#39;: &#39;mlflow-env&#39; } With this conda environment, we are ready to log our pipeline: mlflow.spark.log_model(p_model, &quot;lemmatizer&quot;, conda_env=conda_env) You should see an output similar to this one: (6) Spark Jobs (1) MLflow run *Logged 1 run to an experiment in MLflow. Learn more* Experiment UI On the top right corner of your notebook, you will see the Experiment widget, and inside, as shown in the image below. You can also access Experiments UI if you switch your environment from “Data Science &amp; Engineering” to “Machine Learning”, on the left panel… Once in the experiment UI, you will see the following screen, where your experiments are tracked. If you click on the Start Time cell of your experiment, you will reach the registered MLFlow run. On the left panel you will see the MLFlow model and some other artifacts, as the conda.yml and pip_requirements.txt that manage the dependencies of your models. On the right panel, you will see two snippets, about how to call to the model for inference internally from Databricks. Snippet for calling with a Pandas Dataframe: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; Load model as a Spark UDF. loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model) Predict on a Spark DataFrame. columns = list(df.columns) df.withColumn(&#39;predictions&#39;, loaded_model(*columns)).collect() Snippet for calling with a Spark Dataframe. We won’t include it in this documentation because that snippet does not include SPark NLP specificities. To make it work, the correct snippet should be: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model) ### Predict on a Spark DataFrame. res_spark = loaded_model.predict(df_1_spark.rdd) IMPORTANT: You will only get the last column (prediction) results, which is a list of Rows of Annotation Types. To convert the result list into a Spark Dataframe, use the following schema: import pyspark.sql.types as T import pyspark.sql.functions as f annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) And then, get the results (for example, in res_spark) and apply the schema: spark_res = spark.createDataFrame(res_pandas[0], schema=annotationType) Calling the experiment for production purposes using MLFlow Rest API Instead of choosing a Batch Inference, you can select REST API. This will lead you to another screen, when the model will be loaded for production purposes in an independent cluster. Once deployed, you will be able to: Check the endpoint URL to consume the model externally; Test the endpoint writing a json (in our example, ‘text’ is our first input col of the pipeline, so it shoud look similar to: {&quot;text&quot;: &quot;This is a test of how the lemmatizer works&quot;} You can see the response in the same screen. Check what is the Python code or cURL command to do that very same thing programatically. By just using that Python code, you can already consume it for production purposes from any external web app. IMPORTANT: As per 17/02/2022, there is an issue being studied by Databricks team, regarding the creation on the fly of job clusters to serve MLFlow models that require configuring the Spark Session with specific jars. This will be fixed in later versions of Databricks. In the meantime, the way to go is using Databricks Jobs API. Calling the experiment for production purposes using Databricks Asynchronous Jobs API Creating the notebook for the inference job And last, but not least, another approach to consume models for production purposes. the Jobs API. Databricks has its own API for managing jobs, that allows you to instantiate any notebook or script as a job, run it, stop it, and manage all the life cycle. And you can configure the cluster where this job will run before hand, what prevents having the issue described in point 3. To do that: Create a new production cluster, as described before, cloning you training environment but adapting it to your needs for production purposes. Make sure the Spark Config is right, as described at the beginning of this documentation. Create a new notebook. Always check that the jars are in the session: spark.sparkContext.getConf().get(&#39;spark.jars.packages&#39;) Out[2]: &#39;com.johnsnowlabs.nlp:spark-nlp_2.12:[YOUR_SPARKNLP_VERSION],org.mlflow:mlflow-spark:1.21.0&#39; Add the Spark NLP imports. import mlflow import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import pandas as pd from sparknlp.training import CoNLL import pyspark from pyspark.sql import SparkSession import pyspark.sql.types as T import pyspark.sql.functions as f import json Let’s define that an input param called text will be sent in the request. Let’s get the text from that parameter using dbutils. input = &quot;&quot; try: input = dbutils.widgets.get(&quot;text&quot;) print(&#39;&quot;text&quot; input found: &#39; + input) except: print(&#39;Unable to run: dbutils.widgets.get(&quot;text&quot;). Setting it to NOT_SET&#39;) input = &quot;NOT_SET&quot; Right now, the input text will be in input var. You can trigger an exception or set the input to some default value if the parameter does not come in the request. Let’s create a Spark Dataframe with the input df = spark.createDataFrame([[input]]).toDF(&#39;text&#39;) And now, we just need to use the snippet for Spark Dataframe to consume MLFlow models, described above: import mlflow import pyspark.sql.types as T import pyspark.sql.functions as f logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model) Predict on a Spark DataFrame. res_spark = loaded_model.predict(df_1_spark.rdd) annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) spark_res = spark.createDataFrame(res_spark[0], schema=annotationType) Let’s transform our lemmatized tokens from the Dataframe into a list of strings: lemmas = spark_res.select(&quot;result&quot;).collect() txt_results = [x[&#39;result&#39;] for x in lemmas] And finally, let’s use again dbutils to tell Databricks to spin off the run and return an exit parameter: the list of token strings. dbutils.notebook.exit(json.dumps({ &quot;status&quot;: &quot;OK&quot;, &quot;results&quot;: txt_results })) Configuring the job Last, but not least. We need to precreate the job, so that we run it from the API. We could do that using the API as well, but we will show you how to do it using the UI. On the left panel, go to Jobs and then Create Job. In the jobs screen, you will see you job created. It’s not running, it’s prepared to be called on demand, programatically or in the interface, with a text input param. Let’s see how to do that: Running the job In the jobs screen, if you click on the job, you will enter the Job screen, and be able to set your text input parameter and run the job manually. You can use this for testing purposes, but the interesting part is calling it externally, using the Databricks Jobs API. Using the Databricks Jobs API, from for example, Postman. POST HTTP request URL: https://[your_databricks_instance]/api/2.1/jobs/run-now Authorization: [use Bearer Token. You can get it from Databricks, Settings, User Settings, Generate New Token.] Body: { &quot;job_id&quot;: [job_id, check it in the Jobs screen], &quot;notebook_params&quot;: {&quot;text&quot;: &quot;This is an example of how well the lemmatizer works&quot;} } As it’s an asynchronous call, it will return the number a number of run, but no results. You will need to query for results using the number of the run and the following url https://[your_databricks_instance]/2.1/jobs/runs/get-output You will get a big json, but the most relevant info, the output, will be up to the end: Results (list of lemmatized words) {&quot;notebook_output&quot;: { &quot;status&quot;: &quot;OK&quot;, &quot;results&quot;: [&quot;This&quot;, &quot;is&quot;, &quot;a&quot;, &quot;example&quot;, &quot;of&quot;, &quot;how&quot;, &quot;lemmatizer&quot;, &quot;work&quot;] }} The notebook will be prepared in the job, but idle, until you call it programatically, what will instantiate a run. Check the Jobs API for more information about what you can do with it and how to adapt it to your solutions for production purposes. Do you want to know more? Visit John Snow Labs and Spark NLP Technical Documentation websites Follow us on Medium: Spark NLP and Veysel Kocaman Write to support@johnsnowlabs.com for any additional request you may have",
    "url": "/docs/en/serving_spark_nlp_via_api_databricks_mlflow",
    "relUrl": "/docs/en/serving_spark_nlp_via_api_databricks_mlflow"
  },
  "65": {
    "id": "65",
    "title": "Social Determinant - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/social_determinant",
    "relUrl": "/social_determinant"
  },
  "66": {
    "id": "66",
    "title": "Spark NLP",
    "content": "Requirements &amp; Setup Spark NLP is built on top of Apache Spark 3.x. For using Spark NLP you need: Java 8 and 11 Apache Spark 3.3.x, 3.2.x, 3.1.x, 3.0.x GPU (optional): Spark NLP {{ site.sparknlp_version }} is built with TensorFlow 2.7.1 and the following NVIDIA® software are only required for GPU support: NVIDIA® GPU drivers version 450.80.02 or higher CUDA® Toolkit 11.2 cuDNN SDK 8.1.0 It is recommended to have basic knowledge of the framework and a working environment before using Spark NLP. Please refer to Spark documentation to get started with Spark. Install Spark NLP in Python Scala and Java Databricks EMR Join our Slack channel Join our channel, to ask for help and share your feedback. Developers and users can help each other getting started here. Spark NLP Slack Spark NLP in Action Make sure to check out our demos built by Streamlit to showcase Spark NLP in action: Spark NLP Demo Spark NLP Examples If you prefer learning by example, check this repository: Spark NLP Examples It is full of fresh examples and even a docker container if you want to skip installation. Below, you can follow into a more theoretical and thorough quick start guide. Where to go next If you need more detailed information about how to install Spark NLP you can check the Installation page Detailed information about Spark NLP concepts, annotators and more may be found HERE",
    "url": "/docs/en/spark-nlp",
    "relUrl": "/docs/en/spark-nlp"
  },
  "67": {
    "id": "67",
    "title": "Speech and Vision Recognition - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/speech_vision_recognition",
    "relUrl": "/speech_vision_recognition"
  },
  "68": {
    "id": "68",
    "title": "Summarization",
    "content": "Summarization is the task of generating concise and informative summaries from longer documents. This is useful for a wide range of applications, such as summarizing news articles, legal documents, or any large texts where key points need to be extracted. Spark NLP offers advanced summarization models that can create high-quality summaries efficiently. Summarization models take input text and generate shorter versions while preserving essential information. Common use cases include: News Summaries: Automatically condensing long news articles into brief, digestible summaries. Legal Documents: Summarizing lengthy contracts, case studies, or legal opinions. Research Papers: Extracting key insights and conclusions from scientific papers. By leveraging summarization models, organizations can efficiently process large amounts of textual data and extract critical information, making it easier to consume and understand complex documents. Picking a Model When choosing a summarization model, consider factors like the length of the input text and the desired summary style (e.g., extractive or abstractive). Some models are better suited for shorter inputs, while others excel in handling long documents. Evaluate whether your task requires sentence-level summaries or paragraph-level condensation. Consider the domain of the text, such as legal, scientific, or general news, as domain-specific models often perform better. Explore the available summarization models at Spark NLP Models to find the one that best suits your summarization needs. Recommended Models for Summarization Tasks General Summarization: For most summarization tasks, consider models like bart-large-cnn and t5-base are well suited for generating concise summaries. By selecting the right model, you can efficiently condense long documents into meaningful summaries, saving time and effort. How to use {% include programmingLanguageSelectScalaPython.html %} import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Step 1: Assemble raw text data into a format that Spark NLP can process documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;documents&quot;) # Step 2: Load a pretrained BART model for summarization bart = BartTransformer.pretrained(&quot;distilbart_xsum_12_6&quot;) .setTask(&quot;summarize:&quot;) .setInputCols([&quot;documents&quot;]) .setMaxOutputLength(200) .setOutputCol(&quot;summaries&quot;) # Step 3: Create a pipeline with the document assembler and BART model pipeline = Pipeline().setStages([documentAssembler, bart]) # Step 4: Sample data - a long text passage for summarization data = spark.createDataFrame([[ &quot;Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a &quot; + &quot;downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness&quot; + &quot; of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this &quot; + &quot;paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework &quot; + &quot;that converts all text-based language problems into a text-to-text format. Our systematic study compares &quot; + &quot;pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens &quot; + &quot;of language understanding tasks. By combining the insights from our exploration with scale and our new &quot; + &quot;Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering &quot; + &quot;summarization, question answering, text classification, and more. To facilitate future work on transfer &quot; + &quot;learning for NLP, we release our data set, pre-trained models, and code.&quot; ]]).toDF(&quot;text&quot;) # Step 5: Apply the pipeline to generate the summary result = pipeline.fit(data).transform(data) # Step 6: Display the summary result.select(&quot;summaries.result&quot;).show(truncate=False) # +--+ # |result | # +--+ # |[transfer learning has emerged as a powerful technique in natural language processing (NLP) the effectiveness of transfer learning has given rise to a diversity of approaches, | # |methodologies, and practice .] | # +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.seq2seq.GPT2Transformer import org.apache.spark.ml.Pipeline // Step 1: Document Assembler to prepare the text data val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;documents&quot;) // Step 2: Load BART model for text generation with customization val bart = BartTransformer.pretrained(&quot;distilbart_xsum_12_6&quot;) .setInputCols(Array(&quot;documents&quot;)) .setMinOutputLength(10) .setMaxOutputLength(30) .setDoSample(true) .setTopK(50) .setOutputCol(&quot;generation&quot;) // Step 3: Define the pipeline stages val pipeline = new Pipeline().setStages(Array(documentAssembler, bart)) // Step 4: Input text data to be summarized val data = Seq( &quot;PG&amp;E stated it scheduled the blackouts in response to forecasts for high winds &quot; + &quot;amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were &quot; + &quot;scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.&quot; ).toDF(&quot;text&quot;) // Step 5: Fit the model and apply the pipeline val result = pipeline.fit(data).transform(data) // Step 6: Show the generated summary results.select(&quot;generation.result&quot;).show(truncate = false) // +--+ // |result | // +--+ // |[Nearly 800 thousand customers were affected by the shutoffs.]| // +--+ Try Real-Time Demos! If you want to see the outputs of text classification models in real time, visit our interactive demos: Sparknlp Text Summarization – A live demo where you can try your inputs on text classification models on the go. Text summarization – An interactive demo for sentiment and emotion detection. Useful Resources Here are some resources to get you started with summarization in Spark NLP: Articles and Guides Empowering NLP with Spark NLP and T5 Model: Text Summarization and Question Answering Notebooks Document Summarization with BART 1, 2 T5 Workshop with Spark NLP",
    "url": "/docs/en/tasks/summarization",
    "relUrl": "/docs/en/tasks/summarization"
  },
  "69": {
    "id": "69",
    "title": "Summarize & Paraphrase - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/summarize_paraphrase",
    "relUrl": "/summarize_paraphrase"
  },
  "70": {
    "id": "70",
    "title": "Table Question Answering",
    "content": "Table question answering is the task of answering questions from structured tabular data. This is particularly useful for applications like financial reports, databases, and other contexts where information is stored in tables. Spark NLP provides state-of-the-art solutions for table question answering, enabling accurate extraction and generation of answers from tables in various formats. Table question answering models process tabular data and the question to output the most relevant answer. Common use cases include: Financial Reports: Automatically extracting insights from financial data tables. Databases: Querying relational databases or spreadsheet data to extract specific information. Business Intelligence: Enabling non-technical users to interact with and extract data from complex tables using natural language. By leveraging table question answering, organizations can build systems capable of understanding tabular structures, making it easier to answer complex queries and automate data extraction. Picking a Model When selecting a model for table question answering, consider factors such as the complexity of the table and the nature of the query. Some models work better with numerical data, while others may handle textual data or multi-row operations more effectively. Evaluate the format of the tables you are working with (e.g., CSV, Excel, or SQL tables), and ensure that the model can process the tabular structure accurately. Also, consider the domain of your tables, such as finance, healthcare, or retail, as some models may be pre-trained on specific domains. Explore models tailored for table question answering at Spark NLP Models, where you’ll find various options for different table QA tasks. Recommended Models for Specific Table Question Answering Tasks General Table QA: Consider models such as tapas-large-finetuned-wtq for answering questions across different types of tables. SQL Query Generation: Use models like t5-small-wikiSQL to automatically generate SQL queries from natural language inputs. By selecting the right model for table question answering, you can extract valuable insights from structured data and answer complex queries efficiently. How to use {% include programmingLanguageSelectScalaPython.html %} import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Document Assembler: Assembles table JSON and questions into documents document_assembler = MultiDocumentAssembler() .setInputCols(&quot;table_json&quot;, &quot;questions&quot;) .setOutputCols(&quot;document_table&quot;, &quot;document_questions&quot;) # Sentence Detector: Splits the questions into individual sentences sentence_detector = SentenceDetector() .setInputCols([&quot;document_questions&quot;]) .setOutputCol(&quot;questions&quot;) # Table Assembler: Converts the table document to the proper format table_assembler = TableAssembler() .setInputCols([&quot;document_table&quot;]) .setOutputCol(&quot;table&quot;) # Tapas Model: Loads pretrained Tapas for table question answering tapas = TapasForQuestionAnswering .pretrained() .setInputCols([&quot;questions&quot;, &quot;table&quot;]) .setOutputCol(&quot;answers&quot;) # Pipeline: Combines all stages pipeline = Pipeline(stages=[ document_assembler, sentence_detector, table_assembler, tapas ]) # Sample JSON data for the table json_data = &quot;&quot;&quot; { &quot;header&quot;: [&quot;name&quot;, &quot;money&quot;, &quot;age&quot;], &quot;rows&quot;: [ [&quot;Donald Trump&quot;, &quot;$100,000,000&quot;, &quot;75&quot;], [&quot;Elon Musk&quot;, &quot;$20,000,000,000,000&quot;, &quot;55&quot;] ] } &quot;&quot;&quot; # Fit and transform the data with the pipeline model = pipeline.fit(data) model .transform(data) .selectExpr(&quot;explode(answers) AS answer&quot;) .select(&quot;answer.metadata.question&quot;, &quot;answer.result&quot;) .show(truncate=False) # Expected Output: # +--+-+ # |question |result | # +--+-+ # |Who earns 100,000,000? |Donald Trump | # |Who has more money? |Elon Musk | # |How much they all earn?|COUNT($100,000,000, $20,000,000,000,000)| # |How old are they? |AVERAGE(75, 55) | # +--+-+ import spark.implicits._ import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline // Questions: Sample questions about the table data val questions = &quot;&quot;&quot; |Who earns 100,000,000? |Who has more money? |How old are they? |&quot;&quot;&quot;.stripMargin.trim // Table Data: JSON format for table with name, money, and age columns val jsonData = &quot;&quot;&quot; |{ | &quot;header&quot;: [&quot;name&quot;, &quot;money&quot;, &quot;age&quot;], | &quot;rows&quot;: [ | [&quot;Donald Trump&quot;, &quot;$100,000,000&quot;, &quot;75&quot;], | [&quot;Elon Musk&quot;, &quot;$20,000,000,000,000&quot;, &quot;55&quot;] | ] |} |&quot;&quot;&quot;.stripMargin.trim // DataFrame: Create DataFrame with table data and questions val data = Seq((jsonData, questions)) .toDF(&quot;json_table&quot;, &quot;questions&quot;) .repartition(1) // Document Assembler: Assemble the table JSON and questions into documents val docAssembler = new MultiDocumentAssembler() .setInputCols(&quot;json_table&quot;, &quot;questions&quot;) .setOutputCols(&quot;document_table&quot;, &quot;document_questions&quot;) // Sentence Detector: Detects individual questions from the text val sentenceDetector = SentenceDetectorDLModel .pretrained() .setInputCols(Array(&quot;document_questions&quot;)) .setOutputCol(&quot;question&quot;) // Table Assembler: Converts JSON table data into table format val tableAssembler = new TableAssembler() .setInputFormat(&quot;json&quot;) .setInputCols(Array(&quot;document_table&quot;)) .setOutputCol(&quot;table&quot;) // Tapas Model: Pretrained model for table question answering val tapas = TapasForQuestionAnswering .pretrained() .setInputCols(Array(&quot;question&quot;, &quot;table&quot;)) .setOutputCol(&quot;answer&quot;) // Pipeline: Combine all components into a pipeline val pipeline = new Pipeline() .setStages( Array( docAssembler, sentenceDetector, tableAssembler, tapas)) // Model: Fit the pipeline to the data val pipelineModel = pipeline.fit(data) val result = pipeline.fit(data).transform(data) // Show Results: Explode answers and show the results for each question result .selectExpr(&quot;explode(answer) as answer&quot;) .selectExpr( &quot;answer.metadata.question&quot;, &quot;answer.result&quot;) // Expected Output: // +--+-+ // |question |result | // +--+-+ // |Who earns 100,000,000? |Donald Trump | // |Who has more money? |Elon Musk | // |How much they all earn?|COUNT($100,000,000, $20,000,000,000,000)| // |How old are they? |AVERAGE(75, 55) | // +--+-+ Try Real-Time Demos! If you want to see the outputs of table question answering models in real time, visit our interactive demos: Tapas for Table Question Answering – TAPAS answers questions from tabular data. Tapex for Table QA – TAPEX handles complex table queries and computations. SQL Query Generation – Converts natural language questions into SQL queries from tables. Useful Resources Want to dive deeper into table question answering with Spark NLP? Here are some curated resources to help you get started and explore further: Articles and Guides Empowering NLP with Spark NLP and TAPAS Model: Table Question Answering Table-based Question Answering with Spark NLP Notebooks TAPAS Model for Table Question Answering SQL Code Generation from Tables TableQA with Spark NLP",
    "url": "/docs/en/tasks/table_question_answering",
    "relUrl": "/docs/en/tasks/table_question_answering"
  },
  "71": {
    "id": "71",
    "title": "Text Classification",
    "content": "Text classification is the process of assigning a category or label to a piece of text, such as an email, tweet, or review. It plays a crucial role in natural language processing (NLP), where it is used to automatically organize text into predefined categories. Spark NLP provides various solutions to address text classification challenges effectively. In this context, text classification involves analyzing a document’s content to categorize it into one or more predefined groups. Common use cases include: Organizing news articles into categories like politics, sports, entertainment, or technology. Conducting sentiment analysis, where customer reviews of products or services are classified as positive, negative, or neutral. By leveraging text classification, organizations can enhance their ability to process and understand large volumes of text data efficiently. Picking a Model When selecting a model for text classification, it’s crucial to evaluate several factors to ensure optimal performance for your specific use case. Start by analyzing the nature of your data, considering whether it is formal or informal and its length (e.g., tweets vs. reviews). Determine if your task requires binary classification (like spam detection) or multiclass classification (such as categorizing news topics), as some models excel in specific scenarios. Next, assess the model complexity; simpler models like Logistic Regression work well for straightforward tasks, while more complex models like BERT are suited for nuanced understanding. Consider the availability of labeled data—larger datasets allow for training sophisticated models, whereas smaller datasets may benefit from pre-trained options. Define key performance metrics (e.g., accuracy, F1 score) to inform your choice, and ensure the model’s interpretability meets your requirements. Finally, account for resource constraints, as advanced models will demand more memory and processing power. To explore and select from a variety of models, visit Spark NLP Models, where you can find models tailored for different tasks and datasets. Recommended Models for Specific Text Classification Tasks Sentiment Analysis: Use models specifically designed for sentiment detection, such as distilbert_sequence_classifier_sst2. News Categorization: Models like distilroberta-finetuned-financial-news-sentiment-analysis are ideal for classifying news articles into relevant categories. Review Analysis: For product reviews, consider using distilbert_base_uncased_finetuned_sentiment_amazon for more nuanced insights. If you have specific needs that are not covered by existing models, you can train your own model tailored to your unique requirements. Follow the guidelines provided in the Spark NLP Training Documentation to get started on creating and training a model suited for your text classification task. By thoughtfully considering these factors and using the right models, you can enhance your NLP applications significantly. How to use {% include programmingLanguageSelectScalaPython.html %} import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Assembling the document from the input text documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) # Tokenizing the text tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) # Loading a pre-trained sequence classification model # You can replace `BertForSequenceClassification.pretrained()` with your selected model # For example: BertForSequenceClassification.pretrained(&quot;distilbert_sequence_classifier_sst2&quot;, &quot;en&quot;) sequenceClassifier = BertForSequenceClassification.pretrained() .setInputCols([&quot;token&quot;, &quot;document&quot;]) .setOutputCol(&quot;label&quot;) .setCaseSensitive(True) # Defining the pipeline with document assembler, tokenizer, and classifier pipeline = Pipeline().setStages([ documentAssembler, tokenizer, sequenceClassifier ]) # Creating a sample DataFrame data = spark.createDataFrame([[&quot;I loved this movie when I was a child.&quot;, &quot;It was pretty boring.&quot;]]).toDF(&quot;text&quot;) # Fitting the pipeline and transforming the data result = pipeline.fit(data).transform(data) # Showing the classification result result.select(&quot;label.result&quot;).show(truncate=False) ++ |result| ++ |[pos] | |[neg] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline // Step 1: Convert raw text into document format val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) // Step 2: Tokenize the document into words val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) // Step 3: Load a pre-trained BERT model for sequence classification val sequenceClassifier = BertForSequenceClassification.pretrained() .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;label&quot;) .setCaseSensitive(true) // Step 4: Define the pipeline with stages for document assembly, tokenization, and classification val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, sequenceClassifier )) // Step 5: Create sample data and apply the pipeline val data = Seq(&quot;I loved this movie when I was a child.&quot;, &quot;It was pretty boring.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) // Step 6: Show the classification results result.select(&quot;label.result&quot;).show(false) ++ |result| ++ |[pos] | |[neg] | ++ Try Real-Time Demos! If you want to see the outputs of text classification models in real time, visit our interactive demos: BERT Annotators Demo – A live demo where you can try your inputs on text classification models on the go. Sentiment &amp; Emotion Detection Demo – An interactive demo for sentiment and emotion detection. Useful Resources Want to dive deeper into text classification with Spark NLP? Here are some curated resources to help you get started and explore further: Articles and Guides Mastering Text Classification with Spark NLP Unlocking the Power of Sentiment Analysis with Deep Learning Sentiment Analysis with Spark NLP without Machine Learning Financial Sentiment Analysis Using SparkNLP Achieving 95% Accuracy Notebooks Text Classification with ClassifierDL Training Scripts Training Multi-class Text and Sentiment Classification models Training a text classification model with INSTRUCTOR Embeddings",
    "url": "/docs/en/tasks/text_classification",
    "relUrl": "/docs/en/tasks/text_classification"
  },
  "72": {
    "id": "72",
    "title": "Text Generation",
    "content": "Text generation is the task of generating meaningful text based on a given input. It is widely used in various natural language processing (NLP) applications such as summarization, machine translation, conversational agents, and more. Spark NLP provides SOTA solutions for text generation, enabling you to produce high-quality and contextually relevant text outputs. Text generation models create text sequences by predicting the next word or sequence of words based on the input prompt. Common use cases include: Summarization: Automatically generating concise summaries from longer text. Machine Translation: Translating text from one language to another while maintaining meaning and fluency. Conversational Agents: Building intelligent systems that can hold natural and coherent conversations with users. By leveraging text generation, organizations can build systems capable of generating human-like text, making it useful for content creation, automated writing, and more. Picking a Model When selecting a model for text generation, consider several important factors. First, determine the type of output you require (e.g., summarization, translation, or free-form generation). Decide whether your task needs structured output like summaries or creative text generation. Next, evaluate the style and language of the data you’ll be working with—are you dealing with formal language (e.g., research papers) or informal language (e.g., social media)? Model performance metrics such as perplexity, BLEU score, or ROUGE score are also crucial for understanding the quality of the generated text. Finally, take into account the computational resources available, as some models (e.g., GPT or T5) may require significant memory and processing power. Explore models tailored for text generation at Spark NLP Models, where you’ll find various options for different text generation tasks. Recommended Models for Specific Text Generation Tasks Summarization: Use models like t5-base and bart-large-cnn for general-purpose text summarization tasks. Machine Translation: Consider models such as t5_base and m2m100_418M you can also consider searching models with the Marian Transformer Annotator class for translating between non-english languages. Conversational Agents: For building chatbots and dialogue systems, use models like gpt2 to generate coherent and contextually aware responses. By selecting the appropriate text generation model, you can enhance your ability to produce contextually rich and meaningful text outputs tailored to your specific NLP tasks. How to use {% include programmingLanguageSelectScalaPython.html %} import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Assembling the document from the input text documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;documents&quot;) # Loading a pre-trained text generation model # You can replace `T5Transformer.pretrained(&quot;t5_small&quot;, &quot;xx&quot;)` with your selected model and the transformer it&#39;s based on # For example: BartTransformer.pretrained(&quot;bart_large_cnn&quot;) t5 = T5Transformer.pretrained(&quot;t5_small&quot;, &quot;xx&quot;) .setTask(&quot;summarize:&quot;) .setInputCols([&quot;documents&quot;]) .setMaxOutputLength(200) .setOutputCol(&quot;summaries&quot;) # Defining the pipeline with document assembler, tokenizer, and classifier pipeline = Pipeline().setStages([documentAssembler, t5]) # Creating a sample DataFrame data = spark.createDataFrame([[ &quot;Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a &quot; + &quot;downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness&quot; + &quot; of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this &quot; + &quot;paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework &quot; + &quot;that converts all text-based language problems into a text-to-text format. Our systematic study compares &quot; + &quot;pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens &quot; + &quot;of language understanding tasks. By combining the insights from our exploration with scale and our new &quot; + &quot;Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering &quot; + &quot;summarization, question answering, text classification, and more. To facilitate future work on transfer &quot; + &quot;learning for NLP, we release our data set, pre-trained models, and code.&quot; ]]).toDF(&quot;text&quot;) # Fitting the pipeline and transforming the data result = pipeline.fit(data).transform(data) # Showing the results result.select(&quot;summaries.result&quot;).show(truncate=False) +--+ |result | +--+ |[transfer learning has emerged as a powerful technique in natural language processing (NLP) the effectiveness of transfer learning has given rise to a diversity of approaches, | | methodologies, and practice .] | + import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.seq2seq.T5Transformer import org.apache.spark.ml.Pipeline // Step 1: Assembling the document from the input text // Converts the input &#39;text&#39; column into a &#39;document&#39; column, required for NLP tasks val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;documents&quot;) // Step 3: Loading a pre-trained BERT model for token classification // Applies a pre-trained BERT model for Named Entity Recognition (NER) to classify tokens // `T5Transformer.pretrained()` loads the model, and `setInputCols` defines the input columns val t5 = T5Transformer.pretrained(&quot;t5_small&quot;) .setTask(&quot;summarize:&quot;) .setInputCols(Array(&quot;documents&quot;)) .setMaxOutputLength(200) .setOutputCol(&quot;summaries&quot;) // Step 4: Defining the pipeline // The pipeline stages are document assembler, tokenizer, and token classifier val pipeline = new Pipeline().setStages(Array(documentAssembler, t5)) // Step 5: Creating a sample DataFrame // Creates a DataFrame with a sample sentence that will be processed by the pipeline val data = Seq( &quot;Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a &quot; + &quot;downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness&quot; + &quot; of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this &quot; + &quot;paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework &quot; + &quot;that converts all text-based language problems into a text-to-text format. Our systematic study compares &quot; + &quot;pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens &quot; + &quot;of language understanding tasks. By combining the insights from our exploration with scale and our new &quot; + &quot;Colossal Clean Crawled Corpus, we achieve state-of-the-art results on many benchmarks covering &quot; + &quot;summarization, question answering, text classification, and more. To facilitate future work on transfer &quot; + &quot;learning for NLP, we release our data set, pre-trained models, and code.&quot; ).toDF(&quot;text&quot;) // Step 6: Fitting the pipeline and transforming the data // The pipeline is fitted on the input data, then it performs the transformation to generate token labels val result = pipeline.fit(data).transform(data) // Step 7: Showing the results // Displays the &#39;label.result&#39; column, which contains the Named Entity Recognition (NER) labels for each token result.select(&quot;summaries.result&quot;).show(false) +--+ |result | +--+ |[transfer learning has emerged as a powerful technique in natural language processing (NLP) the effectiveness of transfer learning has given rise to a diversity of approaches, | |methodologies, and practice .] | +--+ Try Real-Time Demos! If you want to see the outputs of text generation models in real time, visit our interactive demos: Generative Pre-trained Transformer 2 (OpenAI GPT2) – GPT-2 generates human-like text from prompts. Text-To-Text Transfer Transformer (Google T5) – T5 performs text tasks like summarization and translation. SQL Query Generation – Converts natural language commands into SQL queries. Multilingual Text Translation with MarianMT – Translates text between multiple languages. Useful Resources Want to dive deeper into text generation with Spark NLP? Here are some curated resources to help you get started and explore further: Articles and Guides Empowering NLP with Spark NLP and T5 Model: Text Summarization and Question Answering Multilingual machine translation with Spark NLP Notebooks GPT2Transformer: OpenAI Text-To-Text Transformer LLAMA2Transformer: CausalLM wiht Open Source models SQL Code Generation and Style Transfer with T5 T5 Workshop with Spark NLP Translation in Spark NLP Summarization in Spark NLP OpenAI in SparkNLP",
    "url": "/docs/en/tasks/text_generation",
    "relUrl": "/docs/en/tasks/text_generation"
  },
  "73": {
    "id": "73",
    "title": "Text Preprocessing",
    "content": "Text Preprocessing is the foundational task of cleaning and transforming raw text data into a structured format that can be used in NLP tasks. It involves a series of steps to normalize text, remove noise, and prepare it for deeper analysis. Spark NLP provides a range of tools for efficient and scalable text preprocessing. Key Preprocessing Steps When preprocessing text, consider the following key steps along with the recommended Spark NLP annotators: Tokenization: Break text into smaller units (words, subwords, or sentences). Spell Checking: Correct misspelled words to improve accuracy in NLP tasks. Normalization: Standardize text by converting to lowercase, expanding contractions, or removing accents. Stopword Removal: Remove common, non-informative words (e.g., “the,” “is,” “and”). Lemmatization: Reduce words to their base form (e.g., “running” → “run”). These steps and annotators will help ensure your text data is clean, consistent, and ready for analysis. How to use {% include programmingLanguageSelectScalaPython.html %} import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Document Assembler: Converts input text into a suitable format for NLP processing documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) # Tokenizer: Splits text into individual tokens (words) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;tokens&quot;) # SpellChecker: Corrects misspelled words spellChecker = NorvigSweetingModel.pretrained() .setInputCols([&quot;tokens&quot;]) .setOutputCol(&quot;corrected&quot;) # Normalizer: Cleans and standardizes text data normalizer = Normalizer() .setInputCols([&quot;corrected&quot;]) .setOutputCol(&quot;normalized&quot;) # StopWordsCleaner: Removes stopwords stopwordsCleaner = StopWordsCleaner() .setInputCols([&quot;normalized&quot;]) .setOutputCol(&quot;cleanTokens&quot;) # Lemmatizer: Reduces words to their base form lemmatizer = LemmatizerModel.pretrained() .setInputCols([&quot;cleanTokens&quot;]) .setOutputCol(&quot;lemmas&quot;) # Pipeline: Assembles the document assembler and preprocessing stages pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker, normalizer, stopwordsCleaner, lemmatizer ]) # Input Data: A small example dataset is created and converted to a DataFrame data = spark.createDataFrame([[&quot;Text preprocessing is essential in NLP!&quot;]]).toDF(&quot;text&quot;) # Running the Pipeline: Fits the pipeline to the data and preprocesses the text result = pipeline.fit(data).transform(data) # Output: Displays the processed tokens and lemmas result.select(&quot;lemmas.result&quot;).show(truncate=False) +-+ |lemmas.result | +-+ |[text, preprocess, essential, in, NLP] | +-+ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline import spark.implicits._ // Document Assembler: Converts input text into a suitable format for NLP processing val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) // Tokenizer: Splits text into individual tokens (words) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;tokens&quot;) // SpellChecker: Corrects misspelled words val spellChecker = NorvigSweetingModel.pretrained() .setInputCols(Array(&quot;tokens&quot;)) .setOutputCol(&quot;corrected&quot;) // Normalizer: Cleans and standardizes text data val normalizer = new Normalizer() .setInputCols(Array(&quot;corrected&quot;)) .setOutputCol(&quot;normalized&quot;) // StopWordsCleaner: Removes stopwords val stopwordsCleaner = new StopWordsCleaner() .setInputCols(Array(&quot;normalized&quot;)) .setOutputCol(&quot;cleanTokens&quot;) // Lemmatizer: Reduces words to their base form val lemmatizer = LemmatizerModel.pretrained() .setInputCols(Array(&quot;cleanTokens&quot;)) .setOutputCol(&quot;lemmas&quot;) // Pipeline: Assembles the document assembler and preprocessing stages val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker, normalizer, stopwordsCleaner, lemmatizer )) // Input Data: A small example dataset is created and converted to a DataFrame val data = Seq(&quot;Text preprocessing is essential in NLP!&quot;).toDF(&quot;text&quot;) // Running the Pipeline: Fits the pipeline to the data and preprocesses the text val result = pipeline.fit(data).transform(data) // Display the results result.select(&quot;lemmas.result&quot;).show(false) +-+ |result | +-+ |[text, preprocess, essential, in, NLP] | +-+ Try Real-Time Demos! If you want to see text preprocessing in real-time, check out our interactive demos: Text Preprocessing with Spark NLP – Explore how Spark NLP preprocesses raw text data. Stopwords Removing with Spark NLP – Explore how Spark NLP removes stop words from text. Useful Resources Want to learn more about text preprocessing with Spark NLP? Explore the following resources: Articles and Guides Text cleaning: removing stopwords from text with Spark NLP Unleashing the Power of Text Tokenization with Spark NLP Tokenizing Asian texts into words with word segmentation models in Spark NLP Text Cleaning: Standard Text Normalization with Spark NLP Boost Your NLP Results with Spark NLP Stemming and Lemmatizing Techniques Sample Text Data Preprocessing Implementation In SparkNLP Notebooks Text Preprocessing with SparkNLP Annotators Transformers Text_Preprocessing_with_SparkNLP Word Stemming with Stemmer Document Normalizer Cleaning Stop Words",
    "url": "/docs/en/tasks/text_preprocessing",
    "relUrl": "/docs/en/tasks/text_preprocessing"
  },
  "74": {
    "id": "74",
    "title": "Text Summarization - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/text_summarization",
    "relUrl": "/text_summarization"
  },
  "75": {
    "id": "75",
    "title": "Third Party Projects",
    "content": "There are third party projects that can integrate with Spark NLP. These packages need to be installed separately to be used. If you’d like to integrate your application with Spark NLP, please send us a message! Logging Comet Comet is a meta machine learning platform designed to help AI practitioners and teams build reliable machine learning models for real-world applications by streamlining the machine learning model lifecycle. By leveraging Comet, users can track, compare, explain and reproduce their machine learning experiments. Comet can easily integrated into the Spark NLP workflow with the a dedicated logging class CometLogger to log training and evaluation metrics, pipeline parameters and NER visualization made with sparknlp-display. For more information see the User Guide and for more examples see the Spark NLP Examples. Python API: CometLogger Show Example # Metrics while training an annotator can be logged with for example: import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.logging.comet import CometLogger spark = sparknlp.start() OUTPUT_LOG_PATH = &quot;./run&quot; logger = CometLogger() document = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) embds = ( UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) ) multiClassifier = ( MultiClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;labels&quot;) .setBatchSize(128) .setLr(1e-3) .setThreshold(0.5) .setShufflePerEpoch(False) .setEnableOutputLogs(True) .setOutputLogsPath(OUTPUT_LOG_PATH) .setMaxEpochs(1) ) logger.monitor(logdir=OUTPUT_LOG_PATH, model=multiClassifier) trainDataset = spark.createDataFrame( [(&quot;Nice.&quot;, [&quot;positive&quot;]), (&quot;That&#39;s bad.&quot;, [&quot;negative&quot;])], schema=[&quot;text&quot;, &quot;labels&quot;], ) pipeline = Pipeline(stages=[document, embds, multiClassifier]) pipeline.fit(trainDataset) logger.end() # If you are using a jupyter notebook, it is possible to display the live web # interface with logger.experiment.display(tab=&#39;charts&#39;) MLflow Spark NLP uses Spark MLlib Pipelines, what are natively supported by MLFlow. MLFlow is, as stated in their official webpage, an open source platform for the machine learning lifecycle, that includes: Mlflow Tracking: Record and query experiments: code, data, config, and results MLflow Projects: Package data science code in a format to reproduce runs on any platform MLflow Models: Deploy machine learning models in diverse serving environments Model Registry: Store, annotate, discover, and manage models in a central repository For more information, please see the complete guide at Experiment Tracking.",
    "url": "/docs/en/third-party-projects",
    "relUrl": "/docs/en/third-party-projects"
  },
  "76": {
    "id": "76",
    "title": "Token Classification",
    "content": "Token classification is the task of assigning a label to each token (word or sub-word) in a given text sequence. It is fundamental in various natural language processing (NLP) tasks like named entity recognition (NER), part-of-speech tagging (POS), and more. Spark NLP provides state of the art solutions to tackle token classification challenges effectively, helping you analyze and label individual tokens in a document. Token classification involves processing text at a granular level, labeling each token for its role or entity. Typical use cases include: Named Entity Recognition (NER): Identifying proper names, locations, organizations, etc., within text. Part-of-Speech Tagging (POS): Labeling each token with its grammatical category (e.g., noun, verb, adjective). By utilizing token classification, organizations can enhance their ability to extract detailed insights from text data, enabling applications like information extraction, text annotation, and more. Picking a Model When selecting a model for token classification, it’s important to consider various factors that impact performance. First, analyze the type of entities or tags you want to classify (e.g., named entities, parts of speech). Determine if your task requires fine-grained tagging (such as multiple types of named entities) or a simpler tag set. Next, assess the complexity of your data—does it involve formal text like news articles, or informal text like social media posts? Model performance metrics (e.g., precision, recall, F1 score) are also key to determining whether a model is suitable. Lastly, evaluate your computational resources, as more complex models like BERT may require greater memory and processing power. You can explore and select models for your token classification tasks at Spark NLP Models, where you’ll find various models for specific datasets and challenges. Recommended Models for Specific Token Classification Tasks Named Entity Recognition (NER): Use models like bert-base-NER and xlm-roberta-large-finetuned-conll03-english for general-purpose NER tasks. Part-of-Speech Tagging (POS): For POS tagging, consider using models such as pos_anc. Healthcare NER: For clinical texts, ner_jsl and pos_clinical is tailored for extracting medical entities. If existing models do not meet your requirements, you can train your own custom model using the Spark NLP Training Documentation. By selecting the appropriate model, you can optimize token classification performance for your specific NLP tasks. How to use {% include programmingLanguageSelectScalaPython.html %} import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Assembling the document from the input text documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) # Tokenizing the text tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) # Loading a pre-trained sequence classification model # You can replace `BertForTokenClassification.pretrained()` with your selected model and the transformer it&#39;s based on # For example: XlmRoBertaForTokenClassification.pretrained(&quot;xlmroberta_ner_large_finetuned_conll03_english&quot;,&quot;xx&quot;) tokenClassifier = BertForTokenClassification.pretrained() .setInputCols([&quot;token&quot;, &quot;document&quot;]) .setOutputCol(&quot;label&quot;) .setCaseSensitive(True) # Defining the pipeline with document assembler, tokenizer, and classifier pipeline = Pipeline().setStages([ documentAssembler, tokenizer, tokenClassifier ]) # Creating a sample DataFrame data = spark.createDataFrame([[&quot;John Lenon was born in London and lived in Paris. My name is Sarah and I live in London&quot;]]).toDF(&quot;text&quot;) # Fitting the pipeline and transforming the data result = pipeline.fit(data).transform(data) # Showing the results result.select(&quot;label.result&quot;).show(truncate=False) &lt;!-- ++ |result | ++ |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]| ++ --&gt; import spark.implicits._ import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline // Step 1: Assembling the document from the input text // Converts the input &#39;text&#39; column into a &#39;document&#39; column, required for NLP tasks val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) // Step 2: Tokenizing the text // Splits the &#39;document&#39; column into tokens (words), creating the &#39;token&#39; column val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) // Step 3: Loading a pre-trained BERT model for token classification // Applies a pre-trained BERT model for Named Entity Recognition (NER) to classify tokens // `BertForTokenClassification.pretrained()` loads the model, and `setInputCols` defines the input columns val tokenClassifier = BertForTokenClassification.pretrained() .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;label&quot;) .setCaseSensitive(true) // Step 4: Defining the pipeline // The pipeline stages are document assembler, tokenizer, and token classifier val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, tokenClassifier )) // Step 5: Creating a sample DataFrame // Creates a DataFrame with a sample sentence that will be processed by the pipeline val data = Seq(&quot;John Lenon was born in London and lived in Paris. My name is Sarah and I live in London&quot;).toDF(&quot;text&quot;) // Step 6: Fitting the pipeline and transforming the data // The pipeline is fitted on the input data, then it performs the transformation to generate token labels val result = pipeline.fit(data).transform(data) // Step 7: Showing the results // Displays the &#39;label.result&#39; column, which contains the Named Entity Recognition (NER) labels for each token result.select(&quot;label.result&quot;).show(false) // Output: // ++ // |result | // ++ // |[B-PER, I-PER, O, O, O, B-LOC, O, O, O, B-LOC, O, O, O, O, B-PER, O, O, O, O, B-LOC]| // ++ Try Real-Time Demos! If you want to see the outputs of text classification models in real time, visit our interactive demos: BERT Annotators Demo – A live demo where you can try your inputs on classification models on the go. Named Entity Recognition (NER) – A live demo where you can try your inputs on NER models on the go. POS Tagging – A live demo where you can try your inputs on preception models on the go. Recognize Entities - Live Demos &amp; Notebooks – An interactive demo for Recognizing Entities in text Useful Resources Want to dive deeper into text classification with Spark NLP? Here are some curated resources to help you get started and explore further: Articles and Guides Named Entity Recognition (NER) with BERT in Spark NLP The Ultimate Guide to Rule-based Entity Recognition with Spark NLP In-Depth Comparison of Spark NLP for Healthcare and ChatGPT on Clinical Named Entity Recognition Notebooks Transformers for Token Classification in Spark NLP Training Scripts Training Named Entity Recognition (NER) Deep-Learning models Training Conditional Random Fields (CRF) Named Entity Recognition (NER) models",
    "url": "/docs/en/tasks/token_classification",
    "relUrl": "/docs/en/tasks/token_classification"
  },
  "77": {
    "id": "77",
    "title": "Spark NLP -Training",
    "content": "Training Datasets These are classes to load common datasets to train annotators for tasks such as part-of-speech tagging, named entity recognition, spell checking and more. {% include_relative training_entries/pos.md %} {% include_relative training_entries/conll.md %} {% include_relative training_entries/conllu.md %} {% include_relative training_entries/pubtator.md %} Spell Checkers Dataset (Corpus) In order to train a Norvig or Symmetric Spell Checkers, we need to get corpus data as a spark dataframe. We can read a plain text file and transforms it to a spark dataset. Example: {% include programmingLanguageSelectScalaPython.html %} train_corpus = spark.read .text(&quot;./sherlockholmes.txt&quot;) .withColumnRenamed(&quot;value&quot;, &quot;text&quot;) val trainCorpus = spark.read .text(&quot;./sherlockholmes.txt&quot;) .select(trainCorpus.col(&quot;value&quot;).as(&quot;text&quot;)) Text Processing These are annotators that can be trained to process text for tasks such as dependency parsing, lemmatisation, part-of-speech tagging, sentence detection and word segmentation. {% include_relative training_entries/DependencyParserApproach.md %} {% include_relative training_entries/Lemmatizer.md %} {% include_relative training_entries/PerceptronApproach.md %} {% include_relative training_entries/SentenceDetectorDLApproach.md %} {% include_relative training_entries/TypedDependencyParser.md %} {% include_relative training_entries/WordSegmenterApproach.md %} Spell Checkers These are annotators that can be trained to correct text. {% include_relative training_entries/ContextSpellCheckerApproach.md %} {% include_relative training_entries/NorvigSweeting.md %} {% include_relative training_entries/SymmetricDelete.md %} Token Classification These are annotators that can be trained to recognize named entities in text. {% include_relative training_entries/NerCrfApproach.md %} {% include_relative training_entries/NerDLApproach.md %} Text Classification These are annotators that can be trained to classify text into different classes, such as sentiment. {% include_relative training_entries/ClassifierDLApproach.md %} {% include_relative training_entries/MultiClassifierDLApproach.md %} {% include_relative training_entries/SentimentDLApproach.md %} {% include_relative training_entries/ViveknSentimentApproach.md %} Text Representation These are annotators that can be trained to turn text into a numerical representation. {% include_relative training_entries/Doc2VecApproach.md %} {% include_relative training_entries/Word2VecApproach.md %} External Trainable Models These are annotators that are trained in an external library, which are then loaded into Spark NLP. {% include_relative training_entries/AlbertForTokenClassification.md %} {% include_relative training_entries/BertForSequenceClassification.md %} {% include_relative training_entries/BertForTokenClassification.md %} {% include_relative training_entries/DistilBertForSequenceClassification.md %} {% include_relative training_entries/DistilBertForTokenClassification.md %} {% include_relative training_entries/RoBertaForTokenClassification.md %} {% include_relative training_entries/XlmRoBertaForTokenClassification.md %} TensorFlow Graphs NER DL uses Char CNNs - BiLSTM - CRF Neural Network architecture. Spark NLP defines this architecture through a Tensorflow graph, which requires the following parameters: Tags Embeddings Dimension Number of Chars Spark NLP infers these values from the training dataset used in NerDLApproach annotator and tries to load the graph embedded on spark-nlp package. Currently, Spark NLP has graphs for the most common combination of tags, embeddings, and number of chars values: Tags Embeddings Dimension 10 100 10 200 10 300 10 768 10 1024 25 300 All of these graphs use an LSTM of size 128 and number of chars 100 In case, your train dataset has a different number of tags, embeddings dimension, number of chars and LSTM size combinations shown in the table above, NerDLApproach will raise an IllegalArgumentException exception during runtime with the message below: Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Check https://sparknlp.org/docs/en/graph for instructions to generate the required graph. To overcome this exception message we have to follow these steps: Clone spark-nlp github repo Run python file create_models with number of tags, embeddings dimension and number of char values mentioned on your exception message error. cd spark-nlp/python/tensorflow export PYTHONPATH=lib/ner python create_models.py [number_of_tags] [embeddings_dimension] [number_of_chars] [output_path] This will generate a graph on the directory defined on `output_path argument. Retry training with NerDLApproach annotator but this time use the parameter setGraphFolder with the path of your graph. Note: Make sure that you have Python 3 and Tensorflow 1.15.0 installed on your system since create_models requires those versions to generate the graph successfully. Note: We also have a notebook in the same directory if you prefer Jupyter notebook to cerate your custom graph.",
    "url": "/docs/en/training",
    "relUrl": "/docs/en/training"
  },
  "78": {
    "id": "78",
    "title": "Spark NLP - Transformers",
    "content": "{% assign parent_path = “en/transformer_entries” %} {% for file in site.static_files %} {% if file.path contains parent_path %} {% assign file_name = file.path | remove: parent_path | remove: “/” | prepend: “transformer_entries/” %} {% include_relative {{ file_name }} %} {% endif %} {% endfor %} Import Transformers into Spark NLP Overview We have extended support for HuggingFace 🤗 and TF Hub exported models since 3.1.0 to equivalent Spark NLP 🚀 annotators. Starting this release, you can easily use the saved_model feature in HuggingFace within a few lines of codes and import any BERT, DistilBERT, CamemBERT, RoBERTa, DeBERTa, XLM-RoBERTa, Longformer, BertForTokenClassification, DistilBertForTokenClassification, AlbertForTokenClassification, RoBertaForTokenClassification, DeBertaForTokenClassification, XlmRoBertaForTokenClassification, XlnetForTokenClassification, LongformerForTokenClassification, CamemBertForTokenClassification, CamemBertForSequenceClassification, CamemBertForQuestionAnswering, BertForSequenceClassification, DistilBertForSequenceClassification, AlbertForSequenceClassification, RoBertaForSequenceClassification, DeBertaForSequenceClassification, XlmRoBertaForSequenceClassification, XlnetForSequenceClassification, LongformerForSequenceClassification, AlbertForQuestionAnswering, BertForQuestionAnswering, DeBertaForQuestionAnswering, DistilBertForQuestionAnswering, LongformerForQuestionAnswering, RoBertaForQuestionAnswering, XlmRoBertaForQuestionAnswering, TapasForQuestionAnswering, Vision Transformers (ViT), HubertForCTC, SwinForImageClassification, and ConvNextForImageClassification models to Spark NLP. We will work on the remaining annotators and extend this support to the rest with each release 😊 Compatibility Spark NLP: The equivalent annotator in Spark NLP TF Hub: Models from TF Hub HuggingFace: Models from HuggingFace ONNX: Models from HuggingFace in ONNX format Model Architecture: Which architecture is compatible with that annotator Flags: Fully supported ✅ Partially supported (requires workarounds) ✔️ Under development ❎ Not supported ❌ Spark NLP TF Hub HuggingFace ONNX Model Architecture AlbertEmbeddings ✅ ✅ ✅ ALBERT AlbertForQuestionAnswering   ✅ ❎ TFAlbertForQuestionAnswering AlbertForSequenceClassification   ✅ ❎ TFAlbertForSequenceClassification AlbertForTokenClassification   ✅ ❎ TFAlbertForTokenClassification Automatic Speech Recognition (Wav2Vec2ForCTC)   ❎ ❎ TFWav2Vec2ForCTC BartForZeroShotClassification   ✅ ❎ TFBartForSequenceClassification BartTransformer   ✅ ❎ TFBartForConditionalGeneration BertEmbeddings ✅ ✅ ✅ BERT - Small BERT - ELECTRA BertForQuestionAnswering   ✅ ✅ TFBertForQuestionAnswering BertForSequenceClassification   ✅ ✅ TFBertForSequenceClassification BertForTokenClassification   ✅ ✅ TFBertForTokenClassification BertForZeroShotClassification   ✅ ❎ TFBertForSequenceClassification BertSentenceEmbeddings ✅ ✅ ❎ BERT - Small BERT - ELECTRA CamemBertEmbeddings   ✅ ✅ CamemBERT CamemBertForQuestionAnswering   ✅ ❎ TFCamembertForQuestionAnswering CamemBertForSequenceClassification   ✅ ❎ TFCamemBertForSequenceClassification CamemBertForTokenClassification   ✅ ❎ TFCamemBertForTokenClassification CLIPForZeroShotClassification   ✅ ✅ CLIP ConvNextForImageClassification   ❎ ❎ TFConvNextForImageClassification DeBertaEmbeddings   ✅ ✅ DeBERTa-v2 - DeBERTa-v3 DeBertaForQuestionAnswering   ✅ ❎ TFDebertaV2ForQuestionAnswering DeBertaForSequenceClassification   ✅ ❎ TFDebertaV2ForSequenceClassification DeBertaForTokenClassification   ✅ ❎ TFDebertaV2ForTokenClassification DistilBertEmbeddings   ✅ ✅ DistilBERT DistilBertForQuestionAnswering   ✅ ✅ TFDistilBertForQuestionAnswering DistilBertForSequenceClassification   ✅ ✅ TFDistilBertForSequenceClassification DistilBertForTokenClassification   ✅ ✅ TFDistilBertForTokenClassification DistilBertForZeroShotClassification   ✅ ❎ TFDistilBertForSequenceClassification E5Embeddings   ✅ ✅ SentenceTransformer ElmoEmbeddings ❎   ❌   HubertForCTC   ❎ ❎ TFHubertForCTC InstructorEmbeddings   ✅ ❎ INSTRUCTOR LongformerEmbeddings   ✅ ❌ Longformer LongformerForQuestionAnswering   ✅ ❎ TFLongformerForQuestionAnswering LongformerForSequenceClassification   ✅ ❎ TFLongformerForSequenceClassification LongformerForTokenClassification   ✅ ❎ TFLongformerForTokenClassification MarianTransformer   ❌ ❎   MPNetEmbeddings   ✅ ✅ SentenceTransformer OpenAI GPT2   ❌ ❎   RoBertaEmbeddings   ✅ ✅ RoBERTa - DistilRoBERTa RoBertaForQuestionAnswering   ✅ ❎ TFRobertaForQuestionAnswering RoBertaForSequenceClassification   ✅ ❎ TFRobertaForSequenceClassification RoBertaForTokenClassification   ✅ ❎ TFRobertaForTokenClassification RoBertaForZeroShotClassification   ✅ ❎ TFRobertaForSequenceClassification RoBertaSentenceEmbeddings   ✅ ✅ RoBERTa - DistilRoBERTa SwinForImageClassification   ❎ ❎ TFSwinForImageClassification T5Transformer   ❌ ❎   TapasForQuestionAnswering   ❎ ❎ TFTapasForQuestionAnswering UniversalSentenceEncoder ❎   ❌   VisionEncoderDecoderForImageCaptioning   ✅ ❎ VisionEncoderDecoderModel ViTForImageClassification ❌ ✅ ❎ TFViTForImageClassification WhisperForCTC   ✅ ✅ WhisperForConditionalGeneration XlmRoBertaEmbeddings   ✅ ✅ XLM-RoBERTa XlmRoBertaForQuestionAnswering   ✅ ❎ TFXLMRobertaForQuestionAnswering XlmRoBertaForSequenceClassification   ✅ ❎ TFXLMRobertaForSequenceClassification XlmRoBertaForTokenClassification   ✅ ❎ TFXLMRobertaForTokenClassification XlmRoBertaForZeroShotClassification   ✅ ❎ TFXLMRobertaForSequenceClassification XlmRoBertaSentenceEmbeddings   ✅ ❎ SentenceTransformer XlnetEmbeddings   ✅ ❌ XLNet XlnetForSequenceClassification   ✅ ❎ TFXLNetForSequenceClassification XlnetForTokenClassification   ✅ ❎ TFXLNetForTokenClassificationet ZeroShotNerModel   ✅ ❎ TFRobertaForSequenceClassification Example Notebooks HuggingFace, Optimum, PyTorch, and ONNX Runtime to Spark NLP (ONNX) Spark NLP Notebooks Colab AlbertForQuestionAnswering HuggingFace ONNX in Spark NLP AlbertForQuestionAnswering AlbertForSequenceClassification HuggingFace ONNX in Spark NLP AlbertForSequenceClassification AlbertForTokenClassification HuggingFace ONNX in Spark NLP AlbertForTokenClassification BertEmbeddings HuggingFace ONNX in Spark NLP BERT BertForQuestionAnswering HuggingFace ONNX in Spark NLP BertForQuestionAnswering BertForSequenceClassification HuggingFace ONNX in Spark NLP BertForSequenceClassification BertForTokenClassification HuggingFace ONNX in Spark NLP BertForTokenClassification BertSentenceEmbeddings HuggingFace ONNX in Spark NLP BertSentenceEmbeddings CLIPForZeroShotClassification HuggingFace ONNX in Spark NLP CLIP DeBertaEmbeddings HuggingFace ONNX in Spark NLP DeBERTa DistilBertEmbeddings HuggingFace ONNX in Spark NLP DistilBERT DistilBertForQuestionAnswering HuggingFace ONNX in Spark NLP DistilBertForQuestionAnswering DistilBertForSequenceClassification HuggingFace ONNX in Spark NLP DistilBertForSequenceClassification DistilBertForTokenClassification HuggingFace ONNX in Spark NLP DistilBertForTokenClassification E5Embeddings HuggingFace ONNX in Spark NLP E5 MarianTransformer HuggingFace ONNX in Spark NLP Marian MPNet HuggingFace ONNX in Spark NLP MPNet RoBertaEmbeddings HuggingFace ONNX in Spark NLP RoBERTa RobertaForQuestionAnswering HuggingFace ONNX in Spark NLP RoBertaForQuestionAnswering RoBertaForSequenceClassification HuggingFace ONNX in Spark NLP RoBertaForSequenceClassification RoBertaForTokenClassification HuggingFace ONNX in Spark NLP RoBertaForTokenClassification T5Transformer HuggingFace ONNX in Spark NLP T5 WhisperForCTC HuggingFace ONNX in Spark NLP MPNet XlmRoBertaSentenceEmbeddings HuggingFace ONNX in Spark NLP XlmRoBertaSentenceEmbeddings HuggingFace to Spark NLP (TensorFlow) Spark NLP Notebooks Colab AlbertEmbeddings HuggingFace in Spark NLP - ALBERT AlbertForQuestionAnswering HuggingFace in Spark NLP - AlbertForQuestionAnswering AlbertForSequenceClassification HuggingFace in Spark NLP - AlbertForSequenceClassification AlbertForTokenClassification HuggingFace in Spark NLP - AlbertForTokenClassification BertEmbeddings HuggingFace in Spark NLP - BERT BertForQuestionAnswering HuggingFace in Spark NLP - BertForQuestionAnswering BertForSequenceClassification HuggingFace in Spark NLP - BertForSequenceClassification BertForTokenClassification HuggingFace in Spark NLP - BertForTokenClassification BertForZeroShotClassification HuggingFace in Spark NLP - BertForZeroShotClassification BertSentenceEmbeddings HuggingFace in Spark NLP - BERT Sentence BertSentenceEmbeddings - Fine Tuned Fine Tuned Sentence Bert in Spark NLP CamemBertEmbeddings HuggingFace in Spark NLP - CamemBERT CamemBertForQuestionAnswering HuggingFace in Spark NLP - CamemBertForQuestionAnswering CamemBertForSequenceClassification HuggingFace in Spark NLP - CamemBertForSequenceClassification CamemBertForTokenClassification HuggingFace in Spark NLP - CamemBertForTokenClassification ConvNextForImageClassification HuggingFace in Spark NLP - ConvNextForImageClassification DeBertaEmbeddings HuggingFace in Spark NLP - DeBERTa DeBertaForQuestionAnswering HuggingFace in Spark NLP - DeBertaForQuestionAnswering DistilBertEmbeddings HuggingFace in Spark NLP - DistilBERT DistilBertForQuestionAnswering HuggingFace in Spark NLP - DistilBertForQuestionAnswering DistilBertForSequenceClassification HuggingFace in Spark NLP - DistilBertForSequenceClassification DistilBertForTokenClassification HuggingFace in Spark NLP - DistilBertForTokenClassification DistilBertForZeroClassification HuggingFace in Spark NLP - DistilBertForZeroClassification DistilBertForZeroShotClassification HuggingFace in Spark NLP - DistilBertForZeroShotClassification LongformerEmbeddings HuggingFace in Spark NLP - Longformer LongformerForQuestionAnswering HuggingFace in Spark NLP - LongformerForQuestionAnswering LongformerForSequenceClassification HuggingFace in Spark NLP - LongformerForSequenceClassification RoBertaEmbeddings HuggingFace in Spark NLP - RoBERTa RoBertaForQuestionAnswering HuggingFace in Spark NLP - RoBertaForQuestionAnswering RoBertaForSequenceClassification HuggingFace in Spark NLP - RoBertaForSequenceClassification RoBertaForTokenClassification HuggingFace in Spark NLP - RoBertaForTokenClassification RoBertaForZeroShotClassification HuggingFace in Spark NLP - RoBertaForZeroShotClassification SwinForImageClassification HuggingFace in Spark NLP - SwinForImageClassification ViTForImageClassification HuggingFace in Spark NLP - ViTForImageClassification WhisperForCTC HuggingFace in Spark NLP - WhisperForCTC XlmRoBertaEmbeddings HuggingFace in Spark NLP - XLM-RoBERTa XlmRobertaForQuestionAnswering HuggingFace in Spark NLP - XlmRobertaForQuestionAnswering XlmRoBertaForSequenceClassification HuggingFace in Spark NLP - XlmRoBertaForSequenceClassification XlmRoBertaForTokenClassification HuggingFace in Spark NLP - XlmRoBertaForTokenClassification XlnetEmbeddings HuggingFace in Spark NLP - XLNet XlnetForSequenceClassification HuggingFace in Spark NLP - XlnetForSequenceClassification T5Transformer HuggingFace in Spark NLP - T5   TF Hub to Spark NLP Spark NLP TF Hub Notebooks Colab AlbertEmbeddings TF Hub in Spark NLP - ALBERT BertEmbeddings TF Hub in Spark NLP - BERT BertSentenceEmbeddings TF Hub in Spark NLP - BERT Sentence",
    "url": "/docs/en/transformers",
    "relUrl": "/docs/en/transformers"
  },
  "79": {
    "id": "79",
    "title": "Translation",
    "content": "Translation is the task of converting text from one language into another. This is essential for multilingual applications such as content localization, cross-language communication, and more. Spark NLP offers advanced translation models that provide high-quality translations between multiple languages. Translation models process input text in the source language and generate a corresponding translation in the target language. Common use cases include: Cross-Language Communication: Enabling communication across different languages for global teams. Document Translation: Translating long-form content such as reports, articles, or manuals. By using Spark NLP translation models, you can build scalable translation systems to meet your multilingual needs efficiently and accurately. Picking a Model When choosing a translation model, consider factors such as the source and target languages and the size of the input text. Some models may specialize in specific language pairs or offer better performance for certain types of text (e.g., formal versus informal content). Evaluate whether you need document-level translation or sentence-level translation based on the use case. Explore the available translation models at Spark NLP Models to find the one that best suits your translation tasks. Recommended Models for Translation Tasks General Translation: Consider models such as t5_base and m2m100_418M you can also consider searching models with the Marian Transformer Annotator class for translating between non-english languages. Selecting the appropriate model will ensure you produce accurate and fluent translations, tailored to your specific language pair and domain. How to use {% include programmingLanguageSelectScalaPython.html %} import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Document Assembler: Converts input text into a suitable format for NLP processing documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;documents&quot;) # M2M100 Transformer: Loads the pretrained translation model for English to French m2m100 = M2M100Transformer.pretrained(&quot;m2m100_418M&quot;) .setInputCols([&quot;documents&quot;]) .setMaxOutputLength(50) .setOutputCol(&quot;generation&quot;) .setSrcLang(&quot;zh&quot;) # Source language: Chinese .setTgtLang(&quot;en&quot;) # Target language: English # Pipeline: Assembles the document assembler and the M2M100 translation model pipeline = Pipeline().setStages([documentAssembler, m2m100]) # Input Data: A small example dataset is created and converted to a DataFrame data = spark.createDataFrame([[&quot;生活就像一盒巧克力。&quot;]]).toDF(&quot;text&quot;) # Running the Pipeline: Fits the pipeline to the data and generates translations result = pipeline.fit(data).transform(data) # Output: Displays the translated result result.select(&quot;summaries.generation&quot;).show(truncate=False) +-+ |result | +-+ |[ Life is like a box of chocolate.] | +-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.seq2seq.M2M100Transformer import org.apache.spark.ml.Pipeline // Document Assembler: Converts input text into a suitable format for NLP processing val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;documents&quot;) // M2M100 Transformer: Loads the pretrained translation model for Chinese to English val m2m100 = M2M100Transformer.pretrained(&quot;m2m100_418M&quot;) .setInputCols(Array(&quot;documents&quot;)) .setSrcLang(&quot;zh&quot;) // Source language: Chinese .serTgtLang(&quot;en&quot;) // Target language: English .setMaxOutputLength(100) .setDoSample(false) .setOutputCol(&quot;generation&quot;) // Pipeline: Assembles the document assembler and the M2M100 translation model val pipeline = new Pipeline().setStages(Array(documentAssembler, m2m100)) // Input Data: A small example dataset is created and converted to a DataFrame val data = Seq(&quot;生活就像一盒巧克力。&quot;).toDF(&quot;text&quot;) // Running the Pipeline: Fits the pipeline to the data and generates translations val result = pipeline.fit(data).transform(data) // Output: Displays the translated result result.select(&quot;generation.result&quot;).show(truncate = false) +-+ |result | +-+ |[ Life is like a box of chocolate.] | +-+ Try Real-Time Demos! If you want to see the outputs of text generation models in real time, visit our interactive demos: Text-To-Text Transfer Transformer (Google T5) – T5 performs text tasks like summarization and translation. Multilingual Text Translation with MarianMT – Translates text between multiple languages. M2M100 Multilingual Translation Model – Translates text between multiple languages. Useful Resources Want to dive deeper into text generation with Spark NLP? Here are some curated resources to help you get started and explore further: Articles and Guides Multilingual machine translation with Spark NLP Use Spark NLP offline models for Language Translation Notebooks T5 Workshop with Spark NLP Translation in Spark NLP",
    "url": "/docs/en/tasks/translation",
    "relUrl": "/docs/en/tasks/translation"
  },
  "80": {
    "id": "80",
    "title": "Video Tutorials",
    "content": "{%- include extensions/youtube.html id=&#39;isxffn4Tcds&#39; -%}How to Install NLP Server on Azure {%- include extensions/youtube.html id=&#39;YZFhsZZD6QM&#39; -%}How to Import a License in the NLP Server",
    "url": "/docs/en/nlp_server/video_tutorials",
    "relUrl": "/docs/en/nlp_server/video_tutorials"
  },
  "81": {
    "id": "81",
    "title": "Zero-Shot Classification",
    "content": "Zero-Shot Classification is a method of classifying unseen labels in text without needing any prior training data for those labels. This technique is especially useful for scenarios where pre-defined categories are not available, allowing for flexibility in categorizing text based on descriptions of labels alone. Spark NLP offers state-of-the-art solutions for zero-shot classification, enabling users to classify texts into various categories even when no labeled data is available. Zero-shot classification processes text at a broader level, where the system predicts the most relevant labels based on their descriptions. Typical use cases include: Text Categorization: Automatically classifying text into a set of predefined or custom categories based on label descriptions. By leveraging zero-shot classification, organizations can classify large volumes of text data without the need to curate annotated datasets for each possible label, significantly reducing manual efforts in text annotation and data preparation. Picking a Model When selecting a model for zero-shot classification, it is important to consider several factors that impact performance. First, analyze the range of labels or categories you want to classify. Zero-shot classification is versatile, but choosing models trained on broader datasets often yields better results. Next, consider the complexity of your text. Is it formal or informal? Does it involve domain-specific language such as legal or healthcare text? Performance metrics (e.g., accuracy, precision, recall) help assess whether a model fits your requirements. Additionally, ensure you evaluate your computational resources, as larger models, like those based on transformer architectures, may require significant memory and processing power. You can explore and select models for your zero-shot classification tasks at Spark NLP Models, where you’ll find a variety of models for specific datasets and classification challenges. Recommended Models for Zero-Shot Classification Zero-Shot Text Classification: Consider using models like bart-large-mnli for general-purpose multilingual text data classification across various domains. Zero-Shot Named Entity Recognition (NER): Use models like zero_shot_ner_roberta for identifying entities across various domains and languages without requiring task-specific labeled data. If pre-trained models don’t match your exact needs, you can train your own custom model using the Spark NLP Training Documentation. By selecting the appropriate zero-shot classification model, you can expand your ability to analyze text data without predefined labels, providing flexibility for dynamic and evolving classification tasks. How to use {% include programmingLanguageSelectScalaPython.html %} import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # 1. Document Assembler: Converts raw input text into a document format. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) # 2. Tokenizer: Splits the document into individual tokens (words). tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) # 3. Pre-trained Sequence Classifier (Zero-Shot Classification): Loads a pre-trained BART model for zero-shot classification. sequenceClassifier = BartForZeroShotClassification.pretrained() .setInputCols([&quot;token&quot;, &quot;document&quot;]) .setOutputCol(&quot;label&quot;) .setCaseSensitive(True) # 4. Pipeline: Defines a pipeline with three stages - document assembler, tokenizer, and zero-shot classifier. pipeline = Pipeline().setStages([ documentAssembler, tokenizer, sequenceClassifier ]) # 5. Sample Data: Creating a DataFrame with sample text data to test zero-shot classification. data = spark.createDataFrame([[&quot;I loved this movie when I was a child.&quot;, &quot;It was pretty boring.&quot;]]).toDF(&quot;text&quot;) # 6. Fit and Transform: Fits the pipeline to the data and applies the model for classification. result = pipeline.fit(data).transform(data) # 7. Displaying Results: Shows the classification labels assigned to each text (e.g., positive or negative sentiment). result.select(&quot;label.result&quot;).show(truncate=False) &lt;!-- Sample Output: ++ |result| ++ |[pos] | |[neg] | ++ --&gt; import spark.implicits._ import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline // Assembling the document from the input text val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) // Tokenizing the text val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) // Loading the pre-trained zero-shot classification model (BERT) val sequenceClassifier = BertForZeroShotClassification.pretrained() .setInputCols(&quot;token&quot;, &quot;document&quot;) .setOutputCol(&quot;label&quot;) .setCaseSensitive(true) // Creating a pipeline with document assembler, tokenizer, and classifier val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, sequenceClassifier )) // Creating a sample DataFrame val data = Seq(&quot;I loved this movie when I was a child.&quot;, &quot;It was pretty boring.&quot;).toDF(&quot;text&quot;) // Fitting the pipeline and transforming the data val result = pipeline.fit(data).transform(data) // Showing the results result.select(&quot;label.result&quot;).show(false) // Sample Output: // ++ // |result| // ++ // |[pos] | // |[neg] | // ++ Try Real-Time Demos! If you want to see the outputs of text classification models in real time, visit our interactive demos: BERT Annotators Demo – A live demo where you can try your labels and inputs on zero shot classification models on the go. Zero-Shot Named Entity Recognition (NER) – A live demo where you can try your labels and inputs on zero shot classification models on the go. Useful Resources Want to dive deeper into text classification with Spark NLP? Here are some curated resources to help you get started and explore further: Notebooks Zero-Shot Text Classification in Spark NLP Zero-Shot for Named Entity Recognition",
    "url": "/docs/en/tasks/zero_shot_classification",
    "relUrl": "/docs/en/tasks/zero_shot_classification"
  },
  "82": {
    "id": "82",
    "title": "Zero-shot Image Classification",
    "content": "Zero-shot image classification is a technique in computer vision where a model can classify images into categories that it has never seen before during training. This is achieved by leveraging semantic relationships between the image data and textual descriptions of classes, enabling models to predict labels without specific training on each category. This task is particularly useful for scenarios where obtaining labeled data for every possible category is challenging or expensive, such as real-world applications in e-commerce, media, or biology. Zero-shot classification can help scale image recognition systems without constantly retraining them for new categories. How Zero-shot Image Classification Works The key idea behind zero-shot learning is the generalization capability of models. Instead of being restricted to the labels encountered during training, the model uses external knowledge—typically in the form of text or word embeddings—to make predictions about new classes. In Spark NLP, zero-shot image classification leverages models like CLIP (Contrastive Language–Image Pretraining), which are trained to understand both visual and textual data. These models align the visual representations of images with the semantic representations of text, allowing them to match unseen image categories based on their descriptions. Some common use cases include: Classifying new product images in an e-commerce platform without retraining the model for every new product. Detecting rare or new species of animals using images in wildlife research. Media categorization for content recommendation engines where new labels continuously emerge. Picking a Model When choosing a model for zero-shot image classification, you need to consider several factors: Text and Image Alignment: Choose models that are good at matching visual features to text-based descriptions. Task Complexity: Depending on the complexity of the task, a larger pre-trained model like CLIP or a fine-tuned ViT model might perform better. Efficiency: While zero-shot classification saves time by avoiding retraining, some models are more resource-intensive than others. Make sure the model is efficient enough for your computational setup. You can explore a variety of pre-trained zero-shot models on the Spark NLP Models, where models suited for different tasks and datasets are available. Recommended Models for Zero-shot Image Classification CLIP for General Zero-shot Image Classification: Models like clip_vit_large_patch14 and clip-vit-base-patch32 are well-suited for matching image content with textual labels in a zero-shot setting. How to use {% include programmingLanguageSelectScalaPython.html %} import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Loading images into a Spark DataFrame, with an option to discard invalid images imageDF = spark.read .format(&quot;image&quot;) .option(&quot;dropInvalid&quot;, value=True) .load(&quot;src/test/resources/image/&quot;) # Assembling image data using the ImageAssembler, preparing the input images for further processing imageAssembler = ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) # Defining candidate labels for zero-shot classification candidateLabels = [ &quot;a photo of a bird&quot;, &quot;a photo of a cat&quot;, &quot;a photo of a dog&quot;, &quot;a photo of a hen&quot;, &quot;a photo of a hippo&quot;, &quot;a photo of a room&quot;, &quot;a photo of a tractor&quot;, &quot;a photo of an ostrich&quot;, &quot;a photo of an ox&quot; ] # Initializing the CLIPForZeroShotClassification model imageClassifier = CLIPForZeroShotClassification .pretrained(&quot;clip_vit_large_patch14&quot;, &quot;en&quot;) .setInputCols([&quot;image_assembler&quot;]) .setOutputCol(&quot;label&quot;) .setCandidateLabels(candidateLabels) # Defining a Spark ML pipeline with two stages: the ImageAssembler and the CLIP image classifier pipeline = Pipeline().setStages([imageAssembler, imageClassifier]) # Fitting the pipeline on the image DataFrame and transforming the data to apply classification pipelineDF = pipeline.fit(imageDF).transform(imageDF) # Selecting the image file name and the predicted label result, displaying the output in a readable format pipelineDF .selectExpr(&quot;reverse(split(image.origin, &#39;/&#39;))[0] as image_name&quot;, &quot;label.result&quot;) .show(truncate=False) +--+--+ |image_name |result | +--+--+ |palace.JPEG |[a photo of a room] | |egyptian_cat.jpeg|[a photo of a cat] | |hippopotamus.JPEG|[a photo of a hippo] | |hen.JPEG |[a photo of a hen] | |ostrich.JPEG |[a photo of an ostrich]| |junco.JPEG |[a photo of a bird] | |bluetick.jpg |[a photo of a dog] | |chihuahua.jpg |[a photo of a dog] | |tractor.JPEG |[a photo of a tractor] | |ox.JPEG |[a photo of an ox] | +--+--+ import com.johnsnowlabs.nlp.ImageAssembler import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline // Loading image data into a Spark DataFrame, removing any invalid images val imageDF = ResourceHelper.spark.read .format(&quot;image&quot;) .option(&quot;dropInvalid&quot;, value = true) .load(&quot;src/test/resources/image/&quot;) // Assembling the images with the ImageAssembler, which prepares image data for processing val imageAssembler: ImageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) // Defining an array of candidate labels for zero-shot image classification val candidateLabels = Array( &quot;a photo of a bird&quot;, &quot;a photo of a cat&quot;, &quot;a photo of a dog&quot;, &quot;a photo of a hen&quot;, &quot;a photo of a hippo&quot;, &quot;a photo of a room&quot;, &quot;a photo of a tractor&quot;, &quot;a photo of an ostrich&quot;, &quot;a photo of an ox&quot; ) // Initializing the CLIPForZeroShotClassification model, setting input and output columns // The model classifies images based on comparison to the candidate labels val imageClassifier = CLIPForZeroShotClassification .pretrained() // Loading a pretrained CLIP model .setInputCols(&quot;image_assembler&quot;) .setOutputCol(&quot;label&quot;) .setCandidateLabels(candidateLabels) // Creating and running the Spark ML pipeline with the image assembler and classifier val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier)).fit(imageDF).transform(imageDF) // Selecting and displaying the image file name and classification result pipeline .selectExpr(&quot;reverse(split(image.origin, &#39;/&#39;))[0] as image_name&quot;, &quot;label.result&quot;) // Extracting image names and their classification labels .show(truncate = false) +--+--+ |image_name |result | +--+--+ |palace.JPEG |[a photo of a room] | |egyptian_cat.jpeg|[a photo of a cat] | |hippopotamus.JPEG|[a photo of a hippo] | |hen.JPEG |[a photo of a hen] | |ostrich.JPEG |[a photo of an ostrich]| |junco.JPEG |[a photo of a bird] | |bluetick.jpg |[a photo of a dog] | |chihuahua.jpg |[a photo of a dog] | |tractor.JPEG |[a photo of a tractor] | |ox.JPEG |[a photo of an ox] | +--+--+ Try Real-Time Demos! Explore zero-shot image classification with our interactive demos: CLIP for Zero-shot Image Classification Useful Resources Learn zero-shot image classification with Spark NLP: Notebooks CLIP Classification Notebook Discover how to classify images without labeled data.",
    "url": "/docs/en/tasks/zero_shot_image_classification",
    "relUrl": "/docs/en/tasks/zero_shot_image_classification"
  }
  
}
