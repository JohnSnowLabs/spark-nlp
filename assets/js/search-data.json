{
  "0": {
    "id": "0",
    "title": "404",
    "content": "404 Page not found :( &lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt; General Text Embeddings (GTE) English (gte_small) | gte_small | Spark NLP 5.0.2 HomeDocsModelsDemoBlogStar on GitHub Edit on GitHub John Snow Labs Aug 15, 2023 General Text Embeddings (GTE) English (gte_small)open_sourcebertembeddingsenglishenonnx Description General Text Embeddings (GTE) model. Towards General Text Embeddings with Multi-stage Contrastive Learning The GTE models are trained by Alibaba DAMO Academy. They are mainly based on the BERT framework and currently offer three different sizes of models, including GTE-large, GTE-base, and GTE-small. The GTE models are trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. This enables the GTE models to be applied to various downstream tasks of text embeddings, including information retrieval, semantic textual similarity, text reranking, etc. Model Name Model Size (GB) Dimension Sequence Length Average (56) Clustering (11) Pair Classification (3) Reranking (4) Retrieval (15) STS (10) Summarization (1) Classification (12) gte-large 0.67 1024 512 63.13 46.84 85.00 59.13 52.22 83.35 31.66 73.33 gte-base 0.22 768 512 62.39 46.2 84.57 58.61 51.14 82.3 31.17 73.01 e5-large-v2 1.34 1024 512 62.25 44.49 86.03 56.61 50.56 82.05 30.19 75.24 e5-base-v2 0.44 768 512 61.5 43.80 85.73 55.91 50.29 81.05 30.28 73.84 gte-small 0.07 384 512 61.36 44.89 83.54 57.7 49.46 82.07 30.42 72.31 text-embedding-ada-002 - 1536 8192 60.99 45.9 84.89 56.32 49.25 80.97 30.8 70.93 e5-small-v2 0.13 384 512 59.93 39.92 84.67 54.32 49.04 80.39 31.16 72.94 sentence-t5-xxl 9.73 768 512 59.51 43.72 85.06 56.42 42.24 82.63 30.08 73.42 all-mpnet-base-v2 0.44 768 514 57.78 43.69 83.04 59.36 43.81 80.28 27.49 65.07 sgpt-bloom-7b1-msmarco 28.27 4096 2048 57.59 38.93 81.9 55.65 48.22 77.74 33.6 66.19 all-MiniLM-L12-v2 0.13 384 512 56.53 41.81 82.41 58.44 42.69 79.8 27.9 63.21 all-MiniLM-L6-v2 0.09 384 512 56.26 42.35 82.37 58.04 41.95 78.9 30.81 63.05 contriever-base-msmarco 0.44 768 512 56.00 41.1 82.54 53.14 41.88 76.51 30.36 66.68 sentence-t5-base 0.22 768 512 55.27 40.21 85.18 53.09 33.63 81.14 31.39 69.81 Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = BertEmbeddings.pretrained(&quot;gte_small&quot;, &quot;en&quot;) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained(&quot;gte_small&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) Model Information Model Name: gte_small Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [document, token] Output Labels: [embeddings] Language: en Size: 79.9 MB Case sensitive: true References GTE models are from Dingkun Long PREVIOUSGeneral Text Embeddings (GTE) English (gte_large) © John Snow Labs Inc. Terms of Service | Privacy Policy &lt;/html&gt;",
    "url": "/404.html",
    "relUrl": "/404.html"
  },
  "1": {
    "id": "1",
    "title": "GPU vs CPU benchmark",
    "content": "This section includes benchmarks for different Approach() (training classes), comparing their performance when running in m5.8xlarge CPU vs a Tesla V100 SXM2 GPU, as described in the Machine Specs section below. Different benchmarks, as well as their takeaways and some conclusions of how to get the best of GPU, are included as well, to guide you in the process of getting the best performance out of Spark NLP on GPU. Each major release comes with big improvements, so please, make sure you use at least that version to fully levearge Spark NLP capabilities on GPU. Machine specs CPU An AWS m5.8xlarge machine was used for the CPU benchmarking. This machine consists of 32 vCPUs and 128 GB of RAM, as you can check in the official specification webpage available here GPU A Tesla V100 SXM2 GPU with 32GB of memory was used to calculate the GPU benchmarking. Versions The benchmarking was carried out with the following Spark NLP versions: Spark version: 3.0.2 Hadoop version: 3.2.0 SparkNLP version: 3.3.4 Spark nodes: 1 Benchmark on classifierDLApproach() This experiment consisted of training a Deep Learning Binary Classifier (Question vs Statement classes) at sentence-level, using a fully connected CNN and Bert Sentence Embeddings. Only 1 Spark node was usd for the training. We used the Spark NLP class ClassifierDL and it’s method Approach() as described in the documentation. The pipeline looks as follows: Dataset The size of the dataset was relatively small (200K), consisting of: Training (rows): 162250 Test (rows): 40301 Training params Different batch sizes were tested to demonstrate how GPU performance improves with bigger batches compared to CPU, for a constant number of epochs and learning rate. Epochs: 10 Learning rate: 0.003 Batch sizes: 32, 64, 256, 1024 Results Even for this average-sized dataset, we can observe that GPU is able to beat the CPU machine by a 76% in both training and inference times. Training times depending on batch (in minutes) Batch size CPU GPU 32 66 16.1 64 65 15.3 256 64 14.5 1024 64 14 Inference times (in minutes) The average inference time remained more or less constant regardless the batch size: CPU: 8.7 min GPU: 2 min Performance metrics A weighted F1-score of 0.88 was achieved, with a 0.90 score for question detection and 0.83 for statements. Benchmark on NerDLApproach() This experiment consisted of training a Name Entity Recognition model (token-level), using our class NerDLApproach(), using Bert Word Embeddings and a Char-CNN-BiLSTM Neural Network. Only 1 Spark node was used for the training. We used the Spark NLP class NerDL and it’s method Approach() as described in the documentation. The pipeline looks as follows: Dataset The size of the dataset was small (17K), consisting of: Training (rows): 14041 Test (rows): 3250 Training params Different batch sizes were tested to demonstrate how GPU performance improves with bigger batches compared to CPU, for a constant number of epochs and learning rate. Epochs: 10 Learning rate: 0.003 Batch sizes: 32, 64, 256, 512, 1024, 2048 Results Even for this small dataset, we can observe that GPU is able to beat the CPU machine by a 62% in training time and a 68% in inference times. It’s important to mention that the batch size is very relevant when using GPU, since CPU scales much worse with bigger batch sizes than GPU. Training times depending on batch (in minutes) Batch size CPU GPU 32 9.5 10 64 8.1 6.5 256 6.9 3.5 512 6.7 3 1024 6.5 2.5 2048 6.5 2.5 Inference times (in minutes) Although CPU times in inference remain more or less constant regardless the batch sizes, GPU time experiment good improvements the bigger the batch size is. CPU times: ~29 min Batch size GPU 32 10 64 6.5 256 3.5 512 3 1024 2.5 2048 2.5 Performance metrics A macro F1-score of about 0.92 (0.90 in micro) was achieved, with the following charts extracted from the NERDLApproach() logs: Inference benchmark on BertSentenceEmbeddings() This experiment consisted of benchmarking the improvement obtained in inference by using GPU on BertSentenceEmbeddings(). We used the Spark NLP class BertSentenceEmbeddings() described in the Transformers documentation. The pipeline contains only two components and looks as follows: Dataset The size of the dataset was bigger than the previous ones, with 417735 rows for inference. Results We have observed in previous experiments, using BertSentenceEmbeddings (classifierDL) and also BertEmbeddings (NerDL) how GPU improved both training and inference times. In this case, we observe again big improvements in inference, what is already pointing that one of the main reasons of why GPU improves so much over CPU is the better management of Embeddings (word, sentence level) and bigger batch sizes. Batch sizes: 32, 64, 256, 1024 Inference times depending on batch (in minutes) Batch size CPU GPU 32 80 9.9 64 77 9.8 256 63 9.4 1024 62 9.1 Takeaways: How to get the best of the GPU You will experiment big GPU improvements in the following cases: Embeddings and Transformers are used in your pipeline. Take into consideration that GPU will performance very well in Embeddings / Transformer components, but other components of your pipeline may not leverage as well GPU capabilities; Bigger batch sizes get the best of GPU, while CPU does not scale with bigger batch sizes; Bigger dataset sizes get the best of GPU, while may be a bottleneck while running in CPU and lead to performance drops; MultiGPU training Right now, we don’t support multigpu training (1 model in different GPUs in parallel), but you can train different models in different GPU. Where to look for more information about Training Please, take a look at the Spark NLP and Spark NLP for Healthcare Training sections, and feel free to reach us out in case you want to maximize the performance on your GPU.",
    "url": "/docs/en/CPUvsGPUbenchmark",
    "relUrl": "/docs/en/CPUvsGPUbenchmark"
  },
  "2": {
    "id": "2",
    "title": "Analyze Non-English Text & Documents - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/analyze_non_english_text_documents",
    "relUrl": "/analyze_non_english_text_documents"
  },
  "3": {
    "id": "3",
    "title": "Analyze Spelling & Grammar - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/analyze_spelling_grammar",
    "relUrl": "/analyze_spelling_grammar"
  },
  "4": {
    "id": "4",
    "title": "Spark NLP - Annotators",
    "content": "How to read this section All annotators in Spark NLP share a common interface, this is: Annotation: Annotation(annotatorType, begin, end, result, meta-data, embeddings) AnnotatorType: some annotators share a type. This is not only figurative, but also tells about the structure of the metadata map in the Annotation. This is the one referred in the input and output of annotators. Inputs: Represents how many and which annotator types are expected in setInputCols(). These are column names of output of other annotators in the DataFrames. Output Represents the type of the output in the column setOutputCol(). There are two types of Annotators: Approach: AnnotatorApproach extend Estimators, which are meant to be trained through fit() Model: AnnotatorModel extend from Transformers, which are meant to transform DataFrames through transform() Model suffix is explicitly stated when the annotator is the result of a training process. Some annotators, such as Tokenizer are transformers, but do not contain the word Model since they are not trained annotators. Model annotators have a pretrained() on it’s static object, to retrieve the public pre-trained version of a model. pretrained(name, language, extra_location) -&gt; by default, pre-trained will bring a default model, sometimes we offer more than one model, in this case, you may have to use name, language or extra location to download them. Available Annotators Annotator Description Version BigTextMatcher Annotator to match exact phrases (by token) provided in a file against a Document. Opensource Chunk2Doc Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result. Opensource ChunkEmbeddings This annotator utilizes WordEmbeddings, BertEmbeddings etc. to generate chunk embeddings from either Chunker, NGramGenerator, or NerConverter outputs. Opensource ChunkTokenizer Tokenizes and flattens extracted NER chunks. Opensource Chunker This annotator matches a pattern of part-of-speech tags in order to return meaningful phrases from document. Opensource ClassifierDL ClassifierDL for generic Multi-class Text Classification. Opensource ContextSpellChecker Implements a deep-learning based Noisy Channel Model Spell Algorithm. Opensource DateMatcher Matches standard date formats into a provided format. Opensource DependencyParser Unlabeled parser that finds a grammatical relation between two words in a sentence. Opensource Doc2Chunk Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol. Opensource Doc2Vec Word2Vec model that creates vector representations of words in a text corpus. Opensource DocumentAssembler Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. Opensource DocumentNormalizer Annotator which normalizes raw text from tagged text, e.g. scraped web pages or xml documents, from document type columns into Sentence. Opensource EntityRuler Fits an Annotator to match exact strings or regex patterns provided in a file against a Document and assigns them an named entity. Opensource EmbeddingsFinisher Extracts embeddings from Annotations into a more easily usable form. Opensource Finisher Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. Opensource GraphExtraction Extracts a dependency graph between entities. Opensource GraphFinisher Helper class to convert the knowledge graph from GraphExtraction into a generic format, such as RDF. Opensource ImageAssembler Prepares images read by Spark into a format that is processable by Spark NLP. Opensource LanguageDetectorDL Language Identification and Detection by using CNN and RNN architectures in TensorFlow. Opensource Lemmatizer Finds lemmas out of words with the objective of returning a base dictionary word. Opensource MultiClassifierDL Multi-label Text Classification. Opensource MultiDateMatcher Matches standard date formats into a provided format. Opensource MultiDocumentAssembler Prepares data into a format that is processable by Spark NLP. Opensource NGramGenerator A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). Opensource NerConverter Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. Opensource NerCrf Extracts Named Entities based on a CRF Model. Opensource NerDL This Named Entity recognition annotator is a generic NER model based on Neural Networks. Opensource NerOverwriter Overwrites entities of specified strings. Opensource Normalizer Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary. Opensource NorvigSweeting Spellchecker Retrieves tokens and makes corrections automatically if not found in an English dictionary. Opensource POSTagger (Part of speech tagger) Averaged Perceptron model to tag words part-of-speech. Opensource RecursiveTokenizer Tokenizes raw text recursively based on a handful of definable rules. Opensource RegexMatcher Uses rules to match a set of regular expressions and associate them with a provided identifier. Opensource RegexTokenizer A tokenizer that splits text by a regex pattern. Opensource SentenceDetector Annotator that detects sentence boundaries using regular expressions. Opensource SentenceDetectorDL Detects sentence boundaries using a deep learning approach. Opensource SentenceEmbeddings Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols). Opensource SentimentDL Annotator for multi-class sentiment analysis. Opensource SentimentDetector Rule based sentiment detector, which calculates a score based on predefined keywords. Opensource Stemmer Returns hard-stems out of words with the objective of retrieving the meaningful part of the word. Opensource StopWordsCleaner This annotator takes a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences. Opensource SymmetricDelete Spellchecker Symmetric Delete spelling correction algorithm. Opensource TextMatcher Matches exact phrases (by token) provided in a file against a Document. Opensource Token2Chunk Converts TOKEN type Annotations to CHUNK type. Opensource TokenAssembler This transformer reconstructs a DOCUMENT type annotation from tokens, usually after these have been normalized, lemmatized, normalized, spell checked, etc, in order to use this document annotation in further annotators. Opensource Tokenizer Tokenizes raw text into word pieces, tokens. Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. Opensource TypedDependencyParser Labeled parser that finds a grammatical relation between two words in a sentence. Opensource ViveknSentiment Sentiment analyser inspired by the algorithm by Vivek Narayanan. Opensource WordEmbeddings Word Embeddings lookup annotator that maps tokens to vectors. Opensource Word2Vec Word2Vec model that creates vector representations of words in a text corpus. Opensource WordSegmenter Tokenizes non-english or non-whitespace separated texts. Opensource YakeKeywordExtraction Unsupervised, Corpus-Independent, Domain and Language-Independent and Single-Document keyword extraction. Opensource Available Transformers Additionally, these transformers are available. Transformer Description Version AlbertEmbeddings ALBERT: A Lite BERT for Self-supervised Learning of Language Representations Opensource AlbertForQuestionAnswering AlbertForQuestionAnswering can load ALBERT Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource AlbertForTokenClassification AlbertForTokenClassification can load ALBERT Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource AlbertForSequenceClassification AlbertForSequenceClassification can load ALBERT Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource BartTransformer BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension Transformer Opensource BertForQuestionAnswering BertForQuestionAnswering can load Bert Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource BertForSequenceClassification Bert Models with sequence classification/regression head on top. Opensource BertForTokenClassification BertForTokenClassification can load Bert Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource BertForZeroShotClassification BertForZeroShotClassification using a ModelForSequenceClassification trained on NLI (natural language inference) tasks. Opensource BertSentenceEmbeddings Sentence-level embeddings using BERT. BERT (Bidirectional Encoder Representations from Transformers) provides dense vector representations for natural language by using a deep, pre-trained neural network with the Transformer architecture. Opensource CamemBertEmbeddings CamemBert is based on Facebook’s RoBERTa model released in 2019. Opensource CamemBertForSequenceClassification amemBertForSequenceClassification can load CamemBERT Models with sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for multi-class document classification tasks. Opensource CamemBertForTokenClassification CamemBertForTokenClassification can load CamemBERT Models with a token classification head on top Opensource ConvNextForImageClassification ConvNextForImageClassification is an image classifier based on ConvNet models Opensource DeBertaEmbeddings DeBERTa builds on RoBERTa with disentangled attention and enhanced mask decoder training with half of the data used in RoBERTa. Opensource DeBertaForQuestionAnswering DeBertaForQuestionAnswering can load DeBERTa Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource DistilBertEmbeddings DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. Opensource DistilBertForQuestionAnswering DistilBertForQuestionAnswering can load DistilBert Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource DistilBertForSequenceClassification DistilBertForSequenceClassification can load DistilBERT Models with sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for multi-class document classification tasks. Opensource DistilBertForTokenClassification DistilBertForTokenClassification can load DistilBERT Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource DistilBertForZeroShotClassification DistilBertForZeroShotClassification using a ModelForSequenceClassification trained on NLI (natural language inference) tasks. Opensource ElmoEmbeddings Word embeddings from ELMo (Embeddings from Language Models), a language model trained on the 1 Billion Word Benchmark. Opensource GPT2Transformer GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages. Opensource HubertForCTC Hubert Model with a language modeling head on top for Connectionist Temporal Classification (CTC). Opensource LongformerEmbeddings Longformer is a BERT-like model started from the RoBERTa checkpoint and pretrained for MLM on long documents. Opensource LongformerForQuestionAnswering LongformerForQuestionAnswering can load Longformer Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource LongformerForSequenceClassification LongformerForSequenceClassification can load Longformer Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource LongformerForTokenClassification LongformerForTokenClassification can load Longformer Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource MarianTransformer Marian is an efficient, free Neural Machine Translation framework written in pure C++ with minimal dependencies. Opensource RoBertaEmbeddings RoBERTa: A Robustly Optimized BERT Pretraining Approach Opensource RoBertaForQuestionAnswering RoBertaForQuestionAnswering can load RoBERTa Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource RoBertaForSequenceClassification RoBertaForSequenceClassification can load RoBERTa Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource RoBertaForTokenClassification RoBertaForTokenClassification can load RoBERTa Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource RoBertaForZeroShotClassification RoBertaForZeroShotClassification using a ModelForSequenceClassification trained on NLI (natural language inference) tasks. Opensource RoBertaSentenceEmbeddings Sentence-level embeddings using RoBERTa. Opensource SpanBertCoref A coreference resolution model based on SpanBert. Opensource SwinForImageClassification SwinImageClassification is an image classifier based on Swin. Opensource T5Transformer T5 reconsiders all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Opensource TapasForQuestionAnswering TapasForQuestionAnswering is an implementation of TaPas - a BERT-based model specifically designed for answering questions about tabular data. Opensource UniversalSentenceEncoder The Universal Sentence Encoder encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks. Opensource ViTForImageClassification Vision Transformer (ViT) for image classification. Opensource Wav2Vec2ForCTC Wav2Vec2 Model with a language modeling head on top for Connectionist Temporal Classification (CTC). Opensource XlmRoBertaEmbeddings XlmRoBerta is a large multi-lingual language model, trained on 2.5TB of filtered CommonCrawl Opensource XlmRoBertaForQuestionAnswering XlmRoBertaForQuestionAnswering can load XLM-RoBERTa Models with a span classification head on top for extractive question-answering tasks like SQuAD. Opensource XlmRoBertaForSequenceClassification XlmRoBertaForSequenceClassification can load XLM-RoBERTa Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource XlmRoBertaForTokenClassification XlmRoBertaForTokenClassification can load XLM-RoBERTa Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource XlmRoBertaSentenceEmbeddings Sentence-level embeddings using XLM-RoBERTa. Opensource XlnetEmbeddings XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Opensource XlnetForTokenClassification XlnetForTokenClassification can load XLNet Models with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. Opensource XlnetForSequenceClassification XlnetForSequenceClassification can load XLNet Models with sequence classification/regression head on top e.g. for multi-class document classification tasks. Opensource ZeroShotNer ZeroShotNerModel implements zero shot named entity recognition by utilizing RoBERTa transformer models fine tuned on a question answering task. Opensource BigTextMatcher ApproachModel Annotator to match exact phrases (by token) provided in a file against a Document. A text file of predefined phrases must be provided with setStoragePath. In contrast to the normal TextMatcher, the BigTextMatcher is designed for large corpora. For extended examples of usage, see the BigTextMatcherTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: BigTextMatcher Scala API: BigTextMatcher Source: BigTextMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the entities file is of the form # # ... # dolore magna aliqua # lorem ipsum dolor. sit # laborum # ... # # where each line represents an entity phrase to be extracted. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) data = spark.createDataFrame([[&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;]]).toDF(&quot;text&quot;) entityExtractor = BigTextMatcher() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setStoragePath(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(False) pipeline = Pipeline().setStages([documentAssembler, tokenizer, entityExtractor]) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity)&quot;).show(truncate=False) +--+ |col | +--+ |[chunk, 6, 24, dolore magna aliqua, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 53, 59, laborum, [sentence -&gt; 0, chunk -&gt; 1], []] | +--+ // In this example, the entities file is of the form // // ... // dolore magna aliqua // lorem ipsum dolor. sit // laborum // ... // // where each line represents an entity phrase to be extracted. import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.BigTextMatcher import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val data = Seq(&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;).toDF(&quot;text&quot;) val entityExtractor = new BigTextMatcher() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setStoragePath(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(false) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer, entityExtractor)) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity)&quot;).show(false) +--+ |col | +--+ |[chunk, 6, 24, dolore magna aliqua, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 53, 59, laborum, [sentence -&gt; 0, chunk -&gt; 1], []] | +--+ Instantiated model of the BigTextMatcher. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: BigTextMatcherModel Scala API: BigTextMatcherModel Source: BigTextMatcherModel Chunk2Doc Converts a CHUNK type column back into DOCUMENT. Useful when trying to re-tokenize or do further analysis on a CHUNK result. Input Annotator Types: CHUNK Output Annotator Type: DOCUMENT Python API: Chunk2Doc Scala API: Chunk2Doc Source: Chunk2Doc Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline # Location entities are extracted and converted back into `DOCUMENT` type for further processing data = spark.createDataFrame([[1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;]]).toDF(&quot;id&quot;, &quot;text&quot;) # Extracts Named Entities amongst other things pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) chunkToDoc = Chunk2Doc().setInputCols(&quot;entities&quot;).setOutputCol(&quot;chunkConverted&quot;) explainResult = pipeline.transform(data) result = chunkToDoc.transform(explainResult) result.selectExpr(&quot;explode(chunkConverted)&quot;).show(truncate=False) ++ |col | ++ |[document, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []] | |[document, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]| ++ // Location entities are extracted and converted back into `DOCUMENT` type for further processing import spark.implicits._ import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.Chunk2Doc val data = Seq((1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;)).toDF(&quot;id&quot;, &quot;text&quot;) // Extracts Named Entities amongst other things val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) val chunkToDoc = new Chunk2Doc().setInputCols(&quot;entities&quot;).setOutputCol(&quot;chunkConverted&quot;) val explainResult = pipeline.transform(data) val result = chunkToDoc.transform(explainResult) result.selectExpr(&quot;explode(chunkConverted)&quot;).show(false) ++ |col | ++ |[document, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []] | |[document, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]| ++ ChunkEmbeddings This annotator utilizes WordEmbeddings, BertEmbeddings etc. to generate chunk embeddings from either Chunker, NGramGenerator, or NerConverter outputs. For extended examples of usage, see the Examples and the ChunkEmbeddingsTestSpec. Input Annotator Types: CHUNK, WORD_EMBEDDINGS Output Annotator Type: WORD_EMBEDDINGS Python API: ChunkEmbeddings Scala API: ChunkEmbeddings Source: ChunkEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Extract the Embeddings from the NGrams documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) nGrams = NGramGenerator() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;chunk&quot;) .setN(2) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) # Convert the NGram chunks into Word Embeddings chunkEmbeddings = ChunkEmbeddings() .setInputCols([&quot;chunk&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;chunk_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) pipeline = Pipeline() .setStages([ documentAssembler, sentence, tokenizer, nGrams, embeddings, chunkEmbeddings ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk_embeddings) as result&quot;) .select(&quot;result.annotatorType&quot;, &quot;result.result&quot;, &quot;result.embeddings&quot;) .show(5, 80) ++-+--+ | annotatorType| result| embeddings| ++-+--+ |word_embeddings| This is|[-0.55661, 0.42829502, 0.86661, -0.409785, 0.06316501, 0.120775, -0.0732005, ...| |word_embeddings| is a|[-0.40674996, 0.22938299, 0.50597, -0.288195, 0.555655, 0.465145, 0.140118, 0...| |word_embeddings|a sentence|[0.17417, 0.095253006, -0.0530925, -0.218465, 0.714395, 0.79860497, 0.0129999...| |word_embeddings|sentence .|[0.139705, 0.177955, 0.1887775, -0.45545, 0.20030999, 0.461557, -0.07891501, ...| ++-+--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.{NGramGenerator, Tokenizer} import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.embeddings.ChunkEmbeddings import org.apache.spark.ml.Pipeline // Extract the Embeddings from the NGrams val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val nGrams = new NGramGenerator() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;chunk&quot;) .setN(2) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) // Convert the NGram chunks into Word Embeddings val chunkEmbeddings = new ChunkEmbeddings() .setInputCols(&quot;chunk&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;chunk_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentence, tokenizer, nGrams, embeddings, chunkEmbeddings )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk_embeddings) as result&quot;) .select(&quot;result.annotatorType&quot;, &quot;result.result&quot;, &quot;result.embeddings&quot;) .show(5, 80) ++-+--+ | annotatorType| result| embeddings| ++-+--+ |word_embeddings| This is|[-0.55661, 0.42829502, 0.86661, -0.409785, 0.06316501, 0.120775, -0.0732005, ...| |word_embeddings| is a|[-0.40674996, 0.22938299, 0.50597, -0.288195, 0.555655, 0.465145, 0.140118, 0...| |word_embeddings|a sentence|[0.17417, 0.095253006, -0.0530925, -0.218465, 0.714395, 0.79860497, 0.0129999...| |word_embeddings|sentence .|[0.139705, 0.177955, 0.1887775, -0.45545, 0.20030999, 0.461557, -0.07891501, ...| ++-+--+ ChunkTokenizer ApproachModel Tokenizes and flattens extracted NER chunks. The ChunkTokenizer will split the extracted NER CHUNK type Annotations and will create TOKEN type Annotations. The result is then flattened, resulting in a single array. For extended examples of usage, see the ChunkTokenizerTestSpec. Input Annotator Types: CHUNK Output Annotator Type: TOKEN Python API: ChunkTokenizer Scala API: ChunkTokenizer Source: ChunkTokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) entityExtractor = TextMatcher() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setEntities(&quot;src/test/resources/entity-extractor/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) chunkTokenizer = ChunkTokenizer() .setInputCols([&quot;entity&quot;]) .setOutputCol(&quot;chunk_token&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, entityExtractor, chunkTokenizer ]) data = spark.createDataFrame([[ &quot;Hello world, my name is Michael, I am an artist and I work at Benezar&quot;, &quot;Robert, an engineer from Farendell, graduated last year. The other one, Lucas, graduated last week.&quot; ]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;entity.result as entity&quot; , &quot;chunk_token.result as chunk_token&quot;).show(truncate=False) +--++ |entity |chunk_token | +--++ |[world, Michael, work at Benezar] |[world, Michael, work, at, Benezar] | |[engineer from Farendell, last year, last week]|[engineer, from, Farendell, last, year, last, week]| +--++ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotators.{ChunkTokenizer, TextMatcher, Tokenizer} import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setEntities(&quot;src/test/resources/entity-extractor/test-chunks.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) val chunkTokenizer = new ChunkTokenizer() .setInputCols(&quot;entity&quot;) .setOutputCol(&quot;chunk_token&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, entityExtractor, chunkTokenizer )) val data = Seq( &quot;Hello world, my name is Michael, I am an artist and I work at Benezar&quot;, &quot;Robert, an engineer from Farendell, graduated last year. The other one, Lucas, graduated last week.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;entity.result as entity&quot; , &quot;chunk_token.result as chunk_token&quot;).show(false) +--++ |entity |chunk_token | +--++ |[world, Michael, work at Benezar] |[world, Michael, work, at, Benezar] | |[engineer from Farendell, last year, last week]|[engineer, from, Farendell, last, year, last, week]| +--++ Instantiated model of the ChunkTokenizer. For usage and examples see the documentation of the main class. Input Annotator Types: CHUNK Output Annotator Type: TOKEN Python API: ChunkTokenizerModel Scala API: ChunkTokenizerModel Source: ChunkTokenizerModel Chunker This annotator matches a pattern of part-of-speech tags in order to return meaningful phrases from document. Extracted part-of-speech tags are mapped onto the sentence, which can then be parsed by regular expressions. The part-of-speech tags are wrapped by angle brackets &lt;&gt; to be easily distinguishable in the text itself. This example sentence will result in the form: &quot;Peter Pipers employees are picking pecks of pickled peppers.&quot; &quot;&lt;NNP&gt;&lt;NNP&gt;&lt;NNS&gt;&lt;VBP&gt;&lt;VBG&gt;&lt;NNS&gt;&lt;IN&gt;&lt;JJ&gt;&lt;NNS&gt;&lt;.&gt;&quot; To then extract these tags, regexParsers need to be set with e.g.: val chunker = new Chunker() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;)) When defining the regular expressions, tags enclosed in angle brackets are treated as groups, so here specifically &quot;&lt;NNP&gt;+&quot; means 1 or more nouns in succession. Additional patterns can also be set with addRegexParsers. For more extended examples see the Examples) and the ChunkerTestSpec. Input Annotator Types: DOCUMENT, POS Output Annotator Type: CHUNK Python API: Chunker Scala API: Chunker Source: Chunker Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) POSTag = PerceptronModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) chunker = Chunker() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers([&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;]) pipeline = Pipeline() .setStages([ documentAssembler, sentence, tokenizer, POSTag, chunker ]) data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(truncate=False) +-+ |result | +-+ |[chunk, 0, 11, Peter Pipers, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 13, 21, employees, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 35, 39, pecks, [sentence -&gt; 0, chunk -&gt; 2], []] | |[chunk, 52, 58, peppers, [sentence -&gt; 0, chunk -&gt; 3], []] | +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotators.{Chunker, Tokenizer} import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val POSTag = PerceptronModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val chunker = new Chunker() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;) .setOutputCol(&quot;chunk&quot;) .setRegexParsers(Array(&quot;&lt;NNP&gt;+&quot;, &quot;&lt;NNS&gt;+&quot;)) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentence, tokenizer, POSTag, chunker )) val data = Seq(&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(false) +-+ |result | +-+ |[chunk, 0, 11, Peter Pipers, [sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 13, 21, employees, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 35, 39, pecks, [sentence -&gt; 0, chunk -&gt; 2], []] | |[chunk, 52, 58, peppers, [sentence -&gt; 0, chunk -&gt; 3], []] | +-+ ClassifierDL ApproachModel Trains a ClassifierDL for generic Multi-class Text Classification. ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 100 classes. For instantiated/pretrained models, see ClassifierDLModel. Setting a test dataset to monitor model metrics can be done with .setTestDataset. The method expects a path to a parquet file containing a dataframe that has the same required columns as the training dataframe. The pre-processing steps for the training dataframe should also be applied to the test dataframe. The following example will show how to create the test dataset: val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val preProcessingPipeline = new Pipeline().setStages(Array(documentAssembler, embeddings)) val Array(train, test) = data.randomSplit(Array(0.8, 0.2)) preProcessingPipeline .fit(test) .transform(test) .write .mode(&quot;overwrite&quot;) .parquet(&quot;test_data&quot;) val classifier = new ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setTestDataset(&quot;test_data&quot;) For extended examples of usage, see the Examples [1] [2] and the ClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Note: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. UniversalSentenceEncoder, BertSentenceEmbeddings, or SentenceEmbeddings can be used for the inputCol Python API: ClassifierDLApproach Scala API: ClassifierDLApproach Source: ClassifierDLApproach Show Example PythonScala # In this example, the training data `&quot;sentiment.csv&quot;` has the form of # # text,label # This movie is the best movie I have wached ever! In my opinion this movie can win an award.,0 # This was a terrible movie! The acting was bad really bad!,1 # ... # # Then traning can be done like so: import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline smallCorpus = spark.read.option(&quot;header&quot;,&quot;True&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) docClassifier = ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(5e-3) .setDropout(0.5) pipeline = Pipeline() .setStages( [ documentAssembler, useEmbeddings, docClassifier ] ) pipelineModel = pipeline.fit(smallCorpus) // In this example, the training data `&quot;sentiment.csv&quot;` has the form of // // text,label // This movie is the best movie I have wached ever! In my opinion this movie can win an award.,0 // This was a terrible movie! The acting was bad really bad!,1 // ... // // Then traning can be done like so: import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLApproach import org.apache.spark.ml.Pipeline val smallCorpus = spark.read.option(&quot;header&quot;,&quot;true&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val docClassifier = new ClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(64) .setMaxEpochs(20) .setLr(5e-3f) .setDropout(0.5f) val pipeline = new Pipeline() .setStages( Array( documentAssembler, useEmbeddings, docClassifier ) ) val pipelineModel = pipeline.fit(smallCorpus) ClassifierDL for generic Multi-class Text Classification. ClassifierDL uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. The ClassifierDL annotator uses a deep learning model (DNNs) we have built inside TensorFlow and supports up to 100 classes. This is the instantiated model of the ClassifierDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val classifierDL = ClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;classification&quot;) The default model is &quot;classifierdl_use_trec6&quot;, if no name is provided. It uses embeddings from the UniversalSentenceEncoder and is trained on the TREC-6 dataset. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the ClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: ClassifierDLModel Scala API: ClassifierDLModel Source: ClassifierDLModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) sarcasmDL = ClassifierDLModel.pretrained(&quot;classifierdl_use_sarcasm&quot;) .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sarcasm&quot;) pipeline = Pipeline() .setStages([ documentAssembler, sentence, useEmbeddings, sarcasmDL ]) data = spark.createDataFrame([ [&quot;I&#39;m ready!&quot;], [&quot;If I could put into words how much I love waking up at 6 am on Mondays I would.&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(arrays_zip(sentence, sarcasm)) as out&quot;) .selectExpr(&quot;out.sentence.result as sentence&quot;, &quot;out.sarcasm.result as sarcasm&quot;) .show(truncate=False) +-+-+ |sentence |sarcasm| +-+-+ |I&#39;m ready! |normal | |If I could put into words how much I love waking up at 6 am on Mondays I would.|sarcasm| +-+-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.classifier.dl.ClassifierDLModel import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val sarcasmDL = ClassifierDLModel.pretrained(&quot;classifierdl_use_sarcasm&quot;) .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sarcasm&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentence, useEmbeddings, sarcasmDL )) val data = Seq( &quot;I&#39;m ready!&quot;, &quot;If I could put into words how much I love waking up at 6 am on Mondays I would.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(arrays_zip(sentence, sarcasm)) as out&quot;) .selectExpr(&quot;out.sentence.result as sentence&quot;, &quot;out.sarcasm.result as sarcasm&quot;) .show(false) +-+-+ |sentence |sarcasm| +-+-+ |I&#39;m ready! |normal | |If I could put into words how much I love waking up at 6 am on Mondays I would.|sarcasm| +-+-+ ContextSpellChecker ApproachModel Trains a deep-learning based Noisy Channel Model Spell Algorithm. Correction candidates are extracted combining context information and word information. For instantiated/pretrained models, see ContextSpellCheckerModel. Spell Checking is a sequence to sequence mapping problem. Given an input sequence, potentially containing a certain number of errors, ContextSpellChecker will rank correction sequences according to three things: Different correction candidates for each word — word level. The surrounding text of each word, i.e. it’s context — sentence level. The relative cost of different correction candidates according to the edit operations at the character level it requires — subword level. For an in-depth explanation of the module see the article Applying Context Aware Spell Checking in Spark NLP. For extended examples of usage, see the article Training a Contextual Spell Checker for Italian Language, the Examples and the ContextSpellCheckerTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: ContextSpellCheckerApproach Scala API: ContextSpellCheckerApproach Source: ContextSpellCheckerApproach Show Example PythonScala # For this example, we use the first Sherlock Holmes book as the training dataset. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) spellChecker = ContextSpellCheckerApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;corrected&quot;) .setWordMaxDistance(3) .setBatchSize(24) .setEpochs(8) .setLanguageModelClasses(1650) # dependant on vocabulary size # .addVocabClass(&quot;_NAME_&quot;, names) # Extra classes for correction could be added like this pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker ]) path = &quot;sherlockholmes.txt&quot; dataset = spark.read.text(path) .toDF(&quot;text&quot;) pipelineModel = pipeline.fit(dataset) // For this example, we use the first Sherlock Holmes book as the training dataset. import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.spell.context.ContextSpellCheckerApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val spellChecker = new ContextSpellCheckerApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;corrected&quot;) .setWordMaxDistance(3) .setBatchSize(24) .setEpochs(8) .setLanguageModelClasses(1650) // dependant on vocabulary size // .addVocabClass(&quot;_NAME_&quot;, names) // Extra classes for correction could be added like this val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker )) val path = &quot;src/test/resources/spell/sherlockholmes.txt&quot; val dataset = spark.sparkContext.textFile(path) .toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(dataset) Implements a deep-learning based Noisy Channel Model Spell Algorithm. Correction candidates are extracted combining context information and word information. Spell Checking is a sequence to sequence mapping problem. Given an input sequence, potentially containing a certain number of errors, ContextSpellChecker will rank correction sequences according to three things: Different correction candidates for each word — word level. The surrounding text of each word, i.e. it’s context — sentence level. The relative cost of different correction candidates according to the edit operations at the character level it requires — subword level. For an in-depth explanation of the module see the article Applying Context Aware Spell Checking in Spark NLP. This is the instantiated model of the ContextSpellCheckerApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val spellChecker = ContextSpellCheckerModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;checked&quot;) The default model is &quot;spellcheck_dl&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the ContextSpellCheckerTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: ContextSpellCheckerModel Scala API: ContextSpellCheckerModel Source: ContextSpellCheckerModel DateMatcher Matches standard date formats into a provided format. Reads from different forms of date and time expressions and converts them to a provided date format. Extracts only one date per document. Use with sentence detector to find matches in each sentence. To extract multiple dates from a document, please use the MultiDateMatcher. Reads the following kind of dates: &quot;1978-01-28&quot;, &quot;1984/04/02,1/02/1980&quot;, &quot;2/28/79&quot;, &quot;The 31st of April in the year 2008&quot;, &quot;Fri, 21 Nov 1997&quot;, &quot;Jan 21, ‘97&quot;, &quot;Sun&quot;, &quot;Nov 21&quot;, &quot;jan 1st&quot;, &quot;next thursday&quot;, &quot;last wednesday&quot;, &quot;today&quot;, &quot;tomorrow&quot;, &quot;yesterday&quot;, &quot;next week&quot;, &quot;next month&quot;, &quot;next year&quot;, &quot;day after&quot;, &quot;the day before&quot;, &quot;0600h&quot;, &quot;06:00 hours&quot;, &quot;6pm&quot;, &quot;5:30 a.m.&quot;, &quot;at 5&quot;, &quot;12:59&quot;, &quot;23:59&quot;, &quot;1988/11/23 6pm&quot;, &quot;next week at 7.30&quot;, &quot;5 am tomorrow&quot; For example &quot;The 31st of April in the year 2008&quot; will be converted into 2008/04/31. Pretrained pipelines are available for this module, see Pipelines. For extended examples of usage, see the Examples and the DateMatcherTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DATE Python API: DateMatcher Scala API: DateMatcher Source: DateMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) date = DateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) pipeline = Pipeline().setStages([ documentAssembler, date ]) data = spark.createDataFrame([[&quot;Fri, 21 Nov 1997&quot;], [&quot;next week at 7.30&quot;], [&quot;see you a day after&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;date&quot;).show(truncate=False) +-+ |date | +-+ |[[date, 5, 15, 1997/11/21, [sentence -&gt; 0], []]] | |[[date, 0, 8, 2020/01/18, [sentence -&gt; 0], []]] | |[[date, 10, 18, 2020/01/12, [sentence -&gt; 0], []]]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.DateMatcher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val date = new DateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, date )) val data = Seq(&quot;Fri, 21 Nov 1997&quot;, &quot;next week at 7.30&quot;, &quot;see you a day after&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;date&quot;).show(false) +-+ |date | +-+ |[[date, 5, 15, 1997/11/21, [sentence -&gt; 0], []]] | |[[date, 0, 8, 2020/01/18, [sentence -&gt; 0], []]] | |[[date, 10, 18, 2020/01/12, [sentence -&gt; 0], []]]| +-+ DependencyParser ApproachModel Trains an unlabeled parser that finds a grammatical relations between two words in a sentence. For instantiated/pretrained models, see DependencyParserModel. Dependency parser provides information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The required training data can be set in two different ways (only one can be chosen for a particular model): Dependency treebank in the Penn Treebank format set with setDependencyTreeBank Dataset in the CoNLL-U format set with setConllU Apart from that, no additional training data is needed. See DependencyParserApproachTestSpec for further reference on how to use this API. Input Annotator Types: DOCUMENT, POS, TOKEN Output Annotator Type: DEPENDENCY Python API: DependencyParserApproach Scala API: DependencyParserApproach Source: DependencyParserApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) dependencyParserApproach = DependencyParserApproach() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) .setDependencyTreeBank(&quot;src/test/resources/parser/unlabeled/dependency_treebank&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, posTagger, dependencyParserApproach ]) # Additional training data is not needed, the dependency parser relies on the dependency tree bank / CoNLL-U only. emptyDataSet = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(emptyDataSet) import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val dependencyParserApproach = new DependencyParserApproach() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) .setDependencyTreeBank(&quot;src/test/resources/parser/unlabeled/dependency_treebank&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, posTagger, dependencyParserApproach )) // Additional training data is not needed, the dependency parser relies on the dependency tree bank / CoNLL-U only. val emptyDataSet = Seq.empty[String].toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(emptyDataSet) Unlabeled parser that finds a grammatical relation between two words in a sentence. Dependency parser provides information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. This is the instantiated model of the DependencyParserApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val dependencyParserApproach = DependencyParserModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) The default model is &quot;dependency_conllu&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the DependencyParserApproachTestSpec. Input Annotator Types: [String]DOCUMENT, POS, TOKEN Output Annotator Type: DEPENDENCY Python API: DependencyParserModel Scala API: DependencyParserModel Source: DependencyParserModel Doc2Chunk Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol. Chunk text must be contained within input DOCUMENT. May be either StringType or ArrayType[StringType] (using setIsArray). Useful for annotators that require a CHUNK type input. Input Annotator Types: DOCUMENT Output Annotator Type: CHUNK Python API: Doc2Chunk Scala API: Doc2Chunk Source: Doc2Chunk Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) chunkAssembler = Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) .setIsArray(True) data = spark.createDataFrame([[ &quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;, [&quot;Spark NLP&quot;, &quot;text processing library&quot;, &quot;natural language processing&quot;] ]]).toDF(&quot;text&quot;, &quot;target&quot;) pipeline = Pipeline().setStages([documentAssembler, chunkAssembler]).fit(data) result = pipeline.transform(data) result.selectExpr(&quot;chunk.result&quot;, &quot;chunk.annotatorType&quot;).show(truncate=False) +--++ |result |annotatorType | +--++ |[Spark NLP, text processing library, natural language processing]|[chunk, chunk, chunk]| +--++ import spark.implicits._ import com.johnsnowlabs.nlp.{Doc2Chunk, DocumentAssembler} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val chunkAssembler = new Doc2Chunk() .setInputCols(&quot;document&quot;) .setChunkCol(&quot;target&quot;) .setOutputCol(&quot;chunk&quot;) .setIsArray(true) val data = Seq( (&quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;, Seq(&quot;Spark NLP&quot;, &quot;text processing library&quot;, &quot;natural language processing&quot;)) ).toDF(&quot;text&quot;, &quot;target&quot;) val pipeline = new Pipeline().setStages(Array(documentAssembler, chunkAssembler)).fit(data) val result = pipeline.transform(data) result.selectExpr(&quot;chunk.result&quot;, &quot;chunk.annotatorType&quot;).show(false) +--++ |result |annotatorType | +--++ |[Spark NLP, text processing library, natural language processing]|[chunk, chunk, chunk]| +--++ Doc2Vec ApproachModel Trains a Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. For instantiated/pretrained models, see Doc2VecModel. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: SENTENCE_EMBEDDINGS Python API: Doc2VecApproach Scala API: Doc2VecApproach Source: Doc2VecApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Doc2VecApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings ]) path = &quot;sherlockholmes.txt&quot; dataset = spark.read.text(path).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(dataset) import spark.implicits._ import com.johnsnowlabs.nlp.annotator.{Tokenizer, Doc2VecApproach} import com.johnsnowlabs.nlp.base.DocumentAssembler import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = new Doc2VecApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings )) val path = &quot;src/test/resources/spell/sherlockholmes.txt&quot; val dataset = spark.sparkContext.textFile(path) .toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(dataset) Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. This is the instantiated model of the Doc2VecApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val embeddings = Doc2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;doc2vec_gigaword_300&quot;, if no name is provided. For available pretrained models please see the Models Hub. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: SENTENCE_EMBEDDINGS Python API: Doc2VecModel Scala API: Doc2VecModel Source: Doc2VecModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Doc2VecModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, embeddings, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Tokenizer, Doc2VecModel} import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = Doc2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsFinisher )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ DocumentAssembler Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. The DocumentAssembler reads String columns. Additionally, setCleanupMode can be used to pre-process the text (Default: disabled). For possible options please refer the parameters section. For more extended examples on document pre-processing see the Examples. Input Annotator Types: NONE Output Annotator Type: DOCUMENT Python API: DocumentAssembler Scala API: DocumentAssembler Source: DocumentAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library.&quot;]]).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) result = documentAssembler.transform(data) result.select(&quot;document&quot;).show(truncate=False) +-+ |document | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document&quot;).printSchema root |-- document: array (nullable = True) | |-- element: struct (containsNull = True) | | |-- annotatorType: string (nullable = True) | | |-- begin: integer (nullable = False) | | |-- end: integer (nullable = False) | | |-- result: string (nullable = True) | | |-- metadata: map (nullable = True) | | | |-- key: string | | | |-- value: string (valueContainsNull = True) | | |-- embeddings: array (nullable = True) | | | |-- element: float (containsNull = False) import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler val data = Seq(&quot;Spark NLP is an open-source text processing library.&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val result = documentAssembler.transform(data) result.select(&quot;document&quot;).show(false) +-+ |document | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document&quot;).printSchema root |-- document: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- begin: integer (nullable = false) | | |-- end: integer (nullable = false) | | |-- result: string (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | | |-- embeddings: array (nullable = true) | | | |-- element: float (containsNull = false) DocumentNormalizer Annotator which normalizes raw text from tagged text, e.g. scraped web pages or xml documents, from document type columns into Sentence. Removes all dirty characters from text following one or more input regex patterns. Can apply not wanted character removal with a specific policy. Can apply lower case normalization. For extended examples of usage, see the Examples. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: DocumentNormalizer Scala API: DocumentNormalizer Source: DocumentNormalizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) cleanUpPatterns = [&quot;&lt;[^&gt;]&gt;&quot;] documentNormalizer = DocumentNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;normalizedDocument&quot;) .setAction(&quot;clean&quot;) .setPatterns(cleanUpPatterns) .setReplacement(&quot; &quot;) .setPolicy(&quot;pretty_all&quot;) .setLowercase(True) pipeline = Pipeline().setStages([ documentAssembler, documentNormalizer ]) text = &quot;&quot;&quot; &lt;div id=&quot;theworldsgreatest&quot; class=&#39;my-right my-hide-small my-wide toptext&#39; style=&quot;font-family:&#39;Segoe UI&#39;,Arial,sans-serif&quot;&gt; THE WORLD&#39;S LARGEST WEB DEVELOPER SITE &lt;h1 style=&quot;font-size:300%;&quot;&gt;THE WORLD&#39;S LARGEST WEB DEVELOPER SITE&lt;/h1&gt; &lt;p style=&quot;font-size:160%;&quot;&gt;Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&quot;&quot;&quot; data = spark.createDataFrame([[text]]).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(data) result = pipelineModel.transform(data) result.selectExpr(&quot;normalizedDocument.result&quot;).show(truncate=False) +--+ |result | +--+ |[ the world&#39;s largest web developer site the world&#39;s largest web developer site lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum..]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.DocumentNormalizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val cleanUpPatterns = Array(&quot;&lt;[^&gt;]&gt;&quot;) val documentNormalizer = new DocumentNormalizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;normalizedDocument&quot;) .setAction(&quot;clean&quot;) .setPatterns(cleanUpPatterns) .setReplacement(&quot; &quot;) .setPolicy(&quot;pretty_all&quot;) .setLowercase(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, documentNormalizer )) val text = &quot;&quot;&quot; &lt;div id=&quot;theworldsgreatest&quot; class=&#39;my-right my-hide-small my-wide toptext&#39; style=&quot;font-family:&#39;Segoe UI&#39;,Arial,sans-serif&quot;&gt; THE WORLD&#39;S LARGEST WEB DEVELOPER SITE &lt;h1 style=&quot;font-size:300%;&quot;&gt;THE WORLD&#39;S LARGEST WEB DEVELOPER SITE&lt;/h1&gt; &lt;p style=&quot;font-size:160%;&quot;&gt;Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum..&lt;/p&gt; &lt;/div&gt; &lt;/div&gt;&quot;&quot;&quot; val data = Seq(text).toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(data) val result = pipelineModel.transform(data) result.selectExpr(&quot;normalizedDocument.result&quot;).show(truncate=false) +--+ |result | +--+ |[ the world&#39;s largest web developer site the world&#39;s largest web developer site lorem ipsum is simply dummy text of the printing and typesetting industry. lorem ipsum has been the industry&#39;s standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. it has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. it was popularised in the 1960s with the release of letraset sheets containing lorem ipsum passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ipsum..]| +--+ EmbeddingsFinisher Extracts embeddings from Annotations into a more easily usable form. This is useful for example: WordEmbeddings, BertEmbeddings, SentenceEmbeddings and ChunkEmbeddings. By using EmbeddingsFinisher you can easily transform your embeddings into array of floats or vectors which are compatible with Spark ML functions such as LDA, K-mean, Random Forest classifier or any other functions that require featureCol. For more extended examples see the Examples. Input Annotator Types: EMBEDDINGS Output Annotator Type: NONE Python API: EmbeddingsFinisher Scala API: EmbeddingsFinisher Source: EmbeddingsFinisher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) stopwordsCleaner = StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) gloveEmbeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;cleanTokens&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) embeddingsFinisher = EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_sentence_embeddings&quot;) .setOutputAsVector(True) .setCleanAnnotations(False) data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library.&quot;]]) .toDF(&quot;text&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, normalizer, stopwordsCleaner, gloveEmbeddings, embeddingsFinisher ]).fit(data) result = pipeline.transform(data) resultWithSize = result.selectExpr(&quot;explode(finished_sentence_embeddings) as embeddings&quot;) resultWithSize.show(5, 80) +--+ | embeddings| +--+ |[0.1619900017976761,0.045552998781204224,-0.03229299932718277,-0.685609996318...| |[-0.42416998744010925,1.1378999948501587,-0.5717899799346924,-0.5078899860382...| |[0.08621499687433243,-0.15772999823093414,-0.06067200005054474,0.395359992980...| |[-0.4970499873161316,0.7164199948310852,0.40119001269340515,-0.05761000141501...| |[-0.08170200139284134,0.7159299850463867,-0.20677000284194946,0.0295659992843...| +--+ import spark.implicits._ import org.apache.spark.ml.Pipeline import com.johnsnowlabs.nlp.{DocumentAssembler, EmbeddingsFinisher} import com.johnsnowlabs.nlp.annotator.{Normalizer, StopWordsCleaner, Tokenizer, WordEmbeddingsModel} val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) val stopwordsCleaner = new StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val gloveEmbeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;cleanTokens&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_sentence_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) val data = Seq(&quot;Spark NLP is an open-source text processing library.&quot;) .toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, normalizer, stopwordsCleaner, gloveEmbeddings, embeddingsFinisher )).fit(data) val result = pipeline.transform(data) val resultWithSize = result.selectExpr(&quot;explode(finished_sentence_embeddings)&quot;) .map { row =&gt; val vector = row.getAs[org.apache.spark.ml.linalg.DenseVector](0) (vector.size, vector) }.toDF(&quot;size&quot;, &quot;vector&quot;) resultWithSize.show(5, 80) +-+--+ |size| vector| +-+--+ | 100|[0.1619900017976761,0.045552998781204224,-0.03229299932718277,-0.685609996318...| | 100|[-0.42416998744010925,1.1378999948501587,-0.5717899799346924,-0.5078899860382...| | 100|[0.08621499687433243,-0.15772999823093414,-0.06067200005054474,0.395359992980...| | 100|[-0.4970499873161316,0.7164199948310852,0.40119001269340515,-0.05761000141501...| | 100|[-0.08170200139284134,0.7159299850463867,-0.20677000284194946,0.0295659992843...| +-+--+ EntityRuler ApproachModel Fits an Annotator to match exact strings or regex patterns provided in a file against a Document and assigns them an named entity. The definitions can contain any number of named entities. There are multiple ways and formats to set the extraction resource. It is possible to set it either as a “JSON”, “JSONL” or “CSV” file. A path to the file needs to be provided to setPatternsResource. The file format needs to be set as the “format” field in the option parameter map and depending on the file type, additional parameters might need to be set. To enable regex extraction, setEnablePatternRegex(true) needs to be called. If the file is in a JSON format, then the rule definitions need to be given in a list with the fields “id”, “label” and “patterns”: [ { &quot;id&quot;: &quot;person-regex&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot; w+ s w+&quot;, &quot; w+- w+&quot;] }, { &quot;id&quot;: &quot;locations-words&quot;, &quot;label&quot;: &quot;LOCATION&quot;, &quot;patterns&quot;: [&quot;Winterfell&quot;] } ] The same fields also apply to a file in the JSONL format: {&quot;id&quot;: &quot;names-with-j&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Jon&quot;, &quot;John&quot;, &quot;John Snow&quot;]} {&quot;id&quot;: &quot;names-with-s&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Stark&quot;, &quot;Snow&quot;]} {&quot;id&quot;: &quot;names-with-e&quot;, &quot;label&quot;: &quot;PERSON&quot;, &quot;patterns&quot;: [&quot;Eddard&quot;, &quot;Eddard Stark&quot;]} In order to use a CSV file, an additional parameter “delimiter” needs to be set. In this case, the delimiter might be set by using .setPatternsResource(&quot;patterns.csv&quot;, ReadAs.TEXT, Map(&quot;format&quot;-&gt;&quot;csv&quot;, &quot;delimiter&quot; -&gt; &quot; |&quot;)) PERSON|Jon PERSON|John PERSON|John Snow LOCATION|Winterfell Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: EntityRulerApproach Scala API: EntityRulerApproach Source: EntityRulerApproach Show Example PythonScala # In this example, the entities file as the form of # # PERSON|Jon # PERSON|John # PERSON|John Snow # LOCATION|Winterfell # # where each line represents an entity and the associated string delimited by &quot;|&quot;. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.common import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) entityRuler = EntityRulerApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;entities&quot;) .setPatternsResource( &quot;patterns.csv&quot;, ReadAs.TEXT, {&quot;format&quot;: &quot;csv&quot;, &quot;delimiter&quot;: &quot; |&quot;} ) .setEnablePatternRegex(True) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, entityRuler ]) data = spark.createDataFrame([[&quot;Jon Snow wants to be lord of Winterfell.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(entities)&quot;).show(truncate=False) +--+ |col | +--+ |[chunk, 0, 2, Jon, [entity -&gt; PERSON, sentence -&gt; 0], []] | |[chunk, 29, 38, Winterfell, [entity -&gt; LOCATION, sentence -&gt; 0], []]| +--+ // In this example, the entities file as the form of // // PERSON|Jon // PERSON|John // PERSON|John Snow // LOCATION|Winterfell // // where each line represents an entity and the associated string delimited by &quot;|&quot;. import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.er.EntityRulerApproach import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val entityRuler = new EntityRulerApproach() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;entities&quot;) .setPatternsResource( &quot;src/test/resources/entity-ruler/patterns.csv&quot;, ReadAs.TEXT, {&quot;format&quot;: &quot;csv&quot;, &quot;delimiter&quot;: &quot;|&quot;)} ) .setEnablePatternRegex(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, entityRuler )) val data = Seq(&quot;Jon Snow wants to be lord of Winterfell.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(entities)&quot;).show(false) +--+ |col | +--+ |[chunk, 0, 2, Jon, [entity -&gt; PERSON, sentence -&gt; 0], []] | |[chunk, 29, 38, Winterfell, [entity -&gt; LOCATION, sentence -&gt; 0], []]| +--+ Instantiated model of the EntityRulerApproach. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: EntityRulerModel Scala API: EntityRulerModel Source: EntityRulerModel Finisher Converts annotation results into a format that easier to use. It is useful to extract the results from Spark NLP Pipelines. The Finisher outputs annotation(s) values into String. For more extended examples on document pre-processing see the Examples. Input Annotator Types: ANY Output Annotator Type: NONE Python API: Finisher Scala API: Finisher Source: Finisher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline data = spark.createDataFrame([[1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;]]).toDF(&quot;id&quot;, &quot;text&quot;) # Extracts Named Entities amongst other things pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) finisher = Finisher().setInputCols(&quot;entities&quot;).setOutputCols(&quot;output&quot;) explainResult = pipeline.transform(data) explainResult.selectExpr(&quot;explode(entities)&quot;).show(truncate=False) ++ |entities | ++ |[[chunk, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []], [chunk, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]]| ++ result = finisher.transform(explainResult) result.select(&quot;output&quot;).show(truncate=False) +-+ |output | +-+ |[New York, New Jersey]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.Finisher val data = Seq((1, &quot;New York and New Jersey aren&#39;t that far apart actually.&quot;)).toDF(&quot;id&quot;, &quot;text&quot;) // Extracts Named Entities amongst other things val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;) val finisher = new Finisher().setInputCols(&quot;entities&quot;).setOutputCols(&quot;output&quot;) val explainResult = pipeline.transform(data) explainResult.selectExpr(&quot;explode(entities)&quot;).show(false) ++ |entities | ++ |[[chunk, 0, 7, New York, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 0], []], [chunk, 13, 22, New Jersey, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 1], []]]| ++ val result = finisher.transform(explainResult) result.select(&quot;output&quot;).show(false) +-+ |output | +-+ |[New York, New Jersey]| +-+ GraphExtraction Extracts a dependency graph between entities. The GraphExtraction class takes e.g. extracted entities from a NerDLModel and creates a dependency tree which describes how the entities relate to each other. For that a triple store format is used. Nodes represent the entities and the edges represent the relations between those entities. The graph can then be used to find relevant relationships between words. Both the DependencyParserModel and TypedDependencyParserModel need to be present in the pipeline. There are two ways to set them: Both Annotators are present in the pipeline already. The dependencies are taken implicitly from these two Annotators. Setting setMergeEntities to true will download the default pretrained models for those two Annotators automatically. The specific models can also be set with setDependencyParserModel and setTypedDependencyParserModel: val graph_extraction = new GraphExtraction() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;graph&quot;) .setRelationshipTypes(Array(&quot;prefer-LOC&quot;)) .setMergeEntities(true) //.setDependencyParserModel(Array(&quot;dependency_conllu&quot;, &quot;en&quot;, &quot;public/models&quot;)) //.setTypedDependencyParserModel(Array(&quot;dependency_typed_conllu&quot;, &quot;en&quot;, &quot;public/models&quot;)) To transform the resulting graph into a more generic form such as RDF, see the GraphFinisher. Input Annotator Types: DOCUMENT, TOKEN, NAMED_ENTITY Output Annotator Type: NODE Python API: GraphExtraction Scala API: GraphExtraction Source: GraphExtraction Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) nerTagger = NerDLModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) posTagger = PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) dependencyParser = DependencyParserModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency&quot;) typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols([&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency_type&quot;) graph_extraction = GraphExtraction() .setInputCols([&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;graph&quot;) .setRelationshipTypes([&quot;prefer-LOC&quot;]) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, embeddings, nerTagger, posTagger, dependencyParser, typedDependencyParser, graph_extraction ]) data = spark.createDataFrame([[&quot;You and John prefer the morning flight through Denver&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;graph&quot;).show(truncate=False) +--+ |graph | +--+ |13, 18, prefer, [relationship -&gt; prefer,LOC, path1 -&gt; prefer,nsubj,morning,flat,flight,flat,Denver], []| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel import com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserModel import org.apache.spark.ml.Pipeline import com.johnsnowlabs.nlp.annotators.GraphExtraction val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val nerTagger = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val dependencyParser = DependencyParserModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) val typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols(&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency_type&quot;) val graph_extraction = new GraphExtraction() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;graph&quot;) .setRelationshipTypes(Array(&quot;prefer-LOC&quot;)) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger, posTagger, dependencyParser, typedDependencyParser, graph_extraction )) val data = Seq(&quot;You and John prefer the morning flight through Denver&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;graph&quot;).show(false) +--+ |graph | +--+ |[[node, 13, 18, prefer, [relationship -&gt; prefer,LOC, path1 -&gt; prefer,nsubj,morning,flat,flight,flat,Denver], []]]| +--+ GraphFinisher Helper class to convert the knowledge graph from GraphExtraction into a generic format, such as RDF. Input Annotator Types: NONE Output Annotator Type: NONE Python API: GraphFinisher Scala API: GraphFinisher Source: GraphFinisher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # This is a continuation of the example of # GraphExtraction. To see how the graph is extracted, see the # documentation of that class. graphFinisher = GraphFinisher() .setInputCol(&quot;graph&quot;) .setOutputCol(&quot;graph_finished&quot;) .setOutputAs[False] finishedResult = graphFinisher.transform(result) finishedResult.select(&quot;text&quot;, &quot;graph_finished&quot;).show(truncate=False) +--+--+ |text |graph_finished | +--+--+ |You and John prefer the morning flight through Denver|(morning,flat,flight), (flight,flat,Denver)| +--+--+ // This is a continuation of the example of // [[com.johnsnowlabs.nlp.annotators.GraphExtraction GraphExtraction]]. To see how the graph is extracted, see the // documentation of that class. import com.johnsnowlabs.nlp.GraphFinisher val graphFinisher = new GraphFinisher() .setInputCol(&quot;graph&quot;) .setOutputCol(&quot;graph_finished&quot;) .setOutputAsArray(false) val finishedResult = graphFinisher.transform(result) finishedResult.select(&quot;text&quot;, &quot;graph_finished&quot;).show(false) +--+--+ |text |graph_finished | +--+--+ |You and John prefer the morning flight through Denver|[[(prefer,nsubj,morning), (morning,flat,flight), (flight,flat,Denver)]]| +--+--+ ImageAssembler Prepares images read by Spark into a format that is processable by Spark NLP. This component is needed to process images. Input Annotator Types: NONE Output Annotator Type: IMAGE Python API: ImageAssembler Scala API: ImageAssembler Source: ImageAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from pyspark.ml import Pipeline data = spark.read.format(&quot;image&quot;).load(&quot;./tmp/images/&quot;).toDF(&quot;image&quot;) imageAssembler = ImageAssembler().setInputCol(&quot;image&quot;).setOutputCol(&quot;image_assembler&quot;) result = imageAssembler.transform(data) result.select(&quot;image_assembler&quot;).show() result.select(&quot;image_assembler&quot;).printSchema() root |-- image_assembler: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- origin: string (nullable = true) | | |-- height: integer (nullable = true) | | |-- width: integer (nullable = true) | | |-- nChannels: integer (nullable = true) | | |-- mode: integer (nullable = true) | | |-- result: binary (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) import com.johnsnowlabs.nlp.ImageAssembler import org.apache.spark.ml.Pipeline val imageDF: DataFrame = spark.read .format(&quot;image&quot;) .option(&quot;dropInvalid&quot;, value = true) .load(&quot;src/test/resources/image/&quot;) val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) val pipeline = new Pipeline().setStages(Array(imageAssembler)) val pipelineDF = pipeline.fit(imageDF).transform(imageDF) pipelineDF.printSchema() root |-- image_assembler: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- origin: string (nullable = true) | | |-- height: integer (nullable = false) | | |-- width: integer (nullable = false) | | |-- nChannels: integer (nullable = false) | | |-- mode: integer (nullable = false) | | |-- result: binary (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) LanguageDetectorDL Language Identification and Detection by using CNN and RNN architectures in TensorFlow. LanguageDetectorDL is an annotator that detects the language of documents or sentences depending on the inputCols. The models are trained on large datasets such as Wikipedia and Tatoeba. Depending on the language (how similar the characters are), the LanguageDetectorDL works best with text longer than 140 characters. The output is a language code in Wiki Code style. Pretrained models can be loaded with pretrained of the companion object: Val languageDetector = LanguageDetectorDL.pretrained() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;language&quot;) The default model is &quot;ld_wiki_tatoeba_cnn_21&quot;, default language is &quot;xx&quot; (meaning multi-lingual), if no values are provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples And the LanguageDetectorDLTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: LANGUAGE Python API: LanguageDetectorDL Scala API: LanguageDetectorDL Source: LanguageDetectorDL Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) languageDetector = LanguageDetectorDL.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;language&quot;) pipeline = Pipeline() .setStages([ documentAssembler, languageDetector ]) data = spark.createDataFrame([ [&quot;Spark NLP is an open-source text processing library for advanced natural language processing for the Python, Java and Scala programming languages.&quot;], [&quot;Spark NLP est une bibliothèque de traitement de texte open source pour le traitement avancé du langage naturel pour les langages de programmation Python, Java et Scala.&quot;], [&quot;Spark NLP ist eine Open-Source-Textverarbeitungsbibliothek für fortgeschrittene natürliche Sprachverarbeitung für die Programmiersprachen Python, Java und Scala.&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;language.result&quot;).show(truncate=False) ++ |result| ++ |[en] | |[fr] | |[de] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.ld.dl.LanguageDetectorDL import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val languageDetector = LanguageDetectorDL.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;language&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, languageDetector )) val data = Seq( &quot;Spark NLP is an open-source text processing library for advanced natural language processing for the Python, Java and Scala programming languages.&quot;, &quot;Spark NLP est une bibliothèque de traitement de texte open source pour le traitement avancé du langage naturel pour les langages de programmation Python, Java et Scala.&quot;, &quot;Spark NLP ist eine Open-Source-Textverarbeitungsbibliothek für fortgeschrittene natürliche Sprachverarbeitung für die Programmiersprachen Python, Java und Scala.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;language.result&quot;).show(false) ++ |result| ++ |[en] | |[fr] | |[de] | ++ Lemmatizer ApproachModel Class to find lemmas out of words with the objective of returning a base dictionary word. Retrieves the significant part of a word. A dictionary of predefined lemmas must be provided with setDictionary. The dictionary can be set as a delimited text file. Pretrained models can be loaded with LemmatizerModel.pretrained. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: Lemmatizer Scala API: Lemmatizer Source: Lemmatizer Show Example PythonScala # In this example, the lemma dictionary `lemmas_small.txt` has the form of # # ... # pick -&gt; pick picks picking picked # peck -&gt; peck pecking pecked pecks # pickle -&gt; pickle pickles pickled pickling # pepper -&gt; pepper peppers peppered peppering # ... # # where each key is delimited by `-&gt;` and values are delimited by ` t` import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = Lemmatizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) pipeline = Pipeline() .setStages([ documentAssembler, sentenceDetector, tokenizer, lemmatizer ]) data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;lemma.result&quot;).show(truncate=False) ++ |result | ++ |[Peter, Pipers, employees, are, pick, peck, of, pickle, pepper, .]| ++ // In this example, the lemma dictionary `lemmas_small.txt` has the form of // // ... // pick -&gt; pick picks picking picked // peck -&gt; peck pecking pecked pecks // pickle -&gt; pickle pickles pickled pickling // pepper -&gt; pepper peppers peppered peppering // ... // // where each key is delimited by `-&gt;` and values are delimited by ` t` import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.Lemmatizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val lemmatizer = new Lemmatizer() .setInputCols(Array(&quot;token&quot;)) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, sentenceDetector, tokenizer, lemmatizer )) val data = Seq(&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;lemma.result&quot;).show(false) ++ |result | ++ |[Peter, Pipers, employees, are, pick, peck, of, pickle, pepper, .]| ++ Instantiated Model of the Lemmatizer. For usage and examples, please see the documentation of that class. For available pretrained models please see the Models Hub. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: LemmatizerModel Scala API: LemmatizerModel Source: LemmatizerModel MultiClassifierDL ApproachModel Trains a MultiClassifierDL for Multi-label Text Classification. MultiClassifierDL uses a Bidirectional GRU with a convolutional model that we have built inside TensorFlow and supports up to 100 classes. For instantiated/pretrained models, see MultiClassifierDLModel. The input to MultiClassifierDL are Sentence Embeddings such as the state-of-the-art UniversalSentenceEncoder, BertSentenceEmbeddings or SentenceEmbeddings. In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to. Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y). Setting a test dataset to monitor model metrics can be done with .setTestDataset. The method expects a path to a parquet file containing a dataframe that has the same required columns as the training dataframe. The pre-processing steps for the training dataframe should also be applied to the test dataframe. The following example will show how to create the test dataset: val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val preProcessingPipeline = new Pipeline().setStages(Array(documentAssembler, embeddings)) val Array(train, test) = data.randomSplit(Array(0.8, 0.2)) preProcessingPipeline .fit(test) .transform(test) .write .mode(&quot;overwrite&quot;) .parquet(&quot;test_data&quot;) val multiClassifier = new MultiClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;label&quot;) .setTestDataset(&quot;test_data&quot;) For extended examples of usage, see the Examples and the MultiClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Note: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. UniversalSentenceEncoder, BertSentenceEmbeddings, SentenceEmbeddings or other sentence based embeddings can be used for the inputCol Python API: MultiClassifierDLApproach Scala API: MultiClassifierDLApproach Source: MultiClassifierDLApproach Show Example PythonScala # In this example, the training data has the form # # +-+--+--+ # | id| text| labels| # +-+--+--+ # |ed58abb40640f983|PN NewsYou mean ... | [toxic]| # |a1237f726b5f5d89|Dude. Place the ...| [obscene, insult]| # |24b0d6c8733c2abe|Thanks - thanks ...| [insult]| # |8c4478fb239bcfc0|&quot; Gee, 5 minutes ...|[toxic, obscene, ...| # +-+--+--+ import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # Process training data to create text with associated array of labels trainDataset.printSchema() # root # |-- id: string (nullable = true) # |-- text: string (nullable = true) # |-- labels: array (nullable = true) # | |-- element: string (containsNull = true) # Then create pipeline for training documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) .setCleanupMode(&quot;shrink&quot;) embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;embeddings&quot;) docClassifier = MultiClassifierDLApproach() .setInputCols(&quot;embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;labels&quot;) .setBatchSize(128) .setMaxEpochs(10) .setLr(1e-3) .setThreshold(0.5) .setValidationSplit(0.1) pipeline = Pipeline() .setStages( [ documentAssembler, embeddings, docClassifier ] ) pipelineModel = pipeline.fit(trainDataset) // In this example, the training data has the form (Note: labels can be arbitrary) // // mr,ref // &quot;name[Alimentum], area[city centre], familyFriendly[no], near[Burger King]&quot;,Alimentum is an adult establish found in the city centre area near Burger King. // &quot;name[Alimentum], area[city centre], familyFriendly[yes]&quot;,Alimentum is a family-friendly place in the city centre. // ... // // It needs some pre-processing first, so the labels are of type `Array[String]`. This can be done like so: import spark.implicits._ import com.johnsnowlabs.nlp.annotators.classifier.dl.MultiClassifierDLApproach import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import org.apache.spark.ml.Pipeline import org.apache.spark.sql.functions.{col, udf} // Process training data to create text with associated array of labels def splitAndTrim = udf { labels: String =&gt; labels.split(&quot;, &quot;).map(x=&gt;x.trim) } val smallCorpus = spark.read .option(&quot;header&quot;, true) .option(&quot;inferSchema&quot;, true) .option(&quot;mode&quot;, &quot;DROPMALFORMED&quot;) .csv(&quot;src/test/resources/classifier/e2e.csv&quot;) .withColumn(&quot;labels&quot;, splitAndTrim(col(&quot;mr&quot;))) .withColumn(&quot;text&quot;, col(&quot;ref&quot;)) .drop(&quot;mr&quot;) smallCorpus.printSchema() // root // |-- ref: string (nullable = true) // |-- labels: array (nullable = true) // | |-- element: string (containsNull = true) // Then create pipeline for training val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) .setCleanupMode(&quot;shrink&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;embeddings&quot;) val docClassifier = new MultiClassifierDLApproach() .setInputCols(&quot;embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;labels&quot;) .setBatchSize(128) .setMaxEpochs(10) .setLr(1e-3f) .setThreshold(0.5f) .setValidationSplit(0.1f) val pipeline = new Pipeline() .setStages( Array( documentAssembler, embeddings, docClassifier ) ) val pipelineModel = pipeline.fit(smallCorpus) MultiClassifierDL for Multi-label Text Classification. MultiClassifierDL Bidirectional GRU with Convolution model we have built inside TensorFlow and supports up to 100 classes. The input to MultiClassifierDL are Sentence Embeddings such as state-of-the-art UniversalSentenceEncoder, BertSentenceEmbeddings or SentenceEmbeddings. This is the instantiated model of the MultiClassifierDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val multiClassifier = MultiClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;categories&quot;) The default model is &quot;multiclassifierdl_use_toxic&quot;, if no name is provided. It uses embeddings from the UniversalSentenceEncoder and classifies toxic comments. The data is based on the Jigsaw Toxic Comment Classification Challenge. For available pretrained models please see the Models Hub. In machine learning, multi-label classification and the strongly related problem of multi-output classification are variants of the classification problem where multiple labels may be assigned to each instance. Multi-label classification is a generalization of multiclass classification, which is the single-label problem of categorizing instances into precisely one of more than two classes; in the multi-label problem there is no constraint on how many of the classes the instance can be assigned to. Formally, multi-label classification is the problem of finding a model that maps inputs x to binary vectors y (assigning a value of 0 or 1 for each element (label) in y). For extended examples of usage, see the Examples and the MultiClassifierDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: MultiClassifierDLModel Scala API: MultiClassifierDLModel Source: MultiClassifierDLModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) multiClassifierDl = MultiClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;classifications&quot;) pipeline = Pipeline() .setStages([ documentAssembler, useEmbeddings, multiClassifierDl ]) data = spark.createDataFrame([ [&quot;This is pretty good stuff!&quot;], [&quot;Wtf kind of crap is this&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;text&quot;, &quot;classifications.result&quot;).show(truncate=False) +--+-+ |text |result | +--+-+ |This is pretty good stuff!|[] | |Wtf kind of crap is this |[toxic, obscene]| +--+-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.classifier.dl.MultiClassifierDLModel import com.johnsnowlabs.nlp.embeddings.UniversalSentenceEncoder import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val multiClassifierDl = MultiClassifierDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;classifications&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, useEmbeddings, multiClassifierDl )) val data = Seq( &quot;This is pretty good stuff!&quot;, &quot;Wtf kind of crap is this&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;text&quot;, &quot;classifications.result&quot;).show(false) +--+-+ |text |result | +--+-+ |This is pretty good stuff!|[] | |Wtf kind of crap is this |[toxic, obscene]| +--+-+ MultiDateMatcher Matches standard date formats into a provided format. Reads the following kind of dates: &quot;1978-01-28&quot;, &quot;1984/04/02,1/02/1980&quot;, &quot;2/28/79&quot;, &quot;The 31st of April in the year 2008&quot;, &quot;Fri, 21 Nov 1997&quot;, &quot;Jan 21, ‘97&quot;, &quot;Sun&quot;, &quot;Nov 21&quot;, &quot;jan 1st&quot;, &quot;next thursday&quot;, &quot;last wednesday&quot;, &quot;today&quot;, &quot;tomorrow&quot;, &quot;yesterday&quot;, &quot;next week&quot;, &quot;next month&quot;, &quot;next year&quot;, &quot;day after&quot;, &quot;the day before&quot;, &quot;0600h&quot;, &quot;06:00 hours&quot;, &quot;6pm&quot;, &quot;5:30 a.m.&quot;, &quot;at 5&quot;, &quot;12:59&quot;, &quot;23:59&quot;, &quot;1988/11/23 6pm&quot;, &quot;next week at 7.30&quot;, &quot;5 am tomorrow&quot; For example &quot;The 31st of April in the year 2008&quot; will be converted into 2008/04/31. For extended examples of usage, see the Examples and the MultiDateMatcherTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DATE Python API: MultiDateMatcher Scala API: MultiDateMatcher Source: MultiDateMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) date = MultiDateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) pipeline = Pipeline().setStages([ documentAssembler, date ]) data = spark.createDataFrame([[&quot;I saw him yesterday and he told me that he will visit us next week&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(date) as dates&quot;).show(truncate=False) +--+ |dates | +--+ |[date, 57, 65, 2020/01/18, [sentence -&gt; 0], []]| |[date, 10, 18, 2020/01/10, [sentence -&gt; 0], []]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.MultiDateMatcher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val date = new MultiDateMatcher() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;date&quot;) .setAnchorDateYear(2020) .setAnchorDateMonth(1) .setAnchorDateDay(11) .setDateFormat(&quot;yyyy/MM/dd&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, date )) val data = Seq(&quot;I saw him yesterday and he told me that he will visit us next week&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(date) as dates&quot;).show(false) +--+ |dates | +--+ |[date, 57, 65, 2020/01/18, [sentence -&gt; 0], []]| |[date, 10, 18, 2020/01/10, [sentence -&gt; 0], []]| +--+ MultiDocumentAssembler Prepares data into a format that is processable by Spark NLP. This is the entry point for every Spark NLP pipeline. The MultiDocumentAssembler can read either a String column or an Array[String]. Additionally, MultiDocumentAssembler.setCleanupMode can be used to pre-process the text (Default: disabled). For possible options please refer the parameters section. For more extended examples on document pre-processing see the Examples. Input Annotator Types: NONE Output Annotator Type: DOCUMENT Python API: MultiDocumentAssembler Scala API: MultiDocumentAssembler Source: MultiDocumentAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from pyspark.ml import Pipeline data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library.&quot;], [&quot;Spark NLP is a state-of-the-art Natural Language Processing library built on top of Apache Spark&quot;]]).toDF(&quot;text&quot;, &quot;text2&quot;) documentAssembler = MultiDocumentAssembler().setInputCols([&quot;text&quot;, &quot;text2&quot;]).setOutputCols([&quot;document1&quot;, &quot;document2&quot;]) result = documentAssembler.transform(data) result.select(&quot;document1&quot;).show(truncate=False) +-+ |document1 | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document1&quot;).printSchema() root |-- document: array (nullable = True) | |-- element: struct (containsNull = True) | | |-- annotatorType: string (nullable = True) | | |-- begin: integer (nullable = False) | | |-- end: integer (nullable = False) | | |-- result: string (nullable = True) | | |-- metadata: map (nullable = True) | | | |-- key: string | | | |-- value: string (valueContainsNull = True) | | |-- embeddings: array (nullable = True) | | | |-- element: float (containsNull = False) import spark.implicits._ import com.johnsnowlabs.nlp.MultiDocumentAssembler val data = Seq(&quot;Spark NLP is an open-source text processing library.&quot;).toDF(&quot;text&quot;) val multiDocumentAssembler = new MultiDocumentAssembler().setInputCols(&quot;text&quot;).setOutputCols(&quot;document&quot;) val result = multiDocumentAssembler.transform(data) result.select(&quot;document&quot;).show(false) +-+ |document | +-+ |[[document, 0, 51, Spark NLP is an open-source text processing library., [sentence -&gt; 0], []]]| +-+ result.select(&quot;document&quot;).printSchema root |-- document: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- annotatorType: string (nullable = true) | | |-- begin: integer (nullable = false) | | |-- end: integer (nullable = false) | | |-- result: string (nullable = true) | | |-- metadata: map (nullable = true) | | | |-- key: string | | | |-- value: string (valueContainsNull = true) | | |-- embeddings: array (nullable = true) | | | |-- element: float (containsNull = false) NGramGenerator A feature transformer that converts the input array of strings (annotatorType TOKEN) into an array of n-grams (annotatorType CHUNK). Null values in the input array are ignored. It returns an array of n-grams where each n-gram is represented by a space-separated string of words. When the input is empty, an empty array is returned. When the input array length is less than n (number of elements per n-gram), no n-grams are returned. For more extended examples see the Examples and the NGramGeneratorTestSpec. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: NGramGenerator Scala API: NGramGenerator Source: NGramGenerator Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) nGrams = NGramGenerator() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;ngrams&quot;) .setN(2) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, nGrams ]) data = spark.createDataFrame([[&quot;This is my sentence.&quot;]]).toDF(&quot;text&quot;) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(ngrams) as result&quot;).show(truncate=False) ++ |result | ++ |[chunk, 0, 6, This is, [sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 5, 9, is my, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 8, 18, my sentence, [sentence -&gt; 0, chunk -&gt; 2], []]| |[chunk, 11, 19, sentence ., [sentence -&gt; 0, chunk -&gt; 3], []]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.NGramGenerator import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val nGrams = new NGramGenerator() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;ngrams&quot;) .setN(2) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, nGrams )) val data = Seq(&quot;This is my sentence.&quot;).toDF(&quot;text&quot;) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(ngrams) as result&quot;).show(false) ++ |result | ++ |[chunk, 0, 6, This is, [sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 5, 9, is my, [sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 8, 18, my sentence, [sentence -&gt; 0, chunk -&gt; 2], []]| |[chunk, 11, 19, sentence ., [sentence -&gt; 0, chunk -&gt; 3], []]| ++ NerConverter Converts a IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their label. Results in CHUNK Annotation type. NER chunks can then be filtered by setting a whitelist with setWhiteList. Chunks with no associated entity (tagged “O”) are filtered. See also Inside–outside–beginning (tagging) for more information. Input Annotator Types: DOCUMENT, TOKEN, NAMED_ENTITY Output Annotator Type: CHUNK Python API: NerConverter Scala API: NerConverter Source: NerConverter Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # This is a continuation of the example of the NerDLModel. See that class # on how to extract the entities. # The output of the NerDLModel follows the Annotator schema and can be converted like so: # # result.selectExpr(&quot;explode(ner)&quot;).show(truncate=False) # +-+ # |col | # +-+ # |[named_entity, 0, 2, B-ORG, [word -&gt; U.N], []] | # |[named_entity, 3, 3, O, [word -&gt; .], []] | # |[named_entity, 5, 12, O, [word -&gt; official], []] | # |[named_entity, 14, 18, B-PER, [word -&gt; Ekeus], []] | # |[named_entity, 20, 24, O, [word -&gt; heads], []] | # |[named_entity, 26, 28, O, [word -&gt; for], []] | # |[named_entity, 30, 36, B-LOC, [word -&gt; Baghdad], []]| # |[named_entity, 37, 37, O, [word -&gt; .], []] | # +-+ # # After the converter is used: converter = NerConverter() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;]) .setOutputCol(&quot;entities&quot;) converter.transform(result).selectExpr(&quot;explode(entities)&quot;).show(truncate=False) ++ |col | ++ |[chunk, 0, 2, U.N, [entity -&gt; ORG, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 14, 18, Ekeus, [entity -&gt; PER, sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 30, 36, Baghdad, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 2], []]| ++ // This is a continuation of the example of the [[com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel NerDLModel]]. See that class // on how to extract the entities. // The output of the NerDLModel follows the Annotator schema and can be converted like so: // // result.selectExpr(&quot;explode(ner)&quot;).show(false) // +-+ // |col | // +-+ // |[named_entity, 0, 2, B-ORG, [word -&gt; U.N], []] | // |[named_entity, 3, 3, O, [word -&gt; .], []] | // |[named_entity, 5, 12, O, [word -&gt; official], []] | // |[named_entity, 14, 18, B-PER, [word -&gt; Ekeus], []] | // |[named_entity, 20, 24, O, [word -&gt; heads], []] | // |[named_entity, 26, 28, O, [word -&gt; for], []] | // |[named_entity, 30, 36, B-LOC, [word -&gt; Baghdad], []]| // |[named_entity, 37, 37, O, [word -&gt; .], []] | // +-+ // // After the converter is used: val converter = new NerConverter() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) .setPreservePosition(false) converter.transform(result).selectExpr(&quot;explode(entities)&quot;).show(false) ++ |col | ++ |[chunk, 0, 2, U.N, [entity -&gt; ORG, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 14, 18, Ekeus, [entity -&gt; PER, sentence -&gt; 0, chunk -&gt; 1], []] | |[chunk, 30, 36, Baghdad, [entity -&gt; LOC, sentence -&gt; 0, chunk -&gt; 2], []]| ++ NerCrf ApproachModel Algorithm for training a Named Entity Recognition Model For instantiated/pretrained models, see NerCrfModel. This Named Entity recognition annotator allows for a generic model to be trained by utilizing a CRF machine learning algorithm. The training data should be a labeled Spark Dataset, e.g. CoNLL 2003 IOB with Annotation type columns. The data should have columns of type DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS and an additional label column of annotator type NAMED_ENTITY. Excluding the label, this can be done with for example a SentenceDetector, a Tokenizer and a PerceptronModel and a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). Optionally the user can provide an entity dictionary file with setExternalFeatures for better accuracy. For extended examples of usage, see the Examples and the NerCrfApproachTestSpec. Input Annotator Types: DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerCrfApproach Scala API: NerCrfApproach Source: NerCrfApproach Show Example PythonScala # This CoNLL dataset already includes the sentence, token, pos and label column with their respective annotator types. # If a custom dataset is used, these need to be defined. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(False) nerTagger = NerCrfApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setMinEpochs(1) .setMaxEpochs(3) .setC0(34) .setL2(3.0) .setOutputCol(&quot;ner&quot;) pipeline = Pipeline().setStages([ documentAssembler, embeddings, nerTagger ]) conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) // This CoNLL dataset already includes the sentence, token, pos and label column with their respective annotator types. // If a custom dataset is used, these need to be defined. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotator.NerCrfApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) .setCaseSensitive(false) val nerTagger = new NerCrfApproach() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;pos&quot;, &quot;embeddings&quot;) .setLabelColumn(&quot;label&quot;) .setMinEpochs(1) .setMaxEpochs(3) .setC0(34) .setL2(3.0) .setOutputCol(&quot;ner&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, embeddings, nerTagger )) val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) Extracts Named Entities based on a CRF Model. This Named Entity recognition annotator allows for a generic model to be trained by utilizing a CRF machine learning algorithm. The data should have columns of type DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS. These can be extracted with for example a SentenceDetector, a Tokenizer and a PerceptronModel This is the instantiated model of the NerCrfApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val nerTagger = NerCrfModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;word_embeddings&quot;, &quot;pos&quot;) .setOutputCol(&quot;ner&quot; The default model is &quot;ner_crf&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples. Input Annotator Types: DOCUMENT, TOKEN, POS, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerCrfModel Scala API: NerCrfModel Source: NerCrfModel NerDL ApproachModel This Named Entity recognition annotator allows to train generic NER model based on Neural Networks. The architecture of the neural network is a Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. For instantiated/pretrained models, see NerDLModel. The training data should be a labeled Spark Dataset, in the format of CoNLL 2003 IOB with Annotation type columns. The data should have columns of type DOCUMENT, TOKEN, WORD_EMBEDDINGS and an additional label column of annotator type NAMED_ENTITY. Excluding the label, this can be done with for example a SentenceDetector, a Tokenizer and a PerceptronModel and a WordEmbeddingsModel (any word embeddings can be chosen, e.g. BertEmbeddings for BERT based embeddings). Setting a test dataset to monitor model metrics can be done with .setTestDataset. The method expects a path to a parquet file containing a dataframe that has the same required columns as the training dataframe. The pre-processing steps for the training dataframe should also be applied to the test dataframe. The following example will show how to create the test dataset with a CoNLL dataset: val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = WordEmbeddingsModel .pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val preProcessingPipeline = new Pipeline().setStages(Array(documentAssembler, embeddings)) val conll = CoNLL() val Array(train, test) = conll .readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) .randomSplit(Array(0.8, 0.2)) preProcessingPipeline .fit(test) .transform(test) .write .mode(&quot;overwrite&quot;) .parquet(&quot;test_data&quot;) val nerTagger = new NerDLApproach() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setTestDataset(&quot;test_data&quot;) For extended examples of usage, see the Examples and the NerDLSpec. Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerDLApproach Scala API: NerDLApproach Source: NerDLApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline # First extract the prerequisites for the NerDLApproach documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = BertEmbeddings.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) # Then the training can start nerTagger = NerDLApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(1) .setRandomSeed(0) .setVerbose(0) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, embeddings, nerTagger ]) # We use the text and labels from the CoNLL dataset conll = CoNLL() trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) pipelineModel = pipeline.fit(trainingData) import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.BertEmbeddings import com.johnsnowlabs.nlp.annotators.ner.dl.NerDLApproach import com.johnsnowlabs.nlp.training.CoNLL import org.apache.spark.ml.Pipeline // First extract the prerequisites for the NerDLApproach val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) // Then the training can start val nerTagger = new NerDLApproach() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(1) .setRandomSeed(0) .setVerbose(0) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) // We use the text and labels from the CoNLL dataset val conll = CoNLL() val trainingData = conll.readDataset(spark, &quot;src/test/resources/conll2003/eng.train&quot;) val pipelineModel = pipeline.fit(trainingData) This Named Entity recognition annotator is a generic NER model based on Neural Networks. Neural Network architecture is Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets. This is the instantiated model of the NerDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val nerModel = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) The default model is &quot;ner_dl&quot;, if no name is provided. For available pretrained models please see the Models Hub. Additionally, pretrained pipelines are available for this module, see Pipelines. Note that some pretrained models require specific types of embeddings, depending on which they were trained on. For example, the default model &quot;ner_dl&quot; requires the WordEmbeddings &quot;glove_100d&quot;. For extended examples of usage, see the Examples and the NerDLSpec. Input Annotator Types: DOCUMENT, TOKEN, WORD_EMBEDDINGS Output Annotator Type: NAMED_ENTITY Python API: NerDLModel Scala API: NerDLModel Source: NerDLModel NerOverwriter Overwrites entities of specified strings. The input for this Annotator have to be entities that are already extracted, Annotator type NAMED_ENTITY. The strings specified with setStopWords will have new entities assigned to, specified with setNewResult. Input Annotator Types: NAMED_ENTITY Output Annotator Type: NAMED_ENTITY Python API: NerOverwriter Scala API: NerOverwriter Source: NerOverwriter Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # First extract the prerequisite Entities documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;bert&quot;) nerTagger = NerDLModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;bert&quot;]) .setOutputCol(&quot;ner&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, embeddings, nerTagger ]) data = spark.createDataFrame([[&quot;Spark NLP Crosses Five Million Downloads, John Snow Labs Announces.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(ner)&quot;).show(truncate=False) # ++ # |col | # ++ # |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | # |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | # |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | # |[named_entity, 18, 21, O, [word -&gt; Five], []] | # |[named_entity, 23, 29, O, [word -&gt; Million], []] | # |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | # |[named_entity, 40, 40, O, [word -&gt; ,], []] | # |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | # |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | # |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | # |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []]| # |[named_entity, 66, 66, O, [word -&gt; .], []] | # ++ # The recognized entities can then be overwritten nerOverwriter = NerOverwriter() .setInputCols([&quot;ner&quot;]) .setOutputCol(&quot;ner_overwritten&quot;) .setStopWords([&quot;Million&quot;]) .setNewResult(&quot;B-CARDINAL&quot;) nerOverwriter.transform(result).selectExpr(&quot;explode(ner_overwritten)&quot;).show(truncate=False) ++ |col | ++ |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | |[named_entity, 18, 21, O, [word -&gt; Five], []] | |[named_entity, 23, 29, B-CARDINAL, [word -&gt; Million], []]| |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | |[named_entity, 40, 40, O, [word -&gt; ,], []] | |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []] | |[named_entity, 66, 66, O, [word -&gt; .], []] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel import com.johnsnowlabs.nlp.annotators.ner.NerOverwriter import org.apache.spark.ml.Pipeline // First extract the prerequisite Entities val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;bert&quot;) val nerTagger = NerDLModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;, &quot;bert&quot;) .setOutputCol(&quot;ner&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, embeddings, nerTagger )) val data = Seq(&quot;Spark NLP Crosses Five Million Downloads, John Snow Labs Announces.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(ner)&quot;).show(false) / ++ |col | ++ |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | |[named_entity, 18, 21, O, [word -&gt; Five], []] | |[named_entity, 23, 29, O, [word -&gt; Million], []] | |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | |[named_entity, 40, 40, O, [word -&gt; ,], []] | |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []]| |[named_entity, 66, 66, O, [word -&gt; .], []] | ++ / // The recognized entities can then be overwritten val nerOverwriter = new NerOverwriter() .setInputCols(&quot;ner&quot;) .setOutputCol(&quot;ner_overwritten&quot;) .setStopWords(Array(&quot;Million&quot;)) .setNewResult(&quot;B-CARDINAL&quot;) nerOverwriter.transform(result).selectExpr(&quot;explode(ner_overwritten)&quot;).show(false) ++ |col | ++ |[named_entity, 0, 4, B-ORG, [word -&gt; Spark], []] | |[named_entity, 6, 8, I-ORG, [word -&gt; NLP], []] | |[named_entity, 10, 16, O, [word -&gt; Crosses], []] | |[named_entity, 18, 21, O, [word -&gt; Five], []] | |[named_entity, 23, 29, B-CARDINAL, [word -&gt; Million], []]| |[named_entity, 31, 39, O, [word -&gt; Downloads], []] | |[named_entity, 40, 40, O, [word -&gt; ,], []] | |[named_entity, 42, 45, B-ORG, [word -&gt; John], []] | |[named_entity, 47, 50, I-ORG, [word -&gt; Snow], []] | |[named_entity, 52, 55, I-ORG, [word -&gt; Labs], []] | |[named_entity, 57, 65, I-ORG, [word -&gt; Announces], []] | |[named_entity, 66, 66, O, [word -&gt; .], []] | ++ Normalizer ApproachModel Annotator that cleans out tokens. Requires stems, hence tokens. Removes all dirty characters from text following a regex pattern and transforms words based on a provided dictionary For extended examples of usage, see the Examples. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: Normalizer Scala API: Normalizer Source: Normalizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) .setLowercase(True) .setCleanupPatterns([&quot;&quot;&quot;[^ w d s]&quot;&quot;&quot;]) # remove punctuations (keep alphanumeric chars) # if we don&#39;t set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z]) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, normalizer ]) data = spark.createDataFrame([[&quot;John and Peter are brothers. However they don&#39;t support each other that much.&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;normalized.result&quot;).show(truncate = False) +-+ |result | +-+ |[john, and, peter, are, brothers, however, they, dont, support, each, other, that, much]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Normalizer, Tokenizer} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) .setLowercase(true) .setCleanupPatterns(Array(&quot;&quot;&quot;[^ w d s]&quot;&quot;&quot;)) // remove punctuations (keep alphanumeric chars) // if we don&#39;t set CleanupPatterns, it will only keep alphabet letters ([^A-Za-z]) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, normalizer )) val data = Seq(&quot;John and Peter are brothers. However they don&#39;t support each other that much.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;normalized.result&quot;).show(truncate = false) +-+ |result | +-+ |[john, and, peter, are, brothers, however, they, dont, support, each, other, that, much]| +-+ Instantiated Model of the Normalizer. For usage and examples, please see the documentation of that class. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: NormalizerModel Scala API: NormalizerModel Source: NormalizerModel NorvigSweeting Spellchecker ApproachModel Trains annotator, that retrieves tokens and makes corrections automatically if not found in an English dictionary. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster (than the standard approach with deletes + transposes + replaces + inserts) and language independent. A dictionary of correct spellings must be provided with setDictionary as a text file, where each word is parsed by a regex pattern. Inspired by Norvig model and SymSpell. For instantiated/pretrained models, see NorvigSweetingModel. For extended examples of usage, see the NorvigSweetingTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: NorvigSweetingApproach Scala API: NorvigSweetingApproach Source: NorvigSweetingApproach Show Example PythonScala # In this example, the dictionary `&quot;words.txt&quot;` has the form of # # ... # gummy # gummic # gummier # gummiest # gummiferous # ... # # This dictionary is then set to be the basis of the spell checker. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) spellChecker = NorvigSweetingApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker ]) pipelineModel = pipeline.fit(trainingData) // In this example, the dictionary `&quot;words.txt&quot;` has the form of // // ... // gummy // gummic // gummier // gummiest // gummiferous // ... // // This dictionary is then set to be the basis of the spell checker. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.spell.norvig.NorvigSweetingApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val spellChecker = new NorvigSweetingApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker )) val pipelineModel = pipeline.fit(trainingData) This annotator retrieves tokens and makes corrections automatically if not found in an English dictionary. Inspired by Norvig model and SymSpell. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster (than the standard approach with deletes + transposes + replaces + inserts) and language independent. This is the instantiated model of the NorvigSweetingApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val spellChecker = NorvigSweetingModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) .setDoubleVariants(true) The default model is &quot;spellcheck_norvig&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of see the NorvigSweetingTestSpec. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: NorvigSweetingModel Scala API: NorvigSweetingModel Source: NorvigSweetingModel POSTagger (Part of speech tagger) ApproachModel Trains an averaged Perceptron model to tag words part-of-speech. Sets a POS tag to each word within a sentence. For pretrained models please see the PerceptronModel. The training data needs to be in a Spark DataFrame, where the column needs to consist of Annotations of type POS. The Annotation needs to have member result set to the POS tag and have a &quot;word&quot; mapping to its word inside of member metadata. This DataFrame for training can easily created by the helper class POS. POS().readDataset(spark, datasetPath).selectExpr(&quot;explode(tags) as tags&quot;).show(false) ++ |tags | ++ |[pos, 0, 5, NNP, [word -&gt; Pierre], []] | |[pos, 7, 12, NNP, [word -&gt; Vinken], []] | |[pos, 14, 14, ,, [word -&gt; ,], []] | |[pos, 31, 34, MD, [word -&gt; will], []] | |[pos, 36, 39, VB, [word -&gt; join], []] | |[pos, 41, 43, DT, [word -&gt; the], []] | |[pos, 45, 49, NN, [word -&gt; board], []] | ... For extended examples of usage, see the Examples and PerceptronApproach tests. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: POS Python API: PerceptronApproach Scala API: PerceptronApproach Source: PerceptronApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) datasetPath = &quot;src/test/resources/anc-pos-corpus-small/test-training.txt&quot; trainingPerceptronDF = POS().readDataset(spark, datasetPath) trainedPos = PerceptronApproach() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) .setPosColumn(&quot;tags&quot;) .fit(trainingPerceptronDF) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, trainedPos ]) data = spark.createDataFrame([[&quot;To be or not to be, is this the question?&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;pos.result&quot;).show(truncate=False) +--+ |result | +--+ |[NNP, NNP, CD, JJ, NNP, NNP, ,, MD, VB, DT, CD, .]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.training.POS import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val datasetPath = &quot;src/test/resources/anc-pos-corpus-small/test-training.txt&quot; val trainingPerceptronDF = POS().readDataset(spark, datasetPath) val trainedPos = new PerceptronApproach() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) .setPosColumn(&quot;tags&quot;) .fit(trainingPerceptronDF) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, trainedPos )) val data = Seq(&quot;To be or not to be, is this the question?&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;pos.result&quot;).show(false) +--+ |result | +--+ |[NNP, NNP, CD, JJ, NNP, NNP, ,, MD, VB, DT, CD, .]| +--+ Averaged Perceptron model to tag words part-of-speech. Sets a POS tag to each word within a sentence. This is the instantiated model of the PerceptronApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) The default model is &quot;pos_anc&quot;, if no name is provided. For available pretrained models please see the Models Hub. Additionally, pretrained pipelines are available for this module, see Pipelines. For extended examples of usage, see the Examples. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: POS Python API: PerceptronModel Scala API: PerceptronModel Source: PerceptronModel RecursiveTokenizer ApproachModel Tokenizes raw text recursively based on a handful of definable rules. Unlike the Tokenizer, the RecursiveTokenizer operates based on these array string parameters only: prefixes: Strings that will be split when found at the beginning of token. suffixes: Strings that will be split when found at the end of token. infixes: Strings that will be split when found at the middle of token. whitelist: Whitelist of strings not to split For extended examples of usage, see the Examples and the TokenizerTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: RecursiveTokenizer Scala API: RecursiveTokenizer Source: RecursiveTokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = RecursiveTokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer ]) data = spark.createDataFrame([[&quot;One, after the Other, (and) again. PO, QAM,&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.select(&quot;token.result&quot;).show(truncate=False) ++ |result | ++ |[One, ,, after, the, Other, ,, (, and, ), again, ., PO, ,, QAM, ,]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.RecursiveTokenizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new RecursiveTokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer )) val data = Seq(&quot;One, after the Other, (and) again. PO, QAM,&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.select(&quot;token.result&quot;).show(false) ++ |result | ++ |[One, ,, after, the, Other, ,, (, and, ), again, ., PO, ,, QAM, ,]| ++ Instantiated model of the RecursiveTokenizer. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: RecursiveTokenizerModel Scala API: RecursiveTokenizerModel Source: RecursiveTokenizerModel RegexMatcher ApproachModel Uses rules to match a set of regular expressions and associate them with a provided identifier. A rule consists of a regex pattern and an identifier, delimited by a character of choice. An example could be &quot; d{4} / d d / d d,date&quot; which will match strings like &quot;1970/01/01&quot; to the identifier &quot;date&quot;. Rules must be provided by either setRules (followed by setDelimiter) or an external file. To use an external file, a dictionary of predefined regular expressions must be provided with setExternalRules. The dictionary can be set as a delimited text file. Pretrained pipelines are available for this module, see Pipelines. For extended examples of usage, see the Examples and the RegexMatcherTestSpec. Input Annotator Types: DOCUMENT Output Annotator Type: CHUNK Python API: RegexMatcher Scala API: RegexMatcher Source: RegexMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the `rules.txt` has the form of # # the s w+, followed by &#39;the&#39; # ceremonies, ceremony # # where each regex is separated by the identifier by `&quot;,&quot;` documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) sentence = SentenceDetector().setInputCols([&quot;document&quot;]).setOutputCol(&quot;sentence&quot;) regexMatcher = RegexMatcher() .setExternalRules(&quot;src/test/resources/regex-matcher/rules.txt&quot;, &quot;,&quot;) .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;regex&quot;) .setStrategy(&quot;MATCH_ALL&quot;) pipeline = Pipeline().setStages([documentAssembler, sentence, regexMatcher]) data = spark.createDataFrame([[ &quot;My first sentence with the first rule. This is my second sentence with ceremonies rule.&quot; ]]).toDF(&quot;text&quot;) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(regex) as result&quot;).show(truncate=False) +--+ |result | +--+ |[chunk, 23, 31, the first, [identifier -&gt; followed by &#39;the&#39;, sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 71, 80, ceremonies, [identifier -&gt; ceremony, sentence -&gt; 1, chunk -&gt; 0], []] | +--+ // In this example, the `rules.txt` has the form of // // the s w+, followed by &#39;the&#39; // ceremonies, ceremony // // where each regex is separated by the identifier by `&quot;,&quot;` import ResourceHelper.spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.RegexMatcher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector().setInputCols(&quot;document&quot;).setOutputCol(&quot;sentence&quot;) val regexMatcher = new RegexMatcher() .setExternalRules(&quot;src/test/resources/regex-matcher/rules.txt&quot;, &quot;,&quot;) .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;regex&quot;) .setStrategy(&quot;MATCH_ALL&quot;) val pipeline = new Pipeline().setStages(Array(documentAssembler, sentence, regexMatcher)) val data = Seq( &quot;My first sentence with the first rule. This is my second sentence with ceremonies rule.&quot; ).toDF(&quot;text&quot;) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(regex) as result&quot;).show(false) +--+ |result | +--+ |[chunk, 23, 31, the first, [identifier -&gt; followed by &#39;the&#39;, sentence -&gt; 0, chunk -&gt; 0], []]| |[chunk, 71, 80, ceremonies, [identifier -&gt; ceremony, sentence -&gt; 1, chunk -&gt; 0], []] | +--+ Instantiated model of the RegexMatcher. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT Output Annotator Type: CHUNK Python API: RegexMatcherModel Scala API: RegexMatcherModel Source: RegexMatcherModel RegexTokenizer A tokenizer that splits text by a regex pattern. The pattern needs to be set with setPattern and this sets the delimiting pattern or how the tokens should be split. By default this pattern is s+ which means that tokens should be split by 1 or more whitespace characters. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: RegexTokenizer Scala API: RegexTokenizer Source: RegexTokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) regexTokenizer = RegexTokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;regexToken&quot;) .setToLowercase(True) .setPattern(&quot; s+&quot;) pipeline = Pipeline().setStages([ documentAssembler, regexTokenizer ]) data = spark.createDataFrame([[&quot;This is my first sentence. nThis is my second.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;regexToken.result&quot;).show(truncate=False) +-+ |result | +-+ |[this, is, my, first, sentence., this, is, my, second.]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.RegexTokenizer import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val regexTokenizer = new RegexTokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;regexToken&quot;) .setToLowercase(true) .setPattern(&quot; s+&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, regexTokenizer )) val data = Seq(&quot;This is my first sentence. nThis is my second.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;regexToken.result&quot;).show(false) +-+ |result | +-+ |[this, is, my, first, sentence., this, is, my, second.]| +-+ SentenceDetector Annotator that detects sentence boundaries using regular expressions. The following characters are checked as sentence boundaries: Lists (“(i), (ii)”, “(a), (b)”, “1., 2.”) Numbers Abbreviations Punctuations Multiple Periods Geo-Locations/Coordinates (“N°. 1026.253.553.”) Ellipsis (“…”) In-between punctuations Quotation marks Exclamation Points Basic Breakers (“.”, “;”) For the explicit regular expressions used for detection, refer to source of PragmaticContentFormatter. To add additional custom bounds, the parameter customBounds can be set with an array: val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) .setCustomBounds(Array(&quot; n n&quot;)) If only the custom bounds should be used, then the parameter useCustomBoundsOnly should be set to true. Each extracted sentence can be returned in an Array or exploded to separate rows, if explodeSentences is set to true. For extended examples of usage, see the Examples. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: SentenceDetector Scala API: SentenceDetector Source: SentenceDetector Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) .setCustomBounds([&quot; n n&quot;]) pipeline = Pipeline().setStages([ documentAssembler, sentence ]) data = spark.createDataFrame([[&quot;This is my first sentence. This my second. How about a third?&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(sentence) as sentences&quot;).show(truncate=False) ++ |sentences | ++ |[document, 0, 25, This is my first sentence., [sentence -&gt; 0], []]| |[document, 27, 41, This my second., [sentence -&gt; 1], []] | |[document, 43, 60, How about a third?, [sentence -&gt; 2], []] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) .setCustomBounds(Array(&quot; n n&quot;)) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence )) val data = Seq(&quot;This is my first sentence. This my second. How about a third?&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(sentence) as sentences&quot;).show(false) ++ |sentences | ++ |[document, 0, 25, This is my first sentence., [sentence -&gt; 0], []]| |[document, 27, 41, This my second., [sentence -&gt; 1], []] | |[document, 43, 60, How about a third?, [sentence -&gt; 2], []] | ++ SentenceDetectorDL ApproachModel Trains an annotator that detects sentence boundaries using a deep learning approach. For pretrained models see SentenceDetectorDLModel. Currently, only the CNN model is supported for training, but in the future the architecture of the model can be set with setModelArchitecture. The default model &quot;cnn&quot; is based on the paper Deep-EOS: General-Purpose Neural Networks for Sentence Boundary Detection (2020, Stefan Schweter, Sajawel Ahmed) using a CNN architecture. We also modified the original implementation a little bit to cover broken sentences and some impossible end of line chars. Each extracted sentence can be returned in an Array or exploded to separate rows, if explodeSentences is set to true. For extended examples of usage, see the Examples and the SentenceDetectorDLSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: SentenceDetectorDLApproach Scala API: SentenceDetectorDLApproach Source: SentenceDetectorDLApproach Show Example PythonScala # The training process needs data, where each data point is a sentence. # In this example the `train.txt` file has the form of # # ... # Slightly more moderate language would make our present situation – namely the lack of progress – a little easier. # His political successors now have great responsibilities to history and to the heritage of values bequeathed to them by Nelson Mandela. # ... # # where each line is one sentence. # Training can then be started like so: import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline trainingData = spark.read.text(&quot;train.txt&quot;).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetectorDLApproach() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) .setEpochsNumber(100) pipeline = Pipeline().setStages([documentAssembler, sentenceDetector]) model = pipeline.fit(trainingData) // The training process needs data, where each data point is a sentence. // In this example the `train.txt` file has the form of // // ... // Slightly more moderate language would make our present situation – namely the lack of progress – a little easier. // His political successors now have great responsibilities to history and to the heritage of values bequeathed to them by Nelson Mandela. // ... // // where each line is one sentence. // Training can then be started like so: import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLApproach import org.apache.spark.ml.Pipeline val trainingData = spark.read.text(&quot;train.txt&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetectorDLApproach() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentences&quot;) .setEpochsNumber(100) val pipeline = new Pipeline().setStages(Array(documentAssembler, sentenceDetector)) val model = pipeline.fit(trainingData) Annotator that detects sentence boundaries using a deep learning approach. Instantiated Model of the SentenceDetectorDLApproach. Detects sentence boundaries using a deep learning approach. Pretrained models can be loaded with pretrained of the companion object: val sentenceDL = SentenceDetectorDLModel.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentencesDL&quot;) The default model is &quot;sentence_detector_dl&quot;, if no name is provided. For available pretrained models please see the Models Hub. Each extracted sentence can be returned in an Array or exploded to separate rows, if explodeSentences is set to true. For extended examples of usage, see the Examples and the SentenceDetectorDLSpec. Input Annotator Types: DOCUMENT Output Annotator Type: DOCUMENT Python API: SentenceDetectorDLModel Scala API: SentenceDetectorDLModel Source: SentenceDetectorDLModel SentenceEmbeddings Converts the results from WordEmbeddings, BertEmbeddings, or ElmoEmbeddings into sentence or document embeddings by either summing up or averaging all the word embeddings in a sentence or a document (depending on the inputCols). This can be configured with setPoolingStrategy, which either be &quot;AVERAGE&quot; or &quot;SUM&quot;. For more extended examples see the Examples. and the SentenceEmbeddingsTestSpec. TIP: Here is how you can explode and convert these embeddings into Vectors or what’s known as Feature column so it can be used in Spark ML regression or clustering functions: PythonScala from org.apache.spark.ml.linal import Vector, Vectors from pyspark.sql.functions import udf # Let&#39;s create a UDF to take array of embeddings and output Vectors @udf(Vector) def convertToVectorUDF(matrix): return Vectors.dense(matrix.toArray.map(_.toDouble)) # Now let&#39;s explode the sentence_embeddings column and have a new feature column for Spark ML pipelineDF.select(explode(&quot;sentence_embeddings.embeddings&quot;).as(&quot;sentence_embedding&quot;)) .withColumn(&quot;features&quot;, convertToVectorUDF(&quot;sentence_embedding&quot;)) import org.apache.spark.ml.linalg.{Vector, Vectors} // Let&#39;s create a UDF to take array of embeddings and output Vectors val convertToVectorUDF = udf((matrix : Seq[Float]) =&gt; { Vectors.dense(matrix.toArray.map(_.toDouble)) }) // Now let&#39;s explode the sentence_embeddings column and have a new feature column for Spark ML pipelineDF.select(explode($&quot;sentence_embeddings.embeddings&quot;).as(&quot;sentence_embedding&quot;)) .withColumn(&quot;features&quot;, convertToVectorUDF($&quot;sentence_embedding&quot;)) Input Annotator Types: DOCUMENT, WORD_EMBEDDINGS Output Annotator Type: SENTENCE_EMBEDDINGS Note: If you choose document as your input for Tokenizer, WordEmbeddings/BertEmbeddings, and SentenceEmbeddings then it averages/sums all the embeddings into one array of embeddings. However, if you choose sentence as inputCols then for each sentence SentenceEmbeddings generates one array of embeddings. Python API: SentenceEmbeddings Scala API: SentenceEmbeddings Source: SentenceEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddingsModel.pretrained() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsSentence = SentenceEmbeddings() .setInputCols([&quot;document&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) .setCleanAnnotations(False) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings, embeddingsSentence, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80) +--+ | result| +--+ |[-0.22093398869037628,0.25130119919776917,0.41810303926467896,-0.380883991718...| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.embeddings.WordEmbeddingsModel import com.johnsnowlabs.nlp.embeddings.SentenceEmbeddings import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsSentence = new SentenceEmbeddings() .setInputCols(Array(&quot;document&quot;, &quot;embeddings&quot;)) .setOutputCol(&quot;sentence_embeddings&quot;) .setPoolingStrategy(&quot;AVERAGE&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsSentence, embeddingsFinisher )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(5, 80) +--+ | result| +--+ |[-0.22093398869037628,0.25130119919776917,0.41810303926467896,-0.380883991718...| +--+ SentimentDL ApproachModel Trains a SentimentDL, an annotator for multi-class sentiment analysis. In natural language processing, sentiment analysis is the task of classifying the affective state or subjective view of a text. A common example is if either a product review or tweet can be interpreted positively or negatively. For the instantiated/pretrained models, see SentimentDLModel. Notes: This annotator accepts a label column of a single item in either type of String, Int, Float, or Double. So positive sentiment can be expressed as either &quot;positive&quot; or 0, negative sentiment as &quot;negative&quot; or 1. UniversalSentenceEncoder, BertSentenceEmbeddings, SentenceEmbeddings or other sentence based embeddings can be used Setting a test dataset to monitor model metrics can be done with .setTestDataset. The method expects a path to a parquet file containing a dataframe that has the same required columns as the training dataframe. The pre-processing steps for the training dataframe should also be applied to the test dataframe. The following example will show how to create the test dataset: val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val embeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val preProcessingPipeline = new Pipeline().setStages(Array(documentAssembler, embeddings)) val Array(train, test) = data.randomSplit(Array(0.8, 0.2)) preProcessingPipeline .fit(test) .transform(test) .write .mode(&quot;overwrite&quot;) .parquet(&quot;test_data&quot;) val classifier = new SentimentDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sentiment&quot;) .setLabelColumn(&quot;label&quot;) .setTestDataset(&quot;test_data&quot;) For extended examples of usage, see the Examples and the SentimentDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: SentimentDLApproach Scala API: SentimentDLApproach Source: SentimentDLApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, `sentiment.csv` is in the form # # text,label # This movie is the best movie I have watched ever! In my opinion this movie can win an award.,0 # This was a terrible movie! The acting was bad really bad!,1 # # The model can then be trained with smallCorpus = spark.read.option(&quot;header&quot;, &quot;True&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence_embeddings&quot;) docClassifier = SentimentDLApproach() .setInputCols([&quot;sentence_embeddings&quot;]) .setOutputCol(&quot;sentiment&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(32) .setMaxEpochs(1) .setLr(5e-3) .setDropout(0.5) pipeline = Pipeline() .setStages( [ documentAssembler, useEmbeddings, docClassifier ] ) pipelineModel = pipeline.fit(smallCorpus) // In this example, `sentiment.csv` is in the form // // text,label // This movie is the best movie I have watched ever! In my opinion this movie can win an award.,0 // This was a terrible movie! The acting was bad really bad!,1 // // The model can then be trained with import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.UniversalSentenceEncoder import com.johnsnowlabs.nlp.annotators.classifier.dl.{SentimentDLApproach, SentimentDLModel} import org.apache.spark.ml.Pipeline val smallCorpus = spark.read.option(&quot;header&quot;, &quot;true&quot;).csv(&quot;src/test/resources/classifier/sentiment.csv&quot;) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val useEmbeddings = UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) val docClassifier = new SentimentDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sentiment&quot;) .setLabelColumn(&quot;label&quot;) .setBatchSize(32) .setMaxEpochs(1) .setLr(5e-3f) .setDropout(0.5f) val pipeline = new Pipeline() .setStages( Array( documentAssembler, useEmbeddings, docClassifier ) ) val pipelineModel = pipeline.fit(smallCorpus) SentimentDL, an annotator for multi-class sentiment analysis. In natural language processing, sentiment analysis is the task of classifying the affective state or subjective view of a text. A common example is if either a product review or tweet can be interpreted positively or negatively. This is the instantiated model of the SentimentDLApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val sentiment = SentimentDLModel.pretrained() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;sentiment&quot;) The default model is &quot;sentimentdl_use_imdb&quot;, if no name is provided. It is english sentiment analysis trained on the IMDB dataset. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the SentimentDLTestSpec. Input Annotator Types: SENTENCE_EMBEDDINGS Output Annotator Type: CATEGORY Python API: SentimentDLModel Scala API: SentimentDLModel Source: SentimentDLModel SentimentDetector ApproachModel Trains a rule based sentiment detector, which calculates a score based on predefined keywords. A dictionary of predefined sentiment keywords must be provided with setDictionary, where each line is a word delimited to its class (either positive or negative). The dictionary can be set as a delimited text file. By default, the sentiment score will be assigned labels &quot;positive&quot; if the score is &gt;= 0, else &quot;negative&quot;. To retrieve the raw sentiment scores, enableScore needs to be set to true. For extended examples of usage, see the Examples and the SentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: SentimentDetector Scala API: SentimentDetector Source: SentimentDetector Show Example PythonScala # In this example, the dictionary `default-sentiment-dict.txt` has the form of # # ... # cool,positive # superb,positive # bad,negative # uninspired,negative # ... # # where each sentiment keyword is delimited by `&quot;,&quot;`. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = Lemmatizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) sentimentDetector = SentimentDetector() .setInputCols([&quot;lemma&quot;, &quot;document&quot;]) .setOutputCol(&quot;sentimentScore&quot;) .setDictionary(&quot;default-sentiment-dict.txt&quot;, &quot;,&quot;, ReadAs.TEXT) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, lemmatizer, sentimentDetector, ]) data = spark.createDataFrame([ [&quot;The staff of the restaurant is nice&quot;], [&quot;I recommend others to avoid because it is too expensive&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;sentimentScore.result&quot;).show(truncate=False) +-+ # ++ for enableScore set to True |result | # |result| +-+ # ++ |[positive]| # |[1.0] | |[negative]| # |[-2.0]| +-+ # ++ // In this example, the dictionary `default-sentiment-dict.txt` has the form of // // ... // cool,positive // superb,positive // bad,negative // uninspired,negative // ... // // where each sentiment keyword is delimited by `&quot;,&quot;`. import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotators.Lemmatizer import com.johnsnowlabs.nlp.annotators.sda.pragmatic.SentimentDetector import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val lemmatizer = new Lemmatizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;lemma&quot;) .setDictionary(&quot;src/test/resources/lemma-corpus-small/lemmas_small.txt&quot;, &quot;-&gt;&quot;, &quot; t&quot;) val sentimentDetector = new SentimentDetector() .setInputCols(&quot;lemma&quot;, &quot;document&quot;) .setOutputCol(&quot;sentimentScore&quot;) .setDictionary(&quot;src/test/resources/sentiment-corpus/default-sentiment-dict.txt&quot;, &quot;,&quot;, ReadAs.TEXT) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, lemmatizer, sentimentDetector, )) val data = Seq( &quot;The staff of the restaurant is nice&quot;, &quot;I recommend others to avoid because it is too expensive&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;sentimentScore.result&quot;).show(false) +-+ // ++ for enableScore set to true |result | // |result| +-+ // ++ |[positive]| // |[1.0] | |[negative]| // |[-2.0]| +-+ // ++ Rule based sentiment detector, which calculates a score based on predefined keywords. This is the instantiated model of the SentimentDetector. For training your own model, please see the documentation of that class. A dictionary of predefined sentiment keywords must be provided with setDictionary, where each line is a word delimited to its class (either positive or negative). The dictionary can be set as a delimited text file. By default, the sentiment score will be assigned labels &quot;positive&quot; if the score is &gt;= 0, else &quot;negative&quot;. To retrieve the raw sentiment scores, enableScore needs to be set to true. For extended examples of usage, see the Examples and the SentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: SentimentDetectorModel Scala API: SentimentDetectorModel Source: SentimentDetectorModel Stemmer Returns hard-stems out of words with the objective of retrieving the meaningful part of the word. For extended examples of usage, see the Examples. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: Stemmer Scala API: Stemmer Source: Stemmer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) stemmer = Stemmer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;stem&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, stemmer ]) data = spark.createDataFrame([[&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;]]) .toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;stem.result&quot;).show(truncate = False) +-+ |result | +-+ |[peter, piper, employe, ar, pick, peck, of, pickl, pepper, .]| +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Stemmer, Tokenizer} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val stemmer = new Stemmer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;stem&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, stemmer )) val data = Seq(&quot;Peter Pipers employees are picking pecks of pickled peppers.&quot;) .toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;stem.result&quot;).show(truncate = false) +-+ |result | +-+ |[peter, piper, employe, ar, pick, peck, of, pickl, pepper, .]| +-+ StopWordsCleaner This annotator takes a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences. By default, it uses stop words from MLlibs StopWordsRemover. Stop words can also be defined by explicitly setting them with setStopWords(value: Array[String]) or loaded from pretrained models using pretrained of its companion object. val stopWords = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) // will load the default pretrained model `&quot;stopwords_en&quot;`. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and StopWordsCleanerTestSpec. NOTE: If you need to setStopWords from a text file, you can first read and convert it into an array of string as follows. PythonScala # your stop words text file, each line is one stop word stopwords = sc.textFile(&quot;/tmp/stopwords/english.txt&quot;).collect() # simply use it in StopWordsCleaner stopWordsCleaner = StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setStopWords(stopwords) .setCaseSensitive(False) # or you can use pretrained models for StopWordsCleaner stopWordsCleaner = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) // your stop words text file, each line is one stop word val stopwords = sc.textFile(&quot;/tmp/stopwords/english.txt&quot;).collect() // simply use it in StopWordsCleaner val stopWordsCleaner = new StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setStopWords(stopwords) .setCaseSensitive(false) // or you can use pretrained models for StopWordsCleaner val stopWordsCleaner = StopWordsCleaner.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: StopWordsCleaner Scala API: StopWordsCleaner Source: StopWordsCleaner Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) stopWords = StopWordsCleaner() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, stopWords ]) data = spark.createDataFrame([ [&quot;This is my first sentence. This is my second.&quot;], [&quot;This is my third sentence. This is my forth.&quot;] ]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;cleanTokens.result&quot;).show(truncate=False) +-+ |result | +-+ |[first, sentence, ., second, .]| |[third, sentence, ., forth, .] | +-+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotators.StopWordsCleaner import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val stopWords = new StopWordsCleaner() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, stopWords )) val data = Seq( &quot;This is my first sentence. This is my second.&quot;, &quot;This is my third sentence. This is my forth.&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;cleanTokens.result&quot;).show(false) +-+ |result | +-+ |[first, sentence, ., second, .]| |[third, sentence, ., forth, .] | +-+ SymmetricDelete Spellchecker ApproachModel Trains a Symmetric Delete spelling correction algorithm. Retrieves tokens and utilizes distance metrics to compute possible derived words. Inspired by SymSpell. For instantiated/pretrained models, see SymmetricDeleteModel. See SymmetricDeleteModelTestSpec for further reference. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: SymmetricDeleteApproach Scala API: SymmetricDeleteApproach Source: SymmetricDeleteApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the dictionary `&quot;words.txt&quot;` has the form of # # ... # gummy # gummic # gummier # gummiest # gummiferous # ... # # This dictionary is then set to be the basis of the spell checker. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) spellChecker = SymmetricDeleteApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, spellChecker ]) pipelineModel = pipeline.fit(trainingData) // In this example, the dictionary `&quot;words.txt&quot;` has the form of // // ... // gummy // gummic // gummier // gummiest // gummiferous // ... // // This dictionary is then set to be the basis of the spell checker. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.spell.symmetric.SymmetricDeleteApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val spellChecker = new SymmetricDeleteApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) .setDictionary(&quot;src/test/resources/spell/words.txt&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, spellChecker )) val pipelineModel = pipeline.fit(trainingData) Symmetric Delete spelling correction algorithm. The Symmetric Delete spelling correction algorithm reduces the complexity of edit candidate generation and dictionary lookup for a given Damerau-Levenshtein distance. It is six orders of magnitude faster (than the standard approach with deletes + transposes + replaces + inserts) and language independent. Inspired by SymSpell. Pretrained models can be loaded with pretrained of the companion object: val spell = SymmetricDeleteModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;spell&quot;) The default model is &quot;spellcheck_sd&quot;, if no name is provided. For available pretrained models please see the Models Hub. See SymmetricDeleteModelTestSpec for further reference. Input Annotator Types: TOKEN Output Annotator Type: TOKEN Python API: SymmetricDeleteModel Scala API: SymmetricDeleteModel Source: SymmetricDeleteModel TextMatcher ApproachModel Annotator to match exact phrases (by token) provided in a file against a Document. A text file of predefined phrases must be provided with setEntities. For extended examples of usage, see the Examples and the TextMatcherTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: TextMatcher Scala API: TextMatcher Source: TextMatcher Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the entities file is of the form # # ... # dolore magna aliqua # lorem ipsum dolor. sit # laborum # ... # # where each line represents an entity phrase to be extracted. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) data = spark.createDataFrame([[&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;]]).toDF(&quot;text&quot;) entityExtractor = TextMatcher() .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setEntities(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(False) pipeline = Pipeline().setStages([documentAssembler, tokenizer, entityExtractor]) results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity) as result&quot;).show(truncate=False) ++ |result | ++ |[chunk, 6, 24, dolore magna aliqua, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 27, 48, Lorem ipsum dolor. sit, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 1], []]| |[chunk, 53, 59, laborum, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 2], []] | ++ // In this example, the entities file is of the form // // ... // dolore magna aliqua // lorem ipsum dolor. sit // laborum // ... // // where each line represents an entity phrase to be extracted. import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.TextMatcher import com.johnsnowlabs.nlp.util.io.ReadAs import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val data = Seq(&quot;Hello dolore magna aliqua. Lorem ipsum dolor. sit in laborum&quot;).toDF(&quot;text&quot;) val entityExtractor = new TextMatcher() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setEntities(&quot;src/test/resources/entity-extractor/test-phrases.txt&quot;, ReadAs.TEXT) .setOutputCol(&quot;entity&quot;) .setCaseSensitive(false) .setTokenizer(tokenizer.fit(data)) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer, entityExtractor)) val results = pipeline.fit(data).transform(data) results.selectExpr(&quot;explode(entity) as result&quot;).show(false) ++ |result | ++ |[chunk, 6, 24, dolore magna aliqua, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 0], []] | |[chunk, 27, 48, Lorem ipsum dolor. sit, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 1], []]| |[chunk, 53, 59, laborum, [entity -&gt; entity, sentence -&gt; 0, chunk -&gt; 2], []] | ++ Instantiated model of the TextMatcher. For usage and examples see the documentation of the main class. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: CHUNK Python API: TextMatcherModel Scala API: TextMatcherModel Source: TextMatcherModel Token2Chunk Converts TOKEN type Annotations to CHUNK type. This can be useful if a entities have been already extracted as TOKEN and following annotators require CHUNK types. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: Token2Chunk Scala API: Token2Chunk Source: Token2Chunk Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) token2chunk = Token2Chunk() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;chunk&quot;) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, token2chunk ]) data = spark.createDataFrame([[&quot;One Two Three Four&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(truncate=False) ++ |result | ++ |[chunk, 0, 2, One, [sentence -&gt; 0], []] | |[chunk, 4, 6, Two, [sentence -&gt; 0], []] | |[chunk, 8, 12, Three, [sentence -&gt; 0], []]| |[chunk, 14, 17, Four, [sentence -&gt; 0], []]| ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.{Token2Chunk, Tokenizer} import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val token2chunk = new Token2Chunk() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;chunk&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, token2chunk )) val data = Seq(&quot;One Two Three Four&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(chunk) as result&quot;).show(false) ++ |result | ++ |[chunk, 0, 2, One, [sentence -&gt; 0], []] | |[chunk, 4, 6, Two, [sentence -&gt; 0], []] | |[chunk, 8, 12, Three, [sentence -&gt; 0], []]| |[chunk, 14, 17, Four, [sentence -&gt; 0], []]| ++ TokenAssembler This transformer reconstructs a DOCUMENT type annotation from tokens, usually after these have been normalized, lemmatized, normalized, spell checked, etc, in order to use this document annotation in further annotators. Requires DOCUMENT and TOKEN type annotations as input. For more extended examples on document pre-processing see the Examples. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: DOCUMENT Python API: TokenAssembler Scala API: TokenAssembler Source: TokenAssembler Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # First, the text is tokenized and cleaned documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentences&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentences&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normalized&quot;) .setLowercase(False) stopwordsCleaner = StopWordsCleaner() .setInputCols([&quot;normalized&quot;]) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(False) # Then the TokenAssembler turns the cleaned tokens into a `DOCUMENT` type structure. tokenAssembler = TokenAssembler() .setInputCols([&quot;sentences&quot;, &quot;cleanTokens&quot;]) .setOutputCol(&quot;cleanText&quot;) data = spark.createDataFrame([[&quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;]]) .toDF(&quot;text&quot;) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, tokenizer, normalizer, stopwordsCleaner, tokenAssembler ]).fit(data) result = pipeline.transform(data) result.select(&quot;cleanText&quot;).show(truncate=False) ++ |cleanText | ++ |0, 80, Spark NLP opensource text processing library advanced natural language processing, [sentence -&gt; 0], []| ++ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotator.SentenceDetector import com.johnsnowlabs.nlp.annotator.Tokenizer import com.johnsnowlabs.nlp.annotator.{Normalizer, StopWordsCleaner} import com.johnsnowlabs.nlp.TokenAssembler import org.apache.spark.ml.Pipeline // First, the text is tokenized and cleaned val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentences&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentences&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normalized&quot;) .setLowercase(false) val stopwordsCleaner = new StopWordsCleaner() .setInputCols(&quot;normalized&quot;) .setOutputCol(&quot;cleanTokens&quot;) .setCaseSensitive(false) // Then the TokenAssembler turns the cleaned tokens into a `DOCUMENT` type structure. val tokenAssembler = new TokenAssembler() .setInputCols(&quot;sentences&quot;, &quot;cleanTokens&quot;) .setOutputCol(&quot;cleanText&quot;) val data = Seq(&quot;Spark NLP is an open-source text processing library for advanced natural language processing.&quot;) .toDF(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, tokenizer, normalizer, stopwordsCleaner, tokenAssembler )).fit(data) val result = pipeline.transform(data) result.select(&quot;cleanText&quot;).show(false) ++ |cleanText | ++ |[[document, 0, 80, Spark NLP opensource text processing library advanced natural language processing, [sentence -&gt; 0], []]]| ++ Tokenizer ApproachModel Tokenizes raw text in document type columns into TokenizedSentence . This class represents a non fitted tokenizer. Fitting it will cause the internal RuleFactory to construct the rules for tokenizing from the input configuration. Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. For extended examples of usage see the Examples and Tokenizer test class Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Note: All these APIs receive regular expressions so please make sure that you escape special characters according to Java conventions. Python API: Tokenizer Scala API: Tokenizer Source: Tokenizer Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline data = spark.createDataFrame([[&quot;I&#39;d like to say we didn&#39;t expect that. Jane&#39;s boyfriend.&quot;]]).toDF(&quot;text&quot;) documentAssembler = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) tokenizer = Tokenizer().setInputCols([&quot;document&quot;]).setOutputCol(&quot;token&quot;).fit(data) pipeline = Pipeline().setStages([documentAssembler, tokenizer]).fit(data) result = pipeline.transform(data) result.selectExpr(&quot;token.result&quot;).show(truncate=False) +--+ |output | +--+ |[I&#39;d, like, to, say, we, didn&#39;t, expect, that, ., Jane&#39;s, boyfriend, .]| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import org.apache.spark.ml.Pipeline val data = Seq(&quot;I&#39;d like to say we didn&#39;t expect that. Jane&#39;s boyfriend.&quot;).toDF(&quot;text&quot;) val documentAssembler = new DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer().setInputCols(&quot;document&quot;).setOutputCol(&quot;token&quot;).fit(data) val pipeline = new Pipeline().setStages(Array(documentAssembler, tokenizer)).fit(data) val result = pipeline.transform(data) result.selectExpr(&quot;token.result&quot;).show(false) +--+ |output | +--+ |[I&#39;d, like, to, say, we, didn&#39;t, expect, that, ., Jane&#39;s, boyfriend, .]| +--+ Tokenizes raw text into word pieces, tokens. Identifies tokens with tokenization open standards. A few rules will help customizing it if defaults do not fit user needs. This class represents an already fitted Tokenizer model. See the main class Tokenizer for more examples of usage. Input Annotator Types: DOCUMENT //A Tokenizer could require only for now a SentenceDetector annotator Output Annotator Type: TOKEN Python API: TokenizerModel Scala API: TokenizerModel Source: TokenizerModel TypedDependencyParser ApproachModel Labeled parser that finds a grammatical relation between two words in a sentence. Its input is either a CoNLL2009 or ConllU dataset. For instantiated/pretrained models, see TypedDependencyParserModel. Dependency parsers provide information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The parser requires the dependant tokens beforehand with e.g. DependencyParser. The required training data can be set in two different ways (only one can be chosen for a particular model): Dataset in the CoNLL 2009 format set with setConll2009 Dataset in the CoNLL-U format set with setConllU Apart from that, no additional training data is needed. See TypedDependencyParserApproachTestSpec for further reference on this API. Input Annotator Types: TOKEN, POS, DEPENDENCY Output Annotator Type: LABELED_DEPENDENCY Python API: TypedDependencyParserApproach Scala API: TypedDependencyParserApproach Source: TypedDependencyParserApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) posTagger = PerceptronModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) .setOutputCol(&quot;pos&quot;) dependencyParser = DependencyParserModel.pretrained() .setInputCols([&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency&quot;) typedDependencyParser = TypedDependencyParserApproach() .setInputCols([&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;]) .setOutputCol(&quot;dependency_type&quot;) .setConllU(&quot;src/test/resources/parser/labeled/train_small.conllu.txt&quot;) .setNumberOfIterations(1) pipeline = Pipeline().setStages([ documentAssembler, sentence, tokenizer, posTagger, dependencyParser, typedDependencyParser ]) # Additional training data is not needed, the dependency parser relies on CoNLL-U only. emptyDataSet = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(emptyDataSet) import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.sbd.pragmatic.SentenceDetector import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.pos.perceptron.PerceptronModel import com.johnsnowlabs.nlp.annotators.parser.dep.DependencyParserModel import com.johnsnowlabs.nlp.annotators.parser.typdep.TypedDependencyParserApproach import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentence = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) val posTagger = PerceptronModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) val dependencyParser = DependencyParserModel.pretrained() .setInputCols(&quot;sentence&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency&quot;) val typedDependencyParser = new TypedDependencyParserApproach() .setInputCols(&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency_type&quot;) .setConllU(&quot;src/test/resources/parser/labeled/train_small.conllu.txt&quot;) .setNumberOfIterations(1) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentence, tokenizer, posTagger, dependencyParser, typedDependencyParser )) // Additional training data is not needed, the dependency parser relies on CoNLL-U only. val emptyDataSet = Seq.empty[String].toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(emptyDataSet) Labeled parser that finds a grammatical relation between two words in a sentence. Its input is either a CoNLL2009 or ConllU dataset. Dependency parsers provide information about word relationship. For example, dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject. This can help you find precise answers to specific questions. The parser requires the dependant tokens beforehand with e.g. DependencyParser. Pretrained models can be loaded with pretrained of the companion object: val typedDependencyParser = TypedDependencyParserModel.pretrained() .setInputCols(&quot;dependency&quot;, &quot;pos&quot;, &quot;token&quot;) .setOutputCol(&quot;dependency_type&quot;) The default model is &quot;dependency_typed_conllu&quot;, if no name is provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the TypedDependencyModelTestSpec. Input Annotator Types: TOKEN, POS, DEPENDENCY Output Annotator Type: LABELED_DEPENDENCY Python API: TypedDependencyParserModel Scala API: TypedDependencyParserModel Source: TypedDependencyParserModel ViveknSentiment ApproachModel Trains a sentiment analyser inspired by the algorithm by Vivek Narayanan https://github.com/vivekn/sentiment/. The algorithm is based on the paper “Fast and accurate sentiment classification using an enhanced Naive Bayes model”. The analyzer requires sentence boundaries to give a score in context. Tokenization is needed to make sure tokens are within bounds. Transitivity requirements are also required. The training data needs to consist of a column for normalized text and a label column (either &quot;positive&quot; or &quot;negative&quot;). For extended examples of usage, see the Examples and the ViveknSentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: ViveknSentimentApproach Scala API: ViveknSentimentApproach Source: ViveknSentimentApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) token = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) normalizer = Normalizer() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;normal&quot;) vivekn = ViveknSentimentApproach() .setInputCols([&quot;document&quot;, &quot;normal&quot;]) .setSentimentCol(&quot;train_sentiment&quot;) .setOutputCol(&quot;result_sentiment&quot;) finisher = Finisher() .setInputCols([&quot;result_sentiment&quot;]) .setOutputCols(&quot;final_sentiment&quot;) pipeline = Pipeline().setStages([document, token, normalizer, vivekn, finisher]) training = spark.createDataFrame([ (&quot;I really liked this movie!&quot;, &quot;positive&quot;), (&quot;The cast was horrible&quot;, &quot;negative&quot;), (&quot;Never going to watch this again or recommend it to anyone&quot;, &quot;negative&quot;), (&quot;It&#39;s a waste of time&quot;, &quot;negative&quot;), (&quot;I loved the protagonist&quot;, &quot;positive&quot;), (&quot;The music was really really good&quot;, &quot;positive&quot;) ]).toDF(&quot;text&quot;, &quot;train_sentiment&quot;) pipelineModel = pipeline.fit(training) data = spark.createDataFrame([ [&quot;I recommend this movie&quot;], [&quot;Dont waste your time!!!&quot;] ]).toDF(&quot;text&quot;) result = pipelineModel.transform(data) result.select(&quot;final_sentiment&quot;).show(truncate=False) ++ |final_sentiment| ++ |[positive] | |[negative] | ++ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.annotators.Normalizer import com.johnsnowlabs.nlp.annotators.sda.vivekn.ViveknSentimentApproach import com.johnsnowlabs.nlp.Finisher import org.apache.spark.ml.Pipeline val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val token = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val normalizer = new Normalizer() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;normal&quot;) val vivekn = new ViveknSentimentApproach() .setInputCols(&quot;document&quot;, &quot;normal&quot;) .setSentimentCol(&quot;train_sentiment&quot;) .setOutputCol(&quot;result_sentiment&quot;) val finisher = new Finisher() .setInputCols(&quot;result_sentiment&quot;) .setOutputCols(&quot;final_sentiment&quot;) val pipeline = new Pipeline().setStages(Array(document, token, normalizer, vivekn, finisher)) val training = Seq( (&quot;I really liked this movie!&quot;, &quot;positive&quot;), (&quot;The cast was horrible&quot;, &quot;negative&quot;), (&quot;Never going to watch this again or recommend it to anyone&quot;, &quot;negative&quot;), (&quot;It&#39;s a waste of time&quot;, &quot;negative&quot;), (&quot;I loved the protagonist&quot;, &quot;positive&quot;), (&quot;The music was really really good&quot;, &quot;positive&quot;) ).toDF(&quot;text&quot;, &quot;train_sentiment&quot;) val pipelineModel = pipeline.fit(training) val data = Seq( &quot;I recommend this movie&quot;, &quot;Dont waste your time!!!&quot; ).toDF(&quot;text&quot;) val result = pipelineModel.transform(data) result.select(&quot;final_sentiment&quot;).show(false) ++ |final_sentiment| ++ |[positive] | |[negative] | ++ Sentiment analyser inspired by the algorithm by Vivek Narayanan https://github.com/vivekn/sentiment/. The algorithm is based on the paper “Fast and accurate sentiment classification using an enhanced Naive Bayes model”. This is the instantiated model of the ViveknSentimentApproach. For training your own model, please see the documentation of that class. The analyzer requires sentence boundaries to give a score in context. Tokenization is needed to make sure tokens are within bounds. Transitivity requirements are also required. For extended examples of usage, see the Examples and the ViveknSentimentTestSpec. Input Annotator Types: TOKEN, DOCUMENT Output Annotator Type: SENTIMENT Python API: ViveknSentimentModel Scala API: ViveknSentimentModel Source: ViveknSentimentModel Word2Vec ApproachModel Trains a Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. For instantiated/pretrained models, see Word2VecModel. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: Word2VecApproach Scala API: Word2VecApproach Source: Word2VecApproach Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Word2VecApproach() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings ]) path = &quot;sherlockholmes.txt&quot; dataset = spark.read.text(path).toDF(&quot;text&quot;) pipelineModel = pipeline.fit(dataset) import spark.implicits._ import com.johnsnowlabs.nlp.annotator.{Tokenizer, Word2VecApproach} import com.johnsnowlabs.nlp.base.DocumentAssembler import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = new Word2VecApproach() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings )) val path = &quot;src/test/resources/spell/sherlockholmes.txt&quot; val dataset = spark.sparkContext.textFile(path) .toDF(&quot;text&quot;) val pipelineModel = pipeline.fit(dataset) Word2Vec model that creates vector representations of words in a text corpus. The algorithm first constructs a vocabulary from the corpus and then learns vector representation of words in the vocabulary. The vector representation can be used as features in natural language processing and machine learning algorithms. We use Word2Vec implemented in Spark ML. It uses skip-gram model in our implementation and a hierarchical softmax method to train the model. The variable names in the implementation match the original C implementation. This is the instantiated model of the Word2VecApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val embeddings = Word2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;word2vec_gigaword_300&quot;, if no name is provided. For available pretrained models please see the Models Hub. Sources : For the original C implementation, see https://code.google.com/p/word2vec/ For the research paper, see Efficient Estimation of Word Representations in Vector Space and Distributed Representations of Words and Phrases and their Compositionality. Input Annotator Types: TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: Word2VecModel Scala API: Word2VecModel Source: Word2VecModel Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = Word2VecModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) pipeline = Pipeline().setStages([ documentAssembler, tokenizer, embeddings, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;This is a sentence.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{Tokenizer, Word2VecModel} import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = Word2VecModel.pretrained() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) val pipeline = new Pipeline().setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsFinisher )) val data = Seq(&quot;This is a sentence.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(1, 80) +--+ | result| +--+ |[0.06222493574023247,0.011579325422644615,0.009919632226228714,0.109361454844...| +--+ WordEmbeddings ApproachModel Word Embeddings lookup annotator that maps tokens to vectors. For instantiated/pretrained models, see WordEmbeddingsModel. A custom token lookup dictionary for embeddings can be set with setStoragePath. Each line of the provided file needs to have a token, followed by their vector representation, delimited by a spaces. ... are 0.39658191506190343 0.630968081620067 0.5393722253731201 0.8428180123359783 were 0.7535235923631415 0.9699218875629833 0.10397182122983872 0.11833962569383116 stress 0.0492683418305907 0.9415954572751959 0.47624463167525755 0.16790967216778263 induced 0.1535748762292387 0.33498936903209897 0.9235178224122094 0.1158772920395934 ... If a token is not found in the dictionary, then the result will be a zero vector of the same dimension. Statistics about the rate of converted tokens, can be retrieved with[WordEmbeddingsModel.withCoverageColumn and WordEmbeddingsModel.overallCoverage. For extended examples of usage, see the Examples and the WordEmbeddingsTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: WordEmbeddings Scala API: WordEmbeddings Source: WordEmbeddings Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline # In this example, the file `random_embeddings_dim4.txt` has the form of the content above. documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) embeddings = WordEmbeddings() .setStoragePath(&quot;src/test/resources/random_embeddings_dim4.txt&quot;, ReadAs.TEXT) .setStorageRef(&quot;glove_4d&quot;) .setDimension(4) .setInputCols([&quot;document&quot;, &quot;token&quot;]) .setOutputCol(&quot;embeddings&quot;) embeddingsFinisher = EmbeddingsFinisher() .setInputCols([&quot;embeddings&quot;]) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(True) .setCleanAnnotations(False) pipeline = Pipeline() .setStages([ documentAssembler, tokenizer, embeddings, embeddingsFinisher ]) data = spark.createDataFrame([[&quot;The patient was diagnosed with diabetes.&quot;]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(truncate=False) +-+ |result | +-+ |[0.9439099431037903,0.4707513153553009,0.806300163269043,0.16176554560661316] | |[0.7966810464859009,0.5551124811172485,0.8861005902290344,0.28284206986427307] | |[0.025029370561242104,0.35177749395370483,0.052506182342767715,0.1887107789516449]| |[0.08617766946554184,0.8399239182472229,0.5395117998123169,0.7864698767662048] | |[0.6599600911140442,0.16109347343444824,0.6041093468666077,0.8913561105728149] | |[0.5955275893211365,0.01899011991918087,0.4397728443145752,0.8911281824111938] | |[0.9840458631515503,0.7599489092826843,0.9417727589607239,0.8624503016471863] | +-+ // In this example, the file `random_embeddings_dim4.txt` has the form of the content above. import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.Tokenizer import com.johnsnowlabs.nlp.embeddings.WordEmbeddings import com.johnsnowlabs.nlp.util.io.ReadAs import com.johnsnowlabs.nlp.EmbeddingsFinisher import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;token&quot;) val embeddings = new WordEmbeddings() .setStoragePath(&quot;src/test/resources/random_embeddings_dim4.txt&quot;, ReadAs.TEXT) .setStorageRef(&quot;glove_4d&quot;) .setDimension(4) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val embeddingsFinisher = new EmbeddingsFinisher() .setInputCols(&quot;embeddings&quot;) .setOutputCols(&quot;finished_embeddings&quot;) .setOutputAsVector(true) .setCleanAnnotations(false) val pipeline = new Pipeline() .setStages(Array( documentAssembler, tokenizer, embeddings, embeddingsFinisher )) val data = Seq(&quot;The patient was diagnosed with diabetes.&quot;).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) result.selectExpr(&quot;explode(finished_embeddings) as result&quot;).show(false) +-+ |result | +-+ |[0.9439099431037903,0.4707513153553009,0.806300163269043,0.16176554560661316] | |[0.7966810464859009,0.5551124811172485,0.8861005902290344,0.28284206986427307] | |[0.025029370561242104,0.35177749395370483,0.052506182342767715,0.1887107789516449]| |[0.08617766946554184,0.8399239182472229,0.5395117998123169,0.7864698767662048] | |[0.6599600911140442,0.16109347343444824,0.6041093468666077,0.8913561105728149] | |[0.5955275893211365,0.01899011991918087,0.4397728443145752,0.8911281824111938] | |[0.9840458631515503,0.7599489092826843,0.9417727589607239,0.8624503016471863] | +-+ Word Embeddings lookup annotator that maps tokens to vectors This is the instantiated model of WordEmbeddings. Pretrained models can be loaded with pretrained of the companion object: val embeddings = WordEmbeddingsModel.pretrained() .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) The default model is &quot;glove_100d&quot;, if no name is provided. For available pretrained models please see the Models Hub. There are also two convenient functions to retrieve the embeddings coverage with respect to the transformed dataset: withCoverageColumn(dataset, embeddingsCol, outputCol): Adds a custom column with word coverage stats for the embedded field: (coveredWords, totalWords, coveragePercentage). This creates a new column with statistics for each row. val wordsCoverage = WordEmbeddingsModel.withCoverageColumn(resultDF, &quot;embeddings&quot;, &quot;cov_embeddings&quot;) wordsCoverage.select(&quot;text&quot;,&quot;cov_embeddings&quot;).show(false) +-+--+ |text |cov_embeddings| +-+--+ |This is a sentence.|[5, 5, 1.0] | +-+--+ overallCoverage(dataset, embeddingsCol): Calculates overall word coverage for the whole data in the embedded field. This returns a single coverage object considering all rows in the field. val wordsOverallCoverage = WordEmbeddingsModel.overallCoverage(wordsCoverage,&quot;embeddings&quot;).percentage 1.0 For extended examples of usage, see the Examples and the WordEmbeddingsTestSpec. Input Annotator Types: DOCUMENT, TOKEN Output Annotator Type: WORD_EMBEDDINGS Python API: WordEmbeddingsModel Scala API: WordEmbeddingsModel Source: WordEmbeddingsModel WordSegmenter ApproachModel Trains a WordSegmenter which tokenizes non-english or non-whitespace separated texts. Many languages are not whitespace separated and their sentences are a concatenation of many symbols, like Korean, Japanese or Chinese. Without understanding the language, splitting the words into their corresponding tokens is impossible. The WordSegmenter is trained to understand these languages and split them into semantically correct parts. This annotator is based on the paper Chinese Word Segmentation as Character Tagging [1]. Word segmentation is treated as a tagging problem. Each character is be tagged as on of four different labels: LL (left boundary), RR (right boundary), MM (middle) and LR (word by itself). The label depends on the position of the word in the sentence. LL tagged words will combine with the word on the right. Likewise, RR tagged words combine with words on the left. MM tagged words are treated as the middle of the word and combine with either side. LR tagged words are words by themselves. Example (from [1], Example 3(a) (raw), 3(b) (tagged), 3(c) (translation)): 上海 计划 到 本 世纪 末 实现 人均 国内 生产 总值 五千 美元 上/LL 海/RR 计/LL 划/RR 到/LR 本/LR 世/LL 纪/RR 末/LR 实/LL 现/RR 人/LL 均/RR 国/LL 内/RR 生/LL 产/RR 总/LL 值/RR 五/LL 千/RR 美/LL 元/RR Shanghai plans to reach the goal of 5,000 dollars in per capita GDP by the end of the century. For instantiated/pretrained models, see WordSegmenterModel. To train your own model, a training dataset consisting of Part-Of-Speech tags is required. The data has to be loaded into a dataframe, where the column is an Annotation of type &quot;POS&quot;. This can be set with setPosColumn. Tip: The helper class POS might be useful to read training data into data frames. For extended examples of usage, see the Examples and the WordSegmenterTest. References: [1] Xue, Nianwen. “Chinese Word Segmentation as Character Tagging.” International Journal of Computational Linguistics &amp; Chinese Language Processing, Volume 8, Number 1, February 2003: Special Issue on Word Formation and Chinese Language Processing, 2003, pp. 29-48. ACLWeb, https://aclanthology.org/O03-4002. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: WordSegmenterApproach Scala API: WordSegmenterApproach Source: WordSegmenterApproach Show Example PythonScala # In this example, `&quot;chinese_train.utf8&quot;` is in the form of # # 十|LL 四|RR 不|LL 是|RR 四|LL 十|RR # # and is loaded with the `POS` class to create a dataframe of `&quot;POS&quot;` type Annotations. import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) wordSegmenter = WordSegmenterApproach() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) .setPosColumn(&quot;tags&quot;) .setNIterations(5) pipeline = Pipeline().setStages([ documentAssembler, wordSegmenter ]) trainingDataSet = POS().readDataset( spark, &quot;src/test/resources/word-segmenter/chinese_train.utf8&quot; ) pipelineModel = pipeline.fit(trainingDataSet) // In this example, `&quot;chinese_train.utf8&quot;` is in the form of // // 十|LL 四|RR 不|LL 是|RR 四|LL 十|RR // // and is loaded with the `POS` class to create a dataframe of `&quot;POS&quot;` type Annotations. import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotators.ws.WordSegmenterApproach import com.johnsnowlabs.nlp.training.POS import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val wordSegmenter = new WordSegmenterApproach() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) .setPosColumn(&quot;tags&quot;) .setNIterations(5) val pipeline = new Pipeline().setStages(Array( documentAssembler, wordSegmenter )) val trainingDataSet = POS().readDataset( ResourceHelper.spark, &quot;src/test/resources/word-segmenter/chinese_train.utf8&quot; ) val pipelineModel = pipeline.fit(trainingDataSet) WordSegmenter which tokenizes non-english or non-whitespace separated texts. Many languages are not whitespace separated and their sentences are a concatenation of many symbols, like Korean, Japanese or Chinese. Without understanding the language, splitting the words into their corresponding tokens is impossible. The WordSegmenter is trained to understand these languages and plit them into semantically correct parts. This annotator is based on the paper Chinese Word Segmentation as Character Tagging. Word segmentation is treated as a tagging problem. Each character is be tagged as on of four different labels: LL (left boundary), RR (right boundary), MM (middle) and LR (word by itself). The label depends on the position of the word in the sentence. LL tagged words will combine with the word on the right. Likewise, RR tagged words combine with words on the left. MM tagged words are treated as the middle of the word and combine with either side. LR tagged words are words by themselves. Example (from [1], Example 3(a) (raw), 3(b) (tagged), 3(c) (translation)): 上海 计划 到 本 世纪 末 实现 人均 国内 生产 总值 五千 美元 上/LL 海/RR 计/LL 划/RR 到/LR 本/LR 世/LL 纪/RR 末/LR 实/LL 现/RR 人/LL 均/RR 国/LL 内/RR 生/LL 产/RR 总/LL 值/RR 五/LL 千/RR 美/LL 元/RR Shanghai plans to reach the goal of 5,000 dollars in per capita GDP by the end of the century. This is the instantiated model of the WordSegmenterApproach. For training your own model, please see the documentation of that class. Pretrained models can be loaded with pretrained of the companion object: val wordSegmenter = WordSegmenterModel.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;words_segmented&quot;) The default model is &quot;wordseg_pku&quot;, default language is &quot;zh&quot;, if no values are provided. For available pretrained models please see the Models Hub. For extended examples of usage, see the Examples and the WordSegmenterTest. References: [1] Xue, Nianwen. “Chinese Word Segmentation as Character Tagging.” International Journal of Computational Linguistics &amp; Chinese Language Processing, Volume 8, Number 1, February 2003: Special Issue on Word Formation and Chinese Language Processing, 2003, pp. 29-48. ACLWeb, https://aclanthology.org/O03-4002. Input Annotator Types: DOCUMENT Output Annotator Type: TOKEN Python API: WordSegmenterModel Scala API: WordSegmenterModel Source: WordSegmenterModel YakeKeywordExtraction Yake is an Unsupervised, Corpus-Independent, Domain and Language-Independent and Single-Document keyword extraction algorithm. Extracting keywords from texts has become a challenge for individuals and organizations as the information grows in complexity and size. The need to automate this task so that text can be processed in a timely and adequate manner has led to the emergence of automatic keyword extraction tools. Yake is a novel feature-based system for multi-lingual keyword extraction, which supports texts of different sizes, domain or languages. Unlike other approaches, Yake does not rely on dictionaries nor thesauri, neither is trained against any corpora. Instead, it follows an unsupervised approach which builds upon features extracted from the text, making it thus applicable to documents written in different languages without the need for further knowledge. This can be beneficial for a large number of tasks and a plethora of situations where access to training corpora is either limited or restricted. The algorithm makes use of the position of a sentence and token. Therefore, to use the annotator, the text should be first sent through a Sentence Boundary Detector and then a tokenizer. Note that each keyword will be given a keyword score greater than 0 (The lower the score better the keyword). Therefore to filter the keywords, an upper bound for the score can be set with setThreshold. For extended examples of usage, see the Examples and the YakeTestSpec. Sources : Campos, R., Mangaravite, V., Pasquali, A., Jatowt, A., Jorge, A., Nunes, C. and Jatowt, A. (2020). YAKE! Keyword Extraction from Single Documents using Multiple Local Features. In Information Sciences Journal. Elsevier, Vol 509, pp 257-289 Paper abstract: As the amount of generated information grows, reading and summarizing texts of large collections turns into a challenging task. Many documents do not come with descriptive terms, thus requiring humans to generate keywords on-the-fly. The need to automate this kind of task demands the development of keyword extraction systems with the ability to automatically identify keywords within the text. One approach is to resort to machine-learning algorithms. These, however, depend on large annotated text corpora, which are not always available. An alternative solution is to consider an unsupervised approach. In this article, we describe YAKE!, a light-weight unsupervised automatic keyword extraction method which rests on statistical text features extracted from single documents to select the most relevant keywords of a text. Our system does not need to be trained on a particular set of documents, nor does it depend on dictionaries, external corpora, text size, language, or domain. To demonstrate the merits and significance of YAKE!, we compare it against ten state-of-the-art unsupervised approaches and one supervised method. Experimental results carried out on top of twenty datasets show that YAKE! significantly outperforms other unsupervised methods on texts of different sizes, languages, and domains. Input Annotator Types: TOKEN Output Annotator Type: CHUNK Python API: YakeKeywordExtraction Scala API: YakeKeywordExtraction Source: YakeKeywordExtraction Show Example PythonScala import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;sentence&quot;) token = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) .setContextChars([&quot;(&quot;, &quot;]&quot;, &quot;?&quot;, &quot;!&quot;, &quot;.&quot;, &quot;,&quot;]) keywords = YakeKeywordExtraction() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;keywords&quot;) .setThreshold(0.6) .setMinNGrams(2) .setNKeywords(10) pipeline = Pipeline().setStages([ documentAssembler, sentenceDetector, token, keywords ]) data = spark.createDataFrame([[ &quot;Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud Next conference in San Francisco this week, the official announcement could come as early as tomorrow. Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. Google itself declined &#39;to comment on rumors&#39;. Kaggle, which has about half a million data scientists on its platform, was founded by Goldbloom and Ben Hamner in 2010. The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, it has managed to stay well ahead of them by focusing on its specific niche. The service is basically the de facto home for running data science and machine learning competitions. With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow and other projects). Kaggle has a bit of a history with Google, too, but that&#39;s pretty recent. Earlier this month, Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google will keep the service running - likely under its current name. While the acquisition is probably more about Kaggle&#39;s community than technology, Kaggle did build some interesting tools for hosting its competition and &#39;kernels&#39;, too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can share this code on the platform (the company previously called them &#39;scripts&#39;). Like similar competition-centric sites, Kaggle also runs a job board, too. It&#39;s unclear what Google will do with that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it&#39;s $12.75) since its launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, NaRavikant, Google chie economist Hal Varian, Khosla Ventures and Yuri Milner&quot; ]]).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) # combine the result and score (contained in keywords.metadata) scores = result .selectExpr(&quot;explode(arrays_zip(keywords.result, keywords.metadata)) as resultTuples&quot;) .selectExpr(&quot;resultTuples[&#39;0&#39;] as keyword&quot;, &quot;resultTuples[&#39;1&#39;].score as score&quot;) # Order ascending, as lower scores means higher importance scores.orderBy(&quot;score&quot;).show(5, truncate = False) ++-+ |keyword |score | ++-+ |google cloud |0.32051516486864573| |google cloud platform|0.37786450577630676| |ceo anthony goldbloom|0.39922830978423146| |san francisco |0.40224744669493756| |anthony goldbloom |0.41584827825302534| ++-+ import spark.implicits._ import com.johnsnowlabs.nlp.base.DocumentAssembler import com.johnsnowlabs.nlp.annotator.{SentenceDetector, Tokenizer} import com.johnsnowlabs.nlp.annotators.keyword.yake.YakeKeywordExtraction import org.apache.spark.ml.Pipeline val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence&quot;) val token = new Tokenizer() .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;token&quot;) .setContextChars(Array(&quot;(&quot;, &quot;)&quot;, &quot;?&quot;, &quot;!&quot;, &quot;.&quot;, &quot;,&quot;)) val keywords = new YakeKeywordExtraction() .setInputCols(&quot;token&quot;) .setOutputCol(&quot;keywords&quot;) .setThreshold(0.6f) .setMinNGrams(2) .setNKeywords(10) val pipeline = new Pipeline().setStages(Array( documentAssembler, sentenceDetector, token, keywords )) val data = Seq( &quot;Sources tell us that Google is acquiring Kaggle, a platform that hosts data science and machine learning competitions. Details about the transaction remain somewhat vague, but given that Google is hosting its Cloud Next conference in San Francisco this week, the official announcement could come as early as tomorrow. Reached by phone, Kaggle co-founder CEO Anthony Goldbloom declined to deny that the acquisition is happening. Google itself declined &#39;to comment on rumors&#39;. Kaggle, which has about half a million data scientists on its platform, was founded by Goldbloom and Ben Hamner in 2010. The service got an early start and even though it has a few competitors like DrivenData, TopCoder and HackerRank, it has managed to stay well ahead of them by focusing on its specific niche. The service is basically the de facto home for running data science and machine learning competitions. With Kaggle, Google is buying one of the largest and most active communities for data scientists - and with that, it will get increased mindshare in this community, too (though it already has plenty of that thanks to Tensorflow and other projects). Kaggle has a bit of a history with Google, too, but that&#39;s pretty recent. Earlier this month, Google and Kaggle teamed up to host a $100,000 machine learning competition around classifying YouTube videos. That competition had some deep integrations with the Google Cloud Platform, too. Our understanding is that Google will keep the service running - likely under its current name. While the acquisition is probably more about Kaggle&#39;s community than technology, Kaggle did build some interesting tools for hosting its competition and &#39;kernels&#39;, too. On Kaggle, kernels are basically the source code for analyzing data sets and developers can share this code on the platform (the company previously called them &#39;scripts&#39;). Like similar competition-centric sites, Kaggle also runs a job board, too. It&#39;s unclear what Google will do with that part of the service. According to Crunchbase, Kaggle raised $12.5 million (though PitchBook says it&#39;s $12.75) since its launch in 2010. Investors in Kaggle include Index Ventures, SV Angel, Max Levchin, Naval Ravikant, Google chief economist Hal Varian, Khosla Ventures and Yuri Milner&quot; ).toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) // combine the result and score (contained in keywords.metadata) val scores = result .selectExpr(&quot;explode(arrays_zip(keywords.result, keywords.metadata)) as resultTuples&quot;) .select($&quot;resultTuples.0&quot; as &quot;keyword&quot;, $&quot;resultTuples.1.score&quot;) // Order ascending, as lower scores means higher importance scores.orderBy(&quot;score&quot;).show(5, truncate = false) ++-+ |keyword |score | ++-+ |google cloud |0.32051516486864573| |google cloud platform|0.37786450577630676| |ceo anthony goldbloom|0.39922830978423146| |san francisco |0.40224744669493756| |anthony goldbloom |0.41584827825302534| ++-+",
    "url": "/docs/en/annotators",
    "relUrl": "/docs/en/annotators"
  },
  "5": {
    "id": "5",
    "title": "Helper functions",
    "content": "Spark NLP Annotation functions The functions presented here help users manipulate annotations, by providing both UDFs and dataframe utilities to deal with them more easily Python In python, the functions are straight forward and have both UDF and Dataframe applications map_annotations(f, output_type: DataType) UDF that applies f(). Requires output DataType from pyspark.sql.types map_annotations_strict(f) UDF that apples an f() method that returns a list of Annotations map_annotations_col(dataframe: DataFrame, f, column: str, output_column: str, annotatyon_type: str, output_type: DataType = Annotation.arrayType()) applies f() to column from dataframe map_annotations_cols(dataframe: DataFrame, f, columns: str, output_column: str, annotatyon_type: str, output_type: DataType = Annotation.arrayType()) applies f() to columns from dataframe filter_by_annotations_col(dataframe, f, column) applies a boolean filter f() to column from dataframe explode_annotations_col(dataframe: DataFrame, column, output_column) explodes annotation column from dataframe Scala In Scala, importing inner functions brings implicits that allow these functions to be applied directly on top of the dataframe mapAnnotations(function: Seq[Annotation] =&gt; T, outputType: DataType) mapAnnotationsStrict(function: Seq[Annotation] =&gt; Seq[Annotation]) mapAnnotationsCol[T: TypeTag](column: String, outputCol: String,annotatorType: String, function: Seq[Annotation] =&gt; T) mapAnnotationsCol[T: TypeTag](cols: Seq[String], outputCol: String,annotatorType: String, function: Seq[Annotation] =&gt; T) eachAnnotationsCol[T: TypeTag](column: String, function: Seq[Annotation] =&gt; Unit) def explodeAnnotationsCol[T: TypeTag](column: String, outputCol: String) Imports: PythonScala from sparknlp.functions import * from sparknlp.annotation import Annotation import com.johnsnowlabs.nlp.functions._ import com.johnsnowlabs.nlp.Annotation Examples: Complete usage examples can be seen here: https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/234-release-candidate/jupyter/annotation/english/spark-nlp-basics/spark-nlp-basics-functions.ipynb PythonScala def my_annoation_map_function(annotations): return list(map(lambda a: Annotation( &#39;my_own_type&#39;, a.begin, a.end, a.result, {&#39;my_key&#39;: &#39;custom_annotation_data&#39;}, []), annotations)) result.select( map_annotations(my_annoation_map_function, Annotation.arrayType())(&#39;token&#39;) ).toDF(&quot;my output&quot;).show(truncate=False) val modified = data.mapAnnotationsCol(&quot;pos&quot;, &quot;mod_pos&quot;,&quot;pos&quot; ,(_: Seq[Annotation]) =&gt; { &quot;hello world&quot; })",
    "url": "/docs/en/auxiliary",
    "relUrl": "/docs/en/auxiliary"
  },
  "6": {
    "id": "6",
    "title": "Classify Documents - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/classify_documents",
    "relUrl": "/classify_documents"
  },
  "7": {
    "id": "7",
    "title": "Spark NLP - General Concepts",
    "content": "Concepts Spark ML provides a set of Machine Learning applications that can be build using two main components: Estimators and Transformers. The Estimators have a method called fit() which secures and trains a piece of data to such application. The Transformer is generally the result of a fitting process and applies changes to the the target dataset. These components have been embedded to be applicable to Spark NLP. Pipelines are a mechanism for combining multiple estimators and transformers in a single workflow. They allow multiple chained transformations along a Machine Learning task. For more information please refer to Spark ML library. Annotation The basic result of a Spark NLP operation is an annotation. It’s structure includes: annotatorType: the type of annotator that generated the current annotation begin: the begin of the matched content relative to raw-text end: the end of the matched content relative to raw-text result: the main output of the annotation metadata: content of matched result and additional information embeddings: (new in 2.0) contains vector mappings if required This object is automatically generated by annotators after a transform process. No manual work is required. However, it is important to clearly understand the structure of an annotation to be able too efficiently use it. Annotators Annotators are the spearhead of NLP functions in Spark NLP. There are two forms of annotators: Annotator Approaches: are those who represent a Spark ML Estimator and require a training stage. They have a function called fit(data) which trains a model based on some data. They produce the second type of annotator which is an annotator model or transformer. Annotator Models: are spark models or transformers, meaning they have a transform(data) function. This function takes as input a dataframe to which it adds a new column containing the result of the current annotation. All transformers are additive, meaning they append to current data, never replace or delete previous information. Both forms of annotators can be included in a Pipeline. All annotators included in a Pipeline will be automatically executed in the defined order and will transform the data accordingly. A Pipeline is turned into a PipelineModel after the fit() stage. The Pipeline can be saved to disk and re-loaded at any time. Common Functions setInputCols(column_names): Takes a list of column names of annotations required by this annotator. Those are generated by the annotators which precede the current annotator in the pipeline. setOutputCol(column_name): Defines the name of the column containing the result of the current annotator. Use this name as an input for other annotators down the pipeline requiring the outputs generated by the current annotator. Quickly annotate some text You can run these examples using Python or Scala. The easiest way to run the python examples is by starting a pyspark jupyter notebook including the spark-nlp package: $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.7 -y $ conda activate sparknlp # spark-nlp by default is based on pyspark 3.x $ pip install spark-nlp==5.0.2 pyspark==3.3.1 jupyter $ jupyter notebook Explain Document ML Spark NLP offers a variety of pretrained pipelines that will help you get started, and get a sense of how the library works. We are constantly working on improving the available content. You can checkout a demo application of the Explain Document ML pipeline here: View Demo Downloading and using a pretrained pipeline Explain Document ML (explain_document_ml) is a pretrained pipeline that does a little bit of everything NLP related. Let’s try it out in scala. Note that the first time you run the below code it might take longer since it downloads the pretrained pipeline from our servers! PythonScala import sparknlp sparknlp.start() from sparknlp.pretrained import PretrainedPipeline explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) annotations = explain_document_pipeline.annotate(&quot;We are very happy about SparkNLP&quot;) print(annotations) OUTPUT: { &#39;stem&#39;: [&#39;we&#39;, &#39;ar&#39;, &#39;veri&#39;, &#39;happi&#39;, &#39;about&#39;, &#39;sparknlp&#39;], &#39;checked&#39;: [&#39;We&#39;, &#39;are&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], &#39;lemma&#39;: [&#39;We&#39;, &#39;be&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], &#39;document&#39;: [&#39;We are very happy about SparkNLP&#39;], &#39;pos&#39;: [&#39;PRP&#39;, &#39;VBP&#39;, &#39;RB&#39;, &#39;JJ&#39;, &#39;IN&#39;, &#39;NNP&#39;], &#39;token&#39;: [&#39;We&#39;, &#39;are&#39;, &#39;very&#39;, &#39;happy&#39;, &#39;about&#39;, &#39;SparkNLP&#39;], &#39;sentence&#39;: [&#39;We are very happy about SparkNLP&#39;] } import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) OUTPUT: explain_document_ml download started this may take some time. Approximate size to download 9.4 MB Download done! Loading the resource. explain_document_pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_ml,en,public/models) val annotations = explainDocumentPipeline.annotate(&quot;We are very happy about SparkNLP&quot;) println(annotations) OUTPUT: Map( stem -&gt; List(we, ar, veri, happi, about, sparknlp), checked -&gt; List(We, are, very, happy, about, SparkNLP), lemma -&gt; List(We, be, very, happy, about, SparkNLP), document -&gt; List(We are very happy about SparkNLP), pos -&gt; ArrayBuffer(PRP, VBP, RB, JJ, IN, NNP), token -&gt; List(We, are, very, happy, about, SparkNLP), sentence -&gt; List(We are very happy about SparkNLP) ) As you can see the explain_document_ml is able to annotate any “document” providing as output a list of stems, check-spelling, lemmas, part of speech tags, tokens and sentence boundary detection and all this “out-of-the-box”!. Using a pretrained pipeline with spark dataframes You can also use the pipeline with a spark dataframe. You just need to create first a spark dataframe with a column named “text” that will work as the input for the pipeline and then use the .transform() method to run the pipeline over that dataframe and store the outputs of the different components in a spark dataframe. Remember than when starting jupyter notebook from pyspark or when running the spark-shell for scala, a Spark Session is started in the background by default within the namespace ‘scala’. PythonScala import sparknlp sparknlp.start() sentences = [ [&#39;Hello, this is an example sentence&#39;], [&#39;And this is a second sentence.&#39;] ] # spark is the Spark Session automatically started by pyspark. data = spark.createDataFrame(sentences).toDF(&quot;text&quot;) # Download the pretrained pipeline from Johnsnowlab&#39;s servers explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) OUTPUT: explain_document_ml download started this may take some time. Approx size to download 9.4 MB [OK!] # Transform &#39;data&#39; and store output in a new &#39;annotations_df&#39; dataframe annotations_df = explain_document_pipeline.transform(data) # Show the results annotations_df.show() OUTPUT: +--+--+--+--+--+--+--+--+ | text| document| sentence| token| checked| lemma| stem| pos| +--+--+--+--+--+--+--+--+ |Hello, this is an...|[[document, 0, 33...|[[document, 0, 33...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, he...|[[pos, 0, 4, UH, ...| |And this is a sec...|[[document, 0, 29...|[[document, 0, 29...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, an...|[[pos, 0, 2, CC, ...| +--+--+--+--+--+--+--+--+ val data = Seq( &quot;Hello, this is an example sentence&quot;, &quot;And this is a second sentence&quot;) .toDF(&quot;text&quot;) data.show(truncate=false) OUTPUT: ++ |text | ++ |Hello, this is an example set | |And this is a second sentence.| ++ val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) val annotations_df = explainDocumentPipeline.transform(data) annotations_df.show() OUTPUT: +--+--+--+--+--+--+--+--+ | text| document| sentence| token| checked| lemma| stem| pos| +--+--+--+--+--+--+--+--+ |Hello, this is an...|[[document, 0, 33...|[[document, 0, 33...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, He...|[[token, 0, 4, he...|[[pos, 0, 4, UH, ...| |And this is a sec...|[[document, 0, 29...|[[document, 0, 29...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, An...|[[token, 0, 2, an...|[[pos, 0, 2, CC, ...| +--+--+--+--+--+--+--+--+ Manipulating pipelines The output of the previous DataFrame was in terms of Annotation objects. This output is not really comfortable to deal with, as you can see by running the code: PythonScala annotations_df.select(&quot;token&quot;).show(truncate=False) OUTPUT: +--+ |token | +--+ |[[token, 0, 4, Hello, [sentence -&gt; 0], [], []], [token, 5, 5, ,, [sentence -&gt; 0], [], []], [token, 7, 10, this, [sentence -&gt; 0], [], []], [token, 12, 13, is, [sentence -&gt; 0], [], []], [token, 15, 16, an, [sentence -&gt; 0], [], []], [token, 18, 24, example, [sentence -&gt; 0], [], []], [token, 26, 33, sentence, [sentence -&gt; 0], [], []]]| |[[token, 0, 2, And, [sentence -&gt; 0], [], []], [token, 4, 7, this, [sentence -&gt; 0], [], []], [token, 9, 10, is, [sentence -&gt; 0], [], []], [token, 12, 12, a, [sentence -&gt; 0], [], []], [token, 14, 19, second, [sentence -&gt; 0], [], []], [token, 21, 28, sentence, [sentence -&gt; 0], [], []], [token, 29, 29, ., [sentence -&gt; 0], [], []]] | +--+ annotations_df.select(&quot;token&quot;).show(truncate=false) OUTPUT: +--+ |token | +--+ |[[token, 0, 4, Hello, [sentence -&gt; 0], [], []], [token, 5, 5, ,, [sentence -&gt; 0], [], []], [token, 7, 10, this, [sentence -&gt; 0], [], []], [token, 12, 13, is, [sentence -&gt; 0], [], []], [token, 15, 16, an, [sentence -&gt; 0], [], []], [token, 18, 24, example, [sentence -&gt; 0], [], []], [token, 26, 33, sentence, [sentence -&gt; 0], [], []]]| |[[token, 0, 2, And, [sentence -&gt; 0], [], []], [token, 4, 7, this, [sentence -&gt; 0], [], []], [token, 9, 10, is, [sentence -&gt; 0], [], []], [token, 12, 12, a, [sentence -&gt; 0], [], []], [token, 14, 19, second, [sentence -&gt; 0], [], []], [token, 21, 28, sentence, [sentence -&gt; 0], [], []], [token, 29, 29, ., [sentence -&gt; 0], [], []]] | +--+ What if we want to deal with just the resulting annotations? We can use the Finisher annotator, retrieve the Explain Document ML pipeline, and add them together in a Spark ML Pipeline. Remember that pretrained pipelines expect the input column to be named “text”. PythonScala from sparknlp import Finisher from pyspark.ml import Pipeline from sparknlp.pretrained import PretrainedPipeline finisher = Finisher().setInputCols([&quot;token&quot;, &quot;lemmas&quot;, &quot;pos&quot;]) explain_pipeline_model = PretrainedPipeline(&quot;explain_document_ml&quot;).model pipeline = Pipeline() .setStages([ explain_pipeline_model, finisher ]) sentences = [ [&#39;Hello, this is an example sentence&#39;], [&#39;And this is a second sentence.&#39;] ] data = spark.createDataFrame(sentences).toDF(&quot;text&quot;) model = pipeline.fit(data) annotations_finished_df = model.transform(data) annotations_finished_df.select(&#39;finished_token&#39;).show(truncate=False) OUTPUT: +-+ |finished_token | +-+ |[Hello, ,, this, is, an, example, sentence]| |[And, this, is, a, second, sentence, .] | +-+ scala&gt; import com.johnsnowlabs.nlp.Finisher scala&gt; import org.apache.spark.ml.Pipeline scala&gt; val finisher = new Finisher().setInputCols(&quot;token&quot;, &quot;lemma&quot;, &quot;pos&quot;) scala&gt; val explainPipelineModel = PretrainedPipeline(&quot;explain_document_ml&quot;).model scala&gt; val pipeline = new Pipeline(). setStages(Array( explainPipelineModel, finisher )) scala&gt; val data = Seq( &quot;Hello, this is an example sentence&quot;, &quot;And this is a second sentence&quot;) .toDF(&quot;text&quot;) scala&gt; val model = pipeline.fit(data) scala&gt; val annotations_df = model.transform(data) scala&gt; annotations_df.select(&quot;finished_token&quot;).show(truncate=false) OUTPUT: +-+ |finished_token | +-+ |[Hello, ,, this, is, an, example, sentence]| |[And, this, is, a, second, sentence, .] | +-+ Setup your own pipeline Annotator types Every annotator has a type. Those annotators that share a type, can be used interchangeably, meaning you could use any of them when needed. For example, when a token type annotator is required by another annotator, such as a sentiment analysis annotator, you can either provide a normalized token or a lemma, as both are of type token. Necessary imports Since version 1.5.0 we are making necessary imports easy to reach, base._ will include general Spark NLP transformers and concepts, while annotator._ will include all annotators that we currently provide. We also need Spark ML pipelines. PythonScala from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ import org.apache.spark.ml.Pipeline DocumentAssembler: Getting data in In order to get through the NLP process, we need to get raw data annotated. There is a special transformer that does this for us: the DocumentAssembler, it creates the first annotation of type Document which may be used by annotators down the road. PythonScala documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val documentAssembler = new DocumentAssembler(). setInputCol(&quot;text&quot;). setOutputCol(&quot;document&quot;) Sentence detection and tokenization In this quick example, we now proceed to identify the sentences in the input document. SentenceDetector requires a Document annotation, which is provided by the DocumentAssembler output, and it’s itself a Document type token. The Tokenizer requires a Document annotation type. That means it works both with DocumentAssembler or SentenceDetector output. In the following example we use the sentence output. PythonScala sentenceDetector = SentenceDetector() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;Sentence&quot;) regexTokenizer = Tokenizer() .setInputCols([&quot;sentence&quot;]) .setOutputCol(&quot;token&quot;) val sentenceDetector = new SentenceDetector(). setInputCols(Array(&quot;document&quot;)). setOutputCol(&quot;sentence&quot;) val regexTokenizer = new Tokenizer(). setInputCols(Array(&quot;sentence&quot;)). setOutputCol(&quot;token&quot;) Spark NLP also includes another special transformer, called Finisher to show tokens in a human language. finisher = Finisher() .setInputCols([&quot;token&quot;]) .setCleanAnnotations(False) val finisher = new Finisher(). setInputCols(&quot;token&quot;). setCleanAnnotations(false) Finisher: Getting data out At the end of each pipeline or any stage that was done by Spark NLP, you may want to get results out whether onto another pipeline or simply write them on disk. The Finisher annotator helps you to clean the metadata (if it’s set to true) and output the results into an array: PythonScala finisher = Finisher() .setInputCols([&quot;token&quot;]) .setIncludeMetadata(True) val finisher = new Finisher() .setInputCols(&quot;token&quot;) .setIncludeMetadata(true) If you need to have a flattened DataFrame (each sub-array in a new column) from any annotations other than struct type columns, you can use explode function from Spark SQL. You can also use Apache Spark functions (SQL) to manipulate the output DataFrame in any way you need. Here we combine the tokens and NER results together: import pyspark.sql.functions as F df.withColumn(&quot;tmp&quot;, F.explode(&quot;chunk&quot;)).select(&quot;tmp.*&quot;) finisher.withColumn(&quot;newCol&quot;, explode(arrays_zip($&quot;finished_token&quot;, $&quot;finished_ner&quot;))) import org.apache.spark.sql.functions._ df.withColumn(&quot;tmp&quot;, explode(col(&quot;chunk&quot;))).select(&quot;tmp.*&quot;) Using Spark ML Pipeline Now we want to put all this together and retrieve the results, we use a Pipeline for this. We use the same data in fit() that we will use in transform since none of the pipeline stages have a training stage. PythonScala pipeline = Pipeline() .setStages([ documentAssembler, sentenceDetector, regexTokenizer, finisher ]) OUTPUT: +-+ |finished_token | +-+ |[hello, ,, this, is, an, example, sentence]| +-+ val pipeline = new Pipeline(). setStages(Array( documentAssembler, sentenceDetector, regexTokenizer, finisher )) val data = Seq(&quot;hello, this is an example sentence&quot;).toDF(&quot;text&quot;) val annotations = pipeline. fit(data). transform(data).toDF(&quot;text&quot;)) annotations.select(&quot;finished_token&quot;).show(truncate=false) OUTPUT: +-+ |finished_token | +-+ |[hello, ,, this, is, an, example, sentence]| +-+ Using Spark NLP’s LightPipeline LightPipeline is a Spark NLP specific Pipeline class equivalent to Spark ML Pipeline. The difference is that it’s execution does not hold to Spark principles, instead it computes everything locally (but in parallel) in order to achieve fast results when dealing with small amounts of data. This means, we do not input a Spark Dataframe, but a string or an Array of strings instead, to be annotated. To create Light Pipelines, you need to input an already trained (fit) Spark ML Pipeline. It’s transform() stage is converted into annotate() instead. PythonScala from sparknlp.base import LightPipeline explain_document_pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) lightPipeline = LightPipeline(explain_document_pipeline.model) OUTPUT: explain_document_ml download started this may take some time. Approx size to download 9.4 MB [OK!] lightPipeline.annotate(&quot;Hello world, please annotate my text&quot;) OUTPUT: {&#39;stem&#39;: [&#39;hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;pleas&#39;, &#39;annot&#39;, &#39;my&#39;, &#39;text&#39;], &#39;checked&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;my&#39;, &#39;text&#39;], &#39;lemma&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;i&#39;, &#39;text&#39;], &#39;document&#39;: [&#39;Hello world, please annotate my text&#39;], &#39;pos&#39;: [&#39;UH&#39;, &#39;NN&#39;, &#39;,&#39;, &#39;VB&#39;, &#39;NN&#39;, &#39;PRP$&#39;, &#39;NN&#39;], &#39;token&#39;: [&#39;Hello&#39;, &#39;world&#39;, &#39;,&#39;, &#39;please&#39;, &#39;annotate&#39;, &#39;my&#39;, &#39;text&#39;], &#39;sentence&#39;: [&#39;Hello world, please annotate my text&#39;]} import com.johnsnowlabs.nlp.base._ val explainDocumentPipeline = PretrainedPipeline(&quot;explain_document_ml&quot;) val lightPipeline = new LightPipeline(explainDocumentPipeline.model) lightPipeline.annotate(&quot;Hello world, please annotate my text&quot;) OUTPUT: Map[String,Seq[String]] = Map( stem -&gt; List(hello, world, ,, pleas, annot, my, text), checked -&gt; List(Hello, world, ,, please, annotate, my, tex), lemma -&gt; List(Hello, world, ,, please, annotate, i, text), document -&gt; List(Hello world, please annotate my text), pos -&gt; ArrayBuffer(UH, NN, ,, VB, NN, PRP$, NN), token -&gt; List(Hello, world, ,, please, annotate, my, text), sentence -&gt; List(Hello world, please annotate my text) ) Training annotators Training methodology Training your own annotators is a key concept when dealing with real life scenarios. Any of the annotators provided above, such as pretrained pipelines and models, can be applied out-of-the-box to a specific use case, but better results are obtained when they are fine-tuned to your specific use-case. Dealing with real life problems ofter requires training your own models. In Spark NLP, we support three ways of training a custom annotator: Train from a dataset. Most annotators are capable of training from a dataset passed to fit() method just as Spark ML does. Annotators that use the suffix Approach are such trainable annotators. Training from fit() is the standard behavior in Spark ML. Annotators have different schema requirements for training. Check the reference to see what are the requirements of each annotators. Training from an external source: Some of our annotators train from an external file or folder passed to the annotator as a param. You will see such ones as setCorpus() or setDictionary() param setter methods, allowing you to configure the input to use. You can set Spark NLP to read them as Spark datasets or LINE_BY_LINE which is usually faster for small files. Last but not least, some of our annotators are Deep Learning based. These models may be trained with the standard AnnotatorApproach API just like any other annotator. For more advanced users, we also allow importing your own graphs or even training from Python and converting them into an AnnotatorModel. Spark NLP Imports base includes general Spark NLP transformers and concepts, annotator includes all annotators that we currently provide, embeddings includes word embedding annotators. Example: PythonScala from sparknlp.base import * from sparknlp.annotator import * from sparknlp.embeddings import * import com.johnsnowlabs.nlp.base._ import com.johnsnowlabs.nlp.annotator._ Spark ML Pipelines SparkML Pipelines are a uniform structure that helps creating and tuning practical machine learning pipelines. Spark NLP integrates with them seamlessly so it is important to have this concept handy. Once a Pipeline is trained with fit(), it becomes a PipelineModel Example: PythonScala from pyspark.ml import Pipeline pipeline = Pipeline().setStages([...]) import org.apache.spark.ml.Pipeline new Pipeline().setStages(Array(...)) LightPipeline LightPipelines are Spark ML pipelines converted into a single machine but multithreaded task, becoming more than 10x times faster for smaller amounts of data (small is relative, but 50k sentences is roughly a good maximum). To use them, simply plug in a trained (fitted) pipeline. Example: PythonScala from sparknlp.base import LightPipeline LightPipeline(someTrainedPipeline).annotate(someStringOrArray) import com.johnsnowlabs.nlp.LightPipeline new LightPipeline(somePipelineModel).annotate(someStringOrArray)) Functions: annotate(string or string[]): returns dictionary list of annotation results fullAnnotate(string or string[]): returns dictionary list of entire annotations content For more details please refer to Using Spark NLP’s LightPipelines. RecursivePipeline Recursive pipelines are SparkNLP specific pipelines that allow a Spark ML Pipeline to know about itself on every Pipeline Stage task, allowing annotators to utilize this same pipeline against external resources to process them in the same way the user decides. Only some of our annotators take advantage of this. RecursivePipeline behaves exactly the same as normal Spark ML pipelines, so they can be used with the same intention. Example: PythonScala from sparknlp.annotator import * recursivePipeline = RecursivePipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, lemmatizer, finisher ]) import com.johnsnowlabs.nlp.RecursivePipeline val recursivePipeline = new RecursivePipeline() .setStages(Array( documentAssembler, sentenceDetector, tokenizer, lemmatizer, finisher )) Params and Features Annotator parameters SparkML uses ML Params to store pipeline parameter maps. In SparkNLP, we also use Features, which are a way to store parameter maps that are larger than just a string or a boolean. These features are serialized as either Parquet or RDD objects, allowing much faster and scalable annotator information. Features are also broadcasted among executors for better performance.",
    "url": "/docs/en/concepts",
    "relUrl": "/docs/en/concepts"
  },
  "8": {
    "id": "8",
    "title": "Contribute",
    "content": "Refer to our GitHub page to take a look at the GH Issues, as the project is yet small. You can create in there your own issues to either work on them yourself or simply propose them. Feel free to clone the repository locally and submit pull requests so we can review them and work together. feedback, ideas and bug reports testing and development training and testing nlp corpora documentation and research Help is always welcome, for any further questions, contact nlp@johnsnowlabs.com. Your own annotator model Creating your first annotator transformer should not be hard, here are a few guidelines to get you started. Lets assume we want a wrapper annotator, which puts a character surrounding tokens provided by a Tokenizer WordWrapper uid is utilized for transformer serialization, AnnotatorModel[MyAnnotator] will contain the common annotator logic We need to use standard constructor for java and python compatibility class WordWrapper(override val uid: String) extends AnnotatorModel[WordWrapper] { def this() = this(Identifiable.randomUID(&quot;WORD_WRAPPER&quot;)) } Annotator attributes This annotator is not flexible if we don’t provide parameters import com.johnsnowlabs.nlp.AnnotatorType._ override val annotatorType: AnnotatorType = TOKEN override val requiredAnnotatorTypes: Array[AnnotatorType] = Array[AnnotatorType](TOKEN) Annotator parameters This annotator is not flexible if we don’t provide parameters protected val character: Param[String] = new Param(this, &quot;character&quot;, &quot;this is the character used to wrap a token&quot;) def setCharacter(value: String): this.type = set(pattern, value) def getCharacter: String = $(pattern) setDefault(character, &quot;@&quot;) Annotator logic Here is how we act, annotations will automatically provide our required annotations We generally use annotatorType for metadata keys override def annotate(annotations: Seq[Annotation]): Seq[Annotation] = { annotations.map(annotation =&gt; { Annotation( annotatorType, annotation.begin, annotation.end, Map(annotatorType -&gt; $(character) + annotation.result + $(character)) }) }",
    "url": "/contribute",
    "relUrl": "/contribute"
  },
  "9": {
    "id": "9",
    "title": "Databricks Solution Accelerators",
    "content": "",
    "url": "/databricks_solution_accelerators",
    "relUrl": "/databricks_solution_accelerators"
  },
  "10": {
    "id": "10",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/demo",
    "relUrl": "/demo"
  },
  "11": {
    "id": "11",
    "title": "Spark NLP in Action",
    "content": "",
    "url": "/demos",
    "relUrl": "/demos"
  },
  "12": {
    "id": "12",
    "title": "Detect Sentiment & Emotion - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/detect_sentiment_emotion",
    "relUrl": "/detect_sentiment_emotion"
  },
  "13": {
    "id": "13",
    "title": "Spark NLP - Developers Guideline",
    "content": "Spark NLP is an open-source library and everyone’s contribution is welcome! In this section we provide a guide on how to setup your environment using IntelliJ IDEA for a smoother start. You can also check our video tutorials available on our YouTube channel: https://www.youtube.com/johnsnowlabs Setting up the Environment Import to IntelliJ IDEA Setup Spark NLP development environment. This section will cover library set up for IntelliJ IDEA. Before you begin, make sure what you have Java and Spark installed in your system. We suggest that you have installed jdk 8 and Apache Spark 2.4.x. To check installation run: java -version and spark-submit --version Next step is to open IntelliJ IDEA. On the Welcome to IntelliJ IDEA screen you will see ability to Check out from Version Controle. Log in into your github account in pop up. After select from a list Spark NLP repo url: https://github.com/JohnSnowLabs/spark-nlp and press clone button. If you don’t see url in the list, clone or fork repo first to your Github account and try again. When the repo cloned IDE will detect SBT file with dependencies. Click Yes to start import from sbt. In the Import from sbt pop up make sure you have JDK 8 detected. Click Ok to proceed and download required resources. If you already had dependences installed you may see the pop up Not empty folder, click Ok to ignore it and reload resources. IntelliJ IDEA will be open and it will start syncing SBT project. It make take some time, you will see the progress in the build output panel in the bottom of the screen. To see the project panel in the left press Alt+1. Next step is to install Python plugin to the IntelliJ IDEA. To do this, open File -&gt; Settings -&gt; Plugins, type Python in the search and select Python plugin by JetBrains. Install this plugin by clicking Install button. After this steps you can check project structure in the File -&gt; Project Structure -&gt; Modules. Make sure what you have spark-nlp and spark-nlp-build folders and no errors in the exported dependencies. In the Project settings check what project SDK is set to 1.8 and in Platform Settings -&gt; SDK&#39;s you have Java installation as well as Python installation. If you don’t see Python installed in the SDK&#39;s tab click + button, add Python SDK with new virtual environment in the project folder with Python 3.x. Compiling, assembly and unit testing Run tests in Scala Click Add configuration in the Top right corner. In the pop up click on the + and look for sbt task. In the Name field put Test. In the Tasks field write down test. After you can disable checkbox in Use sbt shell to have more custom configurations. In the VM parameters increase the memory by changing -Xmx1024M to -Xmx10G and click Ok. If everything was set up correctly you suhould see unabled green button Run ‘Test’ in the top right. Click on it to start running the tests. This algorithm will Run all tests under spark-nlp/src/test/scala/com.johnsnowlabs/ Copy tasks After you created task, click Edit configuration. Select target task and instead of + button you can click copy in the same menu. It will recreate all settings from parent task and create a new task. You can do it for Scala or for Python tasks. Run individual tests Open test file you want to run. For example, spark-nlp/src/test/scala/com.johnsnowlabs/nlp/FinisherTestSpec.scala. Right click on the class name and select Copy reference. It will copy to you buffer classpath - com.johnsnowlabs.nlp.FinisherTestSpec. Copy existing Scala task and Name it as FinisherTest. In the Tasks field write down &quot;testOnly *classpath*&quot; -&gt; &quot;testOnly com.johnsnowlabs.nlp.FinisherTestSpec&quot; and click Ok to save individual scala test run configuration. Press play button to run individual test. Debugging tests To run tests in debug mode click Debug button (next to play button). In this mode task will stop at the given break points. Run tests in Python To run Python test, first you need to configure project structure. Go to File -&gt; Project Settings -&gt; Modules, click on the + button and select New Module. In the pop up choose Python on left menu, select Python SDK from created virtual environment and click Next. Enter python in the Module name and click Finish. After you need to add Spark dependencies. Select created Python module and click on the + button in the Dependencies part. Choose Jars or directories… and find the find installation path of spark (usually the folder name is spark-2.4.5-bin-hadoop2.7). In the Spark folder go to the python/libs and select pyspark.zip to the project. Do the same for another file in the same folder - py4j-0.10.7-src.zip. All available tests are in spark-nlp/python/run-tests.py. Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and look for Python. In the Script path locate file spark-nlp/python/run-tests.py. Also you need to add SPARK_HOME environment variable to the project. Choose Environment variables and add new variable SPARK_HOME. Insert installation path of spark to the Value field. Click Ok to save and close pop up and click Ok to confirm new task creation. Before running the tests we need to install requered python dependencies in the new virtual environment. Select in the bottom menu Terminal and activate your environment with command source venv/bin/activate after install packages by running pip install pyspark==3.3.1 numpy Compiling jar Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and select sbt task. In the Name field put AssemblyCopy. In the Tasks field write down assemblyAndCopy. After you can disable checkbox in Use sbt shell to have more custom configurations. In the VM parameters increase the memory by changing -Xmx1024M to -Xmx6G and click Ok. You can find created jar in the folder spark-nlp/python/lib/sparknlp.jar Note: Assembly command creates a fat jars, that includes all dependencies within Compiling pypi, whl Click Add configuration or Edit configuration in the Top right corner. In the pop up click on the + and select sbt task. In the Name field put AssemblyAndCopyForPyPi. In the Tasks field write down assemblyAndCopyForPyPi. Then you go to spark-nlp/python/ directory and run: python setup.py sdist bdist_wheel You can find created whl and tar.gz in the folder spark-nlp/python/dist/. Use this files to install spark-nlp locally: pip install spark_nlp-2.x.x-py3-none-any.whl",
    "url": "/docs/en/developers",
    "relUrl": "/docs/en/developers"
  },
  "14": {
    "id": "14",
    "title": "Spark NLP - Spark NLP Display",
    "content": "Getting started Spark NLP Display is an open-source python library for visualizing the annotations generated with Spark NLP. It currently offers out-of-the-box suport for the following types of annotations: Dependency Parser Named Entity Recognition Entity Resolution Relation Extraction Assertion Status The ability to quickly visualize the entities/relations/assertion statuses, etc. generated using Spark NLP is a very useful feature for speeding up the development process as well as for understanding the obtained results. Getting all of this in a one liner is extremelly convenient especially when running Jupyter notebooks which offers full support for html visualizations. The visualisation classes work with the outputs returned by both Pipeline.transform() function and LightPipeline.fullAnnotate(). Install Spark NLP Display You can install the Spark NLP Display library via pip by using: pip install spark-nlp-display A complete guideline on how to use the Spark NLP Display library is available here. Visualize a dependency tree For visualizing a dependency trees generated with DependencyParserApproach you can use the following code. from sparknlp_display import DependencyParserVisualizer dependency_vis = DependencyParserVisualizer() dependency_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe. pos_col = &#39;pos&#39;, #specify the pos column dependency_col = &#39;dependency&#39;, #specify the dependency column dependency_type_col = &#39;dependency_type&#39; #specify the dependency type column ) The following image gives an example of html output that is obtained for a test sentence: Visualize extracted named entities The NerVisualizer highlights the named entities that are identified by Spark NLP and also displays their labels as decorations on top of the analyzed text. The colors assigned to the predicted labels can be configured to fit the particular needs of the application. from sparknlp_display import NerVisualizer ner_vis = NerVisualizer() ner_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe label_col=&#39;entities&#39;, #specify the entity column document_col=&#39;document&#39; #specify the document column (default: &#39;document&#39;) labels=[&#39;PER&#39;] #only allow these labels to be displayed. (default: [] - all labels will be displayed) ) ## To set custom label colors: ner_vis.set_label_colors({&#39;LOC&#39;:&#39;#800080&#39;, &#39;PER&#39;:&#39;#77b5fe&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences: Visualize relations The RelationExtractionVisualizer can be used to visualize the relations predicted by Spark NLP. The two entities involved in a relation will be highlighted and their label will be displayed. Also a directed and labeled arc(line) will be used to connect the two entities. from sparknlp_display import RelationExtractionVisualizer re_vis = RelationExtractionVisualizer() re_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe relation_col = &#39;relations&#39;, #specify relations column document_col = &#39;document&#39;, #specify document column show_relations=True #display relation names on arrows (default: True) ) The following image gives an example of html output that is obtained for a couple of test sentences: Visualize assertion status The AssertionVisualizer is a special type of NerVisualizer that also displays on top of the labeled entities the assertion status that was infered by a Spark NLP model. from sparknlp_display import AssertionVisualizer assertion_vis = AssertionVisualizer() assertion_vis.display(pipeline_result[0], label_col = &#39;entities&#39;, #specify the ner result column assertion_col = &#39;assertion&#39; #specify assertion column document_col = &#39;document&#39; #specify the document column (default: &#39;document&#39;) ) ## To set custom label colors: assertion_vis.set_label_colors({&#39;TREATMENT&#39;:&#39;#008080&#39;, &#39;problem&#39;:&#39;#800080&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences: Visualize entity resolution Entity resolution refers to the normalization of named entities predicted by Spark NLP with respect to standard terminologies such as ICD-10, SNOMED, RxNorm etc. You can read more about the available entity resolvers here. The EntityResolverVisualizer will automatically display on top of the NER label the standard code (ICD10 CM, PCS, ICDO; CPT) that corresponds to that entity as well as the short description of the code. If no resolution code could be identified a regular NER-type of visualization will be displayed. from sparknlp_display import EntityResolverVisualizer er_vis = EntityResolverVisualizer() er_vis.display(pipeline_result[0], #should be the results of a single example, not the complete dataframe label_col=&#39;entities&#39;, #specify the ner result column resolution_col = &#39;resolution&#39; document_col=&#39;document&#39; #specify the document column (default: &#39;document&#39;) ) ## To set custom label colors: er_vis.set_label_colors({&#39;TREATMENT&#39;:&#39;#800080&#39;, &#39;PROBLEM&#39;:&#39;#77b5fe&#39;}) #set label colors by specifying hex codes The following image gives an example of html output that is obtained for a couple of test sentences:",
    "url": "/docs/en/display",
    "relUrl": "/docs/en/display"
  },
  "15": {
    "id": "15",
    "title": "John Snow Labs - NLP Documentation",
    "content": "",
    "url": "/docs",
    "relUrl": "/docs"
  },
  "16": {
    "id": "16",
    "title": "East Asian Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/east_asian_languages",
    "relUrl": "/east_asian_languages"
  },
  "17": {
    "id": "17",
    "title": "Enhance Low-Quality Images - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/enhance_low_quality_images",
    "relUrl": "/enhance_low_quality_images"
  },
  "18": {
    "id": "18",
    "title": "European Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/european_languages",
    "relUrl": "/european_languages"
  },
  "19": {
    "id": "19",
    "title": "Spark NLP - Examples",
    "content": "Showcasing notebooks and codes of how to use Spark NLP in Python and Scala. Python Setup $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.7 -y $ conda activate sparknlp $ pip install spark-nlp==5.0.2 pyspark==3.3.1 Google Colab Notebook Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account. Run the following code in Google Colab notebook and start using spark-nlp right away. # This is only to setup PySpark and Spark NLP on Colab !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash This script comes with the two options to define pyspark and spark-nlp versions via options: # -p is for pyspark # -s is for spark-nlp # by default they are set to the latest !bash colab.sh -p 3.2.3 -s 5.0.2 Spark NLP quick start on Google Colab is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines. Kaggle Kernel Run the following code in Kaggle Kernel and start using spark-nlp right away. # Let&#39;s setup Kaggle for Spark NLP and PySpark !wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash Notebooks Tutorials and articles Jupyter Notebooks",
    "url": "/docs/en/examples",
    "relUrl": "/docs/en/examples"
  },
  "20": {
    "id": "20",
    "title": "Extract handwritten texts - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/extract_handwritten_texts",
    "relUrl": "/extract_handwritten_texts"
  },
  "21": {
    "id": "21",
    "title": "Extract Tables - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/extract_tables",
    "relUrl": "/extract_tables"
  },
  "22": {
    "id": "22",
    "title": "Extract Text from Documents - Visual NLP Demos & Notebooks",
    "content": "",
    "url": "/extract_text_from_documents",
    "relUrl": "/extract_text_from_documents"
  },
  "23": {
    "id": "23",
    "title": "Genes, Variants, Phenotypes - Biomedical NLP Demos & Notebooks",
    "content": "",
    "url": "/genes_variants_phenotypes",
    "relUrl": "/genes_variants_phenotypes"
  },
  "24": {
    "id": "24",
    "title": "German - Medical NLP Demos & Notebooks",
    "content": "",
    "url": "/german",
    "relUrl": "/german"
  },
  "25": {
    "id": "25",
    "title": "Spark NLP - Tensorflow Graph",
    "content": "NER DL uses Char CNNs - BiLSTM - CRF Neural Network architecture. Spark NLP defines this architecture through a Tensorflow graph, which requires the following parameters: Tags Embeddings Dimension Number of Chars Spark NLP infers these values from the training dataset used in NerDLApproach annotator and tries to load the graph embedded on spark-nlp package. Currently, Spark NLP has graphs for the most common combination of tags, embeddings, and number of chars values: Tags Embeddings Dimension 10 100 10 200 10 300 10 768 10 1024 25 300 All of these graphs use an LSTM of size 128 and number of chars 100 In case, your train dataset has a different number of tags, embeddings dimension, number of chars and LSTM size combinations shown in the table above, NerDLApproach will raise an IllegalArgumentException exception during runtime with the message below: Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Check https://sparknlp.org/docs/en/graph for instructions to generate the required graph. To overcome this exception message we have to follow these steps: Clone spark-nlp github repo Run python file create_models with number of tags, embeddings dimension and number of char values mentioned on your exception message error. cd spark-nlp/python/tensorflow export PYTHONPATH=lib/ner python ner/create_models.py [number_of_tags] [embeddings_dimension] [number_of_chars] [output_path] This will generate a graph on the directory defined on `output_path argument. Retry training with NerDLApproach annotator but this time use the parameter setGraphFolder with the path of your graph. Note: Make sure that you have Python 3 and Tensorflow 1.15.0 installed on your system since create_models requires those versions to generate the graph successfully. Note: We also have a notebook in the same directory if you prefer Jupyter notebook to cerate your custom graph (create_models.ipynb).",
    "url": "/docs/en/graph",
    "relUrl": "/docs/en/graph"
  },
  "26": {
    "id": "26",
    "title": "Spark NLP - Hardware Acceleration",
    "content": "Spark NLP is a production-ready and fully-featured NLP library that runs natively on Apache Spark. It is already faster on a single machine than other popular NLP libraries let alone in a cluster with multiple machines. In addition, we are constantly optimizing our codes to make them even faster while using fewer resources (memory/CPU). For incense, the Spark NLP 4.0 comes with massive optimizations for GPU and modern CPUs for most of our Transformer-based annotators. That said, some downstream tasks such as Language Models (Transformer models like BERT) or text and token classifiers use Deep Learning via the TensorFlow engine. Therefore, there are ways to optimize them even more by using newer hardware, especially those with accelerations. The following benchmarks have been done by using a single Dell Server with the following specs: GPU: Tesla P100 PCIe 12GB - CUDA Version: 11.3 - Driver Version: 465.19.01 CPU: Intel(R) Xeon(R) CPU E5-2698 v4 @ 2.20GHz - 40 Cores Memory: 80G GPU Perhaps the best and the easiest way in Spark NLP to massively improve a DL-based task(s) is to use GPU. Spark NLP comes with a zero-code change feature to run seamlessly on both CPU and GPU by simply enabling GPU via sparknlp.start(gpu=True) or using directly the Maven package that is for GPU spark-nlp-gpu. (more details) Since the new Transformer models such as BERT for Word and Sentence embeddings are the most computationally available downstream tasks in Spark NLP, we will show a benchmark for inference (prediction) to compare CPU (without any accelerations) to GPU: Model on GPU Spark NLP 3.4.3 vs. 4.0.0 RoBERTa base +560%(6.6x) RoBERTa Large +332%(4.3x) Albert Base +587%(6.9x) Albert Large +332%(4.3x) DistilBERT +659%(7.6x) XLM-RoBERTa Base +638%(7.4x) XLM-RoBERTa Large +365%(4.7x) XLNet Base +449%(5.5x) XLNet Large +267%(3.7x) DeBERTa Base +713%(8.1x) DeBERTa Large +477%(5.8x) Longformer Base +52%(1.5x) Spark NLP 5.0.2 is built with TensorFlow 2.7.1 and the following NVIDIA® software are only required for GPU support: NVIDIA® GPU drivers version 450.80.02 or higher CUDA® Toolkit 11.2 cuDNN SDK 8.1.0 CPU The oneAPI Deep Neural Network Library (oneDNN) optimizations are now available in Spark NLP 4.0.0 which uses TensorFlow 2.7.1. You can enable those CPU optimizations by setting the environment variable TF_ENABLE_ONEDNN_OPTS=1. Intel has been collaborating with Google to optimize its performance on Intel Xeon processor-based platforms using Intel oneAPI Deep Neural Network (oneDNN), an open-source, cross-platform performance library for DL applications. TensorFlow optimizations are enabled via oneDNN to accelerate key performance-intensive operations such as convolution, matrix multiplication, and batch normalization. This feature is experimental as it has to be enabled manually and benchmarked manually to see whether or not your pipeline can benefit from oneDNN accelerations. That being said, it does not always result in accelerating your annotators as it highly depends on the hardware and the NLP tasks. Similar to GPU, if the task is not computational it won’t change the result and it may even slow down the inferences. NOTE: Always have a baseline benchmark without having oneDNN enabled so you can compare it with oneDNN. In addition, always make sure you repeat the same steps if you are moving to another hardware (CPU). Here we compare the last release of Spark NLP 3.4.3 on CPU (normal) with Spark NLP 4.0.0 on CPU with oneDNN enabled. We chose some of the most computational downstream tasks in Spark NLP as they are usually required in the pipeline for other tasks such as NER or text classification): Model on CPU 3.4.x vs. 4.0.0 with oneDNN BERT Base +47% BERT Large +42% RoBERTa Base +51% RoBERTa Large +61% Albert Base +83% Albert Large +58% DistilBERT +80% XLM-RoBERTa Base +82% XLM-RoBERTa Large +72% XLNet Base +50% XLNet Large +27% DeBERTa Base +59% DeBERTa Large +56% CamemBERT Base +97% CamemBERT Large +65% Longformer Base +63% In future TensorFlow releases, the oneDNN will be enabled by default (starting TF 2.9) as this feature becomes more stable and more generic for almost all TF ops. Maximize TensorFlow* Performance on CPU: Considerations and Recommendations for Inference Workloads GPU vs. CPU Webinar: Speed Optimization &amp; Benchmarks in Spark NLP 3: Making the Most of Modern Hardware",
    "url": "/docs/en/hardware_acceleration",
    "relUrl": "/docs/en/hardware_acceleration"
  },
  "27": {
    "id": "27",
    "title": "Identify & Translate Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/identify_translate_languages",
    "relUrl": "/identify_translate_languages"
  },
  "28": {
    "id": "28",
    "title": "Spark NLP 🚀 <span>State of the Art Natural Language Processing</span>",
    "content": "",
    "url": "/",
    "relUrl": "/"
  },
  "29": {
    "id": "29",
    "title": "Infer Meaning & Intent - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/infer_meaning_intent",
    "relUrl": "/infer_meaning_intent"
  },
  "30": {
    "id": "30",
    "title": "Spark NLP - Installation",
    "content": "Spark NLP Cheatsheet # Install Spark NLP from PyPI pip install spark-nlp==5.0.2 # Install Spark NLP from Anacodna/Conda conda install -c johnsnowlabs spark-nlp # Load Spark NLP with Spark Shell spark-shell --packages com.johnsnowlabs.nlp:spark-nlp_2.12:5.0.2 # Load Spark NLP with PySpark pyspark --packages com.johnsnowlabs.nlp:spark-nlp_2.12:5.0.2 # Load Spark NLP with Spark Submit spark-submit --packages com.johnsnowlabs.nlp:spark-nlp_2.12:5.0.2 # Load Spark NLP as external JAR after compiling and building Spark NLP by `sbt assembly` spark-shell --jars spark-nlp-assembly-5.0.2.jar Python Spark NLP supports Python 3.7.x and above depending on your major PySpark version. NOTE: Since Spark version 3.2, Python 3.6 is deprecated. If you are using this python version, consider sticking to lower versions of Spark. Quick Install Let’s create a new Conda environment to manage all the dependencies there. You can use Python Virtual Environment if you prefer or not have any environment. $ java -version # should be Java 8 (Oracle or OpenJDK) $ conda create -n sparknlp python=3.8 -y $ conda activate sparknlp $ pip install spark-nlp==5.0.2 pyspark==3.3.1 Of course you will need to have jupyter installed in your system: pip install jupyter Now you should be ready to create a jupyter notebook running from terminal: jupyter notebook Start Spark NLP Session from python If you need to manually start SparkSession because you have other configurations and sparknlp.start() is not including them, you can manually start the SparkSession: spark = SparkSession.builder .appName(&quot;Spark NLP&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;,&quot;16G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:5.0.2&quot;) .getOrCreate() Scala and Java Maven spark-nlp on Apache Spark 3.0.x, 3.1.x, 3.2.x, 3.3.x, and 3.4.x &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp_2.12&lt;/artifactId&gt; &lt;version&gt;5.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-gpu: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-gpu_2.12&lt;/artifactId&gt; &lt;version&gt;5.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-silicon: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-silicon --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-silicon_2.12&lt;/artifactId&gt; &lt;version&gt;5.0.2&lt;/version&gt; &lt;/dependency&gt; spark-nlp-aarch64: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-aarch64 --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-aarch64_2.12&lt;/artifactId&gt; &lt;version&gt;5.0.2&lt;/version&gt; &lt;/dependency&gt; SBT spark-nlp on Apache Spark 3.0.x, 3.1.x, 3.2.x, 3.3.x, and 3.4.x // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp&quot; % &quot;5.0.2&quot; spark-nlp-gpu: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-gpu libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-gpu&quot; % &quot;5.0.2&quot; spark-nlp-silicon: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-silicon libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-silicon&quot; % &quot;5.0.2&quot; spark-nlp-aarch64: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-aarch64 libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-aarch64&quot; % &quot;5.0.2&quot; Maven Central: https://mvnrepository.com/artifact/com.johnsnowlabs.nlp If you are interested, there is a simple SBT project for Spark NLP to guide you on how to use it in your projects Spark NLP SBT Starter Installation for M1 Macs Starting from version 4.0.0, Spark NLP has experimental support for M1 macs. Note that at the moment, only the standard variant of the M1 is supported. Other variants (e.g. M1 Pro/Max/Ultra, M2) will most likely not work. Make sure the following prerequisites are met: An M1 compiled java version needs to be installed. For example to install the Zulu Java 11 JDK head to Download Azul JDKs and install that java version. To check if the installed java environment is running natively on arm64 and not rosetta, you can run the following commands in your shell: johnsnow@m1mac ~ % cat $(which java) | file - /dev/stdin: Mach-O 64-bit executable arm64 The environment variable JAVA_HOME should also be set to this java version. You can check this by running echo $JAVA_HOME in your terminal. If it is not set, you can set it by adding export JAVA_HOME=$(/usr/libexec/java_home) to your ~/.zshrc file. If you are planning to use Annotators or Pipelines that use the RocksDB library (for example WordEmbeddings, TextMatcher or explain_document_dl_en Pipeline respectively) with spark-submit, then a workaround is required to get it working. See M1 RocksDB workaround for spark-submit with Spark version &gt;= 3.2.0. M1 RocksDB workaround for spark-submit with Spark version &gt;= 3.2.0 Starting from Spark version 3.2.0, Spark includes their own version of the RocksDB dependency. Unfortunately, this is an older version of RocksDB does not include the necessary binaries of M1. To work around this issue, the default packaged RocksDB jar has to be removed from the Spark distribution. For example, if you downloaded Spark version 3.2.0 from the official archives, you will find the following folders in the directory of Spark: $ ls bin conf data examples jars kubernetes LICENSE licenses NOTICE python R README.md RELEASE sbin yarn To check for the RocksDB jar, you can run $ ls jars | grep rocksdb rocksdbjni-6.20.3.jar to find the jar you have to remove. After removing the jar, the pipelines should work as expected. Scala and Java for M1 Adding Spark NLP to your Scala or Java project is easy: Simply change to dependency coordinates to spark-nlp-silicon and add the dependency to your project. How to do this is mentioned above: Scala And Java So for example for Spark NLP with Apache Spark 3.0.x and 3.1.x you will end up with maven coordinates like these: &lt;!-- https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp-silicon --&gt; &lt;dependency&gt; &lt;groupId&gt;com.johnsnowlabs.nlp&lt;/groupId&gt; &lt;artifactId&gt;spark-nlp-silicon_2.12&lt;/artifactId&gt; &lt;version&gt;5.0.2&lt;/version&gt; &lt;/dependency&gt; or in case of sbt: // https://mvnrepository.com/artifact/com.johnsnowlabs.nlp/spark-nlp libraryDependencies += &quot;com.johnsnowlabs.nlp&quot; %% &quot;spark-nlp-silicon&quot; % &quot;5.0.2&quot; If everything went well, you can now start Spark NLP with the m1 flag set to true: import com.johnsnowlabs.nlp.SparkNLP val spark = SparkNLP.start(apple_silicon = true) Python for M1 &amp; M2 First, make sure you have a recent Python 3 installation. johnsnow@m1mac ~ % python3 --version Python 3.9.13 Then we can install the dependency as described in the Python section. It is also recommended to use a virtual environment for this. If everything went well, you can now start Spark NLP with the m1 flag set to True: import sparknlp spark = sparknlp.start(apple_silicon=True) Installation for Linux Aarch64 Systems Starting from version 5.0.2, Spark NLP supports Linux systems running on an aarch64 processor architecture. The necessary dependencies have been built on Ubuntu 16.04, so a recent system with an environment of at least that will be needed. Check the Python section and the Scala And Java section on to install Spark NLP for your system. Starting Spark NLP Spark NLP needs to be started with the aarch64 flag set to true: For Scala: import com.johnsnowlabs.nlp.SparkNLP val spark = SparkNLP.start(aarch64 = true) For Python: import sparknlp spark = sparknlp.start(aarch64=True) Google Colab Notebook Google Colab is perhaps the easiest way to get started with spark-nlp. It requires no installation or setup other than having a Google account. Run the following code in Google Colab notebook and start using spark-nlp right away. # This is only to setup PySpark and Spark NLP on Colab !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash This script comes with the two options to define pyspark and spark-nlp versions via options: # -p is for pyspark # -s is for spark-nlp # by default they are set to the latest !wget http://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.2.3 -s 5.0.2 Spark NLP quick start on Google Colab is a live demo on Google Colab that performs named entity recognitions and sentiment analysis by using Spark NLP pretrained pipelines. Kaggle Kernel Run the following code in Kaggle Kernel and start using spark-nlp right away. # Let&#39;s setup Kaggle for Spark NLP and PySpark !wget http://setup.johnsnowlabs.com/kaggle.sh -O - | bash Spark NLP quick start on Kaggle Kernel is a live demo on Kaggle Kernel that performs named entity recognitions by using Spark NLP pretrained pipeline. Databricks Support Spark NLP 5.0.2 has been tested and is compatible with the following runtimes: CPU: 7.3 7.3 ML 9.1 9.1 ML 10.1 10.1 ML 10.2 10.2 ML 10.3 10.3 ML 10.4 10.4 ML 10.5 10.5 ML 11.0 11.0 ML 11.1 11.1 ML 11.2 11.2 ML 11.3 11.3 ML 12.0 12.0 ML 12.1 12.1 ML 12.2 12.2 ML GPU: 9.1 ML &amp; GPU 10.1 ML &amp; GPU 10.2 ML &amp; GPU 10.3 ML &amp; GPU 10.4 ML &amp; GPU 10.5 ML &amp; GPU 11.0 ML &amp; GPU 11.1 ML &amp; GPU 11.2 ML &amp; GPU 11.3 ML &amp; GPU 12.0 ML &amp; GPU 12.1 ML &amp; GPU 12.2 ML &amp; GPU NOTE: Spark NLP 4.0.x is based on TensorFlow 2.7.x which is compatible with CUDA11 and cuDNN 8.0.2. The only Databricks runtimes supporting CUDA 11 are 9.x and above as listed under GPU. Install Spark NLP on Databricks Create a cluster if you don’t have one already On a new cluster or existing one you need to add the following to the Advanced Options -&gt; Spark tab: spark.kryoserializer.buffer.max 2000M spark.serializer org.apache.spark.serializer.KryoSerializer In Libraries tab inside your cluster you need to follow these steps: 3.1. Install New -&gt; PyPI -&gt; spark-nlp -&gt; Install 3.2. Install New -&gt; Maven -&gt; Coordinates -&gt; com.johnsnowlabs.nlp:spark-nlp_2.12:5.0.2 -&gt; Install Now you can attach your notebook to the cluster and use Spark NLP! NOTE: Databrick’s runtimes support different Apache Spark major releases. Please make sure you choose the correct Spark NLP Maven pacakge name (Maven Coordinate) for your runtime from our Packages Cheatsheet Databricks Notebooks You can view all the Databricks notebooks from this address: https://johnsnowlabs.github.io/spark-nlp-workshop/databricks/index.html Note: You can import these notebooks by using their URLs. EMR Support Spark NLP 5.0.2 has been tested and is compatible with the following EMR releases: emr-6.2.0 emr-6.3.0 emr-6.3.1 emr-6.4.0 emr-6.5.0 emr-6.6.0 emr-6.7.0 emr-6.8.0 emr-6.9.0 emr-6.10.0 Full list of Amazon EMR 6.x releases NOTE: The EMR 6.1.0 and 6.1.1 are not supported. How to create EMR cluster via CLI To lanuch EMR cluster with Apache Spark/PySpark and Spark NLP correctly you need to have bootstrap and software configuration. A sample of your bootstrap script #!/bin/bash set -x -e echo -e &#39;export PYSPARK_PYTHON=/usr/bin/python3 export HADOOP_CONF_DIR=/etc/hadoop/conf export SPARK_JARS_DIR=/usr/lib/spark/jars export SPARK_HOME=/usr/lib/spark&#39; &gt;&gt; $HOME/.bashrc &amp;&amp; source $HOME/.bashrc sudo python3 -m pip install awscli boto spark-nlp set +x exit 0 A sample of your software configuration in JSON on S3 (must be public access): [{ &quot;Classification&quot;: &quot;spark-env&quot;, &quot;Configurations&quot;: [{ &quot;Classification&quot;: &quot;export&quot;, &quot;Properties&quot;: { &quot;PYSPARK_PYTHON&quot;: &quot;/usr/bin/python3&quot; } }] }, { &quot;Classification&quot;: &quot;spark-defaults&quot;, &quot;Properties&quot;: { &quot;spark.yarn.stagingDir&quot;: &quot;hdfs:///tmp&quot;, &quot;spark.yarn.preserve.staging.files&quot;: &quot;true&quot;, &quot;spark.kryoserializer.buffer.max&quot;: &quot;2000M&quot;, &quot;spark.serializer&quot;: &quot;org.apache.spark.serializer.KryoSerializer&quot;, &quot;spark.driver.maxResultSize&quot;: &quot;0&quot;, &quot;spark.jars.packages&quot;: &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:5.0.2&quot; } } ] A sample of AWS CLI to launch EMR cluster: aws emr create-cluster --name &quot;Spark NLP 5.0.2&quot; --release-label emr-6.2.0 --applications Name=Hadoop Name=Spark Name=Hive --instance-type m4.4xlarge --instance-count 3 --use-default-roles --log-uri &quot;s3://&lt;S3_BUCKET&gt;/&quot; --bootstrap-actions Path=s3://&lt;S3_BUCKET&gt;/emr-bootstrap.sh,Name=custome --configurations &quot;https://&lt;public_access&gt;/sparknlp-config.json&quot; --ec2-attributes KeyName=&lt;your_ssh_key&gt;,EmrManagedMasterSecurityGroup=&lt;security_group_with_ssh&gt;,EmrManagedSlaveSecurityGroup=&lt;security_group_with_ssh&gt; --profile &lt;aws_profile_credentials&gt; GCP Dataproc Support Create a cluster if you don’t have one already as follows. At gcloud shell: gcloud services enable dataproc.googleapis.com compute.googleapis.com storage-component.googleapis.com bigquery.googleapis.com bigquerystorage.googleapis.com REGION=&lt;region&gt; BUCKET_NAME=&lt;bucket_name&gt; gsutil mb -c standard -l ${REGION} gs://${BUCKET_NAME} REGION=&lt;region&gt; ZONE=&lt;zone&gt; CLUSTER_NAME=&lt;cluster_name&gt; BUCKET_NAME=&lt;bucket_name&gt; You can set image-version, master-machine-type, worker-machine-type, master-boot-disk-size, worker-boot-disk-size, num-workers as your needs. If you use the previous image-version from 2.0, you should also add ANACONDA to optional-components. And, you should enable gateway. gcloud dataproc clusters create ${CLUSTER_NAME} --region=${REGION} --zone=${ZONE} --image-version=2.0 --master-machine-type=n1-standard-4 --worker-machine-type=n1-standard-2 --master-boot-disk-size=128GB --worker-boot-disk-size=128GB --num-workers=2 --bucket=${BUCKET_NAME} --optional-components=JUPYTER --enable-component-gateway --metadata &#39;PIP_PACKAGES=spark-nlp spark-nlp-display google-cloud-bigquery google-cloud-storage&#39; --initialization-actions gs://goog-dataproc-initialization-actions-${REGION}/python/pip-install.sh On an existing one, you need to install spark-nlp and spark-nlp-display packages from PyPI. Now, you can attach your notebook to the cluster and use the Spark NLP! Amazon Linux 2 Support # Update Package List &amp; Install Required Packages sudo yum update sudo yum install -y amazon-linux-extras sudo yum -y install python3-pip # Create Python virtual environment and activate it: python3 -m venv .sparknlp-env source .sparknlp-env/bin/activate Check JAVA version: For Sparknlp versions above 3.x, please use JAVA-11 Checking Java versions installed on your machine: sudo alternatives --config java You can pick the index number (I am using java-8 as default - index 2): If you dont have java-11 or java-8 in you system, you can easily install via: sudo yum install java-1.8.0-openjdk Now, we can start installing the required libraries: pip install pyspark==3.3.1 pip install spark-nlp Docker Support For having Spark NLP, PySpark, Jupyter, and other ML/DL dependencies as a Docker image you can use the following template: #Download base image ubuntu 18.04 FROM ubuntu:18.04 ENV NB_USER jovyan ENV NB_UID 1000 ENV HOME /home/${NB_USER} ENV PYSPARK_PYTHON=python3 ENV PYSPARK_DRIVER_PYTHON=python3 RUN apt-get update &amp;&amp; apt-get install -y tar wget bash rsync gcc libfreetype6-dev libhdf5-serial-dev libpng-dev libzmq3-dev python3 python3-dev python3-pip unzip pkg-config software-properties-common graphviz RUN adduser --disabled-password --gecos &quot;Default user&quot; --uid ${NB_UID} ${NB_USER} # Install OpenJDK-8 RUN apt-get update &amp;&amp; apt-get install -y openjdk-8-jdk &amp;&amp; apt-get install -y ant &amp;&amp; apt-get clean; # Fix certificate issues RUN apt-get update &amp;&amp; apt-get install ca-certificates-java &amp;&amp; apt-get clean &amp;&amp; update-ca-certificates -f; # Setup JAVA_HOME -- useful for docker commandline ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64/ RUN export JAVA_HOME RUN echo &quot;export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/&quot; &gt;&gt; ~/.bashrc RUN apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* RUN pip3 install --upgrade pip # You only need pyspark and spark-nlp paclages to use Spark NLP # The rest of the PyPI packages are here as examples RUN pip3 install --no-cache-dir pyspark spark-nlp==3.2.3 notebook==5.* numpy pandas mlflow Keras scikit-spark scikit-learn scipy matplotlib pydot tensorflow==2.4.1 graphviz # Make sure the contents of our repo are in ${HOME} RUN mkdir -p /home/jovyan/tutorials RUN mkdir -p /home/jovyan/jupyter COPY data ${HOME}/data COPY jupyter ${HOME}/jupyter COPY tutorials ${HOME}/tutorials RUN jupyter notebook --generate-config COPY jupyter_notebook_config.json /home/jovyan/.jupyter/jupyter_notebook_config.json USER root RUN chown -R ${NB_UID} ${HOME} USER ${NB_USER} WORKDIR ${HOME} # Specify the default command to run CMD [&quot;jupyter&quot;, &quot;notebook&quot;, &quot;--ip&quot;, &quot;0.0.0.0&quot;] Finally, use jupyter_notebook_config.json for the password: { &quot;NotebookApp&quot;: { &quot;password&quot;: &quot;sha1:65adaa6ffb9c:36df1c2086ef294276da703667d1b8ff38f92614&quot; } } Windows Support In order to fully take advantage of Spark NLP on Windows (8 or 10), you need to setup/install Apache Spark, Apache Hadoop, Java and a Pyton environment correctly by following the following instructions: https://github.com/JohnSnowLabs/spark-nlp/discussions/1022 How to correctly install Spark NLP on Windows Follow the below steps to set up Spark NLP with Spark 3.2.3: Download Adopt OpenJDK 1.8 Make sure it is 64-bit Make sure you install it in the root of your main drive C: java. During installation after changing the path, select setting Path Download the pre-compiled Hadoop binaries winutils.exe, hadoop.dll and put it in a folder called C: hadoop bin from https://github.com/cdarlint/winutils/tree/master/hadoop-3.2.0/bin Note: The version above is for Spark 3.2.3, which was built for Hadoop 3.2.0. You might have to change the hadoop version in the link, depending on which Spark version you are using. Download Apache Spark 3.2.3 and extract it to C: spark. Set/add environment variables for HADOOP_HOME to C: hadoop and SPARK_HOME to C: spark. Add %HADOOP_HOME% bin and %SPARK_HOME% bin to the PATH environment variable. Install Microsoft Visual C++ 2010 Redistributed Package (x64). Create folders C: tmp and C: tmp hive If you encounter issues with permissions to these folders, you might need to change the permissions by running the following commands: %HADOOP_HOME% bin winutils.exe chmod 777 /tmp/hive %HADOOP_HOME% bin winutils.exe chmod 777 /tmp/ Requisites for PySpark We recommend using conda to manage your Python environment on Windows. Download Miniconda for Python 3.8 See Quick Install on how to set up a conda environment with Spark NLP. The following environment variables need to be set: PYSPARK_PYTHON=python Optionally, if you want to use the Jupyter Notebook runtime of Spark: first install it in the environment with conda install notebook then set PYSPARK_DRIVER_PYTHON=jupyter, PYSPARK_DRIVER_PYTHON_OPTS=notebook The environment variables can either be directly set in windows, or if only the conda env will be used, with conda env config vars set PYSPARK_PYTHON=python. After setting the variable with conda, you need to deactivate and re-activate the environment. Now you can use the downloaded binary by navigating to %SPARK_HOME% bin and running Either create a conda env for python 3.6, install pyspark==3.3.1 spark-nlp numpy and use Jupyter/python console, or in the same conda env you can go to spark bin for pyspark –packages com.johnsnowlabs.nlp:spark-nlp_2.12:5.0.2. Offline Spark NLP library and all the pre-trained models/pipelines can be used entirely offline with no access to the Internet. If you are behind a proxy or a firewall with no access to the Maven repository (to download packages) or/and no access to S3 (to automatically download models and pipelines), you can simply follow the instructions to have Spark NLP without any limitations offline: Instead of using the Maven package, you need to load our Fat JAR Instead of using PretrainedPipeline for pretrained pipelines or the .pretrained() function to download pretrained models, you will need to manually download your pipeline/model from Models Hub, extract it, and load it. Example of SparkSession with Fat JAR to have Spark NLP offline: spark = SparkSession.builder .appName(&quot;Spark NLP&quot;) .master(&quot;local[*]&quot;) .config(&quot;spark.driver.memory&quot;,&quot;16G&quot;) .config(&quot;spark.driver.maxResultSize&quot;, &quot;0&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.jars&quot;, &quot;/tmp/spark-nlp-assembly-5.0.2.jar&quot;) .getOrCreate() You can download provided Fat JARs from each release notes, please pay attention to pick the one that suits your environment depending on the device (CPU/GPU) and Apache Spark version (3.x) If you are local, you can load the Fat JAR from your local FileSystem, however, if you are in a cluster setup you need to put the Fat JAR on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., hdfs:///tmp/spark-nlp-assembly-5.0.2.jar) Example of using pretrained Models and Pipelines in offline: # instead of using pretrained() for online: # french_pos = PerceptronModel.pretrained(&quot;pos_ud_gsd&quot;, lang=&quot;fr&quot;) # you download this model, extract it, and use .load french_pos = PerceptronModel.load(&quot;/tmp/pos_ud_gsd_fr_2.0.2_2.4_1556531457346/&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;pos&quot;) # example for pipelines # instead of using PretrainedPipeline # pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;) # you download this pipeline, extract it, and use PipelineModel PipelineModel.load(&quot;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&quot;) Since you are downloading and loading models/pipelines manually, this means Spark NLP is not downloading the most recent and compatible models/pipelines for you. Choosing the right model/pipeline is on you If you are local, you can load the model/pipeline from your local FileSystem, however, if you are in a cluster setup you need to put the model/pipeline on a distributed FileSystem such as HDFS, DBFS, S3, etc. (i.e., hdfs:///tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/)",
    "url": "/docs/en/install",
    "relUrl": "/docs/en/install"
  },
  "31": {
    "id": "31",
    "title": "Installation",
    "content": "Deploy using Docker For deploying NLP Server on your instance run the following command. docker run --pull=always -p 5000:5000 johnsnowlabs/nlp-server:latest This will check if the latest docker image is available on your local machine and if not it will automatically download and run it. If you want to keep downloaded models between restarts of the docker image, you can mount a volume. mkdir /var/cache_pretrained chown 1000:1000 /var/cache_pretrained docker run --pull=always -v /var/cache_pretrained:/home/johnsnowlabs/cache_pretrained -p 5000:5000 johnsnowlabs/nlp-server:latest Deploy using AWS Marketplace NLP Server on AWS Marketplace provides one of the fastest and easiest ways to get up and running on Amazon Web Services (AWS). NLP Server is available through AWS Marketplace free of charge. However, to use licensed spells in NLP Server, you need to buy our license from here. You can get NLP Server on AWS Marketplace from this URL. Follow the seven steps instructions or the video tutorial given below to learn how to deploy NLP Server using AWS Marketplace. Make sure you have a valid AWS account and log in to the AWS Marketplace using your credentials. Deploy NLP Server via AWS Marketplace 1.Click on Continue to subscribe button for creating a subscription to the NLP Server product. The software is free of charge. 2.Read the subscription EULA and click on Accept terms button if you want to continue. 3.In a couple of seconds the subscription becomes active. Once it is active you see this screen. 4.Go to AWS Marketplace &gt; Manage subscriptions and click on the Launch new instance button corresponding to the NLP Server subscription. This will redirect you to the following screen. Click on Continue to launch through EC2 button. 5.From the available options select the instance type you want to use for the deployment. Then click on Review and Lauch button. 6.Select an existing key pair or create a new one. This ensures a secured connection to the instance. If you create a new key make sure that you download and safely store it for future usage. Click on the Launch button. 7.While the instance is starting you will see this screen. Then the instance will appear on your EC2 Instances list. The NLP Server can now be accessed via a web browser at http://PUBLIC_EC2_IP . API documentation is also available at http://PUBLIC_EC2_IP/docs Deploy using Azure Marketplace NLP Server on Azure Marketplace provides one of the fastest and easiest ways to get up and running on Microsoft Azure. NLP Server is available through Azure Marketplace free of charge. However, to use licensed spells in NLP Server, you need to buy our license from here. You can get NLP Server on Azure Marketplace from this URL. Follow the video tutorial given below to learn how to deploy NLP Server using Azure Marketplace. Deploy NLP Server using Azure Marketplace",
    "url": "/docs/en/nlp_server/installation",
    "relUrl": "/docs/en/nlp_server/installation"
  },
  "32": {
    "id": "32",
    "title": "Labs, Tests, and Vitals - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/labs_tests_and_vitals",
    "relUrl": "/labs_tests_and_vitals"
  },
  "33": {
    "id": "33",
    "title": "African Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/languages_africa",
    "relUrl": "/languages_africa"
  },
  "34": {
    "id": "34",
    "title": "Languages of India - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/languages_india",
    "relUrl": "/languages_india"
  },
  "35": {
    "id": "35",
    "title": "",
    "content": "",
    "url": "/latest.html",
    "relUrl": "/latest.html"
  },
  "36": {
    "id": "36",
    "title": "Learn",
    "content": "Introductions to Spark NLP Videos State of the Art Natural Language Processing at Scale. David Talby - April 13, 2020 Spark NLP: State of the art natural language processing at scale. David Talby - 4 Jun 2020 What is Spark NLP. John Snow Labs - 30 Jul 2019 Apache Spark NLP Extending Spark ML to Deliver Fast, Scalable, and Unified Natural Language Process. David Talby - 6 May 2019 Natural Language Understanding at Scale with Spark Native NLP, Spark ML &amp;TensorFlow with Alex Thomas. Alex Thomas - 26 Oct 2017 Articles Introducing the Natural Language Processing Library for Apache SparkDavid Talby - October 19, 2017 Improving Clinical Document Understanding on COVID-19 Research with Spark NLPVeysel Kocaman, David Talby - 7 December, 2020 Topic Modelling with PySpark and Spark NLPMaria Obedkova - May 29, 2020 Installing Spark NLP and Spark OCR in air-gapped networks (offline mode)Veysel Kocaman - May 04, 2020 Cleaning and extracting text from HTML/XML documents by using Spark NLPStefano Lori - Jan 13, 2020 A Google Colab Notebook Introducing Spark NLPVeysel Kocaman - September, 2020 State-of-the-art Natural Language Processing at ScaleDavid Talby - April 13, 2020 How to Wrap Your Head Around Spark NLPMustafa Aytuğ Kaya - August 25, 2020 5 Reasons Why Spark NLP Is The Most Widely Used Library In EnterprisesAmbika Choudhury - May 28, 2019 My Experience with SparkNLP Workshop &amp; CertificationAngelina Maria Leigh - August 17, 2020 Out of the box Spark NLP models in actionDia Trambitas - August 14, 2020 Get started with Machine Learning in Java using Spark NLPWill Price - August 27, 2020 SPARK NLP 3: MASSIVE SPEEDUPS &amp; THE LATEST COMPUTE PLATFORMSMaziyar Panahi - March 25, 2021 SPARK NLP 2.7: 720+ NEW MODELS &amp; PIPELINES FOR 192 LANGUAGES!David Talby - January 05, 2021 Python’s NLU Library Videos &quot;Python&#39;s NLU library: 1,000+ Models, 200+ Languages, 1 Line of Code&quot; by: Christian Kasim Loan - 18 June 2021 John Snow Labs NLU: Become a Data Science Superhero with One Line of Python code. Christian Kasim Loan - November, 2020 Articles 1 line to GLOVE Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to XLNET Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to ALBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to COVIDBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to ELECTRA Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 line to BioBERT Word Embeddings with NLU in PythonChristian Kasim Loan - January 17, 2021 1 Line of Code, 350 + NLP Models with John Snow Labs’ NLU in PythonChristian Kasim Loan - September 21, 2020 Easy sentence similarity with BERT Sentence Embeddings using John Snow Labs NLUChristian Kasim Loan - November 20, 2020 Training Deep Learning NLP Classifier TutorialChristian Kasim Loan - November 20, 2020 1 Python Line for ELMo Word Embeddings and t-SNE plots with John Snow Labs’ NLUChristian Kasim Loan - October 24, 2020 1 line of Python code for BERT, ALBERT, ELMO, ELECTRA, XLNET, GLOVE, Part of Speech with NLU and t-SNEChristian Kasim Loan - September 21, 2020 1 line to BERT Word Embeddings with NLU in PythonChristian Kasim Loan - September 21, 2020 Question answering, intent classification, aspect based ner, and new multilingual models in python’s NLU libraryChristian Kasim Loan - February 12, 2021 Intent and action classification, analyze chinese news and crypto market, 200+ languages &amp; answer questions with NLU 1.1.3Christian Kasim Loan - March 02, 2021 Hindi wordembeddings, bengali named entity recognition, 30+ new models, analyze crypto news with NLU 1.1.2Christian Kasim Loan - February 18, 2021 Named Entity Recognition Videos State-of-the-art Clinical Named Entity Recognition in Spark NLP Workshop - Veysel Kocaman Train your own NerDL. John Snow Labs - 7 Oct 2019 Articles State-of-the-art named entity recognition with BERTVeysel Kocaman - February 26th, 2020 State-of-the-art Named Entity Recognition in Spark NLPVeysel Kocaman Spark NLP in action: intelligent, high-accuracy fact extraction from long financial documentsSaif Addin Ellafi - May 5, 2020 Named Entity Recognition (NER) with BERT in Spark NLPVeysel Kocaman - Mar 4, 2020 Document Classification Videos Spark NLP in Action: Learning to read Life Science research - Saif Addin Ellafi. Saif Addin Ellafi - 1 Aug 2018 State of the art emotion and sentiment analysis with Spark NLP (Data science Salon). Dia Trambitas - December 1, 2020 Articles GloVe, ELMo &amp; BERT. A guide to state-of-the-art text classification using Spark NLP Ryan Burke - March 16, 2021 Distributed Topic Modelling using Spark NLP and Spark MLLib(LDA)Satish Silveri - June 11, 2020 Text Classification in Spark NLP with Bert and Universal Sentence EncodersVeysel Kocaman - April 12, 2020 Classification of Unstructured Documents into the Environmental, Social &amp; Governance (ESG) TaxonomyAlina Petukhova - May, 2020 Using Spark NLP to build a drug discovery knowledge graph for COVID-19Vishnu Vettrivel, Alexander Thomas - October 8, 2020 Build Text Categorization Model with Spark NLPSatish Silveri - Jul 8 2020 Topic Modelling with PySpark and Spark NLPMaria Obedkova - May 29 2020 Spark NLP Tasks &amp; Pipelines Videos Spark NLP Annotators, Annotations and Pipelines. John Snow Labs - 23 Oct 2019 Your first Spark NLP Pipeline. John Snow Labs - 23 Oct 2019 Natural Language Understanding at Scale with Spark NLP | DSS 2020. Veysel Kocaman - December 12, 2020 Articles Cleaning and extracting text from HTML/XML documents by using Spark NLPStefano Lori - January 13 2021 NER model with ELMo Embeddings in 5 minutes in Spark-NLPChristian Kasim Loan - Jule 2020 Applying Context Aware Spell Checking in Spark NLPAlberto Andreotti - May 2020 Spark nlp 2.5 delivers state-of-the-art accuracy for spell checking and sentiment analysisIda Lucente - May 12, 2020 Spark NLP 2.4: More Accurate NER, OCR, and Entity ResolutionIda Lucente - February 14, 2020 Introduction to Spark NLP: Foundations and Basic Components (Part-I)Veysel Kocaman - Sep 29, 2019 Introducing Spark NLP: Why would we need another NLP library (Part-I)Veysel Kocaman - October 22, 2019 Introducing Spark NLP: basic components and underlying technologies (Part-III)Veysel Kocaman - December 2, 2019 Explain document DL – Spark NLP pretrained pipelineVeysel Kocaman - January 15, 2020 Spark NLP Walkthrough, powered by TensorFlowSaif Addin Ellafi - Nov 19, 2018 Natural Language Processing with PySpark and Spark-NLPAllison Honold - Feb 5, 2020 Spark NLP for Healthcare Videos Advancing the State of the Art in Applied Natural Language Processing | Healthcare NLP Summit 2021. David Talby - 21 Apr 2021 How to Apply State-of-the-Art Natural Language Processing in Healthcare. David Talby - 15 Sep 2020 Advanced Natural Language Processing with Apache Spark NLP. David Talby - 20 Aug 2020 Applying State-of-the-art Natural Language Processing for Personalized Healthcare. David Talby - April 13, 2020 State-of-the-art Natural Language Processing at Scale. David Talby - April 13, 2020 Apache SPARK NLP: Extending SPARK ML to Deliver Fast, Scalable &amp; Unified Natural Language Processing. David Talby - June 04, 2018 State of the Art Natural Language Processing at Scale. David Talby - June 04, 2018 Spark NLP in Action: Learning to read Life Science research. Saif Addin Ellafi - May 28, 2018 Natural Language Understanding at Scale with Spark-Native NLP, Spark ML, and TensorFlow. Alexander Thomas - October 14, 2018 Apache Spark NLP for Healthcare: Lessons Learned Building Real-World Healthcare AI Systems. Veysel Kocaman - 9 Jul 2020 SNOMED entity resolver. John Snow Labs - 31 Jul 2020 NLP and its applications in Healthcare. Veysel Kocaman - 17 May 2020 Lessons Learned Building Real-World Healthcare AI Systems. Veysel Kocaman - April 13, 2020 Application of Spark NLP for Development of Multi-Modal Prediction Model from EHR | Healthcare NLP. Sutanay Choudhury - 14 Apr 2021 Best Practices in Improving NLP Accuracy for Clinical Use Cases I Healthcare NLP Summit 2021. Rajesh Chamarthi, Veysel Kocaman - 15 Apr 2021 Articles Contextual Parser: Increased Flexibility Extracting Entities in Spark NLPLuca Martial - Feb 09 2022 Named Entity Recognition for Healthcare with SparkNLP NerDL and NerCRFMaggie Yilmaz - Jul 20 2020 Roche automates knowledge extraction from pathology reports with Spark NLPCase Study Spark NLP in action: Improving patient flow forecastingCase Study Using Spark NLP to Enable Real-World Evidence (RWE) and Clinical Decision Support in OncologyVeysel Kocaman - April 13, 2020 Applying State-of-the-art Natural Language Processing for Personalized HealthcareDavid Talby - April 13, 2020 Automated Mapping of Clinical Entities from Natural Language Text to Medical TerminologiesAndrés Fernández - April 29 2020 Contextual Parser in Spark NLP: Extracting Medical Entities ContextuallyAlina Petukhova - May 28 2020 Deep6 accelerates clinical trial recruitment with Spark NLPCase Study SelectData uses AI to better understand home health patientsCase Study Explain Clinical Document Spark NLP Pretrained PipelineVeysel Kocaman - January 20, 2020 Introducing Spark NLP: State of the art NLP Package (Part-II)Veysel Kocaman - January 20, 2020 Automated Adverse Drug Event (ADE) Detection from Text in Spark NLP with BioBertVeysel Kocaman - Octover 4, 2020 Normalize drug names and dosage units with spark NLPDavid Cecchini - February 23, 2021 Spark NLP for healthcare 2.7.3 with biobert extraction models, higher accuracy, de-identification, new radiology ner model &amp; moreVeysel Kocaman - February 09, 2021 Spark OCR &amp; De-Identification Videos Maximizing Text Recognition Accuracy with Image Transformers in Spark OCR. Mykola Melnyk - June 24, 2020 Accurate de-identification, obfuscation, and editing of scanned medical documents and images. Alina Petukhova - August 19, 2020 Accurate De-Identification of Structured &amp; Unstructured Medical Data at Scale. Julio Bonis - March 18, 2020 Articles A Unified CV, OCR &amp; NLP Model Pipeline for Document Understanding at DocuSignPatrick Beukema, Michael Chertushkin - October 6, 2020 Scaling High-Accuracy Text Extraction from Images using Spark OCR on DatabricksMikola Melnyk - July 2, 2020 Spark NLP at Scale Videos Turbocharging State-of-the-art Natural Language Processing on Ray. David Talby - October 3, 2020 Articles Big Data Analysis of Meetup Events using Spark NLP, Kafka and Vegas VisualizationAndrei Deuşteanu - August 25, 2020 Setup Spark NLP on Databricks in 2 Minutes and get the taste of scalable NLPChristian Kasim Loan - May 25, 2020 Real-time trending topic detection using Spark NLP, Kafka and Vegas VisualizationValentina Crisan - Oct 15, 2020 Mueller Report for Nerds! Spark meets NLP with TensorFlow and BERTMaziyar Panahi - May 1, 2019 Spark in Docker in Kubernetes: A Practical Approach for Scalable NLPJürgen Schmidl - Jan 18 2020 Running Spark NLP in Docker Container for Named Entity Recognition and Other NLP FeaturesYuefeng Zhang - Jun 5 2020 Annotation Lab Videos Accelerating Clinical Data Abstraction and Real-World Data Curation with Active Learning, Dia Trambitas - Apr 15, 2021 MLOPS Veysel &amp; Dia. Dia Trambitas, Veysel Kocaman - July 16, 2020 Best Practices &amp; Tools for Accurate Document Annotation and Data Abstraction. Dia Trambitas - May 27, 2020 Articles John Snow Labs’ data annotator &amp; active learning for human-in-the-loop AI is now included with all subscriptionsIda Lucente - May 26, 2020 Auto NLP: Pretrain, Tune &amp; Deploy State-of-the-art Models Without CodingDia Trambitas - October 6, 2020 Lesson Learned annotating training data for healthcare NLP projectsRebecca Leung, Marianne Mak - October 8, 2020 Task review workflows in the annotation labDia Trambitas - March 08, 2021 The annotation lab 1.1 is here with improvements to speed, accuracy, and productivityIda Lucente - January 20, 2021 Tips and tricks on how to annotate assertion in clinical textsMauro Nievas Offidani - November 24, 2020 Spark NLP Benchmarks Articles Biomedical Named Entity Recognition at ScaleVeysel Kocaman, David Talby - November 12, 2020 NLP Industry Survey Analysis: the industry landscape of natural language use cases in 2020Paco Nathan - October 6, 2020 Comparing the Functionality of Open Source Natural Language Processing LibrariesMaziyar Panahi and David Talby - April 7, 2019 SpaCy or Spark NLP — A Benchmarking ComparisonMustafa Aytuğ Kaya - Aug 27, 2020 Comparing production-grade NLP libraries: Training Spark-NLP and spaCy pipelinesSaif Addin Ellafi - February 28, 2018 Comparing production-grade NLP libraries: Running Spark-NLP and spaCy pipelinesSaif Addin Ellafi - February 28, 2018 Comparing production-grade NLP libraries: Accuracy, performance, and scalabilitySaif Addin Ellafi - February 28, 2018 Spark NLP Awards Articles John Snow Labs is healthcare tech outlook’s 2020 healthcare analytics provider of the yearIda Lucente - July 14, 2020 John Snow Labs wins the 2020 artificial intelligence excellence awardIda Lucente - April 27, 2020 John Snow Labs is named ‘2019 ai platform of the yearIda Lucente - August 14, 2019 Spark NLP is the world’s most widely used nlp library by enterprise practitionersIda Lucente - May 6, 2019 John Snow Labs’ spark nlp wins “most significant open source project” at the strata data awardsIda Lucente April 1 - 2019 John Snow Labs named “artificial intelligence solution provider of the year” by cio reviewIda Lucente - February 7, 2019",
    "url": "/learnold",
    "relUrl": "/learnold"
  },
  "37": {
    "id": "37",
    "title": "The NLP Learning Hub",
    "content": "The Technology Spark NLP Auto NLP The Technology in Action NLP on Databricks Industry Trends No-Code AI Responsible NLP Data Philanthropy Announcements Awards",
    "url": "/learn",
    "relUrl": "/learn"
  },
  "38": {
    "id": "38",
    "title": "Legal Document Splitting - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/long_document_splitting",
    "relUrl": "/long_document_splitting"
  },
  "39": {
    "id": "39",
    "title": "Middle Eastern Languages - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/middle_eastern_languages",
    "relUrl": "/middle_eastern_languages"
  },
  "40": {
    "id": "40",
    "title": "Spark NLP - Experiment Tracking",
    "content": "Serialization and Experiment Tracking with MLFlow (Python) About MLFLow Spark NLP uses Spark MLlib Pipelines, what are natively supported by MLFlow. MLFlow is, as stated in their official webpage, an open source platform for the machine learning lifecycle, that includes: Mlflow Tracking: Record and query experiments: code, data, config, and results MLflow Projects: Package data science code in a format to reproduce runs on any platform MLflow Models: Deploy machine learning models in diverse serving environments Model Registry: Store, annotate, discover, and manage models in a central repository MLFlow is also integrated in Databricks, so you will be able to track your experiments in any Databricks environment, and even use MLFLow Model Registry to serve models for production purposes, using the REST API (see section “Productionizing Spark NLP”). We will be using in this documentation Jupyter Notebook syntax. Available configurations There are several ways of deploying a MLFlow Model Registry: 1) Scenario 1: MLflow on localhost with no Tracking Server: This scenario uses a localhost folder (./mlruns by default) to serialize and store your models, but there is no tracking server available (version tracking will be disabled). 2) Scenario 2: MLflow on localhost with a Tracking Server This scenario uses a localhost folder (./mlruns by default) to serialize and store your mdoels, and a database as a Tracking Sever. It uses SQLAlchemy under the hood, so the following databases are supported: mssql, postgresql, mysql, sqlite. We are going to show how to implement this scenario with a mysql database. 3) Scenario 3: MLflow on remote with a Tracking Server This scenario is a remote version of Scenario 2. It uses a remote S3 bucket to serialize and store your mdoels, and a database as a Tracking Sever. Again, it uses SQLAlchemy for the Tracking Server under the hood, so the following databases are supported: mssql, postgresql, mysql, sqlite. In this case, you can use any service as AWS RDS or Azure SQL Database. Requirements As we said before, we are going to showcase Scenario 2. Since we want to have a Experiment Tracking Server with mysql, we will need to install in our server the requirements for it. !sudo apt-get install -y python-mysqldb mysql-server libmysqlclient-dev Also, let’s install a mysql Python interface library, called pymsql, to access mysql databases. !pip install mysqlclient pymysql We will also need MLFlow (this example was tested with version 1.21.0) !pip install mlflow Finally, make sure you follow the Spark NLP installation, available here Instantiating a MySQL database We are going to use Docker to instantiate a MySQL container with a persistent volume, but you can install it directly on your machine without Docker. To do that, we will need to have installed (feel free to skip this step if you will install MySql without Docker): Docker Docker-compose In our case, I used this docker-compose.yml file to instantiate a mysql database with a persistent volume: version: &#39;3&#39; services: # MySQL mflow_models: container_name: mlflow_models image: mysql:8.0 command: mysqld --default-authentication-plugin=mysql_native_password --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci environment: MYSQL_ROOT_PASSWORD: root MYSQL_DATABASE: mlflow_models MYSQL_USER: jsl MYSQL_PASSWORD: passpass MYSQL_ALLOW_EMPTY_PASSWORD: &quot;yes&quot; ports: - &#39;3306:3306&#39; volumes: - &#39;./docker/db/data:/var/lib/mysql&#39; - &#39;./docker/db/my.cnf:/etc/mysql/conf.d/my.cnf&#39; - &#39;./docker/db/sql:/docker-entrypoint-initdb.d&#39; Just by executing the following command in the folder where your docker-compose.yml file is, you will have your MySQL engine, with a mlflow_models database running and prepared for MLFlow Experiment Tracking: !sudo docker-compose up -d . Make sure it’s running using the following command: `!docker ps | grep -o mlflow_models Connection string You will need a connection string that will tell MLFlow (SQLAlchemy) how to reach that database. Connections strings in SQLALchemy have this format: &lt;dialect&gt;+&lt;driver&gt;://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt; In our case, we declare a CONNECTION_STRING var as: CONNECTION_STRING = f&quot;mysql+pymysql://root:root@localhost:3306/mlflow_models&quot; Imports Let’s now import all the libraries we will need. Generic imports import json import os from sklearn.metrics import classification_report import time import mlflow from mlflow.models.signature import infer_signature from urllib.parse import urlparse import pandas as pd import glob Spark NLP imports import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.training import * from pyspark.ml import Pipeline import pyspark.sql.functions as F from sparknlp.training import CoNLL from pyspark.sql import SparkSession Setting the connection string in MLFLow Now that we have imported mlflow, let’s set the connection string we had prepared before. mlflow.set_tracking_uri(CONNECTION_STRING) mlflow.get_tracking_uri() # This checks if it was set properly Constant with pip_requirements MLFLow requires either a conda_env (conda environment) definition of the requirements of your models, or a pip_requirements list with all pip libraries. We will use this second way, so let’s prepare the list with Spark NLP and MLFlow: PIP_REQUIREMENTS = [f&quot;sparknlp=={sparknlp.version()}&quot;, f&quot;mlflow=={mlflow.__version__}&quot;] PIP_REQUIREMENTS # This checks if it was set properly Training a NERDLApproach() We will be showcasing the serialization and experiment tracking of NERDLApproach(). There is one specific util that is able to parse the log of that approach in order to extract the metrics and charts. Let’s get it. Ner Log Parser Util !wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/utils/ner_image_log_parser.py Now, let’s import the library: import ner_image_log_parser Starting a SparkNLP session It’s important we create a Spark NLP Session using the Session Builder, since we need to specify the jars not only of Spark NLP, but also of MLFlow. def start(): builder = SparkSession.builder .appName(&quot;Spark NLP Licensed&quot;) .master(&quot;local[80]&quot;) .config(&quot;spark.driver.memory&quot;, &quot;256G&quot;) .config(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) .config(&quot;spark.kryoserializer.buffer.max&quot;, &quot;2000M&quot;) .config(&quot;spark.driver.maxResultSize&quot;,&quot;4000M&quot;) .config(&quot;spark.jars.packages&quot;, &quot;com.johnsnowlabs.nlp:spark-nlp_2.12:3.3.2,org.mlflow:mlflow-spark:1.21.0&quot;) return builder.getOrCreate() spark = start() Training dataset preparation Let’s download some training and test datasets: !wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.train !wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/src/test/resources/conll2003/eng.testa TRAIN_DATASET = &quot;eng.train&quot; TEST_DATASET = &quot;eng.testa&quot; Let’s read the training dataset: training_data = CoNLL().readDataset(spark, TRAIN_DATASET) training_data.show(3) Let’s get the size: %%time TRAINING_SIZE = training_data.count() TRAINING_SIZE Hyperparameters configuration Let’s configure our hyperparameter values. MODEL_NAME = &#39;&#39; # Add your model name here. Example: clinical_ner EXPERIMENT_NAME = &#39;&#39; # Add your experiment name here. Example: testing_dropout OUTPUT_DIR = f&quot;{MODEL_NAME}_{EXPERIMENT_NAME}_output&quot; # Output folder of all your model artifacts MODEL_DIR = f&quot;model&quot; # Name of the folder where the MLFlow model will be stored MAX_EPOCHS = 10 # Adapt me to your experiment LEARNING_RATE = 0.003 # Adapt me to your experiment BATCH_SIZE = 2048 # Adapt me to your experiment RANDOM_SEED = 0 # Adapt me to your experiment VALIDATION_SPLIT = 0.1 # Adapt me to your experiment Creating the experiment Now, we are ready to instantiate an experiment in MLFlow EXPERIMENT_ID = mlflow.create_experiment(f&quot;{MODEL_NAME}_{EXPERIMENT_NAME}&quot;) Each time you want to test a different thing, change the EXPERIMENT_NAME and rerun the line above to create a new entry in the experiment. By changing the experiment name, a new experiment ID will be generated. Each experiment ID groups all runs in separates folder inside ./mlruns. Pipeline creation document = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) sentence = SentenceDetector() .setInputCols([&#39;document&#39;]) .setOutputCol(&#39;sentence&#39;) token = Tokenizer() .setInputCols([&#39;sentence&#39;]) .setOutputCol(&#39;token&#39;) embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) ner_approach = NerDLApproach() .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setLabelColumn(&quot;label&quot;) .setOutputCol(&quot;ner&quot;) .setMaxEpochs(MAX_EPOCHS) .setLr(LEARNING_RATE) .setBatchSize(BATCH_SIZE) .setRandomSeed(RANDOM_SEED) .setVerbose(1) .setEnableOutputLogs(True) .setIncludeConfidence(True) .setIncludeAllConfidenceScores(True) .setEvaluationLogExtended(True) .setOutputLogsPath(OUTPUT_DIR) .setValidationSplit(VALIDATION_SPLIT) Let’s create a preprocessing pipeline without the NerDLApproach(): ner_preprocessing_pipeline = Pipeline(stages=[ document, sentence, token, embeddings ]) And a training pipeline with it: ner_training_pipeline = Pipeline(stages = ner_preprocessing_pipeline.getStages() + [ner_approach]) Preparing inference objects Now, let’s prepare the inference as well, since we will train and infer afterwards, and store all the results of training and inference as artifacts in our MLFlow object. Test dataset preparation test_data = CoNLL().readDataset(spark, TEST_DATASET) Setting the names of the inference objects INFERENCE_NAME = &quot;inference.parquet&quot; # This is the name of the results inference on the test dataset, serialized in parquet, CLASSIFICATION_REPORT_LOG_NAME = &quot;classification_report.txt&quot; # Name of the classification report from scikit-learn on Ner Entities PREC_REC_F1_NAME = &quot;precrecf1.jpg&quot; # Name of the precision-recall-f1 file MACRO_MICRO_AVG_NAME = &quot;macromicroavg.jpg&quot; # Name of the macro-micro-average file LOSS_NAME = &quot;loss.jpg&quot; # Name of the loss plot file Now, let’s run the experiment The experiment has already been created before (see “Creating the experiment” section). So we take the ID and start a run. Each time you run execute this cell, you will get a different run for the same experiment. If you want to change the experiment id (and name), go back to “Hyperparameters configuration”. As mentioned before, by changing the experiment name, a new experiment ID will be generated. Each experiment ID groups all runs in separates folder inside ./mlruns. with mlflow.start_run(experiment_id=EXPERIMENT_ID) as run: # Printing RUN and EXPERIMENT ID # ============================== print(f&quot;Model name: {MODEL_NAME}&quot;) RUN_ID = run.info.run_id print(f&quot;Run id: {RUN_ID}&quot;) EXPERIMENT_ID = run.info.experiment_id print(f&quot;Experiment id: {EXPERIMENT_ID}&quot;) # Training the model # ================== print(&quot;Starting training...&quot;) start = time.time() ner_model = ner_training_pipeline.fit(training_data) end = time.time() ELAPSED_SEC_TRAINING = end - start print(&quot;- Finished!&quot;) # Saving the model in TensorFlow (ready to be loaded using NerDLModel.load) # ============================== print(&quot;Saving the model...&quot;) ner_model.stages[-1].write().overwrite().save(f&quot;{OUTPUT_DIR}/{MODEL_DIR}/{MODEL_NAME}&quot;) print(&quot;- Finished!&quot;) # Loading the model (to check everything worked) # ============================== print(&quot;Loading back the model...&quot;) loaded_ner_model = NerDLModel.load(f&quot;{OUTPUT_DIR}/{MODEL_DIR}/{MODEL_NAME}&quot;) .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;]) .setOutputCol(&quot;ner&quot;) # Creating the inference pipeline with the loaded model # ============================== ner_prediction_pipeline = Pipeline(stages = ner_preprocessing_pipeline.getStages() + [loaded_ner_model]) # Triggering inference # ============================== print(&quot;Starting inference...&quot;) prediction_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) prediction_model = ner_prediction_pipeline.fit(prediction_data) start = time.time() prediction_model.transform(test_data).write.mode(&#39;overwrite&#39;).parquet(f&quot;{OUTPUT_DIR}/{INFERENCE_NAME}&quot;) end = time.time() ELAPSED_SEC_INFERENCE = end - start print(&quot;- Finished!&quot;) # Calculating NER metrics from logs using scikit-learn &#39;classification_report&#39; # ============================== print(&quot;Starting metric calculation...&quot;) predictions = spark.read.parquet(f&quot;{OUTPUT_DIR}/{INFERENCE_NAME}&quot;) preds_df = predictions.select(F.explode(F.arrays_zip(&#39;token.result&#39;,&#39;label.result&#39;,&#39;ner.result&#39;)).alias(&quot;cols&quot;)) .select(F.expr(&quot;cols[&#39;0&#39;]&quot;).alias(&quot;token&quot;), F.expr(&quot;cols[&#39;1&#39;]&quot;).alias(&quot;ground_truth&quot;), F.expr(&quot;cols[&#39;2&#39;]&quot;).alias(&quot;prediction&quot;)).toPandas() preds_df = preds_df.fillna(value=&#39;O&#39;) with open(f&#39;{OUTPUT_DIR}/{CLASSIFICATION_REPORT_LOG_NAME}&#39;, &#39;w&#39;) as f: metrics = classification_report(preds_df[&#39;ground_truth&#39;], preds_df[&#39;prediction&#39;]) f.write(metrics) metrics_dict = classification_report(preds_df[&#39;ground_truth&#39;], preds_df[&#39;prediction&#39;], output_dict=True) print(&quot;- Finished!&quot;) # Printing metrics # ============================== print(f&quot;Training dataset size: {TRAINING_SIZE}&quot;) print(f&quot;Training time (sec): {ELAPSED_SEC_TRAINING}&quot;) print(f&quot;Inference dataset size: {TEST_SIZE}&quot;) print(f&quot;Inference time (sec): {ELAPSED_SEC_INFERENCE}&quot;) print(f&quot;Metrics: n&quot;) print(metrics) # Logging all our params, metrics, charts and artifacts using MLFlow # - log_param: logs a configuration param # - log_artifacts: logs a folder and all its files # - log_artifact: adds a file # - log_metric: logs a metric, what allows you use the MLFlow UI to visually compare results # ============================== print(&quot;Logging params, artifacts, metrics and charts in MLFlow&quot;) mlflow.log_param(&quot;training_size&quot;, TRAINING_SIZE) mlflow.log_param(&quot;training_time&quot;, ELAPSED_SEC_TRAINING) mlflow.log_param(&quot;model_name&quot;, MODEL_NAME) mlflow.log_param(&quot;test_size&quot;, TEST_SIZE) mlflow.log_param(&quot;test_time&quot;, ELAPSED_SEC_INFERENCE) mlflow.log_param(&quot;run_id&quot;, RUN_ID) mlflow.log_param(&quot;max_epochs&quot;, MAX_EPOCHS) mlflow.log_param(&quot;learning_rate&quot;, LEARNING_RATE) mlflow.log_param(&quot;batch_size&quot;, BATCH_SIZE) mlflow.log_param(&quot;random_seed&quot;, RANDOM_SEED) mlflow.log_param(&quot;validation_split&quot;, VALIDATION_SPLIT) for file in glob.glob(f&quot;{OUTPUT_DIR}/*.log&quot;): images = {} images.update(ner_image_log_parser.get_charts(file, img_prec_rec_f1_path=f&quot;{OUTPUT_DIR}/{PREC_REC_F1_NAME}&quot;, img_macro_micro_avg_path=f&quot;{OUTPUT_DIR}/{MACRO_MICRO_AVG_NAME}&quot;)) images.update(ner_image_log_parser.loss_plot(file, img_loss_path=f&quot;{OUTPUT_DIR}/{LOSS_NAME}&quot;)) mlflow.log_artifacts(OUTPUT_DIR) mlflow.log_artifact(TRAIN_DATASET) mlflow.log_artifact(TEST_DATASET) for k,v in metrics_dict.items(): if isinstance(v, dict): for kv, vv in v.items(): mlflow.log_metric(f&quot;{k}_{kv}&quot;, vv) else: mlflow.log_metric(k, v) print(&quot;- Finished!&quot;) print(&quot;Logging the model in MLFlow&quot;) # ============================== # Logging the model to be explored in the MLFLow UI tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme # Model registry does not work with file store if tracking_url_type_store != &quot;file&quot;: # Register the model # There are other ways to use the Model Registry, which depends on the use case, # please refer to the doc for more information: # https://mlflow.org/docs/latest/model-registry.html#api-workflow mlflow.spark.log_model(ner_model, f&quot;{MODEL_NAME}_{EXPERIMENT_ID}_{RUN_ID}&quot;, registered_model_name=MODEL_NAME, pip_requirements=PIP_REQUIREMENTS) else: mlflow.spark.log_model(ner_model, f&quot;{MODEL_NAME}_{EXPERIMENT_ID}_{RUN_ID}&quot;, pip_requirements=PIP_REQUIREMENTS) print(&quot;- Finished!&quot;) # Saving the model, in case you want to export it # ============================== print(&quot;Saving the model...&quot;) input_example = predictions.select(&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;).limit(1).toPandas() mlflow.spark.save_model(loaded_ner_model, MODEL_NAME, pip_requirements=PIP_REQUIREMENTS, input_example=input_example) print(&quot;- Finished!&quot;) This is an example of the output generated: Model name: NER_base_2048_mlflow Run id: 5f8601fbfc664b3b91c7c61cde31e16d Experiment id: 2 Starting training... - Finished! Saving the model... - Finished! Loading back the model... Starting inference... - Finished! Starting metric calculation... - Finished! Training dataset size: 14041 Training time (sec): 12000.3835768699646 Inference dataset size: 3250 Inference time (sec): 2900.713200330734253 Metrics: precision recall f1-score support B-LOC 0.85 0.82 0.83 1837 B-MISC 0.86 0.83 0.81 922 B-ORG 0.81 0.83 0.82 1341 B-PER 0.86 0.81 0.80 1842 I-LOC 0.80 0.80 0.80 257 I-MISC 0.80 0.80 0.80 346 I-ORG 0.83 0.89 0.80 751 I-PER 0.86 0.83 0.82 1307 O 0.81 0.98 0.84 43792 accuracy 0.87 52395 macro avg 0.88 0.83 0.88 52395 weighted avg 0.84 0.87 0.85 52395 Logging params, artifacts, metrics and charts in MLFlow - Finished! Logging the model in MLFlow Registered model &#39;NER_base_2048_mlflow&#39; already exists. Creating a new version of this model... 2021/11/25 11:51:24 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation. Model name: NER_base_2048_mlflow, version 2 Created version &#39;2&#39; of model &#39;NER_base_2048_mlflow&#39;. - Finished! Saving the model... - Finished! MLFLow UI to check results Now, we just need to launch the MLFLow UI to see: All the experiments All the runs in each experiment The automatic versioning in the Tracking Server database in MySQL THe MLFlow model, and the TensorFlow version as well The UI for comparing the metrics we set using log_metrics The UI for visualizing the image artifacts we have logged (charts) etc !mlflow ui --backend-store-uri $CONNECTION_STRING Some example screenshots",
    "url": "/docs/en/mlflow",
    "relUrl": "/docs/en/mlflow"
  },
  "41": {
    "id": "41",
    "title": "Available Models and Pipelines",
    "content": "",
    "url": "/models",
    "relUrl": "/models"
  },
  "42": {
    "id": "42",
    "title": "NLP Server",
    "content": "This is a ready to use NLP Server for analyzing text documents using NLU library. Over 4500+ industry grade NLP Models in 300+ Languages are available to use via a simple and intuitive UI, without writing a line of code. For more expert users and more complex tasks, NLP Server also provides a REST API that can be used to process high amounts of data. The models, refered to as spells, are provided by the NLU library and powered by the most widely used NLP library in the industry, Spark NLP. NLP Server is free for everyone to download and use. There is no limitation in the amount of text to analyze. You can setup NLP-Server as a Docker Machine in any enviroment or get it via the AWS Marketplace in just 1 click. Web UI The Web UI is accessible at the following URL: http://localhost:5000/ It allows a very simple and intuitive interaction with the NLP Server. As a first step the user chooses the spell from the first dropdown. All NLU spells are available. Then the user has to provide a text document for analysis. This can be done by either copy/pasting text on the text box, or by uploading a csv/json file. After selecting the grouping option, the user clicks on the Preview button to get the results for the first 10 rows of text. REST API NLP Server includes a REST API which can be used to process any amount of data using NLU. Once you deploy the NLP Server, you can access the API documentation at the following URL http://localhost:5000/docs. Integrate via the Rest API Rest APIs are a popular way to integrate different services into one common platform. NLP Server offers its own API to offer a quick programmatic integration with customers’ services and applications. Bellow is a quick overview of the provided endpoints. More details are provided in the API documentation available http://localhost:5000/docs. Start to analyze Endpoint : /results Method : POST Content-Type (Format) : multipart/form-data Parameters: Spell – the spell that you want to use for this analyze (if you want to run multiple spells you should join them with space character) Data – The data to analyse that can be a single text or an array of strings or files. Grouping – can be choosen from [“document”, “sentence”, “entity”, “word”]. The default value is “” for automatic selection based on spell. Format – The format of the provided input. The default value is “text”. Response: uuid – the unique identifier for the analysis process. Check the status of an analysis process Endpoint : /results/{uuid}/status Method : GET Content-Type (Format) : application/json Response: code – the status code that can be one of “progress”, “success”, “failure”, “broken spell”, “invalid license”, “licensed spell with no license” message – the status message Get the results After ensuring the status of an analysis is “success” you can get the results: Endpoint : /results/{uuid} Method : GET Content-Type (Format) : application/json Parameters: target – if the specified target is “preview” you only get a small part of results. Response: A JSON object that contains the results generated by the spell (each spell has their own specific keys) How to use in Python import requests # Invoke Processing with tokenization spell r = requests.post(f&#39;http://localhost:5000/api/results&#39;,json={&quot;spell&quot;: &quot;tokenize&quot;,&quot;data&quot;: &quot;I love NLU! &lt;3&quot;}) # Use the uuid to get your processed data uuid = r.json()[&#39;uuid&#39;] # Get status of processing r = requests.get(f&#39;http://localhost:5000/api/results/{uuid}/status&#39;).json &gt;&gt;&gt; {&#39;status&#39;: {&#39;code&#39;: &#39;success&#39;, &#39;message&#39;: None}} # Get results r = requests.get(f&#39;http://localhost:5000/api/results/{uuid}&#39;).json() &gt;&gt;&gt; {&#39;sentence&#39;: {&#39;0&#39;: [&#39;I love NLU! &lt;3&#39;]}, &#39;document&#39;: {&#39;0&#39;: &#39;I love NLU! &lt;3&#39;}, &#39;token&#39;: {&#39;0&#39;: [&#39;I&#39;, &#39;love&#39;, &#39;NLU&#39;, &#39;!&#39;, &#39;&lt;3&#39;]}} Import a license key Thanks to the close integration between NLP Server and https://my.JohnSnowLabs.com website, users can easily select and import one of the available licenses to be used on NLP Server. The steps to execute for this are: 1.Click on Login via MYJSL button on the menu bar. 2.In the pop-up window click on the Authorize button. 3.After redirecting back to NLP Server click on the Choose License button. 4.In the modal choose the license that you want to use and then click on the Select button. 5.After the above steps you will see this success alert on the top right of the page. That confirms the import of license completed successfully.",
    "url": "/docs/en/nlp_server/nlp_server",
    "relUrl": "/docs/en/nlp_server/nlp_server"
  },
  "43": {
    "id": "43",
    "title": "Spark NLP - Pipelines",
    "content": "Pretrained Pipelines have moved to Models Hub. Please follow this link for the updated list of all models and pipelines: Models Hub English NOTE: noncontrib pipelines are compatible with Windows operating systems. Pipelines Name Explain Document ML explain_document_ml Explain Document DL explain_document_dl Explain Document DL Win explain_document_dl_noncontrib Explain Document DL Fast explain_document_dl_fast Explain Document DL Fast Win explain_document_dl_fast_noncontrib Recognize Entities DL recognize_entities_dl Recognize Entities DL Win recognize_entities_dl_noncontrib OntoNotes Entities Small onto_recognize_entities_sm OntoNotes Entities Large onto_recognize_entities_lg Match Datetime match_datetime Match Pattern match_pattern Match Chunk match_chunks Match Phrases match_phrases Clean Stop clean_stop Clean Pattern clean_pattern Clean Slang clean_slang Check Spelling check_spelling Analyze Sentiment analyze_sentiment Analyze Sentiment DL analyze_sentimentdl_use_imdb Analyze Sentiment DL analyze_sentimentdl_use_twitter Dependency Parse dependency_parse explain_document_ml import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;The Paris metro will soon enter the 21st century, ditching single-use paper tickets for rechargeable electronic cards.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;explain_document_ml&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_ml,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 7 more fields] ++--+--+--+--+--+--+--+--+ | id| text| document| sentence| token| checked| lemmas| stems| pos| ++--+--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...| | 2|The Paris metro w...|[[document, 0, 11...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...| ++--+--+--+--+--+--+--+--+ */ explain_document_dl import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_dl,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 10 more fields] ++--+--+--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| checked| lemma| stem| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[token, 0, 5, Go...|[[document, 0, 10...|[[token, 0, 5, Go...|[[token, 0, 5, Go...|[[token, 0, 5, go...|[[pos, 0, 5, NNP,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...| | 2|The Paris metro w...|[[document, 0, 11...|[[token, 0, 2, Th...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 4, 8, Pa...| ++--+--+--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Google, TensorFlow] | |[Donald John Trump, United States]| +-+ */ recognize_entities_dl import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Google has announced the release of a beta version of the popular TensorFlow machine learning library&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;recognize_entities_dl&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(entity_recognizer_dl,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 6 more fields] ++--+--+--+--+--+--+--+ | id| text| document| sentence| token| embeddings| ner| ner_converter| ++--+--+--+--+--+--+--+ | 1|Google has announ...|[[document, 0, 10...|[[document, 0, 10...|[[token, 0, 5, Go...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 5, Go...| | 2|Donald John Trump...|[[document, 0, 92...|[[document, 0, 92...|[[token, 0, 5, Do...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 16, D...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Google, TensorFlow] | |[Donald John Trump, United States]| +-+ */ onto_recognize_entities_sm Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the OntoNotes corpus and supports the identification of 18 entities. import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London, from 2008 to 2016, before rejoining Parliament. &quot;), (2, &quot;A little less than a decade later, dozens of self-driving startups have cropped up while automakers around the world clamor, wallet in hand, to secure their place in the fast-moving world of fully automated transportation.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;onto_recognize_entities_sm&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.1.0 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(onto_recognize_entities_sm,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 6 more fields] ++--+--+--+--+--+--+ | id| text| document| token| embeddings| ner| entities| ++--+--+--+--+--+--+ | 1|Johnson first ent...|[[document, 0, 17...|[[token, 0, 6, Jo...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 6, Jo...| | 2|A little less tha...|[[document, 0, 22...|[[token, 0, 0, A,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 32, A...| ++--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* ++ |result | ++ |[Johnson, first, 2001, Parliament, eight years, London, 2008 to 2016, Parliament]| |[A little less than a decade later, dozens] | ++ */ onto_recognize_entities_lg Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the OntoNotes corpus and supports the identification of 18 entities. import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;Johnson first entered politics when elected in 2001 as a member of Parliament. He then served eight years as the mayor of London, from 2008 to 2016, before rejoining Parliament. &quot;), (2, &quot;A little less than a decade later, dozens of self-driving startups have cropped up while automakers around the world clamor, wallet in hand, to secure their place in the fast-moving world of fully automated transportation.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;onto_recognize_entities_lg&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.1.0 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(onto_recognize_entities_lg,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 6 more fields] ++--+--+--+--+--+--+ | id| text| document| token| embeddings| ner| entities| ++--+--+--+--+--+--+ | 1|Johnson first ent...|[[document, 0, 17...|[[token, 0, 6, Jo...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 6, Jo...| | 2|A little less tha...|[[document, 0, 22...|[[token, 0, 0, A,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 32, A...| ++--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Johnson, first, 2001, Parliament, eight years, London, 2008, 2016, Parliament]| |[A little less than a decade later, dozens] | +-+ */ match_datetime DateMatcher yyyy/MM/dd import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;I would like to come over and see you in 01/02/2019.&quot;), (2, &quot;Donald John Trump (born June 14, 1946) is the 45th and current president of the United States&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;match_datetime&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(match_datetime,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 4 more fields] ++--+--+--+--+--+ | id| text| document| sentence| token| date| ++--+--+--+--+--+ | 1|I would like to c...|[[document, 0, 51...|[[document, 0, 51...|[[token, 0, 0, I,...|[[date, 41, 50, 2...| | 2|Donald John Trump...|[[document, 0, 92...|[[document, 0, 92...|[[token, 0, 5, Do...|[[date, 24, 36, 1...| ++--+--+--+--+--+ */ annotation.select(&quot;date.result&quot;).show(false) /* ++ |result | ++ |[2019/01/02]| |[1946/06/14]| ++ */ match_pattern RegexMatcher (match phone numbers) import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;You should call Mr. Jon Doe at +33 1 79 01 22 89&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;match_pattern&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(match_pattern,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 4 more fields] ++--+--+--+--+--+ | id| text| document| sentence| token| regex| ++--+--+--+--+--+ | 1|You should call M...|[[document, 0, 47...|[[document, 0, 47...|[[token, 0, 2, Yo...|[[chunk, 31, 47, ...| ++--+--+--+--+--+ */ annotation.select(&quot;regex.result&quot;).show(false) /* +-+ |result | +-+ |[+33 1 79 01 22 89]| +-+ */ match_chunks The pipeline uses regex &lt;DT/&gt;?/&lt;JJ/&gt;*&lt;NN&gt;+ import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val testData = spark.createDataFrame(Seq( (1, &quot;The book has many chapters&quot;), (2, &quot;the little yellow dog barked at the cat&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val pipeline = PretrainedPipeline(&quot;match_chunks&quot;, lang=&quot;en&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 testData: org.apache.spark.sql.DataFrame = [id: int, text: string] pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(match_chunks,en,public/models) annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 5 more fields] ++--+--+--+--+--+--+ | id| text| document| sentence| token| pos| chunk| ++--+--+--+--+--+--+ | 1|The book has many...|[[document, 0, 25...|[[document, 0, 25...|[[token, 0, 2, Th...|[[pos, 0, 2, DT, ...|[[chunk, 0, 7, Th...| | 2|the little yellow...|[[document, 0, 38...|[[document, 0, 38...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|[[chunk, 0, 20, t...| ++--+--+--+--+--+--+ */ annotation.select(&quot;chunk.result&quot;).show(false) /* +--+ |result | +--+ |[The book] | |[the little yellow dog, the cat]| +--+ */ French Pipelines Name Explain Document Large explain_document_lg Explain Document Medium explain_document_md Entity Recognizer Large entity_recognizer_lg Entity Recognizer Medium entity_recognizer_md Feature Description NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura POS Trained by PerceptronApproach annotator on the Universal Dependencies Size Model size indicator, md and lg. The large pipeline uses glove_840B_300 and the medium uses glove_6B_300 WordEmbeddings French explain_document_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_lg&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,fr,public/models) testData: org.apache.spark.sql.DataFrame = [id: bigint, text: string] annotation: org.apache.spark.sql.DataFrame = [id: bigint, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[token, 0, 12, C...|[[pos, 0, 12, ADV...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[token, 0, 7, Em...|[[pos, 0, 7, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /*+-+ |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ French explain_document_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_md&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_md,fr,public/models) testData: org.apache.spark.sql.DataFrame = [id: bigint, text: string] annotation: org.apache.spark.sql.DataFrame = [id: bigint, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[token, 0, 12, C...|[[pos, 0, 12, ADV...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[token, 0, 7, Em...|[[pos, 0, 7, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, au CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ French entity_recognizer_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_lg&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ French entity_recognizer_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_md&quot;, lang=&quot;fr&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;Contrairement à Quentin Tarantino, le cinéma français ne repart pas les mains vides de la compétition cannoise.&quot;), (2, &quot;Emmanuel Jean-Michel Frédéric Macron est le fils de Jean-Michel Macron, né en 1950, médecin, professeur de neurologie au CHU d&#39;Amiens4 et responsable d&#39;enseignement à la faculté de médecine de cette même ville5, et de Françoise Noguès, médecin conseil à la Sécurité sociale&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 0|Contrairement à Q...|[[document, 0, 11...|[[token, 0, 12, C...|[[document, 0, 11...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 16, 32, ...| | 1|Emmanuel Jean-Mic...|[[document, 0, 27...|[[token, 0, 7, Em...|[[document, 0, 27...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 35, E...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /*+-+ |result | +-+ |[Quentin Tarantino] | |[Emmanuel Jean-Michel Frédéric Macron, Jean-Michel Macron, au CHU d&#39;Amiens4, Françoise Noguès, Sécurité sociale]| +-+ */ Italian Pipelines Name Explain Document Large explain_document_lg Explain Document Medium explain_document_md Entity Recognizer Large entity_recognizer_lg Entity Recognizer Medium entity_recognizer_md Feature Description NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Lemma Trained by Lemmatizer annotator on DXC Technology dataset POS Trained by PerceptronApproach annotator on the Universal Dependencies Size Model size indicator, md and lg. The large pipeline uses glove_840B_300 and the medium uses glove_6B_300 WordEmbeddings Italian explain_document_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_lg&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[token, 0, 1, La...|[[pos, 0, 1, DET,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 3, 6, FI...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[token, 0, 4, Re...|[[pos, 0, 4, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +--+ |result | +--+ |[FIFA, Zidane, Materazzi] | |[Reims, Domani, Mondiali femminili]| +--+ */ Italian explain_document_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;explain_document_md&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+--+--+ | id| text| document| token| sentence| lemma| pos| embeddings| ner| entities| ++--+--+--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[token, 0, 1, La...|[[pos, 0, 1, DET,...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 9, La...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[token, 0, 4, Re...|[[pos, 0, 4, PROP...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[La FIFA, Zidane, Materazzi]| |[Reims, Domani, Mondiali] | +-+ */ Italian entity_recognizer_lg import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_lg&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 3, 6, FI...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +--+ |result | +--+ |[FIFA, Zidane, Materazzi] | |[Reims, Domani, Mondiali femminili]| +--+ */ Italian entity_recognizer_md import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP SparkNLP.version() val pipeline = PretrainedPipeline(&quot;entity_recognizer_md&quot;, lang=&quot;it&quot;) val testData = spark.createDataFrame(Seq( (1, &quot;La FIFA ha deciso: tre giornate a Zidane, due a Materazzi&quot;), (2, &quot;Reims, 13 giugno 2019 – Domani può essere la giornata decisiva per il passaggio agli ottavi di finale dei Mondiali femminili.&quot;) )).toDF(&quot;id&quot;, &quot;text&quot;) val annotation = pipeline.transform(testData) annotation.show() /* import com.johnsnowlabs.nlp.pretrained.PretrainedPipeline import com.johnsnowlabs.nlp.SparkNLP 2.0.8 pipeline: com.johnsnowlabs.nlp.pretrained.PretrainedPipeline = PretrainedPipeline(explain_document_lg,it,public/models) testData: org.apache.spark.sql.DataFrame = [id: int, text: string] annotation: org.apache.spark.sql.DataFrame = [id: int, text: string ... 8 more fields] ++--+--+--+--+--+--+--+ | id| text| document| token| sentence| embeddings| ner| entities| ++--+--+--+--+--+--+--+ | 1|La FIFA ha deciso...|[[document, 0, 56...|[[token, 0, 1, La...|[[document, 0, 56...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 9, La...| | 2|Reims, 13 giugno ...|[[document, 0, 12...|[[token, 0, 4, Re...|[[document, 0, 12...|[[word_embeddings...|[[named_entity, 0...|[[chunk, 0, 4, Re...| ++--+--+--+--+--+--+--+ */ annotation.select(&quot;entities.result&quot;).show(false) /* +-+ |result | +-+ |[La FIFA, Zidane, Materazzi]| |[Reims, Domani, Mondiali] | +-+ */ Spanish Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.4.0 es   Download Explain Document Medium explain_document_md 2.4.0 es   Download Explain Document Large explain_document_lg 2.4.0 es   Download Entity Recognizer Small entity_recognizer_sm 2.4.0 es   Download Entity Recognizer Medium entity_recognizer_md 2.4.0 es   Download Entity Recognizer Large entity_recognizer_lg 2.4.0 es   Download Feature Description Lemma Trained by Lemmatizer annotator on lemmatization-lists by Michal Měchura POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Size Model size indicator, sm, md, and lg. The small pipelines use glove_100d, the medium pipelines use glove_6B_300, and large pipelines use glove_840B_300 WordEmbeddings Russian Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.4.4 ru   Download Explain Document Medium explain_document_md 2.4.4 ru   Download Explain Document Large explain_document_lg 2.4.4 ru   Download Entity Recognizer Small entity_recognizer_sm 2.4.4 ru   Download Entity Recognizer Medium entity_recognizer_md 2.4.4 ru   Download Entity Recognizer Large entity_recognizer_lg 2.4.4 ru   Download Feature Description Lemma Trained by Lemmatizer annotator on the Universal Dependencies POS Trained by PerceptronApproach annotator on the Universal Dependencies NER Trained by NerDLApproach annotator with Char CNNs - BiLSTM - CRF and GloVe Embeddings on the WikiNER corpus and supports the identification of PER, LOC, ORG and MISC entities Dutch Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 nl   Download Explain Document Medium explain_document_md 2.5.0 nl   Download Explain Document Large explain_document_lg 2.5.0 nl   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 nl   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 nl   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 nl   Download Norwegian Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 no   Download Explain Document Medium explain_document_md 2.5.0 no   Download Explain Document Large explain_document_lg 2.5.0 no   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 no   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 no   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 no   Download Polish Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 pl   Download Explain Document Medium explain_document_md 2.5.0 pl   Download Explain Document Large explain_document_lg 2.5.0 pl   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 pl   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 pl   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 pl   Download Portuguese Pipeline Name Build lang Description Offline Explain Document Small explain_document_sm 2.5.0 pt   Download Explain Document Medium explain_document_md 2.5.0 pt   Download Explain Document Large explain_document_lg 2.5.0 pt   Download Entity Recognizer Small entity_recognizer_sm 2.5.0 pt   Download Entity Recognizer Medium entity_recognizer_md 2.5.0 pt   Download Entity Recognizer Large entity_recognizer_lg 2.5.0 pt   Download Multi-language Pipeline Name Build lang Description Offline LanguageDetectorDL detect_language_7 2.5.2 xx   Download LanguageDetectorDL detect_language_20 2.5.2 xx   Download The model with 7 languages: Czech, German, English, Spanish, French, Italy, and Slovak The model with 20 languages: Bulgarian, Czech, German, Greek, English, Spanish, Finnish, French, Croatian, Hungarian, Italy, Norwegian, Polish, Portuguese, Romanian, Russian, Slovak, Swedish, Turkish, and Ukrainian How to use Online To use Spark NLP pretrained pipelines, you can call PretrainedPipeline with pipeline’s name and its language (default is en): pipeline = PretrainedPipeline(&#39;explain_document_dl&#39;, lang=&#39;en&#39;) Same in Scala val pipeline = PretrainedPipeline(&quot;explain_document_dl&quot;, lang=&quot;en&quot;) Offline If you have any trouble using online pipelines or models in your environment (maybe it’s air-gapped), you can directly download them for offline use. After downloading offline models/pipelines and extracting them, here is how you can use them iside your code (the path could be a shared storage like HDFS in a cluster): val advancedPipeline = PipelineModel.load(&quot;/tmp/explain_document_dl_en_2.0.2_2.4_1556530585689/&quot;) // To use the loaded Pipeline for prediction advancedPipeline.transform(predictionDF)",
    "url": "/docs/en/pipelines",
    "relUrl": "/docs/en/pipelines"
  },
  "44": {
    "id": "44",
    "title": "Public Health - Biomedical NLP Demos & Notebooks",
    "content": "",
    "url": "/public_health",
    "relUrl": "/public_health"
  },
  "45": {
    "id": "45",
    "title": "Question Answering - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/question_answering",
    "relUrl": "/question_answering"
  },
  "46": {
    "id": "46",
    "title": "Spark NLP - Quick Start",
    "content": "Requirements &amp; Setup Spark NLP is built on top of Apache Spark 3.x. For using Spark NLP you need: Java 8 and 11 Apache Spark 3.3.x, 3.2.x, 3.1.x, 3.0.x It is recommended to have basic knowledge of the framework and a working environment before using Spark NLP. Please refer to Spark documentation to get started with Spark. Install Spark NLP in Python Scala and Java Databricks EMR Join our Slack channel Join our channel, to ask for help and share your feedback. Developers and users can help each other getting started here. Spark NLP Slack Spark NLP in Action Make sure to check out our demos built by Streamlit to showcase Spark NLP in action: Spark NLP Demo Spark NLP Examples If you prefer learning by example, check this repository: Spark NLP Examples It is full of fresh examples and even a docker container if you want to skip installation. Below, you can follow into a more theoretical and thorough quick start guide. Where to go next If you need more detailed information about how to install Spark NLP you can check the Installation page Detailed information about Spark NLP concepts, annotators and more may be found HERE",
    "url": "/docs/en/quickstart",
    "relUrl": "/docs/en/quickstart"
  },
  "47": {
    "id": "47",
    "title": "Recognize Entities - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/recognize_entitie",
    "relUrl": "/recognize_entitie"
  },
  "48": {
    "id": "48",
    "title": "Release Notes",
    "content": "0.7.1 Fields Details Name NLP Server Version 0.7.1 Type Patch Release Date 2022-06-17 Overview We are excited to release NLP Server v0.7.1! We are committed to continuously improve the experience for our users and make our product reliable and easy to use. This release focuses on solving a few bugs and improving the stability of the NLP Server. Key Information For smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications. NLP Server is available on both AWS and Azure marketplaces. Bug Fixes Issue when running NER ONTO spell. Issue when running dep spell. Since the spell was broken it is temporarily blacklisted. Document normalizer included the HTML, XML tags to the output even after normalization. Issue when running language translation spells &lt;from_lang&gt;.translate_to.&lt;to_lang&gt;. Upon cancelation of custom model uploading job exception was seen in the logs. Some few UI related issues and abnormalities during operation. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes"
  },
  "49": {
    "id": "49",
    "title": "Spark NLP - release notes",
    "content": "For all official releases please visit GitHub release notes",
    "url": "/docs/en/release_notes",
    "relUrl": "/docs/en/release_notes"
  },
  "50": {
    "id": "50",
    "title": "NLP Server release notes 0.4.0",
    "content": "0.4.0 Highlights This version of NLP Server offers support for licensed models and annotators. Users can now upload a Spark NLP for Healthcare license file and get access to a wide range of additional annotators and transformers. A valid license key also gives access to more than 400 state-of-the-art healthcare models. Those can be used via easy to learn NLU spells or via API calls. NLP Server now supports better handling of large amounts of data to quickly analyze via UI by offering support for uploading CSV files. Support for floating licenses. Users can now take advantage of the floating license flexibility and use those inside of the NLP Server. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_4_0",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_4_0"
  },
  "51": {
    "id": "51",
    "title": "NLP Server release notes 0.5.0",
    "content": "0.5.0 Highlights Support for easy license import from my.johnsnowlabs.com. Visualize annotation results with Spark NLP Display. Examples of results obtained using popular spells on sample texts have been added to the UI. Performance improvement when previewing the annotations. Support for 22 new models for 23 languages including various African and Indian languages as well as Medical Spanish models powered by NLU 3.4.1 Various bug fixes Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_5_0",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_5_0"
  },
  "52": {
    "id": "52",
    "title": "NLP Server release notes 0.6.0",
    "content": "0.6.0 Fields Details Name NLP Server Version 0.6.0 Type Minor Release Date 2022-04-06 Overview We are excited to release NLP Server v0.6.0! This new release comes with exciting new features and improvements that extend and enhance the capabilities of the NLP Server. This release comes with the ability to share the models with the Annotation Lab. This will enable easy access to custom models uploaded to or trained with the Annotation Lab or to pre-trained models downloaded to Annotation Lab from the NLP Models Hub. As such the NLP Server becomes an easy and quick tool for testing our trained models locally on your own infrastructure with zero data sharing. Another important feature we have introduced is the support for Spark OCR spells. Now we can upload images, PDFs, or other documents to the NLP Server and run OCR spells on top of it. The results of the processed documents are also available for export. The release also includes a few improvements to the existing features and some bug fixes. Key Information For a smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications NLP Server is now available on Azure Marketplace as well as on AWS marketplace. Major Features and Improvements Support for custom models trained with the Annotation Lab Models trained with the Annotation Lab are now available as “custom” spells in the NLP Server. Similarly, models manually uploaded to the Annotation Lab, or downloaded from the NLP Models Hub are also made available for use in the NLP Server. This is only supported in a docker setup at present when both tools are deployed in the same machine. Support for Spark OCR spells OCR spells are now supported by NLP Server in the presence of a valid OCR license. Users can upload an image, PDF, or other supported document format and run the OCR spells on it. The processed results are also available for download as a text document. It is also possible to upload multiple files at once for OCR operation. These files can be images, PDFs, word documents, or a zipped file. Other Improvements Now users can chain multiple spells together to analyze the input data. The order of operation on the input data will be in the sequence of the spell chain from left to right. NLP Server now supports more than 5000+ models in 250+ languages powered by NLU. Bug Fixes Not found error seen when running predictions using certain spells. The prediction job runs in an infinite loop when using certain spells. For input data having new line characters JSON exception was seen when processing the output from NLU. Incorrect license information was seen in the license popup. Spell field cleared abruptly when typing the spells. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_6_0",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_6_0"
  },
  "53": {
    "id": "53",
    "title": "NLP Server release notes 0.6.1",
    "content": "0.6.1 Fields Details Name NLP Server Version 0.6.1 Type Patch Release Date 2022-05-06 Overview We are excited to release NLP Server v0.6.1! We are continually committed towards improving the experience for our users and making our product reliable and easy to use. This release focuses on improving the stability of the NLP Server and cleaning up some annoying bugs. To enhance the user experience, the product now provides interactive and informative responses to the users. The improvements and bug fixes are mentioned in their respective sections below. Key Information For smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications. NLP Server is available on both AWS and Azure marketplace. Improvements Support for new models for Lemmatizers, Parts of Speech Taggers, and Word2Vec Embeddings for over 66 languages, with 20 languages being covered for the first time by NLP Server, including ancient and exotic languages like Ancient Greek, Old Russian, Old French and much more. Bug Fixes The prediction job runs in an infinite loop when using certain spells. Now after 3 retries it aborts the process and informs users appropriately. Issue when running lang spell for language classification. The prediction job runs in an infinite loop when incorrect data format is selected for a given input data. The API request for processing spell didn’t work when format parameter was not provided. Now it uses a default value in such case. Users were unable to login to their MYJSL account from NLP Server. Proper response when there is issue in internet connectivity when running spell. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_6_1",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_6_1"
  },
  "54": {
    "id": "54",
    "title": "NLP Server release notes 0.7.0",
    "content": "0.7.0 Fields Details Name NLP Server Version 0.7.0 Type Minor Release Date 2022-06-07 Overview We are excited to release NLP Server v0.7.0! This new release comes with an exciting new feature of table extraction from various file formats. Table extraction feature enables extracting tabular content from the document. This extracted content is available as JSON and hence can again be processed with different spells for further predictions. The various supported files formats are documents (pdf, doc, docx), slides (ppt, pptx), and zipped content containing the mentioned formats. The improvements are mentioned in their respective sections below. Key Information For smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications. NLP Server is available on both AWS and Azure marketplace. Major Features and Improvements Support for Table extraction NLP Server now supports extracting tabular content from various file types. The currently supported file types are documents (pdf, doc, docx), slides (ppt, pptx), and zipped content containing any of the mentioned formats. These extracted contents are available as JSON output from both UI and API that can easily be converted to suitable Data Frames (e.g., pandas DF) for further processing. The output of the table extraction process can also be viewed in the NLP Server UI as a flat table. Currently, if multiple tables are extracted from the document, then only one of the tables selected randomly will be shown as a preview in the UI. However, upon downloading all the extracted tables are exported in separate JSON dumps combined in a single zipped file. For this version, the table extraction on PDF files is successful only if the PDF contains necessary metadata about the table content. Other Improvements Support for over 600 new models, and over 75 new languages including ancient, dead, and extinct languages. Transformer-based embeddings and token classifiers are powered by state-of-the-art CamemBertEmbeddings and DeBertaForTokenClassification based architectures. Added Portuguese De-identification models, NER models for Gene detection, and RxNorm Sentence resolution model for mapping and extracting pharmaceutical actions as well as treatments. JSON payload is now supported in the request body when using create result API. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_7_0",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_7_0"
  },
  "55": {
    "id": "55",
    "title": "NLP Server release notes 0.7.1",
    "content": "0.7.1 Fields Details Name NLP Server Version 0.7.1 Type Patch Release Date 2022-06-17 Overview We are excited to release NLP Server v0.7.1! We are committed to continuously improve the experience for our users and make our product reliable and easy to use. This release focuses on solving a few bugs and improving the stability of the NLP Server. Key Information For smooth and optimal performance, it is recommended to use an instance with 8 core CPU, and 32GB RAM specifications. NLP Server is available on both AWS and Azure marketplaces. Bug Fixes Issue when running NER ONTO spell. Issue when running dep spell. Since the spell was broken it is temporarily blacklisted. Document normalizer included the HTML, XML tags to the output even after normalization. Issue when running language translation spells &lt;from_lang&gt;.translate_to.&lt;to_lang&gt;. Upon cancelation of custom model uploading job exception was seen in the logs. Some few UI related issues and abnormalities during operation. Versions Version Version Version 0.7.1 0.7.0 0.6.1 0.6.0 0.5.0 0.4.0",
    "url": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_7_1",
    "relUrl": "/docs/en/nlp_server/nlp_server_versions/release_notes_0_7_1"
  },
  "56": {
    "id": "56",
    "title": "Resolve Entities to Terminology Codes - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/resolve_entities_codes",
    "relUrl": "/resolve_entities_codes"
  },
  "57": {
    "id": "57",
    "title": "Spark NLP - Serving with MLFlow on Databricks",
    "content": "This is the first article of the “Serving Spark NLP via API” series, showcasing how to serve Spark NLP using Databricks Jobs and MLFlow Serve APIs. You can find two more approaches (first, using FastAPI and second, using SynapseML) in the Spark NLP for Healthcare documentation page. Background Spark NLP is a Natural Language Understanding Library built on top of Apache Spark, leveranging Spark MLLib pipelines, that allows you to run NLP models at scale, including SOTA Transformers. Therefore, it’s the only production-ready NLP platform that allows you to go from a simple PoC on 1 driver node, to scale to multiple nodes in a cluster, to process big amounts of data, in a matter of minutes. Before starting, if you want to know more about all the advantages of using Spark NLP (as the ability to work at scale on air-gapped environments, for instance) we recommend you to take a look at the following resources: John Snow Labs webpage; The official technical documentation of Spark NLP; Spark NLP channel on Medium; Motivation Spark NLP is server-agnostic, what means it does not come with an integrated API server, but offers a lot of options to serve NLP models using Rest APIs. There is a wide range of possibilities to add a web server and serve Spark NLP pipelines using RestAPI, and in this series of articles we are only describing some of them. Let’s have an overview of how to use Databricks Jobs API and MLFlow Serve as an example for that purpose. Databricks Jobs and MLFlow Serve APIs About Databricks Databricks is an enterprise software company founded by the creators of Apache Spark. The company has also created MLflow, the Serialization and Experiment tracking library you can use (inside or outside databricks), as described in the section “Experiment Tracking”. Databricks develops a web-based platform for working with Spark, that provides automated cluster management and IPython-style notebooks. Their infrastructured is provided for training and production purposes, and is integrated in cloud platforms as Azure and AWS. Spark NLP is a proud partner of Databricks and we offer a seamless integration with them — see Install on Databricks. All Spark NLP capabilities run in Databricks, including MLFlow serialization and Experiment tracking, what can be used for serving Spark NLP for production purposes. About MLFlow MLFlow is a serialization and Experiment Tracking platform, which also natively suports Spark NLP. We have a documentation entry about MLFlow in the “Experiment Tracking” section. It’s highly recommended that you take a look before moving forward in this document, since we will use some of the concepts explained there. We will use MLFlow serialization to serve our Spark NLP models. Strengths Easily configurable and scalable clusters in Databricks Seamless integration of Spark NLP and Databricks for automatically creating Spark NLP clusters (check Install on Databricks URL) Integration with MLFlow, experiment tracking, etc. Configure your training and serving environments separately. Use your serving environment for inference and scale it as you need. Weaknesses This approach does not allow you to customize your endpoints, it uses Databricks JOBS API ones Requires some time and expertise in Databricks to configure everything properly Creating a cluster in Databricks As mentioned before, Spark NLP offers a seamless integration with Databricks. To create a cluster, please follow the instructions in Install on Databricks. That cluster can be then replicated (cloned) for production purposes later on. Configuring Databricks for serving Spark NLP on MLFlow In Databricks Runtime Version, select any Standard runtime, not ML ones… These add their version of MLFlow, and some incompatibilities may arise. For this example, we have used 8.3 (includes Apache Spark 3.1.1, Scala 2.12) The cluster instantiated is prepared to use Spark NLP, but to make it production-ready using MLFlow, we need to add the MLFlow jar, in addition to the Spark NLP jar, as shown in the “Experiment Tracking” section. In that case, we did it adding both jars… (&quot;spark.jars.packages&quot;:&quot; com.johnsnowlabs.nlp:spark-nlp_2.12:[YOUR_SPARKNLP_VERSION],org.mlflow:mlflow-spark:1.21.0&quot;) …into the SparkSession. However, in Databricks, you don’t instantiate programmatically a session, but you configure it in the Compute screen, selecting your Spark NLP cluster, and then going to Configuration -&gt; Advanced Options -&gt; Spark -&gt; Spark Config, as shown in the following image: In addition to Spark Config, we need to add the Spark NLP and MLFlow libraries to the Cluster. You can do that by going to Libraries inside your cluster. Make sure you have spark-nlp and mlflow. If not, you can install them either using PyPI or Maven artifacts. In the image below you can see the PyPI alternative: TIP: You can also use the Libraries section to add the jars (using Maven Coordinates) instead of setting them in the Spark Config, as showed before. Creating a notebook You are ready to create a notebook in Databricks and attach it to the recently created cluster. To do that, go to Create --&gt; Notebook, and select the cluster you want in the dropdown above your notebook. Make sure you have selected the cluster with the right Spark NLP + MLFlow configuration. To check everything is ok, run the following lines: To check the session is running: spark To check jars are in the session: spark.sparkContext.getConf().get(&#39;spark.jars.packages&#39;) You should see the following output from the last line (versions may differ depending on which ones you used to configure your cluster) Out[2]: &#39;com.johnsnowlabs.nlp:spark-nlp_2.12:[YOUR_SPARKNLP_VERSION],org.mlflow:mlflow-spark:1.21.0&#39; Logging the experiment in Databricks using MLFlow As explained in the “Experiment Tracking” section, MLFlow can log Spark MLLib / NLP Pipelines as experiments, to carry out runs on them, track versions, etc. MLFlow is natively integrated in Databricks, so we can leverage the mlflow.spark.log_model() function of the Spark flavour of MLFlow, to start tracking our Spark NLP pipelines. Let’s first import our libraries: import mlflow import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import pandas as pd from sparknlp.training import CoNLL import pyspark from pyspark.sql import SparkSession Then, create a Lemmatization pipeline: documentAssembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer() .setInputCols([&quot;document&quot;]) .setOutputCol(&quot;token&quot;) lemmatizer = LemmatizerModel.pretrained() .setInputCols([&quot;token&quot;]) .setOutputCol(&quot;prediction&quot;) # It&#39;s mandatory to call it prediction pipeline = Pipeline(stages=[ documentAssembler, tokenizer, lemmatizer ]) p_model = pipeline.fit( spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) ) IMPORTANT: Last output column of the last component in the pipeline should be called prediction. Finally, let’s log the experiment. In the Experiment Tracking section, we used the pip_requirements parameter in the log_model() function to set the required libraries: But we mentioned using conda is also available. Let’s use conda in this example: conda_env = { &#39;channels&#39;: [&#39;conda-forge&#39;], &#39;dependencies&#39;: [ &#39;python=3.8.8&#39;, { &quot;pip&quot;: [ &#39;pyspark==3.1.1&#39;, &#39;mlflow==1.21.0&#39;, &#39;spark-nlp==[YOUR_SPARKNLP_VERSION]&#39; ] } ], &#39;name&#39;: &#39;mlflow-env&#39; } With this conda environment, we are ready to log our pipeline: mlflow.spark.log_model(p_model, &quot;lemmatizer&quot;, conda_env=conda_env) You should see an output similar to this one: (6) Spark Jobs (1) MLflow run *Logged 1 run to an experiment in MLflow. Learn more* Experiment UI On the top right corner of your notebook, you will see the Experiment widget, and inside, as shown in the image below. You can also access Experiments UI if you switch your environment from “Data Science &amp; Engineering” to “Machine Learning”, on the left panel… Once in the experiment UI, you will see the following screen, where your experiments are tracked. If you click on the Start Time cell of your experiment, you will reach the registered MLFlow run. On the left panel you will see the MLFlow model and some other artifacts, as the conda.yml and pip_requirements.txt that manage the dependencies of your models. On the right panel, you will see two snippets, about how to call to the model for inference internally from Databricks. Snippet for calling with a Pandas Dataframe: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; Load model as a Spark UDF. loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model) Predict on a Spark DataFrame. columns = list(df.columns) df.withColumn(&#39;predictions&#39;, loaded_model(*columns)).collect() Snippet for calling with a Spark Dataframe. We won’t include it in this documentation because that snippet does not include SPark NLP specificities. To make it work, the correct snippet should be: import mlflow logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model) ### Predict on a Spark DataFrame. res_spark = loaded_model.predict(df_1_spark.rdd) IMPORTANT: You will only get the last column (prediction) results, which is a list of Rows of Annotation Types. To convert the result list into a Spark Dataframe, use the following schema: import pyspark.sql.types as T import pyspark.sql.functions as f annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) And then, get the results (for example, in res_spark) and apply the schema: spark_res = spark.createDataFrame(res_pandas[0], schema=annotationType) Calling the experiment for production purposes using MLFlow Rest API Instead of choosing a Batch Inference, you can select REST API. This will lead you to another screen, when the model will be loaded for production purposes in an independent cluster. Once deployed, you will be able to: Check the endpoint URL to consume the model externally; Test the endpoint writing a json (in our example, ‘text’ is our first input col of the pipeline, so it shoud look similar to: {&quot;text&quot;: &quot;This is a test of how the lemmatizer works&quot;} You can see the response in the same screen. Check what is the Python code or cURL command to do that very same thing programatically. By just using that Python code, you can already consume it for production purposes from any external web app. IMPORTANT: As per 17/02/2022, there is an issue being studied by Databricks team, regarding the creation on the fly of job clusters to serve MLFlow models that require configuring the Spark Session with specific jars. This will be fixed in later versions of Databricks. In the meantime, the way to go is using Databricks Jobs API. Calling the experiment for production purposes using Databricks Asynchronous Jobs API Creating the notebook for the inference job And last, but not least, another approach to consume models for production purposes. the Jobs API. Databricks has its own API for managing jobs, that allows you to instantiate any notebook or script as a job, run it, stop it, and manage all the life cycle. And you can configure the cluster where this job will run before hand, what prevents having the issue described in point 3. To do that: Create a new production cluster, as described before, cloning you training environment but adapting it to your needs for production purposes. Make sure the Spark Config is right, as described at the beginning of this documentation. Create a new notebook. Always check that the jars are in the session: spark.sparkContext.getConf().get(&#39;spark.jars.packages&#39;) Out[2]: &#39;com.johnsnowlabs.nlp:spark-nlp_2.12:[YOUR_SPARKNLP_VERSION],org.mlflow:mlflow-spark:1.21.0&#39; Add the Spark NLP imports. import mlflow import sparknlp from sparknlp.base import * from sparknlp.annotator import * from pyspark.ml import Pipeline import pandas as pd from sparknlp.training import CoNLL import pyspark from pyspark.sql import SparkSession import pyspark.sql.types as T import pyspark.sql.functions as f import json Let’s define that an input param called text will be sent in the request. Let’s get the text from that parameter using dbutils. input = &quot;&quot; try: input = dbutils.widgets.get(&quot;text&quot;) print(&#39;&quot;text&quot; input found: &#39; + input) except: print(&#39;Unable to run: dbutils.widgets.get(&quot;text&quot;). Setting it to NOT_SET&#39;) input = &quot;NOT_SET&quot; Right now, the input text will be in input var. You can trigger an exception or set the input to some default value if the parameter does not come in the request. Let’s create a Spark Dataframe with the input df = spark.createDataFrame([[input]]).toDF(&#39;text&#39;) And now, we just need to use the snippet for Spark Dataframe to consume MLFlow models, described above: import mlflow import pyspark.sql.types as T import pyspark.sql.functions as f logged_model = &#39;runs:/a8cf070528564792bbf66d82211db0a0/lemmatizer&#39; loaded_model = mlflow.pyfunc.load_model(model_uri=logged_model) Predict on a Spark DataFrame. res_spark = loaded_model.predict(df_1_spark.rdd) annotationType = T.StructType([ T.StructField(&#39;annotatorType&#39;, T.StringType(), False), T.StructField(&#39;begin&#39;, T.IntegerType(), False), T.StructField(&#39;end&#39;, T.IntegerType(), False), T.StructField(&#39;result&#39;, T.StringType(), False), T.StructField(&#39;metadata&#39;, T.MapType(T.StringType(), T.StringType()), False), T.StructField(&#39;embeddings&#39;, T.ArrayType(T.FloatType()), False) ]) spark_res = spark.createDataFrame(res_spark[0], schema=annotationType) Let’s transform our lemmatized tokens from the Dataframe into a list of strings: lemmas = spark_res.select(&quot;result&quot;).collect() txt_results = [x[&#39;result&#39;] for x in lemmas] And finally, let’s use again dbutils to tell Databricks to spin off the run and return an exit parameter: the list of token strings. dbutils.notebook.exit(json.dumps({ &quot;status&quot;: &quot;OK&quot;, &quot;results&quot;: txt_results })) Configuring the job Last, but not least. We need to precreate the job, so that we run it from the API. We could do that using the API as well, but we will show you how to do it using the UI. On the left panel, go to Jobs and then Create Job. In the jobs screen, you will see you job created. It’s not running, it’s prepared to be called on demand, programatically or in the interface, with a text input param. Let’s see how to do that: Running the job In the jobs screen, if you click on the job, you will enter the Job screen, and be able to set your text input parameter and run the job manually. You can use this for testing purposes, but the interesting part is calling it externally, using the Databricks Jobs API. Using the Databricks Jobs API, from for example, Postman. POST HTTP request URL: https://[your_databricks_instance]/api/2.1/jobs/run-now Authorization: [use Bearer Token. You can get it from Databricks, Settings, User Settings, Generate New Token.] Body: { &quot;job_id&quot;: [job_id, check it in the Jobs screen], &quot;notebook_params&quot;: {&quot;text&quot;: &quot;This is an example of how well the lemmatizer works&quot;} } As it’s an asynchronous call, it will return the number a number of run, but no results. You will need to query for results using the number of the run and the following url https://[your_databricks_instance]/2.1/jobs/runs/get-output You will get a big json, but the most relevant info, the output, will be up to the end: Results (list of lemmatized words) {&quot;notebook_output&quot;: { &quot;status&quot;: &quot;OK&quot;, &quot;results&quot;: [&quot;This&quot;, &quot;is&quot;, &quot;a&quot;, &quot;example&quot;, &quot;of&quot;, &quot;how&quot;, &quot;lemmatizer&quot;, &quot;work&quot;] }} The notebook will be prepared in the job, but idle, until you call it programatically, what will instantiate a run. Check the Jobs API for more information about what you can do with it and how to adapt it to your solutions for production purposes. Do you want to know more? Check how to productionize Spark NLP in our official documentation here Visit John Snow Labs and Spark NLP Technical Documentation websites Follow us on Medium: Spark NLP and Veysel Kocaman Write to support@johnsnowlabs.com for any additional request you may have",
    "url": "/docs/en/serving_spark_nlp_via_api_databricks_mlflow",
    "relUrl": "/docs/en/serving_spark_nlp_via_api_databricks_mlflow"
  },
  "58": {
    "id": "58",
    "title": "Social Determinant - Clinical NLP Demos & Notebooks",
    "content": "",
    "url": "/social_determinant",
    "relUrl": "/social_determinant"
  },
  "59": {
    "id": "59",
    "title": "Spark NLP",
    "content": "Requirements &amp; Setup Spark NLP is built on top of Apache Spark 3.x. For using Spark NLP you need: Java 8 and 11 Apache Spark 3.3.x, 3.2.x, 3.1.x, 3.0.x GPU (optional): Spark NLP 5.0.2 is built with TensorFlow 2.7.1 and the following NVIDIA® software are only required for GPU support: NVIDIA® GPU drivers version 450.80.02 or higher CUDA® Toolkit 11.2 cuDNN SDK 8.1.0 It is recommended to have basic knowledge of the framework and a working environment before using Spark NLP. Please refer to Spark documentation to get started with Spark. Install Spark NLP in Python Scala and Java Databricks EMR Join our Slack channel Join our channel, to ask for help and share your feedback. Developers and users can help each other getting started here. Spark NLP Slack Spark NLP in Action Make sure to check out our demos built by Streamlit to showcase Spark NLP in action: Spark NLP Demo Spark NLP Examples If you prefer learning by example, check this repository: Spark NLP Examples It is full of fresh examples and even a docker container if you want to skip installation. Below, you can follow into a more theoretical and thorough quick start guide. Where to go next If you need more detailed information about how to install Spark NLP you can check the Installation page Detailed information about Spark NLP concepts, annotators and more may be found HERE",
    "url": "/docs/en/spark-nlp",
    "relUrl": "/docs/en/spark-nlp"
  },
  "60": {
    "id": "60",
    "title": "Speech and Vision Recognition - Spark NLP Demos & Notebooks",
    "content": "",
    "url": "/speech_vision_recognition",
    "relUrl": "/speech_vision_recognition"
  },
  "61": {
    "id": "61",
    "title": "Summarize & Paraphrase - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/summarize_paraphrase",
    "relUrl": "/summarize_paraphrase"
  },
  "62": {
    "id": "62",
    "title": "Text Summarization - Finance NLP Demos & Notebooks",
    "content": "",
    "url": "/text_summarization",
    "relUrl": "/text_summarization"
  },
  "63": {
    "id": "63",
    "title": "Third Party Projects",
    "content": "There are third party projects that can integrate with Spark NLP. These packages need to be installed separately to be used. If you’d like to integrate your application with Spark NLP, please send us a message! Logging Comet Comet is a meta machine learning platform designed to help AI practitioners and teams build reliable machine learning models for real-world applications by streamlining the machine learning model lifecycle. By leveraging Comet, users can track, compare, explain and reproduce their machine learning experiments. Comet can easily integrated into the Spark NLP workflow with the a dedicated logging class CometLogger to log training and evaluation metrics, pipeline parameters and NER visualization made with sparknlp-display. For more information see the User Guide and for more examples see the Spark NLP Examples. Python API: CometLogger Show Example # Metrics while training an annotator can be logged with for example: import sparknlp from sparknlp.base import * from sparknlp.annotator import * from sparknlp.logging.comet import CometLogger spark = sparknlp.start() OUTPUT_LOG_PATH = &quot;./run&quot; logger = CometLogger() document = DocumentAssembler().setInputCol(&quot;text&quot;).setOutputCol(&quot;document&quot;) embds = ( UniversalSentenceEncoder.pretrained() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) ) multiClassifier = ( MultiClassifierDLApproach() .setInputCols(&quot;sentence_embeddings&quot;) .setOutputCol(&quot;category&quot;) .setLabelColumn(&quot;labels&quot;) .setBatchSize(128) .setLr(1e-3) .setThreshold(0.5) .setShufflePerEpoch(False) .setEnableOutputLogs(True) .setOutputLogsPath(OUTPUT_LOG_PATH) .setMaxEpochs(1) ) logger.monitor(logdir=OUTPUT_LOG_PATH, model=multiClassifier) trainDataset = spark.createDataFrame( [(&quot;Nice.&quot;, [&quot;positive&quot;]), (&quot;That&#39;s bad.&quot;, [&quot;negative&quot;])], schema=[&quot;text&quot;, &quot;labels&quot;], ) pipeline = Pipeline(stages=[document, embds, multiClassifier]) pipeline.fit(trainDataset) logger.end() # If you are using a jupyter notebook, it is possible to display the live web # interface with logger.experiment.display(tab=&#39;charts&#39;) MLflow Spark NLP uses Spark MLlib Pipelines, what are natively supported by MLFlow. MLFlow is, as stated in their official webpage, an open source platform for the machine learning lifecycle, that includes: Mlflow Tracking: Record and query experiments: code, data, config, and results MLflow Projects: Package data science code in a format to reproduce runs on any platform MLflow Models: Deploy machine learning models in diverse serving environments Model Registry: Store, annotate, discover, and manage models in a central repository For more information, please see the complete guide at Experiment Tracking.",
    "url": "/docs/en/third-party-projects",
    "relUrl": "/docs/en/third-party-projects"
  },
  "64": {
    "id": "64",
    "title": "Spark NLP -Training",
    "content": "Training Datasets These are classes to load common datasets to train annotators for tasks such as part-of-speech tagging, named entity recognition, spell checking and more. {% include_relative training_entries/pos.md %} {% include_relative training_entries/conll.md %} {% include_relative training_entries/conllu.md %} {% include_relative training_entries/pubtator.md %} Spell Checkers Dataset (Corpus) In order to train a Norvig or Symmetric Spell Checkers, we need to get corpus data as a spark dataframe. We can read a plain text file and transforms it to a spark dataset. Example: {% include programmingLanguageSelectScalaPython.html %} train_corpus = spark.read .text(&quot;./sherlockholmes.txt&quot;) .withColumnRenamed(&quot;value&quot;, &quot;text&quot;) val trainCorpus = spark.read .text(&quot;./sherlockholmes.txt&quot;) .select(trainCorpus.col(&quot;value&quot;).as(&quot;text&quot;)) Text Processing These are annotators that can be trained to process text for tasks such as dependency parsing, lemmatisation, part-of-speech tagging, sentence detection and word segmentation. {% include_relative training_entries/DependencyParserApproach.md %} {% include_relative training_entries/Lemmatizer.md %} {% include_relative training_entries/PerceptronApproach.md %} {% include_relative training_entries/SentenceDetectorDLApproach.md %} {% include_relative training_entries/TypedDependencyParser.md %} {% include_relative training_entries/WordSegmenterApproach.md %} Spell Checkers These are annotators that can be trained to correct text. {% include_relative training_entries/ContextSpellCheckerApproach.md %} {% include_relative training_entries/NorvigSweeting.md %} {% include_relative training_entries/SymmetricDelete.md %} Token Classification These are annotators that can be trained to recognize named entities in text. {% include_relative training_entries/NerCrfApproach.md %} {% include_relative training_entries/NerDLApproach.md %} Text Classification These are annotators that can be trained to classify text into different classes, such as sentiment. {% include_relative training_entries/ClassifierDLApproach.md %} {% include_relative training_entries/MultiClassifierDLApproach.md %} {% include_relative training_entries/SentimentDLApproach.md %} {% include_relative training_entries/ViveknSentimentApproach.md %} Text Representation These are annotators that can be trained to turn text into a numerical representation. {% include_relative training_entries/Doc2VecApproach.md %} {% include_relative training_entries/Word2VecApproach.md %} External Trainable Models These are annotators that are trained in an external library, which are then loaded into Spark NLP. {% include_relative training_entries/AlbertForTokenClassification.md %} {% include_relative training_entries/BertForSequenceClassification.md %} {% include_relative training_entries/BertForTokenClassification.md %} {% include_relative training_entries/DistilBertForSequenceClassification.md %} {% include_relative training_entries/DistilBertForTokenClassification.md %} {% include_relative training_entries/RoBertaForTokenClassification.md %} {% include_relative training_entries/XlmRoBertaForTokenClassification.md %} TensorFlow Graphs NER DL uses Char CNNs - BiLSTM - CRF Neural Network architecture. Spark NLP defines this architecture through a Tensorflow graph, which requires the following parameters: Tags Embeddings Dimension Number of Chars Spark NLP infers these values from the training dataset used in NerDLApproach annotator and tries to load the graph embedded on spark-nlp package. Currently, Spark NLP has graphs for the most common combination of tags, embeddings, and number of chars values: Tags Embeddings Dimension 10 100 10 200 10 300 10 768 10 1024 25 300 All of these graphs use an LSTM of size 128 and number of chars 100 In case, your train dataset has a different number of tags, embeddings dimension, number of chars and LSTM size combinations shown in the table above, NerDLApproach will raise an IllegalArgumentException exception during runtime with the message below: Graph [parameter] should be [value]: Could not find a suitable tensorflow graph for embeddings dim: [value] tags: [value] nChars: [value]. Check https://sparknlp.org/docs/en/graph for instructions to generate the required graph. To overcome this exception message we have to follow these steps: Clone spark-nlp github repo Run python file create_models with number of tags, embeddings dimension and number of char values mentioned on your exception message error. cd spark-nlp/python/tensorflow export PYTHONPATH=lib/ner python create_models.py [number_of_tags] [embeddings_dimension] [number_of_chars] [output_path] This will generate a graph on the directory defined on `output_path argument. Retry training with NerDLApproach annotator but this time use the parameter setGraphFolder with the path of your graph. Note: Make sure that you have Python 3 and Tensorflow 1.15.0 installed on your system since create_models requires those versions to generate the graph successfully. Note: We also have a notebook in the same directory if you prefer Jupyter notebook to cerate your custom graph.",
    "url": "/docs/en/training",
    "relUrl": "/docs/en/training"
  },
  "65": {
    "id": "65",
    "title": "Spark NLP - Transformers",
    "content": "{% assign parent_path = “en/transformer_entries” %} {% for file in site.static_files %} {% if file.path contains parent_path %} {% assign file_name = file.path | remove: parent_path | remove: “/” | prepend: “transformer_entries/” %} {% include_relative {{ file_name }} %} {% endif %} {% endfor %} Import Transformers into Spark NLP Overview We have extended support for HuggingFace 🤗 and TF Hub exported models since 3.1.0 to equivalent Spark NLP 🚀 annotators. Starting this release, you can easily use the saved_model feature in HuggingFace within a few lines of codes and import any BERT, DistilBERT, CamemBERT, RoBERTa, DeBERTa, XLM-RoBERTa, Longformer, BertForTokenClassification, DistilBertForTokenClassification, AlbertForTokenClassification, RoBertaForTokenClassification, DeBertaForTokenClassification, XlmRoBertaForTokenClassification, XlnetForTokenClassification, LongformerForTokenClassification, CamemBertForTokenClassification, CamemBertForSequenceClassification, CamemBertForQuestionAnswering, BertForSequenceClassification, DistilBertForSequenceClassification, AlbertForSequenceClassification, RoBertaForSequenceClassification, DeBertaForSequenceClassification, XlmRoBertaForSequenceClassification, XlnetForSequenceClassification, LongformerForSequenceClassification, AlbertForQuestionAnswering, BertForQuestionAnswering, DeBertaForQuestionAnswering, DistilBertForQuestionAnswering, LongformerForQuestionAnswering, RoBertaForQuestionAnswering, XlmRoBertaForQuestionAnswering, TapasForQuestionAnswering, Vision Transformers (ViT), HubertForCTC, SwinForImageClassification, and ConvNextForImageClassification models to Spark NLP. We will work on the remaining annotators and extend this support to the rest with each release 😊 Compatibility Spark NLP: The equivalent annotator in Spark NLP TF Hub: Models from TF Hub HuggingFace: Models from HuggingFace ONNX: Models from HuggingFace in ONNX format Model Architecture: Which architecture is compatible with that annotator Flags: Fully supported ✅ Partially supported (requires workarounds) ✔️ Under development ❎ Not supported ❌ Spark NLP TF Hub HuggingFace ONNX Model Architecture   BertEmbeddings ✅ ✅ ✅ BERT - Small BERT - ELECTRA ❎ BertSentenceEmbeddings ✅ ✅ ❎ BERT - Small BERT - ELECTRA ❎ DistilBertEmbeddings   ✅ ✅ DistilBERT ❎ CamemBertEmbeddings   ✅ ✅ CamemBERT ❎ RoBertaEmbeddings   ✅ ✅ RoBERTa - DistilRoBERTa ❎ DeBertaEmbeddings   ✅ ✅ DeBERTa-v2 - DeBERTa-v3 ❎ XlmRoBertaEmbeddings   ✅ ✅ XLM-RoBERTa ❎ AlbertEmbeddings ✅ ✅ ✅ ALBERT ❎ XlnetEmbeddings   ✅ ❌ XLNet ❎ LongformerEmbeddings   ✅ ❌ Longformer   ElmoEmbeddings ❎   ❌     UniversalSentenceEncoder ❎   ❌     BertForTokenClassification   ✅ ❎ TFBertForTokenClassification   DistilBertForTokenClassification   ✅ ❎ TFDistilBertForTokenClassification   AlbertForTokenClassification   ✅ ❎ TFAlbertForTokenClassification   RoBertaForTokenClassification   ✅ ❎ TFRobertaForTokenClassification   DeBertaForTokenClassification   ✅ ❎ TFDebertaV2ForTokenClassification   XlmRoBertaForTokenClassification   ✅ ❎ TFXLMRobertaForTokenClassification   XlnetForTokenClassification   ✅ ❎ TFXLNetForTokenClassificationet   LongformerForTokenClassification   ✅ ❎ TFLongformerForTokenClassification   CamemBertForTokenClassification   ✅ ❎ TFCamemBertForTokenClassification   CamemBertForSequenceClassification   ✅ ❎ TFCamemBertForSequenceClassification   CamemBertForQuestionAnswering   ✅ ❎ TFCamembertForQuestionAnswering   BertForSequenceClassification   ✅ ❎ TFBertForSequenceClassification   DistilBertForSequenceClassification   ✅ ❎ TFDistilBertForSequenceClassification   AlbertForSequenceClassification   ✅ ❎ TFAlbertForSequenceClassification   RoBertaForSequenceClassification   ✅ ❎ TFRobertaForSequenceClassification   DeBertaForSequenceClassification   ✅ ❎ TFDebertaV2ForSequenceClassification   XlmRoBertaForSequenceClassification   ✅ ❎ TFXLMRobertaForSequenceClassification   XlnetForSequenceClassification   ✅ ❎ TFXLNetForSequenceClassification   LongformerForSequenceClassification   ✅ ❎ TFLongformerForSequenceClassification   AlbertForQuestionAnswering   ✅ ❎ TFAlbertForQuestionAnswering   BertForQuestionAnswering   ✅ ❎ TFBertForQuestionAnswering   DeBertaForQuestionAnswering   ✅ ❎ TFDebertaV2ForQuestionAnswering   DistilBertForQuestionAnswering   ✅ ❎ TFDistilBertForQuestionAnswering   LongformerForQuestionAnswering   ✅ ❎ TFLongformerForQuestionAnswering   RoBertaForQuestionAnswering   ✅ ❎ TFRobertaForQuestionAnswering   XlmRoBertaForQuestionAnswering   ✅ ❎ TFXLMRobertaForQuestionAnswering   TapasForQuestionAnswering   ❎ ❎ TFTapasForQuestionAnswering   ViTForImageClassification ❌ ✅ ❎ TFViTForImageClassification   Automatic Speech Recognition (Wav2Vec2ForCTC)   ❎ ❎ TFWav2Vec2ForCTC   SwinForImageClassification   ❎ ❎ TFSwinForImageClassification   HubertForCTC   ❎ ❎ TFHubertForCTC   ConvNextForImageClassification   ❎ ❎ TFConvNextForImageClassification   BertForZeroShotClassification   ✅ ❎ TFBertForSequenceClassification   DistilBertForZeroShotClassification   ✅ ❎ TFDistilBertForSequenceClassification   RoBertaForZeroShotClassification   ✅ ❎ TFRobertaForSequenceClassification   T5Transformer   ❌ ❎     MarianTransformer   ❌ ❎     OpenAI GPT2   ❌ ❎     Example Notebooks HuggingFace, Optimum, PyTorch, and ONNX Runtime to Spark NLP (ONNX) Spark NLP Notebooks Colab BertEmbeddings HuggingFace in Spark NLP - BERT BERT DistilBertEmbeddings HuggingFace in Spark NLP - DistilBERT DistilBERT RoBertaEmbeddings HuggingFace in Spark NLP - RoBERTa RoBERTa DeBertaEmbeddings HuggingFace in Spark NLP - DeBERTa DeBERTa HuggingFace to Spark NLP (TensorFlow) Spark NLP Notebooks Colab BertEmbeddings HuggingFace in Spark NLP - BERT BertSentenceEmbeddings HuggingFace in Spark NLP - BERT Sentence DistilBertEmbeddings HuggingFace in Spark NLP - DistilBERT CamemBertEmbeddings HuggingFace in Spark NLP - CamemBERT RoBertaEmbeddings HuggingFace in Spark NLP - RoBERTa DeBertaEmbeddings HuggingFace in Spark NLP - DeBERTa XlmRoBertaEmbeddings HuggingFace in Spark NLP - XLM-RoBERTa AlbertEmbeddings HuggingFace in Spark NLP - ALBERT XlnetEmbeddings HuggingFace in Spark NLP - XLNet LongformerEmbeddings HuggingFace in Spark NLP - Longformer BertForTokenClassification HuggingFace in Spark NLP - BertForTokenClassification DistilBertForTokenClassification HuggingFace in Spark NLP - DistilBertForTokenClassification AlbertForTokenClassification HuggingFace in Spark NLP - AlbertForTokenClassification RoBertaForTokenClassification HuggingFace in Spark NLP - RoBertaForTokenClassification XlmRoBertaForTokenClassification HuggingFace in Spark NLP - XlmRoBertaForTokenClassification CamemBertForTokenClassification HuggingFace in Spark NLP - CamemBertForTokenClassification CamemBertForSequenceClassification HuggingFace in Spark NLP - CamemBertForSequenceClassification CamemBertForQuestionAnswering HuggingFace in Spark NLP - CamemBertForQuestionAnswering BertForSequenceClassification HuggingFace in Spark NLP - BertForSequenceClassification DistilBertForSequenceClassification HuggingFace in Spark NLP - DistilBertForSequenceClassification AlbertForSequenceClassification HuggingFace in Spark NLP - AlbertForSequenceClassification RoBertaForSequenceClassification HuggingFace in Spark NLP - RoBertaForSequenceClassification XlmRoBertaForSequenceClassification HuggingFace in Spark NLP - XlmRoBertaForSequenceClassification XlnetForSequenceClassification HuggingFace in Spark NLP - XlnetForSequenceClassification LongformerForSequenceClassification HuggingFace in Spark NLP - LongformerForSequenceClassification AlbertForQuestionAnswering HuggingFace in Spark NLP - AlbertForQuestionAnswering BertForQuestionAnswering HuggingFace in Spark NLP - BertForQuestionAnswering DeBertaForQuestionAnswering HuggingFace in Spark NLP - DeBertaForQuestionAnswering DistilBertForQuestionAnswering HuggingFace in Spark NLP - DistilBertForQuestionAnswering LongformerForQuestionAnswering HuggingFace in Spark NLP - LongformerForQuestionAnswering RoBertaForQuestionAnswering HuggingFace in Spark NLP - RoBertaForQuestionAnswering XlmRobertaForQuestionAnswering HuggingFace in Spark NLP - XlmRobertaForQuestionAnswering ViTForImageClassification HuggingFace in Spark NLP - ViTForImageClassification ConvNextForImageClassification HuggingFace in Spark NLP - ConvNextForImageClassification SwinForImageClassification HuggingFace in Spark NLP - SwinForImageClassification BertForZeroShotClassification HuggingFace in Spark NLP - BertForZeroShotClassification DistilBertForZeroShotClassification HuggingFace in Spark NLP - DistilBertForZeroShotClassification RoBertaForZeroShotClassification HuggingFace in Spark NLP - RoBertaForZeroShotClassification TF Hub to Spark NLP Spark NLP TF Hub Notebooks Colab BertEmbeddings TF Hub in Spark NLP - BERT BertSentenceEmbeddings TF Hub in Spark NLP - BERT Sentence AlbertEmbeddings TF Hub in Spark NLP - ALBERT",
    "url": "/docs/en/transformers",
    "relUrl": "/docs/en/transformers"
  },
  "66": {
    "id": "66",
    "title": "Video Tutorials",
    "content": "{%- include extensions/youtube.html id=&#39;isxffn4Tcds&#39; -%}How to Install NLP Server on Azure {%- include extensions/youtube.html id=&#39;YZFhsZZD6QM&#39; -%}How to Import a License in the NLP Server",
    "url": "/docs/en/nlp_server/video_tutorials",
    "relUrl": "/docs/en/nlp_server/video_tutorials"
  }
  
}
