<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-07-20T13:49:25+00:00</updated><id>/feed.xml</id><title type="html">Spark NLP</title><subtitle>High Performance NLP with Apache Spark
</subtitle><author><name>{&quot;type&quot;=&gt;nil, &quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;googleplus&quot;=&gt;nil, &quot;telegram&quot;=&gt;nil, &quot;medium&quot;=&gt;nil, &quot;zhihu&quot;=&gt;nil, &quot;douban&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;npm&quot;=&gt;nil}</name></author><entry><title type="html">Detect Movie Entities - MIT Movie Complex (ner_mit_movie_complex_bert_base_cased)</title><link href="/2021/07/20/ner_mit_movie_complex_bert_base_cased_en.html" rel="alternate" type="text/html" title="Detect Movie Entities - MIT Movie Complex (ner_mit_movie_complex_bert_base_cased)" /><published>2021-07-20T00:00:00+00:00</published><updated>2021-07-20T00:00:00+00:00</updated><id>/2021/07/20/ner_mit_movie_complex_bert_base_cased_en</id><content type="html" xml:base="/2021/07/20/ner_mit_movie_complex_bert_base_cased_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;This NER model was trained over the MIT Movie Corpus complex queries dataset to detect movie trivia. We used BertEmbeddings (bert_base_cased) model for the embeddings to train this NER model.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Actor&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Award&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Character_Name&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Director&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Genre&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Opinion&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Origin&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Plot&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Quote&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Relationship&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Soundtrack&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Year&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/ner_mit_movie_complex_bert_base_cased_en_3.1.3_2.4_1626776981448.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertEmbeddings&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bert_base_cased'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerDLModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner_mit_movie_complex_bert_base_cased'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'embeddings'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert_base_cased&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_model&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;NerDLModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner_mit_movie_complex_bert_base_cased&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;',&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;div class=&quot;language-python nlu-block highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nlu&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;My name is John!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en.ner.ner_mit_movie_complex_bert_base_cased'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_level&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;ner_mit_movie_complex_bert_base_cased&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Type:&lt;/td&gt;
      &lt;td&gt;ner&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.1.3+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[sentence, token, embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://groups.csail.mit.edu/sls/downloads/movie/&quot;&gt;https://groups.csail.mit.edu/sls/downloads/movie/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;processed 15904 tokens with 2278 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 2292 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 1664.
accuracy:  88.81%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  88.78%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  72.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.05%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.82
            Actor: precision:  96.46%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  94.97%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  95.71  509
            Award: precision:  63.64%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  61.76%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  62.69  33
   Character_Name: precision:  61.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.54%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  64.89  99
         Director: precision:  83.43%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.36%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.89  181
            Genre: precision:  74.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.85  324
          Opinion: precision:  39.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  46.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  42.70  97
           Origin: precision:  35.37%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  40.85%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  37.91  82
             Plot: precision:  53.95%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  53.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  53.77  621
            Quote: precision:  64.29%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  39.13%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  48.65  14
     Relationship: precision:  48.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  50.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  48.98  50
       Soundtrack: precision:  80.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  57.14%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  66.67  5
             Year: precision:  94.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.88%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  94.05  277
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="bert" /><category term="en" /><category term="english" /><category term="ner" /><category term="movie" /><summary type="html">Description This NER model was trained over the MIT Movie Corpus complex queries dataset to detect movie trivia. We used BertEmbeddings (bert_base_cased) model for the embeddings to train this NER model. Predicted Entities Actor Award Character_Name Director Genre Opinion Origin Plot Quote Relationship Soundtrack Year Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') embeddings = BertEmbeddings\ .pretrained('bert_base_cased', 'en')\ .setInputCols([&quot;token&quot;, &quot;document&quot;])\ .setOutputCol(&quot;embeddings&quot;) ner_model = NerDLModel.pretrained('ner_mit_movie_complex_bert_base_cased', 'en') \ .setInputCols(['document', 'token', 'embeddings']) \ .setOutputCol('ner') ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, embeddings, ner_model, ner_converter ]) example = spark.createDataFrame(pd.DataFrame({'text': ['My name is John!']})) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained(&quot;bert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val ner_model = NerDLModel.pretrained(&quot;ner_mit_movie_complex_bert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;', &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, embeddings, ner_model, ner_converter)) val result = pipeline.fit(Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;)).transform(data) import nlu text = [&quot;My name is John!&quot;] ner_df = nlu.load('en.ner.ner_mit_movie_complex_bert_base_cased').predict(text, output_level='token') Model Information Model Name: ner_mit_movie_complex_bert_base_cased Type: ner Compatibility: Spark NLP 3.1.3+ License: Open Source Edition: Official Input Labels: [sentence, token, embeddings] Output Labels: [ner] Language: en Data Source https://groups.csail.mit.edu/sls/downloads/movie/ Benchmarking processed 15904 tokens with 2278 phrases; found: 2292 phrases; correct: 1664. accuracy: 88.81%; (non-O) accuracy: 88.78%; precision: 72.60%; recall: 73.05%; FB1: 72.82 Actor: precision: 96.46%; recall: 94.97%; FB1: 95.71 509 Award: precision: 63.64%; recall: 61.76%; FB1: 62.69 33 Character_Name: precision: 61.62%; recall: 68.54%; FB1: 64.89 99 Director: precision: 83.43%; recall: 84.36%; FB1: 83.89 181 Genre: precision: 74.07%; recall: 73.62%; FB1: 73.85 324 Opinion: precision: 39.18%; recall: 46.91%; FB1: 42.70 97 Origin: precision: 35.37%; recall: 40.85%; FB1: 37.91 82 Plot: precision: 53.95%; recall: 53.60%; FB1: 53.77 621 Quote: precision: 64.29%; recall: 39.13%; FB1: 48.65 14 Relationship: precision: 48.00%; recall: 50.00%; FB1: 48.98 50 Soundtrack: precision: 80.00%; recall: 57.14%; FB1: 66.67 5 Year: precision: 94.22%; recall: 93.88%; FB1: 94.05 277</summary></entry><entry><title type="html">Detect Movie Entities - MIT Movie Complex (ner_mit_movie_complex_distilbert_base_cased)</title><link href="/2021/07/20/ner_mit_movie_complex_distilbert_base_cased_en.html" rel="alternate" type="text/html" title="Detect Movie Entities - MIT Movie Complex (ner_mit_movie_complex_distilbert_base_cased)" /><published>2021-07-20T00:00:00+00:00</published><updated>2021-07-20T00:00:00+00:00</updated><id>/2021/07/20/ner_mit_movie_complex_distilbert_base_cased_en</id><content type="html" xml:base="/2021/07/20/ner_mit_movie_complex_distilbert_base_cased_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;This NER model was trained over the MIT Movie Corpus complex queries dataset to detect movie trivia. We used DistilBertEmbeddings (distilbert_base_cased) model for the embeddings to train this NER model.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Actor&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Award&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Character_Name&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Director&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Genre&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Opinion&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Origin&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Plot&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Quote&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Relationship&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Soundtrack&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Year&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/ner_mit_movie_complex_distilbert_base_cased_en_3.1.3_2.4_1626777800973.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DistilBertEmbeddings&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'distilbert_base_cased'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerDLModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner_mit_movie_complex_distilbert_base_cased'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'embeddings'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;DistilBertEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;distilbert_base_cased&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_model&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;NerDLModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner_mit_movie_complex_distilbert_base_cased&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;',&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;div class=&quot;language-python nlu-block highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nlu&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;My name is John!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en.ner.ner_mit_movie_complex_distilbert_base_cased'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_level&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;ner_mit_movie_complex_distilbert_base_cased&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Type:&lt;/td&gt;
      &lt;td&gt;ner&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.1.3+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[sentence, token, embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://groups.csail.mit.edu/sls/downloads/movie/&quot;&gt;https://groups.csail.mit.edu/sls/downloads/movie/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;processed 15904 tokens with 2278 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 2277 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 1674.
accuracy:  89.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  88.41%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  73.52%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.49%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.50
            Actor: precision:  96.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  96.13%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  96.32  515
            Award: precision:  51.85%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  41.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  45.90  27
   Character_Name: precision:  72.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  74.16%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.33  91
         Director: precision:  81.77%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  87.71%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.64  192
            Genre: precision:  75.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  74.54%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.77  324
          Opinion: precision:  41.94%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  48.15%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  44.83  93
           Origin: precision:  37.70%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  32.39%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  34.85  61
             Plot: precision:  53.43%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  53.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  53.51  627
            Quote: precision:  56.25%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  39.13%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  46.15  16
     Relationship: precision:  55.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  56.25%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  55.67  49
       Soundtrack: precision:  42.86%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  42.86%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  42.86  7
             Year: precision:  94.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.88%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  94.39  275
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="distilbert" /><category term="en" /><category term="english" /><category term="ner" /><category term="movie" /><summary type="html">Description This NER model was trained over the MIT Movie Corpus complex queries dataset to detect movie trivia. We used DistilBertEmbeddings (distilbert_base_cased) model for the embeddings to train this NER model. Predicted Entities Actor Award Character_Name Director Genre Opinion Origin Plot Quote Relationship Soundtrack Year Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') embeddings = DistilBertEmbeddings\ .pretrained('distilbert_base_cased', 'en')\ .setInputCols([&quot;token&quot;, &quot;document&quot;])\ .setOutputCol(&quot;embeddings&quot;) ner_model = NerDLModel.pretrained('ner_mit_movie_complex_distilbert_base_cased', 'en') \ .setInputCols(['document', 'token', 'embeddings']) \ .setOutputCol('ner') ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, embeddings, ner_model, ner_converter ]) example = spark.createDataFrame(pd.DataFrame({'text': ['My name is John!']})) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val ner_model = NerDLModel.pretrained(&quot;ner_mit_movie_complex_distilbert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;', &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, embeddings, ner_model, ner_converter)) val result = pipeline.fit(Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;)).transform(data) import nlu text = [&quot;My name is John!&quot;] ner_df = nlu.load('en.ner.ner_mit_movie_complex_distilbert_base_cased').predict(text, output_level='token') Model Information Model Name: ner_mit_movie_complex_distilbert_base_cased Type: ner Compatibility: Spark NLP 3.1.3+ License: Open Source Edition: Official Input Labels: [sentence, token, embeddings] Output Labels: [ner] Language: en Data Source https://groups.csail.mit.edu/sls/downloads/movie/ Benchmarking processed 15904 tokens with 2278 phrases; found: 2277 phrases; correct: 1674. accuracy: 89.18%; (non-O) accuracy: 88.41%; precision: 73.52%; recall: 73.49%; FB1: 73.50 Actor: precision: 96.50%; recall: 96.13%; FB1: 96.32 515 Award: precision: 51.85%; recall: 41.18%; FB1: 45.90 27 Character_Name: precision: 72.53%; recall: 74.16%; FB1: 73.33 91 Director: precision: 81.77%; recall: 87.71%; FB1: 84.64 192 Genre: precision: 75.00%; recall: 74.54%; FB1: 74.77 324 Opinion: precision: 41.94%; recall: 48.15%; FB1: 44.83 93 Origin: precision: 37.70%; recall: 32.39%; FB1: 34.85 61 Plot: precision: 53.43%; recall: 53.60%; FB1: 53.51 627 Quote: precision: 56.25%; recall: 39.13%; FB1: 46.15 16 Relationship: precision: 55.10%; recall: 56.25%; FB1: 55.67 49 Soundtrack: precision: 42.86%; recall: 42.86%; FB1: 42.86 7 Year: precision: 94.91%; recall: 93.88%; FB1: 94.39 275</summary></entry><entry><title type="html">Detect Movie Entities - MIT Movie Simple (ner_mit_movie_simple_distilbert_base_cased)</title><link href="/2021/07/20/ner_mit_movie_simple_distilbert_base_cased_en.html" rel="alternate" type="text/html" title="Detect Movie Entities - MIT Movie Simple (ner_mit_movie_simple_distilbert_base_cased)" /><published>2021-07-20T00:00:00+00:00</published><updated>2021-07-20T00:00:00+00:00</updated><id>/2021/07/20/ner_mit_movie_simple_distilbert_base_cased_en</id><content type="html" xml:base="/2021/07/20/ner_mit_movie_simple_distilbert_base_cased_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;This NER model was trained over the MIT Movie Corpus simple queries dataset to detect movie trivia. We used DistilBertEmbeddings (distilbert_base_cased) model for the embeddings to train this NER model.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ACTOR&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CHARACTER&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DIRECTOR&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;GENRE&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PLOT&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RATING&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RATINGS_AVERAGE&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;REVIEW&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SONG&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TITLE&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TRAILER&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;YEAR&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/ner_mit_movie_simple_distilbert_base_cased_en_3.1.3_2.4_1626778585112.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DistilBertEmbeddings&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'distilbert_base_cased'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerDLModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner_mit_movie_simple_distilbert_base_cased'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'embeddings'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;DistilBertEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;distilbert_base_cased&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_model&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;NerDLModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner_mit_movie_simple_distilbert_base_cased&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;',&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;div class=&quot;language-python nlu-block highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nlu&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;My name is John!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en.ner. ner_mit_movie_simple_distilbert_base_cased'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_level&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;ner_mit_movie_simple_distilbert_base_cased&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Type:&lt;/td&gt;
      &lt;td&gt;ner&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.1.3+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[sentence, token, embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://groups.csail.mit.edu/sls/downloads/movie/&quot;&gt;https://groups.csail.mit.edu/sls/downloads/movie/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;processed 24686 tokens with 5339 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 5331 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 4677.
accuracy:  87.88%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  93.74%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  87.73%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  87.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.67
            ACTOR: precision:  88.34%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  95.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  91.64  875
        CHARACTER: precision:  64.56%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  56.67%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  60.36  79
         DIRECTOR: precision:  93.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.43%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.51  414
            GENRE: precision:  91.04%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  94.63%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.80  1161
             PLOT: precision:  70.86%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  72.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  71.57  501
           RATING: precision:  93.16%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  92.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.88  497
  RATINGS_AVERAGE: precision:  83.94%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  85.40  467
           REVIEW: precision:  47.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  14.29%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  21.92  17
             SONG: precision:  76.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  53.70%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  63.04  38
            TITLE: precision:  84.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  83.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.84  552
          TRAILER: precision:  83.87%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.67%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  85.25  31
             YEAR: precision:  95.99%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.19%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  94.57  699

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="en" /><category term="english" /><category term="distibert" /><category term="movie" /><category term="ner" /><summary type="html">Description This NER model was trained over the MIT Movie Corpus simple queries dataset to detect movie trivia. We used DistilBertEmbeddings (distilbert_base_cased) model for the embeddings to train this NER model. Predicted Entities ACTOR CHARACTER DIRECTOR GENRE PLOT RATING RATINGS_AVERAGE REVIEW SONG TITLE TRAILER YEAR Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') embeddings = DistilBertEmbeddings\ .pretrained('distilbert_base_cased', 'en')\ .setInputCols([&quot;token&quot;, &quot;document&quot;])\ .setOutputCol(&quot;embeddings&quot;) ner_model = NerDLModel.pretrained('ner_mit_movie_simple_distilbert_base_cased', 'en') \ .setInputCols(['document', 'token', 'embeddings']) \ .setOutputCol('ner') ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, embeddings, ner_model, ner_converter ]) example = spark.createDataFrame(pd.DataFrame({'text': ['My name is John!']})) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = DistilBertEmbeddings.pretrained(&quot;distilbert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val ner_model = NerDLModel.pretrained(&quot;ner_mit_movie_simple_distilbert_base_cased&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;', &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, embeddings, ner_model, ner_converter)) val result = pipeline.fit(Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;)).transform(data) import nlu text = [&quot;My name is John!&quot;] ner_df = nlu.load('en.ner. ner_mit_movie_simple_distilbert_base_cased').predict(text, output_level='token') Model Information Model Name: ner_mit_movie_simple_distilbert_base_cased Type: ner Compatibility: Spark NLP 3.1.3+ License: Open Source Edition: Official Input Labels: [sentence, token, embeddings] Output Labels: [ner] Language: en Data Source https://groups.csail.mit.edu/sls/downloads/movie/ Benchmarking processed 24686 tokens with 5339 phrases; found: 5331 phrases; correct: 4677. accuracy: 87.88%; (non-O) accuracy: 93.74%; precision: 87.73%; recall: 87.60%; FB1: 87.67 ACTOR: precision: 88.34%; recall: 95.20%; FB1: 91.64 875 CHARACTER: precision: 64.56%; recall: 56.67%; FB1: 60.36 79 DIRECTOR: precision: 93.00%; recall: 84.43%; FB1: 88.51 414 GENRE: precision: 91.04%; recall: 94.63%; FB1: 92.80 1161 PLOT: precision: 70.86%; recall: 72.30%; FB1: 71.57 501 RATING: precision: 93.16%; recall: 92.60%; FB1: 92.88 497 RATINGS_AVERAGE: precision: 83.94%; recall: 86.92%; FB1: 85.40 467 REVIEW: precision: 47.06%; recall: 14.29%; FB1: 21.92 17 SONG: precision: 76.32%; recall: 53.70%; FB1: 63.04 38 TITLE: precision: 84.60%; recall: 83.10%; FB1: 83.84 552 TRAILER: precision: 83.87%; recall: 86.67%; FB1: 85.25 31 YEAR: precision: 95.99%; recall: 93.19%; FB1: 94.57 699</summary></entry><entry><title type="html">Universal sentence encoder for English trained with CMLM (sent_bert_use_cmlm_en_base)</title><link href="/2021/07/20/sent_bert_use_cmlm_en_base_en.html" rel="alternate" type="text/html" title="Universal sentence encoder for English trained with CMLM (sent_bert_use_cmlm_en_base)" /><published>2021-07-20T00:00:00+00:00</published><updated>2021-07-20T00:00:00+00:00</updated><id>/2021/07/20/sent_bert_use_cmlm_en_base_en</id><content type="html" xml:base="/2021/07/20/sent_bert_use_cmlm_en_base_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;Universal sentence encoder for English trained with a conditional masked language model. The universal sentence encoder family of models maps the text into high dimensional vectors that capture sentence-level semantics. Our English-base (en-base) model is trained using a conditional masked language model described in [1]. The model is intended to be used for text classification, text clustering, semantic textual similarity, etc. It can also be used used as modularized input for multimodal tasks with text as a feature. The base model employs a 12 layer BERT transformer architecture.&lt;/p&gt;

&lt;p&gt;The model extends the BERT transformer architecture that is why we use it with BertSentenceEmbeddings.&lt;/p&gt;

&lt;p&gt;[1] Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve. &lt;a href=&quot;https://openreview.net/forum?id=WDVD4lUCTzU&quot;&gt;Universal Sentence Representations Learning with Conditional Masked Language Model. November 2020&lt;/a&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/sent_bert_use_cmlm_en_base_en_3.1.3_2.4_1626782549609.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertSentenceEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sent_bert_use_cmlm_en_base&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence_embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertSentenceEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sent_bert_use_cmlm_en_base&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence_embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;div class=&quot;language-python nlu-block highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nlu&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I hate cancer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Antibiotics aren't painkiller&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;embeddings_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en.embed_sentence.sent_bert_use_cmlm_en_base'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_level&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sentence'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;embeddings_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;sent_bert_use_cmlm_en_base&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.1.3+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[sentence]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[bert]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-base/1&quot;&gt;https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-base/1&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Training News dataset by using ClassifierDL with 120K training examples:

            precision    recall  f1-score   support

    Business       0.84      0.90      0.87      1784
    Sci/Tech       0.92      0.85      0.89      2053
      Sports       0.98      0.96      0.97      1952
       World       0.89      0.93      0.91      1811

    accuracy                           0.91      7600
   macro avg       0.91      0.91      0.91      7600
weighted avg       0.91      0.91      0.91      7600
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="embeddings" /><category term="bert" /><category term="open_source" /><category term="english" /><category term="en" /><category term="cmlm" /><category term="use" /><summary type="html">Description Universal sentence encoder for English trained with a conditional masked language model. The universal sentence encoder family of models maps the text into high dimensional vectors that capture sentence-level semantics. Our English-base (en-base) model is trained using a conditional masked language model described in [1]. The model is intended to be used for text classification, text clustering, semantic textual similarity, etc. It can also be used used as modularized input for multimodal tasks with text as a feature. The base model employs a 12 layer BERT transformer architecture. The model extends the BERT transformer architecture that is why we use it with BertSentenceEmbeddings. [1] Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve. Universal Sentence Representations Learning with Conditional Masked Language Model. November 2020 Live Demo Open in Colab Download How to use PythonScalaNLU embeddings = BertSentenceEmbeddings.pretrained(&quot;sent_bert_use_cmlm_en_base&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;) \ .setOutputCol(&quot;sentence_embeddings&quot;) val embeddings = BertSentenceEmbeddings.pretrained(&quot;sent_bert_use_cmlm_en_base&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) import nlu text = [&quot;I hate cancer&quot;, &quot;Antibiotics aren't painkiller&quot;] embeddings_df = nlu.load('en.embed_sentence.sent_bert_use_cmlm_en_base').predict(text, output_level='sentence') embeddings_df Model Information Model Name: sent_bert_use_cmlm_en_base Compatibility: Spark NLP 3.1.3+ License: Open Source Edition: Official Input Labels: [sentence] Output Labels: [bert] Language: en Case sensitive: false Data Source https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-base/1 Benchmarking Training News dataset by using ClassifierDL with 120K training examples: precision recall f1-score support Business 0.84 0.90 0.87 1784 Sci/Tech 0.92 0.85 0.89 2053 Sports 0.98 0.96 0.97 1952 World 0.89 0.93 0.91 1811 accuracy 0.91 7600 macro avg 0.91 0.91 0.91 7600 weighted avg 0.91 0.91 0.91 7600</summary></entry><entry><title type="html">Universal sentence encoder for English trained with CMLM (sent_bert_use_cmlm_en_large)</title><link href="/2021/07/20/sent_bert_use_cmlm_en_large_en.html" rel="alternate" type="text/html" title="Universal sentence encoder for English trained with CMLM (sent_bert_use_cmlm_en_large)" /><published>2021-07-20T00:00:00+00:00</published><updated>2021-07-20T00:00:00+00:00</updated><id>/2021/07/20/sent_bert_use_cmlm_en_large_en</id><content type="html" xml:base="/2021/07/20/sent_bert_use_cmlm_en_large_en.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;Universal sentence encoder for English trained with a conditional masked language model. The universal sentence encoder family of models maps the text into high dimensional vectors that capture sentence-level semantics. Our English-Large (en-large) model is trained using a conditional masked language model described in [1]. The model is intended to be used for text classification, text clustering, semantic textual similarity, etc. It can also be used used as modularized input for multimodal tasks with text as a feature. The large model employs a 24 layer BERT transformer architecture.&lt;/p&gt;

&lt;p&gt;The model extends the BERT transformer architecture that is why we use it with BertSentenceEmbeddings.&lt;/p&gt;

&lt;p&gt;[1] Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve. &lt;a href=&quot;https://openreview.net/forum?id=WDVD4lUCTzU&quot;&gt;Universal Sentence Representations Learning with Conditional Masked Language Model. November 2020&lt;/a&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/sent_bert_use_cmlm_en_large_en_3.1.3_2.4_1626783107796.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertSentenceEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sent_bert_use_cmlm_en_large&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence_embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertSentenceEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sent_bert_use_cmlm_en_large&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;en&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence_embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;div class=&quot;language-python nlu-block highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nlu&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I hate cancer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Antibiotics aren't painkiller&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;embeddings_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'en.embed_sentence.sent_bert_use_cmlm_en_large'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_level&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sentence'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;embeddings_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;sent_bert_use_cmlm_en_large&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.1.3+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[sentence]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[bert]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;en&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;false&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-large/1&quot;&gt;https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-large/1&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Training News dataset by using ClassifierDL with 120K training examples:

             precision    recall  f1-score   support

    Business       0.88      0.89      0.88      1880
    Sci/Tech       0.91      0.88      0.89      1963
      Sports       0.98      0.95      0.97      1961
       World       0.89      0.94      0.92      1796

    accuracy                           0.92      7600
   macro avg       0.92      0.92      0.92      7600
weighted avg       0.92      0.92      0.92      7600


We evaluate this model on SentEval sentence representation benchmark.

SentEval	MR	CR	SUBJ	MPQA	SST	TREC	MRPC	SICK-E	SICK-R	Avg
USE-CMLM-Base	83.6	89.9	96.2	89.3	88.5	91.0	69.7	82.3	83.4	86.0
USE-CMLM-Large	85.6	89.1	96.6	89.3	91.4	92.4	70.0	82.2	84.5	86.8
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="embeddings" /><category term="bert" /><category term="use" /><category term="en" /><category term="english" /><category term="cmlm" /><category term="open_source" /><summary type="html">Description Universal sentence encoder for English trained with a conditional masked language model. The universal sentence encoder family of models maps the text into high dimensional vectors that capture sentence-level semantics. Our English-Large (en-large) model is trained using a conditional masked language model described in [1]. The model is intended to be used for text classification, text clustering, semantic textual similarity, etc. It can also be used used as modularized input for multimodal tasks with text as a feature. The large model employs a 24 layer BERT transformer architecture. The model extends the BERT transformer architecture that is why we use it with BertSentenceEmbeddings. [1] Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve. Universal Sentence Representations Learning with Conditional Masked Language Model. November 2020 Live Demo Open in Colab Download How to use PythonScalaNLU embeddings = BertSentenceEmbeddings.pretrained(&quot;sent_bert_use_cmlm_en_large&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;) \ .setOutputCol(&quot;sentence_embeddings&quot;) val embeddings = BertSentenceEmbeddings.pretrained(&quot;sent_bert_use_cmlm_en_large&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) import nlu text = [&quot;I hate cancer&quot;, &quot;Antibiotics aren't painkiller&quot;] embeddings_df = nlu.load('en.embed_sentence.sent_bert_use_cmlm_en_large').predict(text, output_level='sentence') embeddings_df Model Information Model Name: sent_bert_use_cmlm_en_large Compatibility: Spark NLP 3.1.3+ License: Open Source Edition: Official Input Labels: [sentence] Output Labels: [bert] Language: en Case sensitive: false Data Source https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-large/1 Benchmarking Training News dataset by using ClassifierDL with 120K training examples: precision recall f1-score support Business 0.88 0.89 0.88 1880 Sci/Tech 0.91 0.88 0.89 1963 Sports 0.98 0.95 0.97 1961 World 0.89 0.94 0.92 1796 accuracy 0.92 7600 macro avg 0.92 0.92 0.92 7600 weighted avg 0.92 0.92 0.92 7600 We evaluate this model on SentEval sentence representation benchmark. SentEval MR CR SUBJ MPQA SST TREC MRPC SICK-E SICK-R Avg USE-CMLM-Base 83.6 89.9 96.2 89.3 88.5 91.0 69.7 82.3 83.4 86.0 USE-CMLM-Large 85.6 89.1 96.6 89.3 91.4 92.4 70.0 82.2 84.5 86.8</summary></entry><entry><title type="html">Universal sentence encoder for 100+ languages trained with CMLM (sent_bert_use_cmlm_multi_base_br)</title><link href="/2021/07/20/sent_bert_use_cmlm_multi_base_br_xx.html" rel="alternate" type="text/html" title="Universal sentence encoder for 100+ languages trained with CMLM (sent_bert_use_cmlm_multi_base_br)" /><published>2021-07-20T00:00:00+00:00</published><updated>2021-07-20T00:00:00+00:00</updated><id>/2021/07/20/sent_bert_use_cmlm_multi_base_br_xx</id><content type="html" xml:base="/2021/07/20/sent_bert_use_cmlm_multi_base_br_xx.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;The universal sentence encoder family of models maps the text into high dimensional vectors that capture sentence-level semantics. Our Multilingual-base bitext retrieval model (multilingual-base-br) is trained using a conditional masked language model described in [1]. The model is intended to be used for text classification, text clustering, semantic textural similarity, etc. The model can be fine-tuned for all of these tasks. The base model employs a 12 layer BERT transformer architecture.&lt;/p&gt;

&lt;p&gt;The model extends the BERT transformer architecture that is why we use it with BertSentenceEmbeddings.&lt;/p&gt;

&lt;p&gt;[1] Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve. &lt;a href=&quot;https://openreview.net/forum?id=WDVD4lUCTzU&quot;&gt;Universal Sentence Representations Learning with Conditional Masked Language Model. November 2020&lt;/a&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/sent_bert_use_cmlm_multi_base_br_xx_3.1.3_2.4_1626783435472.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertSentenceEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sent_bert_use_cmlm_multi_base_br&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence_embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertSentenceEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sent_bert_use_cmlm_multi_base_br&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence_embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;div class=&quot;language-python nlu-block highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nlu&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I hate cancer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Antibiotics aren't painkiller&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;embeddings_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xx.embed_sentence.sent_bert_use_cmlm_multi_base_br'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_level&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sentence'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;embeddings_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;sent_bert_use_cmlm_multi_base_br&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.1.3+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[sentence]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[bert]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;xx&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base-br/1&quot;&gt;https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base-br/1&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;We evaluate this model on XEVAL, translated SentEval sentence representation benchmark. XEVAL will be publicly available soon.

XEVAL	ar	&lt;span class=&quot;nb&quot;&gt;bg	&lt;/span&gt;de	....	zh	15 Languages Average
USE-CMLM-Multilingual-Base	80.6	81.2	82.6	....	81.7	81.2
USE-CMLM-Multilingual-Base + BR	82.6	83.0	84.0	....	83.0	82.8

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="embeddings" /><category term="bert" /><category term="use" /><category term="open_source" /><category term="cmlm" /><category term="xx" /><category term="multilingual" /><summary type="html">Description The universal sentence encoder family of models maps the text into high dimensional vectors that capture sentence-level semantics. Our Multilingual-base bitext retrieval model (multilingual-base-br) is trained using a conditional masked language model described in [1]. The model is intended to be used for text classification, text clustering, semantic textural similarity, etc. The model can be fine-tuned for all of these tasks. The base model employs a 12 layer BERT transformer architecture. The model extends the BERT transformer architecture that is why we use it with BertSentenceEmbeddings. [1] Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve. Universal Sentence Representations Learning with Conditional Masked Language Model. November 2020 Live Demo Open in Colab Download How to use PythonScalaNLU embeddings = BertSentenceEmbeddings.pretrained(&quot;sent_bert_use_cmlm_multi_base_br&quot;, &quot;xx&quot;) \ .setInputCols(&quot;sentence&quot;) \ .setOutputCol(&quot;sentence_embeddings&quot;) val embeddings = BertSentenceEmbeddings.pretrained(&quot;sent_bert_use_cmlm_multi_base_br&quot;, &quot;xx&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) import nlu text = [&quot;I hate cancer&quot;, &quot;Antibiotics aren't painkiller&quot;] embeddings_df = nlu.load('xx.embed_sentence.sent_bert_use_cmlm_multi_base_br').predict(text, output_level='sentence') embeddings_df Model Information Model Name: sent_bert_use_cmlm_multi_base_br Compatibility: Spark NLP 3.1.3+ License: Open Source Edition: Official Input Labels: [sentence] Output Labels: [bert] Language: xx Case sensitive: true Data Source https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base-br/1 Benchmarking We evaluate this model on XEVAL, translated SentEval sentence representation benchmark. XEVAL will be publicly available soon. XEVAL ar bg de .... zh 15 Languages Average USE-CMLM-Multilingual-Base 80.6 81.2 82.6 .... 81.7 81.2 USE-CMLM-Multilingual-Base + BR 82.6 83.0 84.0 .... 83.0 82.8</summary></entry><entry><title type="html">Universal sentence encoder for 100+ languages trained with CMLM (sent_bert_use_cmlm_multi_base)</title><link href="/2021/07/20/sent_bert_use_cmlm_multi_base_xx.html" rel="alternate" type="text/html" title="Universal sentence encoder for 100+ languages trained with CMLM (sent_bert_use_cmlm_multi_base)" /><published>2021-07-20T00:00:00+00:00</published><updated>2021-07-20T00:00:00+00:00</updated><id>/2021/07/20/sent_bert_use_cmlm_multi_base_xx</id><content type="html" xml:base="/2021/07/20/sent_bert_use_cmlm_multi_base_xx.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;The universal sentence encoder family of models maps the text into high dimensional vectors that capture sentence-level semantics. Our Multilingual-base model is trained using a conditional masked language model described in [1]. The model is intended to be used for text classification, text clustering, semantic textual similarity, etc. The base model employs a 12 layer BERT transformer architecture.&lt;/p&gt;

&lt;p&gt;The model extends the BERT transformer architecture that is why we use it with BertSentenceEmbeddings.&lt;/p&gt;

&lt;p&gt;[1] Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve. &lt;a href=&quot;https://openreview.net/forum?id=WDVD4lUCTzU&quot;&gt;Universal Sentence Representations Learning with Conditional Masked Language Model. November 2020&lt;/a&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/sent_bert_use_cmlm_multi_base_xx_3.1.3_2.4_1626783880233.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertSentenceEmbeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sent_bert_use_cmlm_multi_base&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence_embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;BertSentenceEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sent_bert_use_cmlm_multi_base&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;sentence_embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;div class=&quot;language-python nlu-block highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nlu&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;I hate cancer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Antibiotics aren't painkiller&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;embeddings_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xx.embed_sentence.sent_bert_use_cmlm_multi_base'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_level&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sentence'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;embeddings_df&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;sent_bert_use_cmlm_multi_base&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.1.3+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[sentence]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[bert]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;xx&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Case sensitive:&lt;/td&gt;
      &lt;td&gt;true&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1&quot;&gt;https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;We evaluate this model on XEVAL, translated SentEval sentence representation benchmark. XEVAL will be publicly available soon.

XEVAL	ar	&lt;span class=&quot;nb&quot;&gt;bg	&lt;/span&gt;de	....	zh	15 Languages Average
USE-CMLM-Multilingual-Base	80.6	81.2	82.6	....	81.7	81.2
USE-CMLM-Multilingual-Base + BR	82.6	83.0	84.0	....	83.0	82.8

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="embeddings" /><category term="bert" /><category term="use" /><category term="xx" /><category term="multilingual" /><category term="cmlm" /><category term="open_source" /><summary type="html">Description The universal sentence encoder family of models maps the text into high dimensional vectors that capture sentence-level semantics. Our Multilingual-base model is trained using a conditional masked language model described in [1]. The model is intended to be used for text classification, text clustering, semantic textual similarity, etc. The base model employs a 12 layer BERT transformer architecture. The model extends the BERT transformer architecture that is why we use it with BertSentenceEmbeddings. [1] Ziyi Yang, Yinfei Yang, Daniel Cer, Jax Law, Eric Darve. Universal Sentence Representations Learning with Conditional Masked Language Model. November 2020 Live Demo Open in Colab Download How to use PythonScalaNLU embeddings = BertSentenceEmbeddings.pretrained(&quot;sent_bert_use_cmlm_multi_base&quot;, &quot;xx&quot;) \ .setInputCols(&quot;sentence&quot;) \ .setOutputCol(&quot;sentence_embeddings&quot;) val embeddings = BertSentenceEmbeddings.pretrained(&quot;sent_bert_use_cmlm_multi_base&quot;, &quot;xx&quot;) .setInputCols(&quot;sentence&quot;) .setOutputCol(&quot;sentence_embeddings&quot;) import nlu text = [&quot;I hate cancer&quot;, &quot;Antibiotics aren't painkiller&quot;] embeddings_df = nlu.load('xx.embed_sentence.sent_bert_use_cmlm_multi_base').predict(text, output_level='sentence') embeddings_df Model Information Model Name: sent_bert_use_cmlm_multi_base Compatibility: Spark NLP 3.1.3+ License: Open Source Edition: Official Input Labels: [sentence] Output Labels: [bert] Language: xx Case sensitive: true Data Source https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1 Benchmarking We evaluate this model on XEVAL, translated SentEval sentence representation benchmark. XEVAL will be publicly available soon. XEVAL ar bg de .... zh 15 Languages Average USE-CMLM-Multilingual-Base 80.6 81.2 82.6 .... 81.7 81.2 USE-CMLM-Multilingual-Base + BR 82.6 83.0 84.0 .... 83.0 82.8</summary></entry><entry><title type="html">Detect Entities in 8 languages - WIKINER (ner_wikiner_glove_840B_300)</title><link href="/2021/07/19/ner_wikiner_glove_840B_300_xx.html" rel="alternate" type="text/html" title="Detect Entities in 8 languages - WIKINER (ner_wikiner_glove_840B_300)" /><published>2021-07-19T00:00:00+00:00</published><updated>2021-07-19T00:00:00+00:00</updated><id>/2021/07/19/ner_wikiner_glove_840B_300_xx</id><content type="html" xml:base="/2021/07/19/ner_wikiner_glove_840B_300_xx.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;This NER model was trained over WIKINER datasets with 8 languages including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;English&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;French&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;German&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Italian&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Polish&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Portuguese&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Russian&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Spanish&lt;/code&gt;.
We used WordEmbeddings (glove_840B_300) model for the embeddings to train this NER model.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-LOC&lt;/code&gt; 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-LOC&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-ORG&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-ORG&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-PER&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-PER&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/ner_wikiner_glove_840B_300_xx_3.1.3_2.4_1626717450663.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WordEmbeddingsModel&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'glove_840B_300'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'xx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerDLModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner_wikiner_glove_840B_300'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'xx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'embeddings'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;WordEmbeddingsModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;glove_840B_300&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_model&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;NerDLModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner_wikiner_glove_840B_300&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;',&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;div class=&quot;language-python nlu-block highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nlu&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;My name is John!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xx.ner.ner_wikiner_glove_840B_300'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_level&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;ner_wikiner_glove_840B_300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Type:&lt;/td&gt;
      &lt;td&gt;ner&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.1.3+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[sentence, token, embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;xx&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://figshare.com/articles/dataset/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500&quot;&gt;https://figshare.com/articles/dataset/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Average of all languages benchmark &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;multi-label classification and CoNLL Eval&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:

processed 1267027 tokens with 134558 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 132064 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 112519.
accuracy:  84.14%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  96.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  85.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  83.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.40
              LOC: precision:  84.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.12%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.48  59436
             MISC: precision:  78.88%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  67.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.66  19886
              ORG: precision:  81.37%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  70.97%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.82  13014
              PER: precision:  90.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  91.56%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  90.81  39728

Language by language benchmarks &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;multi-label classification and CoNLL Eval&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:

lang:  english
              precision    recall  f1-score   support

       B-LOC       0.85      0.90      0.87      8600
       I-ORG       0.78      0.79      0.78      4249
       I-LOC       0.83      0.79      0.81      3960
       I-PER       0.94      0.93      0.94      4472
       B-ORG       0.84      0.76      0.80      4882
       B-PER       0.93      0.94      0.93      9639

   micro avg       0.87      0.87      0.87     35802
   macro avg       0.86      0.85      0.85     35802
weighted avg       0.87      0.87      0.87     35802

processed 349486 tokens with 30471 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 29911 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 25093.
accuracy:  84.41%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  97.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  83.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.11
              LOC: precision:  83.01%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  87.38%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  85.14  9053
             MISC: precision:  75.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.38%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  71.62  6686
              ORG: precision:  80.78%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  72.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.59  4401
              PER: precision:  92.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.34%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.70  9771




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  french
              precision    recall  f1-score   support

       B-LOC       0.81      0.87      0.84     11482
       I-ORG       0.81      0.74      0.77      2143
       I-LOC       0.75      0.60      0.67      4495
       I-PER       0.95      0.94      0.95      5339
       B-ORG       0.86      0.78      0.82      2556
       B-PER       0.92      0.94      0.93      7524

   micro avg       0.86      0.85      0.85     33539
   macro avg       0.85      0.81      0.83     33539
weighted avg       0.86      0.85      0.85     33539

processed 348522 tokens with 25499 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 25298 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 21261.
accuracy:  80.86%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  97.44%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  84.04%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  83.38%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.71
              LOC: precision:  80.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  85.79%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.83  12303
             MISC: precision:  82.42%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  63.25%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  71.57  3021
              ORG: precision:  83.49%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  75.59%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  79.34  2314
              PER: precision:  91.24%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  92.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.06  7660




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  german
              precision    recall  f1-score   support

       B-LOC       0.85      0.87      0.86     20709
       I-ORG       0.82      0.82      0.82      5933
       I-LOC       0.77      0.79      0.78      6405
       I-PER       0.95      0.97      0.96      8365
       B-ORG       0.83      0.73      0.78      6759
       B-PER       0.92      0.93      0.92     10647

   micro avg       0.86      0.87      0.86     58818
   macro avg       0.86      0.85      0.85     58818
weighted avg       0.86      0.87      0.86     58818

processed 349393 tokens with 46006 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 44918 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 37486.
accuracy:  83.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  95.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  83.45%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  81.48%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.46
              LOC: precision:  82.80%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  85.49%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.13  21382
             MISC: precision:  77.46%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  66.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  71.58  6778
              ORG: precision:  79.88%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  70.72%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.02  5984
              PER: precision:  90.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  91.58%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  91.04  10774




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  italian
              precision    recall  f1-score   support

       B-LOC       0.88      0.92      0.90     13050
       I-ORG       0.78      0.71      0.74      1211
       I-LOC       0.89      0.85      0.87      7454
       I-PER       0.93      0.94      0.94      4539
       B-ORG       0.88      0.72      0.79      2222
       B-PER       0.90      0.93      0.92      7206

   micro avg       0.89      0.89      0.89     35682
   macro avg       0.88      0.85      0.86     35682
weighted avg       0.89      0.89      0.89     35682

processed 349242 tokens with 26227 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 25988 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 22529.
accuracy:  85.99%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  98.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  86.69%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  85.90%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.29
              LOC: precision:  86.33%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  90.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.38  13685
             MISC: precision:  81.88%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  67.03%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.72  3069
              ORG: precision:  85.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  70.52%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  77.46  1824
              PER: precision:  89.54%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  92.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  90.79  7410




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  polish
              precision    recall  f1-score   support

       B-LOC       0.86      0.91      0.88     17757
       I-ORG       0.80      0.69      0.74      2105
       I-LOC       0.83      0.78      0.80      5242
       I-PER       0.88      0.94      0.91      6672
       B-ORG       0.87      0.71      0.78      3700
       B-PER       0.88      0.89      0.88      9670

   micro avg       0.86      0.87      0.87     45146
   macro avg       0.85      0.82      0.83     45146
weighted avg       0.86      0.87      0.86     45146

processed 350132 tokens with 36235 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 35498 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 30071.
accuracy:  83.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  97.12%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  84.71%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.99%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.84
              LOC: precision:  84.80%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  89.59%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.13  18761
             MISC: precision:  78.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  60.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  68.10  3941
              ORG: precision:  86.15%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  70.27%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  77.40  3018
              PER: precision:  86.74%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  87.70%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.22  9778




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  portuguese
              precision    recall  f1-score   support

       B-LOC       0.91      0.94      0.92     14818
       I-ORG       0.84      0.74      0.79      1705
       I-LOC       0.89      0.88      0.89      8354
       I-PER       0.94      0.93      0.93      4338
       B-ORG       0.90      0.77      0.83      2351
       B-PER       0.92      0.93      0.93      6398

   micro avg       0.90      0.90      0.90     37964
   macro avg       0.90      0.87      0.88     37964
weighted avg       0.90      0.90      0.90     37964

processed 348966 tokens with 26513 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 26359 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 23574.
accuracy:  88.48%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  98.39%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  89.43%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  89.17
              LOC: precision:  89.52%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  92.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  91.04  15328
             MISC: precision:  84.55%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  72.47%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  78.05  2525
              ORG: precision:  88.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  75.84%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  81.70  2014
              PER: precision:  91.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  92.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.07  6492




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  russian
              precision    recall  f1-score   support

       B-LOC       0.91      0.93      0.92     14707
       I-ORG       0.78      0.64      0.70      2594
       I-LOC       0.86      0.79      0.82      5047
       I-PER       0.96      0.95      0.95      6366
       B-ORG       0.87      0.75      0.81      3697
       B-PER       0.88      0.91      0.90      7119

   micro avg       0.89      0.87      0.88     39530
   macro avg       0.88      0.83      0.85     39530
weighted avg       0.89      0.87      0.88     39530



&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  spanish
              precision    recall  f1-score   support

       B-LOC       0.86      0.90      0.88     11963
       I-ORG       0.82      0.78      0.80      1950
       I-LOC       0.84      0.81      0.83      6162
       I-PER       0.95      0.93      0.94      4678
       B-ORG       0.83      0.77      0.80      2084
       B-PER       0.93      0.94      0.94      7215

   micro avg       0.88      0.88      0.88     34052
   macro avg       0.87      0.86      0.86     34052
weighted avg       0.88      0.88      0.88     34052

processed 348209 tokens with 24505 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 24375 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 21187.
accuracy:  85.54%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  98.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  86.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.46%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.69
              LOC: precision:  85.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  89.28%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.25  12518
             MISC: precision:  82.54%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  67.78%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.43  2663
              ORG: precision:  81.85%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.39%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  79.03  1945
              PER: precision:  92.66%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.88  7249

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="xx" /><category term="multilingual" /><category term="glove" /><category term="ner" /><summary type="html">Description This NER model was trained over WIKINER datasets with 8 languages including English, French, German, Italian, Polish, Portuguese, Russian, and Spanish. We used WordEmbeddings (glove_840B_300) model for the embeddings to train this NER model. Predicted Entities B-LOC I-LOC B-ORG I-ORG B-PER I-PER Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') embeddings = WordEmbeddingsModel\ .pretrained('glove_840B_300', 'xx')\ .setInputCols([&quot;token&quot;, &quot;document&quot;])\ .setOutputCol(&quot;embeddings&quot;) ner_model = NerDLModel.pretrained('ner_wikiner_glove_840B_300', 'xx') \ .setInputCols(['document', 'token', 'embeddings']) \ .setOutputCol('ner') ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, embeddings, ner_model, ner_converter ]) example = spark.createDataFrame(pd.DataFrame({'text': ['My name is John!']})) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained(&quot;glove_840B_300&quot;, &quot;xx&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val ner_model = NerDLModel.pretrained(&quot;ner_wikiner_glove_840B_300&quot;, &quot;xx&quot;) .setInputCols(&quot;document&quot;', &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, embeddings, ner_model, ner_converter)) val result = pipeline.fit(Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;)).transform(data) import nlu text = [&quot;My name is John!&quot;] ner_df = nlu.load('xx.ner.ner_wikiner_glove_840B_300').predict(text, output_level='token') Model Information Model Name: ner_wikiner_glove_840B_300 Type: ner Compatibility: Spark NLP 3.1.3+ License: Open Source Edition: Official Input Labels: [sentence, token, embeddings] Output Labels: [ner] Language: xx Data Source https://figshare.com/articles/dataset/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500 Benchmarking Average of all languages benchmark (multi-label classification and CoNLL Eval): processed 1267027 tokens with 134558 phrases; found: 132064 phrases; correct: 112519. accuracy: 84.14%; (non-O) accuracy: 96.92%; precision: 85.20%; recall: 83.62%; FB1: 84.40 LOC: precision: 84.89%; recall: 88.12%; FB1: 86.48 59436 MISC: precision: 78.88%; recall: 67.35%; FB1: 72.66 19886 ORG: precision: 81.37%; recall: 70.97%; FB1: 75.82 13014 PER: precision: 90.08%; recall: 91.56%; FB1: 90.81 39728 Language by language benchmarks (multi-label classification and CoNLL Eval): lang: english precision recall f1-score support B-LOC 0.85 0.90 0.87 8600 I-ORG 0.78 0.79 0.78 4249 I-LOC 0.83 0.79 0.81 3960 I-PER 0.94 0.93 0.94 4472 B-ORG 0.84 0.76 0.80 4882 B-PER 0.93 0.94 0.93 9639 micro avg 0.87 0.87 0.87 35802 macro avg 0.86 0.85 0.85 35802 weighted avg 0.87 0.87 0.87 35802 processed 349486 tokens with 30471 phrases; found: 29911 phrases; correct: 25093. accuracy: 84.41%; (non-O) accuracy: 97.30%; precision: 83.89%; recall: 82.35%; FB1: 83.11 LOC: precision: 83.01%; recall: 87.38%; FB1: 85.14 9053 MISC: precision: 75.17%; recall: 68.38%; FB1: 71.62 6686 ORG: precision: 80.78%; recall: 72.82%; FB1: 76.59 4401 PER: precision: 92.08%; recall: 93.34%; FB1: 92.70 9771 ############################### lang: french precision recall f1-score support B-LOC 0.81 0.87 0.84 11482 I-ORG 0.81 0.74 0.77 2143 I-LOC 0.75 0.60 0.67 4495 I-PER 0.95 0.94 0.95 5339 B-ORG 0.86 0.78 0.82 2556 B-PER 0.92 0.94 0.93 7524 micro avg 0.86 0.85 0.85 33539 macro avg 0.85 0.81 0.83 33539 weighted avg 0.86 0.85 0.85 33539 processed 348522 tokens with 25499 phrases; found: 25298 phrases; correct: 21261. accuracy: 80.86%; (non-O) accuracy: 97.44%; precision: 84.04%; recall: 83.38%; FB1: 83.71 LOC: precision: 80.06%; recall: 85.79%; FB1: 82.83 12303 MISC: precision: 82.42%; recall: 63.25%; FB1: 71.57 3021 ORG: precision: 83.49%; recall: 75.59%; FB1: 79.34 2314 PER: precision: 91.24%; recall: 92.89%; FB1: 92.06 7660 ############################### lang: german precision recall f1-score support B-LOC 0.85 0.87 0.86 20709 I-ORG 0.82 0.82 0.82 5933 I-LOC 0.77 0.79 0.78 6405 I-PER 0.95 0.97 0.96 8365 B-ORG 0.83 0.73 0.78 6759 B-PER 0.92 0.93 0.92 10647 micro avg 0.86 0.87 0.86 58818 macro avg 0.86 0.85 0.85 58818 weighted avg 0.86 0.87 0.86 58818 processed 349393 tokens with 46006 phrases; found: 44918 phrases; correct: 37486. accuracy: 83.53%; (non-O) accuracy: 95.57%; precision: 83.45%; recall: 81.48%; FB1: 82.46 LOC: precision: 82.80%; recall: 85.49%; FB1: 84.13 21382 MISC: precision: 77.46%; recall: 66.53%; FB1: 71.58 6778 ORG: precision: 79.88%; recall: 70.72%; FB1: 75.02 5984 PER: precision: 90.50%; recall: 91.58%; FB1: 91.04 10774 ############################### lang: italian precision recall f1-score support B-LOC 0.88 0.92 0.90 13050 I-ORG 0.78 0.71 0.74 1211 I-LOC 0.89 0.85 0.87 7454 I-PER 0.93 0.94 0.94 4539 B-ORG 0.88 0.72 0.79 2222 B-PER 0.90 0.93 0.92 7206 micro avg 0.89 0.89 0.89 35682 macro avg 0.88 0.85 0.86 35682 weighted avg 0.89 0.89 0.89 35682 processed 349242 tokens with 26227 phrases; found: 25988 phrases; correct: 22529. accuracy: 85.99%; (non-O) accuracy: 98.06%; precision: 86.69%; recall: 85.90%; FB1: 86.29 LOC: precision: 86.33%; recall: 90.53%; FB1: 88.38 13685 MISC: precision: 81.88%; recall: 67.03%; FB1: 73.72 3069 ORG: precision: 85.91%; recall: 70.52%; FB1: 77.46 1824 PER: precision: 89.54%; recall: 92.08%; FB1: 90.79 7410 ############################### lang: polish precision recall f1-score support B-LOC 0.86 0.91 0.88 17757 I-ORG 0.80 0.69 0.74 2105 I-LOC 0.83 0.78 0.80 5242 I-PER 0.88 0.94 0.91 6672 B-ORG 0.87 0.71 0.78 3700 B-PER 0.88 0.89 0.88 9670 micro avg 0.86 0.87 0.87 45146 macro avg 0.85 0.82 0.83 45146 weighted avg 0.86 0.87 0.86 45146 processed 350132 tokens with 36235 phrases; found: 35498 phrases; correct: 30071. accuracy: 83.07%; (non-O) accuracy: 97.12%; precision: 84.71%; recall: 82.99%; FB1: 83.84 LOC: precision: 84.80%; recall: 89.59%; FB1: 87.13 18761 MISC: precision: 78.18%; recall: 60.32%; FB1: 68.10 3941 ORG: precision: 86.15%; recall: 70.27%; FB1: 77.40 3018 PER: precision: 86.74%; recall: 87.70%; FB1: 87.22 9778 ############################### lang: portuguese precision recall f1-score support B-LOC 0.91 0.94 0.92 14818 I-ORG 0.84 0.74 0.79 1705 I-LOC 0.89 0.88 0.89 8354 I-PER 0.94 0.93 0.93 4338 B-ORG 0.90 0.77 0.83 2351 B-PER 0.92 0.93 0.93 6398 micro avg 0.90 0.90 0.90 37964 macro avg 0.90 0.87 0.88 37964 weighted avg 0.90 0.90 0.90 37964 processed 348966 tokens with 26513 phrases; found: 26359 phrases; correct: 23574. accuracy: 88.48%; (non-O) accuracy: 98.39%; precision: 89.43%; recall: 88.91%; FB1: 89.17 LOC: precision: 89.52%; recall: 92.60%; FB1: 91.04 15328 MISC: precision: 84.55%; recall: 72.47%; FB1: 78.05 2525 ORG: precision: 88.53%; recall: 75.84%; FB1: 81.70 2014 PER: precision: 91.40%; recall: 92.75%; FB1: 92.07 6492 ############################### lang: russian precision recall f1-score support B-LOC 0.91 0.93 0.92 14707 I-ORG 0.78 0.64 0.70 2594 I-LOC 0.86 0.79 0.82 5047 I-PER 0.96 0.95 0.95 6366 B-ORG 0.87 0.75 0.81 3697 B-PER 0.88 0.91 0.90 7119 micro avg 0.89 0.87 0.88 39530 macro avg 0.88 0.83 0.85 39530 weighted avg 0.89 0.87 0.88 39530 ############################### lang: spanish precision recall f1-score support B-LOC 0.86 0.90 0.88 11963 I-ORG 0.82 0.78 0.80 1950 I-LOC 0.84 0.81 0.83 6162 I-PER 0.95 0.93 0.94 4678 B-ORG 0.83 0.77 0.80 2084 B-PER 0.93 0.94 0.94 7215 micro avg 0.88 0.88 0.88 34052 macro avg 0.87 0.86 0.86 34052 weighted avg 0.88 0.88 0.88 34052 processed 348209 tokens with 24505 phrases; found: 24375 phrases; correct: 21187. accuracy: 85.54%; (non-O) accuracy: 98.07%; precision: 86.92%; recall: 86.46%; FB1: 86.69 LOC: precision: 85.32%; recall: 89.28%; FB1: 87.25 12518 MISC: precision: 82.54%; recall: 67.78%; FB1: 74.43 2663 ORG: precision: 81.85%; recall: 76.39%; FB1: 79.03 1945 PER: precision: 92.66%; recall: 93.10%; FB1: 92.88 7249</summary></entry><entry><title type="html">Detect Entities in 8 languages - WIKINER (ner_wikiner_xlm_roberta_base)</title><link href="/2021/07/19/ner_wikiner_xlm_roberta_base_xx.html" rel="alternate" type="text/html" title="Detect Entities in 8 languages - WIKINER (ner_wikiner_xlm_roberta_base)" /><published>2021-07-19T00:00:00+00:00</published><updated>2021-07-19T00:00:00+00:00</updated><id>/2021/07/19/ner_wikiner_xlm_roberta_base_xx</id><content type="html" xml:base="/2021/07/19/ner_wikiner_xlm_roberta_base_xx.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;This NER model was trained over WIKINER datasets with 8 languages including &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;English&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;French&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;German&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Italian&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Polish&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Portuguese&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Russian&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Spanish&lt;/code&gt;.
We used XlmRoBertaEmbeddings (xlm_roberta_base) model for the embeddings to train this NER model.&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-LOC&lt;/code&gt; 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-LOC&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-ORG&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-ORG&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-PER&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-PER&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/ner_wikiner_xlm_roberta_base_xx_3.1.3_2.4_1626719349008.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xlm_roberta_base'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'xx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerDLModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner_wikiner_xlm_roberta_base'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'xx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'embeddings'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;XlmRoBertaEmbeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;xlm_roberta_base&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_model&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;NerDLModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner_wikiner_xlm_roberta_base&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;',&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;div class=&quot;language-python nlu-block highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nlu&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;My name is John!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xx.ner.ner_wikiner_xlm_roberta_base'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_level&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;ner_wikiner_xlm_roberta_base&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Type:&lt;/td&gt;
      &lt;td&gt;ner&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.1.3+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[sentence, token, embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;xx&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://figshare.com/articles/dataset/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500&quot;&gt;https://figshare.com/articles/dataset/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Average of all languages benchmark &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;multi-label classification and CoNLL Eval&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:

processed 1267026 tokens with 134558 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 132447 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 114590.
accuracy:  85.26%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  97.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  86.52%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  85.16%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  85.83
              LOC: precision:  87.26%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.94  58155
             MISC: precision:  80.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  70.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.82  20432
              ORG: precision:  80.02%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  75.09%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  77.48  14003
              PER: precision:  91.03%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  92.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  91.92  39857



Language by language benchmarks &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;multi-label classification and CoNLL Eval&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:


lang:  english
              precision    recall  f1-score   support

       B-LOC       0.84      0.91      0.87      8600
       I-ORG       0.82      0.81      0.82      4249
       I-LOC       0.84      0.82      0.83      3960
       I-PER       0.95      0.94      0.95      4472
       B-ORG       0.83      0.77      0.80      4882
       B-PER       0.93      0.94      0.94      9639

   micro avg       0.88      0.88      0.88     35802
   macro avg       0.87      0.87      0.87     35802
weighted avg       0.88      0.88      0.88     35802

processed 349485 tokens with 30471 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 30143 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 25648.
accuracy:  85.02%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  97.68%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  85.09%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.63
              LOC: precision:  82.69%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.64%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  85.56  9219
             MISC: precision:  80.26%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  71.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.81  6577
              ORG: precision:  81.13%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  75.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  78.43  4568
              PER: precision:  92.44%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.79%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  93.11  9779




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  french
              precision    recall  f1-score   support

       B-LOC       0.84      0.86      0.85     11482
       I-ORG       0.80      0.77      0.78      2143
       I-LOC       0.81      0.60      0.69      4495
       I-PER       0.97      0.94      0.95      5339
       B-ORG       0.84      0.81      0.82      2556
       B-PER       0.93      0.93      0.93      7524

   micro avg       0.87      0.85      0.86     33539
   macro avg       0.86      0.82      0.84     33539
weighted avg       0.87      0.85      0.86     33539

processed 348522 tokens with 25499 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 25270 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 21525.
accuracy:  82.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  97.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  85.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.42%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.80
              LOC: precision:  82.63%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  85.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.88  11836
             MISC: precision:  80.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  69.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.49  3422
              ORG: precision:  82.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.34%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  81.09  2446
              PER: precision:  92.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  92.72%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.46  7566




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  german
              precision    recall  f1-score   support

       B-LOC       0.88      0.90      0.89     20709
       I-ORG       0.82      0.87      0.85      5933
       I-LOC       0.81      0.82      0.82      6405
       I-PER       0.96      0.97      0.97      8365
       B-ORG       0.82      0.80      0.81      6759
       B-PER       0.93      0.94      0.94     10647

   micro avg       0.88      0.89      0.89     58818
   macro avg       0.87      0.88      0.88     58818
weighted avg       0.88      0.89      0.89     58818

processed 349393 tokens with 46006 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 45517 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 39247.
accuracy:  86.81%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  96.44%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  86.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  85.31%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  85.76
              LOC: precision:  86.58%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.33%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.45  21128
             MISC: precision:  81.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  72.67%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.79  7044
              ORG: precision:  80.11%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.76%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  78.92  6561
              PER: precision:  92.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.59%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.99  10784




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  italian
              precision    recall  f1-score   support

       B-LOC       0.91      0.92      0.91     13050
       I-ORG       0.75      0.81      0.78      1211
       I-LOC       0.92      0.86      0.89      7454
       I-PER       0.96      0.95      0.95      4539
       B-ORG       0.84      0.84      0.84      2222
       B-PER       0.93      0.94      0.93      7206

   micro avg       0.91      0.90      0.91     35682
   macro avg       0.88      0.88      0.88     35682
weighted avg       0.91      0.90      0.91     35682

processed 349242 tokens with 26227 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 25982 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 23079.
accuracy:  88.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  98.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  88.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.41
              LOC: precision:  89.87%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  90.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  90.05  13101
             MISC: precision:  81.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  74.05%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  77.64  3402
              ORG: precision:  83.26%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.36%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.81  2198
              PER: precision:  92.01%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  92.96%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.48  7281




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  polish
              precision    recall  f1-score   support

       B-LOC       0.91      0.93      0.92     17757
       I-ORG       0.79      0.85      0.82      2105
       I-LOC       0.89      0.85      0.87      5242
       I-PER       0.97      0.95      0.96      6672
       B-ORG       0.86      0.84      0.85      3700
       B-PER       0.93      0.94      0.94      9670

   micro avg       0.91      0.91      0.91     45146
   macro avg       0.89      0.89      0.89     45146
weighted avg       0.91      0.91      0.91     45146

processed 350132 tokens with 36235 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 35886 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 32107.
accuracy:  88.31%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  97.98%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  89.47%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.61%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  89.04
              LOC: precision:  90.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  92.52%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  91.33  18221
             MISC: precision:  82.48%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  70.71%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.15  4379
              ORG: precision:  85.37%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.92  3576
              PER: precision:  92.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.21%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  93.01  9710




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  portuguese
              precision    recall  f1-score   support

       B-LOC       0.93      0.93      0.93     14818
       I-ORG       0.80      0.85      0.83      1705
       I-LOC       0.92      0.88      0.90      8354
       I-PER       0.96      0.94      0.95      4338
       B-ORG       0.84      0.86      0.85      2351
       B-PER       0.94      0.94      0.94      6398

   micro avg       0.92      0.91      0.92     37964
   macro avg       0.90      0.90      0.90     37964
weighted avg       0.92      0.91      0.92     37964

processed 348966 tokens with 26513 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 26349 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 23958.
accuracy:  90.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  98.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  90.93%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  90.36%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  90.64
              LOC: precision:  92.13%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  91.97%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.05  14792
             MISC: precision:  84.09%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.46%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  81.71  2784
              ORG: precision:  83.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  85.28%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.39  2401
              PER: precision:  93.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  93.72  6372




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  russian
              precision    recall  f1-score   support

       B-LOC       0.93      0.95      0.94     14707
       I-ORG       0.84      0.73      0.78      2594
       I-LOC       0.86      0.87      0.87      5047
       I-PER       0.98      0.96      0.97      6366
       B-ORG       0.86      0.84      0.85      3697
       B-PER       0.94      0.95      0.94      7119

   micro avg       0.92      0.92      0.92     39530
   macro avg       0.90      0.88      0.89     39530
weighted avg       0.92      0.92      0.92     39530




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  spanish
              precision    recall  f1-score   support

       B-LOC       0.89      0.90      0.89     11963
       I-ORG       0.83      0.79      0.81      1950
       I-LOC       0.89      0.80      0.84      6162
       I-PER       0.97      0.94      0.95      4678
       B-ORG       0.84      0.80      0.82      2084
       B-PER       0.94      0.94      0.94      7215

   micro avg       0.90      0.88      0.89     34052
   macro avg       0.89      0.86      0.88     34052
weighted avg       0.90      0.88      0.89     34052

processed 348209 tokens with 24505 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 24360 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 21446.
accuracy:  86.63%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  98.24%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  88.04%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  87.52%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.78
              LOC: precision:  87.90%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.74%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.32  12078
             MISC: precision:  79.02%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  74.68%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.79  3065
              ORG: precision:  82.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  78.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  80.69  1991
              PER: precision:  93.61%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  93.68  7226
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="ner" /><category term="multilingual" /><category term="xlm_roberta" /><category term="xx" /><category term="wikiner" /><summary type="html">Description This NER model was trained over WIKINER datasets with 8 languages including English, French, German, Italian, Polish, Portuguese, Russian, and Spanish. We used XlmRoBertaEmbeddings (xlm_roberta_base) model for the embeddings to train this NER model. Predicted Entities B-LOC I-LOC B-ORG I-ORG B-PER I-PER Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') embeddings = XlmRoBertaEmbeddings\ .pretrained('xlm_roberta_base', 'xx')\ .setInputCols([&quot;token&quot;, &quot;document&quot;])\ .setOutputCol(&quot;embeddings&quot;) ner_model = NerDLModel.pretrained('ner_wikiner_xlm_roberta_base', 'xx') \ .setInputCols(['document', 'token', 'embeddings']) \ .setOutputCol('ner') ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, embeddings, ner_model, ner_converter ]) example = spark.createDataFrame(pd.DataFrame({'text': ['My name is John!']})) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = XlmRoBertaEmbeddings.pretrained(&quot;xlm_roberta_base&quot;, &quot;xx&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val ner_model = NerDLModel.pretrained(&quot;ner_wikiner_xlm_roberta_base&quot;, &quot;xx&quot;) .setInputCols(&quot;document&quot;', &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, embeddings, ner_model, ner_converter)) val result = pipeline.fit(Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;)).transform(data) import nlu text = [&quot;My name is John!&quot;] ner_df = nlu.load('xx.ner.ner_wikiner_xlm_roberta_base').predict(text, output_level='token') Model Information Model Name: ner_wikiner_xlm_roberta_base Type: ner Compatibility: Spark NLP 3.1.3+ License: Open Source Edition: Official Input Labels: [sentence, token, embeddings] Output Labels: [ner] Language: xx Data Source https://figshare.com/articles/dataset/Learning_multilingual_named_entity_recognition_from_Wikipedia/5462500 Benchmarking Average of all languages benchmark (multi-label classification and CoNLL Eval): processed 1267026 tokens with 134558 phrases; found: 132447 phrases; correct: 114590. accuracy: 85.26%; (non-O) accuracy: 97.23%; precision: 86.52%; recall: 85.16%; FB1: 85.83 LOC: precision: 87.26%; recall: 88.62%; FB1: 87.94 58155 MISC: precision: 80.06%; recall: 70.23%; FB1: 74.82 20432 ORG: precision: 80.02%; recall: 75.09%; FB1: 77.48 14003 PER: precision: 91.03%; recall: 92.83%; FB1: 91.92 39857 Language by language benchmarks (multi-label classification and CoNLL Eval): lang: english precision recall f1-score support B-LOC 0.84 0.91 0.87 8600 I-ORG 0.82 0.81 0.82 4249 I-LOC 0.84 0.82 0.83 3960 I-PER 0.95 0.94 0.95 4472 B-ORG 0.83 0.77 0.80 4882 B-PER 0.93 0.94 0.94 9639 micro avg 0.88 0.88 0.88 35802 macro avg 0.87 0.87 0.87 35802 weighted avg 0.88 0.88 0.88 35802 processed 349485 tokens with 30471 phrases; found: 30143 phrases; correct: 25648. accuracy: 85.02%; (non-O) accuracy: 97.68%; precision: 85.09%; recall: 84.17%; FB1: 84.63 LOC: precision: 82.69%; recall: 88.64%; FB1: 85.56 9219 MISC: precision: 80.26%; recall: 71.82%; FB1: 75.81 6577 ORG: precision: 81.13%; recall: 75.91%; FB1: 78.43 4568 PER: precision: 92.44%; recall: 93.79%; FB1: 93.11 9779 ############################### lang: french precision recall f1-score support B-LOC 0.84 0.86 0.85 11482 I-ORG 0.80 0.77 0.78 2143 I-LOC 0.81 0.60 0.69 4495 I-PER 0.97 0.94 0.95 5339 B-ORG 0.84 0.81 0.82 2556 B-PER 0.93 0.93 0.93 7524 micro avg 0.87 0.85 0.86 33539 macro avg 0.86 0.82 0.84 33539 weighted avg 0.87 0.85 0.86 33539 processed 348522 tokens with 25499 phrases; found: 25270 phrases; correct: 21525. accuracy: 82.17%; (non-O) accuracy: 97.62%; precision: 85.18%; recall: 84.42%; FB1: 84.80 LOC: precision: 82.63%; recall: 85.18%; FB1: 83.88 11836 MISC: precision: 80.10%; recall: 69.62%; FB1: 74.49 3422 ORG: precision: 82.91%; recall: 79.34%; FB1: 81.09 2446 PER: precision: 92.20%; recall: 92.72%; FB1: 92.46 7566 ############################### lang: german precision recall f1-score support B-LOC 0.88 0.90 0.89 20709 I-ORG 0.82 0.87 0.85 5933 I-LOC 0.81 0.82 0.82 6405 I-PER 0.96 0.97 0.97 8365 B-ORG 0.82 0.80 0.81 6759 B-PER 0.93 0.94 0.94 10647 micro avg 0.88 0.89 0.89 58818 macro avg 0.87 0.88 0.88 58818 weighted avg 0.88 0.89 0.89 58818 processed 349393 tokens with 46006 phrases; found: 45517 phrases; correct: 39247. accuracy: 86.81%; (non-O) accuracy: 96.44%; precision: 86.22%; recall: 85.31%; FB1: 85.76 LOC: precision: 86.58%; recall: 88.33%; FB1: 87.45 21128 MISC: precision: 81.40%; recall: 72.67%; FB1: 76.79 7044 ORG: precision: 80.11%; recall: 77.76%; FB1: 78.92 6561 PER: precision: 92.40%; recall: 93.59%; FB1: 92.99 10784 ############################### lang: italian precision recall f1-score support B-LOC 0.91 0.92 0.91 13050 I-ORG 0.75 0.81 0.78 1211 I-LOC 0.92 0.86 0.89 7454 I-PER 0.96 0.95 0.95 4539 B-ORG 0.84 0.84 0.84 2222 B-PER 0.93 0.94 0.93 7206 micro avg 0.91 0.90 0.91 35682 macro avg 0.88 0.88 0.88 35682 weighted avg 0.91 0.90 0.91 35682 processed 349242 tokens with 26227 phrases; found: 25982 phrases; correct: 23079. accuracy: 88.06%; (non-O) accuracy: 98.35%; precision: 88.83%; recall: 88.00%; FB1: 88.41 LOC: precision: 89.87%; recall: 90.22%; FB1: 90.05 13101 MISC: precision: 81.60%; recall: 74.05%; FB1: 77.64 3402 ORG: precision: 83.26%; recall: 82.36%; FB1: 82.81 2198 PER: precision: 92.01%; recall: 92.96%; FB1: 92.48 7281 ############################### lang: polish precision recall f1-score support B-LOC 0.91 0.93 0.92 17757 I-ORG 0.79 0.85 0.82 2105 I-LOC 0.89 0.85 0.87 5242 I-PER 0.97 0.95 0.96 6672 B-ORG 0.86 0.84 0.85 3700 B-PER 0.93 0.94 0.94 9670 micro avg 0.91 0.91 0.91 45146 macro avg 0.89 0.89 0.89 45146 weighted avg 0.91 0.91 0.91 45146 processed 350132 tokens with 36235 phrases; found: 35886 phrases; correct: 32107. accuracy: 88.31%; (non-O) accuracy: 97.98%; precision: 89.47%; recall: 88.61%; FB1: 89.04 LOC: precision: 90.17%; recall: 92.52%; FB1: 91.33 18221 MISC: precision: 82.48%; recall: 70.71%; FB1: 76.15 4379 ORG: precision: 85.37%; recall: 82.51%; FB1: 83.92 3576 PER: precision: 92.82%; recall: 93.21%; FB1: 93.01 9710 ############################### lang: portuguese precision recall f1-score support B-LOC 0.93 0.93 0.93 14818 I-ORG 0.80 0.85 0.83 1705 I-LOC 0.92 0.88 0.90 8354 I-PER 0.96 0.94 0.95 4338 B-ORG 0.84 0.86 0.85 2351 B-PER 0.94 0.94 0.94 6398 micro avg 0.92 0.91 0.92 37964 macro avg 0.90 0.90 0.90 37964 weighted avg 0.92 0.91 0.92 37964 processed 348966 tokens with 26513 phrases; found: 26349 phrases; correct: 23958. accuracy: 90.10%; (non-O) accuracy: 98.60%; precision: 90.93%; recall: 90.36%; FB1: 90.64 LOC: precision: 92.13%; recall: 91.97%; FB1: 92.05 14792 MISC: precision: 84.09%; recall: 79.46%; FB1: 81.71 2784 ORG: precision: 83.51%; recall: 85.28%; FB1: 84.39 2401 PER: precision: 93.91%; recall: 93.53%; FB1: 93.72 6372 ############################### lang: russian precision recall f1-score support B-LOC 0.93 0.95 0.94 14707 I-ORG 0.84 0.73 0.78 2594 I-LOC 0.86 0.87 0.87 5047 I-PER 0.98 0.96 0.97 6366 B-ORG 0.86 0.84 0.85 3697 B-PER 0.94 0.95 0.94 7119 micro avg 0.92 0.92 0.92 39530 macro avg 0.90 0.88 0.89 39530 weighted avg 0.92 0.92 0.92 39530 ############################### lang: spanish precision recall f1-score support B-LOC 0.89 0.90 0.89 11963 I-ORG 0.83 0.79 0.81 1950 I-LOC 0.89 0.80 0.84 6162 I-PER 0.97 0.94 0.95 4678 B-ORG 0.84 0.80 0.82 2084 B-PER 0.94 0.94 0.94 7215 micro avg 0.90 0.88 0.89 34052 macro avg 0.89 0.86 0.88 34052 weighted avg 0.90 0.88 0.89 34052 processed 348209 tokens with 24505 phrases; found: 24360 phrases; correct: 21446. accuracy: 86.63%; (non-O) accuracy: 98.24%; precision: 88.04%; recall: 87.52%; FB1: 87.78 LOC: precision: 87.90%; recall: 88.74%; FB1: 88.32 12078 MISC: precision: 79.02%; recall: 74.68%; FB1: 76.79 3065 ORG: precision: 82.57%; recall: 78.89%; FB1: 80.69 1991 PER: precision: 93.61%; recall: 93.75%; FB1: 93.68 7226</summary></entry><entry><title type="html">Detect Entities in 40 languages - XTREME (ner_xtreme_glove_840B_300)</title><link href="/2021/07/19/ner_xtreme_glove_840B_300_xx.html" rel="alternate" type="text/html" title="Detect Entities in 40 languages - XTREME (ner_xtreme_glove_840B_300)" /><published>2021-07-19T00:00:00+00:00</published><updated>2021-07-19T00:00:00+00:00</updated><id>/2021/07/19/ner_xtreme_glove_840B_300_xx</id><content type="html" xml:base="/2021/07/19/ner_xtreme_glove_840B_300_xx.html">&lt;h2 id=&quot;description&quot;&gt;Description&lt;/h2&gt;

&lt;p&gt;XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization. This NER model was trained over the XTREME dataset by using WordEmbeddings (glove_840B_300).&lt;/p&gt;

&lt;p&gt;This NER model covers a subset of the 40 languages included in XTREME (shown here with their ISO 639-1 code):&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;af&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ar&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bg&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bn&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;de&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;el&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;en&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;es&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;et&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;eu&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fa&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fi&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fr&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;he&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hi&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hu&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;id&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;it&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ja&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jv&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ka&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kk&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ko&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ml&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mr&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ms&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;my&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nl&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pt&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ru&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;sw&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ta&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;te&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;th&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tl&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tr&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ur&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vi&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;yo&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;zh&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;predicted-entities&quot;&gt;Predicted Entities&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-LOC&lt;/code&gt; 
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-LOC&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-ORG&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-ORG&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;B-PER&lt;/code&gt;
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;I-PER&lt;/code&gt;&lt;/p&gt;

&lt;p class=&quot;btn-box&quot;&gt;&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled=&quot;&quot;&gt;Open in Colab&lt;/button&gt;
&lt;a href=&quot;https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/ner_xtreme_glove_840B_300_xx_3.1.3_2.4_1626709939135.zip&quot; class=&quot;button button-orange button-orange-trans arr button-icon&quot;&gt;Download&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-to-use&quot;&gt;How to use&lt;/h2&gt;

&lt;div class=&quot;tabs-box&quot;&gt;
  &lt;div class=&quot;top_tab_li&quot;&gt;   
    &lt;button class=&quot;tab-li code-selector-active python-button&quot;&gt;Python&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active scala-button&quot;&gt;Scala&lt;/button&gt;&lt;button class=&quot;tab-li code-selector-un-active nlu-button&quot;&gt;NLU&lt;/button&gt;
&lt;/div&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;WordEmbeddingsModel&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'glove_840B_300'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'xx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;\
      &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerDLModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner_xtreme_glove_840B_300'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'xx'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'embeddings'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'document'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ner'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; \
    &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'entities'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spark&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'text'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'My name is John!'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]}))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;div class=&quot;language-scala highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;document_assembler&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;DocumentAssembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;embeddings&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;WordEmbeddingsModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;glove_840B_300&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_model&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;NerDLModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;pretrained&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner_xtreme_glove_840B_300&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;xx&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;',&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;embeddings&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ner_converter&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NerConverter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setInputCols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;document&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;token&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ner&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setOutputCol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;entities&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;setStages&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;document_assembler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ner_converter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;val&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;pipeline&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Seq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;empty&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;My&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;John!&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;toDF&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;py&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;div class=&quot;language-python nlu-block highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nlu&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;My name is John!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ner_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'xx.ner.ner_xtreme_glove_840B_300'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_level&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'token'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 class=&quot;model-param&quot; id=&quot;model-information&quot;&gt;Model Information&lt;/h2&gt;

&lt;table class=&quot;table-model&quot;&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Model Name:&lt;/td&gt;
      &lt;td&gt;ner_xtreme_glove_840B_300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Type:&lt;/td&gt;
      &lt;td&gt;ner&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Compatibility:&lt;/td&gt;
      &lt;td&gt;Spark NLP 3.1.3+&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;License:&lt;/td&gt;
      &lt;td&gt;Open Source&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Edition:&lt;/td&gt;
      &lt;td&gt;Official&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Input Labels:&lt;/td&gt;
      &lt;td&gt;[sentence, token, embeddings]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Output Labels:&lt;/td&gt;
      &lt;td&gt;[ner]&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Language:&lt;/td&gt;
      &lt;td&gt;xx&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;data-source&quot;&gt;Data Source&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/google-research/xtreme&quot;&gt;https://github.com/google-research/xtreme&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;benchmarking&quot;&gt;Benchmarking&lt;/h2&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Language by language benchmarks &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;multi-label classification and CoNLL Eval&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;:

&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  af
              precision    recall  f1-score   support

       B-LOC       0.83      0.84      0.83       562
       I-ORG       0.88      0.87      0.87       786
       I-LOC       0.70      0.60      0.65       198
       I-PER       0.90      0.91      0.91       504
       B-ORG       0.87      0.81      0.84       569
       B-PER       0.90      0.89      0.89       356

   micro avg       0.86      0.85      0.85      2975
   macro avg       0.84      0.82      0.83      2975
weighted avg       0.86      0.85      0.85      2975

processed 10808 tokens with 1487 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 1460 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 1230.
accuracy:  84.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  94.64%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  84.25%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.72%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.47
              LOC: precision:  81.15%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.74%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  81.94  573
              ORG: precision:  85.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.79%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.40  533
              PER: precision:  87.85%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  87.36%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.61  354




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  ar
              precision    recall  f1-score   support

       B-LOC       0.88      0.71      0.79      3780
       I-ORG       0.76      0.87      0.81     10045
       I-LOC       0.92      0.80      0.85      9073
       I-PER       0.81      0.87      0.84      7937
       B-ORG       0.76      0.76      0.76      3629
       B-PER       0.82      0.82      0.82      3850

   micro avg       0.82      0.82      0.82     38314
   macro avg       0.82      0.81      0.81     38314
weighted avg       0.83      0.82      0.82     38314

processed 64347 tokens with 11259 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 10564 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 8242.
accuracy:  82.24%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  87.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  78.02%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.53
              LOC: precision:  85.80%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  69.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.63  3050
              ORG: precision:  71.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  72.28%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.05  3652
              PER: precision:  77.73%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.97%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  77.85  3862




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  &lt;span class=&quot;nb&quot;&gt;bg
              &lt;/span&gt;precision    recall  f1-score   support

       B-LOC       0.89      0.91      0.90      6436
       I-ORG       0.82      0.87      0.85      7964
       I-LOC       0.85      0.78      0.82      3213
       I-PER       0.89      0.88      0.89      4982
       B-ORG       0.79      0.77      0.78      3670
       B-PER       0.91      0.86      0.88      3954

   micro avg       0.86      0.86      0.86     30219
   macro avg       0.86      0.85      0.85     30219
weighted avg       0.86      0.86      0.86     30219

processed 83463 tokens with 14060 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 13897 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 11836.
accuracy:  85.80%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  93.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  85.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.67
              LOC: precision:  88.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  90.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  89.14  6593
              ORG: precision:  75.27%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.41  3587
              PER: precision:  89.56%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.19%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.79  3717




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  bn
              precision    recall  f1-score   support

       B-LOC       0.94      0.82      0.88       393
       I-ORG       0.86      0.93      0.89      1031
       I-LOC       0.94      0.82      0.88       703
       I-PER       0.84      0.92      0.88       731
       B-ORG       0.87      0.90      0.88       349
       B-PER       0.87      0.91      0.89       347

   micro avg       0.88      0.89      0.88      3554
   macro avg       0.89      0.88      0.88      3554
weighted avg       0.88      0.89      0.88      3554

processed 4377 tokens with 1089 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 1071 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 932.
accuracy:  89.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  89.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  87.02%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  85.58%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.30
              LOC: precision:  92.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.18  345
              ORG: precision:  85.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.08  363
              PER: precision:  83.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  87.61%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  85.63  363




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  de
              precision    recall  f1-score   support

       B-LOC       0.78      0.77      0.78      4961
       I-ORG       0.77      0.76      0.76      6043
       I-LOC       0.77      0.58      0.66      2289
       I-PER       0.96      0.84      0.89      6792
       B-ORG       0.69      0.73      0.71      4157
       B-PER       0.96      0.83      0.89      4750

   micro avg       0.83      0.77      0.80     28992
   macro avg       0.82      0.75      0.78     28992
weighted avg       0.84      0.77      0.80     28992

processed 97646 tokens with 13868 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 13393 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 10307.
accuracy:  77.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  91.95%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  76.96%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  74.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.62
              LOC: precision:  75.34%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.90%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.61  4866
              ORG: precision:  64.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.15%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  66.13  4411
              PER: precision:  92.52%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  85.90  4116




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  el
              precision    recall  f1-score   support

       B-LOC       0.84      0.84      0.84      4476
       I-ORG       0.79      0.88      0.83      6685
       I-LOC       0.74      0.54      0.62      1919
       I-PER       0.90      0.87      0.88      5392
       B-ORG       0.78      0.81      0.79      3655
       B-PER       0.89      0.84      0.87      4032

   micro avg       0.83      0.83      0.83     26159
   macro avg       0.82      0.80      0.81     26159
weighted avg       0.83      0.83      0.83     26159

processed 90666 tokens with 12164 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 12083 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 9880.
accuracy:  82.97%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  94.09%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  81.77%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  81.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  81.49
              LOC: precision:  82.55%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.73%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.64  4486
              ORG: precision:  75.29%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.95%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.60  3784
              PER: precision:  87.28%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.52%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.83  3813




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  en
              precision    recall  f1-score   support

       B-LOC       0.80      0.77      0.78      4657
       I-ORG       0.77      0.68      0.72     11607
       I-LOC       0.87      0.62      0.72      6447
       I-PER       0.93      0.75      0.83      7480
       B-ORG       0.75      0.65      0.69      4745
       B-PER       0.94      0.82      0.87      4556

   micro avg       0.83      0.71      0.77     39492
   macro avg       0.84      0.71      0.77     39492
weighted avg       0.84      0.71      0.76     39492

processed 80326 tokens with 13958 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 12542 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 9604.
accuracy:  70.66%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  84.66%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  76.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.81%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.48
              LOC: precision:  72.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  69.47%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  70.97  4460
              ORG: precision:  67.03%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  58.02%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  62.20  4107
              PER: precision:  90.97%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.37%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.77  3975




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  es
              precision    recall  f1-score   support

       B-LOC       0.94      0.85      0.89      4725
       I-ORG       0.84      0.91      0.87     11371
       I-LOC       0.90      0.73      0.81      6601
       I-PER       0.95      0.86      0.91      7004
       B-ORG       0.80      0.89      0.84      3576
       B-PER       0.96      0.88      0.92      3959

   micro avg       0.89      0.86      0.87     37236
   macro avg       0.90      0.85      0.87     37236
weighted avg       0.89      0.86      0.87     37236

processed 64727 tokens with 12260 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 11855 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 10412.
accuracy:  85.65%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  91.26%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  87.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.93%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.35
              LOC: precision:  91.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.29%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.52  4263
              ORG: precision:  78.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.24%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  81.94  3951
              PER: precision:  94.48%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  90.53  3641




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  et
              precision    recall  f1-score   support

       B-LOC       0.82      0.82      0.82      5888
       I-ORG       0.85      0.76      0.80      5731
       I-LOC       0.71      0.73      0.72      2467
       I-PER       0.95      0.86      0.90      5471
       B-ORG       0.82      0.70      0.75      3875
       B-PER       0.96      0.85      0.90      4129

   micro avg       0.86      0.79      0.83     27561
   macro avg       0.85      0.79      0.82     27561
weighted avg       0.86      0.79      0.83     27561

processed 80485 tokens with 13892 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 12865 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 10397.
accuracy:  79.45%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  91.98%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  80.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  74.84%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  77.71
              LOC: precision:  75.94%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  75.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.84  5873
              ORG: precision:  75.55%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  64.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  69.78  3325
              PER: precision:  93.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.95%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.87  3667




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  eu
              precision    recall  f1-score   support

       B-LOC       0.84      0.88      0.86      5682
       I-ORG       0.86      0.75      0.80      5560
       I-LOC       0.75      0.78      0.77      2876
       I-PER       0.95      0.88      0.91      5449
       B-ORG       0.81      0.72      0.77      3669
       B-PER       0.94      0.83      0.88      4108

   micro avg       0.87      0.82      0.84     27344
   macro avg       0.86      0.81      0.83     27344
weighted avg       0.87      0.82      0.84     27344

processed 90661 tokens with 13459 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 12843 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 10619.
accuracy:  81.56%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  93.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  82.68%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  78.90%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  80.75
              LOC: precision:  80.69%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.72%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.66  5966
              ORG: precision:  76.70%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.13  3257
              PER: precision:  91.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  85.58  3620




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  fa
              precision    recall  f1-score   support

       B-LOC       0.91      0.81      0.86      3663
       I-ORG       0.89      0.92      0.91     13255
       I-LOC       0.92      0.85      0.88      8547
       I-PER       0.85      0.87      0.86      7900
       B-ORG       0.85      0.84      0.85      3535
       B-PER       0.84      0.84      0.84      3544

   micro avg       0.88      0.87      0.88     40444
   macro avg       0.88      0.86      0.87     40444
weighted avg       0.88      0.87      0.88     40444

processed 59491 tokens with 10742 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 10313 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 8793.
accuracy:  87.31%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  90.55%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  85.26%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  81.86%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.52
              LOC: precision:  89.56%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.88%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.44  3267
              ORG: precision:  83.85%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  83.14%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.49  3505
              PER: precision:  82.69%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.65  3541




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  &lt;span class=&quot;k&quot;&gt;fi
              &lt;/span&gt;precision    recall  f1-score   support

       B-LOC       0.82      0.84      0.83      5629
       I-ORG       0.84      0.79      0.81      5522
       I-LOC       0.56      0.56      0.56      1096
       I-PER       0.97      0.89      0.93      5437
       B-ORG       0.79      0.69      0.74      4180
       B-PER       0.97      0.88      0.92      4745

   micro avg       0.86      0.81      0.84     26609
   macro avg       0.83      0.78      0.80     26609
weighted avg       0.87      0.81      0.84     26609

processed 83660 tokens with 14554 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 13685 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 11218.
accuracy:  81.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  93.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  81.97%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  79.45
              LOC: precision:  77.73%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.13%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  78.42  5730
              ORG: precision:  72.27%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  63.47%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  67.58  3671
              PER: precision:  95.96%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.64%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  91.06  4284




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  fr
              precision    recall  f1-score   support

       B-LOC       0.90      0.79      0.84      4985
       I-ORG       0.82      0.85      0.83     10386
       I-LOC       0.89      0.72      0.79      5859
       I-PER       0.95      0.81      0.87      6528
       B-ORG       0.80      0.83      0.81      3885
       B-PER       0.96      0.85      0.90      4499

   micro avg       0.88      0.81      0.84     36142
   macro avg       0.89      0.81      0.84     36142
weighted avg       0.88      0.81      0.84     36142

processed 68754 tokens with 13369 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 12405 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 10621.
accuracy:  81.03%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  89.43%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  85.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.44%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.42
              LOC: precision:  86.39%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.05%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  80.89  4388
              ORG: precision:  75.96%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  78.74%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  77.33  4027
              PER: precision:  94.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  83.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.84  3990




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  he
              precision    recall  f1-score   support

       B-LOC       0.81      0.67      0.73      5160
       I-ORG       0.67      0.71      0.69      6907
       I-LOC       0.73      0.56      0.63      3133
       I-PER       0.76      0.83      0.79      6816
       B-ORG       0.69      0.59      0.64      4142
       B-PER       0.75      0.78      0.77      4396

   micro avg       0.73      0.71      0.72     30554
   macro avg       0.74      0.69      0.71     30554
weighted avg       0.74      0.71      0.72     30554

processed 85422 tokens with 13698 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 12333 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 8741.
accuracy:  70.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  87.43%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  70.87%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  63.81%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  67.16
              LOC: precision:  78.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  64.36%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  70.60  4248
              ORG: precision:  62.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  53.31%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  57.41  3550
              PER: precision:  70.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  71.93  4535




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  hi
              precision    recall  f1-score   support

       B-LOC       0.84      0.71      0.77       414
       I-ORG       0.79      0.84      0.81      1123
       I-LOC       0.80      0.55      0.65       398
       I-PER       0.74      0.83      0.78       598
       B-ORG       0.76      0.79      0.77       364
       B-PER       0.82      0.82      0.82       450

   micro avg       0.79      0.78      0.78      3347
   macro avg       0.79      0.76      0.77      3347
weighted avg       0.79      0.78      0.78      3347

processed 6005 tokens with 1228 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 1183 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 900.
accuracy:  77.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  85.15%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  76.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.29%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.66
              LOC: precision:  79.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  67.87%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.27  353
              ORG: precision:  72.11%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  75.27%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.66  380
              PER: precision:  76.67%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.67%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.67  450




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  hu
              precision    recall  f1-score   support

       B-LOC       0.84      0.86      0.85      5671
       I-ORG       0.81      0.82      0.81      5341
       I-LOC       0.78      0.73      0.75      2404
       I-PER       0.96      0.87      0.91      5501
       B-ORG       0.81      0.77      0.79      3982
       B-PER       0.96      0.86      0.91      4510

   micro avg       0.87      0.83      0.85     27409
   macro avg       0.86      0.82      0.84     27409
weighted avg       0.87      0.83      0.85     27409

processed 90302 tokens with 14163 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 13631 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 11348.
accuracy:  82.81%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  93.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  83.25%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.12%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  81.66
              LOC: precision:  80.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  83.09%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  81.98  5824
              ORG: precision:  75.76%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  71.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.74  3775
              PER: precision:  93.65%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  83.73%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.41  4032




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  &lt;span class=&quot;nb&quot;&gt;id
              &lt;/span&gt;precision    recall  f1-score   support

       B-LOC       0.92      0.91      0.92      3745
       I-ORG       0.87      0.90      0.89      8584
       I-LOC       0.95      0.94      0.94      7809
       I-PER       0.96      0.85      0.90      6520
       B-ORG       0.86      0.87      0.87      3733
       B-PER       0.96      0.86      0.91      3969

   micro avg       0.92      0.89      0.91     34360
   macro avg       0.92      0.89      0.90     34360
weighted avg       0.92      0.89      0.91     34360

processed 61834 tokens with 11447 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 11094 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 9903.
accuracy:  89.21%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  93.41%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  89.26%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.87
              LOC: precision:  90.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  89.88%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  90.02  3733
              ORG: precision:  83.39%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.14  3800
              PER: precision:  94.58%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.86%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  89.46  3561




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  it
              precision    recall  f1-score   support

       B-LOC       0.91      0.77      0.83      4820
       I-ORG       0.83      0.82      0.83      9222
       I-LOC       0.85      0.62      0.72      4366
       I-PER       0.96      0.87      0.92      5794
       B-ORG       0.80      0.81      0.81      4087
       B-PER       0.97      0.90      0.93      4842

   micro avg       0.88      0.81      0.84     33131
   macro avg       0.89      0.80      0.84     33131
weighted avg       0.88      0.81      0.84     33131

processed 80871 tokens with 13749 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 12679 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 10917.
accuracy:  80.69%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  91.42%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  86.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.62
              LOC: precision:  86.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.05%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  79.17  4075
              ORG: precision:  75.26%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.29%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.77  4143
              PER: precision:  95.90%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  88.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  91.97  4461




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  ja
              precision    recall  f1-score   support

       B-LOC       0.83      0.51      0.64      5094
       I-ORG       0.56      0.56      0.56     24814
       I-LOC       0.83      0.50      0.62     17278
       I-PER       0.84      0.50      0.63     21756
       B-ORG       0.55      0.54      0.55      4267
       B-PER       0.80      0.55      0.65      4085

   micro avg       0.69      0.52      0.60     77294
   macro avg       0.73      0.53      0.61     77294
weighted avg       0.73      0.52      0.60     77294

processed 306959 tokens with 13976 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 10196 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 6870.
accuracy:  52.43%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  86.02%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  67.38%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  49.16%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  56.84
              LOC: precision:  80.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  49.01%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  60.96  3134
              ORG: precision:  52.36%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  47.55%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  49.84  4236
              PER: precision:  75.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  51.14%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  60.89  2826




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  jv
              precision    recall  f1-score   support

       B-LOC       0.88      0.85      0.86        52
       I-ORG       0.71      0.76      0.74        66
       I-LOC       0.67      0.70      0.68        43
       I-PER       0.84      0.70      0.77        44
       B-ORG       0.74      0.78      0.76        40
       B-PER       0.90      0.72      0.80        25

   micro avg       0.77      0.76      0.76       270
   macro avg       0.79      0.75      0.77       270
weighted avg       0.78      0.76      0.77       270

processed 678 tokens with 117 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 112 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 90.
accuracy:  75.56%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  88.94%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  80.36%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  78.60
              LOC: precision:  84.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.77%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.35  50
              ORG: precision:  73.81%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.61  42
              PER: precision:  85.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.56  20




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  ka
              precision    recall  f1-score   support

       B-LOC       0.82      0.72      0.77      5288
       I-ORG       0.84      0.83      0.83      7800
       I-LOC       0.72      0.59      0.65      2191
       I-PER       0.80      0.88      0.84      4666
       B-ORG       0.79      0.66      0.72      3807
       B-PER       0.80      0.81      0.80      3962

   micro avg       0.81      0.77      0.79     27714
   macro avg       0.79      0.75      0.77     27714
weighted avg       0.81      0.77      0.79     27714

processed 81921 tokens with 13057 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 11833 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 9017.
accuracy:  77.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  90.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  76.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  69.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.45
              LOC: precision:  78.11%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.68%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.09  4650
              ORG: precision:  73.14%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  61.15%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  66.61  3183
              PER: precision:  76.42%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.16%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.79  4000




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  kk
              precision    recall  f1-score   support

       B-LOC       0.76      0.79      0.77       383
       I-ORG       0.63      0.80      0.70       592
       I-LOC       0.78      0.49      0.60       210
       I-PER       0.84      0.84      0.84       466
       B-ORG       0.63      0.61      0.62       355
       B-PER       0.82      0.74      0.78       377

   micro avg       0.72      0.74      0.73      2383
   macro avg       0.74      0.71      0.72      2383
weighted avg       0.73      0.74      0.73      2383

processed 7936 tokens with 1115 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 1081 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 744.
accuracy:  74.11%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  89.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  68.83%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  66.73%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  67.76
              LOC: precision:  74.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.55%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.06  398
              ORG: precision:  52.48%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  50.70%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  51.58  343
              PER: precision:  78.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  70.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.48  340




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  ko
              precision    recall  f1-score   support

       B-LOC       0.88      0.81      0.85      5855
       I-ORG       0.75      0.81      0.78      5437
       I-LOC       0.79      0.82      0.80      2712
       I-PER       0.74      0.84      0.79      3468
       B-ORG       0.81      0.66      0.72      4319
       B-PER       0.76      0.80      0.78      4249

   micro avg       0.79      0.79      0.79     26040
   macro avg       0.79      0.79      0.79     26040
weighted avg       0.79      0.79      0.79     26040

processed 80841 tokens with 14423 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 13369 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 10274.
accuracy:  78.94%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  90.59%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  76.85%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  71.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.93
              LOC: precision:  83.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  80.16  5427
              ORG: precision:  71.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  58.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  64.38  3509
              PER: precision:  72.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.45  4433




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  ml
              precision    recall  f1-score   support

       B-LOC       0.86      0.60      0.70       443
       I-ORG       0.80      0.88      0.83       774
       I-LOC       0.80      0.32      0.46       219
       I-PER       0.73      0.85      0.78       492
       B-ORG       0.74      0.71      0.72       354
       B-PER       0.72      0.80      0.76       407

   micro avg       0.77      0.75      0.76      2689
   macro avg       0.77      0.69      0.71      2689
weighted avg       0.78      0.75      0.75      2689

processed 6727 tokens with 1204 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 1101 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 810.
accuracy:  74.64%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  87.88%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  73.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  67.28%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  70.28
              LOC: precision:  83.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  57.56%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  68.00  307
              ORG: precision:  70.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  68.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  69.05  344
              PER: precision:  69.78%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.15%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.28  450




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  mr
              precision    recall  f1-score   support

       B-LOC       0.84      0.69      0.76       525
       I-ORG       0.78      0.91      0.84       852
       I-LOC       0.67      0.49      0.57       258
       I-PER       0.85      0.79      0.82       598
       B-ORG       0.78      0.74      0.76       364
       B-PER       0.82      0.79      0.80       375

   micro avg       0.80      0.78      0.79      2972
   macro avg       0.79      0.74      0.76      2972
weighted avg       0.80      0.78      0.78      2972

processed 7356 tokens with 1264 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 1142 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 900.
accuracy:  77.59%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  88.93%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  78.81%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  71.20%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.81
              LOC: precision:  81.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  67.43%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.83  434
              ORG: precision:  73.85%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  70.60%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.19  348
              PER: precision:  80.28%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  78.64  360




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  ms
              precision    recall  f1-score   support

       B-LOC       0.92      0.94      0.93       367
       I-ORG       0.86      0.89      0.87       913
       I-LOC       0.97      0.95      0.96       898
       I-PER       0.95      0.78      0.86       555
       B-ORG       0.82      0.85      0.84       375
       B-PER       0.95      0.84      0.89       373

   micro avg       0.91      0.88      0.90      3481
   macro avg       0.91      0.87      0.89      3481
weighted avg       0.91      0.88      0.90      3481

processed 5874 tokens with 1115 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 1087 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 952.
accuracy:  88.28%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  92.12%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  87.58%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  85.38%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.47
              LOC: precision:  91.94%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  93.19%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  92.56  372
              ORG: precision:  78.81%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  81.33%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  80.05  387
              PER: precision:  92.99%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  81.77%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.02  328




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  my
              precision    recall  f1-score   support

       B-LOC       0.55      0.38      0.45        56
       I-ORG       0.63      0.49      0.55        68
       I-LOC       0.50      0.75      0.60         4
       I-PER       0.30      0.67      0.41        46
       B-ORG       0.84      0.48      0.62        33
       B-PER       0.31      0.53      0.40        30

   micro avg       0.44      0.51      0.47       237
   macro avg       0.52      0.55      0.50       237
weighted avg       0.54      0.51      0.49       237

processed 756 tokens with 119 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 108 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 46.
accuracy:  50.63%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  74.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  42.59%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  38.66%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  40.53
              LOC: precision:  55.26%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  37.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  44.68  38
              ORG: precision:  68.42%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  39.39%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  50.00  19
              PER: precision:  23.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  40.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  29.63  51




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  &lt;span class=&quot;nb&quot;&gt;nl
              &lt;/span&gt;precision    recall  f1-score   support

       B-LOC       0.90      0.85      0.87      5133
       I-ORG       0.83      0.79      0.81      6693
       I-LOC       0.90      0.68      0.77      3662
       I-PER       0.96      0.86      0.91      6371
       B-ORG       0.82      0.80      0.81      3908
       B-PER       0.96      0.88      0.92      4684

   micro avg       0.89      0.82      0.85     30451
   macro avg       0.89      0.81      0.85     30451
weighted avg       0.89      0.82      0.85     30451

processed 85122 tokens with 13725 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 12947 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 11196.
accuracy:  81.69%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  92.93%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  86.48%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  81.57%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.95
              LOC: precision:  86.27%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  81.01%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.55  4820
              ORG: precision:  78.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  77.56  3843
              PER: precision:  94.12%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  89.92  4284




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  pt
              precision    recall  f1-score   support

       B-LOC       0.92      0.85      0.89      4779
       I-ORG       0.83      0.88      0.85     10542
       I-LOC       0.89      0.71      0.79      6467
       I-PER       0.96      0.81      0.88      7310
       B-ORG       0.81      0.85      0.83      3753
       B-PER       0.96      0.85      0.90      4291

   micro avg       0.88      0.83      0.85     37142
   macro avg       0.89      0.82      0.86     37142
weighted avg       0.89      0.83      0.85     37142

processed 63647 tokens with 12823 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 12187 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 10475.
accuracy:  82.53%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  89.27%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  85.95%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  81.69%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.77
              LOC: precision:  87.47%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  81.04%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.13  4428
              ORG: precision:  77.42%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  81.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  79.32  3942
              PER: precision:  93.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.73%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.57  3817




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  ru
              precision    recall  f1-score   support

       B-LOC       0.75      0.81      0.78      4560
       I-ORG       0.78      0.83      0.80      8008
       I-LOC       0.63      0.69      0.66      3060
       I-PER       0.93      0.83      0.88      7544
       B-ORG       0.78      0.72      0.75      4074
       B-PER       0.90      0.79      0.84      3543

   micro avg       0.80      0.79      0.80     30789
   macro avg       0.79      0.78      0.78     30789
weighted avg       0.81      0.79      0.80     30789

processed 71288 tokens with 12177 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 11798 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 9093.
accuracy:  79.34%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  89.47%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  77.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  74.67%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.85
              LOC: precision:  74.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.36%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.64  4884
              ORG: precision:  71.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  66.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  68.97  3770
              PER: precision:  88.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  78.15%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.82  3144




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  sw
              precision    recall  f1-score   support

       B-LOC       0.86      0.84      0.85       388
       I-ORG       0.79      0.85      0.82       763
       I-LOC       0.80      0.67      0.73       568
       I-PER       0.97      0.87      0.92       744
       B-ORG       0.84      0.88      0.86       374
       B-PER       0.97      0.88      0.92       432

   micro avg       0.87      0.83      0.85      3269
   macro avg       0.87      0.83      0.85      3269
weighted avg       0.87      0.83      0.85      3269

processed 5786 tokens with 1194 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 1161 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 1003.
accuracy:  82.90%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  89.63%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  86.39%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.00%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  85.18
              LOC: precision:  81.33%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  78.61%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  79.95  375
              ORG: precision:  81.98%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.36%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.11  394
              PER: precision:  95.66%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.81%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  91.02  392




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  ta
              precision    recall  f1-score   support

       B-LOC       0.81      0.71      0.75       436
       I-ORG       0.77      0.83      0.80       814
       I-LOC       0.75      0.51      0.61       239
       I-PER       0.84      0.91      0.87       615
       B-ORG       0.77      0.68      0.72       383
       B-PER       0.77      0.85      0.81       422

   micro avg       0.79      0.79      0.79      2909
   macro avg       0.78      0.75      0.76      2909
weighted avg       0.79      0.79      0.78      2909

processed 7234 tokens with 1241 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 1179 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 869.
accuracy:  78.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  88.98%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  73.71%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  70.02%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  71.82
              LOC: precision:  77.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  67.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  72.55  380
              ORG: precision:  69.44%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  61.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  65.00  337
              PER: precision:  73.38%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  80.33%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  76.70  462




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  te
              precision    recall  f1-score   support

       B-LOC       0.76      0.51      0.61       450
       I-ORG       0.70      0.74      0.72       633
       I-LOC       0.71      0.44      0.55       178
       I-PER       0.50      0.77      0.61       294
       B-ORG       0.61      0.57      0.59       340
       B-PER       0.55      0.67      0.61       381

   micro avg       0.63      0.64      0.63      2276
   macro avg       0.64      0.62      0.61      2276
weighted avg       0.65      0.64      0.63      2276

processed 8155 tokens with 1171 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 1083 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 627.
accuracy:  63.75%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  85.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  57.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  53.54%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  55.63
              LOC: precision:  73.91%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  49.11%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  59.01  299
              ORG: precision:  53.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  50.88%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  52.34  321
              PER: precision:  50.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  61.15%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  55.21  463




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  th
              precision    recall  f1-score   support

       B-LOC       0.85      0.55      0.67      6503
       I-ORG       0.63      0.67      0.65     56831
       I-LOC       0.84      0.55      0.66     47608
       I-PER       0.86      0.65      0.74     57522
       B-ORG       0.50      0.54      0.52      5151
       B-PER       0.48      0.60      0.53      5316

   micro avg       0.73      0.62      0.67    178931
   macro avg       0.69      0.59      0.63    178931
weighted avg       0.76      0.62      0.67    178931

processed 649606 tokens with 20897 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 16403 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 11059.
accuracy:  61.94%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  87.63%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  67.42%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  52.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  59.30
              LOC: precision:  80.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  51.38%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  62.62  4238
              ORG: precision:  48.03%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  44.13%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  46.00  5469
              PER: precision:  75.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  60.43%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  67.00  6696




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  tl
              precision    recall  f1-score   support

       B-LOC       0.83      0.90      0.86       327
       I-ORG       0.83      0.81      0.82      1045
       I-LOC       0.87      0.85      0.86       706
       I-PER       0.95      0.84      0.89       813
       B-ORG       0.79      0.82      0.81       341
       B-PER       0.94      0.86      0.90       366

   micro avg       0.87      0.84      0.85      3598
   macro avg       0.87      0.85      0.86      3598
weighted avg       0.87      0.84      0.85      3598

processed 4627 tokens with 1034 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 1040 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 857.
accuracy:  83.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  86.73%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  82.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.88%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.64
              LOC: precision:  79.26%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  85.32%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  82.18  352
              ORG: precision:  76.49%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  79.18%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  77.81  353
              PER: precision:  91.94%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  84.15%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.87  335




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  &lt;span class=&quot;nb&quot;&gt;tr
              &lt;/span&gt;precision    recall  f1-score   support

       B-LOC       0.87      0.79      0.83      4914
       I-ORG       0.78      0.89      0.83      6979
       I-LOC       0.83      0.68      0.75      3005
       I-PER       0.95      0.84      0.89      5694
       B-ORG       0.78      0.82      0.80      4154
       B-PER       0.95      0.84      0.89      4519

   micro avg       0.85      0.82      0.84     29265
   macro avg       0.86      0.81      0.83     29265
weighted avg       0.86      0.82      0.84     29265

processed 75731 tokens with 13587 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 12822 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 10708.
accuracy:  82.40%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  92.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  83.51%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  78.81%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  81.09
              LOC: precision:  84.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.82%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  80.33  4485
              ORG: precision:  73.45%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  77.25%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.30  4369
              PER: precision:  93.85%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.41%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  87.76  3968




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  ur
              precision    recall  f1-score   support

       B-LOC       0.92      0.87      0.90       334
       I-ORG       0.86      0.89      0.88      1005
       I-LOC       0.91      0.90      0.91       904
       I-PER       0.89      0.91      0.90       928
       B-ORG       0.86      0.86      0.86       323
       B-PER       0.89      0.89      0.89       363

   micro avg       0.89      0.89      0.89      3857
   macro avg       0.89      0.89      0.89      3857
weighted avg       0.89      0.89      0.89      3857

processed 5027 tokens with 1020 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 1003 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 878.
accuracy:  89.50%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  90.89%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  87.54%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.08%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.80
              LOC: precision:  90.54%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  85.93%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  88.17  317
              ORG: precision:  86.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.07  323
              PER: precision:  86.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  86.23%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.23  363




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  vi
              precision    recall  f1-score   support

       B-LOC       0.88      0.86      0.87      3717
       I-ORG       0.86      0.86      0.86     13562
       I-LOC       0.89      0.85      0.87      8018
       I-PER       0.93      0.81      0.87      7787
       B-ORG       0.82      0.82      0.82      3704
       B-PER       0.92      0.85      0.88      3884

   micro avg       0.88      0.84      0.86     40672
   macro avg       0.88      0.84      0.86     40672
weighted avg       0.88      0.84      0.86     40672

processed 64967 tokens with 11305 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 10904 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 9223.
accuracy:  84.22%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  89.37%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  84.58%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  81.58%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  83.06
              LOC: precision:  85.10%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  83.72%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  84.40  3657
              ORG: precision:  78.77%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  78.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  78.56  3684
              PER: precision:  90.06%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  82.62%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  86.18  3563




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  yo
              precision    recall  f1-score   support

       B-LOC       0.73      0.77      0.75        39
       I-ORG       0.76      0.79      0.78        87
       I-LOC       0.90      0.89      0.90        72
       I-PER       0.94      0.72      0.82        71
       B-ORG       0.68      0.79      0.73        29
       B-PER       0.97      0.70      0.81        43

   micro avg       0.83      0.78      0.81       341
   macro avg       0.83      0.78      0.80       341
weighted avg       0.84      0.78      0.81       341

processed 503 tokens with 111 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 106 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 81.
accuracy:  78.30%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  85.09%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  76.42%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  72.97%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  74.65
              LOC: precision:  73.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  76.92%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.00  41
              ORG: precision:  64.71%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  75.86%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  69.84  34
              PER: precision:  93.55%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  67.44%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  78.38  31




&lt;span class=&quot;c&quot;&gt;###############################&lt;/span&gt;
lang:  zh
              precision    recall  f1-score   support

       B-LOC       0.83      0.71      0.76      4371
       I-ORG       0.75      0.68      0.71     17399
       I-LOC       0.85      0.74      0.79     12282
       I-PER       0.87      0.77      0.82     12897
       B-ORG       0.67      0.65      0.66      3779
       B-PER       0.81      0.76      0.79      3899

   micro avg       0.80      0.72      0.76     54627
   macro avg       0.80      0.72      0.75     54627
weighted avg       0.80      0.72      0.76     54627

processed 207505 tokens with 12532 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; found: 11033 phrases&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; correct: 8345.
accuracy:  72.07%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;non-O&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
accuracy:  90.67%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; precision:  75.64%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  66.59%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  70.83
              LOC: precision:  80.35%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  66.99%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  73.06  3730
              ORG: precision:  67.63%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  59.85%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  63.50  3642
              PER: precision:  78.80%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; recall:  73.17%&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; FB1:  75.88  3661
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="xtreme" /><category term="ner" /><category term="multilingual" /><category term="xx" /><category term="glove" /><summary type="html">Description XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization. This NER model was trained over the XTREME dataset by using WordEmbeddings (glove_840B_300). This NER model covers a subset of the 40 languages included in XTREME (shown here with their ISO 639-1 code): af, ar, bg, bn, de, el, en, es, et, eu, fa, fi, fr, he, hi, hu, id, it, ja, jv, ka, kk, ko, ml, mr, ms, my, nl, pt, ru, sw, ta, te, th, tl, tr, ur, vi, yo, and zh Predicted Entities B-LOC I-LOC B-ORG I-ORG B-PER I-PER Live Demo Open in Colab Download How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') embeddings = WordEmbeddingsModel\ .pretrained('glove_840B_300', 'xx')\ .setInputCols([&quot;token&quot;, &quot;document&quot;])\ .setOutputCol(&quot;embeddings&quot;) ner_model = NerDLModel.pretrained('ner_xtreme_glove_840B_300', 'xx') \ .setInputCols(['document', 'token', 'embeddings']) \ .setOutputCol('ner') ner_converter = NerConverter() \ .setInputCols(['document', 'token', 'ner']) \ .setOutputCol('entities') pipeline = Pipeline(stages=[ document_assembler, tokenizer, embeddings, ner_model, ner_converter ]) example = spark.createDataFrame(pd.DataFrame({'text': ['My name is John!']})) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = WordEmbeddingsModel.pretrained(&quot;glove_840B_300&quot;, &quot;xx&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) val ner_model = NerDLModel.pretrained(&quot;ner_xtreme_glove_840B_300&quot;, &quot;xx&quot;) .setInputCols(&quot;document&quot;', &quot;token&quot;, &quot;embeddings&quot;) .setOutputCol(&quot;ner&quot;) val ner_converter = NerConverter() .setInputCols(&quot;document&quot;, &quot;token&quot;, &quot;ner&quot;) .setOutputCol(&quot;entities&quot;) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, embeddings, ner_model, ner_converter)) val result = pipeline.fit(Seq.empty[&quot;My name is John!&quot;].toDS.toDF(&quot;text&quot;)).transform(data) import nlu text = [&quot;My name is John!&quot;] ner_df = nlu.load('xx.ner.ner_xtreme_glove_840B_300').predict(text, output_level='token') Model Information Model Name: ner_xtreme_glove_840B_300 Type: ner Compatibility: Spark NLP 3.1.3+ License: Open Source Edition: Official Input Labels: [sentence, token, embeddings] Output Labels: [ner] Language: xx Data Source https://github.com/google-research/xtreme Benchmarking Language by language benchmarks (multi-label classification and CoNLL Eval): ############################### lang: af precision recall f1-score support B-LOC 0.83 0.84 0.83 562 I-ORG 0.88 0.87 0.87 786 I-LOC 0.70 0.60 0.65 198 I-PER 0.90 0.91 0.91 504 B-ORG 0.87 0.81 0.84 569 B-PER 0.90 0.89 0.89 356 micro avg 0.86 0.85 0.85 2975 macro avg 0.84 0.82 0.83 2975 weighted avg 0.86 0.85 0.85 2975 processed 10808 tokens with 1487 phrases; found: 1460 phrases; correct: 1230. accuracy: 84.50%; (non-O) accuracy: 94.64%; precision: 84.25%; recall: 82.72%; FB1: 83.47 LOC: precision: 81.15%; recall: 82.74%; FB1: 81.94 573 ORG: precision: 85.18%; recall: 79.79%; FB1: 82.40 533 PER: precision: 87.85%; recall: 87.36%; FB1: 87.61 354 ############################### lang: ar precision recall f1-score support B-LOC 0.88 0.71 0.79 3780 I-ORG 0.76 0.87 0.81 10045 I-LOC 0.92 0.80 0.85 9073 I-PER 0.81 0.87 0.84 7937 B-ORG 0.76 0.76 0.76 3629 B-PER 0.82 0.82 0.82 3850 micro avg 0.82 0.82 0.82 38314 macro avg 0.82 0.81 0.81 38314 weighted avg 0.83 0.82 0.82 38314 processed 64347 tokens with 11259 phrases; found: 10564 phrases; correct: 8242. accuracy: 82.24%; (non-O) accuracy: 87.23%; precision: 78.02%; recall: 73.20%; FB1: 75.53 LOC: precision: 85.80%; recall: 69.23%; FB1: 76.63 3050 ORG: precision: 71.82%; recall: 72.28%; FB1: 72.05 3652 PER: precision: 77.73%; recall: 77.97%; FB1: 77.85 3862 ############################### lang: bg precision recall f1-score support B-LOC 0.89 0.91 0.90 6436 I-ORG 0.82 0.87 0.85 7964 I-LOC 0.85 0.78 0.82 3213 I-PER 0.89 0.88 0.89 4982 B-ORG 0.79 0.77 0.78 3670 B-PER 0.91 0.86 0.88 3954 micro avg 0.86 0.86 0.86 30219 macro avg 0.86 0.85 0.85 30219 weighted avg 0.86 0.86 0.86 30219 processed 83463 tokens with 14060 phrases; found: 13897 phrases; correct: 11836. accuracy: 85.80%; (non-O) accuracy: 93.83%; precision: 85.17%; recall: 84.18%; FB1: 84.67 LOC: precision: 88.08%; recall: 90.23%; FB1: 89.14 6593 ORG: precision: 75.27%; recall: 73.57%; FB1: 74.41 3587 PER: precision: 89.56%; recall: 84.19%; FB1: 86.79 3717 ############################### lang: bn precision recall f1-score support B-LOC 0.94 0.82 0.88 393 I-ORG 0.86 0.93 0.89 1031 I-LOC 0.94 0.82 0.88 703 I-PER 0.84 0.92 0.88 731 B-ORG 0.87 0.90 0.88 349 B-PER 0.87 0.91 0.89 347 micro avg 0.88 0.89 0.88 3554 macro avg 0.89 0.88 0.88 3554 weighted avg 0.88 0.89 0.88 3554 processed 4377 tokens with 1089 phrases; found: 1071 phrases; correct: 932. accuracy: 89.00%; (non-O) accuracy: 89.10%; precision: 87.02%; recall: 85.58%; FB1: 86.30 LOC: precision: 92.17%; recall: 80.92%; FB1: 86.18 345 ORG: precision: 85.40%; recall: 88.83%; FB1: 87.08 363 PER: precision: 83.75%; recall: 87.61%; FB1: 85.63 363 ############################### lang: de precision recall f1-score support B-LOC 0.78 0.77 0.78 4961 I-ORG 0.77 0.76 0.76 6043 I-LOC 0.77 0.58 0.66 2289 I-PER 0.96 0.84 0.89 6792 B-ORG 0.69 0.73 0.71 4157 B-PER 0.96 0.83 0.89 4750 micro avg 0.83 0.77 0.80 28992 macro avg 0.82 0.75 0.78 28992 weighted avg 0.84 0.77 0.80 28992 processed 97646 tokens with 13868 phrases; found: 13393 phrases; correct: 10307. accuracy: 77.18%; (non-O) accuracy: 91.95%; precision: 76.96%; recall: 74.32%; FB1: 75.62 LOC: precision: 75.34%; recall: 73.90%; FB1: 74.61 4866 ORG: precision: 64.23%; recall: 68.15%; FB1: 66.13 4411 PER: precision: 92.52%; recall: 80.17%; FB1: 85.90 4116 ############################### lang: el precision recall f1-score support B-LOC 0.84 0.84 0.84 4476 I-ORG 0.79 0.88 0.83 6685 I-LOC 0.74 0.54 0.62 1919 I-PER 0.90 0.87 0.88 5392 B-ORG 0.78 0.81 0.79 3655 B-PER 0.89 0.84 0.87 4032 micro avg 0.83 0.83 0.83 26159 macro avg 0.82 0.80 0.81 26159 weighted avg 0.83 0.83 0.83 26159 processed 90666 tokens with 12164 phrases; found: 12083 phrases; correct: 9880. accuracy: 82.97%; (non-O) accuracy: 94.09%; precision: 81.77%; recall: 81.22%; FB1: 81.49 LOC: precision: 82.55%; recall: 82.73%; FB1: 82.64 4486 ORG: precision: 75.29%; recall: 77.95%; FB1: 76.60 3784 PER: precision: 87.28%; recall: 82.52%; FB1: 84.83 3813 ############################### lang: en precision recall f1-score support B-LOC 0.80 0.77 0.78 4657 I-ORG 0.77 0.68 0.72 11607 I-LOC 0.87 0.62 0.72 6447 I-PER 0.93 0.75 0.83 7480 B-ORG 0.75 0.65 0.69 4745 B-PER 0.94 0.82 0.87 4556 micro avg 0.83 0.71 0.77 39492 macro avg 0.84 0.71 0.77 39492 weighted avg 0.84 0.71 0.76 39492 processed 80326 tokens with 13958 phrases; found: 12542 phrases; correct: 9604. accuracy: 70.66%; (non-O) accuracy: 84.66%; precision: 76.57%; recall: 68.81%; FB1: 72.48 LOC: precision: 72.53%; recall: 69.47%; FB1: 70.97 4460 ORG: precision: 67.03%; recall: 58.02%; FB1: 62.20 4107 PER: precision: 90.97%; recall: 79.37%; FB1: 84.77 3975 ############################### lang: es precision recall f1-score support B-LOC 0.94 0.85 0.89 4725 I-ORG 0.84 0.91 0.87 11371 I-LOC 0.90 0.73 0.81 6601 I-PER 0.95 0.86 0.91 7004 B-ORG 0.80 0.89 0.84 3576 B-PER 0.96 0.88 0.92 3959 micro avg 0.89 0.86 0.87 37236 macro avg 0.90 0.85 0.87 37236 weighted avg 0.89 0.86 0.87 37236 processed 64727 tokens with 12260 phrases; found: 11855 phrases; correct: 10412. accuracy: 85.65%; (non-O) accuracy: 91.26%; precision: 87.83%; recall: 84.93%; FB1: 86.35 LOC: precision: 91.20%; recall: 82.29%; FB1: 86.52 4263 ORG: precision: 78.06%; recall: 86.24%; FB1: 81.94 3951 PER: precision: 94.48%; recall: 86.89%; FB1: 90.53 3641 ############################### lang: et precision recall f1-score support B-LOC 0.82 0.82 0.82 5888 I-ORG 0.85 0.76 0.80 5731 I-LOC 0.71 0.73 0.72 2467 I-PER 0.95 0.86 0.90 5471 B-ORG 0.82 0.70 0.75 3875 B-PER 0.96 0.85 0.90 4129 micro avg 0.86 0.79 0.83 27561 macro avg 0.85 0.79 0.82 27561 weighted avg 0.86 0.79 0.83 27561 processed 80485 tokens with 13892 phrases; found: 12865 phrases; correct: 10397. accuracy: 79.45%; (non-O) accuracy: 91.98%; precision: 80.82%; recall: 74.84%; FB1: 77.71 LOC: precision: 75.94%; recall: 75.75%; FB1: 75.84 5873 ORG: precision: 75.55%; recall: 64.83%; FB1: 69.78 3325 PER: precision: 93.40%; recall: 82.95%; FB1: 87.87 3667 ############################### lang: eu precision recall f1-score support B-LOC 0.84 0.88 0.86 5682 I-ORG 0.86 0.75 0.80 5560 I-LOC 0.75 0.78 0.77 2876 I-PER 0.95 0.88 0.91 5449 B-ORG 0.81 0.72 0.77 3669 B-PER 0.94 0.83 0.88 4108 micro avg 0.87 0.82 0.84 27344 macro avg 0.86 0.81 0.83 27344 weighted avg 0.87 0.82 0.84 27344 processed 90661 tokens with 13459 phrases; found: 12843 phrases; correct: 10619. accuracy: 81.56%; (non-O) accuracy: 93.91%; precision: 82.68%; recall: 78.90%; FB1: 80.75 LOC: precision: 80.69%; recall: 84.72%; FB1: 82.66 5966 ORG: precision: 76.70%; recall: 68.08%; FB1: 72.13 3257 PER: precision: 91.35%; recall: 80.50%; FB1: 85.58 3620 ############################### lang: fa precision recall f1-score support B-LOC 0.91 0.81 0.86 3663 I-ORG 0.89 0.92 0.91 13255 I-LOC 0.92 0.85 0.88 8547 I-PER 0.85 0.87 0.86 7900 B-ORG 0.85 0.84 0.85 3535 B-PER 0.84 0.84 0.84 3544 micro avg 0.88 0.87 0.88 40444 macro avg 0.88 0.86 0.87 40444 weighted avg 0.88 0.87 0.88 40444 processed 59491 tokens with 10742 phrases; found: 10313 phrases; correct: 8793. accuracy: 87.31%; (non-O) accuracy: 90.55%; precision: 85.26%; recall: 81.86%; FB1: 83.52 LOC: precision: 89.56%; recall: 79.88%; FB1: 84.44 3267 ORG: precision: 83.85%; recall: 83.14%; FB1: 83.49 3505 PER: precision: 82.69%; recall: 82.62%; FB1: 82.65 3541 ############################### lang: fi precision recall f1-score support B-LOC 0.82 0.84 0.83 5629 I-ORG 0.84 0.79 0.81 5522 I-LOC 0.56 0.56 0.56 1096 I-PER 0.97 0.89 0.93 5437 B-ORG 0.79 0.69 0.74 4180 B-PER 0.97 0.88 0.92 4745 micro avg 0.86 0.81 0.84 26609 macro avg 0.83 0.78 0.80 26609 weighted avg 0.87 0.81 0.84 26609 processed 83660 tokens with 14554 phrases; found: 13685 phrases; correct: 11218. accuracy: 81.22%; (non-O) accuracy: 93.00%; precision: 81.97%; recall: 77.08%; FB1: 79.45 LOC: precision: 77.73%; recall: 79.13%; FB1: 78.42 5730 ORG: precision: 72.27%; recall: 63.47%; FB1: 67.58 3671 PER: precision: 95.96%; recall: 86.64%; FB1: 91.06 4284 ############################### lang: fr precision recall f1-score support B-LOC 0.90 0.79 0.84 4985 I-ORG 0.82 0.85 0.83 10386 I-LOC 0.89 0.72 0.79 5859 I-PER 0.95 0.81 0.87 6528 B-ORG 0.80 0.83 0.81 3885 B-PER 0.96 0.85 0.90 4499 micro avg 0.88 0.81 0.84 36142 macro avg 0.89 0.81 0.84 36142 weighted avg 0.88 0.81 0.84 36142 processed 68754 tokens with 13369 phrases; found: 12405 phrases; correct: 10621. accuracy: 81.03%; (non-O) accuracy: 89.43%; precision: 85.62%; recall: 79.44%; FB1: 82.42 LOC: precision: 86.39%; recall: 76.05%; FB1: 80.89 4388 ORG: precision: 75.96%; recall: 78.74%; FB1: 77.33 4027 PER: precision: 94.51%; recall: 83.82%; FB1: 88.84 3990 ############################### lang: he precision recall f1-score support B-LOC 0.81 0.67 0.73 5160 I-ORG 0.67 0.71 0.69 6907 I-LOC 0.73 0.56 0.63 3133 I-PER 0.76 0.83 0.79 6816 B-ORG 0.69 0.59 0.64 4142 B-PER 0.75 0.78 0.77 4396 micro avg 0.73 0.71 0.72 30554 macro avg 0.74 0.69 0.71 30554 weighted avg 0.74 0.71 0.72 30554 processed 85422 tokens with 13698 phrases; found: 12333 phrases; correct: 8741. accuracy: 70.75%; (non-O) accuracy: 87.43%; precision: 70.87%; recall: 63.81%; FB1: 67.16 LOC: precision: 78.18%; recall: 64.36%; FB1: 70.60 4248 ORG: precision: 62.20%; recall: 53.31%; FB1: 57.41 3550 PER: precision: 70.83%; recall: 73.07%; FB1: 71.93 4535 ############################### lang: hi precision recall f1-score support B-LOC 0.84 0.71 0.77 414 I-ORG 0.79 0.84 0.81 1123 I-LOC 0.80 0.55 0.65 398 I-PER 0.74 0.83 0.78 598 B-ORG 0.76 0.79 0.77 364 B-PER 0.82 0.82 0.82 450 micro avg 0.79 0.78 0.78 3347 macro avg 0.79 0.76 0.77 3347 weighted avg 0.79 0.78 0.78 3347 processed 6005 tokens with 1228 phrases; found: 1183 phrases; correct: 900. accuracy: 77.92%; (non-O) accuracy: 85.15%; precision: 76.08%; recall: 73.29%; FB1: 74.66 LOC: precision: 79.60%; recall: 67.87%; FB1: 73.27 353 ORG: precision: 72.11%; recall: 75.27%; FB1: 73.66 380 PER: precision: 76.67%; recall: 76.67%; FB1: 76.67 450 ############################### lang: hu precision recall f1-score support B-LOC 0.84 0.86 0.85 5671 I-ORG 0.81 0.82 0.81 5341 I-LOC 0.78 0.73 0.75 2404 I-PER 0.96 0.87 0.91 5501 B-ORG 0.81 0.77 0.79 3982 B-PER 0.96 0.86 0.91 4510 micro avg 0.87 0.83 0.85 27409 macro avg 0.86 0.82 0.84 27409 weighted avg 0.87 0.83 0.85 27409 processed 90302 tokens with 14163 phrases; found: 13631 phrases; correct: 11348. accuracy: 82.81%; (non-O) accuracy: 93.83%; precision: 83.25%; recall: 80.12%; FB1: 81.66 LOC: precision: 80.91%; recall: 83.09%; FB1: 81.98 5824 ORG: precision: 75.76%; recall: 71.82%; FB1: 73.74 3775 PER: precision: 93.65%; recall: 83.73%; FB1: 88.41 4032 ############################### lang: id precision recall f1-score support B-LOC 0.92 0.91 0.92 3745 I-ORG 0.87 0.90 0.89 8584 I-LOC 0.95 0.94 0.94 7809 I-PER 0.96 0.85 0.90 6520 B-ORG 0.86 0.87 0.87 3733 B-PER 0.96 0.86 0.91 3969 micro avg 0.92 0.89 0.91 34360 macro avg 0.92 0.89 0.90 34360 weighted avg 0.92 0.89 0.91 34360 processed 61834 tokens with 11447 phrases; found: 11094 phrases; correct: 9903. accuracy: 89.21%; (non-O) accuracy: 93.41%; precision: 89.26%; recall: 86.51%; FB1: 87.87 LOC: precision: 90.17%; recall: 89.88%; FB1: 90.02 3733 ORG: precision: 83.39%; recall: 84.89%; FB1: 84.14 3800 PER: precision: 94.58%; recall: 84.86%; FB1: 89.46 3561 ############################### lang: it precision recall f1-score support B-LOC 0.91 0.77 0.83 4820 I-ORG 0.83 0.82 0.83 9222 I-LOC 0.85 0.62 0.72 4366 I-PER 0.96 0.87 0.92 5794 B-ORG 0.80 0.81 0.81 4087 B-PER 0.97 0.90 0.93 4842 micro avg 0.88 0.81 0.84 33131 macro avg 0.89 0.80 0.84 33131 weighted avg 0.88 0.81 0.84 33131 processed 80871 tokens with 13749 phrases; found: 12679 phrases; correct: 10917. accuracy: 80.69%; (non-O) accuracy: 91.42%; precision: 86.10%; recall: 79.40%; FB1: 82.62 LOC: precision: 86.40%; recall: 73.05%; FB1: 79.17 4075 ORG: precision: 75.26%; recall: 76.29%; FB1: 75.77 4143 PER: precision: 95.90%; recall: 88.35%; FB1: 91.97 4461 ############################### lang: ja precision recall f1-score support B-LOC 0.83 0.51 0.64 5094 I-ORG 0.56 0.56 0.56 24814 I-LOC 0.83 0.50 0.62 17278 I-PER 0.84 0.50 0.63 21756 B-ORG 0.55 0.54 0.55 4267 B-PER 0.80 0.55 0.65 4085 micro avg 0.69 0.52 0.60 77294 macro avg 0.73 0.53 0.61 77294 weighted avg 0.73 0.52 0.60 77294 processed 306959 tokens with 13976 phrases; found: 10196 phrases; correct: 6870. accuracy: 52.43%; (non-O) accuracy: 86.02%; precision: 67.38%; recall: 49.16%; FB1: 56.84 LOC: precision: 80.60%; recall: 49.01%; FB1: 60.96 3134 ORG: precision: 52.36%; recall: 47.55%; FB1: 49.84 4236 PER: precision: 75.23%; recall: 51.14%; FB1: 60.89 2826 ############################### lang: jv precision recall f1-score support B-LOC 0.88 0.85 0.86 52 I-ORG 0.71 0.76 0.74 66 I-LOC 0.67 0.70 0.68 43 I-PER 0.84 0.70 0.77 44 B-ORG 0.74 0.78 0.76 40 B-PER 0.90 0.72 0.80 25 micro avg 0.77 0.76 0.76 270 macro avg 0.79 0.75 0.77 270 weighted avg 0.78 0.76 0.77 270 processed 678 tokens with 117 phrases; found: 112 phrases; correct: 90. accuracy: 75.56%; (non-O) accuracy: 88.94%; precision: 80.36%; recall: 76.92%; FB1: 78.60 LOC: precision: 84.00%; recall: 80.77%; FB1: 82.35 50 ORG: precision: 73.81%; recall: 77.50%; FB1: 75.61 42 PER: precision: 85.00%; recall: 68.00%; FB1: 75.56 20 ############################### lang: ka precision recall f1-score support B-LOC 0.82 0.72 0.77 5288 I-ORG 0.84 0.83 0.83 7800 I-LOC 0.72 0.59 0.65 2191 I-PER 0.80 0.88 0.84 4666 B-ORG 0.79 0.66 0.72 3807 B-PER 0.80 0.81 0.80 3962 micro avg 0.81 0.77 0.79 27714 macro avg 0.79 0.75 0.77 27714 weighted avg 0.81 0.77 0.79 27714 processed 81921 tokens with 13057 phrases; found: 11833 phrases; correct: 9017. accuracy: 77.30%; (non-O) accuracy: 90.06%; precision: 76.20%; recall: 69.06%; FB1: 72.45 LOC: precision: 78.11%; recall: 68.68%; FB1: 73.09 4650 ORG: precision: 73.14%; recall: 61.15%; FB1: 66.61 3183 PER: precision: 76.42%; recall: 77.16%; FB1: 76.79 4000 ############################### lang: kk precision recall f1-score support B-LOC 0.76 0.79 0.77 383 I-ORG 0.63 0.80 0.70 592 I-LOC 0.78 0.49 0.60 210 I-PER 0.84 0.84 0.84 466 B-ORG 0.63 0.61 0.62 355 B-PER 0.82 0.74 0.78 377 micro avg 0.72 0.74 0.73 2383 macro avg 0.74 0.71 0.72 2383 weighted avg 0.73 0.74 0.73 2383 processed 7936 tokens with 1115 phrases; found: 1081 phrases; correct: 744. accuracy: 74.11%; (non-O) accuracy: 89.57%; precision: 68.83%; recall: 66.73%; FB1: 67.76 LOC: precision: 74.62%; recall: 77.55%; FB1: 76.06 398 ORG: precision: 52.48%; recall: 50.70%; FB1: 51.58 343 PER: precision: 78.53%; recall: 70.82%; FB1: 74.48 340 ############################### lang: ko precision recall f1-score support B-LOC 0.88 0.81 0.85 5855 I-ORG 0.75 0.81 0.78 5437 I-LOC 0.79 0.82 0.80 2712 I-PER 0.74 0.84 0.79 3468 B-ORG 0.81 0.66 0.72 4319 B-PER 0.76 0.80 0.78 4249 micro avg 0.79 0.79 0.79 26040 macro avg 0.79 0.79 0.79 26040 weighted avg 0.79 0.79 0.79 26040 processed 80841 tokens with 14423 phrases; found: 13369 phrases; correct: 10274. accuracy: 78.94%; (non-O) accuracy: 90.59%; precision: 76.85%; recall: 71.23%; FB1: 73.93 LOC: precision: 83.32%; recall: 77.23%; FB1: 80.16 5427 ORG: precision: 71.82%; recall: 58.35%; FB1: 64.38 3509 PER: precision: 72.91%; recall: 76.06%; FB1: 74.45 4433 ############################### lang: ml precision recall f1-score support B-LOC 0.86 0.60 0.70 443 I-ORG 0.80 0.88 0.83 774 I-LOC 0.80 0.32 0.46 219 I-PER 0.73 0.85 0.78 492 B-ORG 0.74 0.71 0.72 354 B-PER 0.72 0.80 0.76 407 micro avg 0.77 0.75 0.76 2689 macro avg 0.77 0.69 0.71 2689 weighted avg 0.78 0.75 0.75 2689 processed 6727 tokens with 1204 phrases; found: 1101 phrases; correct: 810. accuracy: 74.64%; (non-O) accuracy: 87.88%; precision: 73.57%; recall: 67.28%; FB1: 70.28 LOC: precision: 83.06%; recall: 57.56%; FB1: 68.00 307 ORG: precision: 70.06%; recall: 68.08%; FB1: 69.05 344 PER: precision: 69.78%; recall: 77.15%; FB1: 73.28 450 ############################### lang: mr precision recall f1-score support B-LOC 0.84 0.69 0.76 525 I-ORG 0.78 0.91 0.84 852 I-LOC 0.67 0.49 0.57 258 I-PER 0.85 0.79 0.82 598 B-ORG 0.78 0.74 0.76 364 B-PER 0.82 0.79 0.80 375 micro avg 0.80 0.78 0.79 2972 macro avg 0.79 0.74 0.76 2972 weighted avg 0.80 0.78 0.78 2972 processed 7356 tokens with 1264 phrases; found: 1142 phrases; correct: 900. accuracy: 77.59%; (non-O) accuracy: 88.93%; precision: 78.81%; recall: 71.20%; FB1: 74.81 LOC: precision: 81.57%; recall: 67.43%; FB1: 73.83 434 ORG: precision: 73.85%; recall: 70.60%; FB1: 72.19 348 PER: precision: 80.28%; recall: 77.07%; FB1: 78.64 360 ############################### lang: ms precision recall f1-score support B-LOC 0.92 0.94 0.93 367 I-ORG 0.86 0.89 0.87 913 I-LOC 0.97 0.95 0.96 898 I-PER 0.95 0.78 0.86 555 B-ORG 0.82 0.85 0.84 375 B-PER 0.95 0.84 0.89 373 micro avg 0.91 0.88 0.90 3481 macro avg 0.91 0.87 0.89 3481 weighted avg 0.91 0.88 0.90 3481 processed 5874 tokens with 1115 phrases; found: 1087 phrases; correct: 952. accuracy: 88.28%; (non-O) accuracy: 92.12%; precision: 87.58%; recall: 85.38%; FB1: 86.47 LOC: precision: 91.94%; recall: 93.19%; FB1: 92.56 372 ORG: precision: 78.81%; recall: 81.33%; FB1: 80.05 387 PER: precision: 92.99%; recall: 81.77%; FB1: 87.02 328 ############################### lang: my precision recall f1-score support B-LOC 0.55 0.38 0.45 56 I-ORG 0.63 0.49 0.55 68 I-LOC 0.50 0.75 0.60 4 I-PER 0.30 0.67 0.41 46 B-ORG 0.84 0.48 0.62 33 B-PER 0.31 0.53 0.40 30 micro avg 0.44 0.51 0.47 237 macro avg 0.52 0.55 0.50 237 weighted avg 0.54 0.51 0.49 237 processed 756 tokens with 119 phrases; found: 108 phrases; correct: 46. accuracy: 50.63%; (non-O) accuracy: 74.07%; precision: 42.59%; recall: 38.66%; FB1: 40.53 LOC: precision: 55.26%; recall: 37.50%; FB1: 44.68 38 ORG: precision: 68.42%; recall: 39.39%; FB1: 50.00 19 PER: precision: 23.53%; recall: 40.00%; FB1: 29.63 51 ############################### lang: nl precision recall f1-score support B-LOC 0.90 0.85 0.87 5133 I-ORG 0.83 0.79 0.81 6693 I-LOC 0.90 0.68 0.77 3662 I-PER 0.96 0.86 0.91 6371 B-ORG 0.82 0.80 0.81 3908 B-PER 0.96 0.88 0.92 4684 micro avg 0.89 0.82 0.85 30451 macro avg 0.89 0.81 0.85 30451 weighted avg 0.89 0.82 0.85 30451 processed 85122 tokens with 13725 phrases; found: 12947 phrases; correct: 11196. accuracy: 81.69%; (non-O) accuracy: 92.93%; precision: 86.48%; recall: 81.57%; FB1: 83.95 LOC: precision: 86.27%; recall: 81.01%; FB1: 83.55 4820 ORG: precision: 78.22%; recall: 76.92%; FB1: 77.56 3843 PER: precision: 94.12%; recall: 86.08%; FB1: 89.92 4284 ############################### lang: pt precision recall f1-score support B-LOC 0.92 0.85 0.89 4779 I-ORG 0.83 0.88 0.85 10542 I-LOC 0.89 0.71 0.79 6467 I-PER 0.96 0.81 0.88 7310 B-ORG 0.81 0.85 0.83 3753 B-PER 0.96 0.85 0.90 4291 micro avg 0.88 0.83 0.85 37142 macro avg 0.89 0.82 0.86 37142 weighted avg 0.89 0.83 0.85 37142 processed 63647 tokens with 12823 phrases; found: 12187 phrases; correct: 10475. accuracy: 82.53%; (non-O) accuracy: 89.27%; precision: 85.95%; recall: 81.69%; FB1: 83.77 LOC: precision: 87.47%; recall: 81.04%; FB1: 84.13 4428 ORG: precision: 77.42%; recall: 81.32%; FB1: 79.32 3942 PER: precision: 93.00%; recall: 82.73%; FB1: 87.57 3817 ############################### lang: ru precision recall f1-score support B-LOC 0.75 0.81 0.78 4560 I-ORG 0.78 0.83 0.80 8008 I-LOC 0.63 0.69 0.66 3060 I-PER 0.93 0.83 0.88 7544 B-ORG 0.78 0.72 0.75 4074 B-PER 0.90 0.79 0.84 3543 micro avg 0.80 0.79 0.80 30789 macro avg 0.79 0.78 0.78 30789 weighted avg 0.81 0.79 0.80 30789 processed 71288 tokens with 12177 phrases; found: 11798 phrases; correct: 9093. accuracy: 79.34%; (non-O) accuracy: 89.47%; precision: 77.07%; recall: 74.67%; FB1: 75.85 LOC: precision: 74.10%; recall: 79.36%; FB1: 76.64 4884 ORG: precision: 71.75%; recall: 66.40%; FB1: 68.97 3770 PER: precision: 88.07%; recall: 78.15%; FB1: 82.82 3144 ############################### lang: sw precision recall f1-score support B-LOC 0.86 0.84 0.85 388 I-ORG 0.79 0.85 0.82 763 I-LOC 0.80 0.67 0.73 568 I-PER 0.97 0.87 0.92 744 B-ORG 0.84 0.88 0.86 374 B-PER 0.97 0.88 0.92 432 micro avg 0.87 0.83 0.85 3269 macro avg 0.87 0.83 0.85 3269 weighted avg 0.87 0.83 0.85 3269 processed 5786 tokens with 1194 phrases; found: 1161 phrases; correct: 1003. accuracy: 82.90%; (non-O) accuracy: 89.63%; precision: 86.39%; recall: 84.00%; FB1: 85.18 LOC: precision: 81.33%; recall: 78.61%; FB1: 79.95 375 ORG: precision: 81.98%; recall: 86.36%; FB1: 84.11 394 PER: precision: 95.66%; recall: 86.81%; FB1: 91.02 392 ############################### lang: ta precision recall f1-score support B-LOC 0.81 0.71 0.75 436 I-ORG 0.77 0.83 0.80 814 I-LOC 0.75 0.51 0.61 239 I-PER 0.84 0.91 0.87 615 B-ORG 0.77 0.68 0.72 383 B-PER 0.77 0.85 0.81 422 micro avg 0.79 0.79 0.79 2909 macro avg 0.78 0.75 0.76 2909 weighted avg 0.79 0.79 0.78 2909 processed 7234 tokens with 1241 phrases; found: 1179 phrases; correct: 869. accuracy: 78.51%; (non-O) accuracy: 88.98%; precision: 73.71%; recall: 70.02%; FB1: 71.82 LOC: precision: 77.89%; recall: 67.89%; FB1: 72.55 380 ORG: precision: 69.44%; recall: 61.10%; FB1: 65.00 337 PER: precision: 73.38%; recall: 80.33%; FB1: 76.70 462 ############################### lang: te precision recall f1-score support B-LOC 0.76 0.51 0.61 450 I-ORG 0.70 0.74 0.72 633 I-LOC 0.71 0.44 0.55 178 I-PER 0.50 0.77 0.61 294 B-ORG 0.61 0.57 0.59 340 B-PER 0.55 0.67 0.61 381 micro avg 0.63 0.64 0.63 2276 macro avg 0.64 0.62 0.61 2276 weighted avg 0.65 0.64 0.63 2276 processed 8155 tokens with 1171 phrases; found: 1083 phrases; correct: 627. accuracy: 63.75%; (non-O) accuracy: 85.62%; precision: 57.89%; recall: 53.54%; FB1: 55.63 LOC: precision: 73.91%; recall: 49.11%; FB1: 59.01 299 ORG: precision: 53.89%; recall: 50.88%; FB1: 52.34 321 PER: precision: 50.32%; recall: 61.15%; FB1: 55.21 463 ############################### lang: th precision recall f1-score support B-LOC 0.85 0.55 0.67 6503 I-ORG 0.63 0.67 0.65 56831 I-LOC 0.84 0.55 0.66 47608 I-PER 0.86 0.65 0.74 57522 B-ORG 0.50 0.54 0.52 5151 B-PER 0.48 0.60 0.53 5316 micro avg 0.73 0.62 0.67 178931 macro avg 0.69 0.59 0.63 178931 weighted avg 0.76 0.62 0.67 178931 processed 649606 tokens with 20897 phrases; found: 16403 phrases; correct: 11059. accuracy: 61.94%; (non-O) accuracy: 87.63%; precision: 67.42%; recall: 52.92%; FB1: 59.30 LOC: precision: 80.18%; recall: 51.38%; FB1: 62.62 4238 ORG: precision: 48.03%; recall: 44.13%; FB1: 46.00 5469 PER: precision: 75.18%; recall: 60.43%; FB1: 67.00 6696 ############################### lang: tl precision recall f1-score support B-LOC 0.83 0.90 0.86 327 I-ORG 0.83 0.81 0.82 1045 I-LOC 0.87 0.85 0.86 706 I-PER 0.95 0.84 0.89 813 B-ORG 0.79 0.82 0.81 341 B-PER 0.94 0.86 0.90 366 micro avg 0.87 0.84 0.85 3598 macro avg 0.87 0.85 0.86 3598 weighted avg 0.87 0.84 0.85 3598 processed 4627 tokens with 1034 phrases; found: 1040 phrases; correct: 857. accuracy: 83.82%; (non-O) accuracy: 86.73%; precision: 82.40%; recall: 82.88%; FB1: 82.64 LOC: precision: 79.26%; recall: 85.32%; FB1: 82.18 352 ORG: precision: 76.49%; recall: 79.18%; FB1: 77.81 353 PER: precision: 91.94%; recall: 84.15%; FB1: 87.87 335 ############################### lang: tr precision recall f1-score support B-LOC 0.87 0.79 0.83 4914 I-ORG 0.78 0.89 0.83 6979 I-LOC 0.83 0.68 0.75 3005 I-PER 0.95 0.84 0.89 5694 B-ORG 0.78 0.82 0.80 4154 B-PER 0.95 0.84 0.89 4519 micro avg 0.85 0.82 0.84 29265 macro avg 0.86 0.81 0.83 29265 weighted avg 0.86 0.82 0.84 29265 processed 75731 tokens with 13587 phrases; found: 12822 phrases; correct: 10708. accuracy: 82.40%; (non-O) accuracy: 92.07%; precision: 83.51%; recall: 78.81%; FB1: 81.09 LOC: precision: 84.17%; recall: 76.82%; FB1: 80.33 4485 ORG: precision: 73.45%; recall: 77.25%; FB1: 75.30 4369 PER: precision: 93.85%; recall: 82.41%; FB1: 87.76 3968 ############################### lang: ur precision recall f1-score support B-LOC 0.92 0.87 0.90 334 I-ORG 0.86 0.89 0.88 1005 I-LOC 0.91 0.90 0.91 904 I-PER 0.89 0.91 0.90 928 B-ORG 0.86 0.86 0.86 323 B-PER 0.89 0.89 0.89 363 micro avg 0.89 0.89 0.89 3857 macro avg 0.89 0.89 0.89 3857 weighted avg 0.89 0.89 0.89 3857 processed 5027 tokens with 1020 phrases; found: 1003 phrases; correct: 878. accuracy: 89.50%; (non-O) accuracy: 90.89%; precision: 87.54%; recall: 86.08%; FB1: 86.80 LOC: precision: 90.54%; recall: 85.93%; FB1: 88.17 317 ORG: precision: 86.07%; recall: 86.07%; FB1: 86.07 323 PER: precision: 86.23%; recall: 86.23%; FB1: 86.23 363 ############################### lang: vi precision recall f1-score support B-LOC 0.88 0.86 0.87 3717 I-ORG 0.86 0.86 0.86 13562 I-LOC 0.89 0.85 0.87 8018 I-PER 0.93 0.81 0.87 7787 B-ORG 0.82 0.82 0.82 3704 B-PER 0.92 0.85 0.88 3884 micro avg 0.88 0.84 0.86 40672 macro avg 0.88 0.84 0.86 40672 weighted avg 0.88 0.84 0.86 40672 processed 64967 tokens with 11305 phrases; found: 10904 phrases; correct: 9223. accuracy: 84.22%; (non-O) accuracy: 89.37%; precision: 84.58%; recall: 81.58%; FB1: 83.06 LOC: precision: 85.10%; recall: 83.72%; FB1: 84.40 3657 ORG: precision: 78.77%; recall: 78.35%; FB1: 78.56 3684 PER: precision: 90.06%; recall: 82.62%; FB1: 86.18 3563 ############################### lang: yo precision recall f1-score support B-LOC 0.73 0.77 0.75 39 I-ORG 0.76 0.79 0.78 87 I-LOC 0.90 0.89 0.90 72 I-PER 0.94 0.72 0.82 71 B-ORG 0.68 0.79 0.73 29 B-PER 0.97 0.70 0.81 43 micro avg 0.83 0.78 0.81 341 macro avg 0.83 0.78 0.80 341 weighted avg 0.84 0.78 0.81 341 processed 503 tokens with 111 phrases; found: 106 phrases; correct: 81. accuracy: 78.30%; (non-O) accuracy: 85.09%; precision: 76.42%; recall: 72.97%; FB1: 74.65 LOC: precision: 73.17%; recall: 76.92%; FB1: 75.00 41 ORG: precision: 64.71%; recall: 75.86%; FB1: 69.84 34 PER: precision: 93.55%; recall: 67.44%; FB1: 78.38 31 ############################### lang: zh precision recall f1-score support B-LOC 0.83 0.71 0.76 4371 I-ORG 0.75 0.68 0.71 17399 I-LOC 0.85 0.74 0.79 12282 I-PER 0.87 0.77 0.82 12897 B-ORG 0.67 0.65 0.66 3779 B-PER 0.81 0.76 0.79 3899 micro avg 0.80 0.72 0.76 54627 macro avg 0.80 0.72 0.75 54627 weighted avg 0.80 0.72 0.76 54627 processed 207505 tokens with 12532 phrases; found: 11033 phrases; correct: 8345. accuracy: 72.07%; (non-O) accuracy: 90.67%; precision: 75.64%; recall: 66.59%; FB1: 70.83 LOC: precision: 80.35%; recall: 66.99%; FB1: 73.06 3730 ORG: precision: 67.63%; recall: 59.85%; FB1: 63.50 3642 PER: precision: 78.80%; recall: 73.17%; FB1: 75.88 3661</summary></entry></feed>