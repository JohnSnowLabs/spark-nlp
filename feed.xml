<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-02-10T11:56:39+00:00</updated><id>/feed.xml</id><title type="html">Spark NLP</title><subtitle>High Performance NLP with Apache Spark
</subtitle><author><name>{&quot;type&quot;=&gt;nil, &quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;googleplus&quot;=&gt;nil, &quot;telegram&quot;=&gt;nil, &quot;medium&quot;=&gt;nil, &quot;zhihu&quot;=&gt;nil, &quot;douban&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;npm&quot;=&gt;nil}</name></author><entry><title type="html">French CamemBertForQuestionAnswering Base squadFR (camembert_base_qa_fquad)</title><link href="/2023/02/08/camembert_base_qa_fquad_fr.html" rel="alternate" type="text/html" title="French CamemBertForQuestionAnswering Base squadFR (camembert_base_qa_fquad)" /><published>2023-02-08T00:00:00+00:00</published><updated>2023-02-08T00:00:00+00:00</updated><id>/2023/02/08/camembert_base_qa_fquad_fr</id><content type="html" xml:base="/2023/02/08/camembert_base_qa_fquad_fr.html">## Description

Pretrained CamemBertForQuestionAnswering model, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. `camembert_base_qa_fquad ` is a French model originally fine-tuned on a combo of three French Q&amp;A datasets:

- PIAFv1.1
- FQuADv1.0
- SQuAD-FR (SQuAD automatically translated to French)

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/camembert_base_qa_fquad_fr_4.3.0_3.2_1675865521345.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/camembert_base_qa_fquad_fr_4.3.0_3.2_1675865521345.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
Document_Assembler = MultiDocumentAssembler()\
     .setInputCols([&quot;question&quot;, &quot;context&quot;])\
     .setOutputCols([&quot;document_question&quot;, &quot;document_context&quot;])

Question_Answering = CamemBertForQuestionAnswering(&quot;camembert_base_qa_fquad&quot;,&quot;fr&quot;)\
     .setInputCols([&quot;document_question&quot;, &quot;document_context&quot;])\
     .setOutputCol(&quot;answer&quot;)\
     .setCaseSensitive(True)
    
pipeline = Pipeline(stages=[Document_Assembler, Question_Answering])

data = spark.createDataFrame([[&quot;Où est-ce que je vis?&quot;,&quot;Mon nom est Wolfgang et je vis à Berlin.&quot;]]).toDF(&quot;question&quot;, &quot;context&quot;)

result = pipeline.fit(data).transform(data)
```
```scala
val Document_Assembler = new MultiDocumentAssembler()
     .setInputCols(Array(&quot;question&quot;, &quot;context&quot;))
     .setOutputCols(Array(&quot;document_question&quot;, &quot;document_context&quot;))

val Question_Answering = CamemBertForQuestionAnswering(&quot;camembert_base_qa_fquad&quot;,&quot;fr&quot;)
     .setInputCols(Array(&quot;document_question&quot;, &quot;document_context&quot;))
     .setOutputCol(&quot;answer&quot;)
     .setCaseSensitive(True)
    
val pipeline = new Pipeline().setStages(Array(Document_Assembler, Question_Answering))

val data = Seq(&quot;Où est-ce que je vis?&quot;,&quot;Mon nom est Wolfgang et je vis à Berlin.&quot;).toDS.toDF(&quot;question&quot;, &quot;context&quot;)

val result = pipeline.fit(data).transform(data)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|camembert_base_qa_fquad|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[document_question, document_context]|
|Output Labels:|[answer]|
|Language:|fr|
|Size:|411.9 MB|

## References

https://huggingface.co/etalab-ia/camembert-base-squadFR-fquad-piaf

## Benchmarking

```bash
{&quot;f1&quot;: 80.61, &quot;exact_match&quot;: 59.54}
```</content><author><name>John Snow Labs</name></author><category term="fr" /><category term="french" /><category term="question_answering" /><category term="camembert" /><category term="open_source" /><category term="tensorflow" /><summary type="html">Description Pretrained CamemBertForQuestionAnswering model, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. camembert_base_qa_fquad is a French model originally fine-tuned on a combo of three French Q&amp;amp;A datasets: PIAFv1.1 FQuADv1.0 SQuAD-FR (SQuAD automatically translated to French) Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU Document_Assembler = MultiDocumentAssembler()\ .setInputCols([&quot;question&quot;, &quot;context&quot;])\ .setOutputCols([&quot;document_question&quot;, &quot;document_context&quot;]) Question_Answering = CamemBertForQuestionAnswering(&quot;camembert_base_qa_fquad&quot;,&quot;fr&quot;)\ .setInputCols([&quot;document_question&quot;, &quot;document_context&quot;])\ .setOutputCol(&quot;answer&quot;)\ .setCaseSensitive(True) pipeline = Pipeline(stages=[Document_Assembler, Question_Answering]) data = spark.createDataFrame([[&quot;Où est-ce que je vis?&quot;,&quot;Mon nom est Wolfgang et je vis à Berlin.&quot;]]).toDF(&quot;question&quot;, &quot;context&quot;) result = pipeline.fit(data).transform(data) val Document_Assembler = new MultiDocumentAssembler() .setInputCols(Array(&quot;question&quot;, &quot;context&quot;)) .setOutputCols(Array(&quot;document_question&quot;, &quot;document_context&quot;)) val Question_Answering = CamemBertForQuestionAnswering(&quot;camembert_base_qa_fquad&quot;,&quot;fr&quot;) .setInputCols(Array(&quot;document_question&quot;, &quot;document_context&quot;)) .setOutputCol(&quot;answer&quot;) .setCaseSensitive(True) val pipeline = new Pipeline().setStages(Array(Document_Assembler, Question_Answering)) val data = Seq(&quot;Où est-ce que je vis?&quot;,&quot;Mon nom est Wolfgang et je vis à Berlin.&quot;).toDS.toDF(&quot;question&quot;, &quot;context&quot;) val result = pipeline.fit(data).transform(data) Model Information Model Name: camembert_base_qa_fquad Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [document_question, document_context] Output Labels: [answer] Language: fr Size: 411.9 MB References https://huggingface.co/etalab-ia/camembert-base-squadFR-fquad-piaf Benchmarking {&quot;f1&quot;: 80.61, &quot;exact_match&quot;: 59.54}</summary></entry><entry><title type="html">Zero-Shot Named Entity Recognition (Generic sample)</title><link href="/2023/02/08/zero_shot_ner_roberta_en.html" rel="alternate" type="text/html" title="Zero-Shot Named Entity Recognition (Generic sample)" /><published>2023-02-08T00:00:00+00:00</published><updated>2023-02-08T00:00:00+00:00</updated><id>/2023/02/08/zero_shot_ner_roberta_en</id><content type="html" xml:base="/2023/02/08/zero_shot_ner_roberta_en.html">## Description

This model is trained with Zero-Shot Named Entity Recognition (NER) approach and it can detect any kind of defined entities with no training dataset, just pretrained RoBERTa embeddings (included in the model).

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/zero_shot_ner_roberta_en_4.3.0_3.2_1675890474068.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/zero_shot_ner_roberta_en_4.3.0_3.2_1675890474068.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
documentAssembler = DocumentAssembler() \
    .setInputCol(&quot;text&quot;) \
    .setOutputCol(&quot;document&quot;)

sentenceDetector = SentenceDetector() \
    .setInputCols([&quot;document&quot;]) \
    .setOutputCol(&quot;sentence&quot;)

tokenizer = Tokenizer() \
    .setInputCols([&quot;sentence&quot;]) \
    .setOutputCol(&quot;token&quot;)
    
zero_shot_ner = ZeroShotNerModel.pretrained(&quot;zero_shot_ner_roberta&quot;, &quot;en&quot;, &quot;clincial/models&quot;)\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;])\
    .setOutputCol(&quot;zero_shot_ner&quot;)\
    .setEntityDefinitions(
        {
            &quot;NAME&quot;: [&quot;What is his name?&quot;, &quot;What is my name?&quot;, &quot;What is her name?&quot;],
            &quot;CITY&quot;: [&quot;Which city?&quot;, &quot;Which is the city?&quot;]
        })

ner_converter = NerConverter()\
    .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;zero_shot_ner&quot;])\
    .setOutputCol(&quot;ner_chunk&quot;)\

pipeline = Pipeline(stages = [
    documentAssembler, 
    sentenceDetector, 
    tokenizer, 
    zero_shot_ner, 
    ner_converter])

data = spark.createDataFrame([&quot;Hellen works in London, Paris and Berlin. My name is Clara, I live in New York and Hellen lives in Paris.&quot;,
                              &quot;John is a man who works in London, London and London.&quot;], StringType()).toDF(&quot;text&quot;)

result = pipeline.fit(data).transform(data)
```
```scala
val documentAssembler = new DocumentAssembler()
    .setInputCol(&quot;text&quot;)
    .setOutputCol(&quot;document&quot;)

val sentenceDetector = new SentenceDetector() 
    .setInputCols(Array(&quot;document&quot;)) 
    .setOutputCol(&quot;sentence&quot;)

val tokenizer = new Tokenizer() 
    .setInputCols(Array(&quot;sentence&quot;)) 
    .setOutputCol(&quot;token&quot;)
    
val zero_shot_ner = ZeroShotNerModel.pretrained(&quot;zero_shot_ner_roberta&quot;, &quot;en&quot;, &quot;clincial/models&quot;)
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;))
    .setOutputCol(&quot;zero_shot_ner&quot;)
    .setEntityDefinitions(Map(
            &quot;NAME&quot;-&gt; Array(&quot;What is his name?&quot;, &quot;What is my name?&quot;, &quot;What is her name?&quot;),
            &quot;CITY&quot;-&gt; Array(&quot;Which city?&quot;, &quot;Which is the city?&quot;)
    ))

val ner_converter = new NerConverter()
    .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;zero_shot_ner&quot;))
    .setOutputCol(&quot;ner_chunk&quot;)

val pipeline = new .setStages(Array(
    documentAssembler, 
    sentenceDetector, 
    tokenizer, 
    zero_shot_ner, 
    ner_converter))

val data = Seq(Array(&quot;Hellen works in London, Paris and Berlin. My name is Clara, I live in New York and Hellen lives in Paris.&quot;,
                                     &quot;John is a man who works in London, London and London.&quot;)toDS().toDF(&quot;text&quot;)

val result = pipeline.fit(data).transform(data)
```
&lt;/div&gt;

## Results

```bash
+------+---------+--------+-----+---+----------+
| token|ner_label|sentence|begin|end|confidence|
+------+---------+--------+-----+---+----------+
|Hellen|   B-NAME|       0|    0|  5|0.13306311|
| works|        O|       0|    7| 11|      null|
|    in|        O|       0|   13| 14|      null|
|London|   B-CITY|       0|   16| 21| 0.4064213|
|     ,|        O|       0|   22| 22|      null|
| Paris|   B-CITY|       0|   24| 28|0.04597357|
|   and|        O|       0|   30| 32|      null|
|Berlin|   B-CITY|       0|   34| 39|0.16265489|
|     .|        O|       0|   40| 40|      null|
|    My|        O|       1|   42| 43|      null|
|  name|        O|       1|   45| 48|      null|
|    is|        O|       1|   50| 51|      null|
| Clara|   B-NAME|       1|   53| 57| 0.9274031|
|     ,|        O|       1|   58| 58|      null|
|     I|        O|       1|   60| 60|      null|
|  live|        O|       1|   62| 65|      null|
|    in|        O|       1|   67| 68|      null|
|   New|   B-CITY|       1|   70| 72|0.82799006|
|  York|   I-CITY|       1|   74| 77|0.82799006|
|   and|        O|       1|   79| 81|      null|
|Hellen|   B-NAME|       1|   83| 88|0.40429682|
| lives|        O|       1|   90| 94|      null|
|    in|        O|       1|   96| 97|      null|
| Paris|   B-CITY|       1|   99|103|0.49216735|
|     .|        O|       1|  104|104|      null|
|  John|   B-NAME|       0|    0|  3|0.14063153|
|    is|        O|       0|    5|  6|      null|
|     a|        O|       0|    8|  8|      null|
|   man|        O|       0|   10| 12|      null|
|   who|        O|       0|   14| 16|      null|
| works|        O|       0|   18| 22|      null|
|    in|        O|       0|   24| 25|      null|
|London|   B-CITY|       0|   27| 32|0.15521188|
|     ,|        O|       0|   33| 33|      null|
|London|   B-CITY|       0|   35| 40|0.12151082|
|   and|        O|       0|   42| 44|      null|
|London|   B-CITY|       0|   46| 51| 0.2650951|
|     .|        O|       0|   52| 52|      null|
+------+---------+--------+-----+---+----------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|zero_shot_ner_roberta|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[document_question, document_context]|
|Output Labels:|[answer]|
|Language:|en|
|Size:|463.8 MB|
|Case sensitive:|true|
|Max sentence length:|512|</content><author><name>John Snow Labs</name></author><category term="ner" /><category term="zero_shot" /><category term="roberta" /><category term="qa" /><category term="en" /><category term="open_source" /><category term="tensorflow" /><summary type="html">Description This model is trained with Zero-Shot Named Entity Recognition (NER) approach and it can detect any kind of defined entities with no training dataset, just pretrained RoBERTa embeddings (included in the model). Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU documentAssembler = DocumentAssembler() \ .setInputCol(&quot;text&quot;) \ .setOutputCol(&quot;document&quot;) sentenceDetector = SentenceDetector() \ .setInputCols([&quot;document&quot;]) \ .setOutputCol(&quot;sentence&quot;) tokenizer = Tokenizer() \ .setInputCols([&quot;sentence&quot;]) \ .setOutputCol(&quot;token&quot;) zero_shot_ner = ZeroShotNerModel.pretrained(&quot;zero_shot_ner_roberta&quot;, &quot;en&quot;, &quot;clincial/models&quot;)\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;])\ .setOutputCol(&quot;zero_shot_ner&quot;)\ .setEntityDefinitions( { &quot;NAME&quot;: [&quot;What is his name?&quot;, &quot;What is my name?&quot;, &quot;What is her name?&quot;], &quot;CITY&quot;: [&quot;Which city?&quot;, &quot;Which is the city?&quot;] }) ner_converter = NerConverter()\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;zero_shot_ner&quot;])\ .setOutputCol(&quot;ner_chunk&quot;)\ pipeline = Pipeline(stages = [ documentAssembler, sentenceDetector, tokenizer, zero_shot_ner, ner_converter]) data = spark.createDataFrame([&quot;Hellen works in London, Paris and Berlin. My name is Clara, I live in New York and Hellen lives in Paris.&quot;, &quot;John is a man who works in London, London and London.&quot;], StringType()).toDF(&quot;text&quot;) result = pipeline.fit(data).transform(data) val documentAssembler = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val sentenceDetector = new SentenceDetector() .setInputCols(Array(&quot;document&quot;)) .setOutputCol(&quot;sentence&quot;) val tokenizer = new Tokenizer() .setInputCols(Array(&quot;sentence&quot;)) .setOutputCol(&quot;token&quot;) val zero_shot_ner = ZeroShotNerModel.pretrained(&quot;zero_shot_ner_roberta&quot;, &quot;en&quot;, &quot;clincial/models&quot;) .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;)) .setOutputCol(&quot;zero_shot_ner&quot;) .setEntityDefinitions(Map( &quot;NAME&quot;-&amp;gt; Array(&quot;What is his name?&quot;, &quot;What is my name?&quot;, &quot;What is her name?&quot;), &quot;CITY&quot;-&amp;gt; Array(&quot;Which city?&quot;, &quot;Which is the city?&quot;) )) val ner_converter = new NerConverter() .setInputCols(Array(&quot;sentence&quot;, &quot;token&quot;, &quot;zero_shot_ner&quot;)) .setOutputCol(&quot;ner_chunk&quot;) val pipeline = new .setStages(Array( documentAssembler, sentenceDetector, tokenizer, zero_shot_ner, ner_converter)) val data = Seq(Array(&quot;Hellen works in London, Paris and Berlin. My name is Clara, I live in New York and Hellen lives in Paris.&quot;, &quot;John is a man who works in London, London and London.&quot;)toDS().toDF(&quot;text&quot;) val result = pipeline.fit(data).transform(data) Results +------+---------+--------+-----+---+----------+ | token|ner_label|sentence|begin|end|confidence| +------+---------+--------+-----+---+----------+ |Hellen| B-NAME| 0| 0| 5|0.13306311| | works| O| 0| 7| 11| null| | in| O| 0| 13| 14| null| |London| B-CITY| 0| 16| 21| 0.4064213| | ,| O| 0| 22| 22| null| | Paris| B-CITY| 0| 24| 28|0.04597357| | and| O| 0| 30| 32| null| |Berlin| B-CITY| 0| 34| 39|0.16265489| | .| O| 0| 40| 40| null| | My| O| 1| 42| 43| null| | name| O| 1| 45| 48| null| | is| O| 1| 50| 51| null| | Clara| B-NAME| 1| 53| 57| 0.9274031| | ,| O| 1| 58| 58| null| | I| O| 1| 60| 60| null| | live| O| 1| 62| 65| null| | in| O| 1| 67| 68| null| | New| B-CITY| 1| 70| 72|0.82799006| | York| I-CITY| 1| 74| 77|0.82799006| | and| O| 1| 79| 81| null| |Hellen| B-NAME| 1| 83| 88|0.40429682| | lives| O| 1| 90| 94| null| | in| O| 1| 96| 97| null| | Paris| B-CITY| 1| 99|103|0.49216735| | .| O| 1| 104|104| null| | John| B-NAME| 0| 0| 3|0.14063153| | is| O| 0| 5| 6| null| | a| O| 0| 8| 8| null| | man| O| 0| 10| 12| null| | who| O| 0| 14| 16| null| | works| O| 0| 18| 22| null| | in| O| 0| 24| 25| null| |London| B-CITY| 0| 27| 32|0.15521188| | ,| O| 0| 33| 33| null| |London| B-CITY| 0| 35| 40|0.12151082| | and| O| 0| 42| 44| null| |London| B-CITY| 0| 46| 51| 0.2650951| | .| O| 0| 52| 52| null| +------+---------+--------+-----+---+----------+ Model Information Model Name: zero_shot_ner_roberta Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [document_question, document_context] Output Labels: [answer] Language: en Size: 463.8 MB Case sensitive: true Max sentence length: 512</summary></entry><entry><title type="html">ASR HubertForCTC - asr_hubert_large_ls960</title><link href="/2023/02/07/asr_hubert_large_ls960_en.html" rel="alternate" type="text/html" title="ASR HubertForCTC - asr_hubert_large_ls960" /><published>2023-02-07T00:00:00+00:00</published><updated>2023-02-07T00:00:00+00:00</updated><id>/2023/02/07/asr_hubert_large_ls960_en</id><content type="html" xml:base="/2023/02/07/asr_hubert_large_ls960_en.html">## Description

Hubert Model with a language modeling head on top for Connectionist Temporal Classification (CTC). 
Hubert was proposed in HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed.

The large model fine-tuned on 960h of Librispeech on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz.

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/asr_hubert_large_ls960_en_4.3.0_3.0_1675767067233.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/asr_hubert_large_ls960_en_4.3.0_3.0_1675767067233.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
audio_assembler = AudioAssembler()\
  .setInputCol(&quot;audio_content&quot;)\
  .setOutputCol(&quot;audio_assembler&quot;)

speech_to_text = HubertForCTC.pretrained(&quot;asr_hubert_large_ls960&quot;, &quot;en&quot;)\
  .setInputCols(&quot;audio_assembler&quot;)\
  .setOutputCol(&quot;text&quot;)

pipeline = Pipeline(stages=[
  audio_assembler,
  speech_to_text,
])

pipelineModel = pipeline.fit(audioDf)

pipelineDF = pipelineModel.transform(audioDf)
```
```scala

val audioAssembler = new AudioAssembler()
    .setInputCol(&quot;audio_content&quot;) 
    .setOutputCol(&quot;audio_assembler&quot;)

val speechToText = HubertForCTC
    .pretrained(&quot;asr_hubert_large_ls960&quot;, &quot;en&quot;)
    .setInputCols(&quot;audio_assembler&quot;) 
    .setOutputCol(&quot;text&quot;) 

val pipeline = new Pipeline().setStages(Array(audioAssembler, speechToText))

val pipelineModel = pipeline.fit(audioDf)

val pipelineDF = pipelineModel.transform(audioDf)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|asr_hubert_large_ls960|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[audio_assembler]|
|Output Labels:|[text]|
|Language:|en|
|Size:|1.5 GB|

## References

[https://huggingface.co/facebook/hubert-large-ls960-ft](https://huggingface.co/facebook/hubert-large-ls960-ft)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="hubert" /><category term="audio" /><category term="en" /><category term="english" /><category term="asr" /><category term="speech" /><category term="librispeech_asr" /><category term="tensorflow" /><summary type="html">Description Hubert Model with a language modeling head on top for Connectionist Temporal Classification (CTC). Hubert was proposed in HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units by Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, Abdelrahman Mohamed. The large model fine-tuned on 960h of Librispeech on 16kHz sampled speech audio. When using the model make sure that your speech input is also sampled at 16Khz. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU audio_assembler = AudioAssembler()\ .setInputCol(&quot;audio_content&quot;)\ .setOutputCol(&quot;audio_assembler&quot;) speech_to_text = HubertForCTC.pretrained(&quot;asr_hubert_large_ls960&quot;, &quot;en&quot;)\ .setInputCols(&quot;audio_assembler&quot;)\ .setOutputCol(&quot;text&quot;) pipeline = Pipeline(stages=[ audio_assembler, speech_to_text, ]) pipelineModel = pipeline.fit(audioDf) pipelineDF = pipelineModel.transform(audioDf) val audioAssembler = new AudioAssembler() .setInputCol(&quot;audio_content&quot;) .setOutputCol(&quot;audio_assembler&quot;) val speechToText = HubertForCTC .pretrained(&quot;asr_hubert_large_ls960&quot;, &quot;en&quot;) .setInputCols(&quot;audio_assembler&quot;) .setOutputCol(&quot;text&quot;) val pipeline = new Pipeline().setStages(Array(audioAssembler, speechToText)) val pipelineModel = pipeline.fit(audioDf) val pipelineDF = pipelineModel.transform(audioDf) Model Information Model Name: asr_hubert_large_ls960 Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [audio_assembler] Output Labels: [text] Language: en Size: 1.5 GB References https://huggingface.co/facebook/hubert-large-ls960-ft</summary></entry><entry><title type="html">SwinForImageClassification - image_classifier_swin_base_patch4_window12_384_in22k</title><link href="/2023/02/07/image_classifier_swin_base_patch4_window12_384_in22k_en.html" rel="alternate" type="text/html" title="SwinForImageClassification - image_classifier_swin_base_patch4_window12_384_in22k" /><published>2023-02-07T00:00:00+00:00</published><updated>2023-02-07T00:00:00+00:00</updated><id>/2023/02/07/image_classifier_swin_base_patch4_window12_384_in22k_en</id><content type="html" xml:base="/2023/02/07/image_classifier_swin_base_patch4_window12_384_in22k_en.html">## Description

Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP.

Swin Transformer was introduced in the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Liu et al.

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/image_classifier_swin_base_patch4_window12_384_in22k_en_4.3.0_3.0_1675783085913.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/image_classifier_swin_base_patch4_window12_384_in22k_en_4.3.0_3.0_1675783085913.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
image_assembler = ImageAssembler()\
  .setInputCol(&quot;image&quot;)
  .setOutputCol(&quot;image_assembler&quot;)

imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_base_patch4_window12_384_in22k&quot;, &quot;en&quot;)\
  .setInputCols(&quot;image_assembler&quot;)\
  .setOutputCol(&quot;class&quot;)

pipeline = Pipeline(stages=[
  image_assembler,
  imageClassifier,
])

pipelineModel = pipeline.fit(imageDF)

pipelineDF = pipelineModel.transform(imageDF)
```
```scala

val imageAssembler = new ImageAssembler()
    .setInputCol(&quot;image&quot;)
    .setOutputCol(&quot;image_assembler&quot;)

val imageClassifier = SwinForImageClassification
    .pretrained(&quot;image_classifier_swin_base_patch4_window12_384_in22k&quot;, &quot;en&quot;)
    .setInputCols(&quot;image_assembler&quot;) 
    .setOutputCol(&quot;class&quot;) 

val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier))

val pipelineModel = pipeline.fit(imageDF)

val pipelineDF = pipelineModel.transform(imageDF)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|image_classifier_swin_base_patch4_window12_384_in22k|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[image_assembler]|
|Output Labels:|[class]|
|Language:|en|
|Size:|826.8 MB|

## References

[https://huggingface.co/microsoft/swin_base_patch4_window12_384_in22k](https://huggingface.co/microsoft/swin_base_patch4_window12_384_in22k)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="swin" /><category term="image" /><category term="en" /><category term="english" /><category term="image_classification" /><category term="imagenet" /><category term="tensorflow" /><summary type="html">Description Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. Swin Transformer was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU image_assembler = ImageAssembler()\ .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_base_patch4_window12_384_in22k&quot;, &quot;en&quot;)\ .setInputCols(&quot;image_assembler&quot;)\ .setOutputCol(&quot;class&quot;) pipeline = Pipeline(stages=[ image_assembler, imageClassifier, ]) pipelineModel = pipeline.fit(imageDF) pipelineDF = pipelineModel.transform(imageDF) val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) val imageClassifier = SwinForImageClassification .pretrained(&quot;image_classifier_swin_base_patch4_window12_384_in22k&quot;, &quot;en&quot;) .setInputCols(&quot;image_assembler&quot;) .setOutputCol(&quot;class&quot;) val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier)) val pipelineModel = pipeline.fit(imageDF) val pipelineDF = pipelineModel.transform(imageDF) Model Information Model Name: image_classifier_swin_base_patch4_window12_384_in22k Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [image_assembler] Output Labels: [class] Language: en Size: 826.8 MB References https://huggingface.co/microsoft/swin_base_patch4_window12_384_in22k</summary></entry><entry><title type="html">SwinForImageClassification - image_classifier_swin_base_patch4_window7_224</title><link href="/2023/02/07/image_classifier_swin_base_patch4_window7_224_en.html" rel="alternate" type="text/html" title="SwinForImageClassification - image_classifier_swin_base_patch4_window7_224" /><published>2023-02-07T00:00:00+00:00</published><updated>2023-02-07T00:00:00+00:00</updated><id>/2023/02/07/image_classifier_swin_base_patch4_window7_224_en</id><content type="html" xml:base="/2023/02/07/image_classifier_swin_base_patch4_window7_224_en.html">## Description

Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP.

Swin Transformer was introduced in the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Liu et al.

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/image_classifier_swin_base_patch4_window7_224_en_4.3.0_3.0_1675783112124.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/image_classifier_swin_base_patch4_window7_224_en_4.3.0_3.0_1675783112124.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
image_assembler = ImageAssembler()\
  .setInputCol(&quot;image&quot;)\
  .setOutputCol(&quot;image_assembler&quot;)

imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_base_patch4_window7_224&quot;, &quot;en&quot;)\
  .setInputCols(&quot;image_assembler&quot;)\
  .setOutputCol(&quot;class&quot;)

pipeline = Pipeline(stages=[
  image_assembler,
  imageClassifier,
])

pipelineModel = pipeline.fit(imageDF)

pipelineDF = pipelineModel.transform(imageDF)
```
```scala

val imageAssembler = new ImageAssembler()
    .setInputCol(&quot;image&quot;)
    .setOutputCol(&quot;image_assembler&quot;)

val imageClassifier = SwinForImageClassification
    .pretrained(&quot;image_classifier_swin_base_patch4_window7_224&quot;, &quot;en&quot;)
    .setInputCols(&quot;image_assembler&quot;) 
    .setOutputCol(&quot;class&quot;) 

val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier))

val pipelineModel = pipeline.fit(imageDF)

val pipelineDF = pipelineModel.transform(imageDF)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|image_classifier_swin_base_patch4_window7_224|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[image_assembler]|
|Output Labels:|[class]|
|Language:|en|
|Size:|665.2 MB|

## References

[https://huggingface.co/microsoft/swin_base_patch4_window7_224](https://huggingface.co/microsoft/swin_base_patch4_window7_224)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="swin" /><category term="image" /><category term="en" /><category term="english" /><category term="image_classification" /><category term="imagenet" /><category term="tensorflow" /><summary type="html">Description Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. Swin Transformer was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU image_assembler = ImageAssembler()\ .setInputCol(&quot;image&quot;)\ .setOutputCol(&quot;image_assembler&quot;) imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_base_patch4_window7_224&quot;, &quot;en&quot;)\ .setInputCols(&quot;image_assembler&quot;)\ .setOutputCol(&quot;class&quot;) pipeline = Pipeline(stages=[ image_assembler, imageClassifier, ]) pipelineModel = pipeline.fit(imageDF) pipelineDF = pipelineModel.transform(imageDF) val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) val imageClassifier = SwinForImageClassification .pretrained(&quot;image_classifier_swin_base_patch4_window7_224&quot;, &quot;en&quot;) .setInputCols(&quot;image_assembler&quot;) .setOutputCol(&quot;class&quot;) val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier)) val pipelineModel = pipeline.fit(imageDF) val pipelineDF = pipelineModel.transform(imageDF) Model Information Model Name: image_classifier_swin_base_patch4_window7_224 Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [image_assembler] Output Labels: [class] Language: en Size: 665.2 MB References https://huggingface.co/microsoft/swin_base_patch4_window7_224</summary></entry><entry><title type="html">SwinForImageClassification - image_classifier_swin_base_patch4_window7_224_in22k</title><link href="/2023/02/07/image_classifier_swin_base_patch4_window7_224_in22k_en.html" rel="alternate" type="text/html" title="SwinForImageClassification - image_classifier_swin_base_patch4_window7_224_in22k" /><published>2023-02-07T00:00:00+00:00</published><updated>2023-02-07T00:00:00+00:00</updated><id>/2023/02/07/image_classifier_swin_base_patch4_window7_224_in22k_en</id><content type="html" xml:base="/2023/02/07/image_classifier_swin_base_patch4_window7_224_in22k_en.html">## Description

Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP.

Swin Transformer was introduced in the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Liu et al.

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/image_classifier_swin_base_patch4_window7_224_in22k_en_4.3.0_3.0_1675783139673.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/image_classifier_swin_base_patch4_window7_224_in22k_en_4.3.0_3.0_1675783139673.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
image_assembler = ImageAssembler()\
  .setInputCol(&quot;image&quot;)\
  .setOutputCol(&quot;image_assembler&quot;)

imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_base_patch4_window7_224_in22k&quot;, &quot;en&quot;)\
  .setInputCols(&quot;image_assembler&quot;)\
  .setOutputCol(&quot;class&quot;)

pipeline = Pipeline(stages=[
  image_assembler,
  imageClassifier,
])

pipelineModel = pipeline.fit(imageDF)

pipelineDF = pipelineModel.transform(imageDF)
```
```scala

val imageAssembler = new ImageAssembler()
    .setInputCol(&quot;image&quot;)
    .setOutputCol(&quot;image_assembler&quot;)

val imageClassifier = SwinForImageClassification
    .pretrained(&quot;image_classifier_swin_base_patch4_window7_224_in22k&quot;, &quot;en&quot;)
    .setInputCols(&quot;image_assembler&quot;) 
    .setOutputCol(&quot;class&quot;) 

val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier))

val pipelineModel = pipeline.fit(imageDF)

val pipelineDF = pipelineModel.transform(imageDF)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|image_classifier_swin_base_patch4_window7_224_in22k|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[image_assembler]|
|Output Labels:|[class]|
|Language:|en|
|Size:|825.5 MB|

## References

[https://huggingface.co/microsoft/swin_base_patch4_window7_224_in22k](https://huggingface.co/microsoft/swin_base_patch4_window7_224_in22k)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="swin" /><category term="image" /><category term="en" /><category term="english" /><category term="image_classification" /><category term="imagenet" /><category term="tensorflow" /><summary type="html">Description Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. Swin Transformer was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU image_assembler = ImageAssembler()\ .setInputCol(&quot;image&quot;)\ .setOutputCol(&quot;image_assembler&quot;) imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_base_patch4_window7_224_in22k&quot;, &quot;en&quot;)\ .setInputCols(&quot;image_assembler&quot;)\ .setOutputCol(&quot;class&quot;) pipeline = Pipeline(stages=[ image_assembler, imageClassifier, ]) pipelineModel = pipeline.fit(imageDF) pipelineDF = pipelineModel.transform(imageDF) val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) val imageClassifier = SwinForImageClassification .pretrained(&quot;image_classifier_swin_base_patch4_window7_224_in22k&quot;, &quot;en&quot;) .setInputCols(&quot;image_assembler&quot;) .setOutputCol(&quot;class&quot;) val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier)) val pipelineModel = pipeline.fit(imageDF) val pipelineDF = pipelineModel.transform(imageDF) Model Information Model Name: image_classifier_swin_base_patch4_window7_224_in22k Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [image_assembler] Output Labels: [class] Language: en Size: 825.5 MB References https://huggingface.co/microsoft/swin_base_patch4_window7_224_in22k</summary></entry><entry><title type="html">SwinForImageClassification - image_classifier_swin_tiny_patch4_window7_224</title><link href="/2023/02/07/image_classifier_swin_tiny_patch4_window7_224_en.html" rel="alternate" type="text/html" title="SwinForImageClassification - image_classifier_swin_tiny_patch4_window7_224" /><published>2023-02-07T00:00:00+00:00</published><updated>2023-02-07T00:00:00+00:00</updated><id>/2023/02/07/image_classifier_swin_tiny_patch4_window7_224_en</id><content type="html" xml:base="/2023/02/07/image_classifier_swin_tiny_patch4_window7_224_en.html">## Description

Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP.

Swin Transformer was introduced in the paper [Swin Transformer: Hierarchical Vision Transformer using Shifted Windows](https://arxiv.org/abs/2103.14030) by Liu et al.

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/image_classifier_swin_tiny_patch4_window7_224_en_4.3.0_3.0_1675783170631.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/image_classifier_swin_tiny_patch4_window7_224_en_4.3.0_3.0_1675783170631.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
image_assembler = ImageAssembler()\
  .setInputCol(&quot;image&quot;)\
  .setOutputCol(&quot;image_assembler&quot;)

imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_tiny_patch4_window7_224&quot;, &quot;en&quot;)\
  .setInputCols(&quot;image_assembler&quot;)\
  .setOutputCol(&quot;class&quot;)

pipeline = Pipeline(stages=[
  image_assembler,
  imageClassifier,
])

pipelineModel = pipeline.fit(imageDF)

pipelineDF = pipelineModel.transform(imageDF)
```
```scala

val imageAssembler = new ImageAssembler()
    .setInputCol(&quot;image&quot;)
    .setOutputCol(&quot;image_assembler&quot;)

val imageClassifier = SwinForImageClassification
    .pretrained(&quot;image_classifier_swin_tiny_patch4_window7_224&quot;, &quot;en&quot;)
    .setInputCols(&quot;image_assembler&quot;) 
    .setOutputCol(&quot;class&quot;) 

val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier))

val pipelineModel = pipeline.fit(imageDF)

val pipelineDF = pipelineModel.transform(imageDF)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|image_classifier_swin_tiny_patch4_window7_224|
|Compatibility:|Spark NLP 4.3.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[image_assembler]|
|Output Labels:|[class]|
|Language:|en|
|Size:|216.2 MB|

## References

[https://huggingface.co/microsoft/swin_tiny_patch4_window7_224](https://huggingface.co/microsoft/swin_tiny_patch4_window7_224)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="swin" /><category term="image" /><category term="en" /><category term="english" /><category term="image_classification" /><category term="imagenet" /><category term="tensorflow" /><summary type="html">Description Pretrained Swin model for Image Classification, adapted from Hugging Face and curated to provide scalability and production-readiness using Spark NLP. Swin Transformer was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU image_assembler = ImageAssembler()\ .setInputCol(&quot;image&quot;)\ .setOutputCol(&quot;image_assembler&quot;) imageClassifier = SwinForImageClassification.pretrained(&quot;image_classifier_swin_tiny_patch4_window7_224&quot;, &quot;en&quot;)\ .setInputCols(&quot;image_assembler&quot;)\ .setOutputCol(&quot;class&quot;) pipeline = Pipeline(stages=[ image_assembler, imageClassifier, ]) pipelineModel = pipeline.fit(imageDF) pipelineDF = pipelineModel.transform(imageDF) val imageAssembler = new ImageAssembler() .setInputCol(&quot;image&quot;) .setOutputCol(&quot;image_assembler&quot;) val imageClassifier = SwinForImageClassification .pretrained(&quot;image_classifier_swin_tiny_patch4_window7_224&quot;, &quot;en&quot;) .setInputCols(&quot;image_assembler&quot;) .setOutputCol(&quot;class&quot;) val pipeline = new Pipeline().setStages(Array(imageAssembler, imageClassifier)) val pipelineModel = pipeline.fit(imageDF) val pipelineDF = pipelineModel.transform(imageDF) Model Information Model Name: image_classifier_swin_tiny_patch4_window7_224 Compatibility: Spark NLP 4.3.0+ License: Open Source Edition: Official Input Labels: [image_assembler] Output Labels: [class] Language: en Size: 216.2 MB References https://huggingface.co/microsoft/swin_tiny_patch4_window7_224</summary></entry><entry><title type="html">Chinese Financial NER (sm, bert_embeddings_mengzi_bert_base_fin)</title><link href="/2023/02/04/finner_finance_chinese_sm_zh.html" rel="alternate" type="text/html" title="Chinese Financial NER (sm, bert_embeddings_mengzi_bert_base_fin)" /><published>2023-02-04T00:00:00+00:00</published><updated>2023-02-04T00:00:00+00:00</updated><id>/2023/02/04/finner_finance_chinese_sm_zh</id><content type="html" xml:base="/2023/02/04/finner_finance_chinese_sm_zh.html">## Description

This is the small version of the NER model for Financial Chinese texts, trained in a subset of **ChFinAnn** (see &quot;Datasets used for training&quot;). 

To use this model, use the `BertEmbeddings` model named `bert_embeddings_mengzi_bert_base_fin&quot;` as:

```python
bert_embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_mengzi_bert_base_fin&quot;,&quot;zh&quot;) \
    .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
    .setOutputCol(&quot;embeddings&quot;)
```

Also, please note that the Chinese texts are not separated by white space. The embedding model we use is based on character-level embeddings, so you need to split the text on every character (for example, by setting `.setSplitPattern(&quot;&quot;)` in the `Tokenizer` annotator).

## Predicted Entities

`AveragePrice`, `ClosingDate`, `CompanyName`, `EndDate`, `EquityHolder`, `FrozeShares`, `HighestTradingPrice`, `LaterHoldingShares`, `LegalInstitution`, `LowestTradingPrice`, `OtherType`, `PledgedShares`, `Pledgee`, `ReleasedDate`, `RepurchaseAmount`, `RepurchasedShares`, `StartDate`, `StockAbbr`, `StockCode`, `TotalHoldingRatio`, `TotalHoldingShares`, `TotalPledgedShares`, `TradedShares`, `UnfrozeDate`

{:.btn-box}
[Live Demo](https://demo.johnsnowlabs.com/finance/FINNER_FINANCE_CHINESE){:.button.button-orange}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finner_finance_chinese_sm_zh_1.0.0_3.0_1675554138686.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finner_finance_chinese_sm_zh_1.0.0_3.0_1675554138686.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
documentAssembler = nlp.DocumentAssembler()\
        .setInputCol(&quot;text&quot;)\
        .setOutputCol(&quot;document&quot;)
        
tokenizer = nlp.Tokenizer()\
        .setInputCols([&quot;document&quot;])\
        .setOutputCol(&quot;token&quot;)\
        .setSplitPattern(&quot;&quot;) # Split on char level

embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_mengzi_bert_base_fin&quot;,&quot;en&quot;) \
        .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \
        .setOutputCol(&quot;embeddings&quot;)

ner_model = finance.NerModel.pretrained(&quot;finner_finance_chinese_sm&quot;, &quot;zh&quot;, &quot;finance/models&quot;)\
        .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;])\
        .setOutputCol(&quot;ner&quot;)

ner_converter = nlp.NerConverter()\
        .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;])\
        .setOutputCol(&quot;ner_chunk&quot;)

nlpPipeline = Pipeline(stages=[
        documentAssembler,
        sentenceDetector,
        tokenizer,
        embeddings,
        ner_model,
        ner_converter])

empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)

model = nlpPipeline.fit(empty_data)

text = [&quot;&quot;&quot;近日，渤海水业股份有限公司（以下简称“公司”）收到公司持股5%以上股东李华青女士的《告知函》，获悉李华青女士将其所持有的部分公司股票进行补充质押，具体事项如下：&quot;&quot;&quot;]

res = model. Transform(spark.createDataFrame([text]).toDF(&quot;text&quot;))
res.select(F.explode(F.arrays_zip(res.ner_chunk.result, res.ner_chunk.metadata)).alias(&quot;cols&quot;)) \
      .select(F.expr(&quot;cols['0']&quot;).alias(&quot;chunk&quot;),
              F.expr(&quot;cols['1']['entity']&quot;).alias(&quot;ner_label&quot;),
              F.expr(&quot;cols['1']['confidence']&quot;).alias(&quot;confidence&quot;)).show(truncate=False)

```

&lt;/div&gt;

## Results

```bash
+------------------------------------+------------+----------+
|chunk                               |ner_label   |confidence|
+------------------------------------+------------+----------+
|业股份有限公司（以下简称“公司”）收到|CompanyName |0.7933    |
|质押，具体                          |EquityHolder|0.9378667 |
+------------------------------------+------------+----------+
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finner_finance_chinese_sm|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence, token, embeddings]|
|Output Labels:|[ner]|
|Language:|zh|
|Size:|16.8 MB|

## References

The dataset used for training was a subset of the **chFinAnn** dataset, consisting of financial statements of Chinese listed companies from 2008 to 2018. 

Reference:

- [Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction](https://aclanthology.org/D19-1032) (Zheng et al., EMNLP-IJCNLP 2019)

## Sample text from the training dataset

近日，渤海水业股份有限公司（以下简称“公司”）收到公司持股5%以上股东李华青女士的《告知函》，获悉李华青女士将其所持有的部分公司股票进行补充质押，具体事项如下：

## Benchmarking

```bash
 entity                 precision    recall        f1    support 
 AveragePrice            78.0731   85.1449   81.4558         301 
 ClosingDate             76.0148   57.7031   65.6051         271 
 CompanyName             94.0767   95.9251   94.9919        5605 
 EndDate                 75.487    44.5402   56.0241         616 
 EquityHolder            83.8303   91.319    87.4146        7780 
 FrozeShares             47.4227   31.7241   38.0165          97 
 HighestTradingPrice     74.4186   70.5085   72.4108         559 
 LaterHoldingShares      31.4961   11.9048   17.2786         127 
 LegalInstitution        92.3767   87.6596   89.9563         223 
 LowestTradingPrice      78.5047   52.1739   62.6866         107 
 OtherType               78.2961   36.7619   50.0324         493 
 PledgedShares           78.0776   65.5189   71.249         1779 
 Pledgee                 90.1003   88.656    89.3723        1596 
 ReleasedDate            54.5016   46.1853   50              622 
 RepurchaseAmount        68.323    72.8477   70.5128         322 
 RepurchasedShares       79.5918   70.4819   74.7604         588 
 StartDate               65.4217   77.5493   70.9711        4150 
 StockAbbr               83.7656   82.0521   82.9           2969 
 StockCode               99.8355   99.5626   99.6989        1824 
 TotalHoldingRatio       74.2574   85.1628   79.3371        1515 
 TotalHoldingShares      67.0582   88.7306   76.387         2043 
 TotalPledgedShares      74.5989   86.0226   79.9045        1122 
 TradedShares            76.9231   68.6948   72.5765         910 
 UnfrozeDate              5.88235   5.55556   5.71429         17
```</content><author><name>John Snow Labs</name></author><category term="zh" /><category term="cn" /><category term="finance" /><category term="ner" /><category term="licensed" /><summary type="html">Description This is the small version of the NER model for Financial Chinese texts, trained in a subset of ChFinAnn (see “Datasets used for training”). To use this model, use the BertEmbeddings model named bert_embeddings_mengzi_bert_base_fin&quot; as: bert_embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_mengzi_bert_base_fin&quot;,&quot;zh&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) Also, please note that the Chinese texts are not separated by white space. The embedding model we use is based on character-level embeddings, so you need to split the text on every character (for example, by setting .setSplitPattern(&quot;&quot;) in the Tokenizer annotator). Predicted Entities AveragePrice, ClosingDate, CompanyName, EndDate, EquityHolder, FrozeShares, HighestTradingPrice, LaterHoldingShares, LegalInstitution, LowestTradingPrice, OtherType, PledgedShares, Pledgee, ReleasedDate, RepurchaseAmount, RepurchasedShares, StartDate, StockAbbr, StockCode, TotalHoldingRatio, TotalHoldingShares, TotalPledgedShares, TradedShares, UnfrozeDate Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU documentAssembler = nlp.DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) tokenizer = nlp.Tokenizer()\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;token&quot;)\ .setSplitPattern(&quot;&quot;) # Split on char level embeddings = nlp.BertEmbeddings.pretrained(&quot;bert_embeddings_mengzi_bert_base_fin&quot;,&quot;en&quot;) \ .setInputCols([&quot;sentence&quot;, &quot;token&quot;]) \ .setOutputCol(&quot;embeddings&quot;) ner_model = finance.NerModel.pretrained(&quot;finner_finance_chinese_sm&quot;, &quot;zh&quot;, &quot;finance/models&quot;)\ .setInputCols([&quot;sentence&quot;, &quot;token&quot;, &quot;embeddings&quot;])\ .setOutputCol(&quot;ner&quot;) ner_converter = nlp.NerConverter()\ .setInputCols([&quot;sentence&quot;,&quot;token&quot;,&quot;ner&quot;])\ .setOutputCol(&quot;ner_chunk&quot;) nlpPipeline = Pipeline(stages=[ documentAssembler, sentenceDetector, tokenizer, embeddings, ner_model, ner_converter]) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = nlpPipeline.fit(empty_data) text = [&quot;&quot;&quot;近日，渤海水业股份有限公司（以下简称“公司”）收到公司持股5%以上股东李华青女士的《告知函》，获悉李华青女士将其所持有的部分公司股票进行补充质押，具体事项如下：&quot;&quot;&quot;] res = model. Transform(spark.createDataFrame([text]).toDF(&quot;text&quot;)) res.select(F.explode(F.arrays_zip(res.ner_chunk.result, res.ner_chunk.metadata)).alias(&quot;cols&quot;)) \ .select(F.expr(&quot;cols['0']&quot;).alias(&quot;chunk&quot;), F.expr(&quot;cols['1']['entity']&quot;).alias(&quot;ner_label&quot;), F.expr(&quot;cols['1']['confidence']&quot;).alias(&quot;confidence&quot;)).show(truncate=False) Results +------------------------------------+------------+----------+ |chunk |ner_label |confidence| +------------------------------------+------------+----------+ |业股份有限公司（以下简称“公司”）收到|CompanyName |0.7933 | |质押，具体 |EquityHolder|0.9378667 | +------------------------------------+------------+----------+ Model Information Model Name: finner_finance_chinese_sm Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [sentence, token, embeddings] Output Labels: [ner] Language: zh Size: 16.8 MB References The dataset used for training was a subset of the chFinAnn dataset, consisting of financial statements of Chinese listed companies from 2008 to 2018. Reference: Doc2EDAG: An End-to-End Document-level Framework for Chinese Financial Event Extraction (Zheng et al., EMNLP-IJCNLP 2019) Sample text from the training dataset 近日，渤海水业股份有限公司（以下简称“公司”）收到公司持股5%以上股东李华青女士的《告知函》，获悉李华青女士将其所持有的部分公司股票进行补充质押，具体事项如下： Benchmarking entity precision recall f1 support AveragePrice 78.0731 85.1449 81.4558 301 ClosingDate 76.0148 57.7031 65.6051 271 CompanyName 94.0767 95.9251 94.9919 5605 EndDate 75.487 44.5402 56.0241 616 EquityHolder 83.8303 91.319 87.4146 7780 FrozeShares 47.4227 31.7241 38.0165 97 HighestTradingPrice 74.4186 70.5085 72.4108 559 LaterHoldingShares 31.4961 11.9048 17.2786 127 LegalInstitution 92.3767 87.6596 89.9563 223 LowestTradingPrice 78.5047 52.1739 62.6866 107 OtherType 78.2961 36.7619 50.0324 493 PledgedShares 78.0776 65.5189 71.249 1779 Pledgee 90.1003 88.656 89.3723 1596 ReleasedDate 54.5016 46.1853 50 622 RepurchaseAmount 68.323 72.8477 70.5128 322 RepurchasedShares 79.5918 70.4819 74.7604 588 StartDate 65.4217 77.5493 70.9711 4150 StockAbbr 83.7656 82.0521 82.9 2969 StockCode 99.8355 99.5626 99.6989 1824 TotalHoldingRatio 74.2574 85.1628 79.3371 1515 TotalHoldingShares 67.0582 88.7306 76.387 2043 TotalPledgedShares 74.5989 86.0226 79.9045 1122 TradedShares 76.9231 68.6948 72.5765 910 UnfrozeDate 5.88235 5.55556 5.71429 17</summary></entry><entry><title type="html">Categorize Chat Messages from Customer Service</title><link href="/2023/02/03/finclf_customer_service_category_en.html" rel="alternate" type="text/html" title="Categorize Chat Messages from Customer Service" /><published>2023-02-03T00:00:00+00:00</published><updated>2023-02-03T00:00:00+00:00</updated><id>/2023/02/03/finclf_customer_service_category_en</id><content type="html" xml:base="/2023/02/03/finclf_customer_service_category_en.html">## Description

This is a Text Classification model that can help you classify a chat message from customer service.

## Predicted Entities

`ACCOUNT`, `CANCELLATION_FEE`, `CONTACT`, `DELIVERY`, `FEEDBACK`, `INVOICE`, `NEWSLETTER`, `ORDER`, `PAYMENT`, `REFUND`, `SHIPPING_ADDRESS`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finclf_customer_service_category_en_1.0.0_3.0_1675417487415.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finclf_customer_service_category_en_1.0.0_3.0_1675417487415.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = nlp.DocumentAssembler() \
    .setInputCol(&quot;text&quot;) \
    .setOutputCol(&quot;document&quot;)

embeddings = nlp.UniversalSentenceEncoder.pretrained() \
    .setInputCols(&quot;document&quot;) \
    .setOutputCol(&quot;sentence_embeddings&quot;)

docClassifier = finance.ClassifierDLModel.pretrained(&quot;finclf_customer_service_category&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
    .setInputCols(&quot;sentence_embeddings&quot;) \
    .setOutputCol(&quot;class&quot;)

pipeline = nlp.Pipeline().setStages(
      [
        document_assembler,
        embeddings,
        docClassifier
      ]
    )

empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)
model = pipeline.fit(empty_data)
light_model = nlp.LightPipeline(model)

result = light_model.annotate(&quot;&quot;&quot;can I place an order from Finland?&quot;&quot;&quot;)

result[&quot;class&quot;]
```

&lt;/div&gt;

## Results

```bash
['DELIVERY']
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finclf_customer_service_category|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence_embeddings]|
|Output Labels:|[class]|
|Language:|en|
|Size:|22.7 MB|

## References

https://github.com/bitext/customer-support-intent-detection-evaluation-dataset

## Benchmarking

```bash
label             precision  recall  f1-score  support 
ACCOUNT           0.99       0.99    0.99      180     
CANCELLATION_FEE  1.00       1.00    1.00      30      
CONTACT           0.98       1.00    0.99      60      
DELIVERY          1.00       1.00    1.00      60      
FEEDBACK          0.97       0.95    0.96      60      
INVOICE           1.00       1.00    1.00      60      
NEWSLETTER        0.94       1.00    0.97      30      
ORDER             1.00       0.99    1.00      120     
OTHER             1.00       0.97    0.98      63      
PAYMENT           0.95       1.00    0.98      60      
REFUND            0.99       0.98    0.98      90      
SHIPPING_ADDRESS  1.00       0.98    0.99      60      
accuracy          -          -       0.99      973     
macro-avg         0.99       0.99    0.99      873     
weighted-avg      0.99       0.99    0.99      873  
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="licensed" /><category term="finance" /><category term="customer" /><category term="classification" /><category term="tensorflow" /><summary type="html">Description This is a Text Classification model that can help you classify a chat message from customer service. Predicted Entities ACCOUNT, CANCELLATION_FEE, CONTACT, DELIVERY, FEEDBACK, INVOICE, NEWSLETTER, ORDER, PAYMENT, REFUND, SHIPPING_ADDRESS Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler() \ .setInputCol(&quot;text&quot;) \ .setOutputCol(&quot;document&quot;) embeddings = nlp.UniversalSentenceEncoder.pretrained() \ .setInputCols(&quot;document&quot;) \ .setOutputCol(&quot;sentence_embeddings&quot;) docClassifier = finance.ClassifierDLModel.pretrained(&quot;finclf_customer_service_category&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols(&quot;sentence_embeddings&quot;) \ .setOutputCol(&quot;class&quot;) pipeline = nlp.Pipeline().setStages( [ document_assembler, embeddings, docClassifier ] ) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = pipeline.fit(empty_data) light_model = nlp.LightPipeline(model) result = light_model.annotate(&quot;&quot;&quot;can I place an order from Finland?&quot;&quot;&quot;) result[&quot;class&quot;] Results ['DELIVERY'] Model Information Model Name: finclf_customer_service_category Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [sentence_embeddings] Output Labels: [class] Language: en Size: 22.7 MB References https://github.com/bitext/customer-support-intent-detection-evaluation-dataset Benchmarking label precision recall f1-score support ACCOUNT 0.99 0.99 0.99 180 CANCELLATION_FEE 1.00 1.00 1.00 30 CONTACT 0.98 1.00 0.99 60 DELIVERY 1.00 1.00 1.00 60 FEEDBACK 0.97 0.95 0.96 60 INVOICE 1.00 1.00 1.00 60 NEWSLETTER 0.94 1.00 0.97 30 ORDER 1.00 0.99 1.00 120 OTHER 1.00 0.97 0.98 63 PAYMENT 0.95 1.00 0.98 60 REFUND 0.99 0.98 0.98 90 SHIPPING_ADDRESS 1.00 0.98 0.99 60 accuracy - - 0.99 973 macro-avg 0.99 0.99 0.99 873 weighted-avg 0.99 0.99 0.99 873</summary></entry><entry><title type="html">Extract Intent Type from Customer Service Chat Messages</title><link href="/2023/02/03/finclf_customer_service_intent_type_en.html" rel="alternate" type="text/html" title="Extract Intent Type from Customer Service Chat Messages" /><published>2023-02-03T00:00:00+00:00</published><updated>2023-02-03T00:00:00+00:00</updated><id>/2023/02/03/finclf_customer_service_intent_type_en</id><content type="html" xml:base="/2023/02/03/finclf_customer_service_intent_type_en.html">## Description

This is a Text Classification model that can help you classify a chat message from customer service according to intent type.

## Predicted Entities

`cancel_order`, `change_order`, `change_setup_shipping_address`, `check_cancellation_fee`, `check_payment_methods`, `check_refund_policy`, `complaint`, `contact_customer_service`, `contact_human_agent`, `create_edit_switch_account`, `delete_account`, `delivery_options`, `delivery_period`, `get_check_invoice`, `get_refund`, `newsletter_subscription`, `payment_issue`, `place_order`, `recover_password`, `registration_problems`, `review`, `track_order`, `track_refund`, `other`

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/finance/models/finclf_customer_service_intent_type_en_1.0.0_3.0_1675427852317.zip){:.button.button-orange}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/finance/models/finclf_customer_service_intent_type_en_1.0.0_3.0_1675427852317.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}

```python
document_assembler = nlp.DocumentAssembler() \
    .setInputCol(&quot;text&quot;) \
    .setOutputCol(&quot;document&quot;)

embeddings = nlp.UniversalSentenceEncoder.pretrained() \
    .setInputCols(&quot;document&quot;) \
    .setOutputCol(&quot;sentence_embeddings&quot;)

docClassifier = finance.ClassifierDLModel.pretrained(&quot;finclf_customer_service_intent_type&quot;, &quot;en&quot;, &quot;finance/models&quot;)\
    .setInputCols(&quot;sentence_embeddings&quot;) \
    .setOutputCol(&quot;class&quot;)

pipeline = nlp.Pipeline().setStages(
      [
        document_assembler,
        embeddings,
        docClassifier
      ]
    )

empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;)
model = pipeline.fit(empty_data)
light_model = nlp.LightPipeline(model)

result = light_model.annotate(&quot;&quot;&quot;I have a problem with the deletion of my Premium account.&quot;&quot;&quot;)

result[&quot;class&quot;]
```

&lt;/div&gt;

## Results

```bash
['delete_account']
```

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|finclf_customer_service_intent_type|
|Compatibility:|Finance NLP 1.0.0+|
|License:|Licensed|
|Edition:|Official|
|Input Labels:|[sentence_embeddings]|
|Output Labels:|[class]|
|Language:|en|
|Size:|22.8 MB|

## References

https://github.com/bitext/customer-support-intent-detection-evaluation-dataset

## Benchmarking

```bash
label                          precision  recall  f1-score  support 
cancel_order                   0.88       1.00    0.94      30      
change_order                   1.00       0.90    0.95      30      
change_setup_shipping_address  0.97       0.97    0.97      36      
check_cancellation_fee         0.97       0.97    0.97      30      
check_payment_methods          0.97       0.93    0.95      30      
check_refund_policy            0.97       0.97    0.97      30      
complaint                      0.93       0.93    0.93      30      
contact_customer_service       1.00       1.00    1.00      30      
contact_human_agent            0.97       0.97    0.97      30      
create_edit_switch_account     0.90       0.97    0.93      36      
delete_account                 0.96       0.87    0.91      30      
delivery_options               0.91       1.00    0.95      30      
delivery_period                1.00       0.97    0.98      30      
get_check_invoice              0.92       0.97    0.95      36      
get_refund                     1.00       0.87    0.93      30      
newsletter_subscription        1.00       0.93    0.97      30      
other                          1.00       0.92    0.96      38      
payment_issue                  0.97       1.00    0.98      30      
place_order                    0.97       0.93    0.95      30      
recover_password               0.97       1.00    0.98      30      
registration_problems          1.00       0.97    0.98      30      
review                         0.94       1.00    0.97      30      
track_order                    0.93       0.93    0.93      30      
track_refund                   0.91       1.00    0.95      30      
accuracy                       -          -       0.96      746     
macro-avg                      0.96       0.96    0.96      746     
weighted-avg                   0.96       0.96    0.96      746      
```</content><author><name>John Snow Labs</name></author><category term="en" /><category term="licensed" /><category term="intent" /><category term="finance" /><category term="customer" /><category term="tensorflow" /><summary type="html">Description This is a Text Classification model that can help you classify a chat message from customer service according to intent type. Predicted Entities cancel_order, change_order, change_setup_shipping_address, check_cancellation_fee, check_payment_methods, check_refund_policy, complaint, contact_customer_service, contact_human_agent, create_edit_switch_account, delete_account, delivery_options, delivery_period, get_check_invoice, get_refund, newsletter_subscription, payment_issue, place_order, recover_password, registration_problems, review, track_order, track_refund, other Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = nlp.DocumentAssembler() \ .setInputCol(&quot;text&quot;) \ .setOutputCol(&quot;document&quot;) embeddings = nlp.UniversalSentenceEncoder.pretrained() \ .setInputCols(&quot;document&quot;) \ .setOutputCol(&quot;sentence_embeddings&quot;) docClassifier = finance.ClassifierDLModel.pretrained(&quot;finclf_customer_service_intent_type&quot;, &quot;en&quot;, &quot;finance/models&quot;)\ .setInputCols(&quot;sentence_embeddings&quot;) \ .setOutputCol(&quot;class&quot;) pipeline = nlp.Pipeline().setStages( [ document_assembler, embeddings, docClassifier ] ) empty_data = spark.createDataFrame([[&quot;&quot;]]).toDF(&quot;text&quot;) model = pipeline.fit(empty_data) light_model = nlp.LightPipeline(model) result = light_model.annotate(&quot;&quot;&quot;I have a problem with the deletion of my Premium account.&quot;&quot;&quot;) result[&quot;class&quot;] Results ['delete_account'] Model Information Model Name: finclf_customer_service_intent_type Compatibility: Finance NLP 1.0.0+ License: Licensed Edition: Official Input Labels: [sentence_embeddings] Output Labels: [class] Language: en Size: 22.8 MB References https://github.com/bitext/customer-support-intent-detection-evaluation-dataset Benchmarking label precision recall f1-score support cancel_order 0.88 1.00 0.94 30 change_order 1.00 0.90 0.95 30 change_setup_shipping_address 0.97 0.97 0.97 36 check_cancellation_fee 0.97 0.97 0.97 30 check_payment_methods 0.97 0.93 0.95 30 check_refund_policy 0.97 0.97 0.97 30 complaint 0.93 0.93 0.93 30 contact_customer_service 1.00 1.00 1.00 30 contact_human_agent 0.97 0.97 0.97 30 create_edit_switch_account 0.90 0.97 0.93 36 delete_account 0.96 0.87 0.91 30 delivery_options 0.91 1.00 0.95 30 delivery_period 1.00 0.97 0.98 30 get_check_invoice 0.92 0.97 0.95 36 get_refund 1.00 0.87 0.93 30 newsletter_subscription 1.00 0.93 0.97 30 other 1.00 0.92 0.96 38 payment_issue 0.97 1.00 0.98 30 place_order 0.97 0.93 0.95 30 recover_password 0.97 1.00 0.98 30 registration_problems 1.00 0.97 0.98 30 review 0.94 1.00 0.97 30 track_order 0.93 0.93 0.93 30 track_refund 0.91 1.00 0.95 30 accuracy - - 0.96 746 macro-avg 0.96 0.96 0.96 746 weighted-avg 0.96 0.96 0.96 746</summary></entry></feed>