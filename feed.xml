<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2023-08-24T07:56:46+00:00</updated><id>/feed.xml</id><title type="html">Spark NLP</title><subtitle>High Performance NLP with Apache Spark
</subtitle><author><name>{&quot;type&quot;=&gt;nil, &quot;name&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;avatar&quot;=&gt;nil, &quot;bio&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;facebook&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;weibo&quot;=&gt;nil, &quot;googleplus&quot;=&gt;nil, &quot;telegram&quot;=&gt;nil, &quot;medium&quot;=&gt;nil, &quot;zhihu&quot;=&gt;nil, &quot;douban&quot;=&gt;nil, &quot;linkedin&quot;=&gt;nil, &quot;github&quot;=&gt;nil, &quot;npm&quot;=&gt;nil}</name></author><entry><title type="html">BAAI general embedding English (bge_base)</title><link href="/2023/08/15/bge_base_en.html" rel="alternate" type="text/html" title="BAAI general embedding English (bge_base)" /><published>2023-08-15T00:00:00+00:00</published><updated>2023-08-15T00:00:00+00:00</updated><id>/2023/08/15/bge_base_en</id><content type="html" xml:base="/2023/08/15/bge_base_en.html">## Description

FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification,  clustering, or semantic search.
And it also can be used in vector database for LLMs.

`bge` is short for `BAAI general embedding`.

|              Model              | Language | Description | query instruction for retrieval\* |
|:-------------------------------|:--------:| :--------:| :--------:|
|  [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en) |   English |  rank **1st** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |
|  [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en) |   English |  rank **2nd** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |
|  [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en) |   English | a small-scale model but with competitive performance  | `Represent this sentence for searching relevant passages: `  |
|  [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) |   Chinese | rank **1st** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark | `为这个句子生成表示以用于检索相关文章：`  |
|  [BAAI/bge-large-zh-noinstruct](https://huggingface.co/BAAI/bge-large-zh-noinstruct) |   Chinese | This model is trained without instruction, and rank **2nd** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark |   |
|  [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) |   Chinese |  a base-scale model but has similar ability with `bge-large-zh` | `为这个句子生成表示以用于检索相关文章：`  |
|  [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) |   Chinese | a small-scale model but with competitive performance | `为这个句子生成表示以用于检索相关文章：`  |

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bge_base_en_5.0.2_3.0_1692109953168.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/bge_base_en_5.0.2_3.0_1692109953168.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
document = DocumentAssembler()\ 
    .setInputCol(&quot;text&quot;)\ 
    .setOutputCol(&quot;document&quot;)

tokenizer = Tokenizer()\ 
    .setInputCols([&quot;document&quot;])\ 
    .setOutputCol(&quot;token&quot;) 

embeddings = BertEmbeddings.pretrained(&quot;bge_base&quot;, &quot;en&quot;)\ 
    .setInputCols([&quot;document&quot;, &quot;token&quot;])\ 
    .setOutputCol(&quot;embeddings&quot;)

```
```scala

val document = new DocumentAssembler()
  .setInputCol(&quot;text&quot;)
  .setOutputCol(&quot;document&quot;)

val tokenizer = new Tokenizer() 
    .setInputCols(&quot;document&quot;) 
    .setOutputCol(&quot;token&quot;)
    
val embeddings = BertEmbeddings.pretrained(&quot;bge_base&quot;, &quot;en&quot;)
    .setInputCols(&quot;document&quot;, &quot;token&quot;)
    .setOutputCol(&quot;embeddings&quot;)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|bge_base|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|258.8 MB|
|Case sensitive:|true|

## References

BAAI models are from [BAAI](https://huggingface.co/BAAI)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="bert" /><category term="embeddings" /><category term="english" /><category term="en" /><category term="onnx" /><summary type="html">Description FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. And it also can be used in vector database for LLMs. bge is short for BAAI general embedding. Model Language Description query instruction for retrieval* BAAI/bge-large-en English rank 1st in MTEB leaderboard Represent this sentence for searching relevant passages: BAAI/bge-base-en English rank 2nd in MTEB leaderboard Represent this sentence for searching relevant passages: BAAI/bge-small-en English a small-scale model but with competitive performance Represent this sentence for searching relevant passages: BAAI/bge-large-zh Chinese rank 1st in C-MTEB benchmark 为这个句子生成表示以用于检索相关文章： BAAI/bge-large-zh-noinstruct Chinese This model is trained without instruction, and rank 2nd in C-MTEB benchmark   BAAI/bge-base-zh Chinese a base-scale model but has similar ability with bge-large-zh 为这个句子生成表示以用于检索相关文章： BAAI/bge-small-zh Chinese a small-scale model but with competitive performance 为这个句子生成表示以用于检索相关文章： Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document = DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer()\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;token&quot;) embeddings = BertEmbeddings.pretrained(&quot;bge_base&quot;, &quot;en&quot;)\ .setInputCols([&quot;document&quot;, &quot;token&quot;])\ .setOutputCol(&quot;embeddings&quot;) val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained(&quot;bge_base&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) Model Information Model Name: bge_base Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [document, token] Output Labels: [embeddings] Language: en Size: 258.8 MB Case sensitive: true References BAAI models are from BAAI</summary></entry><entry><title type="html">BAAI general embedding English (bge_large)</title><link href="/2023/08/15/bge_large_en.html" rel="alternate" type="text/html" title="BAAI general embedding English (bge_large)" /><published>2023-08-15T00:00:00+00:00</published><updated>2023-08-15T00:00:00+00:00</updated><id>/2023/08/15/bge_large_en</id><content type="html" xml:base="/2023/08/15/bge_large_en.html">## Description

FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification,  clustering, or semantic search.
And it also can be used in vector database for LLMs.

`bge` is short for `BAAI general embedding`.

|              Model              | Language | Description | query instruction for retrieval\* |
|:-------------------------------|:--------:| :--------:| :--------:|
|  [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en) |   English |  rank **1st** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |
|  [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en) |   English |  rank **2nd** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |
|  [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en) |   English | a small-scale model but with competitive performance  | `Represent this sentence for searching relevant passages: `  |
|  [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) |   Chinese | rank **1st** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark | `为这个句子生成表示以用于检索相关文章：`  |
|  [BAAI/bge-large-zh-noinstruct](https://huggingface.co/BAAI/bge-large-zh-noinstruct) |   Chinese | This model is trained without instruction, and rank **2nd** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark |   |
|  [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) |   Chinese |  a base-scale model but has similar ability with `bge-large-zh` | `为这个句子生成表示以用于检索相关文章：`  |
|  [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) |   Chinese | a small-scale model but with competitive performance | `为这个句子生成表示以用于检索相关文章：`  |

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bge_large_en_5.0.2_3.0_1692109963281.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/bge_large_en_5.0.2_3.0_1692109963281.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
document = DocumentAssembler()\ 
    .setInputCol(&quot;text&quot;)\ 
    .setOutputCol(&quot;document&quot;)

tokenizer = Tokenizer()\ 
    .setInputCols([&quot;document&quot;])\ 
    .setOutputCol(&quot;token&quot;) 

embeddings = BertEmbeddings.pretrained(&quot;bge_large&quot;, &quot;en&quot;)\ 
    .setInputCols([&quot;document&quot;, &quot;token&quot;])\ 
    .setOutputCol(&quot;embeddings&quot;)

```
```scala

val document = new DocumentAssembler()
  .setInputCol(&quot;text&quot;)
  .setOutputCol(&quot;document&quot;)

val tokenizer = new Tokenizer() 
    .setInputCols(&quot;document&quot;) 
    .setOutputCol(&quot;token&quot;)
    
val embeddings = BertEmbeddings.pretrained(&quot;bge_large&quot;, &quot;en&quot;)
    .setInputCols(&quot;document&quot;, &quot;token&quot;)
    .setOutputCol(&quot;embeddings&quot;)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|bge_large|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|794.2 MB|
|Case sensitive:|true|

## References

BAAI models are from [BAAI](https://huggingface.co/BAAI)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="bert" /><category term="embeddings" /><category term="english" /><category term="en" /><category term="onnx" /><summary type="html">Description FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. And it also can be used in vector database for LLMs. bge is short for BAAI general embedding. Model Language Description query instruction for retrieval* BAAI/bge-large-en English rank 1st in MTEB leaderboard Represent this sentence for searching relevant passages: BAAI/bge-base-en English rank 2nd in MTEB leaderboard Represent this sentence for searching relevant passages: BAAI/bge-small-en English a small-scale model but with competitive performance Represent this sentence for searching relevant passages: BAAI/bge-large-zh Chinese rank 1st in C-MTEB benchmark 为这个句子生成表示以用于检索相关文章： BAAI/bge-large-zh-noinstruct Chinese This model is trained without instruction, and rank 2nd in C-MTEB benchmark   BAAI/bge-base-zh Chinese a base-scale model but has similar ability with bge-large-zh 为这个句子生成表示以用于检索相关文章： BAAI/bge-small-zh Chinese a small-scale model but with competitive performance 为这个句子生成表示以用于检索相关文章： Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document = DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer()\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;token&quot;) embeddings = BertEmbeddings.pretrained(&quot;bge_large&quot;, &quot;en&quot;)\ .setInputCols([&quot;document&quot;, &quot;token&quot;])\ .setOutputCol(&quot;embeddings&quot;) val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained(&quot;bge_large&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) Model Information Model Name: bge_large Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [document, token] Output Labels: [embeddings] Language: en Size: 794.2 MB Case sensitive: true References BAAI models are from BAAI</summary></entry><entry><title type="html">BAAI general embedding English (bge_small)</title><link href="/2023/08/15/bge_small_en.html" rel="alternate" type="text/html" title="BAAI general embedding English (bge_small)" /><published>2023-08-15T00:00:00+00:00</published><updated>2023-08-15T00:00:00+00:00</updated><id>/2023/08/15/bge_small_en</id><content type="html" xml:base="/2023/08/15/bge_small_en.html">## Description

FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification,  clustering, or semantic search.
And it also can be used in vector database for LLMs.

`bge` is short for `BAAI general embedding`.

|              Model              | Language | Description | query instruction for retrieval\* |
|:-------------------------------|:--------:| :--------:| :--------:|
|  [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en) |   English |  rank **1st** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |
|  [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en) |   English |  rank **2nd** in [MTEB](https://huggingface.co/spaces/mteb/leaderboard) leaderboard | `Represent this sentence for searching relevant passages: `  |
|  [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en) |   English | a small-scale model but with competitive performance  | `Represent this sentence for searching relevant passages: `  |
|  [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh) |   Chinese | rank **1st** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark | `为这个句子生成表示以用于检索相关文章：`  |
|  [BAAI/bge-large-zh-noinstruct](https://huggingface.co/BAAI/bge-large-zh-noinstruct) |   Chinese | This model is trained without instruction, and rank **2nd** in [C-MTEB](https://github.com/FlagOpen/FlagEmbedding/tree/master/C_MTEB) benchmark |   |
|  [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh) |   Chinese |  a base-scale model but has similar ability with `bge-large-zh` | `为这个句子生成表示以用于检索相关文章：`  |
|  [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh) |   Chinese | a small-scale model but with competitive performance | `为这个句子生成表示以用于检索相关文章：`  |

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bge_small_en_5.0.2_3.0_1692109948749.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/bge_small_en_5.0.2_3.0_1692109948749.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
document = DocumentAssembler()\ 
    .setInputCol(&quot;text&quot;)\ 
    .setOutputCol(&quot;document&quot;)

tokenizer = Tokenizer()\ 
    .setInputCols([&quot;document&quot;])\ 
    .setOutputCol(&quot;token&quot;) 

embeddings = BertEmbeddings.pretrained(&quot;bge_small&quot;, &quot;en&quot;)\ 
    .setInputCols([&quot;document&quot;, &quot;token&quot;])\ 
    .setOutputCol(&quot;embeddings&quot;)

```
```scala

val document = new DocumentAssembler()
  .setInputCol(&quot;text&quot;)
  .setOutputCol(&quot;document&quot;)

val tokenizer = new Tokenizer() 
    .setInputCols(&quot;document&quot;) 
    .setOutputCol(&quot;token&quot;)
    
val embeddings = BertEmbeddings.pretrained(&quot;bge_small&quot;, &quot;en&quot;)
    .setInputCols(&quot;document&quot;, &quot;token&quot;)
    .setOutputCol(&quot;embeddings&quot;)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|bge_small|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|79.9 MB|
|Case sensitive:|true|

## References

BAAI models are from [BAAI](https://huggingface.co/BAAI)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="bert" /><category term="embeddings" /><category term="english" /><category term="en" /><category term="onnx" /><summary type="html">Description FlagEmbedding can map any text to a low-dimensional dense vector which can be used for tasks like retrieval, classification, clustering, or semantic search. And it also can be used in vector database for LLMs. bge is short for BAAI general embedding. Model Language Description query instruction for retrieval* BAAI/bge-large-en English rank 1st in MTEB leaderboard Represent this sentence for searching relevant passages: BAAI/bge-base-en English rank 2nd in MTEB leaderboard Represent this sentence for searching relevant passages: BAAI/bge-small-en English a small-scale model but with competitive performance Represent this sentence for searching relevant passages: BAAI/bge-large-zh Chinese rank 1st in C-MTEB benchmark 为这个句子生成表示以用于检索相关文章： BAAI/bge-large-zh-noinstruct Chinese This model is trained without instruction, and rank 2nd in C-MTEB benchmark   BAAI/bge-base-zh Chinese a base-scale model but has similar ability with bge-large-zh 为这个句子生成表示以用于检索相关文章： BAAI/bge-small-zh Chinese a small-scale model but with competitive performance 为这个句子生成表示以用于检索相关文章： Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document = DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer()\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;token&quot;) embeddings = BertEmbeddings.pretrained(&quot;bge_small&quot;, &quot;en&quot;)\ .setInputCols([&quot;document&quot;, &quot;token&quot;])\ .setOutputCol(&quot;embeddings&quot;) val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained(&quot;bge_small&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) Model Information Model Name: bge_small Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [document, token] Output Labels: [embeddings] Language: en Size: 79.9 MB Case sensitive: true References BAAI models are from BAAI</summary></entry><entry><title type="html">General Text Embeddings (GTE) English (gte_base)</title><link href="/2023/08/15/gte_base_en.html" rel="alternate" type="text/html" title="General Text Embeddings (GTE) English (gte_base)" /><published>2023-08-15T00:00:00+00:00</published><updated>2023-08-15T00:00:00+00:00</updated><id>/2023/08/15/gte_base_en</id><content type="html" xml:base="/2023/08/15/gte_base_en.html">## Description

General Text Embeddings (GTE) model. [Towards General Text Embeddings with Multi-stage Contrastive Learning](https://arxiv.org/abs/2308.03281)

The GTE models are trained by Alibaba DAMO Academy. They are mainly based on the BERT framework and currently offer three different sizes of models, including [GTE-large](https://huggingface.co/thenlper/gte-large), [GTE-base](https://huggingface.co/thenlper/gte-base), and [GTE-small](https://huggingface.co/thenlper/gte-small). The GTE models are trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. This enables the GTE models to be applied to various downstream tasks of text embeddings, including **information retrieval**, **semantic textual similarity**, **text reranking**, etc.

| Model Name | Model Size (GB) | Dimension | Sequence Length | Average (56) | Clustering (11) | Pair Classification (3) | Reranking (4) | Retrieval (15) | STS (10) | Summarization (1) | Classification (12) |
|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| [**gte-large**](https://huggingface.co/thenlper/gte-large) | 0.67 | 1024 | 512 | **63.13** | 46.84 | 85.00 | 59.13 | 52.22 | 83.35 | 31.66 | 73.33 |
| [**gte-base**](https://huggingface.co/thenlper/gte-base) 	| 0.22 | 768 | 512 | **62.39** | 46.2 | 84.57 | 58.61 | 51.14 | 82.3 | 31.17 | 73.01 |
| [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2) | 1.34 | 1024| 512 | 62.25 | 44.49 | 86.03 | 56.61 | 50.56 | 82.05 | 30.19 | 75.24 |
| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2) | 0.44 | 768 | 512 | 61.5 | 43.80 | 85.73 | 55.91 | 50.29 | 81.05 | 30.28 | 73.84 |
| [**gte-small**](https://huggingface.co/thenlper/gte-small) | 0.07 | 384 | 512 | **61.36** | 44.89 | 83.54 | 57.7 | 49.46 | 82.07 | 30.42 | 72.31 |
| [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings) | - | 1536 | 8192 | 60.99 | 45.9 | 84.89 | 56.32 | 49.25 | 80.97 | 30.8 | 70.93 |
| [e5-small-v2](https://huggingface.co/intfloat/e5-base-v2) | 0.13 | 384 | 512 | 59.93 | 39.92 | 84.67 | 54.32 | 49.04 | 80.39 | 31.16 | 72.94 |
| [sentence-t5-xxl](https://huggingface.co/sentence-transformers/sentence-t5-xxl) | 9.73 | 768 | 512 | 59.51 | 43.72 | 85.06 | 56.42 | 42.24 | 82.63 | 30.08 | 73.42 |
| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) 	| 0.44 | 768 | 514 	| 57.78 | 43.69 | 83.04 | 59.36 | 43.81 | 80.28 | 27.49 | 65.07 |
| [sgpt-bloom-7b1-msmarco](https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco) 	| 28.27 | 4096 | 2048 | 57.59 | 38.93 | 81.9 | 55.65 | 48.22 | 77.74 | 33.6 | 66.19 |
| [all-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2) 	| 0.13 | 384 | 512 	| 56.53 | 41.81 | 82.41 | 58.44 | 42.69 | 79.8 | 27.9 | 63.21 |
| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) 	| 0.09 | 384 | 512 	| 56.26 | 42.35 | 82.37 | 58.04 | 41.95 | 78.9 | 30.81 | 63.05 |
| [contriever-base-msmarco](https://huggingface.co/nthakur/contriever-base-msmarco) 	| 0.44 | 768 | 512 	| 56.00 | 41.1 	| 82.54 | 53.14 | 41.88 | 76.51 | 30.36 | 66.68 |
| [sentence-t5-base](https://huggingface.co/sentence-transformers/sentence-t5-base) 	| 0.22 | 768 | 512 	| 55.27 | 40.21 | 85.18 | 53.09 | 33.63 | 81.14 | 31.39 | 69.81 |

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/gte_base_en_5.0.2_3.0_1692109612831.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/gte_base_en_5.0.2_3.0_1692109612831.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
document = DocumentAssembler()\ 
    .setInputCol(&quot;text&quot;)\ 
    .setOutputCol(&quot;document&quot;)

tokenizer = Tokenizer()\ 
    .setInputCols([&quot;document&quot;])\ 
    .setOutputCol(&quot;token&quot;) 

embeddings = BertEmbeddings.pretrained(&quot;gte_base&quot;, &quot;en&quot;)\ 
    .setInputCols([&quot;document&quot;, &quot;token&quot;])\ 
    .setOutputCol(&quot;embeddings&quot;)

```
```scala

val document = new DocumentAssembler()
  .setInputCol(&quot;text&quot;)
  .setOutputCol(&quot;document&quot;)

val tokenizer = new Tokenizer() 
    .setInputCols(&quot;document&quot;) 
    .setOutputCol(&quot;token&quot;)
    
val embeddings = BertEmbeddings.pretrained(&quot;gte_base&quot;, &quot;en&quot;)
    .setInputCols(&quot;document&quot;, &quot;token&quot;)
    .setOutputCol(&quot;embeddings&quot;)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|gte_base|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|258.8 MB|
|Case sensitive:|true|

## References

GTE models are from [Dingkun Long](https://huggingface.co/thenlper)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="bert" /><category term="embeddings" /><category term="english" /><category term="en" /><category term="onnx" /><summary type="html">Description General Text Embeddings (GTE) model. Towards General Text Embeddings with Multi-stage Contrastive Learning The GTE models are trained by Alibaba DAMO Academy. They are mainly based on the BERT framework and currently offer three different sizes of models, including GTE-large, GTE-base, and GTE-small. The GTE models are trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. This enables the GTE models to be applied to various downstream tasks of text embeddings, including information retrieval, semantic textual similarity, text reranking, etc. Model Name Model Size (GB) Dimension Sequence Length Average (56) Clustering (11) Pair Classification (3) Reranking (4) Retrieval (15) STS (10) Summarization (1) Classification (12) gte-large 0.67 1024 512 63.13 46.84 85.00 59.13 52.22 83.35 31.66 73.33 gte-base 0.22 768 512 62.39 46.2 84.57 58.61 51.14 82.3 31.17 73.01 e5-large-v2 1.34 1024 512 62.25 44.49 86.03 56.61 50.56 82.05 30.19 75.24 e5-base-v2 0.44 768 512 61.5 43.80 85.73 55.91 50.29 81.05 30.28 73.84 gte-small 0.07 384 512 61.36 44.89 83.54 57.7 49.46 82.07 30.42 72.31 text-embedding-ada-002 - 1536 8192 60.99 45.9 84.89 56.32 49.25 80.97 30.8 70.93 e5-small-v2 0.13 384 512 59.93 39.92 84.67 54.32 49.04 80.39 31.16 72.94 sentence-t5-xxl 9.73 768 512 59.51 43.72 85.06 56.42 42.24 82.63 30.08 73.42 all-mpnet-base-v2 0.44 768 514 57.78 43.69 83.04 59.36 43.81 80.28 27.49 65.07 sgpt-bloom-7b1-msmarco 28.27 4096 2048 57.59 38.93 81.9 55.65 48.22 77.74 33.6 66.19 all-MiniLM-L12-v2 0.13 384 512 56.53 41.81 82.41 58.44 42.69 79.8 27.9 63.21 all-MiniLM-L6-v2 0.09 384 512 56.26 42.35 82.37 58.04 41.95 78.9 30.81 63.05 contriever-base-msmarco 0.44 768 512 56.00 41.1 82.54 53.14 41.88 76.51 30.36 66.68 sentence-t5-base 0.22 768 512 55.27 40.21 85.18 53.09 33.63 81.14 31.39 69.81 Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document = DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer()\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;token&quot;) embeddings = BertEmbeddings.pretrained(&quot;gte_base&quot;, &quot;en&quot;)\ .setInputCols([&quot;document&quot;, &quot;token&quot;])\ .setOutputCol(&quot;embeddings&quot;) val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained(&quot;gte_base&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) Model Information Model Name: gte_base Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [document, token] Output Labels: [embeddings] Language: en Size: 258.8 MB Case sensitive: true References GTE models are from Dingkun Long</summary></entry><entry><title type="html">General Text Embeddings (GTE) English (gte_large)</title><link href="/2023/08/15/gte_large_en.html" rel="alternate" type="text/html" title="General Text Embeddings (GTE) English (gte_large)" /><published>2023-08-15T00:00:00+00:00</published><updated>2023-08-15T00:00:00+00:00</updated><id>/2023/08/15/gte_large_en</id><content type="html" xml:base="/2023/08/15/gte_large_en.html">## Description

General Text Embeddings (GTE) model. [Towards General Text Embeddings with Multi-stage Contrastive Learning](https://arxiv.org/abs/2308.03281)

The GTE models are trained by Alibaba DAMO Academy. They are mainly based on the BERT framework and currently offer three different sizes of models, including [GTE-large](https://huggingface.co/thenlper/gte-large), [GTE-base](https://huggingface.co/thenlper/gte-base), and [GTE-small](https://huggingface.co/thenlper/gte-small). The GTE models are trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. This enables the GTE models to be applied to various downstream tasks of text embeddings, including **information retrieval**, **semantic textual similarity**, **text reranking**, etc.

| Model Name | Model Size (GB) | Dimension | Sequence Length | Average (56) | Clustering (11) | Pair Classification (3) | Reranking (4) | Retrieval (15) | STS (10) | Summarization (1) | Classification (12) |
|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| [**gte-large**](https://huggingface.co/thenlper/gte-large) | 0.67 | 1024 | 512 | **63.13** | 46.84 | 85.00 | 59.13 | 52.22 | 83.35 | 31.66 | 73.33 |
| [**gte-base**](https://huggingface.co/thenlper/gte-base) 	| 0.22 | 768 | 512 | **62.39** | 46.2 | 84.57 | 58.61 | 51.14 | 82.3 | 31.17 | 73.01 |
| [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2) | 1.34 | 1024| 512 | 62.25 | 44.49 | 86.03 | 56.61 | 50.56 | 82.05 | 30.19 | 75.24 |
| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2) | 0.44 | 768 | 512 | 61.5 | 43.80 | 85.73 | 55.91 | 50.29 | 81.05 | 30.28 | 73.84 |
| [**gte-small**](https://huggingface.co/thenlper/gte-small) | 0.07 | 384 | 512 | **61.36** | 44.89 | 83.54 | 57.7 | 49.46 | 82.07 | 30.42 | 72.31 |
| [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings) | - | 1536 | 8192 | 60.99 | 45.9 | 84.89 | 56.32 | 49.25 | 80.97 | 30.8 | 70.93 |
| [e5-small-v2](https://huggingface.co/intfloat/e5-base-v2) | 0.13 | 384 | 512 | 59.93 | 39.92 | 84.67 | 54.32 | 49.04 | 80.39 | 31.16 | 72.94 |
| [sentence-t5-xxl](https://huggingface.co/sentence-transformers/sentence-t5-xxl) | 9.73 | 768 | 512 | 59.51 | 43.72 | 85.06 | 56.42 | 42.24 | 82.63 | 30.08 | 73.42 |
| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) 	| 0.44 | 768 | 514 	| 57.78 | 43.69 | 83.04 | 59.36 | 43.81 | 80.28 | 27.49 | 65.07 |
| [sgpt-bloom-7b1-msmarco](https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco) 	| 28.27 | 4096 | 2048 | 57.59 | 38.93 | 81.9 | 55.65 | 48.22 | 77.74 | 33.6 | 66.19 |
| [all-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2) 	| 0.13 | 384 | 512 	| 56.53 | 41.81 | 82.41 | 58.44 | 42.69 | 79.8 | 27.9 | 63.21 |
| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) 	| 0.09 | 384 | 512 	| 56.26 | 42.35 | 82.37 | 58.04 | 41.95 | 78.9 | 30.81 | 63.05 |
| [contriever-base-msmarco](https://huggingface.co/nthakur/contriever-base-msmarco) 	| 0.44 | 768 | 512 	| 56.00 | 41.1 	| 82.54 | 53.14 | 41.88 | 76.51 | 30.36 | 66.68 |
| [sentence-t5-base](https://huggingface.co/sentence-transformers/sentence-t5-base) 	| 0.22 | 768 | 512 	| 55.27 | 40.21 | 85.18 | 53.09 | 33.63 | 81.14 | 31.39 | 69.81 |

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/gte_large_en_5.0.2_3.0_1692109628705.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/gte_large_en_5.0.2_3.0_1692109628705.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
document = DocumentAssembler()\ 
    .setInputCol(&quot;text&quot;)\ 
    .setOutputCol(&quot;document&quot;)

tokenizer = Tokenizer()\ 
    .setInputCols([&quot;document&quot;])\ 
    .setOutputCol(&quot;token&quot;) 

embeddings = BertEmbeddings.pretrained(&quot;gte_large&quot;, &quot;en&quot;)\ 
    .setInputCols([&quot;document&quot;, &quot;token&quot;])\ 
    .setOutputCol(&quot;embeddings&quot;)

```
```scala

val document = new DocumentAssembler()
  .setInputCol(&quot;text&quot;)
  .setOutputCol(&quot;document&quot;)

val tokenizer = new Tokenizer() 
    .setInputCols(&quot;document&quot;) 
    .setOutputCol(&quot;token&quot;)
    
val embeddings = BertEmbeddings.pretrained(&quot;gte_large&quot;, &quot;en&quot;)
    .setInputCols(&quot;document&quot;, &quot;token&quot;)
    .setOutputCol(&quot;embeddings&quot;)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|gte_large|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|794.2 MB|
|Case sensitive:|true|

## References

GTE models are from [Dingkun Long](https://huggingface.co/thenlper)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="bert" /><category term="embeddings" /><category term="english" /><category term="en" /><category term="onnx" /><summary type="html">Description General Text Embeddings (GTE) model. Towards General Text Embeddings with Multi-stage Contrastive Learning The GTE models are trained by Alibaba DAMO Academy. They are mainly based on the BERT framework and currently offer three different sizes of models, including GTE-large, GTE-base, and GTE-small. The GTE models are trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. This enables the GTE models to be applied to various downstream tasks of text embeddings, including information retrieval, semantic textual similarity, text reranking, etc. Model Name Model Size (GB) Dimension Sequence Length Average (56) Clustering (11) Pair Classification (3) Reranking (4) Retrieval (15) STS (10) Summarization (1) Classification (12) gte-large 0.67 1024 512 63.13 46.84 85.00 59.13 52.22 83.35 31.66 73.33 gte-base 0.22 768 512 62.39 46.2 84.57 58.61 51.14 82.3 31.17 73.01 e5-large-v2 1.34 1024 512 62.25 44.49 86.03 56.61 50.56 82.05 30.19 75.24 e5-base-v2 0.44 768 512 61.5 43.80 85.73 55.91 50.29 81.05 30.28 73.84 gte-small 0.07 384 512 61.36 44.89 83.54 57.7 49.46 82.07 30.42 72.31 text-embedding-ada-002 - 1536 8192 60.99 45.9 84.89 56.32 49.25 80.97 30.8 70.93 e5-small-v2 0.13 384 512 59.93 39.92 84.67 54.32 49.04 80.39 31.16 72.94 sentence-t5-xxl 9.73 768 512 59.51 43.72 85.06 56.42 42.24 82.63 30.08 73.42 all-mpnet-base-v2 0.44 768 514 57.78 43.69 83.04 59.36 43.81 80.28 27.49 65.07 sgpt-bloom-7b1-msmarco 28.27 4096 2048 57.59 38.93 81.9 55.65 48.22 77.74 33.6 66.19 all-MiniLM-L12-v2 0.13 384 512 56.53 41.81 82.41 58.44 42.69 79.8 27.9 63.21 all-MiniLM-L6-v2 0.09 384 512 56.26 42.35 82.37 58.04 41.95 78.9 30.81 63.05 contriever-base-msmarco 0.44 768 512 56.00 41.1 82.54 53.14 41.88 76.51 30.36 66.68 sentence-t5-base 0.22 768 512 55.27 40.21 85.18 53.09 33.63 81.14 31.39 69.81 Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document = DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer()\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;token&quot;) embeddings = BertEmbeddings.pretrained(&quot;gte_large&quot;, &quot;en&quot;)\ .setInputCols([&quot;document&quot;, &quot;token&quot;])\ .setOutputCol(&quot;embeddings&quot;) val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained(&quot;gte_large&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) Model Information Model Name: gte_large Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [document, token] Output Labels: [embeddings] Language: en Size: 794.2 MB Case sensitive: true References GTE models are from Dingkun Long</summary></entry><entry><title type="html">General Text Embeddings (GTE) English (gte_small)</title><link href="/2023/08/15/gte_small_en.html" rel="alternate" type="text/html" title="General Text Embeddings (GTE) English (gte_small)" /><published>2023-08-15T00:00:00+00:00</published><updated>2023-08-15T00:00:00+00:00</updated><id>/2023/08/15/gte_small_en</id><content type="html" xml:base="/2023/08/15/gte_small_en.html">## Description

General Text Embeddings (GTE) model. [Towards General Text Embeddings with Multi-stage Contrastive Learning](https://arxiv.org/abs/2308.03281)

The GTE models are trained by Alibaba DAMO Academy. They are mainly based on the BERT framework and currently offer three different sizes of models, including [GTE-large](https://huggingface.co/thenlper/gte-large), [GTE-base](https://huggingface.co/thenlper/gte-base), and [GTE-small](https://huggingface.co/thenlper/gte-small). The GTE models are trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. This enables the GTE models to be applied to various downstream tasks of text embeddings, including **information retrieval**, **semantic textual similarity**, **text reranking**, etc.

| Model Name | Model Size (GB) | Dimension | Sequence Length | Average (56) | Clustering (11) | Pair Classification (3) | Reranking (4) | Retrieval (15) | STS (10) | Summarization (1) | Classification (12) |
|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| [**gte-large**](https://huggingface.co/thenlper/gte-large) | 0.67 | 1024 | 512 | **63.13** | 46.84 | 85.00 | 59.13 | 52.22 | 83.35 | 31.66 | 73.33 |
| [**gte-base**](https://huggingface.co/thenlper/gte-base) 	| 0.22 | 768 | 512 | **62.39** | 46.2 | 84.57 | 58.61 | 51.14 | 82.3 | 31.17 | 73.01 |
| [e5-large-v2](https://huggingface.co/intfloat/e5-large-v2) | 1.34 | 1024| 512 | 62.25 | 44.49 | 86.03 | 56.61 | 50.56 | 82.05 | 30.19 | 75.24 |
| [e5-base-v2](https://huggingface.co/intfloat/e5-base-v2) | 0.44 | 768 | 512 | 61.5 | 43.80 | 85.73 | 55.91 | 50.29 | 81.05 | 30.28 | 73.84 |
| [**gte-small**](https://huggingface.co/thenlper/gte-small) | 0.07 | 384 | 512 | **61.36** | 44.89 | 83.54 | 57.7 | 49.46 | 82.07 | 30.42 | 72.31 |
| [text-embedding-ada-002](https://platform.openai.com/docs/guides/embeddings) | - | 1536 | 8192 | 60.99 | 45.9 | 84.89 | 56.32 | 49.25 | 80.97 | 30.8 | 70.93 |
| [e5-small-v2](https://huggingface.co/intfloat/e5-base-v2) | 0.13 | 384 | 512 | 59.93 | 39.92 | 84.67 | 54.32 | 49.04 | 80.39 | 31.16 | 72.94 |
| [sentence-t5-xxl](https://huggingface.co/sentence-transformers/sentence-t5-xxl) | 9.73 | 768 | 512 | 59.51 | 43.72 | 85.06 | 56.42 | 42.24 | 82.63 | 30.08 | 73.42 |
| [all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) 	| 0.44 | 768 | 514 	| 57.78 | 43.69 | 83.04 | 59.36 | 43.81 | 80.28 | 27.49 | 65.07 |
| [sgpt-bloom-7b1-msmarco](https://huggingface.co/bigscience/sgpt-bloom-7b1-msmarco) 	| 28.27 | 4096 | 2048 | 57.59 | 38.93 | 81.9 | 55.65 | 48.22 | 77.74 | 33.6 | 66.19 |
| [all-MiniLM-L12-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2) 	| 0.13 | 384 | 512 	| 56.53 | 41.81 | 82.41 | 58.44 | 42.69 | 79.8 | 27.9 | 63.21 |
| [all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) 	| 0.09 | 384 | 512 	| 56.26 | 42.35 | 82.37 | 58.04 | 41.95 | 78.9 | 30.81 | 63.05 |
| [contriever-base-msmarco](https://huggingface.co/nthakur/contriever-base-msmarco) 	| 0.44 | 768 | 512 	| 56.00 | 41.1 	| 82.54 | 53.14 | 41.88 | 76.51 | 30.36 | 66.68 |
| [sentence-t5-base](https://huggingface.co/sentence-transformers/sentence-t5-base) 	| 0.22 | 768 | 512 	| 55.27 | 40.21 | 85.18 | 53.09 | 33.63 | 81.14 | 31.39 | 69.81 |

{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/gte_small_en_5.0.2_3.0_1692109661017.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/gte_small_en_5.0.2_3.0_1692109661017.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
                
document = DocumentAssembler()\ 
    .setInputCol(&quot;text&quot;)\ 
    .setOutputCol(&quot;document&quot;)

tokenizer = Tokenizer()\ 
    .setInputCols([&quot;document&quot;])\ 
    .setOutputCol(&quot;token&quot;) 

embeddings = BertEmbeddings.pretrained(&quot;gte_small&quot;, &quot;en&quot;)\ 
    .setInputCols([&quot;document&quot;, &quot;token&quot;])\ 
    .setOutputCol(&quot;embeddings&quot;)

```
```scala

val document = new DocumentAssembler()
  .setInputCol(&quot;text&quot;)
  .setOutputCol(&quot;document&quot;)

val tokenizer = new Tokenizer() 
    .setInputCols(&quot;document&quot;) 
    .setOutputCol(&quot;token&quot;)
    
val embeddings = BertEmbeddings.pretrained(&quot;gte_small&quot;, &quot;en&quot;)
    .setInputCols(&quot;document&quot;, &quot;token&quot;)
    .setOutputCol(&quot;embeddings&quot;)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|gte_small|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[document, token]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|79.9 MB|
|Case sensitive:|true|

## References

GTE models are from [Dingkun Long](https://huggingface.co/thenlper)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="bert" /><category term="embeddings" /><category term="english" /><category term="en" /><category term="onnx" /><summary type="html">Description General Text Embeddings (GTE) model. Towards General Text Embeddings with Multi-stage Contrastive Learning The GTE models are trained by Alibaba DAMO Academy. They are mainly based on the BERT framework and currently offer three different sizes of models, including GTE-large, GTE-base, and GTE-small. The GTE models are trained on a large-scale corpus of relevance text pairs, covering a wide range of domains and scenarios. This enables the GTE models to be applied to various downstream tasks of text embeddings, including information retrieval, semantic textual similarity, text reranking, etc. Model Name Model Size (GB) Dimension Sequence Length Average (56) Clustering (11) Pair Classification (3) Reranking (4) Retrieval (15) STS (10) Summarization (1) Classification (12) gte-large 0.67 1024 512 63.13 46.84 85.00 59.13 52.22 83.35 31.66 73.33 gte-base 0.22 768 512 62.39 46.2 84.57 58.61 51.14 82.3 31.17 73.01 e5-large-v2 1.34 1024 512 62.25 44.49 86.03 56.61 50.56 82.05 30.19 75.24 e5-base-v2 0.44 768 512 61.5 43.80 85.73 55.91 50.29 81.05 30.28 73.84 gte-small 0.07 384 512 61.36 44.89 83.54 57.7 49.46 82.07 30.42 72.31 text-embedding-ada-002 - 1536 8192 60.99 45.9 84.89 56.32 49.25 80.97 30.8 70.93 e5-small-v2 0.13 384 512 59.93 39.92 84.67 54.32 49.04 80.39 31.16 72.94 sentence-t5-xxl 9.73 768 512 59.51 43.72 85.06 56.42 42.24 82.63 30.08 73.42 all-mpnet-base-v2 0.44 768 514 57.78 43.69 83.04 59.36 43.81 80.28 27.49 65.07 sgpt-bloom-7b1-msmarco 28.27 4096 2048 57.59 38.93 81.9 55.65 48.22 77.74 33.6 66.19 all-MiniLM-L12-v2 0.13 384 512 56.53 41.81 82.41 58.44 42.69 79.8 27.9 63.21 all-MiniLM-L6-v2 0.09 384 512 56.26 42.35 82.37 58.04 41.95 78.9 30.81 63.05 contriever-base-msmarco 0.44 768 512 56.00 41.1 82.54 53.14 41.88 76.51 30.36 66.68 sentence-t5-base 0.22 768 512 55.27 40.21 85.18 53.09 33.63 81.14 31.39 69.81 Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document = DocumentAssembler()\ .setInputCol(&quot;text&quot;)\ .setOutputCol(&quot;document&quot;) tokenizer = Tokenizer()\ .setInputCols([&quot;document&quot;])\ .setOutputCol(&quot;token&quot;) embeddings = BertEmbeddings.pretrained(&quot;gte_small&quot;, &quot;en&quot;)\ .setInputCols([&quot;document&quot;, &quot;token&quot;])\ .setOutputCol(&quot;embeddings&quot;) val document = new DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = new Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val embeddings = BertEmbeddings.pretrained(&quot;gte_small&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) Model Information Model Name: gte_small Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [document, token] Output Labels: [embeddings] Language: en Size: 79.9 MB Case sensitive: true References GTE models are from Dingkun Long</summary></entry><entry><title type="html">Bart Zero Shot Classifier Large -MNLI (bart_large_zero_shot_classifier_mnli)</title><link href="/2023/08/07/bart_large_zero_shot_classifier_mnli_en.html" rel="alternate" type="text/html" title="Bart Zero Shot Classifier Large -MNLI (bart_large_zero_shot_classifier_mnli)" /><published>2023-08-07T00:00:00+00:00</published><updated>2023-08-07T00:00:00+00:00</updated><id>/2023/08/07/bart_large_zero_shot_classifier_mnli_en</id><content type="html" xml:base="/2023/08/07/bart_large_zero_shot_classifier_mnli_en.html">## Description

This model is intended to be used for zero-shot text classification, especially in English. It is fine-tuned on MNLI by using large BART model.

BartForZeroShotClassification using a ModelForSequenceClassification trained on MNLI  tasks. Equivalent of BartForSequenceClassification models, but these models don’t require a hardcoded number of potential classes, they can be chosen at runtime. It usually means it’s slower but it is much more flexible.

We used TFBartForSequenceClassification to train this model and used BartForZeroShotClassification annotator in Spark NLP 🚀 for prediction at scale!

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/bart_large_zero_shot_classifier_mnli_en_5.1.0_3.0_1691369930633.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/bart_large_zero_shot_classifier_mnli_en_5.1.0_3.0_1691369930633.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
document_assembler = DocumentAssembler() \
.setInputCol('text') \
.setOutputCol('document')

tokenizer = Tokenizer() \
.setInputCols(['document']) \
.setOutputCol('token')

zeroShotClassifier = BartForZeroShotClassification \
.pretrained('bart_large_zero_shot_classifier_mnli', 'en') \
.setInputCols(['token', 'document']) \
.setOutputCol('class') \
.setCaseSensitive(True) \
.setMaxSentenceLength(512) \
.setCandidateLabels([&quot;urgent&quot;, &quot;mobile&quot;, &quot;travel&quot;, &quot;movie&quot;, &quot;music&quot;, &quot;sport&quot;, &quot;weather&quot;, &quot;technology&quot;])

pipeline = Pipeline(stages=[
document_assembler,
tokenizer,
zeroShotClassifier
])

example = spark.createDataFrame([['I have a problem with my iphone that needs to be resolved asap!!']]).toDF(&quot;text&quot;)
result = pipeline.fit(example).transform(example)
```
```scala
val document_assembler = DocumentAssembler()
.setInputCol(&quot;text&quot;)
.setOutputCol(&quot;document&quot;)

val tokenizer = Tokenizer()
.setInputCols(&quot;document&quot;)
.setOutputCol(&quot;token&quot;)

val zeroShotClassifier = BartForSequenceClassification.pretrained(&quot;bart_large_zero_shot_classifier_mnli&quot;, &quot;en&quot;)
.setInputCols(&quot;document&quot;, &quot;token&quot;)
.setOutputCol(&quot;class&quot;)
.setCaseSensitive(true)
.setMaxSentenceLength(512)
.setCandidateLabels(Array(&quot;urgent&quot;, &quot;mobile&quot;, &quot;travel&quot;, &quot;movie&quot;, &quot;music&quot;, &quot;sport&quot;, &quot;weather&quot;, &quot;technology&quot;))

val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, zeroShotClassifier))

val example = Seq(&quot;I have a problem with my iphone that needs to be resolved asap!!&quot;).toDS.toDF(&quot;text&quot;)

val result = pipeline.fit(example).transform(example)
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|bart_large_zero_shot_classifier_mnli|
|Compatibility:|Spark NLP 5.1.0+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, document]|
|Output Labels:|[label]|
|Language:|en|
|Size:|467.1 MB|
|Case sensitive:|true|</content><author><name>John Snow Labs</name></author><category term="bart" /><category term="zero_shot" /><category term="en" /><category term="open_source" /><category term="tensorflow" /><summary type="html">Description This model is intended to be used for zero-shot text classification, especially in English. It is fine-tuned on MNLI by using large BART model. BartForZeroShotClassification using a ModelForSequenceClassification trained on MNLI tasks. Equivalent of BartForSequenceClassification models, but these models don’t require a hardcoded number of potential classes, they can be chosen at runtime. It usually means it’s slower but it is much more flexible. We used TFBartForSequenceClassification to train this model and used BartForZeroShotClassification annotator in Spark NLP 🚀 for prediction at scale! Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU document_assembler = DocumentAssembler() \ .setInputCol('text') \ .setOutputCol('document') tokenizer = Tokenizer() \ .setInputCols(['document']) \ .setOutputCol('token') zeroShotClassifier = BartForZeroShotClassification \ .pretrained('bart_large_zero_shot_classifier_mnli', 'en') \ .setInputCols(['token', 'document']) \ .setOutputCol('class') \ .setCaseSensitive(True) \ .setMaxSentenceLength(512) \ .setCandidateLabels([&quot;urgent&quot;, &quot;mobile&quot;, &quot;travel&quot;, &quot;movie&quot;, &quot;music&quot;, &quot;sport&quot;, &quot;weather&quot;, &quot;technology&quot;]) pipeline = Pipeline(stages=[ document_assembler, tokenizer, zeroShotClassifier ]) example = spark.createDataFrame([['I have a problem with my iphone that needs to be resolved asap!!']]).toDF(&quot;text&quot;) result = pipeline.fit(example).transform(example) val document_assembler = DocumentAssembler() .setInputCol(&quot;text&quot;) .setOutputCol(&quot;document&quot;) val tokenizer = Tokenizer() .setInputCols(&quot;document&quot;) .setOutputCol(&quot;token&quot;) val zeroShotClassifier = BartForSequenceClassification.pretrained(&quot;bart_large_zero_shot_classifier_mnli&quot;, &quot;en&quot;) .setInputCols(&quot;document&quot;, &quot;token&quot;) .setOutputCol(&quot;class&quot;) .setCaseSensitive(true) .setMaxSentenceLength(512) .setCandidateLabels(Array(&quot;urgent&quot;, &quot;mobile&quot;, &quot;travel&quot;, &quot;movie&quot;, &quot;music&quot;, &quot;sport&quot;, &quot;weather&quot;, &quot;technology&quot;)) val pipeline = new Pipeline().setStages(Array(document_assembler, tokenizer, zeroShotClassifier)) val example = Seq(&quot;I have a problem with my iphone that needs to be resolved asap!!&quot;).toDS.toDF(&quot;text&quot;) val result = pipeline.fit(example).transform(example) Model Information Model Name: bart_large_zero_shot_classifier_mnli Compatibility: Spark NLP 5.1.0+ License: Open Source Edition: Official Input Labels: [token, document] Output Labels: [label] Language: en Size: 467.1 MB Case sensitive: true</summary></entry><entry><title type="html">ALBERT Embeddings (Base Uncase)</title><link href="/2023/08/02/albert_base_uncased_en.html" rel="alternate" type="text/html" title="ALBERT Embeddings (Base Uncase)" /><published>2023-08-02T00:00:00+00:00</published><updated>2023-08-02T00:00:00+00:00</updated><id>/2023/08/02/albert_base_uncased_en</id><content type="html" xml:base="/2023/08/02/albert_base_uncased_en.html">## Description

ALBERT is &quot;A Lite&quot; version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper &quot;[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)&quot;

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_base_uncased_en_5.0.2_3.0_1690935260361.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/albert_base_uncased_en_5.0.2_3.0_1690935260361.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.albert.base_uncased').predict(text, output_level='token')
embeddings_df
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|albert_base_uncased|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|42.0 MB|
|Case sensitive:|false|

## References

[https://huggingface.co/albert-base-v2](https://huggingface.co/albert-base-v2)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="en" /><category term="english" /><category term="embeddings" /><category term="albert" /><category term="onnx" /><summary type="html">Description ALBERT is “A Lite” version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.” Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.albert.base_uncased').predict(text, output_level='token') embeddings_df Model Information Model Name: albert_base_uncased Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 42.0 MB Case sensitive: false References https://huggingface.co/albert-base-v2</summary></entry><entry><title type="html">ALBERT Embeddings (Base Uncase)</title><link href="/2023/08/02/albert_base_uncased_opt_en.html" rel="alternate" type="text/html" title="ALBERT Embeddings (Base Uncase)" /><published>2023-08-02T00:00:00+00:00</published><updated>2023-08-02T00:00:00+00:00</updated><id>/2023/08/02/albert_base_uncased_opt_en</id><content type="html" xml:base="/2023/08/02/albert_base_uncased_opt_en.html">## Description

ALBERT is &quot;A Lite&quot; version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper &quot;[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)&quot;

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_base_uncased_opt_en_5.0.2_3.0_1690935304465.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/albert_base_uncased_opt_en_5.0.2_3.0_1690935304465.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.albert.base_uncased').predict(text, output_level='token')
embeddings_df
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|albert_base_uncased_opt|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|115.0 MB|
|Case sensitive:|false|

## References

[https://huggingface.co/albert-base-v2](https://huggingface.co/albert-base-v2)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="en" /><category term="english" /><category term="embeddings" /><category term="albert" /><category term="onnx" /><summary type="html">Description ALBERT is “A Lite” version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.” Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.albert.base_uncased').predict(text, output_level='token') embeddings_df Model Information Model Name: albert_base_uncased_opt Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 115.0 MB Case sensitive: false References https://huggingface.co/albert-base-v2</summary></entry><entry><title type="html">ALBERT Embeddings (Base Uncase)</title><link href="/2023/08/02/albert_base_uncased_quantized_en.html" rel="alternate" type="text/html" title="ALBERT Embeddings (Base Uncase)" /><published>2023-08-02T00:00:00+00:00</published><updated>2023-08-02T00:00:00+00:00</updated><id>/2023/08/02/albert_base_uncased_quantized_en</id><content type="html" xml:base="/2023/08/02/albert_base_uncased_quantized_en.html">## Description

ALBERT is &quot;A Lite&quot; version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper &quot;[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.](https://arxiv.org/abs/1909.11942)&quot;

## Predicted Entities



{:.btn-box}
&lt;button class=&quot;button button-orange&quot; disabled&gt;Live Demo&lt;/button&gt;
&lt;button class=&quot;button button-orange&quot; disabled&gt;Open in Colab&lt;/button&gt;
[Download](https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/albert_base_uncased_quantized_en_5.0.2_3.0_1690935326685.zip){:.button.button-orange.button-orange-trans.arr.button-icon}
[Copy S3 URI](s3://auxdata.johnsnowlabs.com/public/models/albert_base_uncased_quantized_en_5.0.2_3.0_1690935326685.zip){:.button.button-orange.button-orange-trans.button-icon.button-copy-s3}

## How to use



&lt;div class=&quot;tabs-box&quot; markdown=&quot;1&quot;&gt;
{% include programmingLanguageSelectScalaPythonNLU.html %}
```python
embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) \
.setInputCols(&quot;sentence&quot;, &quot;token&quot;) \
.setOutputCol(&quot;embeddings&quot;)
```
```scala
val embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;)
.setInputCols(&quot;sentence&quot;, &quot;token&quot;)
.setOutputCol(&quot;embeddings&quot;)
```

{:.nlu-block}
```python
import nlu

text = [&quot;I love NLP&quot;]
embeddings_df = nlu.load('en.embed.albert.base_uncased').predict(text, output_level='token')
embeddings_df
```
&lt;/div&gt;

{:.model-param}
## Model Information

{:.table-model}
|---|---|
|Model Name:|albert_base_uncased_quantized|
|Compatibility:|Spark NLP 5.0.2+|
|License:|Open Source|
|Edition:|Official|
|Input Labels:|[token, sentence]|
|Output Labels:|[embeddings]|
|Language:|en|
|Size:|46.0 MB|
|Case sensitive:|false|

## References

[https://huggingface.co/albert-base-v2](https://huggingface.co/albert-base-v2)</content><author><name>John Snow Labs</name></author><category term="open_source" /><category term="en" /><category term="english" /><category term="embeddings" /><category term="albert" /><category term="onnx" /><summary type="html">Description ALBERT is “A Lite” version of BERT, a popular unsupervised language representation learning algorithm. ALBERT uses parameter-reduction techniques that allow for large-scale configurations, overcome previous memory limitations, and achieve better behavior with respect to model degradation. The details are described in the paper “ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.” Predicted Entities Live Demo Open in Colab Download Copy S3 URI How to use PythonScalaNLU embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) \ .setInputCols(&quot;sentence&quot;, &quot;token&quot;) \ .setOutputCol(&quot;embeddings&quot;) val embeddings = AlbertEmbeddings.pretrained(&quot;albert_base_uncased&quot;, &quot;en&quot;) .setInputCols(&quot;sentence&quot;, &quot;token&quot;) .setOutputCol(&quot;embeddings&quot;) import nlu text = [&quot;I love NLP&quot;] embeddings_df = nlu.load('en.embed.albert.base_uncased').predict(text, output_level='token') embeddings_df Model Information Model Name: albert_base_uncased_quantized Compatibility: Spark NLP 5.0.2+ License: Open Source Edition: Official Input Labels: [token, sentence] Output Labels: [embeddings] Language: en Size: 46.0 MB Case sensitive: false References https://huggingface.co/albert-base-v2</summary></entry></feed>